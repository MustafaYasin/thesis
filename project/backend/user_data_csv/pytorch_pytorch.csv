username,fullName,bio,email,repository_count,company,avatar_url,isHireable,star_time,followers,following,organizations,repositories,createdAt,updatedAt,twitterUsername,isGitHubStar,isCampusExpert,isDeveloperProgramMember,isSiteAdmin,isViewer,anyPinnableItems,viewerIsFollowing,sponsors,primary_language,yearsofExperience,location,domainofExpertise
alexbw,Alex Wiltschko,,alex.bw@gmail.com,"['Sampling Inference for Bayesian HSMMs and HMMs \n This is a Python library for approximate unsupervised sampling inference in\nBayesian Hidden Markov Models (HMMs) and explicit-duration Hidden semi-Markov\nModels (HSMMs), focusing on the Bayesian Nonparametric extensions, the HDP-HMM\nand HDP-HSMM, via the weak-limit approximation. \n There are also some plugins to extend the functionality: \n \n factorial models \n autoregressive models \n collapsed HDP sampling inference . \n \n The inference can be run in parallel over multiple cores and/or multiple\nmachines (even on EC2!) using  ipython \'s\nexcellent  ipython.parallel  module. Someday I might even document how to do\nit! \n Installing \n You can clone this library and its dependencies with \n git clone --recursive git://github.com/mattjj/pyhsmm.git \n The library depends on  numpy ,  scipy , and, for visualization,  matplotlib . \n Disabling assertions may speed things up; to disable assertions, run your\nCPython interpreter with the  -O  flag. \n A Simple Demonstration \n Here\'s how to draw from the HDP-HSMM posterior over HSMMs given a sequence of\nobservations. (The same example, along with the code to generate the synthetic\ndata loaded in this example, can be found in  examples/basic.py .) \n Let\'s say we have some 2D data in a data.txt file: \n bash\n head -5 data.txt\n-3.711962552600095444e-02 1.456401745267922598e-01\n7.553818775915704942e-02 2.457422192223903679e-01\n-2.465977987699214502e+00 5.537627981813508793e-01\n-7.031638516485749779e-01 1.536468304146855757e-01\n-9.224669847039665971e-01 3.680035337673161489e-01 \n In Python, we can plot the data in a 2D plot, collapsing out the time dimension: \n ```python\nimport numpy as np\nfrom matplotlib import pyplot as plt \n data = np.loadtxt(\'data.txt\')\nplt.plot(data[:,0],data[:,1],\'kx\')\n``` \n \n We can also make a plot of time versus the first principal component: \n python\nfrom pyhsmm.util.plot import pca_project_data\nplt.plot(pca_project_data(data,1)) \n \n To learn an HSMM, we\'ll use  pyhsmm  to create an  HSMM  instance using some\nreasonable hyperparameters. We\'ll ask this model to infer the number of states\nas well (since an HDP-HSMM is instantiated by default), so we\'ll give it an\n Nmax  parameter: \n ```python\nimport pyhsmm\nimport pyhsmm.basic.distributions as distributions \n obs_dim = 2\nNmax = 25 \n obs_hypparams = {\'mu_0\':np.zeros(obs_dim),\n                \'sigma_0\':np.eye(obs_dim),\n                \'kappa_0\':0.3,\n                \'nu_0\':obs_dim+5}\ndur_hypparams = {\'alpha_0\':2*30,\n                 \'beta_0\':2} \n obs_distns = [distributions.Gaussian( obs_hypparams) for state in range(Nmax)]\ndur_distns = [distributions.PoissonDuration( dur_hypparams) for state in range(Nmax)] \n posteriormodel = pyhsmm.models.HSMM(\n        alpha=6.,gamma=6., # better to sample over these; see concentration-resampling.py\n        init_state_concentration=6., # pretty inconsequential\n        obs_distns=obs_distns,\n        dur_distns=dur_distns,\n        trunc=60) # duration truncation speeds things up when it\'s possible\n``` \n (The first two arguments set the ""new-table"" proportionality constant for the\nmeta-Chinese Restaurant Process and the other CRPs, respectively, in the HDP\nprior on transition matrices. For this example, they really don\'t matter at\nall, but on real data it\'s much better to infer these parameters, as in\n examples/concentration_resampling.py .) \n The  trunc  parameter is an optional argument that can speed up inference: it\nsets a truncation limit on the maximum duration for any state. If you don\'t\npass in the  trunc  argument, no truncation is used and all possible state\nduration lengths are considered. \n Then, we add the data we want to condition on: \n python\nposteriormodel.add_data(data) \n (If we had multiple observation sequences to learn from, we could add them to the\nmodel just by calling  add_data()  for each observation sequence.) \n Now we run a resampling loop. For each iteration of the loop, all the latent\nvariables of the model will be resampled by Gibbs sampling steps, including the\ntransition matrix, the observation means and covariances, the duration\nparameters, and the hidden state sequence. We\'ll also copy some samples so that\nwe can plot them. \n python\nmodels = []\nfor idx in progprint_xrange(150):\n    posteriormodel.resample_model()\n    if (idx+1) % 10 == 0:\n        models.append(copy.deepcopy(posteriormodel)) \n Now we can plot our saved samples: \n python\nfig = plt.figure()\nfor idx, model in enumerate(models):\n    plt.clf()\n    model.plot()\n    plt.gcf().suptitle(\'HDP-HSMM sampled after %d iterations\' % (10*(idx+1)))\n    plt.savefig(\'iter_%.3d.png\' % (10*(idx+1))) \n \n I generated these data from an HSMM that looked like this: \n \n So the posterior samples look pretty good! \n Speed \n HSMMs constitute a much more powerful model class than plain-old HMMs, and that\nenhanced power comes with a computational price: each sampling iteration for an\nHSMM is much slower than that of an HMM. But that price is often worthwhile if\nyou want to place priors on state durations or have the model learn duration\nstructure present in the data. (In the example, strong duration structure is\nwhat made the inference algorithm latch onto the correct explanation so\neasily.) In addition, the increased cost of each iteration often pays for\nitself, since HSMM samplers empirically seem to take fewer iterations to\nconverge than comparable HMM samplers. \n Using my nothing-special i7-920 desktop machine and a NumPy/SciPy built against\nIntel\'s MKL BLAS (which generally outperforms ATLAS for vectorized operations)\nalong with the Eigen-backed classes, here\'s how long the demo iterations took: \n $ python examples/hsmm.py\n.........................  [  25/100,    0.05sec avg,    3.95sec ETA ]\n.........................  [  50/100,    0.05sec avg,    2.64sec ETA ]\n.........................  [  75/100,    0.05sec avg,    1.34sec ETA ]\n.........................  [ 100/100,    0.05sec avg,    0.05sec ETA ]\n   0.05sec avg,    5.21sec total \n Extending the Code \n To add your own observation or duration distributions, implement the interfaces\ndefined in  basic/abstractions.py . Also see the plugins. To get a flavor of\nthe style, see  pybasicbayes . \n Contributors \n Contributions by Chia-ying Lee. \n References \n \n \n Matthew J. Johnson and Alan S. Willsky.  Bayesian Nonparametric Hidden\n  Semi-Markov Models .\n  Journal of Machine Learning Research (JMLR), 14:673–701, February 2013. \n \n \n Matthew J. Johnson and Alan S. Willsky,  The Hierarchical Dirichlet Process\n  Hidden Semi-Markov Model . 26th\n  Conference on Uncertainty in Artificial Intelligence (UAI 2010), Avalon,\n  California, July 2010. \n \n \n bibtex\n@article{johnson2013hdphsmm,\n    title={Bayesian Nonparametric Hidden Semi-Markov Models},\n    author={Johnson, Matthew J. and Willsky, Alan S.},\n    journal={Journal of Machine Learning Research},\n    pages={673--701},\n    volume={14},\n    month={February},\n    year={2013},\n}', 'An analgesic for high-performance audio on iOS and OSX. \n Really fast audio in iOS and Mac OS X using Audio Units is hard, and will leave you scarred and bloody. What used to take days can now be done with just a few lines of code. \n Getting Audio \n objective-c\nNovocaine *audioManager = [Novocaine audioManager];\n[audioManager setInputBlock:^(float *newAudio, UInt32 numSamples, UInt32 numChannels) {\n    // Now you\'re getting audio from the microphone every 20 milliseconds or so. How\'s that for easy?\n    // Audio comes in interleaved, so,\n    // if numChannels = 2, newAudio[0] is channel 1, newAudio[1] is channel 2, newAudio[2] is channel 1, etc.\n}];\n[audioManager play]; \n Playing Audio \n objective-c\nNovocaine *audioManager = [Novocaine audioManager];\n[audioManager setOutputBlock:^(float *audioToPlay, UInt32 numSamples, UInt32 numChannels) {\n    // All you have to do is put your audio into ""audioToPlay"".\n}];\n[audioManager play]; \n Does anybody actually use it? \n Yep. Novocaine is result of three years of work on the audio engine of  Octave ,  Fourier  and  oScope , a powerful suite of audio analysis apps. Please do check them out! \n A thing to note: \n The RingBuffer class is written in C++ to make things extra zippy, so the classes that use it will have to be Objective-C++. Change all the files that use RingBuffer from MyClass.m to MyClass.mm. \n Want some examples? \n Inside of ViewController.mm are a bunch of tiny little examples I wrote. Uncomment one and see how it sounds. \nDo note, however, for examples involving play-through, that you should be using headphones. Having the \nmic and speaker close to each other will produce some gnarly feedback.   \n Want to learn the nitty-gritty of Core Audio? \n If you want to get down and dirty, if you want to get brave and get close to the hardware, I can only point you to the places where I learned how to do this stuff. Chris Adamson and Michael Tyson are two giants in the field of iOS audio, and they each wrote indispensable blog posts ( this is Chris\'s ,  this is Michael\'s ). Also, Chris Adamson now has a  whole gosh-darned BOOK on Core Audio . I would have done unspeakable things to get my hands on this when I was first starting. \n', 'My Code for the Netflix Prize \n I\'m not aware of folks having published their code for the Netflix Prize. Here\'s mine. \nUnder the team name ""Hi!"", I competed alone in college. I did it mostly for fun, and to learn modern machine learning techniques. It was an incredibly valuable, but strenuous, time. Well worth it on all fronts, though.  \nI peaked out at #45 or so, and then dropped out to work on my senior thesis, and came in #145 or so.   \nWhat I learned in the process was that smarter wasn\'t always better -- make an algorithm, and then scale it up, and then make a dozen tweaks to it, and then average all of the results together. That\'s how you climbed the leaderboard.    \n Anyhoo, I haven\'t touched this code in awhile, but perhaps it\'ll be useful to folks interested in competitive data mining. \nSpecifically, the lessons I learned: \n \n Get the raw data into a saved and manageable format  fast . The easier it is to load your data in and start mutating it, the better. \n If doing simple pivots on your data is hard, and slows you down from visualizing whats in your data, spend time making data structures which make that easy.   \n Generalize. Iterate. If you have a method you think will work, but it has a lot of knobs, and you don\'t know the best way to set those knobs, make it easy for you to try  every possible iteration . There is often not a good way to figure out what the  best  approach is. You will have to try many of them in order to build up an intuition. Specifically, that means (for me) a pluggable architecture. If there\'s ten ways to try a particular step, make sure you write your overarching algorithm so that it takes a function that you can pass to it, as opposed to having a method hardwired in the code. That way, you can hotswap all your ideas.   \n Speed is a feature. Of course you make sure it works first. But your goal is to see if something works. If an algorithm takes a day to run, but you can spend five hours making it run in 1/3 of a day, do it. You\'ll be running it over and over again, and you\'ll learn more if you can iterate.  \n \n As for the technical nitty-gritty, everything that\'s speed sensitive is written in Cython, which was the best balance of speed and convenience in 2009. If I were to do it al again, I would use (Numba)[http://github.com/numba/numba].   \n The original data is gone, I believe, but I might have it stored somewhere. I\'ll look for that.  ', ""NURBS - Non Uniform Rational B-Splines. \n This python module is a revival and update of Runar Tenfjord's NURBS toolbox, which itself\nis a port of Mark Spink's SCILAB/MATLAB tolbox to python with help of numpy. \n Dependency \n Python 2.0 or above \nNumPy \nDislin -> optional but recomended \nPyOpenGL -> optional   \n Installation \n Just run   python setup.py install   \n License \n Runar Tenfjord originally relased this package under the GPL Version 2 license,  \nso I suppose that means it has to stay that way.    \n Originally by: Runar Tenfjord, runten@netcom.no \nMinor updates to work with NumPy by Alex Wiltschko  "", 'cuda-tests \n Gotta learn some CUDA stuff', 'pypatent \n Scrape patents from the USPTO', 'airruler \n A ruler. Made of air. ', 'paralleltools \n A summary of parallelizing moderate amounts of work in Python', 'nsgt \n Non-stationary Gabor transforms (GitHub mirror of http://grrrr.org/research/software/nsgt/)', 'IDA \n This code accompanies the publication from the Whitesides lab to Lab on a Chip, concerning automated analysis of red blood cell health using affordable field tests. Specifically, this code implements the automated extraction of scanned AMPS tubes from flatbed scanner images, the distillation of those images into 1D data traces, and then the automated identification of the anemic disease state of the blood in those 1D data traces, as well as the prediction of continuously-varying red blood cell (RBC) parameters. \n Installation \n This code requires Python, as well as some extra 3rd-party libraries. All routines have only been tested on Mac OS, but should work on Linux. No guarantees for Windows.\n1) First, we recommend using the ""Anaconda"" Python distribution, specifically the Python 2.7 version.  Download and install here .\n1) With anaconda installed, you will need one extra library, to read TIFF files.\n pip install imageio \n1) You should be able to run the notebooks now. Inside this code repository, start up an IPython notebook:\n jupyter notebook \n1) You should now be able to browser around the ""extraction"" and ""analysis"" folders, which contain the relevant code. \n Running extraction \n The first step required will be to prepare the raw data of TIFF file scans of 4x3=12 tubes from a flatbed scanner. We will also have to associate metadata for each patient that the blood in a tube was drawn from.\nOur end goal will be a 1D array for each tube, along with the corresponding patient data (anemic state & RBC parameters). \n The extraction algorithm is explained step-by-step in the notebook  extraction/Explaining the Extraction Algorithm.ipynb .\nThe implementation we use in the paper (which also fuses patient metadata with AMPS image data) can be found in  extraction/Data Extraction Pipeline.ipynb . Most of the code in this notebook is specific to the particular Excel file format that we used to record patient metadata, and may have to be largely rewritten for reuse. \n Running analysis \n There are two sets of analyses done in the paper: classification and regression. \n Running classification analysis \n The goal of this analysis is to predict disease state only from the 1D data trace extracted from each AMPS tube. We use logistic regression, a linear classifier, and transform the 1D data representation using PCA to remove redundancy and constrain the input dimensionality of the problem. We also used Bayesian Optimization (bayesopt) to tune the hyperparameters of the problem, including the specific output dimension of PCA, the regularization parameter of logistic regression, and some image preprocessing parameters. Unfortunately, the service we used for bayesopt is no longer available. If you would like to automatically tune these parameters, we recommend either using random search (a surprisingly effective hueristic), or the open source library ""Spearmint"", upon which the now-defunct service we used was based, or products from the company SigOpt, which also implements bayesopt. The file we used to automatically tune these hypers is  classify.py .\nThe best set of hyperparameters is stored in the notebook  Analyzing best classification.ipynb .\nThis notebook examines ROC performance of the classifier for discriminating different anemia types, as well as the effect of centrifugation time of the tube on IDA classification AUC performance. \n Running regression analysis. \n Similar to above, we used a defunct bayesopt service to automatically tune some parameters of this algorithm. The original file is in  regress.py . The original methodology will work well, even without automated tuning.\nThe best set of hyperparameters is stored in the notebook  Analyzing best regression.ipynb . \n License \n See the LICENSE file. It is under a GPL license, and this code may only be used for academic purposes.', ""adabayes \n To do: \n \n [x] Find candidate last layer features (DeCAF, Overfeat) \n [x] Find AlexNet code (5 convolutional layers with max-pooling, then three fully connected layers) \n [X]  Find download script for MNIST dataset \n [X]  Find download script for CIFAR10 dataset \n [x] Fork torch-dist repo \n [x] Update torch-dist repo for OS X 10.10 install \n [x] Add new required submodules to distro (nnx, iTorch, ccn2, cunnx, cudnn, sdl2, cutorch) \n [x] Figure out model serialization and loading \n Built-in model serialization.  Loading  and  saving . \n There is also facebook's  Thrift library , which I haven't seen many examples for. \n \n \n [ ] Get the data out of the  DataSource  models that we're using \n [ ] Train and Whetlab a net on MNIST (to get the whole workflow going) \n [ ] Train and Whetlab a net on CIFAR10 \n [ ] Train and Whetlab a net on STL10 \n [ ] Build dumb ensemble on CIFAR10 \n [ ] Build dumb ensemble on STL10 \n [ ] Grok boosting criterion \n [ ] Whetlab a net with progressive ensembling on CIFAR10 \n [ ] Whetlab a net with progressive ensembling on STL10 \n [ ] Whetlab a net with progressive ensembling on the last-layer features of ImageNet \n [ ] Whetlab a net with progressive ensembling and tuned class weights on CIFAR10 \n [ ] Whetlab a net with progressive ensembling and tuned class weights on STL10 \n [ ] Extract last-layer ImageNet features \n [ ] Host last-layer ImageNet features \n [ ] Build download script for last-layer ImageNet features \n [ ] Train and Whetlab a net on last-layer features on ImageNet \n [ ] Build a dumb ensemble on ImageNet \n [ ] Whetlab a net with progressive ensembling and tuned class weights on the last-layer features of ImageNet \n \n Collecting some resources \n AlexNet:\nhttps://github.com/soumith/convnet-benchmarks/blob/master/torch7/imagenet_winners/alexnet.lua \n I think this is NiN:\nhttps://github.com/soumith/convnet-benchmarks/blob/master/torch7/imagenet_winners/googlenet.lua \n OverFeat:\nhttps://github.com/facebook/fbcunn/blob/master/examples/imagenet/models/overfeat_cunn.lua \n Multiclass criterion:\nhttps://github.com/torch/nn/blob/master/doc/criterion.md#classnllcriterion \n Some other interesting nets:\nhttps://github.com/culurciello/profiling \n Bunch of demos, not all nets:\nhttps://github.com/torch/demos \n Overfeat features:\nhttp://cilvr.nyu.edu/doku.php?id=software:overfeat:start \n Decaf ImageNet submission:\nhttps://github.com/UCB-ICSI-Vision-Group/decaf-release/wiki/imagenet \n Some other net implementations:\nhttps://github.com/eladhoffer/ImageNet-Training"", ""Conda recipes for installing packages from the Torch ecosystem. \n NOTE: No longer maintained. \n To install packages \n ``` \n Install anaconda if you don't have it (instructions here for OS X) \n wget http://repo.continuum.io/miniconda/Miniconda-latest-MacOSX-x86_64.sh\nsh Miniconda-latest-MacOSX-x86_64.sh -b -p $HOME/anaconda \n Add anaconda to your $PATH \n export PATH=$HOME/anaconda/bin:$PATH \n Install Lua & Torch \n conda install lua=5.2 lua-science -c alexbw \n Available versions of Lua: 2.0, 5.1, 5.2, 5.3 \n 2.0 is LuaJIT \n ``` \n To build packages \n ``` \n Install anaconda if you don't have it (instructions here for OS X) \n wget http://repo.continuum.io/miniconda/Miniconda-latest-MacOSX-x86_64.sh\nsh Miniconda-latest-MacOSX-x86_64.sh -b -p $HOME/anaconda \n Add anaconda to your $PATH \n export PATH=$HOME/anaconda/bin:$PATH \n Get the newest version of conda, as well as some conda build tools \n conda update conda -y\nconda install conda-build anaconda-client -y \n Build all packages \n sh build_all.sh \n Ideally, all you have to do to install everything is this \n conda install lua=5.2 lua-science\n``` \n TODO:\n - https://github.com/AlexMili/torch-dataframe\n - https://github.com/twitter/torch-ipc\n - https://github.com/twitter/torch-distlearn\n - https://github.com/twitter/torch-dataset \n Resources: \n \n \n Making packages relocatable (LuaJIT hard-codes path) \n \n \n Build instructions for luarocks \n \n \n Build instructions for Lua \n \n \n Patching files with git diffs is finicky \n \n \n Linking against readline  (need a few extra flags, and link against -lncursesw, not -lncurses) \n \n \n Upgrading old Debian .  Also this . \n \n \n Misc notes:\n metadata:ns_cfg() — defines for YAML directives\nmain_build: —\xa0defines version numbers \nenviron:get_lua_include_dir() —\xa0uses version numbers to locate the include directory\nconfig:Config._get_lua —\xa0uses version numbers to locate the binary\nThis is where the linked package name is converted into what is used"", ""validata \n Continuous integration for your data \n We do continuous integration on code. Why not data?\nValidata is a small package to run basic sanity checks on your data.\nI haven't found anything that aggregates all of these checks and tricks in one place. \n There is only one method which is exposed,  check(data,labels)  (optionally taking data or labels).\nIf any data check fails, it throws a well-named error, as well as hints for how you might fix the problem -- data covariance matrix ill-conditioned? Try whitening. \n Initially, this will be a Python/NumPy only package running basic checks, but hopefully it becomes a resource of data sanity and sanitation checks.\nStill very much a work in progress. \n Examples (some implemented, some not) include: \n \n If your labels are one-hot, are you using all slots? \n Is the covariance matrix of your data ill-conditioned? \n Do you have any constant variables? \n Can you train a classifier to distinguish train and test data, using whether they are in train or test as a label? Indicates different data distributions. \n If you're using integer labels, are the unique labels contiguous? \n Do you have just one unique label? \n Is the data under different labels statistically separable? \n If you have an old dataset and a new dataset (or two halves of the same dataset), is the distribution of each dimension stationary? Check for divergence with a KS test. \n What else? I end up applying these tricks in a very ad hoc fashion, whenever a subtle bug pops up, and not rigorously before each project I tackle. I'd like to stuff all these tricks in one place, and run them like a unit test, or a continuous integration test, on data that I start working with. \n \n Should probably also think about engarde"", ""bayarea-dl-summerschool \n Torch notebooks and slides for the Bay Area Deep Learning Summer School \n Installation Instructions \n Install anaconda if you don't have it (instructions here for OS X) \n wget http://repo.continuum.io/miniconda/Miniconda-latest-MacOSX-x86_64.sh\nsh Miniconda-latest-MacOSX-x86_64.sh -b -p $HOME/anaconda \n Add anaconda to your $PATH \n export PATH=$HOME/anaconda/bin:$PATH \n Install Lua & Torch \n ```\nconda install lua=5.2 lua-science -c alexbw \n Although, you could install other Lua versions like 2.0 (LuaJIT), 5.1, 5.2 and 5.3 \n ``` \n Clone this repository and start the notebook server \n ```\ngit clone https://github.com/alexbw/bayarea-dl-summerschool.git\ncd bayarea-dl-summerschool\nitorch notebook \n Will open a browser tab, then you can navigate to the notebooks \n ```""]",['repository_count'],Google,https://avatars.githubusercontent.com/u/161935?u=f722a589176ab3dce6285bcce174117e3c103ec3&v=4,False,2016-08-15 18:39:55,396,332,0,2009-12-04T14:50:40Z,2022-08-23T18:31:52Z,,False,False,False,False,False,True,False,0,"{'C': 5, 'Objective-C': 5, 'C++': 1, 'Python': 14, 'JavaScript': 3, 'C#': 1, 'Shell': 2, 'Jupyter Notebook': 5, 'Lua': 4}",4,"Boston, MA",Data Analytics,44,0.1,0.26,0.06,0
iassael,Yannis Assael,Research Scientist,iassael@gmail.com,"['DEARanking \n Proposing a hybrid DEA/Polynomial Interpolation (DEA/PI) algorithm for the raking of protected areas: An application in Greece', 'Find The Word (Τηλεκύβος Cheat) \n Find the greek word from the given letters. Application to cheat and solve games like ""Τηλεκύβος"" and ""Βρες τη λέξη""', 'csoxcal \n Oxford University, Computer Science Calendar Filter for Google Calendar use', 'A Hybrid Parallel Implementation of the Aho-Corasick and Wu-Manber Algorithms Using NVIDIA CUDA and MPI Evaluated on a Biological Sequence Database. \n Charalampos S. Kouzinopoulos, Yannis M. Assael, Themistoklis K. Pyrgiotis, Konstantinos G. Margaritis \n Multiple matching algorithms are used to locate the occurrences of patterns from a finite pattern set in a large input string. Aho-Corasick and Wu-Manber, two of the most well known algorithms for multiple matching require an increased computing power, particularly in cases where large-size datasets must be processed, as is common in computational biology applications. Over the past years, Graphics Processing Units (GPUs) have evolved to powerful parallel processors outperforming Central Processing Units (CPUs) in scientific calculations. Moreover, multiple GPUs can be used in parallel, forming hybrid computer cluster configurations to achieve an even higher processing throughput. This paper evaluates the speedup of the parallel implementation of the Aho-Corasick and Wu-Manber algorithms on a hybrid GPU cluster, when used to process a snapshot of the Expressed Sequence Tags of the human genome and for different problem parameters. \n Links \n arXiv pre-print \n Bibtex \n @article{kouzinopoulos2015hybrid,\n  title={A Hybrid Parallel Implementation of the Aho-Corasick and Wu-Manber Algorithms Using NVIDIA CUDA and MPI Evaluated on a Biological Sequence Database},\n  author={Kouzinopoulos, Charalampos S. and Assael, Yannis M. and Pyrgiotis, Themistoklis K. and Margaritis, Konstantinos G.},\n  journal={International Journal on Artificial Intelligence Tools},\n  volume={24},\n  number={1},\n  pages={1540001},\n  year={2015},\n  publisher={World Scientific}\n} \n License \n Code licensed under the GNU General Public License v3.0.', 'RKHS Function \n Description \n Synthetic heteroscedastic 1D function generated from 2 Squared Exponential kernels for Bayesian Optimization method evaluation tasks  \n Input Space \n x ∈ [0, 1] \n Global Maximum \n x=0.89235, f(x)=5.73839 \n Authors \n Copyright (C) 2014 Ziyu Wang, John-Alexander Assael, Nando de Freitas\npublished under GPLv3 license \n Screenshot \n', 'Decomposition module for Torch7 \n \n \n Principal Component Analysis (PCA) \n \n \n Whitened Principal Component Analysis (W-PCA) \n \n \n Linear Discriminant Analysis (LDA) \n \n \n Locality Preserving Projections (LPP) \n \n \n Neighbourhood Preserving Projections (NPP) \n \n \n Fast Independent Component Analysis (FastICA) \n \n \n by John-Alexander Assael \n http://www.johnassael.com \n https://github.com/iassael/torch7-decomposition \n Installation \n Clone this repository or download the source code. \n Usage \n Call  decomposition = require ""decomposition"" \nand then any of the following: \n \n \n decomposition.pca(x) , \n \n \n decomposition.lda(x, y) , \n \n \n decomposition.lpp(x) , \n \n \n decomposition.npp(x) , \n \n \n decomposition.fastica(x) . \n \n \n Alternativly, you can use iTorch notebook and open  decomposition.ipynb . \n Contributing \n \n Fork it! \n Create your feature branch:  git checkout -b my-new-feature \n Commit your changes:  git commit -am \'Add some feature\' \n Push to the branch:  git push origin my-new-feature \n Submit a pull request \n \n Notes \n The implementations were developed in terms of learning and may not be optimal. \n License \n Copyright (C) 2015 John-Alexander Assael (www.johnassael.com)\nhttps://github.com/iassael/torch7-decomposition \n The MIT License (MIT) \n Permission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the ""Software""), to deal in\nthe Software without restriction, including without limitation the rights to\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\nof the Software, and to permit persons to whom the Software is furnished to do\nso, subject to the following conditions: \n The above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software. \n THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.', 'Torch7 impementation of: \n Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images \n by Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, Martin Riedmiller (http://arxiv.org/abs/1506.07365) \n Implemented by John-Alexander M. Assael (iassael@gmail.com) and Marc P. Deisenroth. \n The MIT License (MIT)\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the ""Software""), to deal in\nthe Software without restriction, including without limitation the rights to\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\nof the Software, and to permit persons to whom the Software is furnished to do\nso, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n', 'Regularization of Neural Networks using DropConnect \n Li Wan, Matthew Zeiler, Sixin Zhang, Yann LeCun, Rob Fergus \n Dept. of Computer Science, Courant Institute of Mathematical Science, New York University \n Torch7 implementation by John-Alexander M. Assael', 'Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) \n Djork-Arné Clevert, Thomas Unterthiner, Sepp Hochreiter \n Torch7 implementation by John-Alexander M. Assael', 'Deep Exploration via Bootstrapped DQN \n Ian Osband, Charles Blundell, Alexander Pritzel, Benjamin Van Roy \n Usage \n nn.Bootstrap(nn.Linear(size_in, size_out), 10, 0.08) \n Implemented by Yannis M. Assael (www.yannisassael.com)', 'Recurrent Batch Normalization \n Batch-Normalized LSTMs \n Tim Cooijmans, Nicolas Ballas, César Laurent, Çağlar Gülçehre, Aaron Courville \n http://arxiv.org/abs/1603.09025 \n Usage \n local rnn = LSTM(input_size, rnn_size, n, dropout, bn) \n n = number of layers (1-N) \n dropout = probability of dropping a neuron (0-1) \n bn = batch normalization (true, false) \n Example \n https://github.com/iassael/char-rnn \n Performance \n Validation scores on char-rnn with default options \n \n Implemented in Torch by Yannis M. Assael (www.yannisassael.com)', 'Grid World DQN using torch7 \n This is a naive DQN example implemented in torch7 to help future research. \n The environment is based on  rlenvs  of  Kaixhin  and the model makes use of ""Increasing the Action Gap"" ( http://arxiv.org/abs/1512.04860 ). \n Implemented by Yannis M. Assael ( yannisassael.com )', 'Single pendulum Deterministic Policy Gradient example using torch7 \n Continuous Control with Deep Reinforcement Learning \n Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra \n http://arxiv.org/abs/1509.02971 \n Dependecies \n luarocks install Math-RungeKutta\nluarocks install csvigo\nluarocks install image\nluarocks install hdf5 \n Implemented by Yannis M. Assael ( yannisassael.com )', 'Using deep Q-learning to understand the tax evasion behavior of risk-averse firms \n Links \n \n arXiv preprint \n Expert Systems with Applications \n \n Execution \n $ # Requirements: torch\n$ th tax-dqn.lua \n Bibtex \n @article{goumagias2018using,\n  title={Using deep Q-learning to understand the tax evasion behavior of risk-averse firms},\n  author={Goumagias, Nikolaos D and Hristu-Varsakelis, Dimitrios and Assael, Yannis M},\n  journal={Expert Systems with Applications},\n  volume={101},\n  pages={258--270},\n  year={2018},\n  publisher={Elsevier}\n}\n \n License \n Code licensed under the Apache License v2.0', 'Restoring ancient text using deep learning \n A case study on Greek epigraphy \n Yannis Assael * , Thea Sommerschield * , Jonathan Prag \n \n Ancient History relies on disciplines such as Epigraphy, the study of ancient inscribed texts, for evidence of the recorded past. However, these texts, ""inscriptions"", are often damaged over the centuries, and illegible parts of the text must be restored by specialists, known as epigraphists.\nThis work presents a novel assistive method for providing text restorations using deep neural networks.\nTo the best of our knowledge, Pythia is the first ancient text restoration model that recovers missing characters from a damaged text input.\nIts architecture is carefully designed to handle long-term context information, and deal efficiently with missing or corrupted character and word representations. \nTo train it, we wrote a non-trivial pipeline to convert PHI, the largest digital corpus of ancient Greek inscriptions, to machine actionable text, which we call PHI-ML.\nOn PHI-ML, Pythia\'s predictions achieve a 30.1% character error rate, compared to the 57.3% of human epigraphists. Moreover, in 73.5% of cases the ground-truth sequence was among the Top-20 hypotheses of Pythia, which effectively demonstrates the impact of such an assistive method on the field of digital epigraphy, and sets the state-of-the-art in ancient text restoration. \n \n \nPythia-Bi-Word processing the phrase μηδέν ἄγαν (mēdén ágan) ""nothing in excess"", a fabled maxim inscribed on Apollo\'s temple in Delphi. The letters ""γα"" are missing, and annotated with ""?"". Since word ἄ??ν contains missing characters, its embedding is treated as unknown (""unk""). The decoder outputs correctly ""γα"".\n \n References \n \n arXiv pre-print \n EMNLP-IJCNLP 2019 \n Digital Classicist Wiki \n DeepMind research blog \n University of Oxford news blog \n \n When using any of this project\'s source code, please cite:\n @inproceedings{assael2019restoring,\n  title={Restoring ancient text using deep learning: a case study on {Greek} epigraphy},\n  author={Assael, Yannis and Sommerschield, Thea and Prag, Jonathan},\n  booktitle={Empirical Methods in Natural Language Processing},\n  pages={6369--6376},\n  year={2019}\n} \n Pythia online \n To aid further research in the field we created an online interactive python notebook, where researchers can query one of our models to get text restorations and visualise the attention weights. \n \n Google Colab \n \n Pythia offline \n The following snippets provide references for regenerating PHI-ML and training new models offline. \n Dependencies \n pip install -r requirements.txt && \\\npython -m nltk.downloader punkt \n PHI-ML dataset generation \n ``` \n Download PHI (this will take a while) \n python -c \'import pythia.data.phi_download; pythia.data.phi_download.main()\' \n Process and generate PHI-ML \n python -c \'import pythia.data.phi_process; pythia.data.phi_process.main()\'\n```\nPreprocessed PHI-ML uploaded by @Holger.Danske800:  link \n Training \n python -c \'import pythia.train; pythia.train.main()\' \n Evaluation \n python -c \'import pythia.test; pythia.test.main()\' --load_checkpoint=""your_model_path/"" \n Docker execution \n ./build.sh\n./run.sh <GPU_ID> python -c \'import pythia.train; pythia.train.main()\' \n License \n Apache License, Version 2.0 \n \n \nDamaged inscription: a decree concerning the Acropolis of Athens (485/4 BCE).  IG  I 3  4B. (CC BY-SA 3.0, WikiMedia)\n', 'Vivechrom RGB color matcher \n Yannis Assael ( www.assael.gr ) \n Description \n This colaboratory notebook is used to find the closest vivechrom.gr paint code given an RGB color. \n Instructions \n \n Open this  Google Colab Notebook . \n To edit and run the code, you need to click \'Open in Playground Mode\' if you can see it at the top of the page, or save a local copy somewhere on your computer. \n The cells need to be run one by one by pressing shift+enter or by using the play button. \n Fill the  target_color_rgb  value with your target RGB color. \n Run the cell to get suggestions ordered by color similarity. \n \n License \n ```\nCopyright 2020 Yannis Assael \n Licensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at \n http://www.apache.org/licenses/LICENSE-2.0\n \n Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```', '\n \n \n Restoring and attributing ancient texts using deep neural networks \n Yannis Assael 1,* , Thea Sommerschield 2,3,* , Brendan Shillingford 1 , Mahyar Bordbar 1 , John Pavlopoulos 4 ,\nMarita Chatzipanagiotou 4 , Ion Androutsopoulos 4 , Jonathan Prag 3 , Nando de Freitas 1 \n 1  DeepMind, United Kingdom \n 2  Ca’ Foscari University of Venice, Italy \n 3  University of Oxford, United Kingdom \n 4  Athens University of Economics and Business, Greece \n *  Authors contributed equally to this work \n \n \n Ancient History relies on disciplines such as Epigraphy, the study of inscribed\ntexts known as ""inscriptions"", for evidence of the thought, language, society\nand history of past civilizations. However, over the centuries many inscriptions\nhave been damaged to the point of illegibility, transported far from their\noriginal location, and their date of writing is steeped in uncertainty. We\npresent Ithaca, the first Deep Neural Network for the textual restoration,\ngeographical and chronological attribution of ancient Greek inscriptions. Ithaca\nis designed to assist and expand the historian’s workflow: its architecture\nfocuses on collaboration, decision support, and interpretability. \n \n \n Restoration of damaged inscription: this inscription ( IG  I 3  4B) records a decree concerning the Acropolis of Athens and dates 485/4 BCE. (CC BY-SA 3.0, WikiMedia) \n \n While Ithaca alone achieves 62% accuracy when restoring damaged texts, as soon\nas historians use Ithaca their performance leaps from 25% to 72%, confirming\nthis synergistic research aid’s impact. Ithaca can attribute inscriptions to\ntheir original location with 71% accuracy and can date them with a distance of\nless than 30 years from ground-truth ranges, redating key texts of Classical\nAthens and contributing to topical debates in Ancient History. This work shows\nhow models like Ithaca can unlock the cooperative potential between AI and\nhistorians, transformationally impacting the way we study and write about one of\nthe most significant periods in human history. \n \n \n Ithaca\'s architecture processing the phrase ""δήμο το αθηναίων"" (""the people of Athens""). The first 3 characters of the phrase were hidden and their restoration is proposed. In tandem, Ithaca also predicts the inscription’s region and date. \n \n References \n \n Nature article \n DeepMind blog \n \n When using any of this project\'s source code, please cite: \n @article{asssome2022restoring,\n  title = {Restoring and attributing ancient texts using deep neural networks},\n  author = {Assael*, Yannis and Sommerschield*, Thea and Shillingford, Brendan and Bordbar, Mahyar and Pavlopoulos, John and Chatzipanagiotou, Marita and Androutsopoulos, Ion and Prag, Jonathan and de Freitas, Nando},\n  doi = {10.1038/s41586-022-04448-z},\n  journal = {Nature},\n  year = {2022}\n} \n Ithaca inference online \n To aid further research in the field we created an online interactive python notebook, where researchers can query one of our trained models to get text restorations, visualise attention weights, and more. \n \n Ithaca Interactive Interface \n Google Colab for using Ithaca for your research \n \n Ithaca inference offline \n Advanced users who want to perform inference using the trained model may want\nto do so manually using the  ithaca  library directly. \n First, to install the  ithaca  library and its dependencies, run:\n sh\npip install . \n Then, download the model via\n sh\ncurl --output checkpoint.pkl https://storage.googleapis.com/ithaca-resources/models/checkpoint_v1.pkl \n An example of using the library can be run via\n sh\npython inference_example.py --input_file=example_input.txt \nwhich will run restoration and attribution on\nthe text in  example_input.txt . \n To run it with different input text, run\n```sh\npython inference_example.py --input=""..."" \n or using text in a UTF-8 encoded text file: \n python inference_example.py --input_file=some_other_input_file.txt\n``` \n The restoration or attribution JSON can be saved to a file:\n sh\npython inference_example.py \\\n  --input_file=example_input.txt \\\n  --attribute_json=attribute.json \\\n  --restore_json=restore.json \n For full help, run:\n sh\npython inference_example.py --help \n Dataset generation \n Ithaca was trained on The Packard Humanities Institute’s\n"" Searchable Greek Inscriptions "" public\ndataset. The processing workflow for generating the machine-actionable text and\nmetadata, as well as further details on the train, validation and test splits\nare available at  I.PHI dataset . \n Training Ithaca \n See  train/README.md  for instructions. \n License \n Apache License, Version 2.0']",['repository_count'],DeepMind,https://avatars.githubusercontent.com/u/216211?u=52e4429ac6db432677480a6129748fb6e5350b85&v=4,True,2016-08-15 19:14:32,266,40,0,2010-03-05T00:49:40Z,2022-07-29T17:00:44Z,,False,False,False,False,False,True,False,0,"{'JavaScript': 1, 'Java': 2, 'PHP': 1, 'C': 1, 'Python': 5, 'Lua': 11, 'Jupyter Notebook': 5}",6,London,Data Management,91,0.95,0.26,0.58,0
bshillingford,Brendan Shillingford,,,"['id3tag-fix', 'codehackathon2014 \n Team members:\nBrendan Shillingford\nDaisy Shih\nKevin Lim \n http://data.gc.ca/eng/canadian-open-data-experience-code', 'pycmdline-template \n Template for making your one-off Python utilities more user-friendly. \n Add  @arg  and  @cmdline  decorators to a main function as below; this provides an easier interface to the  logging  and  optparse  modules. \n Examples \n Example 1: simple program \n This program accepts 2 arguments: one a flag passed as  --count=COUNT , or  -n COUNT ; another is a position argument, listed without a dash-prefixed flag name before it; and a default help flag  --help / -h  printing a usage page. \n The arguments to the decorators are the same as in  optparse . \n ```python \n !/usr/bin/env python \n from simpleargs import cmdline, arg \n @arg(\'argument\', help=""pass ARG to program"", metavar=""ARG"") # simple positional argument\n@arg(\'--count\', \'-n\', help=""number of bars to foo"", type=int, default=1)\n@cmdline(description=""This program foos bars a given number of times."")\ndef main(argument, count): \n print(""Hello there."")\n\nprint(""Your Argument: %s"" % argument)\n\nfor i in range(count):\n    print(""Fooing bar #%d"" % i)\nprint(""Fooed %d bars."" % count)\n \n if  name  == \' main \':\n    main()\n``` \n Example 2: simple program + logging \n ``` \n !/usr/bin/env python \n import logging as L\nfrom simpleargs import cmdline, arg, loglevel \n @arg(\'argument\', help=""pass ARG to program"", metavar=""ARG"") # simple positional argument\n@arg(\'--count\', \'-n\', type=int, default=1)\n@loglevel()                                                 # if on, log level DEBUG, else INFO\n@cmdline(description=""This program foos bars a given number of times."")\ndef main(argument, count): \n print ""Hello there.""\n\nL.info(""You passed an argument."")\nL.debug(""Your Argument: %s"" % argument)\n\nfor i in range(count):\n    L.debug(""Fooing bar #%d"" % i)\nL.info(""Fooed %d bars."" % count)\n \n if  name  == \' main \':\n    main()\n```', 'Starbucks WiFi autologin \n Automatically accept ToS when connecting. \n Barely tested. \n This tool is purely for educational purposes only, and should \nnot be used in a real situation. \n Dependencies: \n \n Python 2 \n requests \n BeautifulSoup \n \n Installation: \n Can just execute starbuckswifi.py directly, but install if you want. \n Install via:\n``` \n python2 setup.py install \n When you connect to an accesspoint, run \n$ python2 -m starbuckswifi\n```', 'REST and MQTT for IFTTT \n What\'s IFTTT? \n IFTTT is a service that enables users to  connect different web applications  (e.g., Facebook, Evernote, Weather, Dropbox, etc.) together through simple conditional statements known as ""Recipes"". [Source: Wikipedia] \n A recipe is a (trigger, action) pair. However, IFTTT doesn\'t support fully custom actions. However, it does support publishing to any Wordpress blog. This app emulates Wordpress\'s XML-RPC API so that you can send a  GET / POST / PUT  REST request, or publish a MQTT message, all by choosing the ""create post"" action in IFTTT. \n Install \n Simply clone this repository and upload as a Heroku app, or host it however you want. \n For local testing, you can use  flaskrun.py . If desired, use a  virtualenv , and install all the packages in  requirements.txt . \n Usage \n Set the  title  and  body  of the post. Everything else is ignored. \n Examples: \n \n title =  GET http://httpbin.org/get?some=stuff&here \n title =  POST http://example.com/something , body =  whatever data you want to post  (similarly for  PUT ) \n title =  MQTT-PUB mqtt://iot.eclipse.org/topic/name/goes/here?qos=1&retain=T , body =  the payload of your request \n \n Notice the URI format used to encode MQTT settings. \n MQTT-PUB syntax \n The URI is parsed as follows:  mqtt://{hostname}[:{port}]/{topic-name}[?{settings}] . The post\'s body is used as the payload and may be empty, but  note IFTTT may remove newlines or mess with whitespace . Hence, I suggest using JSON if possible. \n Settings must be URI-encoded as usual; the following are valid:\n*  qos  =  0  or  1  or  2 , default is  0  (see MQTT specifications for semantics)\n*  retain  =  true  or  false , default is  false  ( 1 ,  T ,  t , and  tRUe  are all accepted as true)\n*  protocol  =  MQTTv31  or  MQTTv311 , latter is default', ""autobw \n autobw  is a simple library for automatically performing  a backwards pass, given only a forwards pass,  in Torch. \n A major advantage of this is that the neural network's  structure need not be fixed before runtime . This allows for easy implementation of structures such as recurrent networks. See the example below. \n Backpropagation is often described as a method for propagating gradients through a computational graph. One way to implement it for graphs is to explicitly construct a graph given by the user, then evaluate the computational nodes in the order specified in the forward pass, then again but in reverse for the backward pass.  \n Install \n luarocks install https://raw.githubusercontent.com/bshillingford/autobw.torch/master/autobw-scm-1.rockspec \n Details \n A method that's closer to how one may reason about a neural network is to explicitly write down a forward pass while recording the statements as they are being executed, then execute the statements' derivative computations (aka adjoint) in reverse. This is equivalent to specifying a computation graph, but more explicit, and allows the user to use  control-flow such as for loops and conditionals . \n This is similar to the approach taken by implementations of reverse-mode automatic differentiation, see e.g.  http://arxiv.org/abs/1502.05767 . \n Examples: \n A simple example of computing  linear(x1) + x2 * sigmoid(x3) , but  randomly  replacing  sigmoid(x3)  with  x3  sometimes:\n```lua\nlin = nn.Linear(5,5)\nadd = nn.CAddTable()\nmul = nn.CMulTable()\nsigm = nn.Sigmoid() \n tape = autobw.Tape() \n -------------- START OF FORWARD PASS --------------\n-- records the sequence of operations\ntape:begin()\ncoin_flip = torch.rand(1)[1]\nval1 = lin:forward(x1) \n if coin_flip > 0.5 then\n  maybe_sigmoid = sigm:forward(x3)\nelse\n  maybe_sigmoid = x3\nend \n result = add:forward{val1, mul:forward{x2, maybe_sigmoid}}\ntape:stop()\n-------------- END OF FORWARD PASS -------------- \n -- Play it back in reverse:\ntape:backward() \n -- Now, the gradients are in the four nn.Module objects as usual.\n``` \n Note: I don't actually use the gradients at all here, and I don't set them to zero first, just to keep the example simple.\nSee also  our nngraph practical  for the equivalent in  nngraph . \n RNN Example \n See the  examples folder  for a  fully functional rnn example  with toy data. \n LSTM example \n The LSTM example  https://github.com/oxford-cs-ml-2015/practical6  can easily be shortened by using this. We delete the backward pass, and simply play it back from the recorded forward pass:\n```lua\n-- setup autodiff\ntape = Tape() -- TODO: local \n -- do fwd/bwd and return loss, grad_params\nfunction feval(x)\n    if x ~= params then\n        params:copy(x)\n    end\n    grad_params:zero() \n ------------------ get minibatch -------------------\nlocal x, y = loader:next_batch()\n\n------------------- forward pass -------------------\ntape:begin() -----------------\nlocal embeddings = {}            -- input embeddings\nlocal lstm_c = {[0]=initstate_c} -- internal cell states of LSTM\nlocal lstm_h = {[0]=initstate_h} -- output values of LSTM\nlocal predictions = {}           -- softmax outputs\nlocal loss = 0\n\nfor t=1,opt.seq_length do\n    embeddings[t] = clones.embed[t]:forward(x[{{}, t}])\n\n    -- we're feeding the *correct* things in here, alternatively\n    -- we could sample from the previous timestep and embed that, but that's\n    -- more commonly done for LSTM encoder-decoder models\n    lstm_c[t], lstm_h[t] = unpack(clones.lstm[t]:forward{embeddings[t], lstm_c[t-1], lstm_h[t-1]})\n\n    predictions[t] = clones.softmax[t]:forward(lstm_h[t])\n    loss = loss + clones.criterion[t]:forward(predictions[t], y[{{}, t}])\nend\ntape:stop() -----------------\n\n------------------ backward pass -------------------\ntape:backward()\n\n------------------------ misc ----------------------\n-- transfer final state to initial state (BPTT)\ninitstate_c:copy(lstm_c[#lstm_c])\ninitstate_h:copy(lstm_h[#lstm_h])\n\n-- clip gradient element-wise\ngrad_params:clamp(-5, 5)\n\nreturn loss, grad_params\n \n end\n```"", ""gzfile \n Conveniently (and with decent performance) read and write data from  gzip  \nfiles directly. Useful for text or very large files. \n Implemented as an FFI wrapper for zlib, including wrappers for C  FILE  \nfunctions like  fscanf  and  fwrite . More can be easily added. \n Install:\n luarocks install https://raw.githubusercontent.com/bshillingford/lua_gzipfile/master/gzfile-scm-1.rockspec \n Example: \n Read 200 floats from a gzipped file directly into a torch tensor:\n```lua\nrequire 'torch'\nlocal GZFile = require 'gzfile.GZFile' \n local tensor = torch.FloatTensor(200)\nlocal f = GZFile('floats.gz', 'rb')\nf:readbuf(tensor:data(), 200 4)  -- sizeof(float)=4; read 200 4 bytes\nf:close()\n-- now do stuff with the tensor\n```\nNote: compressing floats can be useful for neural nets, since similar values at\nsimilar orders of magnitude will often results in repeated byte patterns. \n To access the underlying  FILE*  handle, use the  handle  property of  GZFile . \n Functions implemented: \n \n Constructor:  file = GZFile(filename, mode) \n    Opens the file for reading or writing using the given mode.\n    See zlib gzopen() documentation for supported modes. \n :close()  Closes the file. Later operations will fail. \n :write(str) \n    Writes a lua string. Doesn't allocate memory, just casts using ffi.\n    Calls fwrite and returns its ret val, i.e. number of bytes written. \n :read(nbytes) \n    Reads to a buffer then turns into a lua string.\n    Allocates memory on each call, so slightly inefficient if you do many\n    reads. \n :writebuf(ptr, nbytes) \n    Writes data from the given buffer to the file.\n    Returns number of bytes written. \n :readbuf(ptr, nbytes) \n    Reads into the given location in memory.\n    Returns number of bytes read. \n :flush() \n :peek() \n :tell() \n    Returns the position in the file using ftell. \n :seek(offset, origin) \n    Seek using fseek, relative to beginning of file.\n    Note the argument order matches C's rather than Lua's io.\n    Returns new position from ftell. \n :getc() \n :scanf(fmt, typestring) \n    Calls fscanf, but only for reading a single field e.g.  %s .\n    Allocates memory automatically, this memory will be gc'd by lua.\n    Typestring examples:  'float[1]' ,  'uint8_t[1]' ,  'char[16]' . \n"", 'Class visualizer for lua/torch \n Class hierarchy visualizer for torch classes, and classic classes (see github.com/deepmind/classic) \n Currently generates a d3 visualization, easily adapted to graphviz. \n Example of viewing the classes in  nn \n th generate.lua htmld3tree nn > output.html \nThen open  output.html  in your browser. \n Usage: \n sh\nth [-lclassic] generate.lua output_mode package1 [package2 [package3 ...] ] \nwhere  output_mode  is  htmld3force  or  htmld3tree . See e.g.  htmld3tree.lua ; more can be added. \n \n Output is to stdout; redirect it to a file like  output.html . \n Use  -lclassic  to monitor classic classes instead. (see  https://github.com/deepmind/classic ) \n \n Useful extensions/TODOs: \n \n Output to graphviz \n Parse libraries for documentation and include it in the visualization? \n At the least, actually do something useful when clicking on the classes. \n \n Internals \n For  torch.class  classes, it simply monkeypatches this function to monitor created classes when\nthe packages get  require d. For classic classes, it uses a central (private) registry storing \nthe classes and their parents.', 'nnquery : query large neural network graph structures in Torch \n NN modules in Torch are often complex graph structures, like  nn.Container s and its subclasses and  nn.gModules  ( nngraph ), arbitrarily nested. This makes it tedious to extract nn modules when debugging, monitoring training progress, or testing. \n nnquery  provides a facility to query these arbitrarily complex DAGs. XPath and CSS are designed to handle trees, whereas this library supports querying DAGs like neural nets.\nThe API is loosely inspired by a mix of XPath, CSS queries, and .NET\'s LINQ. \n See below for a simple example, and a more complete example of extracting things from an LSTM. \n Installation \n Install  nnquery :\n luarocks install https://raw.githubusercontent.com/bshillingford/nnquery/master/rocks/nnquery-scm-1.rockspec \nTotem is optional, and used for unit tests. \n Usage \n There are two important base classes that nearly everything is derived from: \n \n Element  (full name:  nnquery.Element ) \n ElementList \n \n Every object you wish to query is wrapped in an  Element , and sequences/collections of these\nare represented using  ElementList s. \n To wrap an object in an  Element  so you can query it:\n```lua\nlocal nnq = require \'nnquery\'\nlocal seq = nn.Sequential()\n    :add(nn.Tanh())\n    :add(nn.ReLU()) \n local tanh = nnq(seq):children():first()\n```\nOn the last line, \n \n nnq(seq)  wraps  seq  into an  Element ; \n :children()  returns an  ElementList  of two  Elements  for  seq \'s children; \n :first()  returns the first  Element  in the  ElementList . \n \n Realistic example with an LSTM: \n This is an example of using various functions in  Element  and  ElementList :\n```lua\nrequire \'nn\'\nrequire \'nngraph\'\nlocal nnq = require \'nnquery\' \n -- nngraph implementation of LSTM timestep, from Oxford course\'s practical #6\nfunction create_lstm(opt)\n  local x = nn.Identity()()\n  local prev_c = nn.Identity()()\n  local prev_h = nn.Identity()() \n function new_input_sum()\n    -- transforms input\n    local i2h            = nn.Linear(opt.rnn_size, opt.rnn_size)(x)\n    -- transforms previous timestep\'s output\n    local h2h            = nn.Linear(opt.rnn_size, opt.rnn_size)(prev_h)\n    return nn.CAddTable()({i2h, h2h})\n  end \n local in_gate          = nn.Sigmoid()(new_input_sum())\n  local forget_gate      = nn.Sigmoid()(new_input_sum())\n  local out_gate         = nn.Sigmoid()(new_input_sum())\n  local in_transform     = nn.Tanh()(new_input_sum()) \n local next_c           = nn.CAddTable()({\n      nn.CMulTable()({forget_gate, prev_c}),\n      nn.CMulTable()({in_gate,     in_transform})\n  })\n  local next_h           = nn.CMulTable()({out_gate, nn.Tanh()(next_c)}) \n nngraph.annotateNodes()\n  local mod = nn.gModule({x, prev_c, prev_h}, {next_c, next_h})\n  mod.name = ""LSTM""\n  return mod\nend \n -- Example network\nlocal foo = nn.Sequential()\n    :add(nn.Module())\n    :add(create_lstm{rnn_size=3})\n    :add(nn.ReLU())\n    :add(nn.ReLU())\n    :add(nn.Linear(3, 4)) \n -- Find the LSTM in a few different ways:\nlocal lstm = nnq(foo)   -- Wrap the module in an Element object using the default context\n                        -- which allows querying nn containers and nngraph\'s gmodules.\n    :descendants()      -- Get all descendants below this node in the graph\n    :where(function(e)  -- Filter Elements by the given predicate\n      return e:classIs(nnq.NNGraphGModuleElement)\n    end)\n    :only()             -- Returns the first element in the returned sequence, and\n                        -- asserts that it is the only element in the sequence.\n                        -- (shortcut for list:first() and assert(list:count() == 1))\nlocal lstm2 = nnq(foo)\n    :children()         -- Returns the contained modules of the nn.Sequential object as an\n                        -- ElementList\n    :nth(2)             -- Grabs the 2nd child of the nn.Sequential\n                        -- (alternate shorthand syntax: nnq(foo):children()[2])\nlocal lstm3 = nnq(foo)\n    :descendants()      --  \n    :attr{name=\'LSTM\'}  -- Get only the objects with a name attribute set to \'LSTM\',\n                        -- where it\'ll check both raw attributes and attempt to call\n                        -- the function assuming it\'s a getter method, i.e. check \n                        -- module:name() == \'LSTM\'.\n    :only()\nassert(lstm:val() == lstm2:val() and lstm2:val() == lstm3:val(),\n    \'they should all return the same LSTM gmodule\') \n -- Get the output nodes of the nngraph gmodule as an ElementList:\nlocal outputs = lstm:outputs()\n-- Two ways to get the count for an ElementList:\nprint(\'The LSTM gmodule has \'..outputs:count()..\' outputs, they are:\' outputs)\nprint(\'The LSTM gmodule has \'..#outputs..\' outputs, they are:\', outputs)\nassert(outputs:first():name() == \'next_c\')  -- :name() is available on NNGraphNodeElements,\n                                            -- as a shortcut for:\nassert(outputs:first():val().data.annotations.name == \'next_c\')  \n -- Let\'s find the forget gate:\nlocal forget_gate = lstm:descendants():attr{name=\'forget_gate\'}:only()\nprint(forget_gate)\n-- But it\'s the sigmoid, not the gate\'s pre-activations, so let\'s get the sum:\nlocal input_sum = forget_gate:parent() -- This is an alias for :parents():only().\n                                       -- Note: nngraph nodes can have multiple parents (i.e.\n                                       -- inputs \nassert(torch.isTypeOf(input_sum:val().data.module, nn.CAddTable))\nassert(torch.isTypeOf(input_sum:module(), nn.CAddTable)) -- alias for :val().data.module\n``` \n Further details: \n Wrapping objects into elements and similar operations only make sense relative to a  context , an instance of  nnquery.Context , which contains a list of  Element  types and conditions on which to instantiate depending on what type is provided to it. Additionally, the context caches  Element s, so that wrapping the same object twice returns the same instance of the  Element  subclass.\n nnquery/init.lua  contains the construction of a default context (accessible as  nnquery.default ) that contains all the implemented  Element  types, similarly to this:\n lua\nlocal ctx = nnq.Context()\nctx:reg(nnq.NNGraphGModuleElement, nnq.NNGraphGModuleElement.isGmodule)\nctx:reg(nnq.NNGraphNodeElement, nnq.NNGraphNodeElement.isNode)\nctx:reg(nnq.ContainerElement, nnq.ContainerElement.isContainer) -- after since gModule IS_A Container\nctx:default(nnq.ChildlessElement) \n Note that there is no true ""root"" node, unlike an XML/HTML document; the root is simply the place where the query begins. Therefore, one cannot[*] search for the root\'s parents, even if the root module is contained in (for example) a container. \n [*] Usually. Unless an element\'s parents are pre-populated from a previous query. \n Documentation \n Further documentation can be found in doc comment style before class definitions and method definitions in the code itself. \n TODO: extract these into markdown format and put links here \n Developing \n Extending \n You may have your own  nn  modules that are not handled by the existing handlers. In this case,\nyou can implement your own  Element  object (see the existing ones for examples), and create your own context that adds a handler for this  Element . See the default context (see above) for details. \n Contributing \n Bug reports are appreciated, preferably with a pull request for a test that breaks existing code and a patch that fixes it. If you do, please adhere to the (informal) code style in the existing code where appropriate.', ""Torch serialization reader for Python \n \n \n Mostly direct port of the torch7 Lua and C serialization implementation to \nPython, depending only on  numpy  (and the standard library:  array  \nand  struct ). Sharing of objects including  torch.Tensor s is preserved. \n python\nimport torchfile\nstuff = torchfile.load('a_bunch_of_stuff.t7') \n Installation: \n Install from  PyPI :\n sh\npip install torchfile \nor clone this repository, then:\n sh\npython setup.py install \n Supports Python 2.7, 3.4, 3.5, 3.6. Probably others too. \n More examples: \n Write from torch, read from Python: \n Lua:\n lua\n+th> torch.save('/tmp/test.t7', {hello=123, world=torch.rand(1,2,3)}) \nPython:\n python\nIn [3]: o = torchfile.load('/tmp/test.t7')\nIn [4]: print o['world'].shape\n(1, 2, 3)\nIn [5]: o\nOut[5]: \n{'hello': 123, 'world': array([[[ 0.52291083,  0.29261517,  0.11113465],\n         [ 0.01017287,  0.21466237,  0.26572137]]])} \n Arbitary torch classes supported: \n ```python\nIn [1]: import torchfile \n In [2]: o = torchfile.load('testfiles_x86_64/gmodule_with_linear_identity.t7') \n In [3]: o.forwardnodes[3].data.module\nOut[3]: TorchObject(nn.Identity, {'output': array([], dtype=float64), 'gradInput': array([], dtype=float64)}) \n In [4]: for node in o.forwardnodes: print(repr(node.data.module))                                                                                                           \nNone\nNone\nNone\nTorchObject(nn.Identity, {'output': array([], dtype=float64), 'gradInput': array([], dtype=float64)})\nNone\nTorchObject(nn.Identity, {'output': array([], dtype=float64), 'gradInput': array([], dtype=float64)})\nTorchObject(nn.Linear, {'weight': array([[-0.0248373 ],\n       [ 0.17503954]]), 'gradInput': array([], dtype=float64), 'gradWeight': array([[  1.22317168e-312],\n       [  1.22317168e-312]]), 'bias': array([ 0.05159848, -0.25367146]), 'gradBias': array([  1.22317168e-312,   1.22317168e-312]), 'output': array([], dtype=float64)})\nTorchObject(nn.CAddTable, {'output': array([], dtype=float64), 'gradInput': []})\nNone \n In [5]: o.forwardnodes[6].data.module.weight\nOut[5]: \narray([[-0.0248373 ],\n       [ 0.17503954]]) \n In [6]: o.forwardnodes[6].data.module.bias\nOut[6]: array([ 0.05159848, -0.25367146])\n``` \n More complex writing from torch: \n Lua:\n lua\n+th> f = torch.DiskFile('/tmp/test.t7', 'w'):binary()\n+th> f:writeBool(false)\n+th> f:writeObject({hello=123})\n+th> f:writeInt(456)\n+th> f:close() \nPython:\n python\nIn [1]: import torchfile\nIn [2]: with open('/tmp/test.t7','rb') as f:\n   ...:     r = torchfile.T7Reader(f)\n   ...:     print(r.read_boolean())\n   ...:     print(r.read_obj())\n   ...:     print(r.read_int())\n   ...: \nFalse\n{'hello': 123}\n456 \n Supported types: \n \n nil  to Python  None \n numbers to Python floats, or by default a heuristic changes them to ints or\n   longs if they are integral \n booleans \n strings: read as byte strings (Python 3) or normal strings (Python 2), like\n   lua strings which don't support unicode, and that can contain null chars \n tables converted to a special dict (*); if they are list-like (i.e. have\n   numeric keys from 1 through n) they become a python list by default \n Torch classes: supports Tensors and Storages, and most classes such as \n   modules. Trivially extensible much like the Torch serialization code.\n   Trivial torch classes like most  nn.Module  subclasses become \n    TorchObject s. The  torch_readers  dict contains the mapping from class\n   names to reading functions. \n functions: loaded into the  LuaFunction   namedtuple ,\n   which simply wraps the raw serialized data, i.e. upvalues and code.\n   These are mostly useless, but exist so you can deserialize anything. \n tds.Hash, tds.Vec \n \n (*) Since Lua allows you to index a table with a table but Python does not, we \n    replace dicts with a subclass that is hashable, and change its\n    equality comparison behaviour to compare by reference.\n    See  hashable_uniq_dict . \n Test files demonstrating various features: \n ```python\nIn [1]: import torchfile \n In [2]: torchfile.load('testfiles_x86_64/list_table.t7')\nOut[2]: ['hello', 'world', 'third item', 123] \n In [3]: torchfile.load('testfiles_x86_64/doubletensor.t7')\nOut[3]: \narray([[ 1. ,  2. ,  3. ],\n       [ 4. ,  5. ,  6.9]]) \n ...also other files demonstrating various types. \n ``` \n The example  t7  files will work on any modern Intel or AMD 64-bit CPU, but the\ncode will use the native byte ordering etc. Currently, the implementation \nassumes the system-dependent binary Torch format, but minor refactoring can \ngive support for the ascii format as well."", ""Linux Spotify ad muter \n Listens to track changes with dbus. Assumes pulseaudio, and mutes master during ads (i.e. not just spotify's stream! [1]). \n License: BSD. Based on  https://muffinresearch.co.uk/linux-spotify-track-notifier-with-added-d-bus-love/ . \n [1] TODO: only mute spotify stream, read pavucontrol source code to figure out how. Currently mutes pulseaudio using  amixer -D pulse sset Master {off|on} ."", ""Fork of  fb.debugger : dependency-free \n fb.debugger  is an excellent debugger for torch and lua in general, and can be found at  https://github.com/facebook/fblualib .\nHowever,  fblualib  and its dependencies are quite heavy but you may just want the debugger. This repository is a fork of  fb.debugger  with dependencies integrated and/or removed.  \n To run a script and drop into the debugger on an error, simply do:\n sh\nfbdbg-run script_name.lua your_arg1 your_arg2 ... \n Install: \n luarocks install https://raw.githubusercontent.com/bshillingford/fbdebugger-minimal/master/fbdebugger-standalone-1.rockspec \n Dependencies: \n \n penlight >= 1.3.1 \n libedit  installed in your system ( libedit.so  anywhere in the library search path). \n \n \n Original README: \n fb-debugger: A source-level Lua debugger \n This package implements a source-level Lua debugger. \n Usage \n You may enter the debugger in two different ways:\n* explicitly: at the point of interest, do\n lua\nlocal debugger = require('fb.debugger')\ndebugger.enter() \n  and you will be dropped in the debugger\n* automatically when you hit an (uncaught) error: if using\n   fb.trepl , you may set the environment variable\n   LUA_DEBUG_ON_ERROR  to  1 , and you'll be dropped in the debugger\n  whenever your code raises an uncaught error. \n Debugger commands \n help  will give you a list of commands, inspired by\n gdb . The following commands exist and behave\nsimilarly to their gdb counterparts:\n*  help  displays help\n*  where  /  backtrace  /  bt  displays the current stack trace (with a\n  marker for the currently selected frame)\n*  frame  selects a given frame\n*  up  /  down  moves the currently selected frame up / down one\n*  b  /  break  sets a breakpoint at a given location (specified either as\n   <file>:<line_number>  or  <function_name> ; the function name is looked up\n  in the scope of the current frame)\n*  info breakpoints  lists breakpoints\n*  enable ,  disable ,  delete  enable, disable, and delete a breakpoint,\n  respectively\n*  next  /  n  single-steps one line, skipping over function calls\n*  step  /  s  single-steps one line, descending into function calls\n*  finish  continues execution until the function in the currently selected\n  frame returns\n*  continue  /  c  continues program execution until the next breakpoint,\n  or until the next time the debugger is reentered (via  debugger.enter()  or\n  automatically in case of error)\n*  locals  /  vlocals  shows locals in scope in the current frame;  vlocals \n  also shows values (verbose)\n*  globals  /  vglobals  shows all globals\n*  upvalues  /  vupvalues  shows the current function's upvalues\n*  exec  /  e  executes code in the scope of the current frame\n*  print  /  p  evaluates an expression in the scope of the current frame and\n  prints the result\n*  list  /  l  lists source code (if available); by default it lists the\n  function in the current frame, but it accepts a location argument just like\n   break ; just like gdb, repeating  l  without arguments continues listing\n  the same file\n*  quit  /  q  quits the debugger; the program is resumed. \n Note that  locals ,  globals , or  upvalues  will occasionally show a\nsynthetic name for a variable (such as  _dbgl_tmp_4 ). These indicate variables\nthat have been shadowed in the current scope (and so their original name\nnow refers to something else) or internal Lua temporaries (modifying those\nis ill-advised)."", 'LipNet: End-to-End Sentence-level Lipreading. \n Yannis M. Assael, Brendan Shillingford, Shimon Whiteson, Nando de Freitas \n Links \n -  arXiv pre-print \n Bibtex \n @article{assael2016lipnet,\n  title={LipNet: End-to-End Sentence-level Lipreading},\n  author={Assael, Yannis M and Shillingford, Brendan and Whiteson, Shimon and de Freitas, Nando},\n  journal={GPU Technology Conference},\n  year={2017}\n}\n \n License \n Code licensed under the Apache License v2.0.', 'num2word \n Line-for-line Lua port of  http://stackoverflow.com/questions/25150316/convert-numbers-to-english-strings \n Python version fixed a ""zero thousand"" bug. Reasonably high quality for numbers smaller than a billion, but a few odd spellings left uncorrected e.g. 18. \n Test of equivalent implementation \n bash\nluajit test_num2word.lua > lua.txt\npython test_num2word.py > py.txt\ndiff lua.txt py.txt || echo ""they are different""', 'Wrapper for CUDA profiler start/stop API functions. Zero dependencies. \n Example:\n```python\nimport cudaprofile \n cudaprofile.start() \n ... do expensive cuda stuff ... \n cudaprofile.stop()\n ``\nand run the script from nvprof or nvvp`. \n You may want to use  nvprof  with  --profile-from-start-off  and only call  start()  when desired.', 'wifi-locate (Python) \n Locates the Wi-Fi-enabled machine using nearby Wi-Fi access points\' relative signal strengths. Uses Google\'s API. \n To use, call  linux_scan  or  osx_scan , then give the result to  locate  which returns  (accuracy, (lat,lng)) . \n Pretty useful for  xflux  or fetching weather. \n Quick start: \n Install:\n bash\npip install git+https://github.com/bshillingford/wifi-locate \n Example:\n python\nfrom wifilocate import locate, linux_scan\naccuracy, latlng = locate(linux_scan(device=""wlan0"", iwlist_path=\'/sbin/iwlist\'))\nprint(accuracy, latlng)  # e.g. 25, (50.1234567, -1.234567) \n Details \n Calls Google\'s API (most likely used in Firefox, based on the URL). The module supports Python 2 and 3, and only depends on  requests . If you don\'t yet have requests, consider my dependency on it a favour. It\'s great. \n In Linux this uses  iwlist , and in OS X it uses a little-known but built-in utility called  airport .', ""This is a fork of the excellent extension Better Google Tasks: \n \n https://chrome.google.com/webstore/detail/better-google-tasks/denjcdefjebbmlihdoojnebochnkgcin?hl=en-GB \n http://richwells.me/blog/better-google-tasks/ \n \n I added functionality for handling URLs to a specific task list, like this:\n https://mail.google.com/tasks/canvas#List Name Here \nand changing the default task list to open via the extension's options dialog."", ""sharearray \n Have you worried about creating large identical numpy arrays across processes due to RAM wastage, e.g. datasets that are big enough to fit in RAM but large enough to cause concern when running multiple jobs using the same data?\n sharearray  efficiently caches numpy arrays in RAM (using shared memory in  /dev/shm , no root needed) locally on a machine. \n Usage is simple, using the  cache  function or  decorator  decorator.\nA first call saves the result of the call into the built-in RAM disk, and\nreturns a read-only memory-mapped view into it.\nSince it's in RAM, there's no performance penalty.\nAny subsequent calls with the same ID will return an identical read-only memory mapped view,\neven across processes. The IDs are  global . \n Installation:\n pip install git+https://github.com/bshillingford/python-sharearray \nor\n git clone https://github.com/bshillingford/python-sharearray\npython setup.py install \n Usage \n Using  decorator : \n ```python\n@sharearray.decorator('some_unique_id', verbose=False)\ndef get_training_data():\n    # create largeish / expensive-to-generate data\n    return my_array # some instance of np.ndarray \n first call, across all processes, creates the array \n arr_view = get_training_data() \n all further calls are cached/memoized: we return a view into memory \n arr_view_2 = get_training_data()\n``` \n Using the  cache  function: \n ```python\nimport sharearray\nimport numpy as np\narr = sharearray.cache('my_global_id', lambda: create_large_array()) \n or: \n arr = sharearray.cache('my_global_id', lambda: create_large_array())\n ``\nwhere, for instance, create_large_array` returns a large training set, potentially performing expensive feature transformations or data augmentations first. \n By default, the file is at  /dev/shm/sharearray_my_global_id.npy , and to avoid concurrency\nissues when first generating the array, and to avoid duplicated computation,  \n For futher details, read the docstrings. You may be interested in the  timeout ,  verbose , and  log_func  arguments (to either  cache  or  decorator ). \n PyTorch \n Since PyTorch does not yet support memmapped files (at time of writing), we can instead just create torch Tensors that point to the memory mapped by numpy:\n python\ndata_numpy = get_training_data()          # numpy.ndarray\ndata_torch = torch.from_numpy(data_numpy) # torch.Tensor \n Notes \n TODO: support returning multiple arrays (e.g. as a tuple or dict) from the callback / decorated function \n There exist similar libraries in Python already, but this just makes it easier to do as a memoization-style API. Also, this module is a single file, and does not write anything in C.""]",['repository_count'],,https://avatars.githubusercontent.com/u/2326749?v=4,False,2016-08-16 06:35:51,446,56,2,2012-09-11T18:58:38Z,2022-11-02T15:27:37Z,,False,False,False,False,False,True,False,0,"{'Python': 14, 'Lua': 7, 'CSS': 1, 'HTML': 1, 'Jupyter Notebook': 1}",8,,Data Modeling,15,0.54,0.82,0.16,0
hmansell,Howard Mansell,,howard@mansells.us,[],['repository_count'],Facebook,https://avatars.githubusercontent.com/u/980875?u=d414d46e55b397533a7031f563db6ecd5388c405&v=4,False,2016-08-24 15:41:21,30,1,0,2011-08-15T12:02:25Z,2022-10-12T23:39:03Z,,False,False,False,False,False,True,False,0,{},9,New York,Reinfrocement Learning,66,0.93,0.37,0.98,0
clementfarabet,Clement Farabet,Building AI.,clement@madbits.ai,"['sys \n Has moved to a more community friendly  repo .', 'xlua \n Has moved to a more community friendly  repo .', 'image \n Has moved to a more community friendly  repo .', 'nnx: experimental \'nn\' components \n The original neural network from Torch7,  nn , contains stable and widely\nused modules. \'nnx\' contains more experimental, unproven modules, and\noptimizations. Modules that become stable and which are proven useful make \ntheir way into \'nn\' (some already have). \n Library Documentation \n This section includes documentation for the following objects: \n \n SoftMaxTree  : a hierarchical log-softmax Module; \n TreeNLLCriterion  : a negative log-likelihood Criterion for the SoftMaxTree; \n CTCCriterion  : a Connectionist Temporal Classification Criterion based on  warp-ctc ; \n PushTable (and PullTable)  : extracts a table element and inserts it later in the network; \n MultiSoftMax  : performs a softmax over the last dimension of a 2D or 3D input; \n SpatialReSampling  : performs bilinear resampling of a 3D or 4D input image; \n [QDRiemaNNLinear] (#nnx.QDRiemaNNLinear) : quasi-diagonal reduction for Riemannian gradient descent \n Recurrent  : a generalized recurrent neural network container; \n \n \n SoftMaxTree \n A hierarchy of parameterized log-softmaxes. Used for computing the likelihood of a leaf class. \nThis Module should be used in conjunction with the  TreeNLLCriterion . \nUsing this for large vocabularies (100,000 and more) greatly accelerates training and evaluation \nof neural network language models (NNLM). \nA vocabulary hierarchy is provided via the  dp  package\'s\n BillionWords \n DataSource . \n The constructor takes 2 mandatory and 4 optional arguments : \n *  inputSize  : the number of units in the input embedding representation;\n *  hierarchy  : a Tensor mapping one  parent_id  to many  child_id  (a tree);\n *  rootId  : a number identifying the root node in the hierarchy. Defaults to  -1 ;\n *  accUpdate  : when the intent is to use  backwardUpdate  or  accUpdateGradParameters , set this to true to save memory. Defaults to false;\n *  static  : when true (the defualt), returns parameters with keys that don\'t change from batch to batch;\n *  verbose  : prints some additional information concerning the hierarchy during construction. \n The  forward  method returns an  output  Tensor of size 1D, while \n backward  returns a table  {gradInput, gradTarget} . The second \nvariable is just a Tensor of zeros , such that the  targets  can be \npropagated through  Containers  \nlike  ParallelTable . \n ```lua \n \n input = torch.randn(5,10)\ntarget = torch.IntTensor{20,24,27,10,12}\ngradOutput = torch.randn(5)\nroot_id = 29\ninput_size = 10  \nhierarchy = { \n \n [29]=torch.IntTensor{30,1,2}, [1]=torch.IntTensor{3,4,5}, \n   [2]=torch.IntTensor{6,7,8}, [3]=torch.IntTensor{9,10,11},\n   [4]=torch.IntTensor{12,13,14}, [5]=torch.IntTensor{15,16,17},\n   [6]=torch.IntTensor{18,19,20}, [7]=torch.IntTensor{21,22,23},\n   [8]=torch.IntTensor{24,25,26,27,28}\n}\nsmt = nn.SoftMaxTree(input_size, hierarchy, root_id)\nsmt:forward{input, target}\n-3.5186\n-3.8950\n-3.7433\n-3.3071\n-3.0522\n[torch.DoubleTensor of dimension 5]\nsmt:backward({input, target}, gradOutput)\n{\n  1 : DoubleTensor - size: 5x10\n  2 : IntTensor - size: 5\n} \n \n \n ``` \n \n TreeNLLCriterion \n Measures the Negative log-likelihood (NLL) for  SoftMaxTrees . \nUsed for maximizing the likelihood of SoftMaxTree outputs.\nThe SoftMaxTree Module outputs a column Tensor representing the log likelihood\nof each target in the batch. Thus SoftMaxTree requires the targets.\nSo this Criterion only computes the negative of those outputs, as \nwell as its corresponding gradients. \n \n \n PushTable (and PullTable) \n PushTable and PullTable work together. The first can be put earlier\nin a digraph of Modules such that it can communicate with a \nPullTable located later in the graph.  PushTable:forward(input)  \nfor an  input  table of Tensors to the output, excluding one, the index of which \nis specified by the  index  argument in the  PushTable(index)  constructor.\nThe Tensor identified by this  index  is communicated to one or many \nPullTables created via the  PushTable:pull(index)  factory method. \nThese can be inserted later in the digraph such that \na call to  PushTable:forward(input) , where  input  is a table or a Tensor, \nwill output a table with the previously  pushed  Tensor inserted \nat index  index . \n An example utilizing the above  SoftMaxTree  Module\nand a Linear Module demonstrates how the PushTable can be used to \nforward the  target  Tensor without any other \n Table Modules :\n```lua \n \n mlp = nn.Sequential()\nlinear = nn.Linear(50,100)\npush = nn.PushTable(2)\npull = push:pull(2)\nmlp:add(push)\nmlp:add(nn.SelectTable(1))\nmlp:add(linear)\nmlp:add(pull)\nmlp:add(smt) --smt is a SoftMaxTree instance\nmlp:forward{input, target} -- input and target are defined above\n-3.5186\n-3.8950\n-3.7433\n-3.3071\n-3.0522\n[torch.DoubleTensor of dimension 5]\nmlp:backward({input, target}, gradOutput) -- so is gradOutput\n{\n  1 : DoubleTensor - size: 5x10\n  2 : IntTensor - size: 5\n}\n The above code is equivalent to the following: lua\nmlp2 = nn.Sequential()\npara = nn.ParallelTable()\npara:add(linear)\npara:add(nn.Identity())\nmlp2:add(para)\nmlp2:add(smt)\nmlp2:forward{input, target}\n-3.5186\n-3.8950\n-3.7433\n-3.3071\n-3.0522\n[torch.DoubleTensor of dimension 5]\nmlp2:backward({input, target}, gradOutput)\n{\n  1 : DoubleTensor - size: 5x10\n  2 : IntTensor - size: 5\n}\n```\nIn some cases, this can simplify the digraph of Modules. Note that \na PushTable can be associated to many PullTables, but each PullTable \nis associated to only one PushTable. \n \n \n CTCCriterion \n criterion = nn.CTCCriterion() \nCreates a Criterion based on Baidus\'  warp-ctc  implementation.\nThis Module measures the loss between a 3D output of (batch x time x inputdim) and a target without needing alignment of inputs and labels.\nMust have installed warp-ctc which can be installed via luarocks:\n luarocks install http://raw.githubusercontent.com/baidu-research/warp-ctc/master/torch_binding/rocks/warp-ctc-scm-1.rockspec \nSupports cuda via:\n criterion = nn.CTCCriterion():cuda() \nExample:\n```\noutput = torch.Tensor({{{1,2,3,4,5},{6,7,8,9,10}}}) -- Tensor of size 1x1x5 (batch x time x inputdim).\nlabel = {{1,3}}\nsizes = torch.Tensor({2}) -- Size of each sequence (sequence-length) in the batch as a tensor\nctcCriterion = nn.CTCCriterion() \n err = ctcCriterion:forward(output,label,sizes)\ngradOut = ctcCriterion:backward(output,label)\nprint(""----CPU----"")\nprint(""Error : "" .. err)\nprint(""Gradients :"")\nprint(gradOut) \n ctcCriterion = ctcCriterion:cuda() -- Switch to cuda implementation.\noutput = output:cuda() \n err = ctcCriterion:forward(output,label,sizes)\ngradOut = ctcCriterion:backward(output,label)\nprint(""----GPU----"")\nprint(""Error : "" .. err)\nprint(""Gradients :"")\nprint(gradOut)\n``` \n gives the output:\n```\n----CPU---- \nError : 4.9038286209106 \nGradients : \n(1,.,.) = \n  0.0117 -0.9683  0.0861  0.2341  0.6364\n  0.0117  0.0317  0.0861 -0.7659  0.6364\n[torch.FloatTensor of size 1x2x5] \n ----GPU---- \nError : 4.9038290977478 \nGradients : \n(1,.,.) = \n  0.0117 -0.9683  0.0861  0.2341  0.6364\n  0.0117  0.0317  0.0861 -0.7659  0.6364\n[torch.CudaTensor of size 1x2x5]\n```\n \n MultiSoftMax \n This Module takes 2D or 3D input and performs a softmax over the last dimension. \nIt uses the existing  SoftMax  \nCUDA/C code to do so such that the Module can be used on both GPU and CPU. \nThis can be useful for  keypoint detection . \n \n SpatialReSampling \n Applies a 2D re-sampling over an input image composed of\nseveral input planes (or channels, colors). The input tensor in  forward(input)  is \nexpected to be a 3D or 4D tensor of size :  [batchSize x] nInputPlane x width x height . \nThe number of output planes will be the same as the number of input\nplanes. \n The re-sampling is done using  bilinear interpolation . \nFor a simple nearest-neihbor upsampling, use  nn.SpatialUpSampling() ,\nand for a simple average-based down-sampling, use \n nn.SpatialDownSampling() . \n If the input image is a 3D tensor of size  nInputPlane x height x width ,\nthe output image size will be  nInputPlane x oheight x owidth  where\n owidth  and  oheight  are given to the constructor. \n Instead of  owidth  and  oheight , one can provide  rwidth  and  rheight , \nsuch that  owidth = iwidth*rwidth  and  oheight = iheight*rheight . \n As an example, we can run the following code on the famous Lenna image:\n lua\nrequire \'image\'                                                           \nrequire \'nnx\'\ninput = image.loadPNG(\'doc/image/Lenna.png\')\nl = nn.SpatialReSampling{owidth=150,oheight=150}\noutput = l:forward(input)\nimage.save(\'doc/image/Lenna-150x150-bilinear.png\', output) \n The input: \n   \n The re-sampled output: \n   \n \n QDRiemaNNLinear \n The Quasi-Diagonal Riemannian Neural Network Linear (QDRiemaNNLinear) module is an implementation\nof the quasi-diagonal reduction of metrics, used for Riemannian gradient descent.\nThe algorithm is defined in Riemannian metrics for neural networks I: feedforward networks by Yann Ollivier (http://arxiv.org/abs/1303.0818) and an efficient implementation is described in Practical Riemannian Neural Networks by Yann Ollivier and Gaetan Marceau-Caron (http://arxiv.org/abs/1602.08007).\nTo use this module, simply replace  nn.Linear(ninput,noutput)  with  nnx.QDRiemaNNLinear(ninput,noutput) .\nAs always, the step-size must be chosen accordingly.\nTwo additional arguments are also possible:\n* gamma (default=0.01): determine the update rate of the metric for a minibatch setting, i.e., (1-gamma) * oldMetric + gamma newMetric. Smaller minibatches require a smaller gamma. A default value depending on the size of the minibatches is  gamma = 1. - torch.pow(1.-1./nTraining,miniBatchSize)  where  nTraining  is the number of training examples of the dataset and  miniBatchSize  is the number of training examples per minibatch. \n* qdFlag (default=true): Whether to use the quasi-diagonal reduction (true) or only the diagonal (false). The former should be better. \n This module is a straightforward implementation of the outer product gradient descent. \n Requirements \n \n Torch7 (www.torch.ch) \n \n Installation \n \n Install Torch7 (refer to its own documentation). \n clone this project into dev directory of Torch7. \n Rebuild torch, it will include new projects too. \n \n Use the library \n First run torch, and load nnx: \n sh\n$ torch   \n ``` lua \n \n require \'nnx\'\n``` \n \n Once loaded, tab-completion will help you navigate through the\nlibrary (note that most function are added directly to nn): \n ``` lua \n \n nnx. + TAB\n...\nnn. + TAB\n``` \n \n In particular, it\'s good to verify that all modules provided pass their\ntests: \n ``` lua \n \n nnx.test_all()\nnnx.test_omp()\n``` \n \n \n Recurrent \n DEPRECATED July 6th, 2015. Use  rnn  instead.', ""This repo is a container for all my Torch7 packages. \n Note: all these packages used to be distributed into a big messy repo \ncalled XLearn. \n Retrieve all packages \n This repo is empty, and only contains references to other GIT\nrepos. You can retrieve all of them like this: \n sh\n$ git submodule init\n$ git submodule update \n Install \n 1/ Torch7 and dependencies: \n On Linux (Ubuntu > 9.04): \n sh\n$ apt-get install gcc g++ git libreadline5-dev cmake wget libqt4-core libqt4-gui libqt4-dev \n On Mac OS (Leopard, or more), using  Homebrew : \n sh\n$ brew install git readline cmake wget qt \n Then on both platforms: \n sh\n$ git clone https://github.com/andresy/torch\n$ cd torch\n$ mkdir build; cd build\n$ cmake ..\n$ make\n$ [sudo] make install \n 2/ Packages: \n Once Torch7 is installed, it comes with a package manager\nthat you can use to either install packages from the web: \n sh\n$ torch-pkg install pkg-name\n$ torch-pkg --help \n or build them locally, if you are planning to work on the \nsources: \n sh\n$ cd pkg-name\n$ torch-pkg deploy \n Use Torch7 \n First run torch, and load a package: \n sh\n$ torch   \n ``` lua \n \n require 'imgraph'\n``` \n \n Once loaded, tab-completion will help you navigate through the\nlibrary (note: tab-completion will only work if you have\nQt4 and readline): \n ``` lua \n \n imgraph. + TAB\nimgraph.colorize(           imgraph.connectcomponents( \nimgraph.graph(              imgraph.histpooling(       \nimgraph.segmentmst(         imgraph.testme(            \nimgraph.watershed(          imgraph.gradient(\n``` \n \n Most packages then provide a testme() function to quickly see\nwhat it does: \n ``` lua \n \n imgraph.testme()\n``` \n \n Checkout the demos & tutorials \n sh\n$ cd demos   \n this repo contains demos, and tutorials to get started. Looking\nat the code is the best way to get there! \n Developers \n If you would like to develop one of the submodules you should check\nout the master branch of that module:  \n sh\n$ cd nnx\n$ git checkout master\n$ git pull \n This puts you at the head of development for that submodule, and in\nthe proper branch to commit any changes you make to the git repository\nfor that module.  To check out all the submodules in developer mode we\nhave added the script : \n sh\n$ ./gitall.sh \n  a simple command to repeat a git command to all subdirectories \n syntax: \n ./gitall.sh  \n eg: \n \n switch all submodules to the master branch\n  ./gitall.sh checkout master \n pull updates for all submodules\n  ./gitall.sh pull \n other useful\n  ./gitall.sh status\n  ./gitall.sh diff \n \n WARNING: will blindly send command(s) to git in each directory"", ""imgraph: a package to create/manipulate graphs on images \n This package provides standard functions to\ncreate and manipulate edge-weighted graphs \nof images: create a graph, segment it, \ncompute its watershed, or its connected\ncomponents... \n Install \n 1/ Torch7 is required: \n Dependencies, on Linux (Ubuntu > 9.04): \n sh\n$ apt-get install gcc g++ git libreadline5-dev cmake wget libqt4-core libqt4-gui libqt4-dev libboost-all-dev \n Dependencies, on Mac OS (Leopard, or more), using  Homebrew : \n sh\n$ brew install git readline cmake wget qt \n Then on both platforms: \n sh\n$ git clone https://github.com/andresy/torch\n$ cd torch\n$ mkdir build; cd build\n$ cmake ..\n$ make\n$ [sudo] make install \n 2/ Once Torch7 is available, install this package: \n sh\n$ [sudo] torch-rocks install imgraph \n Use the library \n First run torch, and load imgraph: \n sh\n$ torch   \n ``` lua \n \n require 'imgraph'\n``` \n \n Once loaded, tab-completion will help you navigate through the\nlibrary: \n ``` lua \n \n imgraph. + TAB\nimgraph.colorize(           imgraph.connectcomponents( \nimgraph.graph(              imgraph.histpooling(       \nimgraph.segmentmst(         imgraph.testme(            \nimgraph.watershed(          imgraph.gradient(\n``` \n \n To get quickly started, run the testme() function: \n ``` lua \n \n imgraph.testme()\n``` \n \n which computes a few things on the famous image of Lena: \n"", ""neuFlow \n neuFlow  is dataflow architecture optimized for large array/tensor\ntransforms, and especially image processing operations.  More info about the\narchitecture, hardware and applications can be found\n here . \n this package \n This package is a compiler toolkit for neuFlow. It is entirely written in\n Lua , and relies on\n Torch7  to represent N-dimensional arrays\nefficiently. It also interfaces Torch7's neural-network package natively. \n how to install \n Torch7 must be install first, a task most easily accomplished using the single\nline  install script . \n or alternatively to install Torch7 and the neuFlow package by hand, you will\nneed to install a few dependencies. \n On Linux (Ubuntu): \n sh\n$ apt-get install gcc g++ git libreadline5-dev cmake wget\n$ apt-get install libqt4-core libqt4-gui libqt4-dev\n$ apt-get install ffmpeg gnuplot \n On Mac OS X (> 10.5): get  Homebrew \nand then: \n sh\n$ brew install git readline cmake wget\n$ brew install qt\n$ brew install ffmpeg gnuplot \n You're ready to install Torch7 (www.torch.ch). The most up to date instructions\ncan be found at the  Torch7 github page . \n ``` sh\n$ git clone git://github.com/andresy/torch.git\n$ cd torch\n$ mkdir build\n$ cd build \n $ cmake ..\nOR\n$ cmake .. -DCMAKE_INSTALL_PREFIX=/my/install/path\n``` \n Or if you already have a previous Torch7 installed: \n sh\n$ luarocks install torch WITH_LUA_JIT=1 # Torch7, an efficient numeric library for Lua \n You will also need additional packages: \n sh\n$ luarocks install image        # an image library for Torch7\n$ luarocks install nnx          # lots of extra neural-net modules\n$ luarocks install camera       # a camera interface for Linux/MacOS\n$ luarocks install ffmpeg       # a video decoder for most formats\n$ luarocks install inline-c     # inline C capability \n Now that Torch7 has been installed the neuflow package can be installed.\nInstalling the neuflow package requires you to download the source code\nrepository. It'll give you access to some demos, to get started: \n sh\n$ git clone https://github.com/clementfarabet/neuflow.git\n$ cd neuflow\n$ luarocks make \n how to run code on neuFlow \n Demos are located in demos/. To get started, you'll need\na standard Xilinx dev board for the Virtex 6: [the ML605 Kit]\n(http://www.xilinx.com/products/devkits/EK-V6-ML605-G.htm).\nWe provide an image of neuFlow that's pre synthesized/mapped/routed\nfor the Virtex6 VLX240T on this platform. \n To run any of the demos, follow these instructions (tested on\nUbuntu 9.04, 10.04 and Mac OS X 10.5, 10.6 and 10.7). \n ``` sh\n$ git clone https://github.com/clementfarabet/neuflow.git\n$ cd neuflow \n make Xilinx tools available (it implies you have them \n installed somewhere...) \n $ source $XILINX_INSTALL_PATH/settings**.sh \n turn on the ML605, plug the JTAG cable then load one of \n our pre-built bitfiles *: \n $ cd scripts\n$ ./get-latest-neuflow-image\n$ ./load-bitfile neuFlow-ml605.bit \n at this points, you just have wait 2 seconds that the Ethernet \n LEDs are back on (out of reset) \n run the simplest demo, a loopback client, to verify your setup **: \n $ cd ../demos\n$ sudo torch loopback.lua # on Linux\nor\n$ ./loopback.lua # on OSX \n before loading a new demo, you have to reset neuFlow: for \n now it is done by pressing the SW10 button (cpu rst) \n then you can run a typical convnet-based program, a face detector: \n $ sudo torch face-detector.lua # on Linux\nor\n$ ./face-detector.lua # on OSX\n``` \n (*) the load-bitfile script assumes that you have properly installed Xilinx's\nUSB cable driver. On RedHat and derivatives it works out of the box when\ninstalling Xilinx ISE, but on Ubuntu you'll have to follow these instructions:\nhttp://rmdir.de/~michael/xilinx/.  This is not doable on Mac OS X\nunfortunately. I usually flash the ML605 board using Ubuntu (even a virtual box\nversion works), and then run all the demos under Mac OS X. \n (**) you need to have admin privileges on your machine (sudo) to be able to\ninteract with neuFlow, as we're using a custom low-level Ethernet framing\nprotocol."", ""UNSUP \n A package for unsupervised learning in Torch. \n Provides modules that are compatible with  nn  ( LinearPsd ,  ConvPsd ,  AutoEncoder , ...),\nand self-contained algorithms ( k-means ,  PCA ). \n Requirements \n Basic dependencies: \n \n Torch7 (github.com/andresy/torch) \n kex    (github.com/koraykv/tools) \n optim  (github.cim/koraykv/optim) \n \n To run the demo scripts, you also need the following: \n \n image (github.com/clementfarabet/lua---image) \n sys   (github.com/clementfarabet/lua---sys) \n xlua  (github.com/clementfarabet/lua---xlua) \n \n Installation \n Build/Install: \n \n Install Torch7 (refer to its own documentation). \n clone all other repos (including this one) into dev directory of Torch7. \n Rebuild torch, it will include all these projects too. \n \n Alternatively, you can use torch's package manager. Once\nTorch is installed, you can install  unsup :  $ torch-pkg install unsup ."", 'para||el: a (simple) parallel computing framework for Torch \n This package provides a simple mechanism to dispatch and run Torch/Lua code\nas independant processes and communicate via ZeroMQ sockets. Processes\ncan be forked locally or on remote machines. \n Install \n Install ZeroMQ 3 : \n bash\nsudo apt-get install libzmq3-dev libzmq3 \n Install Torch7 per instructions at http://torch.ch/ . \n Download and compile this package using luarocks: \n bash\n[sudo] luarocks install parallel \n or  \n bash\ngit clone https://github.com/clementfarabet/lua---parallel.git\ncd lua---parallel\nluarocks make \n Use the library \n API, in very short: \n Load/start up package: \n lua\nrequire \'parallel\' \n Fork a new process, or N new processes, locally: \n lua\nparallel.fork()\nparallel.nfork(4) \n Fork remote processes. In that following code, we fork 4 processes on myserver.org,\nand 6 processes on myserver2.org. \n lua\nparallel.nfork( {4, ip=\'myserver.org\', protocol=\'ssh\', lua=\'/path/to/remote/torch\'},\n                {6, ip=\'myserver2.org\', protocol=\'ssh\', lua=\'/path/to/remote/torch\'} ) \n Even more flexible, a list of machines can be established first, so that \na call to sfork() [smart fork] can automatically distribute the forked processes\nonto the available machines: \n ``` lua\nparallel.addremote( {ip=\'server1.org\', cores=8, lua=\'/path/to/torch\', protocol=\'ssh -Y\'},\n                    {ip=\'server2.org\', cores=16, lua=\'/path/to/torch\', protocol=\'ssh -Y\'},\n                    {ip=\'server3.org\', cores=4, lua=\'/path/to/torch\', protocol=\'ssh -Y\'} )\nparallel.sfork(16) \n -- in this example, the 16 processes will be distributed over the 3 machines:\n-- server1.org: 6 processes\n-- server2.org: 6 processes\n-- server3.org: 4 processes\n``` \n In the spirit of  really  abstracting where the jobs are executed, calibrate() can\nbe called to estimate the compute power of each machine, so that you can distribute\nyour load accordingly. \n lua\nparallel.addremote(...)\nparallel.calibrate()\nforked = parallel.sfork(parallel.remotes.cores)  -- fork as many processes as cores available\nfor _,forked in ipairs(forked) do\n   print(\'id: \' .. forked.id .. \', speed = \' .. forked.speed)\nend\n-- the speed of each process is a number ]0..1]. A coef of 1 means that it is the\n-- fastest process available, and 0.5 for example would mean that the process is 2x\n-- slower \n Once processes have been forked, they all exist in a table: parallel.children, and\nall methods (exec,send,receive,join) work either on individual processes, or on\ngroups of processes. \n The first thing to do is to load these new processes with code. The code given\ncan either be a function, with no arguments (it won\'t have any env when executing\nin the new process), or a string. Whether it is a string or a function, both\nget serialized into strings, and reloaded on the process side, using loadstring(). \n ``` lua\n-- define process\' code:\ncode = function()\n   -- arbitrary code contained here\n   require \'torch\'\n   t = torch.Tensor(10)\n   print(t) \n -- any process can access its id, its parent\'s id [and children\'s id]\n   print(parallel.id)\n   print(parallel.parent.id)\n   if parallel.children[1] then print(parallel.children[1].id) end \n -- if arguments were passed, they\'re found in the regular ... table       \n   args = {...}   \n   print(args[1])\nend \n -- execute code in given process(es), with optional arguments:\nparallel.children:exec(code) \n -- this is equivalent to:\nfor _,child in ipairs(parallel.child) do\n    child:exec(code)\nend\n``` \n parallel implements a simple yield/join mechanism to allow a parent to sync\nand affect the behavior of its children. \n ``` lua\n-- child code:\ncode = function()\n   while true do\n      print(\'something\')\n      parallel.yield()\n   end\nend\nc = parallel.fork()\nc:exec(code) \n -- parent code\nfor i = 1,10 do\n    c:join()\nend \n -- each time join() is called, it waits for the child to yield, and vice-versa.\n-- in that example, \'something\' only gets printed when the parent joins its child\n``` \n Slightly more complex things can be implemented with yield/join: join() can take\na string as an argument, which is returned by the corresponding yield(). This\nis useful to control branching in your children: \n ``` lua\n-- child code:\ncode = function()\n   while true do\n      print(\'something\')\n      m = parallel.yield()\n      if m == \'break\' then break end\n   end\nend\nc = parallel.fork()\nc:exec(code) \n -- parent code\nc:join(\'break\')\n``` \n Sometimes you might want to wait for a process to actually terminate (die), so that\nyou can start new ones. The proper way to do this is to use the sync() function, \nwhich waits for the PID of that process to fully disappear from the OS. It also\nclears the child from the parallel.children list, and decrement parallel.nchildren. \n lua\ncode = function()\n     -- do nothing and die\nend\nparallel.nfork(1)              -- fork one process\nparallel.children:exec(code)   -- execute dummy code\nprint(parallel.nchildren)      -- prints: 1\nparallel.children:sync()       -- wait for all children (here only 1) to die\nprint(parallel.nchildren)      -- prints: 0\nparallel.nfork(2)              -- fork 2 processes\nprint(parallel.nchildren)      -- prints: 2\nprint(parallel.children[1])    -- prints: nil\nprint(parallel.children[2])    -- prints: table --- current running processes always\nprint(parallel.children[3])    -- prints: table --- exist in children[process.id] \n When creating a child (parallel.fork), a connection is established\nto transfer data between the two processes. Two functions send() and receive()\ncan be used to  efficiently  transfer data between these processes. Any Lua type, \nand all Torch7 type (tensor, storage, ...) can be transferred this way. The transmission\nis efficient for numeric data, as serialization merely involves a binary copy and\nsome extra headers for book-keeping (see serialization in Torch7\'s manual). \n ``` lua\n-- define some code for children\nsomecode = function()\n   while true do\n      -- in an infinite loop, receive objects from parent:\n      local obj = parallel.parent:receive()\n      -- print\n      parallel.print(\'received object:\', obj)\n   end\nend \n -- dispatch two processes:\nparallel.nfork(2)\nparallel.children:exec(somecode) \n -- and send them some data:\nt = {\'a table\', entry2=\'with arbitrary entries\', tensor=torch.Tensor(100,100)}\nwhile true do\n    parallel.children[1]:send(t)        -- send the whole table to child 1\n    parallel.children[2]:send(t.entry2) -- just send an entry to child 2\nend\n``` \n A convenient print function that prepends the process ID issuing the print: \n ``` lua \n \n parallel.print(\'something\') \n \n   something\n``` \n Last, but not least: always run your parent code in a protected call, to catch\npotential errors, Ctrl+C, and the likes, and terminate nicely. By terminating\nnicely, I mean: killing all remote processes that you forked... If you don\'t\ndo so, you leave you remote machines (and potentially yours) with hanging \nprocesses that are just waiting to receive data, and will not hesitate to get\nback in business the next time you run your parent code :-) \n ``` lua\nworker = function()\n       -- some worker code\nend \n parent = function()\n       -- some parent code\nend \n ok,err = pcall(parent)\nif not ok then\n   print(err)\n   parallel.close()   -- this is the key call: doing this will insure leaving a clean\n                      -- state, whatever the error was (ctrl+c, internal error, ...)\nend\n``` \n A simple complete example: \n ``` lua\n-- required libs\nrequire \'parallel\' \n -- define code for workers:\nfunction worker()\n   -- a worker starts with a blank stack, we need to reload\n   -- our libraries\n   require \'sys\'\n   require \'torch\' \n -- print from worker:\n   parallel.print(\'Im a worker, my ID is: \' .. parallel.id .. \' and my IP: \' .. parallel.ip) \n -- define a storage to receive data from top process\n   while true do\n      -- yield = allow parent to terminate me\n      m = parallel.yield()\n      if m == \'break\' then break end \n   -- receive data\n  local t = parallel.parent:receive()\n  parallel.print(\'received object with norm: \', t.data:norm())\n\n  -- send some data back\n  parallel.parent:send(\'this is my response\')\n \n end\nend \n -- define code for parent:\nfunction parent()\n   -- print from top process\n   parallel.print(\'Im the parent, my ID is: \' .. parallel.id) \n -- fork N processes\n   parallel.nfork(4) \n -- exec worker code in each process\n   parallel.children:exec(worker) \n -- create a complex object to send to workers\n   t = {name=\'my variable\', data=torch.randn(100,100)} \n -- transmit object to each worker\n   parallel.print(\'transmitting object with norm: \', t.data:norm())\n   for i = 1,1000 do\n      parallel.children:join()\n      parallel.children:send(t)\n      replies = parallel.children:receive()\n   end\n   parallel.print(\'transmitted data to all children\') \n -- sync/terminate when all workers are done\n   parallel.children:join(\'break\')\n   parallel.print(\'all processes terminated\')\nend \n -- protected execution:\nok,err = pcall(parent)\nif not ok then print(err) parallel.close() end\n``` \n License \n Copyright (c) 2011 Clement Farabet, Marco Scoffier \n Permission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n""Software""), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions: \n The above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software. \n THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.', ""LOpenGM: Lua bindings for OpenGM \n OpenGM  is a C++ library for graphical \nmodeling, and inference. The Lua\nbindings provide a simple way of describing graphs, from Lua, and then\noptimizing them with OpenGM. \n Note: this package is superseded by 'gm', a more general and\nsimple (pure Lua) package for graphical models. \n License \n LOpenGM Copyright (c) 2011 Clement Farabet (Lua Bindings) \n OpenGM  Copyright (c) 2010 by Bjoern Andres and Joerg Hendrik Kappes. \n This software was developed by Bjoern Andres and Joerg Hendrik Kappes.\nEnquiries shall be directed to: \n bjoern.andres@iwr.uni-heidelberg.de, kappes@math.uni-heidelberg.de \n All advertising materials mentioning features or use of this software must\ndisplay the following acknowledgement: ``This product includes the OpenGM\nlibrary developed by Bjoern Andres and Joerg Hendrik Kappes. Please direct\nenquiries concerning OpenGM to bjoern.andres@iwr.uni-heidelberg.de,\nkappes@math.uni-heidelberg.de''. \n Redistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met: \n \n Redistributions of source code must retain the above copyright notice,\n  this list of conditions and the following disclaimer. \n Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution. \n All advertising materials mentioning features or use of this software must\n  display the following acknowledgement: ``This product includes the OpenGM\n  library developed by Bjoern Andres and Joerg Hendrik Kappes. Please direct\n  enquiries concerning OpenGM to bjoern.andres@iwr.uni-heidelberg.de,\n  kappes@math.uni-heidelberg.de''. \n The names of the authors must not be used to endorse or promote products\n  derived from this software without specific prior written permission. \n \n THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR IMPLIED\nWARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\nMERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO\nEVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\nPROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\nOR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\nWHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\nOTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF\nADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n Install \n 1/ Torch7 is required: \n Dependencies, on Linux (Ubuntu > 9.04): \n sh\n$ apt-get install gcc g++ git libreadline5-dev cmake wget libqt4-core libqt4-gui libqt4-dev \n Dependencies, on Mac OS (Leopard, or more), using  Homebrew : \n sh\n$ brew install git readline cmake wget qt \n Then on both platforms: \n sh\n$ git clone https://github.com/andresy/torch\n$ cd torch\n$ mkdir build; cd build\n$ cmake ..\n$ make\n$ [sudo] make install \n 2/ Once Torch7 is available, install this package: \n sh\n$ [sudo] torch-pkg install opengm \n Use the library \n API, in very short: \n Load/start up package: \n lua\nrequire 'opengm' \n Construct a graph: \n lua\ng = opengm.Graph(...) \n Optimize a graph: \n lua\ng:optimize{} \n Display a graph, using Graphviz: \n lua\ng:show{} \n A simple complete example: \n ```lua\n-- load opengm\nrequire 'opengm' \n -- standard factors\nf = opengm.factors \n -- define variables\nvariables = {'car', 'person', 'building', 'street', 'vehicle'} \n -- define factors\nfactors = {-- unary factors (prior probabilities of each class):\n           {f.prior(0.9),  {'car'}},\n           {f.prior(0.01), {'person'}},\n           {f.prior(0.7),  {'building'}},\n           {f.prior(0.8),  {'street'}},\n           {f.prior(0.4),  {'vehicle'}},\n           -- Potts factors (joint probabilities):\n           {f.band(0),     {'car',      'person'}},\n           {f.band(0),     {'person',   'building'}},\n           {f.band(0),     {'building', 'street'}},\n           {f.band(0),     {'car',      'building'}},\n           {f.band(0),     {'building', 'vehicle'}},\n           {f.band(0),     {'street',   'vehicle'}},\n           {f.band(0),     {'person',   'vehicle'}},\n           {f.bimplies(1), {'car',      'vehicle'}}} \n -- create graph\ng = opengm.Graph(variables, factors) \n -- optimize graph\ng:optimize{method='a*', verbose=true} \n -- print graph\nprint(g)\n``` \n Running the script above outputs: \n <opengm> optimizing... \nstep 1: E=3.99758, c=0\nstep 2: E=3.63212, c=2.19722\nstep 3: E=3.63212, c=2.19722\n<opengm.Graph>\n  + nb of variables: 4\n  + nb of factors: 6\n  + graph is acyclic\n  + current (optimized) variable states: \n    - car [1]\n    - person [0]\n    - building [0]\n    - street [0]\n    - vehicle [1]"", ""Torch7 Library for iOS \n Torch7 provides a Matlab-like environment for state-of-the-art machine\nlearning algorithms. It is easy to use and provides a very efficient\nimplementation, thanks to an easy and fast scripting language (Lua) and a\nunderlying C implementation. \n This package has been modified (or just hacked) to fully compile\nTorch7 for iOS (iPad/iPhone) for all architectures (armv7, armv7a, arm64, i386 (simulator), x86_64 (simulator)) \n Requirements \n Torch7 needs to be installed prior to building the iOS\nversion. 'torch' needs to be available in the user's path. \n I recommend doing the easy install if you have not installed Torch7.\nhttp://torch.ch/docs/getting-started.html \n Building The Framework \n Simply run:\n$ ./generate_ios_framework \n This will build all torch's libraries as static libs, and export them\nin a single dir: framework/. The dir is ready to be included in\nan iOS project: it includes an example class to load Torch from within\nyour Objective C project. \n For examples full examples that utilize this class (Torch.m) please see \nthe ios_examples/ folder. More examples to come soon. \n Running \n When creating your Objective-C project simply import the class\nTorch.m/.h; include all the libs to the linker; add Torch.framework & Accelrate.framework\nand add all the Lua files as resources. define YOUR_FILE.lua and add it as \na resource. Run YOUR_FILE.lua using the method defined in Torch.h/.m"", ""liuflow: a wrapper around C.Liu's optical flow \n Note: this bit of code is a simple wrapper around the optical-flow\nalgorithm developped/published by C.Liu: \n C. Liu. Beyond Pixels: Exploring New Representations and Applications\nfor Motion Analysis. Doctoral Thesis. Massachusetts Institute of \nTechnology. May 2009. \n More at: http://people.csail.mit.edu/celiu/OpticalFlow/"", ""videograph: a package to create/manipulate graphs on videos \n This package provides standard functions to\ncreate and manipulate edge-weighted graphs \nof videos: create a graph, segment it, get \nits adjacency matrix, ... \n Install \n 1/ Torch7 is required: \n Dependencies, on Linux (Ubuntu > 9.04): \n sh\n$ apt-get install gcc g++ git libreadline5-dev cmake wget libqt4-core libqt4-gui libqt4-dev \n Dependencies, on Mac OS (Leopard, or more), using  Homebrew : \n sh\n$ brew install git readline cmake wget qt \n Then on both platforms: \n sh\n$ git clone https://github.com/andresy/torch\n$ cd torch\n$ mkdir build; cd build\n$ cmake ..\n$ make\n$ [sudo] make install \n 2/ Once Torch7 is available, install this package: \n sh\n$ [sudo] torch-pkg install videograph \n Use the library \n First run torch, and load videograph: \n sh\n$ torch   \n ``` lua \n \n require 'videograph'\n``` \n \n ..."", ""IPAM Graduate Summer School \n On Deep Learning, Feature Learning\nJuly 9 - 27, 2012 \n More info here.   \n Day 1: Setup \n \n \n Welcome to the Practical Sessions for the summer school \n \n \n Objectives: \n \n \n implementation-level understanding of supervised and unsupervised learning algorithms \n \n \n Many algorithms are more similar than researchers in the field\n    might have you believe! \n \n \n a sense of hyper-parameter sensitivities and run-times for various\n  algorithms \n \n \n appreciation for two approaches to programming deep learning experiments \n \n \n Code fragments for interactive exploration \n \n \n Full-blown application \n \n \n exposure to programming languages and software stacks: \n \n \n Python, NumPy, SciPy, Theano \n \n \n Lua, Torch7 \n \n \n \n \n Schedule: 1 hour on four days this first week \n \n \n Monday 12PM - 1PM:  Lua/Torch, Python/Theano, logging in to EC2 \n \n \n Tuesday 4PM - 5PM:  Supervised Learning in Lua and Python \n \n \n Wednesday 4PM - 5PM:  Unsupervised Learning in Lua and Python \n \n \n Thursday 4PM - 5PM:  TBA \n \n \n \n \n Session Structure \n \n \n Time is short for these practical sessions! \n \n \n Each day will start with two walk-throughs of things you can experiment with\n    (we'll try to be quick, to give you time afterward!) \n \n \n After the walk-throughs you can log in to an Amazon EC2 node where we've set\n    things up. \n \n \n For lack of time - you will have to choose whether to do the Lua thing or\n    the Python thing in the in-classroom time each day. \n \n \n We will negotiate with the organizers to leave the EC2 node up after the sessions \n \n \n We will be around all week - feel free to ask questions any time! \n \n \n We will be available by email after the first week. \n \n \n \n \n 10 mins crash course in Python, numpy \n \n \n intro to IPython notebook \n \n \n 10 mins crash course in Lua, Torch7 \n \n \n will review very basic Lua and Torch concepts, to get people started \n \n \n Remaining time - getting people into groups and setting them up to run the sample code\n  on laptop or EC2. Once they get it running, they can go for lunch or stick\n  around and play with things. \n \n \n Day 2: Supervised Learning \n \n \n Models: SVM, MLP, ConvNets, (Logistic Regression?) \n \n \n Data Sets: MNIST, CIFAR, Google Street View House Numbers (SVHN).\n  SVHN is an interesting new data set, very few results are available at this time \n  (and is more computer visionny that MNIST). \n \n \n Optimization Methods: SGD, ASGD, L-BFGS; batch vs. mini-batch vs. online \n \n \n Day 3: Feature Learning \n \n \n Python: Imprinting, K-Means, Autoencoder, De-noising Autoencoder, RBM,\n  (Sparse Coding?) \n \n \n Torch: Linar Autencoder, Convolutional Autoencoder, Linear and \n  Convolutional PSD (Predictive Sparse Decomposition) Autoencoder \n \n \n Day 4: To Be Decided \n \n \n Persitant Contrastive Divergence? \n \n \n Theano? \n \n \n Recurrent Neural Networks? \n \n \n GPU Programming 101? \n \n \n Torch/nn extensions: write your own modules \n \n"", 'csvigo: a package to handle CSV files (read and write). \n Install: \n First install Torch7 (www.torch.ch) then simply install this package\nusing luarocks: \n luarocks install csvigo \n Use: \n The library provides 2 high-level functions: csvigo.load and csvigo.save. To get help\non these functions, simply do: \n ``` \n \n csvigo.save()\ncsvigo.load()\n``` \n \n Loading a CSV file in \'query\' mode gives you a convenient query function that\nyou can use to query subsets of your original CSV file. To get help on this query\nfunction, simply do: \n ``` \n \n query = csvigo.load{path=\'somefile.csv\', mode=\'query\'}\nquery(\'help\')\n-- print some help\nall = query(\'all\')\nsubset = query(\'union\', {somevar=someval, someothervar={val1, val2}})\n``` \n \n Large CSV mode \n CSVigo supports efficient loading of very large CSV files into memory.\nThe loaded data structure is a read-only table with efficiency hidden under the hood. \n Loading: \n lua\nm = csvigo.load({path = ""my_large.csv"", mode = ""large""}) \n Printing by default only prints first 10 and last 10 rows\n lua\nprint(m) \n Individual element access\n lua\nprint(m[32]) \n Size of table:\n lua\nprint(#m) \n For loop over entries: \n Type 1:\n lua\nfor i=1, #m do\n    print(m[i]) -- get element\nend \n Type 2:\n lua\nfor k,v in ipairs(m) do\n    print(k)\n    print(v)\nend \n Type 3:\n lua\nfor k,v in pairs(m) do\n    print(k)\n    print(v)\nend \n Read-only table\n lua\n-- read only table, will error here:\nm[13] = \'a\'', 'json: a package to handle json (read and write). \n TAKEN From JSON4Lua, originally written for Lua 5.1. \n Wrapped for Torch7 (torch-rocks). \n JSON4Lua: JSON encoding / decoding support for the Lua language.\njson Module. \n Author: Craig Mason-Jones\nHomepage: http://json.luaforge.net/\nVersion: 0.9.40\nThis module is released under the MIT License (MIT). \n Install: \n First install Torch7 (www.torch.ch) then simply install this package\nusing torch-rocks: \n torch-rocks install json \n Usage: \n This module exposes 4 functions: \n ```\njson_string = encode(o)\n-- returns the table / string / boolean / number / nil / json.null value as a JSON-encoded string. \n o = decode(json_string)\n-- returns a Lua object populated with the data encoded in the JSON string json_string. \n save(json_file, o)\n-- saves the table / string / boolean / number / nil / json.null value as a JSON-encoded file. \n o = load(json_file)\n-- returns a Lua object populated with the data encoded in the JSON file.\n```', 'Torch Web Terminal \n This is a browser-based terminal for  Torch7 . \nThe goal of this project is to supersed the Qt4 interface, and to \nenable full graphics capabilities within the browser. \n This project is built around  Node.js , \na super lightweight asynchronous framework to build servers. In\nour case, the server is only use to connect clients (browser \nterminals) to Torch7 kernels. For now, one server instance can\nsupport an arbitrary number of clients, but each client only\nhas access to one Torch7 kernel. \n Dependencies \n You will need to install a couple of dependencies to enable\nthis web terminal: \n \n \n Node.js, which can be found  here , and\n  should also be installable with your system\'s package manager \n \n \n NPM: Node\'s package manager (sometimes comes with Node.js) \n \n \n Three Node.js packages:  ejs ,  stripcolorcodes  and  express \n    (version 2.x) \n \n \n For instance, on MacOS: \n bash\n$ brew install nodejs\n$ curl http://npmjs.org/install.sh | sh\n$ npm install express@2.x ejs stripcolorcodes \n Installation \n This project is bundled as a  torch-pkg  project, and can be \neasily installed like this: \n bash\n$ torch-pkg -local install webterm \n Or, if you have downloaded this repository on your machine, and\nyou are in its directory: \n bash\n$ torch-pkg -local deploy \n Note1: you have to deploy webterm locally (-local flag), as the node \npackages are only available to the current user. This could probably\nbe fixed, but I still don\'t know how. \n Note2: depending on the version of Node.js, you might have to do\nthe NPM install in the package directory, i.e.: \n bash\n$ cd ~/.torch/usr/share/torch/lua/webterm/\n$ npm install express@2.x ejs stripcolorcodes \n Running it \n webterm  is a standard package, so you can either require it from\na running torch instance, or start torch with it like that: \n bash\n$ torch -lwebterm \n This should produce the following output: \n text\nTry the IDE: torch -ide\nType help() for more info\nTorch 7.0  Copyright (C) 2001-2011 Idiap, NEC Labs, NYU\n==> Torch server listening on port 8080\n==> Open http://localhost:8080/ in your browser!\n=><= Torch instance started for [t7] \n At this stage, you just have to open a browser and go to \n http://localhost:8080/ . The cool thing\nof course is that you can access this adress remotely. Beware though\nthat this might open up serious security issues. \n Functions \n In the broswer, you will see a terminal, which provides full history\nand live completion. Completed entries are shown on the left pane, and\nare actual hyperlinks to documentation: \n \n One cool thing about a browser-based terminal is that all the plots\nand renderings you can generate during your session can be transparently\npiped to the console: \n \n The mechanism we use to do this is very simple: the image, or plot, is dumped\nas a png into the root of the Node.js server; and we then simply print a string\nof that form:  <img src=""dumped.png""/>  to the terminal. \n In fact, this mechanism is completely general: try doing this in the terminal: \n lua\nprint \'<h1>Some title</h1> <p>a paragraph...</a>\' \n Now even more powerful: you can really print arbitrary html there, so printing \nsomething like: \n lua\nprint \'<script> console.log(""this is javascript!"") </script>\' \n ... will just work perfectly fine! \n Multiple Users \n By default, the user is set to  t7 , which is what you should see in the terminal.\nYou can create a new user by appending the string  ?user=bob  to the URL. That\'ll\ncreate a  completely  new Torch7 kernel, which only Bob sees. \n TO DO \n \n completion is still buggy: it starts screwing up after too many nested parenthesis \n inline help (triggered by the  ?  symbol) is shitty: we should use the full html\n  help instead of the poor text-based help \n I\'d love to have notebook-like capabilities, where we can load a markdown file into\n  the browser (using the URL would be ok for now,  e.g.   ?file=myscript.md ), and \n  the text part would get rendered as html, and all the code blocks will be transformed\n  in interpretable code blocks,  ala  Mathematica/IPython. \n that last point implies that we need more flexible code entries, where we can go\n  back and forth to edit the code. \n ctrl+C: not working yet. It generates a INT signal, but it doesn\'t seem to\n  do much for now. \n', 'Taken from:\nhttp://www.steve.org.uk/Software/lua-fs/docs/index.html', ""Torch (Easy) Install \n This is just an easy install script for Torch7. Eventually, it will be folded into the main repo. \n The goal of this script is to enable one line installs. To install Torch on any machine, just do: \n curl -s https://raw.github.com/clementfarabet/torchinstall/master/install-all | bash\n \n Once installed, you should be able to run Torch with basic libraries: \n torch -lparallel -loptim -lpl -limage\n \n This script has been tested on MacOS X 10.8, and Ubuntu 12.04. It should work on earlier \nUbuntus and MacOS Xs, but other platforms are not supported. \n On Ubuntu you'll need 'sudo' privileges, as the default install is global, \nand the script needs to install dependencies. \n If you've already installed the dependencies, and don't have root privileges, you \ncan use this command to just install Torch: \n curl -s https://raw.github.com/clementfarabet/torchinstall/master/install-torch | bash\n \n By default, it will install Torch in /usr/local/ , you can override this\ndefault path by doing: \n curl -s https://raw.github.com/clementfarabet/torchinstall/master/install-torch | PREFIX=~/local bash\n \n Torch7 now ships wih Luarocks, bundlde into an executable called torch-rocks.\nYou can install new packages like this: \n torch-rocks search lua-cjson\ntorch-rocks install lua-cjson\n \n By default, torch-rocks includes a link to our own Rocks repository, hosted\n here . If you wish to publish your \npackages as rocks for Torch, simply clone this repo, add your rocks, and\nmake a pull request on Github! \n Updating from a previous version \n Note that if you are coming from a previous version you are advise to clean up the old installation\nwith the following commands \n rm -rf ~/.luarocks\nrm -rf /usr/local/lib/luarocks/\nrm -rf /usr/local/lib/lua/\nrm -rf /usr/local/share/torch/\nrm -rf /usr/local/share/lua/\nrm -rf /usr/local/lua/\nrm -rf /usr/local/etc/luarocks/"", ""nn2 \n nn2 is the successor of nn. The main thing we're trying to achieve here is: \n \n better consistency across modalities (Volumetric, Spatial, Temporal) \n better performance by packing features in memory \n \n TODO List \n 'Spatial' modules need to invert their convention, by packing the features\nin memory. Modules affected: \n \n SpatialConvolution \n SpatialConvolutionMap        \n SpatialMaxPooling \n SpatialSubSampling \n Spatial*Normalization \n SpatialLPPooling \n SpatialZeroPadding \n \n 'Volumetric' modules: \n \n VolumetricConvolution \n"", 'Lunatic: Python in Lua \n Run a python interpreter within Lua. Pass data between python and lua. \n Bug-fixed fork of  lua---python  which itself is forked from  Lunatic Page . \n See  Lunatic Page  for original documentation. \n Install \n Clone this repo locally \n $ git clone git@github.com:dylski/lua---python.git\n \n Build and install \n $ cd lua---python\n$ luarocks make python-scm-0.rockspec\n', 'torchffi \n Has moved to a more community friendly  repo .', 'XML <> Lua \n This was taken from  this site . Licensed under an MIT license. \n Simply repackaged their code for Torch.\nI also added a parse() function, which simplifies the xml->table cnoversion.', ""GraphicsMagick \n A simple Lua wrapper to  GraphicsMagick . \n Only tested on Mac OSX, with GraphicsMagick installed via Homebrew. \n gm.convert \n This is just a binding to the command line convert utility (images are not loaded\ninto Lua's memory). Examples: \n lua\ngm = require 'graphicsmagick'\ngm.convert{\n   input = '/path/to/image.png',\n   output = '/path/to/image.jpg',\n   size = '128x128',\n   quality = 95,\n   verbose = true\n} \n gm.info \n Similarly, gm.info(file) is a simple binding to the command line utility.\nIt's handy to extra the geometry of an image, as well as its exif metadata.\nOn top of it, if geolocation is found, the GPS location is nicely formatted. \n lua\ngm = require 'graphicsmagick'\ninfo = gm.info('some.jpeg')\nprint(info)\n{\n   width : 1024\n   height : 768\n   date : 2013:01:01 00:00:01\n   location :\n     {\n       longitude : W80.13\n       latitude : N25.79\n     }\n   format : JPEG\n   exif :\n     {\n        Make : Apple\n        FocalLength : 413/100\n        ...\n     }\n} \n gm.Image \n This is a full C interface to GraphicsMagick's Wand API. We expose one Class: the\nImage class, which allows loading and saving images, transforming them, and\nimporting/exporting them from/to torch Tensors. \n Load library: \n lua\ngm = require 'graphicsmagick' \n First, we provide two high-level functions to load/save directly into/form tensors: \n lua\nimg = gm.load('/path/to/image.png' [, type])    -- type = 'float' (default) | 'double' | 'byte'\ngm.save('/path/to/image.jpg' [,quality])        -- quality = 0 to 100 (for jpegs only) \n The following provide a more controlled flow for loading/saving jpegs. \n Create an image, from a file: \n lua\nimage = gm.Image('/path/to/image.png')\n-- or\nimage = gm.Image()\nimage:load('/path/to/image.png') \n Create an image, from a file, with a hint about the max size to be used: \n ```lua\nimage:load('/path/to/image.png', width [, height]) \n -- this tells the image loader that we won't need larger images than\n-- what's specified. This can speedup loading by factors of 5 to 10.\n``` \n Save an image to disk: \n ```lua\nimage:save('filename.ext') \n -- where:\n-- ext must be a know image format (jpg, JPEG, PNG, ...)\n-- (GraphicsMagick supports tons of them)\n``` \n Create an image, from a Tensor: \n ```lua\nimage = gm.Image(tensor,colorSpace,dimensions)\n-- or\nimage = gm.Image()\nimage:load(tensor,colorSpace,dimensions) \n -- where:\n-- colorSpace is: a string made of these characters: R,G,B,A,C,Y,M,K,I\n--                (for example: 'RGB', 'RGBA', 'I', or 'BGRA', ...)\n--                R: red, G: green, ... I: intensity\n--\n-- dimensions is: a string made of these characters: D,H,W\n--                (for example: 'DHW' or 'HWD')\n--                D: depth, H: height, W: width\n``` \n Export an image to a Tensor: \n ```lua\nimage = gm.Image('path.jpg')\nimage:toTensor(type,colorSpace,dimensions) \n -- where:\n-- type : 'float', 'double', or 'byte'\n-- colorSpace : same as above\n-- dimensions : same as above\n``` \n When exporting Tensors, we can specify the color space: \n ```lua\nlab = image:toTensor('float', 'LAB')\n-- equivalent to:\nimage:colorspace('LAB')\nlab = image:toTensor('float') \n -- color spaces available, for now:\n-- 'LAB', 'HSL', 'HWB' and 'YUV'\n``` \n Images can also be read/written from/to Lua strings, or binary blobs.\nThis is convenient for in memory manipulation (e.g. when downloading\nimages from the web, no need to write it to disk): \n ```lua\nblob,size = image:toBlob()\nimage:fromBlob(blob,size) \n str = image:toString()\nimage:fromString(str)\n``` \n In this library, we use a single function to read/write parameters\n(instead of the more classical get/set).  \n Here's an example of a resize: \n ```lua\n-- get dimensions:\nwidth,height = image:size() \n -- resize:\nimage:size(512,384) \n -- resize by only imposing the largest dimension:\nimage:size(512) \n -- resize by imposing the smallest dimension:\nimage:size(nil,512)\n``` \n Some basic transformations: \n lua\n-- flip or flop an image:\nimage:flip()\nimage:flop() \n Sharpen: \n lua\n-- Sharpens the image whith radius=0, sigma=0.6\nimage:sharpen(0, 0.6) \n Show an image (this makes use of Tensors, and Torch's Qt backend): \n lua\nimage:show() \n One cool thing about this library is that all the functions can be cascaded.\nHere's an example: \n lua\n-- Open, transform and save back:\ngm.Image('input.jpg'):flip():size(128):save('thumb.jpg')"", 'RestClient \n A simple client for REST APIs. This package provides a few functions \nto get and post from/to restful APIs.', ""LBFGS \n LibLBFGS (C Lib) wrapper. \n This is an FFI interface to  LibLBFGS . \n Installation \n Simply build and install  LibLBFGS  \n(with no SSE2 support, for now I don't support aligned memory blocks). \n This package can be installed with Luarocks. \n Usage \n The code in test.lua demonstrates how to use the solver. Its interface\nis 100% compatible to the solvers in  optim ."", ""CURL \n A simple interface to CURL. \n Provides two functions:  get  and  post . \n get : \n ```lua\n-- load lib:\ncurl = require 'curl' \n -- getting random pages:\nres = curl.get('http://www.google.com') \n -- with query:\nres = curl.get('http://www.google.com', {safe='off', output='search', oq='test'}) \n -- complete API:\nres = curl.get{\n    host = 'http://blogname.blogspot.com',\n    path = '/feeds/posts/default',\n    query = {\n        alt = 'json'\n    },\n    format = 'json' -- parses the output: json -> Lua table\n} \n -- Getting an image, and decoding it:\nimg = curl.get('http://www.webstandards.org/files/acid2/reference.png')\nrequire('graphicsmagick').Image():fromString(img):show()\n``` \n post : \n lua\n-- post has the same API, with a form parameter (instead of query)\nres = curl.post{\n    host = 'http://myserver.com',\n    path = '/',\n    form = {\n        username = 'bob',\n        password = 'key',\n        somefiletoupload = '@/local/path/to/file.jpg'\n    }\n}"", ""gfx.js: a browser-based graphics server \n Originally forked from the amazing  tty.js . \n The goal is to extend this project to support the creation of rich media windows,\non top of the terminal windows. \n The idea is simple: the server watches a directory, and monitors the creation &\nmodification of HTML files; upong modification / creation, it creates a new window\non the client side (browser), which simply render the HTML.  \n Clients are easy to develop: one simply needs to dump HTML into the watched\ndirectory to have it rendered by the browser. \n For now, I'm focusing on one client, written in Lua, for \n Torch7 . \n \n Check out  tty.js  for reference on the\noriginal project. Note: I'm simply extending their project, not modifying\nany of the core structure, so it should remain compatible. \n Installation \n You have to have Node.js (important, Version >= 0.10.0), NPM, and Torch7\ninstalled. With older versions of Node.js, things won't be stable. You also\nneed libgraphicsmagick-dev to be installed: \n ```sh \n OS X \n brew install graphicsmagick \n Ubuntu \n apt-get install libgraphicsmagick1-dev\napt-get install graphicsmagick\n``` \n Then simply run: \n sh\nluarocks install https://raw.github.com/clementfarabet/gfx.js/master/gfx.js-scm-0.rockspec \n Or, if you cloned the repo locally: \n sh\nluarocks make \n Execution \n Once installed, you can start/stop the server like this (I'm assuming a LuaJIT-based install): \n luajit -lgfx.start\nluajit -lgfx.stop \n And then open up a tab in your browser, at  http://localhost:8000 . \n The browser acts as a passive graphics backend. The server monitors the creation of new\nresources (charts, plots, videos, ...), and lets the browser know it should render them. \n The framework is very flexible: resources can be rendered by a client (luajit) with no\nbrowser open, and even no server listening/running. The resources generated will still\nbe saved, and can be visualized later (very useful to generate resources/charts on\na server with no X session). \n You can optionally specify a different port as an env variable, if the default (8000)\nis not available: \n PORT=4321 luajit -lgfx.start\nPORT=4321 luajit -lgfx.stop \n Also, we provide a useful PS script, which lists running servers: \n luajit -lgfx.ps \n On Mac OS, we also provide a shortcut to start the server in the background and automatically\nopen the browser at the right location: \n luajit -lgfx.go \n Alternatively, you can do things step by step: \n ```\nluajit -lgfx.start \n starts a server... \n luajit \n starts a Torch session \n ``` \n At the prompt, you can load the gfx.js client, and render things: \n lua\ngfx = require 'gfx.js'\ngfx.image(image.lena())\ngfx.image({\n   image.lena()\n   image.lena()\n   image.lena()\n   image.lena()\n   image.lena()\n   image.lena()\n   image.lena()\n   image.lena()\n}, {zoom=0.5, legends={'Image 1', 'Image 2'}}) \n This will produce this output: \n \n I've also slowly started to integrate plots from  NVD3 , and bind\nthem to Torch, so that they can seamlessly be called from the Torch repl: \n ```lua\ngfx.chart(data, {\n   chart = 'line', -- or: bar, stacked, multibar, scatter\n   width = 600,\n   height = 450,\n}) \n -- where data has the form:\ndata = {\n    {\n        key = 'Legend 1',\n        color = '#0f0',\n        values = { {x=0,y=0}, {x=1,y=1}, ... },\n    },\n    {\n        key = 'Legend 2',\n        color = '#00f',\n        values = { {x=0,y=0}, {x=1,y=1}, ... },\n    },\n} \n -- or, for a single dataset:\ndata = {\n    key = 'Legend',\n    values = { {x=0,y=0} , ... }\n} \n -- values can be provided in convenient ways:\nvalues = { {x=0,y=0[,size=0]}, ... }\nvalues = { {0,0,0}, ... }\nvalues = torch.randn(100,2)\nvalues = torch.randn(100,3)  -- the 3rd dimension is the optional size, only used by certain charts\nvalues = torch.randn(100) -- in this case, y is provided, x defaults to range(0,N-1) \n -- shortcuts are also provided for quick plots:\ngfx.chart(torch.randn(100,2), {chart='scatter'})\ngfx.chart(torch.randn(100), {chart='line'})  -- y is provided, x will be a range(1,N)\ngfx.chart({ torch.randn(100), torch.randn(100) })  -- multiple datasets\ngfx.chart({ {1,2,3,4,5,6,7,8,7,6,5,4,3,2,1}, torch.randn(100) })  -- multiple datasets, table format\n``` \n As explained above, one can generate resources/charts/figures with no server listening.\nOne can connect a server later on, and redraw the last resources generated. Here are a few\nuseful commands for that: \n lua\ngfx = require 'gfx.js'\nids = gfx.list(10) -- will list the last 10 figures generated (each figure has a unique ID)\nprint(ids[1])\n-- will print something like: dom_1212817597132847893127489\ngfx.redraw(ids[1]) -- will redraw this resource\ngfx.redraw(10) -- will redraw the last 10 resources available (sorted by descending time) \n Finally, the server gets slower as the number of resources/charts/images grows in the \nwatched directory. It's useful to sometimes clear this cache manually:\n gfx.clear()"", ""PERSIST \n A persisting table for Lua. \n Built using Redis, it's a simple abstraction that allows\none to write/read from a table that persists over sessions (the\nkey/vals are persisted in Redis). \n ```lua\n-- load lib:\np = require('persist')() \n -- write a few things to it:\np.test = 'something'\np.test2 = {\n    some = 'table',\n    nested = {is=1}\n}\n``` \n Shut down, start again: \n ```lua\n-- load lib:\np = require('persist')() \n -- still there?\nprint(p.test)\nprint(p.test2)\n``` \n The following options can be passed: \n lua\np = require('persist')({\n   url = 'localhost',\n   port = 6379,\n   verbose = false, -- this is not only used on startup\n   namespace = 'th',  -- this is the namespace in Redis\n   clear = false, -- clear all the data\n})"", 'ASyNC \n An async framework for Lua/Torch, based on  LibUV \n(using Tim Caswell\'s  luv  library). \n This lib is heavily inspired on the Node.js architecture. It\'s fun, elegant, and\nshould be extremely efficient (a lot of testing is required). \n The examples in  tests/  should provide enough documentation on the API. \n License \n MIT License \n Examples \n Starting the event loop. At the end of any program, the event loop must be started.\nNothing will be interpreted after this call, as it takes control of the runtime. \n lua\nasync.go() \n It\'s useful to have a REPL (interpreter) running asynchronously, for debugging and\nlive control of your programs: \n lua\nasync.repl()  -- fires up an asyncronous repl \n lua\nasync.repl.listen({host=\'0.0.0.0\', port=8080})   -- fires up an async repl through a TCP server\nasync.repl.connect({host=\'0.0.0.0\', port=8080})  -- connects to a remote repl through a TCP client \n Common JS like timer controls:\n lua\nasync.setInterval(millis, function()\n   print(\'printed every N millis\')\nend)\nasync.setTimeout(millis, function()\n   print(\'printed once in N millis\')\nend) \n CPU Info. Useful to know how many processors are available.\nThis is a synchronous call. \n lua\nprint(async.cpuInfo()) \n A TCP server: \n ```lua\nasync.tcp.listen({host=\'0.0.0.0\', port=8080}, function(client)\n   -- Receive:\n   client.ondata(function(chunk)\n      -- Data:\n      print(\'received: \' .. chunk) \n   -- Reply:\n  client.write(\'thanks!\')\n \n end) \n -- Done:\n   client.onend(function()\n      print(\'client gone...\')\n   end)\nend)\n``` \n A TCP client: \n ```lua\nasync.tcp.connect({host=\'127.0.0.1\', port=8080}, function(client)\n   -- Write something\n   client.write(\'something .. \' .. i) \n -- Callbacks\n   client.ondata(function(chunk)\n      print(\'received: \' .. chunk)\n      client.close()\n   end) \n -- Done:\n   client.onend(function()\n      print(\'connection closed...\')\n   end)\nend)\n``` \n File I/O. The low level interface is not complete yet, but the high-level one\nis final: \n lua\nasync.fs.readFile(\'LICENSE\', function(content)\n   print(content)\n   async.fs.writeFile(\'LICENSE.copy\', content, function(status, err)\n      print(\'==> wrote file: \' .. (status or err))\n   end)\nend) \n A lower-level interface is also available, for C-level performance. The upside:\nno copy is done, the user callback gets the raw pointer to the network buffer (read)\nand writes tap directly into the raw buffer, provided by the user. The downside:\nthe buffer returned by the ""ondata"" callback lives only for the scope of that callback,\nand must be copied by the user... \n ```lua\n-- assuming a client handle: \n local b = require \'buffer\' \n client.onrawdata(function(chunk)\n   -- chunk is a Buffer object (https://github.com/clementfarabet/buffer)\n   print(chunk) \n -- chunk will not be valid past this point, so its content must be copied,\n   -- not just referenced...\n   local safe = chunk:clone()\n   -- safe can be past around... \n -- the most common use is to copy that chunk into an existing storage,\n   -- for instance a tensor:\n   -- (assuming tensor is a torch.Tensor)\n   local dst = b(tensor)  -- creates a destination buffer on the tensor (a view, no copy)\n   dst:copy(src)\nend) \n -- write() also accepts buffers:\nclient.write( b\'this is a string saved in a buffer object\' ) \n -- last, the sync() interface can be set up in raw mode:\nclient.syncraw()\nlocal buffer = client.read()\n-- ...\n``` \n We also provide a simple async interface to CURL. \n Provides two functions:  get  and  post . \n get : \n ```lua\n-- simple URL:\nasync.curl.get(\'http://www.google.com\', function(res)\n    print(res)\nend) \n -- complete API:\nasync.curl.get({\n    host = \'http://blogname.blogspot.com\',\n    path = \'/feeds/posts/default\',\n    query = {\n        alt = \'json\'\n    },\n    format = \'json\' -- parses the output: json -> Lua table\n}, function(res)\n   print(res)\nend) \n -- Getting an image, and decoding it:\ncurl.get(\'http://www.webstandards.org/files/acid2/reference.png\', function(res)\n  local decoded = require(\'graphicsmagick\').Image():fromString(res)\nend)\n``` \n post : \n ```lua\n-- post has the same API, with a form parameter (instead of query):\nasync.curl.post({\n    host = \'http://myserver.com\',\n    path = \'/\',\n    form = {\n        username = \'bob\',\n        password = \'key\',\n        somefiletoupload = \'@/local/path/to/file.jpg\'\n    }\n}, function(res)\n   print(res)\nend) \n -- or a simple file upload:\nasync.curl.post({\n    host = \'http://myserver.com\',\n    path = \'/\',\n    file = \'@/path/to/file.png\',\n}, function(res)\n   print(res)\nend)\n```', ""Manifold \n A package to manipulate manifolds, for Torch7. \n Install \n sh\nluarocks install manifold \n Dependencies \n In order to be able to run the binaries, you need to install the package  libatlas3-base .\nOn a Ubuntu machine you can execute the following commands. \n sudo apt-get update\nsudo apt-get install libatlas3-base \n Use \n ```lua\n-- package:\nm = require 'manifold' \n -- a dataset:\nt = torch.randn(100,10) -- 100 samples, 10-dim each \n -- basic functions:\nns = m.neighbors(t) -- return the matrix of neighbors for all samples (sorted)\nds = m.distances(t) -- return the matrix of distances (L2)\nts = m.removeDuplicates(t) -- remove duplicates from dataset \n -- embeddings:\np = m.embedding.random(t, {dim=2})  -- embed samples into a 2D plane, using random projections\np = m.embedding.lle(t, {dim=2, neighbors=3})  -- embed samples into a 2D plane, using 3 neighbor (LLE)\np = m.embedding.tsne(t, {dim=2, perplexity=30})  -- embed samples into a 2D plane, using tSNE\n``` \n Demos \n To run the demos, simply type the following commands. \n sh\ncd demos\nqlua demo_swissroll.lua\nqlua demo_tsne.lua \n Below is an example of a t-SNE map produced on 5,000 MNIST digits by the  demos/demo_tsne.lua  demo. \n"", 'Buffer \n A buffer object for LuaJIT. The goal: efficient, C-speed, byte manipulation\nfor LuaJIT. \n Also provides interfaces to Torch\'s tensors and storages, for easy serialization. \n Install \n luarocks install buffer \n Simple use cases \n Load lib: \n ```lua \n \n b = require \'buffer\'\n``` \n \n Create a buffer, from a string, with a size, or from\nanother buffer: \n ```lua \n \n buf = b\'some\'\nprint(buf)\n \nbuf = b(10)\nprint(buf)\n \nbuf2 = b(buf)\nprint(buf2)\n \nbuf[1] = 10\nbuf[2] = 20\nprint(buf2)\n \n``` \n \n Creating buffers never makes copies. A buffer created from a string\nalways references the content of the string. A buffer created from\nanother buffer references the same buffer. \n Concatenating two buffers is done like it\'s done for strings: \n ```lua \n \n a = b\'some\' .. b\'thing\'\nstr = a:toString()\nprint(str)\nsomething\n``` \n \n The  toString  method simply returns a Lua string from the buffer. \nIn this case, the string is a copy, which won\'t be affected by further\nchanges of the buffer: \n ```lua \n \n a[1] = a[1] + 1\nprint(str)\nsomething\nprint(a:toString())\ntomething\n``` \n \n A slicing operator is provided: \n ```lua \n \n a = b\'testing\'\nprint(a[{1,4}])\ntest\na[{1,4}] = \'sing\'\na[{1,4}] = b\'sing\'  -- both supported\nprint(a)\nsinging\n``` \n \n A buffer can be created from a list of buffers, which provides efficient\nconcatenation: \n ```lua \n \n a1 = b\'test\'\na2 = b\'test\'\na3 = b\'again\'\na = b(a1,a2,a3)\nprint(a:toString())\ntesttestagain\nb = b( {a1,a2,a3} )\nprint(b:toString())\ntesttestagain\n``` \n \n Finally, cloning a buffer allows clean memory separation: \n ```lua \n \n a = b\'test\'\nc = a:clone()\n``` \n \n More advanced constructors are also available, to mount buffers on arbitrary\nmanaged or unmanaged chunks of memory. See tests for examples. \n Last, if Torch is available, converters are available from buffers to tensors\nand back. This is especially handy for multithreaded / multimachine environments,\nwhere exchanging tensors must be done at optimal speed (i.e. with no complex \nserialization). \n ```lua \n \n t = torch.FloatTensor(10):normal()\nbuf = b(t)\n-- buf is now a view on t\'s underlying contiguous storage\n-- buf could be transmitted over sockets / threads, as raw binary data (see async for use cases) \n \n -- from buf, new storages or tensors can be constructed like this: \n \n tt = buf:toFloatStorage()\ntt = buf:toFloatTensor()\ntt = buf:toFloatTensor(2,5)\n-- these are all views on the original storage of t.\n``` \n \n License \n Code was originally inspired from the Luvit folks. \n Copyright 2013-2014 Clement Farabet (MADBITS)\nCopyright 2012 The Luvit Authors. All Rights Reserved. \n Licensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at \n http://www.apache.org/licenses/LICENSE-2.0\n \n Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS-IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.', 'LuaForever \n Run a Lua/LuaJIT script forever. \n Example: \n ```bash\nluajit myscript.lua arg1 arg2 \n error, script crashes... \n luaforever myscript.lua arg1 arg2 \n error, script crashes... \n myscript.lua restarts automatically \n error, script crashes... \n myscript.lua restarts automatically \n ... \n ```', ""THMAP \n A simple distributed framework to map jobs/work onto multiple workers. \n The framework provides two binaries:  thmap  and  thnode .  thmap  is a controller,\nthat lets you mirror commands to multiple  thnode  instances, and absorb the output\nof all these  thnode  instances in //. \n The general philosophy is that you run a bunch of  thnode  instances on multiple machines,\nthen run  thmap  everytime you need to schedule/run new scripts onto these nodes. \n thmap  lets you git pull, and reload scripts, so that you can easily update a \ndistributed code base. \n Install \n sh\nluarocks install thmap \n Use \n On the compute machine(s), run  thnode : \n sh\nthnode \n Write a config file that lists all the available nodes ( nodes.lua ): \n lua\n-- example configuration, with two nodes:\nreturn {\n   {host='ip1', port=10001},\n   {host='ip2', port=10001}\n} \n (by default,  thnode  listens to port 10001, then increments if not available). \n Then start  thmap  to start monitoring and dispatching jobs: \n sh\nthmap --nodes nodes.lua\nip1:10001> ip2:10002> spawn('th', {'test.lua'})\n... \n The above shows you a double shell: you are connected to two  thnode  instances,\nand any command you issue will be mirrored to both. \n In its current version,  thmap  supports the following commands: \n \n spawn a job:             spawn('th', {'script.lua', 'opt1', 'opt2', ...}, {autorestart=true}) \n restart running jobs:    restart() \n list running jobs:       ps() \n exec git command:        git 'pull'  /  git 'status' \n git pull + restart:      update() \n kill all zombies:        zombies() \n"", 'Schedlua \n Basic scheduler.', ""(Note: this package is provided for compatibility with public tutorials.\n It is not maintained anymore.) \n GM: Graphical Models for Torch/LuaJIT \n This package provides standard functions to create (arbitrary) \nundirected graphical models, using adjacency matrices. \n A graph is described by an adjacency matrix, node potentials\nand edge potentials.  \n Install \n sh\n$ git clone ...\n$ [sudo] luarocks make \n Use \n First run torch, and load gm: \n sh\n$ th   \n ``` lua \n \n require 'gm'\n``` \n \n Once loaded, see and run the examples: \n ``` lua \n \n gm.examples.simple()\ngm.examples.trainMRF()\ngm.examples.trainCRF()\n``` \n"", ""nnop \n Parameter-free / operation-only Neural Network primitives\nfor torch/nn. \n Motivation, Goals \n Sometimes, it's useful to treat parameters as regular states,\nto either impose certain constraints on them, or simply\nmake weight sharing visible / straight-forward. \n The original design of  nn  treats\ntrainable parameters as special variables. This package,  nnop ,\nbuilds on  nn  and  nngraph , but separates parameters from operations. \n It introduces a new module,  nn.Parameters , which provides trainable\nparameters, but does not do any computation. Every other parameterized\nnode ( nn.Linear ,  nn.SpatialConvolution , ...) needs to be wrapped in\n nnop  to decouple trainable parameters, and become pure operation nodes. \n TODO \n \n wrap remaining parametrized nodes ( nn.SpatialConvolution , ...) \n simplify/unify auto-generated parameterNodes? \n \n Examples \n Weight sharing \n In this example, 2 modules are connected to a same set of trainable\nparameters. This is weight sharing. \n ```lua\n-- Create parameters:\nlinearWeight = nnop.Parameters(10,100)()\nlinearBias = nnop.Parameters(10)() \n -- Create multiple layers, all connected to these parameters:\ninput1 = nn.Identity()()\ninput2 = nn.Identity()()\nlinear1 = nnop.Linear()({input1, linearWeight, linearBias})\nlinear2 = nnop.Linear()({input2, linearWeight, linearBias}) \n -- Graph:\ngraph = nn.gModule({input1,input2}, {linear1,linear2}) \n -- Tests:\nres = graph:forward({torch.randn(100), torch.randn(100)})\nassert(type(res) == 'table' and #res == 2) \n input = torch.randn(100)\nres = graph:forward({input, input})\nassert(res[1]:dist( res[2] ) == 0)\n``` \n Penalty on a set of parameters \n In this example, we add an L1 penalty on a set of weight. \n When parameters are provided to the nnop.Linear constructor,\nparameter nodes are automatically created (and automatically\nconnected in the graph!). We use this in this example, this\nway we don't have to create the parameter nodes, but are still\nfree to access them and add a penalty on them. \n ```lua\n-- create base modules:\nlinear1 = nnop.Linear(10,100)\ntanh1 = nn.Tanh()\nlinear2 = nnop.Linear(100,2) \n -- bind them in a graph:\ninput = nn.Identity()()\nlayer1 = linear1(input)\nlayer2 = tanh1(layer1)\nlayer3 = linear2(layer2) \n -- get weights and impose penalty:\nweight1 = linear1.parameterNodes.weightNode\nsparse1 = nn.L1Penalty(.001)(weight1) \n -- build final model:\nmodel = nn.gModule({input}, {layer3}) \n -- train the model:\nfor i = 1,10 do\n   input = torch.rand(10)\n   output = model:forward(input)\n   gradOutput = torch.rand(2)\n   gradInput = model:updateGradInput(input, gradOutput)\n   model:accGradParameters(input, gradOutput)\nend\n```"", ""nnfunc \n Functionalize nn modules: the goal of this package is to make it\neasy to develop 3rd-party frameworks, by re-exposing nn modules\nas functions. Basically provide a functional API to nn. \n Every instantiated module becomes a simple state-less function:\ninput data and parameters must be provided as inputs to this function;\nsame thing for gradients. For convenience and efficiency, the state\nof the underlying nn module is still relied on for caching (every function\nreturned by nnfunc is a closure relying on an instantiated nn module). \n API \n Expose packages \n Any package that provides  nn.Module  children can be exposed. \n lua\nnnfunc.functionalize 'nn'   -- done by default by nnfunc\nnnfunc.functionalize 'nnx'  -- bundle new package... \n Once called, every module in the source package is available to\nuse as a function; see examples below. \n API #1 \n A single function that evaluates the module, and automatically\ncomputes gradients if  gradOutput  is provided. \n ```lua\n-- this returns a function that can be used to eval this module and\n-- its gradients:\nlayer = nnfunc.nn.Linear(10,100) \n -- compute module's output:\nprediction = layer({\n   input = torch.randn(10),\n   weight = torch.randn(100,10), bias = torch.randn(100),\n})\n-- prediction looks like this:\n-- {\n--    output = torch.Tensor(100)\n-- } \n -- output can be user-provided, optionally:\nprediction = layer({\n   input = torch.randn(10),\n   weight = torch.randn(100,10), bias = torch.randn(100),\n   output = torch.Tensor(100),\n})\n-- output is now valid \n -- compute gradients (backprop) - this happens automatically\n-- because gradOutput is provided:\ngrads = layer({\n   input = torch.randn(10),\n   weight = torch.randn(100,10), bias = torch.randn(100),\n   gradOutput = torch.randn(100),\n})\n-- grads looks like this:\n-- {\n--    gradInput = torch.Tensor(10),\n--    gradWeight = torch.Tensor(100,10),\n--    gradBias = torch.Tensor(100),\n-- } \n -- the user can also provide all the tensors for computed gradients,\n-- if her application requires that they be owned externally:\ngrads = layer({\n   input = torch.randn(10),\n   weight = torch.randn(100,10), bias = torch.randn(100),\n   gradOutput = torch.randn(100),\n   gradWeight = torch.zeros(100,10), bias = torch.zeros(100),\n   gradInput = torch.zeros(10),\n})\n-- user-provided gradInput, gradWeight and gradBias are now\n-- valid!\n``` \n API #2 \n Two separate functions: one for eval, one for gradients. This\ncan be useful when separate function pointers need to be used\nto register gradients. \n ```lua\n-- two separate functions:\nlayer,gradLayer = nnfunc.nn.Linear(10,100) \n -- compute module's output [same as API #1]:\nprediction = layer({\n   input = torch.randn(10),\n   weight = torch.randn(100,10), bias = torch.randn(100),\n}) \n -- compute gradients (backprop) [separate function for grads]:\ngrads = gradLayer({\n   input = torch.randn(10),\n   weight = torch.randn(100,10), bias = torch.randn(100),\n   gradOutput = torch.randn(100),\n})\n``` \n A hash table is also maintained to retrieve gradients associated\nto any object created: \n ```lua\n-- two separate functions:\nlayer,gradLayer = nnfunc.nn.Linear(10,100) \n -- gradLayer could be retrieve like this:\ngradLayer2 = nnfunc.gradsOf[layer]\nassert(gradLayer2 == gradLayer)\n```"", 'Something.', ""Regress \n A very simple regression test package. \n ```lua\nlocal test = require 'regress' \n test {\n   test1 = function()\n      test.mustBeTrue(a == b, 'a should == b')\n   end, \n test2 = function()\n      test.shouldBeTrue(a == b, 'a should == b')\n   end,\n}\n```""]",['repository_count'],MadbitsAI,https://avatars.githubusercontent.com/u/188959?v=4,False,2016-09-23 00:09:18,676,18,1,2010-01-24T19:09:14Z,2022-11-17T22:36:38Z,clmt,False,False,False,False,False,True,False,0,"{'C': 18, 'Lua': 45, 'Shell': 2, 'C++': 3, 'PHP': 1, 'JavaScript': 3, 'Vim Script': 1, 'Perl': 1, 'Python': 1, 'Cuda': 1}",5,Palo Alto,Data Science,4,0.54,0.91,0.8,0
ajabri,A. Jabri,PhD student,,"['pytorch-maml \n This is a PyTorch implementation of the supervised learning experiments from the paper \nModel-Agnostic Meta-Learning (MAML): https://arxiv.org/abs/1703.03400 \n Important : You will need the latest version of PyTorch, v.0.2.0 to run this code (otherwise you will get errors about \ndouble backwards not being supported). \n Currently, only the Omniglot experiments have been replicated here. The hyper-parameters are the same as those used in the original \nTensorflow implementation, except that only 1 random seed is used here. \n 5-way 1-shot training, best performance 98.9% \n \n 20-way 1-shot training, best performance 92% \n \n Note: the 20-way performance is slightly lower than that reported in the paper (they report 95.8%). If you can see why this might be,\nplease let me know. Also in this experiment, we can see evidence of overfitting to the meta-training set. \n The 5-way results are achieved by simply meta-testing the network trained on the 1-shot task on the 5-shot task (e.g. for the 5-way 5-shot result, test the 5-way 1-shot trained network with 5-shots). Again the 20-way result is lower here than reported in the paper. \n This repo also contains code for running maml experiments on permuted MNIST (tasks are created by shuffling the labels).\nThis is a nice sanity check task.', 'carml', ""Space-Time Correspondence as a Contrastive Random Walk \n  ![](https://github.com/ajabri/videowalk/raw/master/figs/teaser_animation.gif)  \n \n \n \n This is the repository for  Space-Time Correspondence as a Contrastive Random Walk , published at NeurIPS 2020.   \n [ Paper ]\n[ Project Page ]\n[ Slides ]\n[ Poster ]\n[ Talk ] \n @inproceedings{jabri2020walk,\n    Author = {Allan Jabri and Andrew Owens and Alexei A. Efros},\n    Title = {Space-Time Correspondence as a Contrastive Random Walk},\n    Booktitle = {Advances in Neural Information Processing Systems},\n    Year = {2020},\n} \nConsider citing our work or acknowledging this repository if you found this code to be helpful :) \n Requirements \n \n pytorch (>1.3) \n torchvision (0.6.0) \n cv2 \n matplotlib \n skimage \n imageio \n \n For visualization ( --visualize ):\n- wandb\n- visdom\n- sklearn \n Train \n An example training command is:\n python -W ignore train.py --data-path /path/to/kinetics/ \\\n--frame-aug grid --dropout 0.1 --clip-len 4 --temp 0.05 \\\n--model-type scratch --workers 16 --batch-size 20  \\\n--cache-dataset --data-parallel --visualize --lr 0.0001 \n This yields a model with performance on DAVIS as follows (see below for evaluation instructions), provided as  pretrained.pth :\n J&F-Mean    J-Mean  J-Recall  J-Decay    F-Mean  F-Recall   F-Decay\n  0.67606  0.645902  0.758043   0.2031  0.706219   0.83221  0.246789 \n Arguments of interest: \n \n --dropout : The rate of edge dropout (default  0.1 ). \n --clip-len : Length of video sequence. \n --temp : Softmax temperature. \n --model-type : Type of encoder. Use  scratch  or  scratch_zeropad  if training from scratch. Use  imagenet18  to load an Imagenet-pretrained network. Use  scratch  with  --resume  if reloading a checkpoint. \n --batch-size : I've managed to train models with batch sizes between 6 and 24. If you have can afford a larger batch size, consider increasing the  --lr  from 0.0001 to 0.0003. \n --frame-aug :  grid  samples a grid of patches to get nodes;  none  will just use a single image and use embeddings in the feature map as nodes. \n --visualize : Log diagonistics to  wandb  and data visualizations to  visdom . \n \n Data \n We use the official  torchvision.datasets.Kinetics400  class for training. You can find directions for downloading Kinetics  here . In particular, the code expects the path given for kinetics to contain a  train_256  subdirectory. \n You can also provide  --data-path  with a file with a list of directories of images, or a path to a directory of directory of images. In this case, clips are randomly subsampled from the directory. \n Visualization \n By default, the training script will log diagnostics to  wandb  and data visualizations to  visdom . \n Pretrained Model \n You can find the model resulting from the training command above at  pretrained.pth .\nWe are still training updated ablation models and will post them when ready. \n \n Evaluation: Label Propagation \n The label propagation algorithm is described in  test.py .  The output of  test.py  (predicted label maps) must be post-processed for evaluation. \n DAVIS \n To evaluate a trained model on the DAVIS task, clone the  davis2017-evaluation  repository, and prepare the data by downloading the  2017 dataset  and modifying the paths provided in  eval/davis_vallist.txt . Then, run: \n Label Propagation: \n python test.py --filelist /path/to/davis/vallist.txt \\\n--model-type scratch --resume ../pretrained.pth --save-path /save/path \\\n--topk 10 --videoLen 20 --radius 12  --temperature 0.05  --cropSize -1 \nThough  test.py  expects a model file created with  train.py , it can easily be modified to be used with other networks. Note that we simply use the same temperature used at training time. \n You can also run the ImageNet baseline with the command below.\n python test.py --filelist /path/to/davis/vallist.txt \\\n--model-type imagenet18 --save-path /save/path \\\n--topk 10 --videoLen 20 --radius 12  --temperature 0.05  --cropSize -1 \n Post-Process: \n``` \n Convert \n python eval/convert_davis.py --in_folder /save/path/ --out_folder /converted/path --dataset /davis/path/ \n Compute metrics \n python /path/to/davis2017-evaluation/evaluation_method.py \\\n--task semi-supervised   --results_path /converted/path --set val \\\n--davis_path /path/to/davis/\n``` \n You can generate the above commands with the script below, where removing  --dryrun  will actually run them in sequence.\n python eval/run_test.py --model-path /path/to/model --L 20 --K 10  --T 0.05 --cropSize -1 --dryrun \n Test-time Adaptation \n To do.""]",['repository_count'],UC Berkeley / BAIR,https://avatars.githubusercontent.com/u/1448621?u=29c6a12f5b1911d99562a9ac53153ef2df80941f&v=4,False,2016-10-14 23:17:29,152,12,0,2012-02-18T06:59:44Z,2022-10-03T20:31:27Z,,False,False,False,False,False,True,False,0,"{'Java': 3, 'Jupyter Notebook': 2, 'Python': 2}",7,,Mobile,58,1.0,0.3,0.19,0
korymath,Kory,"Research Scientist DeepMind, 

CS PhD University of Alberta

Intern Apple Special Projects Group, Google Brain, Twitter Cortex ",korymath@gmail.com,"['Gaze Vector Regression Testing \n Directory structure: \n git pull https://github.com/korymath/gazevectorregression \n Data files as *.mat files should be located in \n /data \n run_basic_exp will run on all *.mat files in the /data folder. \n run_single_test provides a single experiment and you need to set train and test data files \n Model building \n Model is built in build_single_model.m \n Can edit the way that the model is built by changing the mdl to other model builders from MATLAB. \n modelBuildingIdeas.m has several ideas to try. I think that the best option will be a bagged regression, wishing there was a more automated trial set up in matlab. \n One thing to do would be to output all the experiments to train and test directories so that we could rerun validations in Python autoML libraries, and faster c-modules to see if we could build up a solid single model. \n Notes on data from : \n all calibrations are sweep, eyes, free \n the column switches between sweep and task \n RXX - XX patient number \n Calib(C|P) - cups or pasta, when this changes, task changes, always three for each\nassume new task mean adjustment \n B  - ignore (both eye and movement) \n NN - 01, 02, 03 in sequence trials -- 1 and 2 are before then a whole bunch of trials, then 3 is the after trials calibration \n _combined_segments - eye data, \n TODO:  \n TESTING 1 on 2 and 1 on 3 compare', 'ContinuousTimeActorCritic \n Continuous Time Natural Actor Critic Reinforcement Learning \n This repository is a basic actor-critic reinforcement learning implementation based on the following papers: \n http://www.ualberta.ca/~pilarski/docs/papers/Pilarski_2011_ICORR.pdf \n and \n http://www.ualberta.ca/~pilarski/docs/papers/Pilarski_2013_ICORR_Postprint.pdf \n How can ACRL be used for multi-joint control of robotic limbs? ', 'cont-RL-order \n Experimenting with mountain car continuous task using order switching \n Basic Mountain Car Q-learning with some hot mods.  \n To plot use the plot function with the following syntax: \n python plot.py value500ep', 'Pyggy \n Artificial Improvisation \n This is a chat bot used in a live performance.', ""The Plotto Plot Machine \n A program to automate the algorithm described in  William Wallace Cook's Plotto, The Masterbook of All Plots . \n This program is written in Python for ease & legibility. \n This program relies upon the text of Plotto to generate story structure outlines. \n The text of Plotto is obtained from: https://openlibrary.org/works/OL16087095W/Plotto. \n This work is licensed under a  Creative Commons Attribution-ShareAlike 4.0 International License ."", 'runkeeper-testing \n Modified code from  @kylemath  for comparing multiple similar length trials of the same distance.  \n Details \n This matlab code works by smoothing the time derivative (smoothLength) of the GPX data and then plotting comparable runs of similar distances on the same plot. \n It also adds markers for kilometers and miles.  \n It works really well to see the comparison of two (or mulitple) runs, as can be seen right here:  \n', '#StockView \n go/StockView \n Twitter can tell us what\'s happening, but it is not great for finding business information. \n With this extension, you will be able to see live stock information right in your Twitter feed providing context for the current discussion. \n Now, when you search Twitter for cashtags like \'$TWTR\', the stock chart will be displayed so you can really know what is happening. \n \n Install \n #StockView is packaged as a chrome extension. You can download it here https://github.com/korymath/StockView/raw/master/app.crx. \n To install, you should just drag \'n\' drop the .crx file into Chrome.\nIn Chrome, open Extensions tab (chrome://extensions), drag \'n\' drop the .crx file and you are good to go. \n Amusingly ""for regular Windows users who are not skilled with computers, it is practically not possible to install and use extensions from outside the Chrome Web Store."" -- http://stackoverflow.com/questions/24577024/install-chrome-extension-not-in-the-store \n Usage \n Try searching for a single ( $TWTR ) or multiple ( $TWTR $MSFT $FB ) cashtags, using the syntax $SYMBOL. \n Development Screenshots \n Before \n Company is suggested, perhaps an account is shown and several recent tweets populate the timeline below. \n \n v1 \n Rough chart inserted nicely. \n \n v2 \n Beautiful chart comes in. \n \n v3 \n Multiple stock tickers. \n \n Technology \n #StockView uses  Highstocks  for visualization, and the data comes from the  Yahoo Stock API . ', 'brds \n A not-so-intelligent intelligent thing \n Install \n ~~~\npip install -r requirements.txt\n~~~ \n Run \n ~~~\npython decomp.py\n~~~', 'simple-chatroom \n Basic Python Chatroom', 'emrl \n emrl', 'Visual Illusions \n Can we teach a machine to categorise visual illusions and generate new ones?  \n Dataset \n \n https://github.com/robertmaxwilliams/optical-illusion-dataset \n https://www.moillusions.com/ \n http://viperlib.york.ac.uk/ \n https://twitter.com/AkiyoshiKitaoka \n http://www.psy.ritsumei.ac.jp/~akitaoka/saishin58e.html -- dating back to June 2002 (http://www.psy.ritsumei.ac.jp/~akitaoka/o1saishe.html) \n \n Details, Categories and Meta Data \n https://en.wikipedia.org/wiki/List_of_optical_illusions \n Related work \n Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images (https://arxiv.org/abs/1412.1897)', 'partitioning-groups \n An elegant way to partition groups. ', 'Talk Powerpoint Generator \n \n \n \n This program automatically generates PowerPoints about any topic.\nThese presentation slide decks can be used by improvisers for the improvisational comedy format  ""Improvised TED talk""  or  ""Powerpoint Karaoke"" .\nIn such games, the actors have to present an unseen presentation slide deck, but pretend to be an expert and explain  ""their""  slide show choices. \n Demo \n Ty out this generator on our online platform:  talkgenerator.com . \n Example \n \n Easy Install and Run \n Our program relies on certain APIs that require authentication in order to use it.\nCreate a file named  .env  (don\'t forget the period) in your project directory, and fill this with the correct API keys as described on our  wiki page about this . \n ```sh \n Make a new Python 3 virtual environment \n python3 -m venv venv; \n Activate the virtual environment \n source venv/bin/activate; \n Upgrade pip and install  requirements \n pip install --upgrade pip setuptools;\npython3 -m pip install -r requirements.txt; \n Download NLTK dependencies \n python run_nltk_download.py; \n Install the Talk Generator \n pip install -e .; \n Generate a 10 slide talk with topic peanuts \n talkgenerator --topic ""peanuts"" --num_slides 10\n``` \n Run arguments \n | Argument               | Description               |\n| ---------------------- | ------------------------- |\n|  topic  | The topic of the generator. This works best if it is a common, well-known noun. Use comma-separated words to generate a slide deck about multiple topics |\n|  slides  | The number of slides in the generated presentation ( default: 10 ) |\n|  schema  | The presentation schema to use when generating the presentation. Currently, only two modes are implemented, being  default  and  test  (for testing during development) |\n|  title  | Title of the presentation. Either  topic  or this one should to be set in order to generate a slide deck (just setting  topic  is usually more fun though)  |\n|  presenter  | The name that will be present on the first slide. Leave blank for an automatically generated name |\n|  output_folder  | The folder to output the generated presentations ( default:  ./output/ ) |\n|  save_ppt  | If this flag is true( default ), the generated powerpoint will be saved on the computer in the  output_folder |\n|  open_ppt  | If this flag is true ( default ), the generated powerpoint will automatically open after generating|\n|  parallel  | If this flag is true ( default ), the generator will generate all slides in parallel | \n Program structure \n See the  wiki  to know more about the inner implementation. \n Tests \n Test files are  tests/*.py , prefixed with  test_ . Test files use the  unittest  module.\nThey can easily be run all together when using PyCharm by right clicking on  talk-generator  and pressing  Run \'Unittests in talk-generator\' \n sh\ncoverage run -m pytest; coverage html \n Test coverage is automatically handled by  codecov . Tests are automatically run with CircleCI based on the  .yml  file in the  .circleci  directory. \n Credits \n This generator is made by\n Thomas Winters \nand  Kory Mathewson ,\nwith contributions from\n Shaun Farrugia \nand  Julian Faid . \n If you would like to refer to this project in academic work, please cite the following paper: \n Winters T., Mathewson K.W. (2019)  Automatically Generating Engaging Presentation Slide Decks . In: Ekárt A., Liapis A., Castro Pena M. (eds) Computational Intelligence in Music, Sound, Art and Design. EvoMUSART 2019. Lecture Notes in Computer Science, vol 11453. Springer, Cham \n sh\n@InProceedings{winters2019tedric,\n    author=""Winters, Thomas\n    and Mathewson, Kory W."",\n    editor=""Ek{\\\'a}rt, Anik{\\\'o}\n    and Liapis, Antonios\n    and Castro Pena, Mar{\\\'i}a Luz"",\n    title=""Automatically Generating Engaging Presentation Slide Decks"",\n    booktitle=""Computational Intelligence in Music, Sound, Art and Design"",\n    year=""2019"",\n    publisher=""Springer International Publishing"",\n    address=""Cham"",\n    pages=""127--141"",\n    isbn=""978-3-030-16667-0""\n} \n License \n MIT License. Copyright (c) 2018-2020  Kory Mathewson  and  Thomas Winters', 'jann \n \n \n \n Hi. I am  jann . I am a retrieval-based chatbot. I would make a great baseline. \n Allow me to (re)introduce myself \n I uses approximate nearest neighbor lookup using  Spotify\'s Annoy (Apache License 2.0)  library, over a distributed semantic embedding space ( Google\'s Universal Sentence Encoder (code: Apache License 2.0)  from  TensorFlow Hub . \n Objectives \n The goal of  jann  is to explicitly describes each step of the process of building a semantic similarity retrieval-based text chatbot. It is designed to be able to use diverse text source as input (e.g. Facebook messages, tweets, emails, movie lines, speeches, restaurant reviews, ...) so long as the data is collected in a single text file to be ready for processing. \n Install and configure requirements \n Note:  jann  development is tested with Python 3.8.6 on macOS 11.5.2 and Ubuntu 20.04. \n To run  jann  on your local system or a server, you will need to perform the following installation steps. \n ```sh \n OSX: Install homebrew \n /bin/bash -c ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)"" \n OSX: Install wget \n brew install wget \n Configure and activate virtual environment \n python3.8 -m venv venv\nsource venv/bin/activate \n python --version \n Ensure Python 3.8.10 \n Upgrade Pip \n pip install --upgrade pip setuptools \n Install requirements \n pip install -r requirements.txt \n Install Jann \n python setup.py install \n Set environmental variable for TensorFlow Hub \n export TFHUB_CACHE_DIR=Jann/data/module \n Make the TFHUB_CACHE_DIR \n mkdir -p ${TFHUB_CACHE_DIR} \n Download and unpack the Universal Sentence Encoder Lite model (~25 MB) \n wget ""https://tfhub.dev/google/universal-sentence-encoder-lite/2?tf-hub-format=compressed"" -O ${TFHUB_CACHE_DIR}/module_lite.tar.gz\ncd ${TFHUB_CACHE_DIR};\nmkdir -p universal-sentence-encoder-lite-2 && tar -zxvf module_lite.tar.gz -C universal-sentence-encoder-lite-2;\ncd -\n``` \n Download Cornell Movie Dialog Database \n Download the  Cornell Movie Dialog Corpus , and extract to  data/CMDC . \n ```sh \n Change directory to CMDC data subdirectory \n mkdir -p Jann/data/CMDC\ncd Jann/data/CMDC/ \n Download the corpus \n wget http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip \n Unzip the corpus and move lines and convos to the main directory \n unzip cornell_movie_dialogs_corpus.zip\nmv cornell\\ movie-dialogs\\ corpus/movie_lines.txt movie_lines.txt\nmv cornell\\ movie-dialogs\\ corpus/movie_conversations.txt movie_conversations.txt \n Change direcory to jann\'s main directory \n cd -\n``` \n As an example, we might use the first 50 lines of movie dialogue from the  Cornell Movie Dialog Corpus . \n You can set the number of lines from the corpus you want to use by changing the parameter  export NUMLINES=\'50\'  in  run_examples/run_CMDC.sh . \n Tests \n sh\npytest --cov-report=xml --cov-report=html --cov=Jann \n You should see all the tests passing. \n (simple) Run Basic Example \n ```sh\ncd Jann \n make sure that the run code is runnable \n chmod +x run_examples/run_CMDC.sh \n run it \n ./run_examples/run_CMDC.sh\n``` \n (advanced) Running Model Building \n jann  is composed of several submodules, each of which can be run in sequence as follows: \n ```sh \n Ensure that the virtual environment is activated \n source venv/bin/activate \n Change directory to Jann \n cd Jann \n Number of lines from input source to use \n export NUMTREES=\'100\' \n Number of neighbors to return \n export NUMNEIGHBORS=\'10\' \n Define the environmental variables \n export INFILE=""data/CMDC/all_lines_50.txt"" \n Embed the lines using the encoder (Universal Sentence Encoder) \n python embed_lines.py --infile=${INFILE} --verbose \n Process the embeddings and save as unique strings and numpy array \n python process_embeddings.py --infile=${INFILE} --verbose \n Index the embeddings using an approximate nearest neighbor (annoy) \n python index_embeddings.py --infile=${INFILE} --verbose --num_trees=${NUMTREES} \n Build a simple command line interaction for model testing \n python interact_with_model.py --infile=${INFILE} --verbose --num_neighbors=${NUMNEIGHBORS}\n``` \n Interaction \n For interaction with the model, the only files needed are the unique strings ( _unique_strings.csv ) and the Annoy index ( .ann ) file.  \n With the unique strings and the index file you can build a basic interaction.  \n This is demonstrated in the  interact_with_model.py  file. \n Pairs \n Conversational dialogue is composed of sequences of utterances. The sequence can be seen as pairs of utterances: inputs and responses. \n Nearest neighbours to a given input will find neighbours which are semantically related to the input. By storing input<>response pairs, rather than only inputs,  jann  can respond with a response to similar inputs. This example is shown in  run_examples/run_CMDC_pairs.sh . \n Run Web Server \n jann  is designed to run as a web service to be queried by a dialogue interface builder. For instance,  jann  is natively configured to be compatible with  Dialogflow Webhook Service . The web service runs using the Flask micro-framework and uses the performance-oriented  gunicorn  application server to launch the application with 4 workers. \n ```sh\ncd Jann \n run the pairs set up and test the interaction \n ./run_examples/run_CMDC_pairs.sh \n pairs set up will write files needed for web server deployment \n default data_key is all_lines_0 \n start development server \n python app.py \n or serve the pairs model with gunicorn and 4 workers \n gunicorn --bind 0.0.0.0:8000 app:JANN -w 4\n``` \n Monitoring \n It is helpful to see a Flask Monitoring dashboard to monitor statistics on the bot. There is a  Flask-MonitoringDashboard  which is already installed as part of Jann, see  Jann/app.py . \n To view the dashboard, navigate to  http://0.0.0.0:8000/dashboard . The default user/pass is:  admin  /  admin . \n Load / Lag Testing with Locust \n Once  jann  is running, in a new terminal window you can test the load on the server with  Locust , as defined in  Jann/tests/locustfile.py : \n sh\nsource venv/bin/activate\ncd Jann/tests\nlocust --host=http://0.0.0.0:8000 \n You can then navigate a web browser to  http://0.0.0.0:8089/ , and simulate  N  users spawning at  M  users per second and making requests to  jann . \n Testing the model by hand \n sh\ncurl --header ""Content-Type: application/json"" \\\n  --request POST \\\n  --data \'{""queryResult"": {""queryText"": ""that sounds really depressing""}}\' \\\n  http://0.0.0.0:8000/model_inference \n Response: \n sh\n{""fulfillmentText"":""Oh, come on, man. Tell me you wouldn\'t love it!""} \n Custom Datasets \n You can use any dataset you want! Format your source text with a single entry on each line, as follows: \n ```sh \n data/custom_data/example.txt \n This is the first line.\nThis is the second line, a response to the first line.\nThis is the third line.\nThis is the fourth line, a response to the third line.\n``` \n Using other Universal Sentence Encoder embedding modules \n There are  a collection of Universal Sentence Encoders  trained on a variety of data. \n Note from  TensorFlow Hub : The module performs best effort text input preprocessing, therefore it is not required to preprocess the data before applying the module. \n ```sh \n Standard Model (914 MB) \n wget \'https://tfhub.dev/google/universal-sentence-encoder/4?tf-hub-format=compressed\' -O module_standard.tar.gz\nmkdir -p universal-sentence-encoder && tar -zxvf module_standard.tar.gz -C universal-sentence-encoder\n``` \n Annoy parameters \n There are two parameters for the Approximate Nearest Neighbour: \n \n set  n_trees  as large as possible given the amount of memory you can afford, \n set  search_k  as large as possible given the time constraints you have for the queries. This parameter is a interaction tradeoff between accuracy and speed. \n \n Run details for GCP serving using nginx and uwsgi \n You will need to configure your server with the necessary software: \n ```sh\nsudo apt update\nsudo apt -y upgrade\nsudo apt install unzip python3-pip python3-dev python3-venv build-essential libssl-dev libffi-dev python3-setuptools\nsudo apt-get install nginx\ngit clone https://github.com/korymath/jann \n and follow the installation and configuration steps above \n sudo /etc/init.d/nginx start    # start nginx\n``` \n Then, you can reference a more in-depth guide  here . And here is a walkthrough on how to configure  nginx on GCP . \n You will need the uwsgi_params file, which is available in the nginx directory of the uWSGI distribution, or from  the nginx GitHub repository . \n ```sh \n uwsgi_param  QUERY_STRING       $query_string;\nuwsgi_param  REQUEST_METHOD     $request_method;\nuwsgi_param  CONTENT_TYPE       $content_type;\nuwsgi_param  CONTENT_LENGTH     $content_length; \n uwsgi_param  REQUEST_URI        $request_uri;\nuwsgi_param  PATH_INFO          $document_uri;\nuwsgi_param  DOCUMENT_ROOT      $document_root;\nuwsgi_param  SERVER_PROTOCOL    $server_protocol;\nuwsgi_param  REQUEST_SCHEME     $scheme;\nuwsgi_param  HTTPS              $https if_not_empty; \n uwsgi_param  REMOTE_ADDR        $remote_addr;\nuwsgi_param  REMOTE_PORT        $remote_port;\nuwsgi_param  SERVER_PORT        $server_port;\nuwsgi_param  SERVER_NAME        $server_name;\n``` \n Copy it into your project directory (e.g.  /home/${USER}/jann/uwsgi_params ).\nIn a moment we will tell nginx to refer to it. \n We will serve our application over HTTP on port 80, so we need to enable it: \n sh\nsudo ufw allow \'Nginx HTTP\' \n This will allow HTTP traffic on port 80, the default HTTP port. \n We can check the rule has been applied with: \n ```sh\nsudo ufw status \n Status: active \n To                         Action      From \n --                         ------      ---- \n Nginx HTTP                 ALLOW       Anywhere \n Nginx HTTP (v6)            ALLOW       Anywhere (v6) \n ``` \n Make a Systemd unit file: \n sh\n[Unit]\nDescription=JANN as a well served Flask application.\nAfter=network.target\n[Service]\nUser=korymath\nGroup=www-data\nWorkingDirectory=/home/korymath/jann/Jann\nEnvironment=""PATH=/home/korymath/jann/venv/bin""\nExecStart=/home/korymath/jann/venv/bin/uwsgi --ini wsgi.ini\n[Install]\nWantedBy=multi-user.target \n Then, copy the following into a file on your server, \nnamed:  /etc/nginx/sites-available/JANN.conf \n ```sh \n JANN.conf \n server {\n    listen      80;\n    server_name 35.209.230.155;\n    location / {\n        include     /home/korymath/jann/uwsgi_params;\n        uwsgi_pass unix:/home/korymath/jann/Jann/jann.sock;\n    }\n}\n``` \n Then, we tell nginx how to refer to the server \n ```sh \n link the site configuration to nginx enabled sites \n sudo ln -s /etc/nginx/sites-available/JANN.conf /etc/nginx/sites-enabled/ \n restart nginx \n sudo systemctl restart nginx \n restart jann \n sudo systemctl restart jann\n``` \n Common Errors/Warnings and Solutions \n sh\n/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module \'tensorflow.python.framework.fast_tensor_util\' does not match runtime version 3.6\n  return f(*args, **kwds) \n Solution (for OSX 10.13): \n sh\npip install --ignore-installed --upgrade https://github.com/lakshayg/tensorflow-build/releases/download/tf1.9.0-macos-py27-py36/tensorflow-1.9.0-cp36-cp36m-macosx_10_13_x86_64.whl \n FileNotFoundError \n sh\nFileNotFoundError: [Errno 2] No such file or directory: \'data/CMDC/movie_lines.txt\' \n Solution: \n sh\nEnsure that the input movie lines file is extracted to the correct path \n ValueError \n sh\nValueError: Signature \'spm_path\' is missing from meta graph. \n Solution \n Currently  jann  is configured to use the  universal-sentence-encoder-lite  module from TFHub as it is small, lightweight, and ready for rapid deployment. This module depends on the  SentencePiece  library and the SentencePiece model published with the module. \n You will need to make some minor code adjustments to use the heaviery modules (such as  universal-sentence-encoder \nand  universal-sentence-encoder-large . \n Start Contributing \n The guide for contributors can be found  here . It covers everything you need to know to start contributing to  jann . \n References \n \n Universal Sentence Encoder on TensorFlow Hub \n Cer, Daniel, et al. \'Universal sentence encoder.\' arXiv preprint arXiv:1803.11175 (2018). \n Danescu-Niculescu-Mizil, Cristian, and Lillian Lee. \'Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs.\' Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics. Association for Computational Linguistics, 2011. \n \n Credits \n jann  is made with love by  Kory Mathewson . \n Icon made by  Freepik  from  www.flaticon.com  is licensed by  CC 3.0 BY .', '\n dAIrector (🤖 + 📖) \n dAIrector is an automated director which collaborates with humans storytellers. \n Documentation \n Go to  https://korymath.github.io/dairector/ \n Set Up \n ```sh \n install homebrew \n /usr/bin/ruby -e ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"" \n install and upgrade portaudio, swig, git, python3 \n brew install --upgrade portaudio swig git python3 \n set up the python3 virtual environment \n virtualenv -p python3 env \n activate the virtual environment \n source env/bin/activate \n install requirements \n pip install -r requirements.txt \n in case of an error with pyaudio, may need to point to brew intall directly \n see https://stackoverflow.com/questions/33513522/when-installing-pyaudio-pip-cannot-find-portaudio-h-in-usr-local-include \n for more information \n pip install --global-option=\'build_ext\' --global-option=\'-I/Users/korymath/homebrew/Cellar/portaudio/19.6.0/include\' --global-option=\'-L/Users/korymath/homebrew/Cellar/portaudio/19.6.0/lib\' pyaudio \n get the trained model and example files \n wget https://storage.googleapis.com/api-project-941639660937.appspot.com/dairector_pretrained_examples.zip \n unpack the files \n unzip dairector_pretrained_examples.zip\n``` \n Run \n ```sh \n first ensure that your environment is activated \n source env/bin/activate \n example 1a, generate a new subgraph from the entire plotto conflict graph \n python markovgenerator.py -t outputfile.json plottoconflicts.json \n example 1b, interactive story telling using the plot subgraph and tv tropes hints \n python -W ignore storyteller.py outputfile.json tvtropes.json tvtropesmodel.bin plottomodel.bin\n``` \n Interactive Beat Generation \n The storyteller is interactive, it understands the following commands:\n* next [ cue_text ]\n* hint [ cue_text ]\n* quit \n next  uses the vector model from  plottomodel.bin  to find the next story beat based on the given cue text. \n hint  uses the  tvtropesmodel.bin  to find an appropriate trope. \n Install pocketsphinx \n sh\npip install pocketsphinx==0.1.15\npip install PyAudio==0.2.11 \n Basic Usage \n Improvisors on stage can cue the system to provide the next plot point or the next hint.\nThe improvisors provide the dialogue for each plot clause. \n Training a new model \n sh\npython topicvectors.py tvtropesmodel.bin tvtropes.json \n Cite \n sh\n@inproceedings{eger2018dairector,\n  author = {{Eger}, M. and {Mathewson}, K.~W.},\n  title = ""{dAIrector: Automatic Story Beat Generation through Knowledge Synthesis}"",\n  booktitle = {AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE18), Joint Workshop on Intelligent Narrative Technologies and Intelligent Cinematography and Editing},\n  publisher = {AAAI},\n  year = 2018,\n  address={Edmonton, Alberta, Canada},\n  month = 10,\n} \n License \n This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-sa/3.0/ or send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.', 'TEDRIC Analysis \n 1. Set up Environment \n sh\nvirtualenv -p python3 env\nsource env/bin/activate\npip install -r requirements.txt \n 2. Run \n ```sh\njupyter notebook \n then run the TEDRIC_Analysis file \n ```', '\n \n \n \n \n  PROJECT LOGO  \n \n \n \n \n \n aNAOmate \n \n    A web-based interface for controlling SoftBank\'s Aldebaran Nao (V5/V6) robots.\n     \n Explore the docs » \n \n \n Report Bug \n    ·\n     Request Feature \n \n  TABLE OF CONTENTS  \n Table of Contents \n \n About the Project \n Built With \n Getting Started \n Prerequisites \n Installation \n Usage \n Roadmap \n Contributing \n License \n Contact \n Acknowledgements \n \n  ABOUT THE PROJECT  \n About The Project \n aNAOmate  is an web-based interface for controlling SoftBank\'s Aldebaran Nao (V5/V6) robots. \n \n Slidedeck \n Built With \n \n NAOqi API \n Bootstrap \n \n  GETTING STARTED  \n Getting Started \n To get a local copy up and running follow these simple steps. \n Prerequisites \n Things you need to use the software and how to install them.\n* npm\n sh\nnpm install npm@latest -g \n Installation \n \n Clone the repo\n sh\ngit clone https:://github.com/QuinnyB/aNAOmate.git \n \n  USAGE EXAMPLES  \n Usage \n For examples, please refer to the  Documentation \n Editing TMI (Touch Move Interface for the Nao Robot) \n \n Make changes in the  TMI  directory \n Test the changes by following the quick testing instructions. \n Once satisfied, ensure that all necessary files are linked in  TMI/TMI.pml . \n Once links are complete, open the  TMI/TMI.pml  in Choregraphe \n File -> Build Application Package  and save to the  package  directory. \n Once the package is saved, install on the robot. (Note: NAOqi must be properly installed on your system for the installation script to work) \n \n sh\n/usr/bin/python package/install_pkg.py $ROBOT_IP $PACKAGE_FILE_NAME \n \n Once the package is installed, you will see:  Installation complete.  (This installs the application to  /home/nao/.local/share/PackageManager/apps/TMI  and runs the application if  autorun=""true""  in the  TMI/manifest \n The application interface is now running, and starts automatically by default. \n Navigate to  http://$ROBOT_IP/apps/TMI  to see developed the interface. \n This interface should match the tested changes in Step 2. \n \n Using the aNAOmate interface \n TODO \n Behaviours \n TODO \n Configure Development Environment \n TODO \n  ROADMAP  \n Roadmap \n See the  open issues  for a list of proposed features (and known issues). \n Authors \n \n Quinn Boser  -  Website \n Riley Dawson  -  Website \n Josh Whitney  -  Website \n Kory Mathewson  -  Website \n \n See the list of  contributors  who participated in this project. \n  CONTRIBUTING  \n Contributing \n Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are  greatly appreciated . \n \n Fork the Project \n Create your Feature Branch ( git checkout -b feature/AmazingFeature ) \n Commit your Changes ( git commit -m \'Add some AmazingFeature\' ) \n Push to the Branch ( git push origin feature/AmazingFeature ) \n Open a Pull Request \n \n  LICENSE  \n License \n Distributed under the MIT License. See  LICENSE  for more information. \n  CONTACT  \n Contact \n Josh Whitney -  @JoshJRWhitney  - joshjrwhitney@gmail.com \n Project Link:  https://github.com/QuinnyB/aNAOmate \n  ACKNOWLEDGEMENTS  \n Acknowledgements \n The authors of this project would like to graciously acknowledge to continued support of the community. \n  MARKDOWN LINKS & IMAGES  \n  https://www.markdownguide.org/basic-syntax/#reference-style-links ', '2019 Edmonton Fringe Festival Show Finder \n There is a  live demo . \n Credits \n Built by Kory Mathewson. Using show data from the Edmonton Fringe Festival Tickets  website . Based on  CSV to HTML Table  by Derek Eder.', 'Equal Groups K-Means clustering \n """"""Equal Groups K-Means clustering utlizing the scikit-learn api and related utilities."""""" \n From:\nhttps://github.com/ndanielsen/Same-Size-K-Means/blob/master/clustering/equal_groups.py \n And made to work with python3 \n """"""Equal Groups K-Means clustering\n90 percent of this is the Kmeans implmentations with the equal groups logic\nlocated in  _labels_inertia_precompute_dense()  which follows the steps laid\nout in the Elki Same-size k-Means Variation tutorial.\nhttps://elki-project.github.io/tutorial/same-size_k_means\nPlease note that this implementation only works in scikit-learn 17.X as later\nversions having breaking changes to this implementation.\nParameters \n \n n_groups : int, optional, default: 8\n    The number of clusters to form as well as the number of\n    centroids to generate.\nmax_iter : int, default: 300\n    Maximum number of iterations of the k-means algorithm for a\n    single run.\nn_init : int, default: 10\n    Number of time the k-means algorithm will be run with different\n    centroid seeds. The final results will be the best output of\n    n_init consecutive runs in terms of inertia.\ninit : {\'k-means++\', \'random\' or an ndarray}\n    Method for initialization, defaults to \'k-means++\':\n    \'k-means++\' : selects initial cluster centers for k-mean\n    clustering in a smart way to speed up convergence. See section\n    Notes in k_init for more details.\n    \'random\': choose k observations (rows) at random from data for\n    the initial centroids.\n    If an ndarray is passed, it should be of shape (n_groups, n_features)\n    and gives the initial centers.\nprecompute_distances : {\'auto\', True, False}\n    Precompute distances (faster but takes more memory).\n    \'auto\' : do not precompute distances if n_samples * n_groups > 12\n    million. This corresponds to about 100MB overhead per job using\n    double precision.\n    True : always precompute distances\n    False : never precompute distances\ntol : float, default: 1e-4\n    Relative tolerance with regards to inertia to declare convergence\nrandom_state : integer or numpy.RandomState, optional\n    The generator used to initialize the centers. If an integer is\n    given, it fixes the seed. Defaults to the global numpy random\n    number generator.\nverbose : int, default 0\n    Verbosity mode.\ncopy_x : boolean, default True\n    When pre-computing distances it is more numerically accurate to center\n    the data first.  If copy_x is True, then the original data is not\n    modified.  If False, the original data is modified, and put back before\n    the function returns, but small numerical differences may be introduced\n    by subtracting and then adding the data mean. \n Attributes \n cluster_centers_ : array, [n_groups, n_features]\n    Coordinates of cluster centers\nlabels_ :\n    Labels of each point\ninertia_ : float\n    Sum of distances of samples to their closest cluster center. \n Notes \n The k-means problem is solved using Lloyd\'s algorithm.\nThe average complexity is given by O(k n T), were n is the number of\nsamples and T is the number of iteration.\nThe worst case complexity is given by O(n^(k+2/p)) with\nn = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii,\n\'How slow is the k-means method?\' SoCG2006)\nIn practice, the k-means algorithm is very fast (one of the fastest\nclustering algorithms available), but it falls in local minima. That\'s why\nit can be useful to restart it several times.\nSee also \n \n MiniBatchKMeans:\n    Alternative online implementation that does incremental updates\n    of the centers positions using mini-batches.\n    For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n    probably much faster to than the default batch implementation.\n""""""', 'ParitBOT Dashboard', ""\n Hello, I'm Kory! \n \n \n I am a Research Scientist with  DeepMind  and a Lab Scientist with the  Creative Destruction Lab .  \n I have a Ph.D. in Computing Science from the  University of Alberta  with the  Alberta Machine Intelligence Institute .  \n My research focuses on understanding interaction between intelligent systems.  \n I am currently focusing on  🧑\u200d🦳-🤖 interfaces. \n You might also know me as an improvisational theatre performance artist with  Rapid Fire Theatre .  \n I like to fuse my interests by developing artificial intelligences to perform theatre alongside.  \n For more,  https://korymathewson.com/  and  Improbotics"", 'P5.EEGEdu \n P5.EEGEdu is an educational website to learn about coding live animations with electroencephalogram (EEG) data. It is a teaching tool that allows for students to quickly interact with their own brain waves.  \n Visit  https://p5.eegedu.com/  for the live p5 sandbox website. \n Installation for Development \n If you are interested in developing p5.EEGEdu, here are some instructions to get you started. \n Note: Currently p5.EEGEdu development requires a Mac OSX operating system.  \n To start, you will need to install  Homebrew  and  yarn . These are easy one-line installations for Mac users:  \n ```sh \n Install homebrew \n /usr/bin/ruby -e ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"" \n Install yarn \n NOTE: this will also install Node.js if it is not already installed. \n brew install yarn  \n Node.js must be version 10.x for Muse interaction \n Thus, if you are getting version issues, install n and switch versions \n sudo npm install -g n \n sudo n 10.16.0 \n ``` \n Then, in terminal, clone the git repo and enter the folder: \n sh\ngit clone https://github.com/kylemath/p5.EEGEdu\ncd p5.EEGEdu \n You then need to install the required packages for EEGEdu \n sh\nyarn install \n Local Development Environment \n Then, you can run the  Development Environment  of p5.EEGEdu: \n sh\nyarn start dev \n If it is working correctly, the p5.EEGEdu application will open in a browser window at http://localhost:3000. \n Local Production Environment \n To start the  Local Production Environment , you can use the following commands:  \n sh\nyarn run build\nserve -s build \n Deployment \n p5.EEGEdu  is running on  Firebase  and deployment happens automagically using GitHub post-commit hooks, or  Actions , as they are commonly called. You can see how the application is build and deployed by  inspecting the workflow . \n Contributing \n The guide for contributors can be found  here . It covers everything you need to know to start contributing to p5.EEGEdu. \n Development Roadmap \n References \n \n https://github.com/urish/muse-js - based toolbox for interacting with muse  \n https://github.com/NeuroJS/angular-muse - demo with streaming data in Angular, record button,  \n https://github.com/tanvach/muse-fft  - starting point react demo \n https://github.com/neurosity/eeg-pipes - easy pipable operations on eeg data from muse-js \n https://reactjs.org/  - React for web development \n https://www.chartjs.org/docs/latest/ - interactive charts \n https://github.com/urish/muse-lsl  - maybe useful to stream to LSL \n \n Credits \n p5.EEGEdu  - An Interactive Electrophysiology P5 Animation Coding Sandbox with the Interaxon Muse brought to you by Mathewson Sons \n License \n p5.EEGEdu is licensed under The MIT License (MIT)', 'iccc-ai-art', 'Just For Laughs \n \n Setup \n ```sh \n Get the code \n git clone https://github.com/korymath/justforlaughs;\ncd justforlaughs/;\npython3 -m venv venv;\nsource venv/bin/activate; \n Install requirements \n pip install --upgrade pip;\npip install -r requirements.txt;\n``` \n Run \n ```sh \n Run the web app \n python app.py; \n Run the standalone laughter detector \n python segment_laughter.py --input_audio_file example_audio.wav; \n It should output that it found 1 laugh in the example, save just the laugh cropped from the input, and the time window when laugh happened. \n Example: \n found 1 laughs. \n [{\'filename\': \'output/laugh_0.wav\', \'start\': 2.6453333333333333, \'end\': 5.261913043478261}] \n ``` \n Fixes \n For running on mac, need to ensure that the libsndfile is correclty pathed, ref: https://github.com/bastibe/python-soundfile/issues/310\n sh\nenv DYLD_LIBRARY_PATH=""/opt/homebrew/lib:$DYLD_LIBRARY_PATH"" python app.py \n Credits \n Client-sider in-browser detection from: \n \n https://github.com/tensorflow/tfjs-models/tree/master/speech-commands -  LICENSE \n \n Laughter detection model from: \n \n https://github.com/jrgillick/laughter-detection -  LICENSE \n \n Audio interface and recording adapted from: \n \n https://github.com/mattdiamond/Recorderjs -  LICENSE \n https://github.com/addpipe/simple-recorderjs-demo -  No LICENSE ,  Blog post \n', 'ELO Demo \n Setup and Install \n sh\npython3 -m venv venv;\nsource venv/bin/activate;\npip install --upgrade pip;\npip install -r requirements.txt; \n Firebase Datastore \n Ensure that  key.json  is present in root directory. For more information, see  the documentation  on deploying a similar application. \n Run \n sh\npython main.py']",['repository_count'],DeepMind,https://avatars.githubusercontent.com/u/178099?u=1d02f6838c74b227184844ff80372450b08b355a&v=4,True,2016-10-18 17:11:58,309,345,0,2010-01-07T18:23:04Z,2022-11-14T18:43:08Z,korymath,False,False,True,False,False,True,False,0,"{'MATLAB': 4, 'Jupyter Notebook': 7, 'Lua': 1, 'Python': 16, 'JavaScript': 5, 'HTML': 1, 'CSS': 1, 'TypeScript': 1}",5,"Montreal, Canada",AI for Education,40,0.38,0.25,0.61,0
nicholas-leonard,Nicholas Léonard,"TensorFlow, Python, Scala, Java, Lua/Torch, C, CUDA, deep learning, machine learning, data science",nileonard@expedia.com,"['nnx: experimental \'nn\' components \n The original neural network from Torch7,  nn , contains stable and widely\nused modules. \'nnx\' contains more experimental, unproven modules, and\noptimizations. Modules that become stable and which are proven useful make \ntheir way into \'nn\' (some already have). \n Library Documentation \n This section includes documentation for the following objects: \n \n SoftMaxTree  : a hierarchical log-softmax Module; \n TreeNLLCriterion  : a negative log-likelihood Criterion for the SoftMaxTree; \n CTCCriterion  : a Connectionist Temporal Classification Criterion based on  warp-ctc ; \n PushTable (and PullTable)  : extracts a table element and inserts it later in the network; \n MultiSoftMax  : performs a softmax over the last dimension of a 2D or 3D input; \n SpatialReSampling  : performs bilinear resampling of a 3D or 4D input image; \n [QDRiemaNNLinear] (#nnx.QDRiemaNNLinear) : quasi-diagonal reduction for Riemannian gradient descent \n Recurrent  : a generalized recurrent neural network container; \n \n \n SoftMaxTree \n A hierarchy of parameterized log-softmaxes. Used for computing the likelihood of a leaf class. \nThis Module should be used in conjunction with the  TreeNLLCriterion . \nUsing this for large vocabularies (100,000 and more) greatly accelerates training and evaluation \nof neural network language models (NNLM). \nA vocabulary hierarchy is provided via the  dp  package\'s\n BillionWords \n DataSource . \n The constructor takes 2 mandatory and 4 optional arguments : \n *  inputSize  : the number of units in the input embedding representation;\n *  hierarchy  : a Tensor mapping one  parent_id  to many  child_id  (a tree);\n *  rootId  : a number identifying the root node in the hierarchy. Defaults to  -1 ;\n *  accUpdate  : when the intent is to use  backwardUpdate  or  accUpdateGradParameters , set this to true to save memory. Defaults to false;\n *  static  : when true (the defualt), returns parameters with keys that don\'t change from batch to batch;\n *  verbose  : prints some additional information concerning the hierarchy during construction. \n The  forward  method returns an  output  Tensor of size 1D, while \n backward  returns a table  {gradInput, gradTarget} . The second \nvariable is just a Tensor of zeros , such that the  targets  can be \npropagated through  Containers  \nlike  ParallelTable . \n ```lua \n \n input = torch.randn(5,10)\ntarget = torch.IntTensor{20,24,27,10,12}\ngradOutput = torch.randn(5)\nroot_id = 29\ninput_size = 10  \nhierarchy = { \n \n [29]=torch.IntTensor{30,1,2}, [1]=torch.IntTensor{3,4,5}, \n   [2]=torch.IntTensor{6,7,8}, [3]=torch.IntTensor{9,10,11},\n   [4]=torch.IntTensor{12,13,14}, [5]=torch.IntTensor{15,16,17},\n   [6]=torch.IntTensor{18,19,20}, [7]=torch.IntTensor{21,22,23},\n   [8]=torch.IntTensor{24,25,26,27,28}\n}\nsmt = nn.SoftMaxTree(input_size, hierarchy, root_id)\nsmt:forward{input, target}\n-3.5186\n-3.8950\n-3.7433\n-3.3071\n-3.0522\n[torch.DoubleTensor of dimension 5]\nsmt:backward({input, target}, gradOutput)\n{\n  1 : DoubleTensor - size: 5x10\n  2 : IntTensor - size: 5\n} \n \n \n ``` \n \n TreeNLLCriterion \n Measures the Negative log-likelihood (NLL) for  SoftMaxTrees . \nUsed for maximizing the likelihood of SoftMaxTree outputs.\nThe SoftMaxTree Module outputs a column Tensor representing the log likelihood\nof each target in the batch. Thus SoftMaxTree requires the targets.\nSo this Criterion only computes the negative of those outputs, as \nwell as its corresponding gradients. \n \n \n PushTable (and PullTable) \n PushTable and PullTable work together. The first can be put earlier\nin a digraph of Modules such that it can communicate with a \nPullTable located later in the graph.  PushTable:forward(input)  \nfor an  input  table of Tensors to the output, excluding one, the index of which \nis specified by the  index  argument in the  PushTable(index)  constructor.\nThe Tensor identified by this  index  is communicated to one or many \nPullTables created via the  PushTable:pull(index)  factory method. \nThese can be inserted later in the digraph such that \na call to  PushTable:forward(input) , where  input  is a table or a Tensor, \nwill output a table with the previously  pushed  Tensor inserted \nat index  index . \n An example utilizing the above  SoftMaxTree  Module\nand a Linear Module demonstrates how the PushTable can be used to \nforward the  target  Tensor without any other \n Table Modules :\n```lua \n \n mlp = nn.Sequential()\nlinear = nn.Linear(50,100)\npush = nn.PushTable(2)\npull = push:pull(2)\nmlp:add(push)\nmlp:add(nn.SelectTable(1))\nmlp:add(linear)\nmlp:add(pull)\nmlp:add(smt) --smt is a SoftMaxTree instance\nmlp:forward{input, target} -- input and target are defined above\n-3.5186\n-3.8950\n-3.7433\n-3.3071\n-3.0522\n[torch.DoubleTensor of dimension 5]\nmlp:backward({input, target}, gradOutput) -- so is gradOutput\n{\n  1 : DoubleTensor - size: 5x10\n  2 : IntTensor - size: 5\n}\n The above code is equivalent to the following: lua\nmlp2 = nn.Sequential()\npara = nn.ParallelTable()\npara:add(linear)\npara:add(nn.Identity())\nmlp2:add(para)\nmlp2:add(smt)\nmlp2:forward{input, target}\n-3.5186\n-3.8950\n-3.7433\n-3.3071\n-3.0522\n[torch.DoubleTensor of dimension 5]\nmlp2:backward({input, target}, gradOutput)\n{\n  1 : DoubleTensor - size: 5x10\n  2 : IntTensor - size: 5\n}\n```\nIn some cases, this can simplify the digraph of Modules. Note that \na PushTable can be associated to many PullTables, but each PullTable \nis associated to only one PushTable. \n \n \n CTCCriterion \n criterion = nn.CTCCriterion() \nCreates a Criterion based on Baidus\'  warp-ctc  implementation.\nThis Module measures the loss between a 3D output of (batch x time x inputdim) and a target without needing alignment of inputs and labels.\nMust have installed warp-ctc which can be installed via luarocks:\n luarocks install http://raw.githubusercontent.com/baidu-research/warp-ctc/master/torch_binding/rocks/warp-ctc-scm-1.rockspec \nSupports cuda via:\n criterion = nn.CTCCriterion():cuda() \nExample:\n```\noutput = torch.Tensor({{{1,2,3,4,5},{6,7,8,9,10}}}) -- Tensor of size 1x1x5 (batch x time x inputdim).\nlabel = {{1,3}}\nsizes = torch.Tensor({2}) -- Size of each sequence (sequence-length) in the batch as a tensor\nctcCriterion = nn.CTCCriterion() \n err = ctcCriterion:forward(output,label,sizes)\ngradOut = ctcCriterion:backward(output,label)\nprint(""----CPU----"")\nprint(""Error : "" .. err)\nprint(""Gradients :"")\nprint(gradOut) \n ctcCriterion = ctcCriterion:cuda() -- Switch to cuda implementation.\noutput = output:cuda() \n err = ctcCriterion:forward(output,label,sizes)\ngradOut = ctcCriterion:backward(output,label)\nprint(""----GPU----"")\nprint(""Error : "" .. err)\nprint(""Gradients :"")\nprint(gradOut)\n``` \n gives the output:\n```\n----CPU---- \nError : 4.9038286209106 \nGradients : \n(1,.,.) = \n  0.0117 -0.9683  0.0861  0.2341  0.6364\n  0.0117  0.0317  0.0861 -0.7659  0.6364\n[torch.FloatTensor of size 1x2x5] \n ----GPU---- \nError : 4.9038290977478 \nGradients : \n(1,.,.) = \n  0.0117 -0.9683  0.0861  0.2341  0.6364\n  0.0117  0.0317  0.0861 -0.7659  0.6364\n[torch.CudaTensor of size 1x2x5]\n```\n \n MultiSoftMax \n This Module takes 2D or 3D input and performs a softmax over the last dimension. \nIt uses the existing  SoftMax  \nCUDA/C code to do so such that the Module can be used on both GPU and CPU. \nThis can be useful for  keypoint detection . \n \n SpatialReSampling \n Applies a 2D re-sampling over an input image composed of\nseveral input planes (or channels, colors). The input tensor in  forward(input)  is \nexpected to be a 3D or 4D tensor of size :  [batchSize x] nInputPlane x width x height . \nThe number of output planes will be the same as the number of input\nplanes. \n The re-sampling is done using  bilinear interpolation . \nFor a simple nearest-neihbor upsampling, use  nn.SpatialUpSampling() ,\nand for a simple average-based down-sampling, use \n nn.SpatialDownSampling() . \n If the input image is a 3D tensor of size  nInputPlane x height x width ,\nthe output image size will be  nInputPlane x oheight x owidth  where\n owidth  and  oheight  are given to the constructor. \n Instead of  owidth  and  oheight , one can provide  rwidth  and  rheight , \nsuch that  owidth = iwidth*rwidth  and  oheight = iheight*rheight . \n As an example, we can run the following code on the famous Lenna image:\n lua\nrequire \'image\'                                                           \nrequire \'nnx\'\ninput = image.loadPNG(\'doc/image/Lenna.png\')\nl = nn.SpatialReSampling{owidth=150,oheight=150}\noutput = l:forward(input)\nimage.save(\'doc/image/Lenna-150x150-bilinear.png\', output) \n The input: \n   \n The re-sampled output: \n   \n \n QDRiemaNNLinear \n The Quasi-Diagonal Riemannian Neural Network Linear (QDRiemaNNLinear) module is an implementation\nof the quasi-diagonal reduction of metrics, used for Riemannian gradient descent.\nThe algorithm is defined in Riemannian metrics for neural networks I: feedforward networks by Yann Ollivier (http://arxiv.org/abs/1303.0818) and an efficient implementation is described in Practical Riemannian Neural Networks by Yann Ollivier and Gaetan Marceau-Caron (http://arxiv.org/abs/1602.08007).\nTo use this module, simply replace  nn.Linear(ninput,noutput)  with  nnx.QDRiemaNNLinear(ninput,noutput) .\nAs always, the step-size must be chosen accordingly.\nTwo additional arguments are also possible:\n* gamma (default=0.01): determine the update rate of the metric for a minibatch setting, i.e., (1-gamma) * oldMetric + gamma newMetric. Smaller minibatches require a smaller gamma. A default value depending on the size of the minibatches is  gamma = 1. - torch.pow(1.-1./nTraining,miniBatchSize)  where  nTraining  is the number of training examples of the dataset and  miniBatchSize  is the number of training examples per minibatch. \n* qdFlag (default=true): Whether to use the quasi-diagonal reduction (true) or only the diagonal (false). The former should be better. \n This module is a straightforward implementation of the outer product gradient descent. \n Requirements \n \n Torch7 (www.torch.ch) \n \n Installation \n \n Install Torch7 (refer to its own documentation). \n clone this project into dev directory of Torch7. \n Rebuild torch, it will include new projects too. \n \n Use the library \n First run torch, and load nnx: \n sh\n$ torch   \n ``` lua \n \n require \'nnx\'\n``` \n \n Once loaded, tab-completion will help you navigate through the\nlibrary (note that most function are added directly to nn): \n ``` lua \n \n nnx. + TAB\n...\nnn. + TAB\n``` \n \n In particular, it\'s good to verify that all modules provided pass their\ntests: \n ``` lua \n \n nnx.test_all()\nnnx.test_omp()\n``` \n \n \n Recurrent \n DEPRECATED July 6th, 2015. Use  rnn  instead.', 'IFT6266 : Emotion Recognition \n This model integrates with my pylear2 fork:\nhttps://github.com/nicholas-leonard/pylearn2 \n Transfered by experimental log on github wiki: \nhttps://github.com/nicholas-leonard/ift6266/wiki', ""\n Torch-7 for Android \n \n Torch7 provides a Matlab-like environment for state-of-the-art machine\nlearning algorithms. It is easy to use and provides a very efficient\nimplementation, thanks to an easy and fast scripting language (Lua) and a\nunderlying C implementation. \n Modified to be compiled and used with Android \n Features \n \n Loading of lua packages from the apk directly. \n This is done by writing a custom package.loader\n  Reference: http://www.lua.org/manual/5.1/manual.html#pdf-package.loaders\n  The loader is in torchandroid.cpp as loader_android \n torchandroid.h and torchandroid.cpp give lots of helper functions to make life easier \n Print function overriden to redirect to logcat (only handles strings for now) \n Function to get apk assets as bytes (very useful) \n \n Requirements \n Android NDK and Android SDK \n Samples \n \n A sample project has been provided in android-demo \n android-demo/jni/torchdemo.cpp is a simple use-case \n android-demo/assets/main.lua is the file that is run \n Vinayak Ghokale from e-lab Purdue (https://github.com/e-lab) contributed a face detector demo, which showcases a fuller use-case. \n That's in the facedetector_e-lab folder. I made some changes to it to load assets etc. from apk as opposed to the sdcard, but it remains untouched otherwise. \n \n Building Torch \n \n open build.sh and modify ANDROID_NDK to your android ndk path. \n run build script\n$ sh build.sh \n \n You can use torch in your android apps. The relevant directories are\n* include - include directories\n* lib - static libs cross-compiled for armeabi-v7a\n* share - lua files \n Building Example \n \n Build Torch atleast once using the steps above. \n [Optional] Connect your android phone in debugging mode,\n              to automatically install the apk. \n Change directory into android-demo folder. \n Run build script.\n$ sh build.sh \n Run the app TorchDemo on your phone. \n"", 'common \n Repository used in common by my other projects \n Dependencies \n libpqtypes : \n    http://libpqtypes.esilo.com/\n    requires a manual linking on ubuntu\n    user version 1.5.0\nboost: \n    http://www.boost.org/\n    http://www.boost.org/doc/libs/1_51_0/libs/serialization/doc/index.html\n    mostly for serialization, i.e. archives\npostgresql:\n    http://www.postgresql.org/\n    need a working database server 8.4+ and dev header files\ngeneralhashfunctions: \n    http://www.partow.net/programming/hashfunctions/   \n    included in repository for convenience\n    use his Makefile', 'ift6085 \n Mixture of Clusters', 'delicious \n Experiments on the delicious.com dataset', 'hps \n Hyper Parameter Search: a framework for coding, training and testing pylearn2 models in PostgreSQL.', 'hypermind \n A distributed neural network framework for the brave', 'equanimity \n A distributed conditional computing experiment using neural \ndecision trees.', 'dp Package Reference Manual \n \n dp  is a  d ee p  learning library designed for streamlining \nresearch and development using the  Torch7  distribution. \nIt emphasizes flexibility through the elegant use of object-oriented \n design patterns . \n Documentation \n This package includes lots of documentations and tutorials which you \nwill find hosted on  readthedocs .\nIf you prefer, you can consult the docs using  github .', 'cunnx \n Experimental cuda nn package', 'cub \n Allows for installing CUB via luarocks', 'mydp \n A dummy repository to use as a starting point for you own tests, classes, scripts and experiments extending dp.', 'torchx \n This package contains various torch extensions:\n *  concat  : concatenates a table of tensors.\n *  find  : finds all indices of a given value.\n *  group  : sorts and groups similar tensor variables together. \n *  remap  : recursively applies a function to tables of Tensors.\n *  md5  : used for hashing strings. \n And some  paths  extensions :\n *  indexdir  : index a directory of millions of files for faster listing. \n \n [res] torch.concat([res], tensors, [dim]) \n Concatenates a table of Tensors along dimension  dim .\n *  res  is a tensor holding the concatenation of Tensors  tensor .\n *  tensors  is a table of tensors. Each tensor should have the same amount of dimensions and the same size for non- dim  dimensions.\n *  dim  is the dimension along which the tensors will be concatenated. Defaults to 1. \n Example:\n```lua \n \n res = torch.concat({torch.rand(2,3),torch.randn(2,1),torch.randn(2,2)},2)\nprint(res)\n 0.8621  0.7776  0.3284 -1.2884 -0.4939  0.6049\n 0.8404  0.8996  0.5704  0.3911 -0.0428 -1.4627\n[torch.DoubleTensor of dimension 2x6]\n``` \n \n \n [res] torch.find(tensor, val, [dim]) \n Finds all indices of a given value  val  in Tensor  tensor . \nReturns a table of these indices by traversing the tensor one row \nat a time. When  dim=2 , the only valid value for dim other than  nil  (the default),\nthe function expects a matrix and returns the row-wise indices of each found \nvalue  val  in the row. \n 1D example:\n```lua \n \n res = torch.find(torch.Tensor{1,2,3,1,1,2}, 1)\nunpack(res)\n1  4  5\n``` \n \n 2D example:\n``` \n \n tensor = torch.Tensor{{1,2,3,4,5},{5,6,0.6,0,2}}\nunpack(torch.find(tensor, 2))\n2   10 \nunpack(torch.find(tensor:t(), 2))\n3   10 \nunpack(torch.find(tensor, 2, 2))\n{2}  {5}\nunpack(torch.find(tensor:t(), 2, 2))\n{ }  {1}  { }  { }  {2}\n``` \n \n \n [res, val, idx] torch.group([val, idx], tensor, [samegrp, desc]) \n Sorts and groups similar tensor variables together.\n *  res  is a table of  {idx=torch.LongTensor,val=torch.Tensor} .\n *  val  is a Tensor of the same type as  tensor . It will be used to store and return the sorted values.\n *  idx  is a  torch.LongTensor  used to store the sorted indices.\n *  tensor  is a Tensor that will have its values sorted, and then grouped by the  samegrp  function.\n *  samegrp  is a function taking two argument :  first_val  is the first value of the current group, while  val  is the current value of the current group. When the function returns true, it is assumed that  val  is of the same group as  first_val . Defaults to  function(first_val, val) return first_val == val; end \n *  desc  is a boolean indicating whether the  tensor  gets sorted in descending order. Defaults to false. \n Example:\n```lua \n \n tensor = torch.Tensor{5,3,4,5,3,5}\nres, val, idx = torch.group(tensor)\nres\n{\n  3 : \n    {\n      idx : LongTensor - size: 2\n      val : DoubleTensor - size: 2\n    }\n  4 : \n    {\n      idx : LongTensor - size: 1\n      val : DoubleTensor - size: 1\n    }\n  5 : \n    {\n      idx : LongTensor - size: 3\n      val : DoubleTensor - size: 3\n    }\n}\n``` \n \n \n [t1, t2] torch.remap(t1, t2, f(x,y) [p1, p2]) \n Recursively applies function  f(x,y)  [to tables [of tables,...] of] Tensors\n t1  and  t2 . When prototypes  p1  or  p2  are provided, they are used \nto initialized any missing Tensors in  t1  or  t2 . \n Example:\n```lua \n \n t1 = {torch.randn(3,4), {torch.randn(3,4), torch.randn(2,4), {torch.randn(1)}}}\nt2 = {torch.randn(3,4), {torch.randn(3,4), torch.randn(2,4), {torch.randn(1)}}}\ntorch.remap(t1, t2, function(x, y) x:add(y) end)\n{\n  1 : DoubleTensor - size: 3x4\n  2 : \n    {\n      1 : DoubleTensor - size: 3x4\n      2 : DoubleTensor - size: 2x4\n      3 : \n        {\n          1 : DoubleTensor - size: 1\n        }\n    }\n}\n{\n  1 : DoubleTensor - size: 3x4\n  2 : \n    {\n      1 : DoubleTensor - size: 3x4\n      2 : DoubleTensor - size: 2x4\n      3 : \n        {\n          1 : DoubleTensor - size: 1\n        }\n    }\n}\n It also creates missing tensors: lua\nt2, t3 = torch.remap(t2, nil, function(x, y) y:resizeAs(x):copy(x) end)\nprint(t3)\n{\n  1 : DoubleTensor - size: 3x4\n  2 : \n    {\n      1 : DoubleTensor - size: 3x4\n      2 : DoubleTensor - size: 2x4\n      3 : \n        {\n          1 : DoubleTensor - size: 1\n        }\n    }\n}\n When in doubt, first tensor has priority: lua\nt4, t2 = torch.remap({torch.DoubleTensor()}, t2, function(x, y) x:resize(y:size()):copy(y) end, torch.LongTensor())\nprint(t4)\n{\n  1 : DoubleTensor - size: 3x4\n}\nt2, t5 = torch.remap(t2, {torch.DoubleTensor()}, function(x, y) y:resize(x:size()):copy(x) end, torch.LongTensor())\nprint(t5)\n{\n  1 : DoubleTensor - size: 3x4\n  2 : \n    {\n      1 : LongTensor - size: 3x4\n      2 : LongTensor - size: 2x4\n      3 : \n        {\n          1 : LongTensor - size: 1\n        }\n    }\n}\n``` \n \n \n torch.md5 \n Pure Lua module copy-pasted from  this repo  (for some reasons I can\'t get \ngit submodule to work with luarocks). The module includes two functions:\n lua\nlocal md5_as_hex   = torch.md5.sumhexa(message)   -- returns a hex string\nlocal md5_as_data  = torch.md5.sum(message)     -- returns raw bytes \nThe  torch.md5.sumhexa  function takes a string and returns another string:\n lua\ntorch.md5.sumhexa(\'helloworld!\')\n420e57b017066b44e05ea1577f6e2e12 \n \n [obj] paths.indexdir(path, [ext, use_cache, ignore]) \n lua\nfiles = paths.indexdir(""/path/to/files/"", \'png\', true)\nimages = {}\nfor i=1,files:size() do\n   local img = image.load(files:filename(i))\n   table.insert(images, img)\nend \n This function can be used to create an object indexing all files having \nextensions  ext  (a string or a list thereof) in directory  path  (string or list thereof). \nUseful for directories containing many thousands of files. The function \ncaches the resulting list to disk in  /tmp  such that it can be used \nfor later calls when  use_cache=true  (default is false). \nArgument  ignore  species a pattern to ignore (e.g. "" frame "" will ignore all files containing  ""frame"" ).', 'Torch Manual \n Torch has lots of libraries and corresponding reference documentation. \nBut this is where you can try to make some sense out of all of it. \n The Basics \n \n Introduction \n Getting Started \n Torch Ecosystem \n Programming in Lua \n Tensors \n Object-Oriented Programming \n Foreign Function Interface \n \n Neural Network Training \n \n Datasets \n Building your own dataset  \n Modules \n Writing your own torch module  \n Criterions \n Optimization \n \n Library Reference \n \n Torch  : tensors, class factory, serialization, BLAS ; \n nn  : neural network Modules and Criterions; \n optim  : SGD, LBFGS and other optimization functions ; \n gnuplot  : ploting and data visualization ; \n paths  : make directories, concatenate file paths, and other filesystem utilities ; \n image  : save, load, crop, scale, warp, translate images and such ; \n trepl  : the torch LuaJIT interpreter ; \n cwrap  : used for wrapping C/CUDA functions in Lua ; \n', 'dpnn : deep extensions to nn \n This package provides many useful features that aren\'t part of the main nn package. \nThese include  sharedClone , which allows you to clone a module and share \nparameters or gradParameters with the original module, without incuring any memory overhead.\nWe also redefined  type  such that the type-cast preserves Tensor sharing within a structure of modules.  \n The package provides the following Modules: \n \n Decorator  : abstract class to change the behaviour of an encapsulated module ; \n DontCast  : prevent encapsulated module from being casted by  Module:type()  ; \n Serial  : decorate a module makes its serialized output more compact ;  \n NaN  : decorate a module to detect the source of NaN errors ; \n Inception  : implements the Inception module of the GoogleLeNet article ; \n Collapse  : just like  nn.View(-1) ; \n Convert  : convert between different tensor types or shapes; \n ZipTable  : zip a table of tables into a table of tables; \n ZipTableOneToMany  : zip a table of element  el  and table of elements into a table of pairs of element  el  and table elements; \n CAddTensorTable  : adds a tensor to a table of tensors of the same size; \n ReverseTable  : reverse the order of elements in a table; \n PrintSize  : prints the size of inputs and gradOutputs (useful for debugging); \n Clip  : clips the inputs to a min and max value; \n Constant  : outputs a constant value given an input (which is ignored); \n SpatialUniformCrop  : uniformly crops patches from a input; \n SpatialGlimpse  : takes a fovead glimpse of an image at a given location; \n WhiteNoise  : adds isotropic Gaussian noise to the signal when in training mode; \n OneHot  : transforms a tensor of indices into  one-hot  encoding; \n Kmeans  :  Kmeans  clustering layer. Forward computes distances with respect to centroids and returns index of closest centroid. Centroids can be updated using gradient descent. Centroids could be initialized randomly or by using  kmeans++  algoirthm; \n SpatialRegionDropout  : Randomly dropouts a region (top, bottom, leftmost, rightmost) of the input image. Works with batch and any number of channels; \n FireModule  : FireModule as mentioned in the  SqueezeNet ; \n NCEModule  : optimized placeholder for a  Linear  +  SoftMax  using  noise-contrastive estimation . \n SpatialFeatNormalization  : Module for widely used preprocessing step of mean zeroing and standardization for images. \n SpatialBinaryConvolution  : Module for binary spatial convolution (Binary weights) as mentioned in  XNOR-Net . \n SimpleColorTransform  : Module for adding independent random noise to input image channels. \n PCAColorTransform  : Module for adding noise to input image using Principal Components Analysis. \n \n The following modules and criterions can be used to implement the REINFORCE algorithm : \n \n Reinforce  : abstract class for REINFORCE modules; \n ReinforceBernoulli  : samples from Bernoulli distribution; \n ReinforceNormal  : samples from Normal distribution; \n ReinforceGamma  : samples from Gamma distribution; \n ReinforceCategorical  : samples from Categorical (Multinomial with one sample) distribution; \n VRClassReward  : criterion for variance-reduced classification-based reward; \n BinaryClassReward  : criterion for variance-reduced binary classification reward (like  VRClassReward , but for binary classes); \n \n Additional differentiable criterions\n *  BinaryLogisticRegression  : criterion for binary logistic regression;\n *  SpatialBinaryLogisticRegression  : criterion for pixel wise binary logistic regression;\n *  NCECriterion  : criterion exclusively used with  NCEModule .\n *  ModuleCriterion  : adds an optional  inputModule  and  targetModule  before a decorated criterion;\n *  BinaryLogisticRegression  : criterion for binary logistic regression.\n *  SpatialBinaryLogisticRegression  : criterion for pixel wise binary logistic regression. \n A lot of the functionality implemented here was pulled from \n dp , which makes heavy use of this package. \nHowever, dpnn can be used without dp (for e.g. you can use it with optim), \nwhich is one of the main reasons why we made it. \n Tutorials \n Sagar Waghmare  wrote a nice  tutorial \non how to use dpnn with nngraph to reproduce the \n Lateral Connections in Denoising Autoencoders Support Supervised Learning .  \n A brief (1 hours) overview of Torch7, which includes some details about  dpnn , \nis available via this  NVIDIA GTC Webinar video . In any case, this presentation gives a nice overview of Logistic Regression, Multi-Layer Perceptrons, Convolutional Neural Networks and Recurrent Neural Networks using Torch7. \n \n Module \n The Module interface has been further extended with methods that facilitate \nstochastic gradient descent like  updateGradParameters  (i.e. momentum learning), \n weightDecay ,  maxParamNorm  (for regularization), and so on. \n \n Module.dpnn_parameters \n A table that specifies the name of parameter attributes. \nDefaults to  {\'weight\', \'bias\'} , which is a static variable (i.e. table exists in class namespace). \nSub-classes can define their own table statically.  \n \n Module.dpnn_gradParameters \n A table that specifies the name of gradient w.r.t. parameter attributes. \nDefaults to  {\'gradWeight\', \'gradBias\'} , which is a static variable (i.e. table exists in class namespace). \nSub-classes can define their own table statically.  \n \n [self] Module:type(type_str) \n This function converts all the parameters of a module to the given  type_str . \nThe  type_str  can be one of the types defined for  torch.Tensor \nlike  torch.DoubleTensor ,  torch.FloatTensor  and  torch.CudaTensor . \nUnlike the  type method \ndefined in  nn , this one was overriden to \nmaintain the sharing of  storage \namong Tensors. This is especially useful when cloning modules share  parameters  and  gradParameters . \n \n [clone] Module:sharedClone([shareParams, shareGradParams]) \n Similar to  clone .\nYet when  shareParams = true  (the default), the cloned module will share the parameters \nwith the original module. \nFurthermore, when  shareGradParams = true  (the default), the clone module will share \nthe gradients w.r.t. parameters with the original module.\nThis is equivalent to :\n lua\nclone = mlp:clone()\nclone:share(mlp, \'weight\', \'bias\', \'gradWeight\', \'gradBias\') \nyet it is much more efficient, especially for modules with lots of parameters, as these \nTensors aren\'t needlessly copied during the  clone .\nThis is particularly useful for  Recurrent neural networks  \nwhich require efficient copies with shared parameters and gradient w.r.t. parameters for each time-step. \n \n Module:maxParamNorm([maxOutNorm, maxInNorm]) \n This method implements a hard constraint on the upper bound of the norm of output and/or input neuron weights \n (Hinton et al. 2012, p. 2)  .\nIn a weight matrix, this is a contraint on rows ( maxOutNorm ) and/or columns ( maxInNorm ), respectively. \nHas a regularization effect analogous to  weightDecay , but with easier to optimize hyper-parameters. \nAssumes that parameters are arranged ( output dim x ... x input dim ). \nOnly affects parameters with more than one dimension.\nThe method should normally be called after  updateParameters . \nIt uses the C/CUDA optimized  torch.renorm  function.\nHint :  maxOutNorm = 2  usually does the trick.  \n \n [momGradParams] Module:momentumGradParameters() \n Returns a table of Tensors ( momGradParams ). For each element in the \ntable, a corresponding parameter ( params ) and gradient w.r.t. parameters \n( gradParams ) is returned by a call to  parameters .\nThis method is used internally by  updateGradParameters . \n \n Module:updateGradParameters(momFactor [, momDamp, momNesterov]) \n Applies classic momentum or Nesterov momentum  (Sutskever, Martens et al, 2013)  to parameter gradients. \nEach parameter Tensor ( params ) has a corresponding Tensor of the same size for gradients w.r.t. parameters ( gradParams ).\nWhen using momentum learning, another Tensor is added for each parameter Tensor ( momGradParams ).\nThis method should be called before  updateParameters \nas it affects the gradients w.r.t. parameters. \n Classic momentum is computed as follows : \n lua\nmomGradParams = momFactor*momGradParams + (1-momDamp)*gradParams\ngradParams = momGradParams   \n where  momDamp  has a default value of  momFactor . \n Nesterov momentum ( momNesterov = true ) is computed as follows (the first line is the same as classic momentum): \n lua\nmomGradParams = momFactor*momGradParams + (1-momDamp)*gradParams\ngradParams = gradParams + momFactor*momGradParams  \nThe default is to use classic momentum ( momNesterov = false ). \n \n Module:weightDecay(wdFactor [, wdMinDim]) \n Decays the weight of the parameterized models. \nImplements an L2 norm loss on parameters with dimensions greater or equal to  wdMinDim  (default is 2).\nThe resulting gradients are stored into the corresponding gradients w.r.t. parameters.\nSuch that this method should be called before  updateParameters . \n \n Module:gradParamClip(cutoffNorm [, moduleLocal]) \n Implements a contrainst on the norm of gradients w.r.t. parameters  (Pascanu et al. 2012) .\nWhen  moduleLocal = false  (the default), the norm is calculated globally to Module for which this is called.\nSo if you call it on an MLP, the norm is computed on the concatenation of all parameter Tensors.\nWhen  moduleLocal = true , the norm constraint is applied \nto the norm of all parameters in each component (non-container) module.\nThis method is useful to prevent the exploding gradient in \n Recurrent neural networks . \n \n Module:reinforce(reward) \n This method is used by Criterions that implement the REINFORCE algorithm like  VRClassReward . \nWhile vanilla backpropagation (gradient descent using the chain rule), \nREINFORCE Criterions broadcast a  reward  to all REINFORCE modules between the  forward  and the  backward .\nIn this way, when the following call to  backward  reaches the REINFORCE modules, \nthese will compute a  gradInput  using the broadcasted  reward .\nThe  reward  is broadcast to all REINFORCE modules contained \nwithin  model  by calling  model:reinforce(reward) . \nNote that the  reward  should be a 1D tensor of size  batchSize , \ni.e. each example in a batch has its own scalar reward. \n Refer to  this example \nfor a complete training script making use of the REINFORCE interface. \n \n Decorator \n lua\ndmodule = nn.Decorator(module) \n This module is an abstract class used to decorate a  module . This means \nthat method calls to  dmodule  will call the same method on the encapsulated \n module , and return its results. \n \n DontCast \n lua\ndmodule = nn.DontCast(module) \n This module is a decorator. Use it to decorate a module that you don\'t\nwant to be cast when the  type()  method is called. \n lua\nmodule = nn.DontCast(nn.Linear(3,4):float())\nmodule:double()\nth> print(module:forward(torch.FloatTensor{1,2,3}))\n 1.0927\n-1.9380\n-1.8158\n-0.0805\n[torch.FloatTensor of size 4]   \n \n Serial \n lua\ndmodule = nn.Serial(module, [tensortype])\ndmodule:[light,medium,heavy]Serial()   \n This module is a decorator that can be used to control the serialization/deserialization \nbehavior of the encapsulated module. Basically, making the resulting string or \nfile heavy (the default), medium or light in terms of size.  \n Furthermore, when specified, the  tensortype  attribute (e.g  torch.FloatTensor ,  torch.DoubleTensor  and so on.),\ndetermines what type the module will be cast to during serialization. \nNote that this will also be the type of the deserialized object.\nThe default serialization  tensortype  is  nil , i.e. the module is serialized as is.  \n The  heavySerial()  has the serialization process serialize every attribute in the module graph, \nwhich is the default behavior of nn.  \n The  mediumSerial()  has the serialization process serialize \neverything except the attributes specified in each module\'s  dpnn_mediumEmpty \ntable, which has a default value of  {\'output\', \'gradInput\', \'momGradParams\', \'dpnn_input\'} .\nDuring serialization, whether they be tables or Tensors, these attributes are emptied (no storage).\nSome modules overwrite the default  Module.dpnn_mediumEmpty  static attribute with their own. \n The  lightSerial()  has the serialization process empty \neverything a call to  mediumSerial(type)  would (so it uses  dpnn_mediumEmpty ).\nBut also empties all the parameter gradients specified by the \nattribute  dpnn_gradParameters , which defaults to  {gradWeight, gradBias} . \n We recomment using  mediumSerial()  for training, and  lightSerial()  for \nproduction (feed-forward-only models). \n \n NaN \n lua\ndmodule = nn.NaN(module, [id])   \n The  NaN  module asserts that the  output  and  gradInput  of the decorated  module  do not contain NaNs.\nThis is useful for locating the source of those pesky NaN errors. \nThe  id  defaults to automatically incremented values of  1,2,3,... . \n For example : \n lua\nlinear = nn.Linear(3,4)\nmlp = nn.Sequential()\nmlp:add(nn.NaN(nn.Identity()))\nmlp:add(nn.NaN(linear))\nmlp:add(nn.NaN(nn.Linear(4,2)))\nprint(mlp)   \n As you can see the  NaN  layers are have unique ids : \n lua\nnn.Sequential {\n  [input -> (1) -> (2) -> (3) -> output]\n  (1): nn.NaN(1) @ nn.Identity\n  (2): nn.NaN(2) @ nn.Linear(3 -> 4)\n  (3): nn.NaN(3) @ nn.Linear(4 -> 2)\n}   \n And if we fill the  bias  of the linear module with NaNs and call  forward : \n lua\nnan = math.log(math.log(0)) -- this is a nan value\nlinear.bias:fill(nan)\nmlp:forward(torch.randn(2,3))   \n We get a nice error message:\n lua\n/usr/local/share/lua/5.1/dpnn/NaN.lua:39: NaN found in parameters of module :\nnn.NaN(2) @ nn.Linear(3 -> 4)   \n For a quick one-liner to catch NaNs anywhere inside a model (for example, a  nn.Sequential  or any other  nn.Container ), we can use this with the  nn.Module.replace  function:\n lua\nmodel:replace(function(module) return nn.NaN(module) end) \n \n Inception \n References : \n \n A.  Going Deeper with Convolutions \n B.  GoogleLeNet \n \n lua\nmodule = nn.Inception(config)   \n This module uses  n +2 parallel ""columns"". \nThe original paper uses 2+2 where the first two are (but there could be more than two): \n \n 1x1 conv (reduce) -> relu -> 5x5 conv -> relu \n 1x1 conv (reduce) -> relu -> 3x3 conv -> relu  \n \n and where the other two are :  \n \n 3x3 maxpool -> 1x1 conv (reduce/project) -> relu  \n 1x1 conv (reduce) -> relu.  \n \n This module allows the first group of columns to be of any \nnumber while the last group consist of exactly two columns.\nThe 1x1 convoluations are used to reduce the number of input channels \n(or filters) such that the capacity of the network doesn\'t explode. \nWe refer to these here has  reduce . \nSince each column seems to have one and only one reduce, their initial \nconfiguration options are specified in lists of n+2 elements. \n The sole argument  config  is a table taking the following key-values : \n \n Required Arguments : \n inputSize  : number of input channels or colors, e.g. 3; \n outputSize  : numbers of filters in the non-1x1 convolution kernel sizes, e.g.  {32,48} \n reduceSize  : numbers of filters in the 1x1 convolutions (reduction) used in each column, e.g.  {48,64,32,32} . The last 2 are used respectively for the max pooling (projection) column (the last column in the paper) and the column that has nothing but a 1x1 conv (the first column in the paper). This table should have two elements more than the outputSize \n Optional Arguments : \n reduceStride  : strides of the 1x1 (reduction) convolutions. Defaults to  {1,1,...} . \n transfer  : transfer function like  nn.Tanh , nn.Sigmoid ,  nn.ReLU ,  nn.Identity , etc. It is used after each reduction (1x1 convolution) and convolution. Defaults to  nn.ReLU . \n batchNorm  : set this to  true  to use batch normalization. Defaults to  false . Note that batch normalization can be awesome \n padding  : set this to  true  to add padding to the input of the convolutions such that output width and height are same as that of the original non-padded  input . Defaults to  true . \n kernelSize  : size ( height = width ) of the non-1x1 convolution kernels. Defaults to  {5,3} . \n kernelStride  : stride of the kernels ( height = width ) of the convolution. Defaults to  {1,1} \n poolSize : size ( height = width ) of the spatial max pooling used in the next-to-last column. Defaults to 3. \n poolStride  : stride ( height = width ) of the spatial max pooling. Defaults to 1. \n \n For a complete example using this module, refer to the following :\n *  deep inception training script  ;\n *  openface facial recognition  (the model definition is  here ). \n \n Collapse \n lua\nmodule = nn.Collapse(nInputDim)   \n This module is the equivalent of:\n view = nn.View(-1)\nview:setNumInputDim(nInputDim)  \nIt collapses all non-batch dimensions. This is useful for converting \na spatial feature map to the single dimension required by a dense \nhidden layer like Linear. \n \n Convert \n lua\nmodule = nn.Convert([inputShape, outputShape])  \nModule to convert between different data formats.\nFor example, we can flatten images by using :\n lua\nmodule = nn.Convert(\'bchw\', \'bf\')  \nor equivalently\n lua\nmodule = nn.Convert(\'chw\', \'f\')  \nLets try it with an input:\n lua\nprint(module:forward(torch.randn(3,2,3,1)))\n 0.5692 -0.0190  0.5243  0.7530  0.4230  1.2483\n-0.9142  0.6013  0.5608 -1.0417 -1.4014  1.0177\n-1.5207 -0.1641 -0.4166  1.4810 -1.1725 -1.0037\n[torch.DoubleTensor of size 3x6]  \nYou could also try: \n ```lua\nmodule = nn.Convert(\'chw\', \'hwc\')\ninput = torch.randn(1,2,3,2)\ninput:select(2,1):fill(1)\ninput:select(2,2):fill(2)\nprint(input)\n(1,1,.,.) = \n  1  1\n  1  1\n  1  1\n(1,2,.,.) = \n  2  2\n  2  2\n  2  2\n[torch.DoubleTensor of size 1x2x3x2]\nprint(module:forward(input))\n(1,1,.,.) = \n  1  2\n  1  2 \n (1,2,.,.) = \n  1  2\n  1  2 \n (1,3,.,.) = \n  1  2\n  1  2\n[torch.DoubleTensor of size 1x3x2x2]\n```  \n Furthermore, it automatically converts the  input  to have the same type as  self.output \n(i.e. the type of the module).\nSo you can also just use is for automatic input type converions:\n lua\nmodule = nn.Convert()\nprint(module.output) -- type of module\n[torch.DoubleTensor with no dimension]\ninput = torch.FloatTensor{1,2,3}\nprint(module:forward(input))\n 1\n 2\n 3\n[torch.DoubleTensor of size 3] \n \n ZipTable \n lua\nmodule = nn.ZipTable() \n Zips a table of tables into a table of tables. \n Example:\n lua\nprint(module:forward{ {\'a1\',\'a2\'}, {\'b1\',\'b2\'}, {\'c1\',\'c2\'} })\n{ {\'a1\',\'b1\',\'c1\'}, {\'a2\',\'b2\',\'c2\'} } \n \n ZipTableOneToMany \n lua\nmodule = nn.ZipTableOneToMany() \n Zips a table of element  el  and table of elements  tab  into a table of tables, where the i-th table contains the element  el  and the i-th element in table  tab \n Example:\n lua\nprint(module:forward{ \'el\', {\'a\',\'b\',\'c\'} })\n{ {\'el\',\'a\'}, {\'el\',\'b\'}, {\'el\',\'c\'} } \n \n CAddTensorTable \n lua\nmodule = nn.CAddTensorTable() \n Adds the first element  el  of the input table  tab  to each tensor contained in the second element of  tab , which is itself a table \n Example:\n lua\nprint(module:forward{ (0,1,1), {(0,0,0),(1,1,1)} })\n{ (0,1,1), (1,2,2) } \n \n ReverseTable \n lua\nmodule = nn.ReverseTable() \n Reverses the order of elements in a table. \n Example: \n lua\nprint(module:forward{1,2,3,4})\n{4,3,2,1} \n \n PrintSize \n lua\nmodule = nn.PrintSize(name) \n This module is useful for debugging complicated module composites. \nIt prints the size of the  input  and  gradOutput  during  forward \nand  backward  propagation respectively.\nThe  name  is a string used to identify the module along side the printed size. \n \n Clip \n lua\nmodule = nn.Clip(minval, maxval) \n This module clips  input  values such that the output is between  minval  and  maxval . \n \n Constant \n lua\nmodule = nn.Constant(value, nInputDim) \n This module outputs a constant value given an input.\nIf  nInputDim  is specified, it uses the input to determine the size of the batch. \nThe  value  is then replicated over the batch. \nOtherwise, the  value  Tensor is output as is.\nDuring  backward , the returned  gradInput  is a zero Tensor of the same size as the  input .\nThis module has no trainable parameters.  \n You can use this with nn.ConcatTable() to append constant inputs to an input :  \n lua\nnn.ConcatTable():add(nn.Constant(v)):add(nn.Identity()) \n This is useful when you want to output a value that is independent of the \ninput to the neural network (see  this example ). \n \n SpatialUniformCrop \n lua\nmodule = nn.SpatialUniformCrop(oheight, owidth) \n During training, this module will output a cropped patch of size  oheight, owidth \nwithin the boundaries of the  input  image.\nFor each example, a location is sampled from a uniform distribution \nsuch that each possible patch has an equal probability of being sampled. \n During evaluation, the center patch is cropped and output. \n This module is commonly used at the input layer to artificially \naugment the size of the dataset to prevent overfitting. \n \n SpatialGlimpse \n Ref. A.  Recurrent Model for Visual Attention \n lua\nmodule = nn.SpatialGlimpse(size, depth, scale) \n A glimpse is the concatenation of down-scaled cropped images of \nincreasing scale around a given location in a given image.\nThe input is a pair of Tensors:  {image, location} \n location  are  (y,x)  coordinates of the center of the different scales \nof patches to be cropped from image  image . \nCoordinates are between  (-1,-1)  (top-left) and  (1,1)  (bottom-right).\nThe  output  is a batch of glimpses taken in image at location  (y,x) . \n size  can be either a scalar which specifies the  width = height  of glimpses, \nor a table of  {height, width}  to support a rectangular shape of glimpses.\n depth  is number of patches to crop per glimpse (one patch per depth).\n scale  determines the  size(t) = scale * size(t-1)  of successive cropped patches. \n So basically, this module can be used to focus the attention of the model \non a region of the input  image . \nIt is commonly used with the  RecurrentAttention  \nmodule (see  this example ). \n \n WhiteNoise \n lua\nmodule = nn.WhiteNoise([mean, stdev]) \n Useful in training [Denoising Autoencoders] (http://arxiv.org/pdf/1507.02672v1.pdf). \nTakes  mean  and  stdev  of the normal distribution as input. \nDefault values for mean and standard deviation are 0 and 0.1 respectively. \nWith  module:training() , noise is added during forward. \nDuring  backward  gradients are passed as it is. \nWith  module:evaluate()  the mean is added to the input. \n \n SpatialRegionDropout \n lua\nmodule = nn.SpatialRegionDropout(p) \nFollowing is an example of  SpatialRegionDropout  outputs on the famous lena image. \n Input \n \n Outputs \n   \n \n FireModule \n Ref: http://arxiv.org/pdf/1602.07360v1.pdf\n lua\nmodule = nn.FireModule(nInputPlane, s1x1, e1x1, e3x3, activation) \nFireModule is comprised of two submodules 1) A  squeeze  convolution module comprised of  1x1  filters followed by 2) an  expand  module that is comprised of a mix of  1x1  and  3x3  convolution filters.\nArguments:  s1x1 : number of  1x1  filters in the squeeze submodule,  e1x1 : number of  1x1  filters in the expand submodule,  e3x3 : number of  3x3  filters in the expand submodule. It is recommended that  s1x1  be less than  (e1x1+e3x3)  if you want to limit the number of input channels to the  3x3  filters in the expand submodule.\nFireModule works only with batches, for single sample convert the sample to a batch of size 1. \n \n SpatialFeatNormalization \n lua\nmodule = nn.SpatialFeatNormalization(mean, std) \nThis module normalizies each feature channel of input image based on its corresponding mean and standard deviation scalar values. This module does not learn the  mean  and  std , they are provided as arguments. \n \n SpatialBinaryConvolution \n lua\nmodule = nn.SpatialBinaryConvolution(nInputPlane, nOutputPlane, kW, kH) \nFunctioning of SpatialBinaryConvolution is similar to nn/SpatialConvolution. Only difference is that Binary weights are used for forward/backward and floating point weights are used for weight updates. Check  Binary-Weight-Network  section of  XNOR-net . \n \n SimpleColorTransform \n lua\nrange = torch.rand(inputChannels) -- Typically range is specified by user.\nmodule = nn.SimpleColorTransform(inputChannels, range) \nThis module performs a simple data augmentation technique. SimpleColorTransform module adds random noise to each color channel independently. In more advanced data augmentation technique noise is added using principal components of color channels. For that please check  PCAColorTransform \n \n PCAColorTransform \n lua\neigenVectors = torch.rand(inputChannels, inputChannels) -- Eigen Vectors\neigenValues = torch.rand(inputChannels) -- Eigen\nstd = 0.1 -- Std deviation of normal distribution with mean zero for noise.\nmodule = nn.PCAColorTransform(inputChannels, eigenVectors, eigenValues, std) \nThis module performs a data augmentation using Principal Component analysis of pixel values. When in training mode, mulitples of principal components are added to input image pixels. Magnitude of value added (noise) is dependent upon the corresponding eigen value and a random value sampled from a Gaussian distribution with mean zero and  std  (default 0.1) standard deviation. This technique was used in the famous  AlexNet  paper. \n \n OneHot \n lua\nmodule = nn.OneHot(outputSize) \n Transforms a tensor of  input  indices having integer values between 1 and  outputSize  into\na tensor of one-hot vectors of size  outputSize .  \n Forward an index to get a one-hot vector : \n ```lua \n \n module = nn.OneHot(5) -- 5 classes\nmodule:forward(torch.LongTensor{3})\n 0  0  1  0  0\n[torch.DoubleTensor of size 1x5]\n```  \n \n Forward a batch of 3 indices. Notice that these need not be stored as  torch.LongTensor  : \n ```lua \n \n module:forward(torch.Tensor{3,2,1})\n 0  0  1  0  0\n 0  1  0  0  0\n 1  0  0  0  0\n[torch.DoubleTensor of size 3x5]\n```  \n \n Forward batch of  2 x 3  indices : \n ```lua\noh:forward(torch.Tensor{{3,2,1},{1,2,3}})\n(1,.,.) = \n  0  0  1  0  0\n  0  1  0  0  0\n  1  0  0  0  0 \n (2,.,.) = \n  1  0  0  0  0\n  0  1  0  0  0\n  0  0  1  0  0\n[torch.DoubleTensor of size 2x3x5]\n```  \n \n Kmeans \n lua\nkm = nn.Kmeans(k, dim) \n k  is the number of centroids and  dim  is the dimensionality of samples.\nYou can either initialize centroids randomly from input samples or by using  kmeans++  algorithm. \n lua\nkm:initRandom(samples) -- Randomly initialize centroids from input samples.\nkm:initKmeansPlus(samples) -- Use Kmeans++ to initialize centroids.   \n Example showing how to use Kmeans module to do standard Kmeans clustering. \n ```lua\nattempts = 10\niter = 100 -- Number of iterations\nbestKm = nil\nbestLoss = math.huge\nlearningRate = 1\nfor j=1, attempts do\n   local km = nn.Kmeans(k, dim)\n   km:initKmeansPlus(samples)\n   for i=1, iter do\n      km:zeroGradParameters()\n      km:forward(samples) -- sets km.loss\n      km:backward(samples, gradOutput) -- gradOutput is ignored \n   -- Gradient Descent weight/centroids update\n  km:updateParameters(learningRate)\n \n end \n if km.loss < bestLoss then\n      bestLoss = km.loss\n      bestKm = km:clone()\n   end\nend\n `` nn.Kmeans()` module maintains loss only for the latest forward. If you want to maintain loss over the whole dataset then you who would need do it my adding the module loss for every forward. \n You can also use  nn.Kmeans()  as an auxillary layer in your network. \nA call to  forward  will generate an  output  containing the index of the nearest cluster for each sample in the batch.\nThe  gradInput  generated by  updateGradInput  will be zero.  \n \n ModuleCriterion \n lua\ncriterion = nn.ModuleCriterion(criterion [, inputModule, targetModule, castTarget])   \n This criterion decorates a  criterion  by allowing the  input  and  target  to be \nfed through an optional  inputModule  and  targetModule  before being passed to the \n criterion . The  inputModule  must not contain parameters as these would not be updated.  \n When  castTarget = true  (the default), the  targetModule  is cast along with the  inputModule  and \n criterion . Otherwise, the  targetModule  isn\'t.   \n \n NCEModule \n Ref. A  RNNLM training with NCE for Speech Recognition \n lua\nncem = nn.NCEModule(inputSize, outputSize, k, unigrams, [Z])   \n When used in conjunction with  NCECriterion , \nthe  NCEModule  implements  noise-contrastive estimation . \n The point of the NCE is to speedup computation for large  Linear  +  SoftMax  layers.\nComputing a forward/backward for  Linear(inputSize, outputSize)  for a large  outputSize  can be very expensive.\nThis is common when implementing language models having with large vocabularies of a million words.\nIn such cases, NCE can be an efficient alternative to computing the full  Linear  +  SoftMax  during training and \ncross-validation. \n The  inputSize  and  outputSize  are the same as for the  Linear  module. \nThe number of noise samples to be drawn per example is  k . A value of 25 should work well. \nIncreasing it will yield better results, while a smaller value will be more efficient to process.\nThe  unigrams  is a tensor of size  outputSize  that contains the frequencies or probability distribution over classes.\nIt is used to sample noise samples via a fast implementation of  torch.multinomial .\nThe  Z  is the normalization constant of the approximated SoftMax. \nThe default is  math.exp(9)  as specified in Ref. A. \n For inference, or measuring perplexity, the full  Linear  +  SoftMax  will need to \nbe computed. The  NCEModule  can do this by switching on the following : \n lua\nncem:evaluate()\nncem.normalized = true   \n Furthermore, to simulate  Linear  +  LogSoftMax  instead, one need only add the following to the above: \n lua\nncem.logsoftmax = true   \n An example is provided via the rnn package. \n \n NCECriterion \n lua\nncec = nn.NCECriterion()   \n This criterion only works with an  NCEModule  on the output layer.\nTogether, they implement  noise-contrastive estimation . \n \n Reinforce \n Ref A.  Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning \n Abstract class for modules that implement the REINFORCE algorithm (ref. A). \n lua\nmodule = nn.Reinforce([stochastic]) \n The  reinforce(reward)  method is called by a special Reward Criterion (e.g.  VRClassReward ).\nAfter which, when backward is called, the reward will be used to generate gradInputs. \nWhen  stochastic=true , the module is stochastic (i.e. samples from a distribution) \nduring evaluation and training.\nWhen  stochastic=false  (the default), the module is only stochastic during training. \n The REINFORCE rule for a module can be summarized as follows :\n lua\n            d ln(f(output,input))\ngradInput = ---------------------  * reward\n                  d input \nwhere the  reward  is what is provided by a Reward criterion like \n VRClassReward  via the  reinforce  method.\nThe criterion will normally be responsible for the following formula :\n lua\nreward = a*(R - b) \nwhere  a  is the alpha of the original paper, i.e. a reward scale,\n R  is the raw reward (usually 0 or 1), and  b  is the baseline reward, \nwhich is often taken to be the expected raw reward  R . \n The  output  is usually sampled from a probability distribution  f() \nparameterized by the  input . \nSee  ReinforceBernoulli  for a concrete derivation. \n Also, as you can see, the gradOutput is ignored. So within a backpropagation graph,\nthe  Reinforce  modules will replace the backpropagated gradients ( gradOutput ) \nwith their own obtained from the broadcasted  reward . \n \n ReinforceBernoulli \n Ref A.  Simple Statistical Gradient-Following Algorithms for\nConnectionist Reinforcement Learning \n lua\nmodule = nn.ReinforceBernoulli([stochastic]) \n A  Reinforce  subclass that implements the REINFORCE algorithm \n(ref. A p.230-236) for the Bernoulli probability distribution.\nInputs are bernoulli probabilities  p . \nDuring training, outputs are samples drawn from this distribution.\nDuring evaluation, when  stochastic=false , outputs are the same as the inputs.\nUses the REINFORCE algorithm (ref. A p.230-236) which is \nimplemented through the  reinforce  interface ( gradOutputs  are ignored). \n Given the following variables :  \n \n f  : bernoulli probability mass function \n x  : the sampled values (0 or 1) (i.e.  self.output ) \n p  : probability of sampling a 1 \n \n the derivative of the log bernoulli w.r.t. probability  p  is :\n d ln(f(output,input))   d ln(f(x,p))    (x - p)\n--------------------- = ------------ = ---------\n      d input               d p         p(1 - p) \n \n ReinforceNormal \n Ref A.  Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning \n lua\nmodule = nn.ReinforceNormal(stdev, [stochastic]) \n A  Reinforce  subclass that implements the REINFORCE algorithm \n(ref. A p.238-239) for a Normal (i.e. Gaussian) probability distribution.\nInputs are the means of the normal distribution.\nThe  stdev  argument specifies the standard deviation of the distribution. \nDuring training, outputs are samples drawn from this distribution.\nDuring evaluation, when  stochastic=false , outputs are the same as the inputs, i.e. the means.\nUses the REINFORCE algorithm (ref. A p.238-239) which is \nimplemented through the  reinforce  interface ( gradOutputs  are ignored). \n Given the following variables :  \n \n f  : normal probability density function \n x  : the sampled values (i.e.  self.output ) \n u  : mean ( input ) \n s  : standard deviation ( self.stdev ) \n \n the derivative of log normal w.r.t. mean  u  is :\n d ln(f(x,u,s))   (x - u)\n-------------- = -------\n     d u           s^2 \n As an example, it is used to sample locations for the  RecurrentAttention  \nmodule (see  this example ). \n \n ReinforceGamma \n Ref A.  Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning \n lua\nmodule = nn.ReinforceGamma(scale, [stochastic])   \n A  Reinforce  subclass that implements the REINFORCE algorithm \n(ref. A) for a  Gamma probability distribution  \nparametrized by shape (k) and scale (theta) variables.\nInputs are the shapes of the gamma distribution.\nDuring training, outputs are samples drawn from this distribution.\nDuring evaluation, when  stochastic=false , outputs are equal to the mean, defined as the product of\nshape and scale ie.  k*theta .\nUses the REINFORCE algorithm (ref. A) which is \nimplemented through the  reinforce  interface ( gradOutputs  are ignored). \n Given the following variables :  \n \n f  : gamma probability density function \n g  : digamma function \n x  : the sampled values (i.e.  self.output ) \n k  : shape ( input ) \n t  : scale \n \n the derivative of log gamma w.r.t. shape  k  is :\n d ln(f(x,k,t))\n-------------- = ln(x) - g(k) - ln(t)\n      d k   \n \n ReinforceCategorical \n Ref A.  Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning \n lua\nmodule = nn.ReinforceCategorical([stochastic]) \n A  Reinforce  subclass that implements the REINFORCE algorithm \n(ref. A) for a Categorical (i.e. Multinomial with one sample) probability distribution.\nInputs are the categorical probabilities of the distribution :  p[1], p[2], ..., p[k] .\nThese are usually the output of a SoftMax. \nFor  n  categories, both the  input  and  output  ares of size  batchSize x n .\nDuring training, outputs are samples drawn from this distribution.\nThe outputs are returned in one-hot encoding i.e. \nthe output for each example has exactly one category having a 1, while the remainder are zero.\nDuring evaluation, when  stochastic=false , outputs are the same as the inputs, i.e. the probabilities  p .\nUses the REINFORCE algorithm (ref. A) which is \nimplemented through the  reinforce  interface ( gradOutputs  are ignored). \n Given the following variables :  \n \n f  : categorical probability mass function \n x  : the sampled indices (one per sample) ( self.output  is the one-hot encoding of these indices) \n p  : probability vector ( p[1], p[2], ..., p[k] ) ( input ) \n \n the derivative of log categorical w.r.t. probability vector  p  is :\n d ln(f(x,p))     1/p[i]    if i = x  \n------------ =   \n    d p          0         otherwise   \n \n VRClassReward \n Ref A.  Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning \n This Reward criterion implements the REINFORCE algoritm (ref. A) for classification models.\nSpecifically, it is a Variance Reduces (VR) classification reinforcement leanring (reward-based) criterion. \n lua\nvcr = nn.VRClassReward(module [, scale, criterion])   \n While it conforms to the Criterion interface (which it inherits), \nit does not backpropagate gradients (except for the baseline  b ; see below).\nInstead, a  reward  is broadcast to the  module  via the  reinforce  method. \n The criterion implements the following formula :\n lua\nreward = a*(R - b) \nwhere  a  is the alpha described in Ref. A, i.e. a reward  scale  (defaults to 1),\n R  is the raw reward (0 for incorrect and 1 for correct classification), \nand  b  is the baseline reward, which is often taken to be the expected raw reward  R . \n The  target  of the criterion is a tensor of class indices.\nThe  input  to the criterion is a table  {y,b}  where  y  is the probability \n(or log-probability) of classes (usually the output of a SoftMax), \nand  b  is the baseline reward discussed above.  \n For each example, if  argmax(y)  is equal to the  target  class, the raw reward  R = 1 , otherwize  R = 0 . \n As for  b , its  gradInputs  are obtained from the  criterion , which defaults to  MSECriterion .\nThe  criterion \'s target is the commensurate raw reward  R .\nUsing  a*(R-b)  instead of  a*R  to obtain a  reward  is what makes this class variance reduced (VR).\nBy reducing the variance, the training can converge faster (Ref. A).\nThe predicted  b  can be nothing more than the expectation  E(R) . \n Note : for RNNs with R = 1 for last step in sequence, encapsulate it\nin  nn.ModuleCriterion(VRClassReward, nn.SelectTable(-1)) . \n For an example, this criterion is used along with the  RecurrentAttention  \nmodule to  train a recurrent model for visual attention . \n \n BinaryClassReward \n lua\nbcr = nn.BinaryClassReward(module [, scale, criterion])   \n This module implements  VRClassReward  for binary classification problems.\nSo basically, the  input  is still a table of two tensors. \nThe first input tensor is of size  batchsize  containing Bernoulli probabilities.\nThe second input tensor is the baseline prediction described in  VRClassReward .\nThe targets contain zeros and ones. \n \n BinaryLogisticRegression \n Ref A.  Learning to Segment Object Candidates \nThis criterion implements the score criterion mentioned in (ref. A). \n lua\ncriterion = nn.BinaryLogisticRegression()   \n BinaryLogisticRegression implements following cost function for binary classification. \n ``` \n log( 1 + exp( -y_k * score(x_k) ) ) \n `` \nwhere y_k is binary target score(x_k) is the corresponding prediction. y_k has value {-1, +1} and score(x_k) has value in [-1, +1]`. \n \n SpatialBinaryLogisticRegression \n Ref A.  Learning to Segment Object Candidates \n This criterion implements the spatial component of the criterion mentioned in  (ref. A). \n lua\ncriterion = nn.SpatialBinaryLogisticRegression()   \n SpatialBinaryLogisticRegression implements following cost function for binary pixel classification.\n 1\n_______ sum_ij [ log( 1 + exp( -m_ij * f_ij ) ) ]\n 2*w*h  \nwhere  m_ij  is target binary image and  f_ij  is the corresponding prediction.  m_ij  has value  {-1, +1}  and  f_ij  has value in  [-1, +1] .', 'hpo \n Hyper-parameter optimization', 'slides \n \n Boston ML \n NVIDIA GTC \n AI Camp  ( pptx ) : 12 July 2016, United Nations HQ, New York, 15min, Overview of Torch and Language Models. \n', 'rnn-examples \n Examples for the RNN library / framework', 'DrMAD \n To provide an efficient and easy-to-use hyperparameter tuning toolbox for Torch deep learning ecosystems. \n It combines Bayesian optimization (BO) and automatic differentiation (AD). For the Bayesian optimization module,\nwe will extend on  hypero ; the automatic differentiation part is based on\nDrMAD method, https://arxiv.org/abs/1601.00917. \n It is the only tool that can tune thousands of continuous hyperparameters (e.g. L2 penalties for each neuron or\nlearning rates for each layer) with a reasonable time/computational budget -- reads: outside Google. \n Current Status \n Skechy code for tuning L2 penalties and learning rates on MNIST dataset with CUDA support. \n TODO \n \n API for tuning learning rates, weight decay and momentum.  \n Experiments on ImageNet \n \n Dependencies \n \n Twitter Torch Autograd : the next version will not depend on this.  \n \n How to run \n \n drmad_mnist.lua  is for tuning L2 penalties on MNIST.  \n cuda_drmad_mnist.lua  is for tuning L2 penalties on MNIST with CUDA.  \n lr_drmad_mnist.lua  is for tuning learning rates and L2 penalties on MNIST.   \n \n Tricks \n Rally with ( Net2Net ) \n ImageNet dataset usually needs ~450,000 iterations. DrMAD may not approxiate this long trajectory well.  \n One approach would be to repeatedly initialize the weights using Net2Net, from small subsets to larget subsets\nand finally to the full dataset. \n Acceleration with Model Compression \n We will add a regression loss at every layer, which is also used in  Deep Q-Networks for Accelerating the Training of Deep Neural Networks . However, the aim here is not to compress the model, so we do not decrease the number of parameters.  \n BO and AD \n BO is a global optimization method (it can handle 20 hyperparameters at most), whereas AD can only find local solutions\n(it can handle thousands of hyperparameters because it uses gradient information). We first use BO to get some initial\naverage hyperparameters. Then we use AD method to further search for diverse local hyperparameters. \n Contact \n If you have any problems or suggestions, please contact me: jie.fu A~_~T u.nus.edu~~cation~~', 'This package is a basic Reinforcement Learning package written in LUA for Torch. It implements some simple environments and learning policies (Policy Gradient and Deep Q Learning). It also can be easily used with the OpenAI Gym package by using lutorpy (example given in the opeanaigym directory). \n Tutorials are provided in the tutorials directory \n Dependencies \n Lua: \n* Torch7\n* nn, dpnn\n* logroll, json, alewrap \n For using openAI Gym:\n* openai gym\n* lutorpy \n Installation \n \n In the torch directory: luarocks make \n Install lutorpy and open AI \n lauch the python script (example.py) \n \n WARNING : If you use an openAI Gym ATARI environment, a new sensor must be developped: it will be avaiable in the next few days (since openAI and alewrap do not store the ATARI images in the same format) \n Author: Ludovic DENOYER', 'Torch in Action \n This repository contains the code for the Torch in Action book. \n Chapter 1 : Meeting Torch\n *  facedetect : toy face detection dataset (directory with only four samples);\n *  train.lua : example face detection training script (listings 1.1, 1.2 and 1.3); \n Chapter 2 : Preparing a dataset\n *  mnist : MNIST dataset in binary format as downloaded from  yann.lecun.com ;\n *  createdataset.lua : code for serializing the MNIST dataset into  .t7  files and generating samples (section 2.3);\n *  dataloader.lua : code for listing 2.1, 2.2, 2.3 and 2.5. Defines the  DataLoader  and  TensorLoader  classes);\n *  iteratedataset.lua : code for listing 2.5. This script tests the  dataloader.lua  file by iterating through it. Only works if  createdataset.lua  was executed before hand;\n *  getmnistsample.lua : script for generating MNIST samples consolidated as a single image (used to generate figure 2.1); \n Chapter 3 : Training simple neural networks\n *  trainlogreg.lua : Training script for applying binary logistic regression on OR dataset. The model is trained using stochastic gradient descent (listing 3.1);\n *  logreg.log : log file created by running  th trainlogreg.lua > logreg.log ;\n *  trainlogreg-mnist.lua : Script for training a multinomial logistic regression model (saved as  logreg-mnist.t7 ) using SGD on the MNIST dataset. Training stops after 200 epochs where each epoch consists of 10000 samples divided into mini-batches of 32 random samples, or reaching an estimated empirical risk lower than 0.007, whichever comes first. The resulting model is evaluated on the entire training set of 50000 samples and saved to disk (listing 3.2);\n *  logreg-mnist.log : log file created by running  th trainlogreg-mnist.lua > logreg-mnist.log . The data can be used to generate a learning curve. Open the file from your favorite spreadsheet application (Microsoft Excel, LibreOffice Calc, etc.) and specify that values are separated by semicolons;\n *  backward.lua : demonstrates gradient descent through a criterion. Using the input as a parameter, the loss is minized by tacking a step in opposite direction of gradient (section 8.1.3); \n Chapter 4 : Generalizing deep neural networks\n *  tanh.xlsx : plot of the hyperbolic tangent activation function (figure 4.2);\n *  trainmlp-xor.lua : script for training an MLP with one hidden layer composed of 2 units on the XOR dataset. Used to generate  xor-mlp.xlsx  and figure 4.3;\n *  xor-mlp.xlsx : diagram outlining the boundaries of an MLP trained on the XOR dataset (figure 4.3);\n *  overfitting.xlsx : contains learing curve and model overfitting example (figure 4.4 and 4.5);\n *  trainmlp-mnist.lua : upgrades  trainlogreg-mnist.lua  by moving the definition of hyper-parameters to the cmd-line (listing 4.1 and 4.2).\n *  trainmlp-mnist-crossvalidate.lua : upgrades  trainmlp-mnist.lua  with cross-validation (listing 4.3);\n *  trainmlp-mnist-earlystop.lua : upgrades  trainmlp-mnist-crossvalidate.lua  with early-stopping (listing 4.4);\n *  trainmlp-mnist-weightdecay.lua : upgrades  trainmlp-mnist-earlystop.lua  with weight decay regularization (listing 4.5);\n *  trainmlp-mnist-hyperopt.lua : upgrades  trainmlp-mnist-weightdecay.lua  to facilitate hyper-parameter optimization (listing 4.6);\n *  hyperopt-mnist.xlsx : spreadsheet used to hyper-optimize the  trainmlp-mnist-hyperopt.lua  script (figure 4.7 and 4.8);\n *  relu.xlsx : plot of the rectified linear unit (figure 4.9).', ""rnn: recurrent neural networks \n This is a Recurrent Neural Network library that extends Torch's nn. \nYou can use it to build RNNs, LSTMs, GRUs, BRNNs, BLSTMs, and so forth and so on.\nThis library includes documentation for the following objects: \n Modules that consider successive calls to  forward  as different time-steps in a sequence :\n *  AbstractRecurrent  : an abstract class inherited by Recurrent and LSTM;\n *  Recurrent  : a generalized recurrent neural network container;\n *  LSTM  : a vanilla Long-Short Term Memory module;\n  *  FastLSTM  : a faster  LSTM  with optional support for batch normalization;\n *  GRU  : Gated Recurrent Units module;\n *  Recursor  : decorates a module to make it conform to the  AbstractRecurrent  interface;\n *  Recurrence  : decorates a module that outputs  output(t)  given  {input(t), output(t-1)} ;\n *  NormStabilizer  : implements  norm-stabilization  criterion (add this module between RNNs); \n Modules that  forward  entire sequences through a decorated  AbstractRecurrent  instance :\n *  AbstractSequencer  : an abstract class inherited by Sequencer, Repeater, RecurrentAttention, etc.;\n *  Sequencer  : applies an encapsulated module to all elements in an input sequence  (Tensor or Table);\n *  SeqLSTM  : a very fast version of  nn.Sequencer(nn.FastLSTM)  where the  input  and  output  are tensors;\n  *  SeqLSTMP  :  SeqLSTM  with a projection layer;\n *  SeqGRU  : a very fast version of  nn.Sequencer(nn.GRU)  where the  input  and  output  are tensors;\n *  SeqBRNN  : Bidirectional RNN based on SeqLSTM;\n *  BiSequencer  : used for implementing Bidirectional RNNs and LSTMs;\n *  BiSequencerLM  : used for implementing Bidirectional RNNs and LSTMs for language models;\n *  Repeater  : repeatedly applies the same input to an AbstractRecurrent instance;\n *  RecurrentAttention  : a generalized attention model for  REINFORCE modules ; \n Miscellaneous modules and criterions :\n *  MaskZero  : zeroes the  output  and  gradOutput  rows of the decorated module for commensurate  input  rows which are tensors of zeros;\n *  TrimZero  : same behavior as  MaskZero , but more efficient when  input  contains lots zero-masked rows;\n *  LookupTableMaskZero  : extends  nn.LookupTable  to support zero indexes for padding. Zero indexes are forwarded as tensors of zeros;\n *  MaskZeroCriterion  : zeros the  gradInput  and  err  rows of the decorated criterion for commensurate  input  rows which are tensors of zeros;\n *  SeqReverseSequence  : reverses an input sequence on a specific dimension; \n Criterions used for handling sequential inputs and targets :\n *  SequencerCriterion  : sequentially applies the same criterion to a sequence of inputs and targets (Tensor or Table).\n *  RepeaterCriterion  : repeatedly applies the same criterion with the same target on a sequence. \n \n Examples \n The following are example training scripts using this package : \n \n RNN/LSTM/GRU  for Penn Tree Bank dataset; \n Noise Contrastive Estimate  for training multi-layer  SeqLSTM  language models on the  Google Billion Words dataset . The example uses  MaskZero  to train independent variable length sequences using the  NCEModule  and  NCECriterion . This script is our fastest yet boasting speeds of 20,000 words/second (on NVIDIA Titan X) with a 2-layer LSTM having 250 hidden units, a batchsize of 128 and sequence length of a 100. Note that you will need to have  Torch installed with Lua instead of LuaJIT ; \n Recurrent Model for Visual Attention  for the MNIST dataset; \n Encoder-Decoder LSTM  shows you how to couple encoder and decoder  LSTMs  for sequence-to-sequence networks; \n Simple Recurrent Network  shows a simple example for building and training a simple recurrent neural network; \n Simple Sequencer Network  is a version of the above script that uses the Sequencer to decorate the  rnn  instead; \n Sequence to One  demonstrates how to do many to one sequence learning as is the case for sentiment analysis; \n Multivariate Time Series  demonstrates how train a simple RNN to do multi-variate time-series predication. \n \n External Resources \n \n rnn-benchmarks  : benchmarks comparing Torch (using this library), Theano and TensorFlow. \n Harvard Jupyter Notebook Tutorial  : an in-depth tutorial for how to use the Element-Research rnn package by Harvard University; \n dpnn  : this is a dependency of the  rnn  package. It contains useful nn extensions, modules and criterions; \n dataload  : a collection of torch dataset loaders; \n RNN/LSTM/BRNN/BLSTM training script   for Penn Tree Bank or Google Billion Words datasets; \n A brief (1 hours) overview of Torch7, which includes some details about the  rnn  packages (at the end), is available via this  NVIDIA GTC Webinar video . In any case, this presentation gives a nice overview of Logistic Regression, Multi-Layer Perceptrons, Convolutional Neural Networks and Recurrent Neural Networks using Torch7; \n Sequence to Sequence mapping using encoder-decoder RNNs  : a complete training example using synthetic data. \n ConvLSTM  is a repository for training a  Spatio-temporal video autoencoder with differentiable memory . \n An  time series example  for univariate timeseries prediction. \n \n Citation \n If you use  rnn  in your work, we'd really appreciate it if you could cite the following paper: \n Léonard, Nicholas, Sagar Waghmare, Yang Wang, and Jin-Hwa Kim.  rnn: Recurrent Library for Torch.  arXiv preprint arXiv:1511.07889 (2015). \n Any significant contributor to the library will also get added as an author to the paper.\nA  significant contributor  \nis anyone who added at least 300 lines of code to the library. \n Troubleshooting \n Most issues can be resolved by updating the various dependencies:\n bash\nluarocks install torch\nluarocks install nn\nluarocks install dpnn\nluarocks install torchx \n If you are using CUDA :\n bash\nluarocks install cutorch\nluarocks install cunn\nluarocks install cunnx \n And don't forget to update this package :\n bash\nluarocks install rnn \n If that doesn't fix it, open and issue on github. \n \n AbstractRecurrent \n An abstract class inherited by  Recurrent ,  LSTM  and  GRU .\nThe constructor takes a single argument :\n lua\nrnn = nn.AbstractRecurrent([rho]) \nArgument  rho  is the maximum number of steps to backpropagate through time (BPTT).\nSub-classes can set this to a large number like 99999 (the default) if they want to backpropagate through \nthe entire sequence whatever its length. Setting lower values of rho are \nuseful when long sequences are forward propagated, but we only whish to \nbackpropagate through the last  rho  steps, which means that the remainder \nof the sequence doesn't need to be stored (so no additional cost).  \n [recurrentModule] getStepModule(step) \n Returns a module for time-step  step . This is used internally by sub-classes \nto obtain copies of the internal  recurrentModule . These copies share \n parameters  and  gradParameters  but each have their own  output ,  gradInput  \nand any other intermediate states.  \n setOutputStep(step) \n This is a method reserved for internal use by  Recursor  \nwhen doing backward propagation. It sets the object's  output  attribute\nto point to the output at time-step  step . \nThis method was introduced to solve a very annoying bug. \n \n maskZero(nInputDim) \n Decorates the internal  recurrentModule  with  MaskZero . \nThe  output  Tensor (or table thereof) of the  recurrentModule \nwill have each row (i.e. samples) zeroed when the commensurate row of the  input  \nis a tensor of zeros.  \n The  nInputDim  argument must specify the number of non-batch dims \nin the first Tensor of the  input . In the case of an  input  table,\nthe first Tensor is the first one encountered when doing a depth-first search. \n Calling this method makes it possible to pad sequences with different lengths in the same batch with zero vectors. \n When a sample time-step is masked (i.e.  input  is a row of zeros), then \nthe hidden state is effectively reset (i.e. forgotten) for the next non-mask time-step.\nIn other words, it is possible seperate unrelated sequences with a masked element. \n trimZero(nInputDim) \n Decorates the internal  recurrentModule  with  TrimZero .  \n [output] updateOutput(input) \n Forward propagates the input for the current step. The outputs or intermediate \nstates of the previous steps are used recurrently. This is transparent to the \ncaller as the previous outputs and intermediate states are memorized. This \nmethod also increments the  step  attribute by 1. \n \n updateGradInput(input, gradOutput) \n Like  backward , this method should be called in the reverse order of \n forward  calls used to propagate a sequence. So for example : \n ```lua\nrnn = nn.LSTM(10, 10) -- AbstractRecurrent instance\nlocal outputs = {}\nfor i=1,nStep do -- forward propagate sequence\n   outputs[i] = rnn:forward(inputs[i])\nend \n for i=nStep,1,-1 do -- backward propagate sequence in reverse order\n   gradInputs[i] = rnn:backward(inputs[i], gradOutputs[i])\nend \n rnn:forget()\n```  \n The reverse order implements backpropagation through time (BPTT). \n accGradParameters(input, gradOutput, scale) \n Like  updateGradInput , but for accumulating gradients w.r.t. parameters. \n recycle(offset) \n This method goes hand in hand with  forget . It is useful when the current\ntime-step is greater than  rho , at which point it starts recycling \nthe oldest  recurrentModule   sharedClones , \nsuch that they can be reused for storing the next step. This  offset  \nis used for modules like  nn.Recurrent  that use a different module \nfor the first step. Default offset is 0. \n \n forget(offset) \n This method brings back all states to the start of the sequence buffers, \ni.e. it forgets the current sequence. It also resets the  step  attribute to 1.\nIt is highly recommended to call  forget  after each parameter update. \nOtherwise, the previous state will be used to activate the next, which \nwill often lead to instability. This is caused by the previous state being\nthe result of now changed parameters. It is also good practice to call \n forget  at the start of each new sequence. \n \n maxBPTTstep(rho) \n This method sets the maximum number of time-steps for which to perform \nbackpropagation through time (BPTT). So say you set this to  rho = 3  time-steps,\nfeed-forward for 4 steps, and then backpropgate, only the last 3 steps will be \nused for the backpropagation. If your AbstractRecurrent instance is wrapped \nby a  Sequencer , this will be handled auto-magically by the Sequencer.\nOtherwise, setting this value to a large value (i.e. 9999999), is good for most, if not all, cases. \n \n backwardOnline() \n This method was deprecated Jan 6, 2016. \nSince then, by default,  AbstractRecurrent  instances use the \nbackwardOnline behaviour. \nSee  updateGradInput  for details. \n training() \n In training mode, the network remembers all previous  rho  (number of time-steps)\nstates. This is necessary for BPTT.  \n evaluate() \n During evaluation, since their is no need to perform BPTT at a later time, \nonly the previous step is remembered. This is very efficient memory-wise, \nsuch that evaluation can be performed using potentially infinite-length \nsequence. \n \n Recurrent \n References :\n * A.  Sutsekever Thesis Sec. 2.5 and 2.8 \n * B.  Mikolov Thesis Sec. 3.2 and 3.3 \n * C.  RNN and Backpropagation Guide \n A  composite Module  for implementing Recurrent Neural Networks (RNN), excluding the output layer.  \n The  nn.Recurrent(start, input, feedback, [transfer, rho, merge])  constructor takes 6 arguments:\n *  start  : the size of the output (excluding the batch dimension), or a Module that will be inserted between the  input  Module and  transfer  module during the first step of the propagation. When  start  is a size (a number or  torch.LongTensor ), then this  start  Module will be initialized as  nn.Add(start)  (see Ref. A).\n *  input  : a Module that processes input Tensors (or Tables). Output must be of same size as  start  (or its output in the case of a  start  Module), and same size as the output of the  feedback  Module.\n *  feedback  : a Module that feedbacks the previous output Tensor (or Tables) up to the  merge  module.\n *  merge  : a  table Module  that merges the outputs of the  input  and  feedback  Module before being forwarded through the  transfer  Module.\n *  transfer  : a non-linear Module used to process the output of the  merge  module, or in the case of the first step, the output of the  start  Module.\n *  rho  : the maximum amount of backpropagation steps to take back in time. Limits the number of previous steps kept in memory. Due to the vanishing gradients effect, references A and B recommend  rho = 5  (or lower). Defaults to 99999. \n An RNN is used to process a sequence of inputs. \nEach step in the sequence should be propagated by its own  forward  (and  backward ), \none  input  (and  gradOutput ) at a time. \nEach call to  forward  keeps a log of the intermediate states (the  input  and many  Module.outputs ) \nand increments the  step  attribute by 1. \nMethod  backward  must be called in reverse order of the sequence of calls to  forward  in \norder to backpropgate through time (BPTT). This reverse order is necessary \nto return a  gradInput  for each call to  forward .  \n The  step  attribute is only reset to 1 when a call to the  forget  method is made. \nIn which case, the Module is ready to process the next sequence (or batch thereof).\nNote that the longer the sequence, the more memory that will be required to store all the \n output  and  gradInput  states (one for each time step).  \n To use this module with batches, we suggest using different \nsequences of the same size within a batch and calling  updateParameters  \nevery  rho  steps and  forget  at the end of the sequence.  \n Note that calling the  evaluate  method turns off long-term memory; \nthe RNN will only remember the previous output. This allows the RNN \nto handle long sequences without allocating any additional memory. \n For a simple concise example of how to make use of this module, please consult the \n simple-recurrent-network.lua \ntraining script. \n \n Decorate it with a Sequencer \n Note that any  AbstractRecurrent  instance can be decorated with a  Sequencer  \nsuch that an entire sequence (a table) can be presented with a single  forward/backward  call.\nThis is actually the recommended approach as it allows RNNs to be stacked and makes the \nrnn conform to the Module interface, i.e. each call to  forward  can be \nfollowed by its own immediate call to  backward  as each  input  to the \nmodel is an entire sequence, i.e. a table of tensors where each tensor represents\na time-step. \n lua\nseq = nn.Sequencer(module) \n The  simple-sequencer-network.lua  training script\nis equivalent to the above mentionned  simple-recurrent-network.lua \nscript, except that it decorates the  rnn  with a  Sequencer  which takes \na table of  inputs  and  gradOutputs  (the sequence for that batch).\nThis lets the  Sequencer  handle the looping over the sequence. \n You should only think about using the  AbstractRecurrent  modules without \na  Sequencer  if you intend to use it for real-time prediction. \nActually, you can even use an  AbstractRecurrent  instance decorated by a  Sequencer \nfor real time prediction by calling  Sequencer:remember()  and presenting each \ntime-step  input  as  {input} . \n Other decorators can be used such as the  Repeater  or  RecurrentAttention .\nThe  Sequencer  is only the most common one.  \n \n LSTM \n References :\n * A.  Speech Recognition with Deep Recurrent Neural Networks \n * B.  Long-Short Term Memory \n * C.  LSTM: A Search Space Odyssey \n * D.  nngraph LSTM implementation on github \n This is an implementation of a vanilla Long-Short Term Memory module. \nWe used Ref. A's LSTM as a blueprint for this module as it was the most concise.\nYet it is also the vanilla LSTM described in Ref. C.  \n The  nn.LSTM(inputSize, outputSize, [rho])  constructor takes 3 arguments:\n *  inputSize  : a number specifying the size of the input;\n *  outputSize  : a number specifying the size of the output;\n *  rho  : the maximum amount of backpropagation steps to take back in time. Limits the number of previous steps kept in memory. Defaults to 9999. \n   \n The actual implementation corresponds to the following algorithm:\n lua\ni[t] = σ(W[x->i]x[t] + W[h->i]h[t−1] + W[c->i]c[t−1] + b[1->i])      (1)\nf[t] = σ(W[x->f]x[t] + W[h->f]h[t−1] + W[c->f]c[t−1] + b[1->f])      (2)\nz[t] = tanh(W[x->c]x[t] + W[h->c]h[t−1] + b[1->c])                   (3)\nc[t] = f[t]c[t−1] + i[t]z[t]                                         (4)\no[t] = σ(W[x->o]x[t] + W[h->o]h[t−1] + W[c->o]c[t] + b[1->o])        (5)\nh[t] = o[t]tanh(c[t])                                                (6) \nwhere  W[s->q]  is the weight matrix from  s  to  q ,  t  indexes the time-step,\n b[1->q]  are the biases leading into  q ,  σ()  is  Sigmoid ,  x[t]  is the input,\n i[t]  is the input gate (eq. 1),  f[t]  is the forget gate (eq. 2), \n z[t]  is the input to the cell (which we call the hidden) (eq. 3), \n c[t]  is the cell (eq. 4),  o[t]  is the output gate (eq. 5), \nand  h[t]  is the output of this module (eq. 6). Also note that the \nweight matrices from cell to gate vectors are diagonal  W[c->s] , where  s  \nis  i , f , or  o . \n As you can see, unlike  Recurrent , this \nimplementation isn't generic enough that it can take arbitrary component Module\ndefinitions at construction. However, the LSTM module can easily be adapted \nthrough inheritance by overriding the different factory methods :\n  *  buildGate  : builds generic gate that is used to implement the input, forget and output gates;\n  *  buildInputGate  : builds the input gate (eq. 1). Currently calls  buildGate ;\n  *  buildForgetGate  : builds the forget gate (eq. 2). Currently calls  buildGate ;\n  *  buildHidden  : builds the hidden (eq. 3);\n  *  buildCell  : builds the cell (eq. 4);\n  *  buildOutputGate  : builds the output gate (eq. 5). Currently calls  buildGate ;\n  *  buildModel  : builds the actual LSTM model which is used internally (eq. 6). \n Note that we recommend decorating the  LSTM  with a  Sequencer  \n(refer to  this  for details). \n \n FastLSTM \n A faster version of the  LSTM . \nBasically, the input, forget and output gates, as well as the hidden state are computed at one fellswoop. \n Note that  FastLSTM  does not use peephole connections between cell and gates. The algorithm from  LSTM  changes as follows:\n lua\ni[t] = σ(W[x->i]x[t] + W[h->i]h[t−1] + b[1->i])                      (1)\nf[t] = σ(W[x->f]x[t] + W[h->f]h[t−1] + b[1->f])                      (2)\nz[t] = tanh(W[x->c]x[t] + W[h->c]h[t−1] + b[1->c])                   (3)\nc[t] = f[t]c[t−1] + i[t]z[t]                                         (4)\no[t] = σ(W[x->o]x[t] + W[h->o]h[t−1] + b[1->o])                      (5)\nh[t] = o[t]tanh(c[t])                                                (6) \ni.e. omitting the summands  W[c->i]c[t−1]  (eq. 1),  W[c->f]c[t−1]  (eq. 2), and  W[c->o]c[t]  (eq. 5). \n usenngraph \n This is a static attribute of the  FastLSTM  class. The default value is  false .\nSetting  usenngraph = true  will force all new instantiated instances of  FastLSTM  \nto use  nngraph 's  nn.gModule  to build the internal  recurrentModule  which is \ncloned for each time-step. \n \n Recurrent Batch Normalization \n This extends the  FastLSTM  class to enable faster convergence during training by zero-centering the input-to-hidden and hidden-to-hidden transformations. \nIt reduces the  internal covariate shift  between time steps. It is an implementation of Cooijmans et. al.'s  Recurrent Batch Normalization . The hidden-to-hidden transition of each LSTM cell is normalized according to \n lua\ni[t] = σ(BN(W[x->i]x[t]) + BN(W[h->i]h[t−1]) + b[1->i])                      (1)\nf[t] = σ(BN(W[x->f]x[t]) + BN(W[h->f]h[t−1]) + b[1->f])                      (2)\nz[t] = tanh(BN(W[x->c]x[t]) + BN(W[h->c]h[t−1]) + b[1->c])                   (3)\nc[t] = f[t]c[t−1] + i[t]z[t]                                                 (4)\no[t] = σ(BN(W[x->o]x[t]) + BN(W[h->o]h[t−1]) + b[1->o])                      (5)\nh[t] = o[t]tanh(c[t])                                                        (6)  \nwhere the batch normalizing transform is:                                  \n lua\n  BN(h; gamma, beta) = beta + gamma *      hd - E(hd)\n                                      ------------------\n                                       sqrt(E(σ(hd) + eps)) \nwhere  hd  is a vector of (pre)activations to be normalized,  gamma , and  beta  are model parameters that determine the mean and standard deviation of the normalized activation.  eps  is a regularization hyperparameter to keep the division numerically stable and  E(hd)  and  E(σ(hd))  are the estimates of the mean and variance in the mini-batch respectively. The authors recommend initializing  gamma  to a small value and found 0.1 to be the value that did not cause vanishing gradients.  beta , the shift parameter, is  null  by default. \n To turn on batch normalization during training, do:\n lua\nnn.FastLSTM.bn = true\nlstm = nn.FastLSTM(inputsize, outputsize, [rho, eps, momentum, affine]   \n where  momentum  is same as  gamma  in the equation above (defaults to 0.1),  eps  is defined above and  affine  is a boolean whose state determines if the learnable affine transform is turned off ( false ) or on ( true , the default). \n \n GRU \n References :\n * A.  Learning Phrase Representations Using RNN Encoder-Decoder For Statistical Machine Translation. \n * B.  Implementing a GRU/LSTM RNN with Python and Theano \n * C.  An Empirical Exploration of Recurrent Network Architectures \n * D.  Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling \n * E.  RnnDrop: A Novel Dropout for RNNs in ASR \n * F.  A Theoretically Grounded Application of Dropout in Recurrent Neural Networks \n This is an implementation of Gated Recurrent Units module.  \n The  nn.GRU(inputSize, outputSize [,rho [,p [, mono]]])  constructor takes 3 arguments likewise  nn.LSTM  or 4 arguments for dropout:\n *  inputSize  : a number specifying the size of the input;\n *  outputSize  : a number specifying the size of the output;\n *  rho  : the maximum amount of backpropagation steps to take back in time. Limits the number of previous steps kept in memory. Defaults to 9999;\n *  p  : dropout probability for inner connections of GRUs.\n *  mono  : Monotonic sample for dropouts inside GRUs. Only needed in a  TrimZero  +  BGRU (p>0) situation. \n   \n The actual implementation corresponds to the following algorithm:\n lua\nz[t] = σ(W[x->z]x[t] + W[s->z]s[t−1] + b[1->z])            (1)\nr[t] = σ(W[x->r]x[t] + W[s->r]s[t−1] + b[1->r])            (2)\nh[t] = tanh(W[x->h]x[t] + W[hr->c](s[t−1]r[t]) + b[1->h])  (3)\ns[t] = (1-z[t])h[t] + z[t]s[t-1]                           (4) \nwhere  W[s->q]  is the weight matrix from  s  to  q ,  t  indexes the time-step,  b[1->q]  are the biases leading into  q ,  σ()  is  Sigmoid ,  x[t]  is the input and  s[t]  is the output of the module (eq. 4). Note that unlike the  LSTM , the GRU has no cells. \n The GRU was benchmark on  PennTreeBank  dataset using  recurrent-language-model.lua  script. \nIt slightly outperfomed  FastLSTM , however, since LSTMs have more parameters than GRUs, \nthe dataset larger than  PennTreeBank  might change the performance result. \nDon't be too hasty to judge on which one is the better of the two (see Ref. C and D). \n Memory   examples/s\n    FastLSTM      176M        16.5K \n    GRU            92M        15.8K \n Memory  is measured by the size of  dp.Experiment  save file.  examples/s  is measured by the training speed at 1 epoch, so, it may have a disk IO bias. \n \n RNN dropout (see Ref. E and F) was benchmark on  PennTreeBank  dataset using  recurrent-language-model.lua  script, too. The details can be found in the script. In the benchmark,  GRU  utilizes a dropout after  LookupTable , while  BGRU , stands for Bayesian GRUs, uses dropouts on inner connections (naming as Ref. F), but not after  LookupTable . \n As Yarin Gal (Ref. F) mentioned, it is recommended that one may use  p = 0.25  for the first attempt. \n \n \n Recursor \n This module decorates a  module  to be used within an  AbstractSequencer  instance.\nIt does this by making the decorated module conform to the  AbstractRecurrent  interface,\nwhich like the  LSTM  and  Recurrent  classes, this class inherits.  \n lua\nrec = nn.Recursor(module[, rho]) \n For each successive call to  updateOutput  (i.e.  forward ), this \ndecorator will create a  stepClone()  of the decorated  module . \nSo for each time-step, it clones the  module . Both the clone and \noriginal share parameters and gradients w.r.t. parameters. However, for \nmodules that already conform to the  AbstractRecurrent  interface, \nthe clone and original module are one and the same (i.e. no clone). \n Examples : \n Let's assume I want to stack two LSTMs. I could use two sequencers : \n lua\nlstm = nn.Sequential()\n   :add(nn.Sequencer(nn.LSTM(100,100)))\n   :add(nn.Sequencer(nn.LSTM(100,100))) \n Using a  Recursor , I make the same model with a single  Sequencer  : \n lua\nlstm = nn.Sequencer(\n   nn.Recursor(\n      nn.Sequential()\n         :add(nn.LSTM(100,100))\n         :add(nn.LSTM(100,100))\n      )\n   ) \n Actually, the  Sequencer  will wrap any non- AbstractRecurrent  module automatically, \nso I could simplify this further to : \n lua\nlstm = nn.Sequencer(\n   nn.Sequential()\n      :add(nn.LSTM(100,100))\n      :add(nn.LSTM(100,100))\n   ) \n I can also add a  Linear  between the two  LSTM s. In this case,\na  Linear  will be cloned (and have its parameters shared) for each time-step,\nwhile the  LSTM s will do whatever cloning internally : \n lua\nlstm = nn.Sequencer(\n   nn.Sequential()\n      :add(nn.LSTM(100,100))\n      :add(nn.Linear(100,100))\n      :add(nn.LSTM(100,100))\n   )   \n AbstractRecurrent  instances like  Recursor ,  Recurrent  and  LSTM  are \nexpcted to manage time-steps internally. Non- AbstractRecurrent  instances\ncan be wrapped by a  Recursor  to have the same behavior.  \n Every call to  forward  on an  AbstractRecurrent  instance like  Recursor  \nwill increment the  self.step  attribute by 1, using a shared parameter clone\nfor each successive time-step (for a maximum of  rho  time-steps, which defaults to 9999999).\nIn this way,  backward  can be called in reverse order of the  forward  calls \nto perform backpropagation through time (BPTT). Which is exactly what \n AbstractSequencer  instances do internally.\nThe  backward  call, which is actually divided into calls to  updateGradInput  and \n accGradParameters , decrements by 1 the  self.udpateGradInputStep  and  self.accGradParametersStep \nrespectively, starting at  self.step .\nSuccessive calls to  backward  will decrement these counters and use them to \nbackpropagate through the appropriate internall step-wise shared-parameter clones. \n Anyway, in most cases, you will not have to deal with the  Recursor  object directly as\n AbstractSequencer  instances automatically decorate non- AbstractRecurrent  instances\nwith a  Recursor  in their constructors. \n For a concrete example of its use, please consult the  simple-recurrent-network.lua \ntraining script for an example of its use. \n \n Recurrence \n A extremely general container for implementing pretty much any type of recurrence. \n lua\nrnn = nn.Recurrence(recurrentModule, outputSize, nInputDim, [rho]) \n Unlike  Recurrent , this module doesn't manage a separate \nmodules like  inputModule ,  startModule ,  mergeModule  and the like.\nInstead, it only manages a single  recurrentModule , which should \noutput a Tensor or table :  output(t)  \ngiven an input table :  {input(t), output(t-1)} .\nUsing a mix of  Recursor  (say, via  Sequencer ) with  Recurrence , one can implement \npretty much any type of recurrent neural network, including LSTMs and RNNs. \n For the first step, the  Recurrence  forwards a Tensor (or table thereof)\nof zeros through the recurrent layer (like LSTM, unlike Recurrent).\nSo it needs to know the  outputSize , which is either a number or \n torch.LongStorage , or table thereof. The batch dimension should be \nexcluded from the  outputSize . Instead, the size of the batch dimension \n(i.e. number of samples) will be extrapolated from the  input  using \nthe  nInputDim  argument. For example, say that our input is a Tensor of size \n 4 x 3  where  4  is the number of samples, then  nInputDim  should be  1 .\nAs another example, if our input is a table of table [...] of tensors \nwhere the first tensor (depth first) is the same as in the previous example,\nthen our  nInputDim  is also  1 . \n As an example, let's use  Sequencer  and  Recurrence  \nto build a Simple RNN for language modeling : \n ```lua\nrho = 5\nhiddenSize = 10\noutputSize = 5 -- num classes\nnIndex = 10000 \n -- recurrent module\nrm = nn.Sequential()\n   :add(nn.ParallelTable()\n      :add(nn.LookupTable(nIndex, hiddenSize))\n      :add(nn.Linear(hiddenSize, hiddenSize)))\n   :add(nn.CAddTable())\n   :add(nn.Sigmoid()) \n rnn = nn.Sequencer(\n   nn.Sequential()\n      :add(nn.Recurrence(rm, hiddenSize, 1))\n      :add(nn.Linear(hiddenSize, outputSize))\n      :add(nn.LogSoftMax())\n)\n``` \n Note : We could very well reimplement the  LSTM  module using the\nnewer  Recursor  and  Recurrent  modules, but that would mean \nbreaking backwards compatibility for existing models saved on disk. \n \n NormStabilizer \n Ref. A :  Regularizing RNNs by Stabilizing Activations \n This module implements the  norm-stabilization  criterion: \n lua\nns = nn.NormStabilizer([beta])   \n This module regularizes the hidden states of RNNs by minimizing the difference between the\nL2-norms of consecutive steps. The cost function is defined as :\n loss = beta * 1/T sum_t( ||h[t]|| - ||h[t-1]|| )^2  \nwhere  T  is the number of time-steps. Note that we do not divide the gradient by  T \nsuch that the chosen  beta  can scale to different sequence sizes without being changed. \n The sole argument  beta  is defined in ref. A. Since we don't divide the gradients by\nthe number of time-steps, the default value of  beta=1  should be valid for most cases.  \n This module should be added between RNNs (or LSTMs or GRUs) to provide better regularization of the hidden states. \nFor example :\n lua\nlocal stepmodule = nn.Sequential()\n   :add(nn.FastLSTM(10,10))\n   :add(nn.NormStabilizer())\n   :add(nn.FastLSTM(10,10))\n   :add(nn.NormStabilizer())\nlocal rnn = nn.Sequencer(stepmodule)   \n To use it with  SeqLSTM  you can do something like this :\n lua\nlocal rnn = nn.Sequential()\n   :add(nn.SeqLSTM(10,10))\n   :add(nn.Sequencer(nn.NormStabilizer()))\n   :add(nn.SeqLSTM(10,10))\n   :add(nn.Sequencer(nn.NormStabilizer()))   \n \n AbstractSequencer \n This abstract class implements a light interface shared by \nsubclasses like :  Sequencer ,  Repeater ,  RecurrentAttention ,  BiSequencer  and so on. \n \n Sequencer \n The  nn.Sequencer(module)  constructor takes a single argument,  module , which is the module \nto be applied from left to right, on each element of the input sequence. \n lua\nseq = nn.Sequencer(module) \n This Module is a kind of  decorator  \nused to abstract away the intricacies of  AbstractRecurrent  modules. While an  AbstractRecurrent  instance \nrequires that a sequence to be presented one input at a time, each with its own call to  forward  (and  backward ),\nthe  Sequencer  forwards an  input  sequence (a table) into an  output  sequence (a table of the same length).\nIt also takes care of calling  forget  on AbstractRecurrent instances. \n Input/Output Format \n The  Sequencer  requires inputs and outputs to be of shape  seqlen x batchsize x featsize  : \n \n seqlen  is the number of time-steps that will be fed into the  Sequencer . \n batchsize  is the number of examples in the batch. Each example is its own independent sequence. \n featsize  is the size of the remaining non-batch dimensions. So this could be  1  for language models, or  c x h x w  for convolutional models, etc. \n \n \n Above is an example input sequence for a character level language model.\nIt has  seqlen  is 5 which means that it contains sequences of 5 time-steps. \nThe openning  {  and closing  }  illustrate that the time-steps are elements of a Lua table, although \nit also accepts full Tensors of shape  seqlen x batchsize x featsize . \nThe  batchsize  is 2 as their are two independent sequences :  { H, E, L, L, O }  and  { F, U, Z, Z, Y, } .\nThe  featsize  is 1 as their is only one feature dimension per character and each such character is of size 1.\nSo the input in this case is a table of  seqlen  time-steps where each time-step is represented by a  batchsize x featsize  Tensor. \n \n Above is another example of a sequence (input or output). \nIt has a  seqlen  of 4 time-steps. \nThe  batchsize  is again 2 which means there are two sequences.\nThe  featsize  is 3 as each time-step of each sequence has 3 variables.\nSo each time-step (element of the table) is represented again as a tensor\nof size  batchsize x featsize . \nNote that while in both examples the  featsize  encodes one dimension, \nit could encode more.  \n Example \n For example,  rnn  : an instance of nn.AbstractRecurrent, can forward an  input  sequence one forward at a time:\n lua\ninput = {torch.randn(3,4), torch.randn(3,4), torch.randn(3,4)}\nrnn:forward(input[1])\nrnn:forward(input[2])\nrnn:forward(input[3])   \n Equivalently, we can use a Sequencer to forward the entire  input  sequence at once: \n lua\nseq = nn.Sequencer(rnn)\nseq:forward(input)   \n We can also forward Tensors instead of Tables : \n lua\n-- seqlen x batchsize x featsize\ninput = torch.randn(3,3,4)\nseq:forward(input)   \n Details \n The  Sequencer  can also take non-recurrent Modules (i.e. non-AbstractRecurrent instances) and apply it to each \ninput to produce an output table of the same length. \nThis is especially useful for processing variable length sequences (tables). \n Internally, the  Sequencer  expects the decorated  module  to be an \n AbstractRecurrent  instance. When this is not the case, the  module  \nis automatically decorated with a  Recursor  module, which makes it \nconform to the  AbstractRecurrent  interface.  \n Note : this is due a recent update (27 Oct 2015), as before this \n AbstractRecurrent  and and non- AbstractRecurrent  instances needed to \nbe decorated by their own  Sequencer . The recent update, which introduced the \n Recursor  decorator, allows a single  Sequencer  to wrap any type of module, \n AbstractRecurrent , non- AbstractRecurrent  or a composite structure of both types.\nNevertheless, existing code shouldn't be affected by the change. \n For a concise example of its use, please consult the  simple-sequencer-network.lua \ntraining script. \n \n remember([mode]) \n When  mode='neither'  (the default behavior of the class), the Sequencer will additionally call  forget  before each call to  forward . \nWhen  mode='both'  (the default when calling this function), the Sequencer will never call  forget .\nIn which case, it is up to the user to call  forget  between independent sequences.\nThis behavior is only applicable to decorated AbstractRecurrent  modules .\nAccepted values for argument  mode  are as follows : \n \n 'eval' only affects evaluation (recommended for RNNs) \n 'train' only affects training \n 'neither' affects neither training nor evaluation (default behavior of the class) \n 'both' affects both training and evaluation (recommended for LSTMs) \n \n forget() \n Calls the decorated AbstractRecurrent module's  forget  method. \n \n SeqLSTM \n This module is a faster version of  nn.Sequencer(nn.FastLSTM(inputsize, outputsize))  : \n lua\nseqlstm = nn.SeqLSTM(inputsize, outputsize)   \n Each time-step is computed as follows (same as  FastLSTM ): \n lua\ni[t] = σ(W[x->i]x[t] + W[h->i]h[t−1] + b[1->i])                      (1)\nf[t] = σ(W[x->f]x[t] + W[h->f]h[t−1] + b[1->f])                      (2)\nz[t] = tanh(W[x->c]x[t] + W[h->c]h[t−1] + b[1->c])                   (3)\nc[t] = f[t]c[t−1] + i[t]z[t]                                         (4)\no[t] = σ(W[x->o]x[t] + W[h->o]h[t−1] + b[1->o])                      (5)\nh[t] = o[t]tanh(c[t])                                                (6)   \n A notable difference is that this module expects the  input  and  gradOutput  to \nbe tensors instead of tables. The default shape is  seqlen x batchsize x inputsize  for\nthe  input  and  seqlen x batchsize x outputsize  for the  output  : \n ```lua\ninput = torch.randn(seqlen, batchsize, inputsize)\ngradOutput = torch.randn(seqlen, batchsize, outputsize) \n output = seqlstm:forward(input)\ngradInput = seqlstm:backward(input, gradOutput)\n```  \n Note that if you prefer to transpose the first two dimension (i.e.  batchsize x seqlen  instead of the default  seqlen x batchsize )\nyou can set  seqlstm.batchfirst = true  following initialization. \n For variable length sequences, set  seqlstm.maskzero = true . \nThis is equivalent to calling  maskZero(1)  on a  FastLSTM  wrapped by a  Sequencer :\n lua\nfastlstm = nn.FastLSTM(inputsize, outputsize)\nfastlstm:maskZero(1)\nseqfastlstm = nn.Sequencer(fastlstm)   \n For  maskzero = true , input sequences are expected to be seperated by tensor of zeros for a time step. \n The  seqlstm:toFastLSTM()  method generates a  FastLSTM  instance initialized with the parameters \nof the  seqlstm  instance. Note however that the resulting parameters will not be shared (nor can they ever be). \n Like the  FastLSTM , the  SeqLSTM  does not use peephole connections between cell and gates (see  FastLSTM  for details). \n Like the  Sequencer , the  SeqLSTM  provides a  remember  method. \n Note that a  SeqLSTM  cannot replace  FastLSTM  in code that decorates it with a\n AbstractSequencer  or  Recursor  as this would be equivalent to  Sequencer(Sequencer(FastLSTM)) .\nYou have been warned. \n \n SeqLSTMP \n References:\n * A.  LSTM RNN Architectures for Large Scale Acoustic Modeling \n * B.  Exploring the Limits of Language Modeling \n lua\nlstmp = nn.SeqLSTMP(inputsize, hiddensize, outputsize)   \n The  SeqLSTMP  is a subclass of  SeqLSTM . \nIt differs in that after computing the hidden state  h[t]  (eq. 6), it is \nprojected onto  r[t]  using a simple linear transform (eq. 7). \nThe computation of the gates also uses the previous such projection  r[t-1]  (eq. 1, 2, 3, 5).\nThis differs from  SeqLSTM  which uses  h[t-1]  instead of  r[t-1] . \n The computation of a time-step outlined in  SeqLSTM  is replaced with the following:\n lua\ni[t] = σ(W[x->i]x[t] + W[r->i]r[t−1] + b[1->i])                      (1)\nf[t] = σ(W[x->f]x[t] + W[r->f]r[t−1] + b[1->f])                      (2)\nz[t] = tanh(W[x->c]x[t] + W[h->c]r[t−1] + b[1->c])                   (3)\nc[t] = f[t]c[t−1] + i[t]z[t]                                         (4)\no[t] = σ(W[x->o]x[t] + W[r->o]r[t−1] + b[1->o])                      (5)\nh[t] = o[t]tanh(c[t])                                                (6)\nr[t] = W[h->r]h[t]                                                   (7)   \n The algorithm is outlined in ref. A and benchmarked with state of the art results on the Google billion words dataset in ref. B.\n SeqLSTMP  can be used with an  hiddensize >> outputsize  such that the effective size of the memory cells  c[t]  \nand gates  i[t] ,  f[t]  and  o[t]  can be much larger than the actual input  x[t]  and output  r[t] .\nFor fixed  inputsize  and  outputsize , the  SeqLSTMP  will be able to remember much more information than the  SeqLSTM . \n \n SeqGRU \n This module is a faster version of  nn.Sequencer(nn.GRU(inputsize, outputsize))  : \n lua\nseqGRU = nn.SeqGRU(inputsize, outputsize)   \n Usage of SeqGRU differs from GRU in the same manner as SeqLSTM differs from LSTM. Therefore see  SeqLSTM  for more details. \n \n SeqBRNN \n lua\nbrnn = nn.SeqBRNN(inputSize, outputSize, [batchFirst], [merge])   \n A bi-directional RNN that uses SeqLSTM. Internally contains a 'fwd' and 'bwd' module of SeqLSTM. Expects an input shape of  seqlen x batchsize x inputsize .\nBy setting [batchFirst] to true, the input shape can be  batchsize x seqLen x inputsize . Merge module defaults to CAddTable(), summing the outputs from each\noutput layer. \n Example:\n input = torch.rand(1, 1, 5)\nbrnn = nn.SeqBRNN(5, 5)\nprint(brnn:forward(input))  \nPrints an output of a 1x1x5 tensor. \n \n BiSequencer \n Applies encapsulated  fwd  and  bwd  rnns to an input sequence in forward and reverse order.\nIt is used for implementing Bidirectional RNNs and LSTMs. \n lua\nbrnn = nn.BiSequencer(fwd, [bwd, merge]) \n The input to the module is a sequence (a table) of tensors\nand the output is a sequence (a table) of tensors of the same length.\nApplies a  fwd  rnn (an  AbstractRecurrent  instance) to each element in the sequence in\nforward order and applies the  bwd  rnn in reverse order (from last element to first element).\nThe  bwd  rnn defaults to: \n lua\nbwd = fwd:clone()\nbwd:reset() \n For each step (in the original sequence), the outputs of both rnns are merged together using\nthe  merge  module (defaults to  nn.JoinTable(1,1) ). \nIf  merge  is a number, it specifies the  JoinTable \nconstructor's  nInputDim  argument. Such that the  merge  module is then initialized as : \n lua\nmerge = nn.JoinTable(1,merge) \n Internally, the  BiSequencer  is implemented by decorating a structure of modules that makes \nuse of 3 Sequencers for the forward, backward and merge modules. \n Similarly to a  Sequencer , the sequences in a batch must have the same size.\nBut the sequence length of each batch can vary. \n Note : make sure you call  brnn:forget()  after each call to  updateParameters() . \nAlternatively, one could call  brnn.bwdSeq:forget()  so that only  bwd  rnn forgets.\nThis is the minimum requirement, as it would not make sense for the  bwd  rnn to remember future sequences. \n \n BiSequencerLM \n Applies encapsulated  fwd  and  bwd  rnns to an input sequence in forward and reverse order.\nIt is used for implementing Bidirectional RNNs and LSTMs for Language Models (LM). \n lua\nbrnn = nn.BiSequencerLM(fwd, [bwd, merge]) \n The input to the module is a sequence (a table) of tensors\nand the output is a sequence (a table) of tensors of the same length.\nApplies a  fwd  rnn (an  AbstractRecurrent  instance to the \nfirst  N-1  elements in the sequence in forward order.\nApplies the  bwd  rnn in reverse order to the last  N-1  elements (from second-to-last element to first element).\nThis is the main difference of this module with the  BiSequencer .\nThe latter cannot be used for language modeling because the  bwd  rnn would be trained to predict the input it had just be fed as input. \n \n The  bwd  rnn defaults to: \n lua\nbwd = fwd:clone()\nbwd:reset() \n While the  fwd  rnn will output representations for the last  N-1  steps,\nthe  bwd  rnn will output representations for the first  N-1  steps.\nThe missing outputs for each rnn ( the first step for the  fwd , the last step for the  bwd )\nwill be filled with zero Tensors of the same size the commensure rnn's outputs.\nThis way they can be merged. If  nn.JoinTable  is used (the default), then the first \nand last output elements will be padded with zeros for the missing  fwd  and  bwd  rnn outputs, respectively. \n For each step (in the original sequence), the outputs of both rnns are merged together using\nthe  merge  module (defaults to  nn.JoinTable(1,1) ). \nIf  merge  is a number, it specifies the  JoinTable \nconstructor's  nInputDim  argument. Such that the  merge  module is then initialized as : \n lua\nmerge = nn.JoinTable(1,merge) \n Similarly to a  Sequencer , the sequences in a batch must have the same size.\nBut the sequence length of each batch can vary. \n Note that LMs implemented with this module will not be classical LMs as they won't measure the \nprobability of a word given the previous words. Instead, they measure the probabiliy of a word\ngiven the surrounding words, i.e. context. While for mathematical reasons you may not be able to use this to measure the \nprobability of a sequence of words (like a sentence), \nyou can still measure the pseudo-likeliness of such a sequence (see  this  for a discussion). \n \n Repeater \n This Module is a  decorator  similar to  Sequencer .\nIt differs in that the sequence length is fixed before hand and the input is repeatedly forwarded \nthrough the wrapped  module  to produce an output table of length  nStep :\n lua\nr = nn.Repeater(module, nStep) \nArgument  module  should be an  AbstractRecurrent  instance.\nThis is useful for implementing models like  RCNNs ,\nwhich are repeatedly presented with the same input. \n \n RecurrentAttention \n References : \n \n A.  Recurrent Models of Visual Attention \n B.  Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning \n \n This module can be used to implement the Recurrent Attention Model (RAM) presented in Ref. A :\n lua\nram = nn.RecurrentAttention(rnn, action, nStep, hiddenSize) \n rnn  is an  AbstractRecurrent  instance. \nIts input is  {x, z}  where  x  is the input to the  ram  and  z  is an \naction sampled from the  action  module. \nThe output size of the  rnn  must be equal to  hiddenSize . \n action  is a  Module  \nthat uses a  REINFORCE module  (ref. B) like \n ReinforceNormal , \n ReinforceCategorical , or \n ReinforceBernoulli  \nto sample actions given the previous time-step's output of the  rnn . \nDuring the first time-step, the  action  module is fed with a Tensor of zeros of size  input:size(1) x hiddenSize .\nIt is important to understand that the sampled actions do not receive gradients \nbackpropagated from the training criterion. \nInstead, a reward is broadcast from a Reward Criterion like  VRClassReward  Criterion to \nthe  action 's REINFORCE module, which will backprogate graidents computed from the  output  samples \nand the  reward . \nTherefore, the  action  module's outputs are only used internally, within the RecurrentAttention module. \n nStep  is the number of actions to sample, i.e. the number of elements in the  output  table. \n hiddenSize  is the output size of the  rnn . This variable is necessary \nto generate the zero Tensor to sample an action for the first step (see above). \n A complete implementation of Ref. A is available  here . \n \n MaskZero \n This module zeroes the  output  rows of the decorated module \nfor commensurate  input  rows which are tensors of zeros. \n lua\nmz = nn.MaskZero(module, nInputDim) \n The  output  Tensor (or table thereof) of the decorated  module \nwill have each row (samples) zeroed when the commensurate row of the  input  \nis a tensor of zeros.  \n The  nInputDim  argument must specify the number of non-batch dims \nin the first Tensor of the  input . In the case of an  input  table,\nthe first Tensor is the first one encountered when doing a depth-first search. \n This decorator makes it possible to pad sequences with different lengths in the same batch with zero vectors. \n Caveat:  MaskZero  not guarantee that the  output  and  gradInput  tensors of the internal modules \nof the decorated  module  will be zeroed as well when the  input  is zero as well. \n MaskZero  only affects the immediate  gradInput  and  output  of the module that it encapsulates.\nHowever, for most modules, the gradient update for that time-step will be zero because \nbackpropagating a gradient of zeros will typically yield zeros all the way to the input.\nIn this respect, modules to avoid in encapsulating inside a  MaskZero  are  AbsractRecurrent  \ninstances as the flow of gradients between different time-steps internally. \nInstead, call the  AbstractRecurrent.maskZero  method\nto encapsulate the internal  recurrentModule . \n \n TrimZero \n WARNING : only use this module if your input contains lots of zeros. \nIn almost all cases,  MaskZero  will be faster, especially with CUDA. \n Ref. A :  TrimZero: A Torch Recurrent Module for Efficient Natural Language Processing \n The usage is the same with  MaskZero . \n lua\nmz = nn.TrimZero(module, nInputDim) \n The only difference from  MaskZero  is that it reduces computational costs by varying a batch size, if any, for the case that varying lengths are provided in the input. \nNotice that when the lengths are consistent,  MaskZero  will be faster, because  TrimZero  has an operational cost.  \n In short, the result is the same with  MaskZero 's, however,  TrimZero  is faster than  MaskZero  only when sentence lengths is costly vary. \n In practice, e.g. language model,  TrimZero  is expected to be faster than  MaskZero  about 30%. (You can test with it using  test/test_trimzero.lua .) \n \n LookupTableMaskZero \n This module extends  nn.LookupTable  to support zero indexes. Zero indexes are forwarded as zero tensors. \n lua\nlt = nn.LookupTableMaskZero(nIndex, nOutput) \n The  output  Tensor will have each row zeroed when the commensurate row of the  input  is a zero index.  \n This lookup table makes it possible to pad sequences with different lengths in the same batch with zero vectors. \n \n MaskZeroCriterion \n This criterion zeroes the  err  and  gradInput  rows of the decorated criterion \nfor commensurate  input  rows which are tensors of zeros. \n lua\nmzc = nn.MaskZeroCriterion(criterion, nInputDim) \n The  gradInput  Tensor (or table thereof) of the decorated  criterion \nwill have each row (samples) zeroed when the commensurate row of the  input  \nis a tensor of zeros. The  err  will also disregard such zero rows. \n The  nInputDim  argument must specify the number of non-batch dims \nin the first Tensor of the  input . In the case of an  input  table,\nthe first Tensor is the first one encountered when doing a depth-first search. \n This decorator makes it possible to pad sequences with different lengths in the same batch with zero vectors. \n \n SeqReverseSequence \n lua\nreverseSeq = nn.SeqReverseSequence(dim) \n Reverses an input tensor on a specified dimension. The reversal dimension can be no larger than three. \n Example:\n```\ninput = torch.Tensor({{1,2,3,4,5}, {6,7,8,9,10}})\nreverseSeq = nn.SeqReverseSequence(1)\nprint(reverseSeq:forward(input)) \n Gives us an output of torch.Tensor({{6,7,8,9,10},{1,2,3,4,5}})\n``` \n \n SequencerCriterion \n This Criterion is a  decorator : \n lua\nc = nn.SequencerCriterion(criterion, [sizeAverage])   \n Both the  input  and  target  are expected to be a sequence, either as a table or Tensor. \nFor each step in the sequence, the corresponding elements of the input and target \nwill be applied to the  criterion .\nThe output of  forward  is the sum of all individual losses in the sequence. \nThis is useful when used in conjunction with a  Sequencer . \n If  sizeAverage  is  true  (default is  false ), the  output  loss and  gradInput  is averaged over each time-step. \n \n RepeaterCriterion \n This Criterion is a  decorator : \n lua\nc = nn.RepeaterCriterion(criterion)   \n The  input  is expected to be a sequence (table or Tensor). A single  target  is \nrepeatedly applied using the same  criterion  to each element in the  input  sequence.\nThe output of  forward  is the sum of all individual losses in the sequence.\nThis is useful for implementing models like  RCNNs ,\nwhich are repeatedly presented with the same target.""]",['repository_count'],Expedia Group,https://avatars.githubusercontent.com/u/3520613?u=afd249ee477fe1c60fcb0f063f58300aa20435ef&v=4,False,2016-10-19 14:28:14,837,285,3,2013-02-09T18:54:43Z,2021-06-11T19:24:54Z,,False,False,False,False,False,True,False,0,"{'Lua': 13, 'Python': 6, 'CMake': 1, 'C++': 2, 'Cuda': 3, 'C': 1}",10,"Montreal, Canada",Data Governance,18,0.31,0.55,0.04,0
