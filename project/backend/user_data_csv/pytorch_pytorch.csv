username,fullName,bio,email,repository_count,company,avatar_url,isHireable,star_time,followers,following,organizations,repositories,createdAt,updatedAt,twitterUsername,isGitHubStar,isCampusExpert,isDeveloperProgramMember,isSiteAdmin,isViewer,anyPinnableItems,viewerIsFollowing,sponsors,primary_language,yearsofExperience,location,domainofExpertise
alexbw,Alex Wiltschko,,alex.bw@gmail.com,"['Sampling Inference for Bayesian HSMMs and HMMs \n This is a Python library for approximate unsupervised sampling inference in\nBayesian Hidden Markov Models (HMMs) and explicit-duration Hidden semi-Markov\nModels (HSMMs), focusing on the Bayesian Nonparametric extensions, the HDP-HMM\nand HDP-HSMM, via the weak-limit approximation. \n There are also some plugins to extend the functionality: \n \n factorial models \n autoregressive models \n collapsed HDP sampling inference . \n \n The inference can be run in parallel over multiple cores and/or multiple\nmachines (even on EC2!) using  ipython \'s\nexcellent  ipython.parallel  module. Someday I might even document how to do\nit! \n Installing \n You can clone this library and its dependencies with \n git clone --recursive git://github.com/mattjj/pyhsmm.git \n The library depends on  numpy ,  scipy , and, for visualization,  matplotlib . \n Disabling assertions may speed things up; to disable assertions, run your\nCPython interpreter with the  -O  flag. \n A Simple Demonstration \n Here\'s how to draw from the HDP-HSMM posterior over HSMMs given a sequence of\nobservations. (The same example, along with the code to generate the synthetic\ndata loaded in this example, can be found in  examples/basic.py .) \n Let\'s say we have some 2D data in a data.txt file: \n bash\n head -5 data.txt\n-3.711962552600095444e-02 1.456401745267922598e-01\n7.553818775915704942e-02 2.457422192223903679e-01\n-2.465977987699214502e+00 5.537627981813508793e-01\n-7.031638516485749779e-01 1.536468304146855757e-01\n-9.224669847039665971e-01 3.680035337673161489e-01 \n In Python, we can plot the data in a 2D plot, collapsing out the time dimension: \n ```python\nimport numpy as np\nfrom matplotlib import pyplot as plt \n data = np.loadtxt(\'data.txt\')\nplt.plot(data[:,0],data[:,1],\'kx\')\n``` \n \n We can also make a plot of time versus the first principal component: \n python\nfrom pyhsmm.util.plot import pca_project_data\nplt.plot(pca_project_data(data,1)) \n \n To learn an HSMM, we\'ll use  pyhsmm  to create an  HSMM  instance using some\nreasonable hyperparameters. We\'ll ask this model to infer the number of states\nas well (since an HDP-HSMM is instantiated by default), so we\'ll give it an\n Nmax  parameter: \n ```python\nimport pyhsmm\nimport pyhsmm.basic.distributions as distributions \n obs_dim = 2\nNmax = 25 \n obs_hypparams = {\'mu_0\':np.zeros(obs_dim),\n                \'sigma_0\':np.eye(obs_dim),\n                \'kappa_0\':0.3,\n                \'nu_0\':obs_dim+5}\ndur_hypparams = {\'alpha_0\':2*30,\n                 \'beta_0\':2} \n obs_distns = [distributions.Gaussian( obs_hypparams) for state in range(Nmax)]\ndur_distns = [distributions.PoissonDuration( dur_hypparams) for state in range(Nmax)] \n posteriormodel = pyhsmm.models.HSMM(\n        alpha=6.,gamma=6., # better to sample over these; see concentration-resampling.py\n        init_state_concentration=6., # pretty inconsequential\n        obs_distns=obs_distns,\n        dur_distns=dur_distns,\n        trunc=60) # duration truncation speeds things up when it\'s possible\n``` \n (The first two arguments set the ""new-table"" proportionality constant for the\nmeta-Chinese Restaurant Process and the other CRPs, respectively, in the HDP\nprior on transition matrices. For this example, they really don\'t matter at\nall, but on real data it\'s much better to infer these parameters, as in\n examples/concentration_resampling.py .) \n The  trunc  parameter is an optional argument that can speed up inference: it\nsets a truncation limit on the maximum duration for any state. If you don\'t\npass in the  trunc  argument, no truncation is used and all possible state\nduration lengths are considered. \n Then, we add the data we want to condition on: \n python\nposteriormodel.add_data(data) \n (If we had multiple observation sequences to learn from, we could add them to the\nmodel just by calling  add_data()  for each observation sequence.) \n Now we run a resampling loop. For each iteration of the loop, all the latent\nvariables of the model will be resampled by Gibbs sampling steps, including the\ntransition matrix, the observation means and covariances, the duration\nparameters, and the hidden state sequence. We\'ll also copy some samples so that\nwe can plot them. \n python\nmodels = []\nfor idx in progprint_xrange(150):\n    posteriormodel.resample_model()\n    if (idx+1) % 10 == 0:\n        models.append(copy.deepcopy(posteriormodel)) \n Now we can plot our saved samples: \n python\nfig = plt.figure()\nfor idx, model in enumerate(models):\n    plt.clf()\n    model.plot()\n    plt.gcf().suptitle(\'HDP-HSMM sampled after %d iterations\' % (10*(idx+1)))\n    plt.savefig(\'iter_%.3d.png\' % (10*(idx+1))) \n \n I generated these data from an HSMM that looked like this: \n \n So the posterior samples look pretty good! \n Speed \n HSMMs constitute a much more powerful model class than plain-old HMMs, and that\nenhanced power comes with a computational price: each sampling iteration for an\nHSMM is much slower than that of an HMM. But that price is often worthwhile if\nyou want to place priors on state durations or have the model learn duration\nstructure present in the data. (In the example, strong duration structure is\nwhat made the inference algorithm latch onto the correct explanation so\neasily.) In addition, the increased cost of each iteration often pays for\nitself, since HSMM samplers empirically seem to take fewer iterations to\nconverge than comparable HMM samplers. \n Using my nothing-special i7-920 desktop machine and a NumPy/SciPy built against\nIntel\'s MKL BLAS (which generally outperforms ATLAS for vectorized operations)\nalong with the Eigen-backed classes, here\'s how long the demo iterations took: \n $ python examples/hsmm.py\n.........................  [  25/100,    0.05sec avg,    3.95sec ETA ]\n.........................  [  50/100,    0.05sec avg,    2.64sec ETA ]\n.........................  [  75/100,    0.05sec avg,    1.34sec ETA ]\n.........................  [ 100/100,    0.05sec avg,    0.05sec ETA ]\n   0.05sec avg,    5.21sec total \n Extending the Code \n To add your own observation or duration distributions, implement the interfaces\ndefined in  basic/abstractions.py . Also see the plugins. To get a flavor of\nthe style, see  pybasicbayes . \n Contributors \n Contributions by Chia-ying Lee. \n References \n \n \n Matthew J. Johnson and Alan S. Willsky.  Bayesian Nonparametric Hidden\n  Semi-Markov Models .\n  Journal of Machine Learning Research (JMLR), 14:673–701, February 2013. \n \n \n Matthew J. Johnson and Alan S. Willsky,  The Hierarchical Dirichlet Process\n  Hidden Semi-Markov Model . 26th\n  Conference on Uncertainty in Artificial Intelligence (UAI 2010), Avalon,\n  California, July 2010. \n \n \n bibtex\n@article{johnson2013hdphsmm,\n    title={Bayesian Nonparametric Hidden Semi-Markov Models},\n    author={Johnson, Matthew J. and Willsky, Alan S.},\n    journal={Journal of Machine Learning Research},\n    pages={673--701},\n    volume={14},\n    month={February},\n    year={2013},\n}', 'An analgesic for high-performance audio on iOS and OSX. \n Really fast audio in iOS and Mac OS X using Audio Units is hard, and will leave you scarred and bloody. What used to take days can now be done with just a few lines of code. \n Getting Audio \n objective-c\nNovocaine *audioManager = [Novocaine audioManager];\n[audioManager setInputBlock:^(float *newAudio, UInt32 numSamples, UInt32 numChannels) {\n    // Now you\'re getting audio from the microphone every 20 milliseconds or so. How\'s that for easy?\n    // Audio comes in interleaved, so,\n    // if numChannels = 2, newAudio[0] is channel 1, newAudio[1] is channel 2, newAudio[2] is channel 1, etc.\n}];\n[audioManager play]; \n Playing Audio \n objective-c\nNovocaine *audioManager = [Novocaine audioManager];\n[audioManager setOutputBlock:^(float *audioToPlay, UInt32 numSamples, UInt32 numChannels) {\n    // All you have to do is put your audio into ""audioToPlay"".\n}];\n[audioManager play]; \n Does anybody actually use it? \n Yep. Novocaine is result of three years of work on the audio engine of  Octave ,  Fourier  and  oScope , a powerful suite of audio analysis apps. Please do check them out! \n A thing to note: \n The RingBuffer class is written in C++ to make things extra zippy, so the classes that use it will have to be Objective-C++. Change all the files that use RingBuffer from MyClass.m to MyClass.mm. \n Want some examples? \n Inside of ViewController.mm are a bunch of tiny little examples I wrote. Uncomment one and see how it sounds. \nDo note, however, for examples involving play-through, that you should be using headphones. Having the \nmic and speaker close to each other will produce some gnarly feedback.   \n Want to learn the nitty-gritty of Core Audio? \n If you want to get down and dirty, if you want to get brave and get close to the hardware, I can only point you to the places where I learned how to do this stuff. Chris Adamson and Michael Tyson are two giants in the field of iOS audio, and they each wrote indispensable blog posts ( this is Chris\'s ,  this is Michael\'s ). Also, Chris Adamson now has a  whole gosh-darned BOOK on Core Audio . I would have done unspeakable things to get my hands on this when I was first starting. \n', 'My Code for the Netflix Prize \n I\'m not aware of folks having published their code for the Netflix Prize. Here\'s mine. \nUnder the team name ""Hi!"", I competed alone in college. I did it mostly for fun, and to learn modern machine learning techniques. It was an incredibly valuable, but strenuous, time. Well worth it on all fronts, though.  \nI peaked out at #45 or so, and then dropped out to work on my senior thesis, and came in #145 or so.   \nWhat I learned in the process was that smarter wasn\'t always better -- make an algorithm, and then scale it up, and then make a dozen tweaks to it, and then average all of the results together. That\'s how you climbed the leaderboard.    \n Anyhoo, I haven\'t touched this code in awhile, but perhaps it\'ll be useful to folks interested in competitive data mining. \nSpecifically, the lessons I learned: \n \n Get the raw data into a saved and manageable format  fast . The easier it is to load your data in and start mutating it, the better. \n If doing simple pivots on your data is hard, and slows you down from visualizing whats in your data, spend time making data structures which make that easy.   \n Generalize. Iterate. If you have a method you think will work, but it has a lot of knobs, and you don\'t know the best way to set those knobs, make it easy for you to try  every possible iteration . There is often not a good way to figure out what the  best  approach is. You will have to try many of them in order to build up an intuition. Specifically, that means (for me) a pluggable architecture. If there\'s ten ways to try a particular step, make sure you write your overarching algorithm so that it takes a function that you can pass to it, as opposed to having a method hardwired in the code. That way, you can hotswap all your ideas.   \n Speed is a feature. Of course you make sure it works first. But your goal is to see if something works. If an algorithm takes a day to run, but you can spend five hours making it run in 1/3 of a day, do it. You\'ll be running it over and over again, and you\'ll learn more if you can iterate.  \n \n As for the technical nitty-gritty, everything that\'s speed sensitive is written in Cython, which was the best balance of speed and convenience in 2009. If I were to do it al again, I would use (Numba)[http://github.com/numba/numba].   \n The original data is gone, I believe, but I might have it stored somewhere. I\'ll look for that.  ', ""NURBS - Non Uniform Rational B-Splines. \n This python module is a revival and update of Runar Tenfjord's NURBS toolbox, which itself\nis a port of Mark Spink's SCILAB/MATLAB tolbox to python with help of numpy. \n Dependency \n Python 2.0 or above \nNumPy \nDislin -> optional but recomended \nPyOpenGL -> optional   \n Installation \n Just run   python setup.py install   \n License \n Runar Tenfjord originally relased this package under the GPL Version 2 license,  \nso I suppose that means it has to stay that way.    \n Originally by: Runar Tenfjord, runten@netcom.no \nMinor updates to work with NumPy by Alex Wiltschko  "", 'cuda-tests \n Gotta learn some CUDA stuff', 'pypatent \n Scrape patents from the USPTO', 'airruler \n A ruler. Made of air. ', 'paralleltools \n A summary of parallelizing moderate amounts of work in Python', 'nsgt \n Non-stationary Gabor transforms (GitHub mirror of http://grrrr.org/research/software/nsgt/)', 'IDA \n This code accompanies the publication from the Whitesides lab to Lab on a Chip, concerning automated analysis of red blood cell health using affordable field tests. Specifically, this code implements the automated extraction of scanned AMPS tubes from flatbed scanner images, the distillation of those images into 1D data traces, and then the automated identification of the anemic disease state of the blood in those 1D data traces, as well as the prediction of continuously-varying red blood cell (RBC) parameters. \n Installation \n This code requires Python, as well as some extra 3rd-party libraries. All routines have only been tested on Mac OS, but should work on Linux. No guarantees for Windows.\n1) First, we recommend using the ""Anaconda"" Python distribution, specifically the Python 2.7 version.  Download and install here .\n1) With anaconda installed, you will need one extra library, to read TIFF files.\n pip install imageio \n1) You should be able to run the notebooks now. Inside this code repository, start up an IPython notebook:\n jupyter notebook \n1) You should now be able to browser around the ""extraction"" and ""analysis"" folders, which contain the relevant code. \n Running extraction \n The first step required will be to prepare the raw data of TIFF file scans of 4x3=12 tubes from a flatbed scanner. We will also have to associate metadata for each patient that the blood in a tube was drawn from.\nOur end goal will be a 1D array for each tube, along with the corresponding patient data (anemic state & RBC parameters). \n The extraction algorithm is explained step-by-step in the notebook  extraction/Explaining the Extraction Algorithm.ipynb .\nThe implementation we use in the paper (which also fuses patient metadata with AMPS image data) can be found in  extraction/Data Extraction Pipeline.ipynb . Most of the code in this notebook is specific to the particular Excel file format that we used to record patient metadata, and may have to be largely rewritten for reuse. \n Running analysis \n There are two sets of analyses done in the paper: classification and regression. \n Running classification analysis \n The goal of this analysis is to predict disease state only from the 1D data trace extracted from each AMPS tube. We use logistic regression, a linear classifier, and transform the 1D data representation using PCA to remove redundancy and constrain the input dimensionality of the problem. We also used Bayesian Optimization (bayesopt) to tune the hyperparameters of the problem, including the specific output dimension of PCA, the regularization parameter of logistic regression, and some image preprocessing parameters. Unfortunately, the service we used for bayesopt is no longer available. If you would like to automatically tune these parameters, we recommend either using random search (a surprisingly effective hueristic), or the open source library ""Spearmint"", upon which the now-defunct service we used was based, or products from the company SigOpt, which also implements bayesopt. The file we used to automatically tune these hypers is  classify.py .\nThe best set of hyperparameters is stored in the notebook  Analyzing best classification.ipynb .\nThis notebook examines ROC performance of the classifier for discriminating different anemia types, as well as the effect of centrifugation time of the tube on IDA classification AUC performance. \n Running regression analysis. \n Similar to above, we used a defunct bayesopt service to automatically tune some parameters of this algorithm. The original file is in  regress.py . The original methodology will work well, even without automated tuning.\nThe best set of hyperparameters is stored in the notebook  Analyzing best regression.ipynb . \n License \n See the LICENSE file. It is under a GPL license, and this code may only be used for academic purposes.', ""adabayes \n To do: \n \n [x] Find candidate last layer features (DeCAF, Overfeat) \n [x] Find AlexNet code (5 convolutional layers with max-pooling, then three fully connected layers) \n [X]  Find download script for MNIST dataset \n [X]  Find download script for CIFAR10 dataset \n [x] Fork torch-dist repo \n [x] Update torch-dist repo for OS X 10.10 install \n [x] Add new required submodules to distro (nnx, iTorch, ccn2, cunnx, cudnn, sdl2, cutorch) \n [x] Figure out model serialization and loading \n Built-in model serialization.  Loading  and  saving . \n There is also facebook's  Thrift library , which I haven't seen many examples for. \n \n \n [ ] Get the data out of the  DataSource  models that we're using \n [ ] Train and Whetlab a net on MNIST (to get the whole workflow going) \n [ ] Train and Whetlab a net on CIFAR10 \n [ ] Train and Whetlab a net on STL10 \n [ ] Build dumb ensemble on CIFAR10 \n [ ] Build dumb ensemble on STL10 \n [ ] Grok boosting criterion \n [ ] Whetlab a net with progressive ensembling on CIFAR10 \n [ ] Whetlab a net with progressive ensembling on STL10 \n [ ] Whetlab a net with progressive ensembling on the last-layer features of ImageNet \n [ ] Whetlab a net with progressive ensembling and tuned class weights on CIFAR10 \n [ ] Whetlab a net with progressive ensembling and tuned class weights on STL10 \n [ ] Extract last-layer ImageNet features \n [ ] Host last-layer ImageNet features \n [ ] Build download script for last-layer ImageNet features \n [ ] Train and Whetlab a net on last-layer features on ImageNet \n [ ] Build a dumb ensemble on ImageNet \n [ ] Whetlab a net with progressive ensembling and tuned class weights on the last-layer features of ImageNet \n \n Collecting some resources \n AlexNet:\nhttps://github.com/soumith/convnet-benchmarks/blob/master/torch7/imagenet_winners/alexnet.lua \n I think this is NiN:\nhttps://github.com/soumith/convnet-benchmarks/blob/master/torch7/imagenet_winners/googlenet.lua \n OverFeat:\nhttps://github.com/facebook/fbcunn/blob/master/examples/imagenet/models/overfeat_cunn.lua \n Multiclass criterion:\nhttps://github.com/torch/nn/blob/master/doc/criterion.md#classnllcriterion \n Some other interesting nets:\nhttps://github.com/culurciello/profiling \n Bunch of demos, not all nets:\nhttps://github.com/torch/demos \n Overfeat features:\nhttp://cilvr.nyu.edu/doku.php?id=software:overfeat:start \n Decaf ImageNet submission:\nhttps://github.com/UCB-ICSI-Vision-Group/decaf-release/wiki/imagenet \n Some other net implementations:\nhttps://github.com/eladhoffer/ImageNet-Training"", ""Conda recipes for installing packages from the Torch ecosystem. \n NOTE: No longer maintained. \n To install packages \n ``` \n Install anaconda if you don't have it (instructions here for OS X) \n wget http://repo.continuum.io/miniconda/Miniconda-latest-MacOSX-x86_64.sh\nsh Miniconda-latest-MacOSX-x86_64.sh -b -p $HOME/anaconda \n Add anaconda to your $PATH \n export PATH=$HOME/anaconda/bin:$PATH \n Install Lua & Torch \n conda install lua=5.2 lua-science -c alexbw \n Available versions of Lua: 2.0, 5.1, 5.2, 5.3 \n 2.0 is LuaJIT \n ``` \n To build packages \n ``` \n Install anaconda if you don't have it (instructions here for OS X) \n wget http://repo.continuum.io/miniconda/Miniconda-latest-MacOSX-x86_64.sh\nsh Miniconda-latest-MacOSX-x86_64.sh -b -p $HOME/anaconda \n Add anaconda to your $PATH \n export PATH=$HOME/anaconda/bin:$PATH \n Get the newest version of conda, as well as some conda build tools \n conda update conda -y\nconda install conda-build anaconda-client -y \n Build all packages \n sh build_all.sh \n Ideally, all you have to do to install everything is this \n conda install lua=5.2 lua-science\n``` \n TODO:\n - https://github.com/AlexMili/torch-dataframe\n - https://github.com/twitter/torch-ipc\n - https://github.com/twitter/torch-distlearn\n - https://github.com/twitter/torch-dataset \n Resources: \n \n \n Making packages relocatable (LuaJIT hard-codes path) \n \n \n Build instructions for luarocks \n \n \n Build instructions for Lua \n \n \n Patching files with git diffs is finicky \n \n \n Linking against readline  (need a few extra flags, and link against -lncursesw, not -lncurses) \n \n \n Upgrading old Debian .  Also this . \n \n \n Misc notes:\n metadata:ns_cfg() — defines for YAML directives\nmain_build: —\xa0defines version numbers \nenviron:get_lua_include_dir() —\xa0uses version numbers to locate the include directory\nconfig:Config._get_lua —\xa0uses version numbers to locate the binary\nThis is where the linked package name is converted into what is used"", ""validata \n Continuous integration for your data \n We do continuous integration on code. Why not data?\nValidata is a small package to run basic sanity checks on your data.\nI haven't found anything that aggregates all of these checks and tricks in one place. \n There is only one method which is exposed,  check(data,labels)  (optionally taking data or labels).\nIf any data check fails, it throws a well-named error, as well as hints for how you might fix the problem -- data covariance matrix ill-conditioned? Try whitening. \n Initially, this will be a Python/NumPy only package running basic checks, but hopefully it becomes a resource of data sanity and sanitation checks.\nStill very much a work in progress. \n Examples (some implemented, some not) include: \n \n If your labels are one-hot, are you using all slots? \n Is the covariance matrix of your data ill-conditioned? \n Do you have any constant variables? \n Can you train a classifier to distinguish train and test data, using whether they are in train or test as a label? Indicates different data distributions. \n If you're using integer labels, are the unique labels contiguous? \n Do you have just one unique label? \n Is the data under different labels statistically separable? \n If you have an old dataset and a new dataset (or two halves of the same dataset), is the distribution of each dimension stationary? Check for divergence with a KS test. \n What else? I end up applying these tricks in a very ad hoc fashion, whenever a subtle bug pops up, and not rigorously before each project I tackle. I'd like to stuff all these tricks in one place, and run them like a unit test, or a continuous integration test, on data that I start working with. \n \n Should probably also think about engarde"", ""bayarea-dl-summerschool \n Torch notebooks and slides for the Bay Area Deep Learning Summer School \n Installation Instructions \n Install anaconda if you don't have it (instructions here for OS X) \n wget http://repo.continuum.io/miniconda/Miniconda-latest-MacOSX-x86_64.sh\nsh Miniconda-latest-MacOSX-x86_64.sh -b -p $HOME/anaconda \n Add anaconda to your $PATH \n export PATH=$HOME/anaconda/bin:$PATH \n Install Lua & Torch \n ```\nconda install lua=5.2 lua-science -c alexbw \n Although, you could install other Lua versions like 2.0 (LuaJIT), 5.1, 5.2 and 5.3 \n ``` \n Clone this repository and start the notebook server \n ```\ngit clone https://github.com/alexbw/bayarea-dl-summerschool.git\ncd bayarea-dl-summerschool\nitorch notebook \n Will open a browser tab, then you can navigate to the notebooks \n ```""]",['repository_count'],Google,https://avatars.githubusercontent.com/u/161935?u=f722a589176ab3dce6285bcce174117e3c103ec3&v=4,False,2016-08-15 18:39:55,396,332,0,2009-12-04T14:50:40Z,2022-08-23T18:31:52Z,,False,False,False,False,False,True,False,0,"{'C': 5, 'Objective-C': 5, 'C++': 1, 'Python': 14, 'JavaScript': 3, 'C#': 1, 'Shell': 2, 'Jupyter Notebook': 5, 'Lua': 4}",3,"Boston, MA",Data Warehousing,60,0.99,0.47,0.89,0
iassael,Yannis Assael,Research Scientist,iassael@gmail.com,"['DEARanking \n Proposing a hybrid DEA/Polynomial Interpolation (DEA/PI) algorithm for the raking of protected areas: An application in Greece', 'Find The Word (Τηλεκύβος Cheat) \n Find the greek word from the given letters. Application to cheat and solve games like ""Τηλεκύβος"" and ""Βρες τη λέξη""', 'csoxcal \n Oxford University, Computer Science Calendar Filter for Google Calendar use', 'A Hybrid Parallel Implementation of the Aho-Corasick and Wu-Manber Algorithms Using NVIDIA CUDA and MPI Evaluated on a Biological Sequence Database. \n Charalampos S. Kouzinopoulos, Yannis M. Assael, Themistoklis K. Pyrgiotis, Konstantinos G. Margaritis \n Multiple matching algorithms are used to locate the occurrences of patterns from a finite pattern set in a large input string. Aho-Corasick and Wu-Manber, two of the most well known algorithms for multiple matching require an increased computing power, particularly in cases where large-size datasets must be processed, as is common in computational biology applications. Over the past years, Graphics Processing Units (GPUs) have evolved to powerful parallel processors outperforming Central Processing Units (CPUs) in scientific calculations. Moreover, multiple GPUs can be used in parallel, forming hybrid computer cluster configurations to achieve an even higher processing throughput. This paper evaluates the speedup of the parallel implementation of the Aho-Corasick and Wu-Manber algorithms on a hybrid GPU cluster, when used to process a snapshot of the Expressed Sequence Tags of the human genome and for different problem parameters. \n Links \n arXiv pre-print \n Bibtex \n @article{kouzinopoulos2015hybrid,\n  title={A Hybrid Parallel Implementation of the Aho-Corasick and Wu-Manber Algorithms Using NVIDIA CUDA and MPI Evaluated on a Biological Sequence Database},\n  author={Kouzinopoulos, Charalampos S. and Assael, Yannis M. and Pyrgiotis, Themistoklis K. and Margaritis, Konstantinos G.},\n  journal={International Journal on Artificial Intelligence Tools},\n  volume={24},\n  number={1},\n  pages={1540001},\n  year={2015},\n  publisher={World Scientific}\n} \n License \n Code licensed under the GNU General Public License v3.0.', 'RKHS Function \n Description \n Synthetic heteroscedastic 1D function generated from 2 Squared Exponential kernels for Bayesian Optimization method evaluation tasks  \n Input Space \n x ∈ [0, 1] \n Global Maximum \n x=0.89235, f(x)=5.73839 \n Authors \n Copyright (C) 2014 Ziyu Wang, John-Alexander Assael, Nando de Freitas\npublished under GPLv3 license \n Screenshot \n', 'Decomposition module for Torch7 \n \n \n Principal Component Analysis (PCA) \n \n \n Whitened Principal Component Analysis (W-PCA) \n \n \n Linear Discriminant Analysis (LDA) \n \n \n Locality Preserving Projections (LPP) \n \n \n Neighbourhood Preserving Projections (NPP) \n \n \n Fast Independent Component Analysis (FastICA) \n \n \n by John-Alexander Assael \n http://www.johnassael.com \n https://github.com/iassael/torch7-decomposition \n Installation \n Clone this repository or download the source code. \n Usage \n Call  decomposition = require ""decomposition"" \nand then any of the following: \n \n \n decomposition.pca(x) , \n \n \n decomposition.lda(x, y) , \n \n \n decomposition.lpp(x) , \n \n \n decomposition.npp(x) , \n \n \n decomposition.fastica(x) . \n \n \n Alternativly, you can use iTorch notebook and open  decomposition.ipynb . \n Contributing \n \n Fork it! \n Create your feature branch:  git checkout -b my-new-feature \n Commit your changes:  git commit -am \'Add some feature\' \n Push to the branch:  git push origin my-new-feature \n Submit a pull request \n \n Notes \n The implementations were developed in terms of learning and may not be optimal. \n License \n Copyright (C) 2015 John-Alexander Assael (www.johnassael.com)\nhttps://github.com/iassael/torch7-decomposition \n The MIT License (MIT) \n Permission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the ""Software""), to deal in\nthe Software without restriction, including without limitation the rights to\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\nof the Software, and to permit persons to whom the Software is furnished to do\nso, subject to the following conditions: \n The above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software. \n THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.', 'Torch7 impementation of: \n Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images \n by Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, Martin Riedmiller (http://arxiv.org/abs/1506.07365) \n Implemented by John-Alexander M. Assael (iassael@gmail.com) and Marc P. Deisenroth. \n The MIT License (MIT)\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the ""Software""), to deal in\nthe Software without restriction, including without limitation the rights to\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\nof the Software, and to permit persons to whom the Software is furnished to do\nso, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n', 'Regularization of Neural Networks using DropConnect \n Li Wan, Matthew Zeiler, Sixin Zhang, Yann LeCun, Rob Fergus \n Dept. of Computer Science, Courant Institute of Mathematical Science, New York University \n Torch7 implementation by John-Alexander M. Assael', 'Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) \n Djork-Arné Clevert, Thomas Unterthiner, Sepp Hochreiter \n Torch7 implementation by John-Alexander M. Assael', 'Deep Exploration via Bootstrapped DQN \n Ian Osband, Charles Blundell, Alexander Pritzel, Benjamin Van Roy \n Usage \n nn.Bootstrap(nn.Linear(size_in, size_out), 10, 0.08) \n Implemented by Yannis M. Assael (www.yannisassael.com)', 'Recurrent Batch Normalization \n Batch-Normalized LSTMs \n Tim Cooijmans, Nicolas Ballas, César Laurent, Çağlar Gülçehre, Aaron Courville \n http://arxiv.org/abs/1603.09025 \n Usage \n local rnn = LSTM(input_size, rnn_size, n, dropout, bn) \n n = number of layers (1-N) \n dropout = probability of dropping a neuron (0-1) \n bn = batch normalization (true, false) \n Example \n https://github.com/iassael/char-rnn \n Performance \n Validation scores on char-rnn with default options \n \n Implemented in Torch by Yannis M. Assael (www.yannisassael.com)', 'Grid World DQN using torch7 \n This is a naive DQN example implemented in torch7 to help future research. \n The environment is based on  rlenvs  of  Kaixhin  and the model makes use of ""Increasing the Action Gap"" ( http://arxiv.org/abs/1512.04860 ). \n Implemented by Yannis M. Assael ( yannisassael.com )', 'Single pendulum Deterministic Policy Gradient example using torch7 \n Continuous Control with Deep Reinforcement Learning \n Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra \n http://arxiv.org/abs/1509.02971 \n Dependecies \n luarocks install Math-RungeKutta\nluarocks install csvigo\nluarocks install image\nluarocks install hdf5 \n Implemented by Yannis M. Assael ( yannisassael.com )', 'Using deep Q-learning to understand the tax evasion behavior of risk-averse firms \n Links \n \n arXiv preprint \n Expert Systems with Applications \n \n Execution \n $ # Requirements: torch\n$ th tax-dqn.lua \n Bibtex \n @article{goumagias2018using,\n  title={Using deep Q-learning to understand the tax evasion behavior of risk-averse firms},\n  author={Goumagias, Nikolaos D and Hristu-Varsakelis, Dimitrios and Assael, Yannis M},\n  journal={Expert Systems with Applications},\n  volume={101},\n  pages={258--270},\n  year={2018},\n  publisher={Elsevier}\n}\n \n License \n Code licensed under the Apache License v2.0', 'Restoring ancient text using deep learning \n A case study on Greek epigraphy \n Yannis Assael * , Thea Sommerschield * , Jonathan Prag \n \n Ancient History relies on disciplines such as Epigraphy, the study of ancient inscribed texts, for evidence of the recorded past. However, these texts, ""inscriptions"", are often damaged over the centuries, and illegible parts of the text must be restored by specialists, known as epigraphists.\nThis work presents a novel assistive method for providing text restorations using deep neural networks.\nTo the best of our knowledge, Pythia is the first ancient text restoration model that recovers missing characters from a damaged text input.\nIts architecture is carefully designed to handle long-term context information, and deal efficiently with missing or corrupted character and word representations. \nTo train it, we wrote a non-trivial pipeline to convert PHI, the largest digital corpus of ancient Greek inscriptions, to machine actionable text, which we call PHI-ML.\nOn PHI-ML, Pythia\'s predictions achieve a 30.1% character error rate, compared to the 57.3% of human epigraphists. Moreover, in 73.5% of cases the ground-truth sequence was among the Top-20 hypotheses of Pythia, which effectively demonstrates the impact of such an assistive method on the field of digital epigraphy, and sets the state-of-the-art in ancient text restoration. \n \n \nPythia-Bi-Word processing the phrase μηδέν ἄγαν (mēdén ágan) ""nothing in excess"", a fabled maxim inscribed on Apollo\'s temple in Delphi. The letters ""γα"" are missing, and annotated with ""?"". Since word ἄ??ν contains missing characters, its embedding is treated as unknown (""unk""). The decoder outputs correctly ""γα"".\n \n References \n \n arXiv pre-print \n EMNLP-IJCNLP 2019 \n Digital Classicist Wiki \n DeepMind research blog \n University of Oxford news blog \n \n When using any of this project\'s source code, please cite:\n @inproceedings{assael2019restoring,\n  title={Restoring ancient text using deep learning: a case study on {Greek} epigraphy},\n  author={Assael, Yannis and Sommerschield, Thea and Prag, Jonathan},\n  booktitle={Empirical Methods in Natural Language Processing},\n  pages={6369--6376},\n  year={2019}\n} \n Pythia online \n To aid further research in the field we created an online interactive python notebook, where researchers can query one of our models to get text restorations and visualise the attention weights. \n \n Google Colab \n \n Pythia offline \n The following snippets provide references for regenerating PHI-ML and training new models offline. \n Dependencies \n pip install -r requirements.txt && \\\npython -m nltk.downloader punkt \n PHI-ML dataset generation \n ``` \n Download PHI (this will take a while) \n python -c \'import pythia.data.phi_download; pythia.data.phi_download.main()\' \n Process and generate PHI-ML \n python -c \'import pythia.data.phi_process; pythia.data.phi_process.main()\'\n```\nPreprocessed PHI-ML uploaded by @Holger.Danske800:  link \n Training \n python -c \'import pythia.train; pythia.train.main()\' \n Evaluation \n python -c \'import pythia.test; pythia.test.main()\' --load_checkpoint=""your_model_path/"" \n Docker execution \n ./build.sh\n./run.sh <GPU_ID> python -c \'import pythia.train; pythia.train.main()\' \n License \n Apache License, Version 2.0 \n \n \nDamaged inscription: a decree concerning the Acropolis of Athens (485/4 BCE).  IG  I 3  4B. (CC BY-SA 3.0, WikiMedia)\n', 'Vivechrom RGB color matcher \n Yannis Assael ( www.assael.gr ) \n Description \n This colaboratory notebook is used to find the closest vivechrom.gr paint code given an RGB color. \n Instructions \n \n Open this  Google Colab Notebook . \n To edit and run the code, you need to click \'Open in Playground Mode\' if you can see it at the top of the page, or save a local copy somewhere on your computer. \n The cells need to be run one by one by pressing shift+enter or by using the play button. \n Fill the  target_color_rgb  value with your target RGB color. \n Run the cell to get suggestions ordered by color similarity. \n \n License \n ```\nCopyright 2020 Yannis Assael \n Licensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at \n http://www.apache.org/licenses/LICENSE-2.0\n \n Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```', '\n \n \n Restoring and attributing ancient texts using deep neural networks \n Yannis Assael 1,* , Thea Sommerschield 2,3,* , Brendan Shillingford 1 , Mahyar Bordbar 1 , John Pavlopoulos 4 ,\nMarita Chatzipanagiotou 4 , Ion Androutsopoulos 4 , Jonathan Prag 3 , Nando de Freitas 1 \n 1  DeepMind, United Kingdom \n 2  Ca’ Foscari University of Venice, Italy \n 3  University of Oxford, United Kingdom \n 4  Athens University of Economics and Business, Greece \n *  Authors contributed equally to this work \n \n \n Ancient History relies on disciplines such as Epigraphy, the study of inscribed\ntexts known as ""inscriptions"", for evidence of the thought, language, society\nand history of past civilizations. However, over the centuries many inscriptions\nhave been damaged to the point of illegibility, transported far from their\noriginal location, and their date of writing is steeped in uncertainty. We\npresent Ithaca, the first Deep Neural Network for the textual restoration,\ngeographical and chronological attribution of ancient Greek inscriptions. Ithaca\nis designed to assist and expand the historian’s workflow: its architecture\nfocuses on collaboration, decision support, and interpretability. \n \n \n Restoration of damaged inscription: this inscription ( IG  I 3  4B) records a decree concerning the Acropolis of Athens and dates 485/4 BCE. (CC BY-SA 3.0, WikiMedia) \n \n While Ithaca alone achieves 62% accuracy when restoring damaged texts, as soon\nas historians use Ithaca their performance leaps from 25% to 72%, confirming\nthis synergistic research aid’s impact. Ithaca can attribute inscriptions to\ntheir original location with 71% accuracy and can date them with a distance of\nless than 30 years from ground-truth ranges, redating key texts of Classical\nAthens and contributing to topical debates in Ancient History. This work shows\nhow models like Ithaca can unlock the cooperative potential between AI and\nhistorians, transformationally impacting the way we study and write about one of\nthe most significant periods in human history. \n \n \n Ithaca\'s architecture processing the phrase ""δήμο το αθηναίων"" (""the people of Athens""). The first 3 characters of the phrase were hidden and their restoration is proposed. In tandem, Ithaca also predicts the inscription’s region and date. \n \n References \n \n Nature article \n DeepMind blog \n \n When using any of this project\'s source code, please cite: \n @article{asssome2022restoring,\n  title = {Restoring and attributing ancient texts using deep neural networks},\n  author = {Assael*, Yannis and Sommerschield*, Thea and Shillingford, Brendan and Bordbar, Mahyar and Pavlopoulos, John and Chatzipanagiotou, Marita and Androutsopoulos, Ion and Prag, Jonathan and de Freitas, Nando},\n  doi = {10.1038/s41586-022-04448-z},\n  journal = {Nature},\n  year = {2022}\n} \n Ithaca inference online \n To aid further research in the field we created an online interactive python notebook, where researchers can query one of our trained models to get text restorations, visualise attention weights, and more. \n \n Ithaca Interactive Interface \n Google Colab for using Ithaca for your research \n \n Ithaca inference offline \n Advanced users who want to perform inference using the trained model may want\nto do so manually using the  ithaca  library directly. \n First, to install the  ithaca  library and its dependencies, run:\n sh\npip install . \n Then, download the model via\n sh\ncurl --output checkpoint.pkl https://storage.googleapis.com/ithaca-resources/models/checkpoint_v1.pkl \n An example of using the library can be run via\n sh\npython inference_example.py --input_file=example_input.txt \nwhich will run restoration and attribution on\nthe text in  example_input.txt . \n To run it with different input text, run\n```sh\npython inference_example.py --input=""..."" \n or using text in a UTF-8 encoded text file: \n python inference_example.py --input_file=some_other_input_file.txt\n``` \n The restoration or attribution JSON can be saved to a file:\n sh\npython inference_example.py \\\n  --input_file=example_input.txt \\\n  --attribute_json=attribute.json \\\n  --restore_json=restore.json \n For full help, run:\n sh\npython inference_example.py --help \n Dataset generation \n Ithaca was trained on The Packard Humanities Institute’s\n"" Searchable Greek Inscriptions "" public\ndataset. The processing workflow for generating the machine-actionable text and\nmetadata, as well as further details on the train, validation and test splits\nare available at  I.PHI dataset . \n Training Ithaca \n See  train/README.md  for instructions. \n License \n Apache License, Version 2.0']",['repository_count'],DeepMind,https://avatars.githubusercontent.com/u/216211?u=52e4429ac6db432677480a6129748fb6e5350b85&v=4,True,2016-08-15 19:14:32,266,40,0,2010-03-05T00:49:40Z,2022-07-29T17:00:44Z,,False,False,False,False,False,True,False,0,"{'JavaScript': 1, 'Java': 2, 'PHP': 1, 'C': 1, 'Python': 5, 'Lua': 11, 'Jupyter Notebook': 5}",8,London,Data Modeling,92,0.11,0.73,0.08,0
bshillingford,Brendan Shillingford,,,"['id3tag-fix', 'codehackathon2014 \n Team members:\nBrendan Shillingford\nDaisy Shih\nKevin Lim \n http://data.gc.ca/eng/canadian-open-data-experience-code', 'pycmdline-template \n Template for making your one-off Python utilities more user-friendly. \n Add  @arg  and  @cmdline  decorators to a main function as below; this provides an easier interface to the  logging  and  optparse  modules. \n Examples \n Example 1: simple program \n This program accepts 2 arguments: one a flag passed as  --count=COUNT , or  -n COUNT ; another is a position argument, listed without a dash-prefixed flag name before it; and a default help flag  --help / -h  printing a usage page. \n The arguments to the decorators are the same as in  optparse . \n ```python \n !/usr/bin/env python \n from simpleargs import cmdline, arg \n @arg(\'argument\', help=""pass ARG to program"", metavar=""ARG"") # simple positional argument\n@arg(\'--count\', \'-n\', help=""number of bars to foo"", type=int, default=1)\n@cmdline(description=""This program foos bars a given number of times."")\ndef main(argument, count): \n print(""Hello there."")\n\nprint(""Your Argument: %s"" % argument)\n\nfor i in range(count):\n    print(""Fooing bar #%d"" % i)\nprint(""Fooed %d bars."" % count)\n \n if  name  == \' main \':\n    main()\n``` \n Example 2: simple program + logging \n ``` \n !/usr/bin/env python \n import logging as L\nfrom simpleargs import cmdline, arg, loglevel \n @arg(\'argument\', help=""pass ARG to program"", metavar=""ARG"") # simple positional argument\n@arg(\'--count\', \'-n\', type=int, default=1)\n@loglevel()                                                 # if on, log level DEBUG, else INFO\n@cmdline(description=""This program foos bars a given number of times."")\ndef main(argument, count): \n print ""Hello there.""\n\nL.info(""You passed an argument."")\nL.debug(""Your Argument: %s"" % argument)\n\nfor i in range(count):\n    L.debug(""Fooing bar #%d"" % i)\nL.info(""Fooed %d bars."" % count)\n \n if  name  == \' main \':\n    main()\n```', 'Starbucks WiFi autologin \n Automatically accept ToS when connecting. \n Barely tested. \n This tool is purely for educational purposes only, and should \nnot be used in a real situation. \n Dependencies: \n \n Python 2 \n requests \n BeautifulSoup \n \n Installation: \n Can just execute starbuckswifi.py directly, but install if you want. \n Install via:\n``` \n python2 setup.py install \n When you connect to an accesspoint, run \n$ python2 -m starbuckswifi\n```', 'REST and MQTT for IFTTT \n What\'s IFTTT? \n IFTTT is a service that enables users to  connect different web applications  (e.g., Facebook, Evernote, Weather, Dropbox, etc.) together through simple conditional statements known as ""Recipes"". [Source: Wikipedia] \n A recipe is a (trigger, action) pair. However, IFTTT doesn\'t support fully custom actions. However, it does support publishing to any Wordpress blog. This app emulates Wordpress\'s XML-RPC API so that you can send a  GET / POST / PUT  REST request, or publish a MQTT message, all by choosing the ""create post"" action in IFTTT. \n Install \n Simply clone this repository and upload as a Heroku app, or host it however you want. \n For local testing, you can use  flaskrun.py . If desired, use a  virtualenv , and install all the packages in  requirements.txt . \n Usage \n Set the  title  and  body  of the post. Everything else is ignored. \n Examples: \n \n title =  GET http://httpbin.org/get?some=stuff&here \n title =  POST http://example.com/something , body =  whatever data you want to post  (similarly for  PUT ) \n title =  MQTT-PUB mqtt://iot.eclipse.org/topic/name/goes/here?qos=1&retain=T , body =  the payload of your request \n \n Notice the URI format used to encode MQTT settings. \n MQTT-PUB syntax \n The URI is parsed as follows:  mqtt://{hostname}[:{port}]/{topic-name}[?{settings}] . The post\'s body is used as the payload and may be empty, but  note IFTTT may remove newlines or mess with whitespace . Hence, I suggest using JSON if possible. \n Settings must be URI-encoded as usual; the following are valid:\n*  qos  =  0  or  1  or  2 , default is  0  (see MQTT specifications for semantics)\n*  retain  =  true  or  false , default is  false  ( 1 ,  T ,  t , and  tRUe  are all accepted as true)\n*  protocol  =  MQTTv31  or  MQTTv311 , latter is default', ""autobw \n autobw  is a simple library for automatically performing  a backwards pass, given only a forwards pass,  in Torch. \n A major advantage of this is that the neural network's  structure need not be fixed before runtime . This allows for easy implementation of structures such as recurrent networks. See the example below. \n Backpropagation is often described as a method for propagating gradients through a computational graph. One way to implement it for graphs is to explicitly construct a graph given by the user, then evaluate the computational nodes in the order specified in the forward pass, then again but in reverse for the backward pass.  \n Install \n luarocks install https://raw.githubusercontent.com/bshillingford/autobw.torch/master/autobw-scm-1.rockspec \n Details \n A method that's closer to how one may reason about a neural network is to explicitly write down a forward pass while recording the statements as they are being executed, then execute the statements' derivative computations (aka adjoint) in reverse. This is equivalent to specifying a computation graph, but more explicit, and allows the user to use  control-flow such as for loops and conditionals . \n This is similar to the approach taken by implementations of reverse-mode automatic differentiation, see e.g.  http://arxiv.org/abs/1502.05767 . \n Examples: \n A simple example of computing  linear(x1) + x2 * sigmoid(x3) , but  randomly  replacing  sigmoid(x3)  with  x3  sometimes:\n```lua\nlin = nn.Linear(5,5)\nadd = nn.CAddTable()\nmul = nn.CMulTable()\nsigm = nn.Sigmoid() \n tape = autobw.Tape() \n -------------- START OF FORWARD PASS --------------\n-- records the sequence of operations\ntape:begin()\ncoin_flip = torch.rand(1)[1]\nval1 = lin:forward(x1) \n if coin_flip > 0.5 then\n  maybe_sigmoid = sigm:forward(x3)\nelse\n  maybe_sigmoid = x3\nend \n result = add:forward{val1, mul:forward{x2, maybe_sigmoid}}\ntape:stop()\n-------------- END OF FORWARD PASS -------------- \n -- Play it back in reverse:\ntape:backward() \n -- Now, the gradients are in the four nn.Module objects as usual.\n``` \n Note: I don't actually use the gradients at all here, and I don't set them to zero first, just to keep the example simple.\nSee also  our nngraph practical  for the equivalent in  nngraph . \n RNN Example \n See the  examples folder  for a  fully functional rnn example  with toy data. \n LSTM example \n The LSTM example  https://github.com/oxford-cs-ml-2015/practical6  can easily be shortened by using this. We delete the backward pass, and simply play it back from the recorded forward pass:\n```lua\n-- setup autodiff\ntape = Tape() -- TODO: local \n -- do fwd/bwd and return loss, grad_params\nfunction feval(x)\n    if x ~= params then\n        params:copy(x)\n    end\n    grad_params:zero() \n ------------------ get minibatch -------------------\nlocal x, y = loader:next_batch()\n\n------------------- forward pass -------------------\ntape:begin() -----------------\nlocal embeddings = {}            -- input embeddings\nlocal lstm_c = {[0]=initstate_c} -- internal cell states of LSTM\nlocal lstm_h = {[0]=initstate_h} -- output values of LSTM\nlocal predictions = {}           -- softmax outputs\nlocal loss = 0\n\nfor t=1,opt.seq_length do\n    embeddings[t] = clones.embed[t]:forward(x[{{}, t}])\n\n    -- we're feeding the *correct* things in here, alternatively\n    -- we could sample from the previous timestep and embed that, but that's\n    -- more commonly done for LSTM encoder-decoder models\n    lstm_c[t], lstm_h[t] = unpack(clones.lstm[t]:forward{embeddings[t], lstm_c[t-1], lstm_h[t-1]})\n\n    predictions[t] = clones.softmax[t]:forward(lstm_h[t])\n    loss = loss + clones.criterion[t]:forward(predictions[t], y[{{}, t}])\nend\ntape:stop() -----------------\n\n------------------ backward pass -------------------\ntape:backward()\n\n------------------------ misc ----------------------\n-- transfer final state to initial state (BPTT)\ninitstate_c:copy(lstm_c[#lstm_c])\ninitstate_h:copy(lstm_h[#lstm_h])\n\n-- clip gradient element-wise\ngrad_params:clamp(-5, 5)\n\nreturn loss, grad_params\n \n end\n```"", ""gzfile \n Conveniently (and with decent performance) read and write data from  gzip  \nfiles directly. Useful for text or very large files. \n Implemented as an FFI wrapper for zlib, including wrappers for C  FILE  \nfunctions like  fscanf  and  fwrite . More can be easily added. \n Install:\n luarocks install https://raw.githubusercontent.com/bshillingford/lua_gzipfile/master/gzfile-scm-1.rockspec \n Example: \n Read 200 floats from a gzipped file directly into a torch tensor:\n```lua\nrequire 'torch'\nlocal GZFile = require 'gzfile.GZFile' \n local tensor = torch.FloatTensor(200)\nlocal f = GZFile('floats.gz', 'rb')\nf:readbuf(tensor:data(), 200 4)  -- sizeof(float)=4; read 200 4 bytes\nf:close()\n-- now do stuff with the tensor\n```\nNote: compressing floats can be useful for neural nets, since similar values at\nsimilar orders of magnitude will often results in repeated byte patterns. \n To access the underlying  FILE*  handle, use the  handle  property of  GZFile . \n Functions implemented: \n \n Constructor:  file = GZFile(filename, mode) \n    Opens the file for reading or writing using the given mode.\n    See zlib gzopen() documentation for supported modes. \n :close()  Closes the file. Later operations will fail. \n :write(str) \n    Writes a lua string. Doesn't allocate memory, just casts using ffi.\n    Calls fwrite and returns its ret val, i.e. number of bytes written. \n :read(nbytes) \n    Reads to a buffer then turns into a lua string.\n    Allocates memory on each call, so slightly inefficient if you do many\n    reads. \n :writebuf(ptr, nbytes) \n    Writes data from the given buffer to the file.\n    Returns number of bytes written. \n :readbuf(ptr, nbytes) \n    Reads into the given location in memory.\n    Returns number of bytes read. \n :flush() \n :peek() \n :tell() \n    Returns the position in the file using ftell. \n :seek(offset, origin) \n    Seek using fseek, relative to beginning of file.\n    Note the argument order matches C's rather than Lua's io.\n    Returns new position from ftell. \n :getc() \n :scanf(fmt, typestring) \n    Calls fscanf, but only for reading a single field e.g.  %s .\n    Allocates memory automatically, this memory will be gc'd by lua.\n    Typestring examples:  'float[1]' ,  'uint8_t[1]' ,  'char[16]' . \n"", 'Class visualizer for lua/torch \n Class hierarchy visualizer for torch classes, and classic classes (see github.com/deepmind/classic) \n Currently generates a d3 visualization, easily adapted to graphviz. \n Example of viewing the classes in  nn \n th generate.lua htmld3tree nn > output.html \nThen open  output.html  in your browser. \n Usage: \n sh\nth [-lclassic] generate.lua output_mode package1 [package2 [package3 ...] ] \nwhere  output_mode  is  htmld3force  or  htmld3tree . See e.g.  htmld3tree.lua ; more can be added. \n \n Output is to stdout; redirect it to a file like  output.html . \n Use  -lclassic  to monitor classic classes instead. (see  https://github.com/deepmind/classic ) \n \n Useful extensions/TODOs: \n \n Output to graphviz \n Parse libraries for documentation and include it in the visualization? \n At the least, actually do something useful when clicking on the classes. \n \n Internals \n For  torch.class  classes, it simply monkeypatches this function to monitor created classes when\nthe packages get  require d. For classic classes, it uses a central (private) registry storing \nthe classes and their parents.', 'nnquery : query large neural network graph structures in Torch \n NN modules in Torch are often complex graph structures, like  nn.Container s and its subclasses and  nn.gModules  ( nngraph ), arbitrarily nested. This makes it tedious to extract nn modules when debugging, monitoring training progress, or testing. \n nnquery  provides a facility to query these arbitrarily complex DAGs. XPath and CSS are designed to handle trees, whereas this library supports querying DAGs like neural nets.\nThe API is loosely inspired by a mix of XPath, CSS queries, and .NET\'s LINQ. \n See below for a simple example, and a more complete example of extracting things from an LSTM. \n Installation \n Install  nnquery :\n luarocks install https://raw.githubusercontent.com/bshillingford/nnquery/master/rocks/nnquery-scm-1.rockspec \nTotem is optional, and used for unit tests. \n Usage \n There are two important base classes that nearly everything is derived from: \n \n Element  (full name:  nnquery.Element ) \n ElementList \n \n Every object you wish to query is wrapped in an  Element , and sequences/collections of these\nare represented using  ElementList s. \n To wrap an object in an  Element  so you can query it:\n```lua\nlocal nnq = require \'nnquery\'\nlocal seq = nn.Sequential()\n    :add(nn.Tanh())\n    :add(nn.ReLU()) \n local tanh = nnq(seq):children():first()\n```\nOn the last line, \n \n nnq(seq)  wraps  seq  into an  Element ; \n :children()  returns an  ElementList  of two  Elements  for  seq \'s children; \n :first()  returns the first  Element  in the  ElementList . \n \n Realistic example with an LSTM: \n This is an example of using various functions in  Element  and  ElementList :\n```lua\nrequire \'nn\'\nrequire \'nngraph\'\nlocal nnq = require \'nnquery\' \n -- nngraph implementation of LSTM timestep, from Oxford course\'s practical #6\nfunction create_lstm(opt)\n  local x = nn.Identity()()\n  local prev_c = nn.Identity()()\n  local prev_h = nn.Identity()() \n function new_input_sum()\n    -- transforms input\n    local i2h            = nn.Linear(opt.rnn_size, opt.rnn_size)(x)\n    -- transforms previous timestep\'s output\n    local h2h            = nn.Linear(opt.rnn_size, opt.rnn_size)(prev_h)\n    return nn.CAddTable()({i2h, h2h})\n  end \n local in_gate          = nn.Sigmoid()(new_input_sum())\n  local forget_gate      = nn.Sigmoid()(new_input_sum())\n  local out_gate         = nn.Sigmoid()(new_input_sum())\n  local in_transform     = nn.Tanh()(new_input_sum()) \n local next_c           = nn.CAddTable()({\n      nn.CMulTable()({forget_gate, prev_c}),\n      nn.CMulTable()({in_gate,     in_transform})\n  })\n  local next_h           = nn.CMulTable()({out_gate, nn.Tanh()(next_c)}) \n nngraph.annotateNodes()\n  local mod = nn.gModule({x, prev_c, prev_h}, {next_c, next_h})\n  mod.name = ""LSTM""\n  return mod\nend \n -- Example network\nlocal foo = nn.Sequential()\n    :add(nn.Module())\n    :add(create_lstm{rnn_size=3})\n    :add(nn.ReLU())\n    :add(nn.ReLU())\n    :add(nn.Linear(3, 4)) \n -- Find the LSTM in a few different ways:\nlocal lstm = nnq(foo)   -- Wrap the module in an Element object using the default context\n                        -- which allows querying nn containers and nngraph\'s gmodules.\n    :descendants()      -- Get all descendants below this node in the graph\n    :where(function(e)  -- Filter Elements by the given predicate\n      return e:classIs(nnq.NNGraphGModuleElement)\n    end)\n    :only()             -- Returns the first element in the returned sequence, and\n                        -- asserts that it is the only element in the sequence.\n                        -- (shortcut for list:first() and assert(list:count() == 1))\nlocal lstm2 = nnq(foo)\n    :children()         -- Returns the contained modules of the nn.Sequential object as an\n                        -- ElementList\n    :nth(2)             -- Grabs the 2nd child of the nn.Sequential\n                        -- (alternate shorthand syntax: nnq(foo):children()[2])\nlocal lstm3 = nnq(foo)\n    :descendants()      --  \n    :attr{name=\'LSTM\'}  -- Get only the objects with a name attribute set to \'LSTM\',\n                        -- where it\'ll check both raw attributes and attempt to call\n                        -- the function assuming it\'s a getter method, i.e. check \n                        -- module:name() == \'LSTM\'.\n    :only()\nassert(lstm:val() == lstm2:val() and lstm2:val() == lstm3:val(),\n    \'they should all return the same LSTM gmodule\') \n -- Get the output nodes of the nngraph gmodule as an ElementList:\nlocal outputs = lstm:outputs()\n-- Two ways to get the count for an ElementList:\nprint(\'The LSTM gmodule has \'..outputs:count()..\' outputs, they are:\' outputs)\nprint(\'The LSTM gmodule has \'..#outputs..\' outputs, they are:\', outputs)\nassert(outputs:first():name() == \'next_c\')  -- :name() is available on NNGraphNodeElements,\n                                            -- as a shortcut for:\nassert(outputs:first():val().data.annotations.name == \'next_c\')  \n -- Let\'s find the forget gate:\nlocal forget_gate = lstm:descendants():attr{name=\'forget_gate\'}:only()\nprint(forget_gate)\n-- But it\'s the sigmoid, not the gate\'s pre-activations, so let\'s get the sum:\nlocal input_sum = forget_gate:parent() -- This is an alias for :parents():only().\n                                       -- Note: nngraph nodes can have multiple parents (i.e.\n                                       -- inputs \nassert(torch.isTypeOf(input_sum:val().data.module, nn.CAddTable))\nassert(torch.isTypeOf(input_sum:module(), nn.CAddTable)) -- alias for :val().data.module\n``` \n Further details: \n Wrapping objects into elements and similar operations only make sense relative to a  context , an instance of  nnquery.Context , which contains a list of  Element  types and conditions on which to instantiate depending on what type is provided to it. Additionally, the context caches  Element s, so that wrapping the same object twice returns the same instance of the  Element  subclass.\n nnquery/init.lua  contains the construction of a default context (accessible as  nnquery.default ) that contains all the implemented  Element  types, similarly to this:\n lua\nlocal ctx = nnq.Context()\nctx:reg(nnq.NNGraphGModuleElement, nnq.NNGraphGModuleElement.isGmodule)\nctx:reg(nnq.NNGraphNodeElement, nnq.NNGraphNodeElement.isNode)\nctx:reg(nnq.ContainerElement, nnq.ContainerElement.isContainer) -- after since gModule IS_A Container\nctx:default(nnq.ChildlessElement) \n Note that there is no true ""root"" node, unlike an XML/HTML document; the root is simply the place where the query begins. Therefore, one cannot[*] search for the root\'s parents, even if the root module is contained in (for example) a container. \n [*] Usually. Unless an element\'s parents are pre-populated from a previous query. \n Documentation \n Further documentation can be found in doc comment style before class definitions and method definitions in the code itself. \n TODO: extract these into markdown format and put links here \n Developing \n Extending \n You may have your own  nn  modules that are not handled by the existing handlers. In this case,\nyou can implement your own  Element  object (see the existing ones for examples), and create your own context that adds a handler for this  Element . See the default context (see above) for details. \n Contributing \n Bug reports are appreciated, preferably with a pull request for a test that breaks existing code and a patch that fixes it. If you do, please adhere to the (informal) code style in the existing code where appropriate.', ""Torch serialization reader for Python \n \n \n Mostly direct port of the torch7 Lua and C serialization implementation to \nPython, depending only on  numpy  (and the standard library:  array  \nand  struct ). Sharing of objects including  torch.Tensor s is preserved. \n python\nimport torchfile\nstuff = torchfile.load('a_bunch_of_stuff.t7') \n Installation: \n Install from  PyPI :\n sh\npip install torchfile \nor clone this repository, then:\n sh\npython setup.py install \n Supports Python 2.7, 3.4, 3.5, 3.6. Probably others too. \n More examples: \n Write from torch, read from Python: \n Lua:\n lua\n+th> torch.save('/tmp/test.t7', {hello=123, world=torch.rand(1,2,3)}) \nPython:\n python\nIn [3]: o = torchfile.load('/tmp/test.t7')\nIn [4]: print o['world'].shape\n(1, 2, 3)\nIn [5]: o\nOut[5]: \n{'hello': 123, 'world': array([[[ 0.52291083,  0.29261517,  0.11113465],\n         [ 0.01017287,  0.21466237,  0.26572137]]])} \n Arbitary torch classes supported: \n ```python\nIn [1]: import torchfile \n In [2]: o = torchfile.load('testfiles_x86_64/gmodule_with_linear_identity.t7') \n In [3]: o.forwardnodes[3].data.module\nOut[3]: TorchObject(nn.Identity, {'output': array([], dtype=float64), 'gradInput': array([], dtype=float64)}) \n In [4]: for node in o.forwardnodes: print(repr(node.data.module))                                                                                                           \nNone\nNone\nNone\nTorchObject(nn.Identity, {'output': array([], dtype=float64), 'gradInput': array([], dtype=float64)})\nNone\nTorchObject(nn.Identity, {'output': array([], dtype=float64), 'gradInput': array([], dtype=float64)})\nTorchObject(nn.Linear, {'weight': array([[-0.0248373 ],\n       [ 0.17503954]]), 'gradInput': array([], dtype=float64), 'gradWeight': array([[  1.22317168e-312],\n       [  1.22317168e-312]]), 'bias': array([ 0.05159848, -0.25367146]), 'gradBias': array([  1.22317168e-312,   1.22317168e-312]), 'output': array([], dtype=float64)})\nTorchObject(nn.CAddTable, {'output': array([], dtype=float64), 'gradInput': []})\nNone \n In [5]: o.forwardnodes[6].data.module.weight\nOut[5]: \narray([[-0.0248373 ],\n       [ 0.17503954]]) \n In [6]: o.forwardnodes[6].data.module.bias\nOut[6]: array([ 0.05159848, -0.25367146])\n``` \n More complex writing from torch: \n Lua:\n lua\n+th> f = torch.DiskFile('/tmp/test.t7', 'w'):binary()\n+th> f:writeBool(false)\n+th> f:writeObject({hello=123})\n+th> f:writeInt(456)\n+th> f:close() \nPython:\n python\nIn [1]: import torchfile\nIn [2]: with open('/tmp/test.t7','rb') as f:\n   ...:     r = torchfile.T7Reader(f)\n   ...:     print(r.read_boolean())\n   ...:     print(r.read_obj())\n   ...:     print(r.read_int())\n   ...: \nFalse\n{'hello': 123}\n456 \n Supported types: \n \n nil  to Python  None \n numbers to Python floats, or by default a heuristic changes them to ints or\n   longs if they are integral \n booleans \n strings: read as byte strings (Python 3) or normal strings (Python 2), like\n   lua strings which don't support unicode, and that can contain null chars \n tables converted to a special dict (*); if they are list-like (i.e. have\n   numeric keys from 1 through n) they become a python list by default \n Torch classes: supports Tensors and Storages, and most classes such as \n   modules. Trivially extensible much like the Torch serialization code.\n   Trivial torch classes like most  nn.Module  subclasses become \n    TorchObject s. The  torch_readers  dict contains the mapping from class\n   names to reading functions. \n functions: loaded into the  LuaFunction   namedtuple ,\n   which simply wraps the raw serialized data, i.e. upvalues and code.\n   These are mostly useless, but exist so you can deserialize anything. \n tds.Hash, tds.Vec \n \n (*) Since Lua allows you to index a table with a table but Python does not, we \n    replace dicts with a subclass that is hashable, and change its\n    equality comparison behaviour to compare by reference.\n    See  hashable_uniq_dict . \n Test files demonstrating various features: \n ```python\nIn [1]: import torchfile \n In [2]: torchfile.load('testfiles_x86_64/list_table.t7')\nOut[2]: ['hello', 'world', 'third item', 123] \n In [3]: torchfile.load('testfiles_x86_64/doubletensor.t7')\nOut[3]: \narray([[ 1. ,  2. ,  3. ],\n       [ 4. ,  5. ,  6.9]]) \n ...also other files demonstrating various types. \n ``` \n The example  t7  files will work on any modern Intel or AMD 64-bit CPU, but the\ncode will use the native byte ordering etc. Currently, the implementation \nassumes the system-dependent binary Torch format, but minor refactoring can \ngive support for the ascii format as well."", ""Linux Spotify ad muter \n Listens to track changes with dbus. Assumes pulseaudio, and mutes master during ads (i.e. not just spotify's stream! [1]). \n License: BSD. Based on  https://muffinresearch.co.uk/linux-spotify-track-notifier-with-added-d-bus-love/ . \n [1] TODO: only mute spotify stream, read pavucontrol source code to figure out how. Currently mutes pulseaudio using  amixer -D pulse sset Master {off|on} ."", ""Fork of  fb.debugger : dependency-free \n fb.debugger  is an excellent debugger for torch and lua in general, and can be found at  https://github.com/facebook/fblualib .\nHowever,  fblualib  and its dependencies are quite heavy but you may just want the debugger. This repository is a fork of  fb.debugger  with dependencies integrated and/or removed.  \n To run a script and drop into the debugger on an error, simply do:\n sh\nfbdbg-run script_name.lua your_arg1 your_arg2 ... \n Install: \n luarocks install https://raw.githubusercontent.com/bshillingford/fbdebugger-minimal/master/fbdebugger-standalone-1.rockspec \n Dependencies: \n \n penlight >= 1.3.1 \n libedit  installed in your system ( libedit.so  anywhere in the library search path). \n \n \n Original README: \n fb-debugger: A source-level Lua debugger \n This package implements a source-level Lua debugger. \n Usage \n You may enter the debugger in two different ways:\n* explicitly: at the point of interest, do\n lua\nlocal debugger = require('fb.debugger')\ndebugger.enter() \n  and you will be dropped in the debugger\n* automatically when you hit an (uncaught) error: if using\n   fb.trepl , you may set the environment variable\n   LUA_DEBUG_ON_ERROR  to  1 , and you'll be dropped in the debugger\n  whenever your code raises an uncaught error. \n Debugger commands \n help  will give you a list of commands, inspired by\n gdb . The following commands exist and behave\nsimilarly to their gdb counterparts:\n*  help  displays help\n*  where  /  backtrace  /  bt  displays the current stack trace (with a\n  marker for the currently selected frame)\n*  frame  selects a given frame\n*  up  /  down  moves the currently selected frame up / down one\n*  b  /  break  sets a breakpoint at a given location (specified either as\n   <file>:<line_number>  or  <function_name> ; the function name is looked up\n  in the scope of the current frame)\n*  info breakpoints  lists breakpoints\n*  enable ,  disable ,  delete  enable, disable, and delete a breakpoint,\n  respectively\n*  next  /  n  single-steps one line, skipping over function calls\n*  step  /  s  single-steps one line, descending into function calls\n*  finish  continues execution until the function in the currently selected\n  frame returns\n*  continue  /  c  continues program execution until the next breakpoint,\n  or until the next time the debugger is reentered (via  debugger.enter()  or\n  automatically in case of error)\n*  locals  /  vlocals  shows locals in scope in the current frame;  vlocals \n  also shows values (verbose)\n*  globals  /  vglobals  shows all globals\n*  upvalues  /  vupvalues  shows the current function's upvalues\n*  exec  /  e  executes code in the scope of the current frame\n*  print  /  p  evaluates an expression in the scope of the current frame and\n  prints the result\n*  list  /  l  lists source code (if available); by default it lists the\n  function in the current frame, but it accepts a location argument just like\n   break ; just like gdb, repeating  l  without arguments continues listing\n  the same file\n*  quit  /  q  quits the debugger; the program is resumed. \n Note that  locals ,  globals , or  upvalues  will occasionally show a\nsynthetic name for a variable (such as  _dbgl_tmp_4 ). These indicate variables\nthat have been shadowed in the current scope (and so their original name\nnow refers to something else) or internal Lua temporaries (modifying those\nis ill-advised)."", 'LipNet: End-to-End Sentence-level Lipreading. \n Yannis M. Assael, Brendan Shillingford, Shimon Whiteson, Nando de Freitas \n Links \n -  arXiv pre-print \n Bibtex \n @article{assael2016lipnet,\n  title={LipNet: End-to-End Sentence-level Lipreading},\n  author={Assael, Yannis M and Shillingford, Brendan and Whiteson, Shimon and de Freitas, Nando},\n  journal={GPU Technology Conference},\n  year={2017}\n}\n \n License \n Code licensed under the Apache License v2.0.', 'num2word \n Line-for-line Lua port of  http://stackoverflow.com/questions/25150316/convert-numbers-to-english-strings \n Python version fixed a ""zero thousand"" bug. Reasonably high quality for numbers smaller than a billion, but a few odd spellings left uncorrected e.g. 18. \n Test of equivalent implementation \n bash\nluajit test_num2word.lua > lua.txt\npython test_num2word.py > py.txt\ndiff lua.txt py.txt || echo ""they are different""', 'Wrapper for CUDA profiler start/stop API functions. Zero dependencies. \n Example:\n```python\nimport cudaprofile \n cudaprofile.start() \n ... do expensive cuda stuff ... \n cudaprofile.stop()\n ``\nand run the script from nvprof or nvvp`. \n You may want to use  nvprof  with  --profile-from-start-off  and only call  start()  when desired.', 'wifi-locate (Python) \n Locates the Wi-Fi-enabled machine using nearby Wi-Fi access points\' relative signal strengths. Uses Google\'s API. \n To use, call  linux_scan  or  osx_scan , then give the result to  locate  which returns  (accuracy, (lat,lng)) . \n Pretty useful for  xflux  or fetching weather. \n Quick start: \n Install:\n bash\npip install git+https://github.com/bshillingford/wifi-locate \n Example:\n python\nfrom wifilocate import locate, linux_scan\naccuracy, latlng = locate(linux_scan(device=""wlan0"", iwlist_path=\'/sbin/iwlist\'))\nprint(accuracy, latlng)  # e.g. 25, (50.1234567, -1.234567) \n Details \n Calls Google\'s API (most likely used in Firefox, based on the URL). The module supports Python 2 and 3, and only depends on  requests . If you don\'t yet have requests, consider my dependency on it a favour. It\'s great. \n In Linux this uses  iwlist , and in OS X it uses a little-known but built-in utility called  airport .', ""This is a fork of the excellent extension Better Google Tasks: \n \n https://chrome.google.com/webstore/detail/better-google-tasks/denjcdefjebbmlihdoojnebochnkgcin?hl=en-GB \n http://richwells.me/blog/better-google-tasks/ \n \n I added functionality for handling URLs to a specific task list, like this:\n https://mail.google.com/tasks/canvas#List Name Here \nand changing the default task list to open via the extension's options dialog."", ""sharearray \n Have you worried about creating large identical numpy arrays across processes due to RAM wastage, e.g. datasets that are big enough to fit in RAM but large enough to cause concern when running multiple jobs using the same data?\n sharearray  efficiently caches numpy arrays in RAM (using shared memory in  /dev/shm , no root needed) locally on a machine. \n Usage is simple, using the  cache  function or  decorator  decorator.\nA first call saves the result of the call into the built-in RAM disk, and\nreturns a read-only memory-mapped view into it.\nSince it's in RAM, there's no performance penalty.\nAny subsequent calls with the same ID will return an identical read-only memory mapped view,\neven across processes. The IDs are  global . \n Installation:\n pip install git+https://github.com/bshillingford/python-sharearray \nor\n git clone https://github.com/bshillingford/python-sharearray\npython setup.py install \n Usage \n Using  decorator : \n ```python\n@sharearray.decorator('some_unique_id', verbose=False)\ndef get_training_data():\n    # create largeish / expensive-to-generate data\n    return my_array # some instance of np.ndarray \n first call, across all processes, creates the array \n arr_view = get_training_data() \n all further calls are cached/memoized: we return a view into memory \n arr_view_2 = get_training_data()\n``` \n Using the  cache  function: \n ```python\nimport sharearray\nimport numpy as np\narr = sharearray.cache('my_global_id', lambda: create_large_array()) \n or: \n arr = sharearray.cache('my_global_id', lambda: create_large_array())\n ``\nwhere, for instance, create_large_array` returns a large training set, potentially performing expensive feature transformations or data augmentations first. \n By default, the file is at  /dev/shm/sharearray_my_global_id.npy , and to avoid concurrency\nissues when first generating the array, and to avoid duplicated computation,  \n For futher details, read the docstrings. You may be interested in the  timeout ,  verbose , and  log_func  arguments (to either  cache  or  decorator ). \n PyTorch \n Since PyTorch does not yet support memmapped files (at time of writing), we can instead just create torch Tensors that point to the memory mapped by numpy:\n python\ndata_numpy = get_training_data()          # numpy.ndarray\ndata_torch = torch.from_numpy(data_numpy) # torch.Tensor \n Notes \n TODO: support returning multiple arrays (e.g. as a tuple or dict) from the callback / decorated function \n There exist similar libraries in Python already, but this just makes it easier to do as a memoization-style API. Also, this module is a single file, and does not write anything in C.""]",['repository_count'],,https://avatars.githubusercontent.com/u/2326749?v=4,False,2016-08-16 06:35:51,446,56,2,2012-09-11T18:58:38Z,2022-11-02T15:27:37Z,,False,False,False,False,False,True,False,0,"{'Python': 14, 'Lua': 7, 'CSS': 1, 'HTML': 1, 'Jupyter Notebook': 1}",9,,Data Analysis,89,0.99,0.9,0.38,0
hmansell,Howard Mansell,,howard@mansells.us,[],['repository_count'],Facebook,https://avatars.githubusercontent.com/u/980875?u=d414d46e55b397533a7031f563db6ecd5388c405&v=4,False,2016-08-24 15:41:21,30,1,0,2011-08-15T12:02:25Z,2022-10-12T23:39:03Z,,False,False,False,False,False,True,False,0,{},1,New York,Data Integration,99,0.69,0.19,0.54,0
clementfarabet,Clement Farabet,Building AI.,clement@madbits.ai,"['sys \n Has moved to a more community friendly  repo .', 'xlua \n Has moved to a more community friendly  repo .', 'image \n Has moved to a more community friendly  repo .', 'nnx: experimental \'nn\' components \n The original neural network from Torch7,  nn , contains stable and widely\nused modules. \'nnx\' contains more experimental, unproven modules, and\noptimizations. Modules that become stable and which are proven useful make \ntheir way into \'nn\' (some already have). \n Library Documentation \n This section includes documentation for the following objects: \n \n SoftMaxTree  : a hierarchical log-softmax Module; \n TreeNLLCriterion  : a negative log-likelihood Criterion for the SoftMaxTree; \n CTCCriterion  : a Connectionist Temporal Classification Criterion based on  warp-ctc ; \n PushTable (and PullTable)  : extracts a table element and inserts it later in the network; \n MultiSoftMax  : performs a softmax over the last dimension of a 2D or 3D input; \n SpatialReSampling  : performs bilinear resampling of a 3D or 4D input image; \n [QDRiemaNNLinear] (#nnx.QDRiemaNNLinear) : quasi-diagonal reduction for Riemannian gradient descent \n Recurrent  : a generalized recurrent neural network container; \n \n \n SoftMaxTree \n A hierarchy of parameterized log-softmaxes. Used for computing the likelihood of a leaf class. \nThis Module should be used in conjunction with the  TreeNLLCriterion . \nUsing this for large vocabularies (100,000 and more) greatly accelerates training and evaluation \nof neural network language models (NNLM). \nA vocabulary hierarchy is provided via the  dp  package\'s\n BillionWords \n DataSource . \n The constructor takes 2 mandatory and 4 optional arguments : \n *  inputSize  : the number of units in the input embedding representation;\n *  hierarchy  : a Tensor mapping one  parent_id  to many  child_id  (a tree);\n *  rootId  : a number identifying the root node in the hierarchy. Defaults to  -1 ;\n *  accUpdate  : when the intent is to use  backwardUpdate  or  accUpdateGradParameters , set this to true to save memory. Defaults to false;\n *  static  : when true (the defualt), returns parameters with keys that don\'t change from batch to batch;\n *  verbose  : prints some additional information concerning the hierarchy during construction. \n The  forward  method returns an  output  Tensor of size 1D, while \n backward  returns a table  {gradInput, gradTarget} . The second \nvariable is just a Tensor of zeros , such that the  targets  can be \npropagated through  Containers  \nlike  ParallelTable . \n ```lua \n \n input = torch.randn(5,10)\ntarget = torch.IntTensor{20,24,27,10,12}\ngradOutput = torch.randn(5)\nroot_id = 29\ninput_size = 10  \nhierarchy = { \n \n [29]=torch.IntTensor{30,1,2}, [1]=torch.IntTensor{3,4,5}, \n   [2]=torch.IntTensor{6,7,8}, [3]=torch.IntTensor{9,10,11},\n   [4]=torch.IntTensor{12,13,14}, [5]=torch.IntTensor{15,16,17},\n   [6]=torch.IntTensor{18,19,20}, [7]=torch.IntTensor{21,22,23},\n   [8]=torch.IntTensor{24,25,26,27,28}\n}\nsmt = nn.SoftMaxTree(input_size, hierarchy, root_id)\nsmt:forward{input, target}\n-3.5186\n-3.8950\n-3.7433\n-3.3071\n-3.0522\n[torch.DoubleTensor of dimension 5]\nsmt:backward({input, target}, gradOutput)\n{\n  1 : DoubleTensor - size: 5x10\n  2 : IntTensor - size: 5\n} \n \n \n ``` \n \n TreeNLLCriterion \n Measures the Negative log-likelihood (NLL) for  SoftMaxTrees . \nUsed for maximizing the likelihood of SoftMaxTree outputs.\nThe SoftMaxTree Module outputs a column Tensor representing the log likelihood\nof each target in the batch. Thus SoftMaxTree requires the targets.\nSo this Criterion only computes the negative of those outputs, as \nwell as its corresponding gradients. \n \n \n PushTable (and PullTable) \n PushTable and PullTable work together. The first can be put earlier\nin a digraph of Modules such that it can communicate with a \nPullTable located later in the graph.  PushTable:forward(input)  \nfor an  input  table of Tensors to the output, excluding one, the index of which \nis specified by the  index  argument in the  PushTable(index)  constructor.\nThe Tensor identified by this  index  is communicated to one or many \nPullTables created via the  PushTable:pull(index)  factory method. \nThese can be inserted later in the digraph such that \na call to  PushTable:forward(input) , where  input  is a table or a Tensor, \nwill output a table with the previously  pushed  Tensor inserted \nat index  index . \n An example utilizing the above  SoftMaxTree  Module\nand a Linear Module demonstrates how the PushTable can be used to \nforward the  target  Tensor without any other \n Table Modules :\n```lua \n \n mlp = nn.Sequential()\nlinear = nn.Linear(50,100)\npush = nn.PushTable(2)\npull = push:pull(2)\nmlp:add(push)\nmlp:add(nn.SelectTable(1))\nmlp:add(linear)\nmlp:add(pull)\nmlp:add(smt) --smt is a SoftMaxTree instance\nmlp:forward{input, target} -- input and target are defined above\n-3.5186\n-3.8950\n-3.7433\n-3.3071\n-3.0522\n[torch.DoubleTensor of dimension 5]\nmlp:backward({input, target}, gradOutput) -- so is gradOutput\n{\n  1 : DoubleTensor - size: 5x10\n  2 : IntTensor - size: 5\n}\n The above code is equivalent to the following: lua\nmlp2 = nn.Sequential()\npara = nn.ParallelTable()\npara:add(linear)\npara:add(nn.Identity())\nmlp2:add(para)\nmlp2:add(smt)\nmlp2:forward{input, target}\n-3.5186\n-3.8950\n-3.7433\n-3.3071\n-3.0522\n[torch.DoubleTensor of dimension 5]\nmlp2:backward({input, target}, gradOutput)\n{\n  1 : DoubleTensor - size: 5x10\n  2 : IntTensor - size: 5\n}\n```\nIn some cases, this can simplify the digraph of Modules. Note that \na PushTable can be associated to many PullTables, but each PullTable \nis associated to only one PushTable. \n \n \n CTCCriterion \n criterion = nn.CTCCriterion() \nCreates a Criterion based on Baidus\'  warp-ctc  implementation.\nThis Module measures the loss between a 3D output of (batch x time x inputdim) and a target without needing alignment of inputs and labels.\nMust have installed warp-ctc which can be installed via luarocks:\n luarocks install http://raw.githubusercontent.com/baidu-research/warp-ctc/master/torch_binding/rocks/warp-ctc-scm-1.rockspec \nSupports cuda via:\n criterion = nn.CTCCriterion():cuda() \nExample:\n```\noutput = torch.Tensor({{{1,2,3,4,5},{6,7,8,9,10}}}) -- Tensor of size 1x1x5 (batch x time x inputdim).\nlabel = {{1,3}}\nsizes = torch.Tensor({2}) -- Size of each sequence (sequence-length) in the batch as a tensor\nctcCriterion = nn.CTCCriterion() \n err = ctcCriterion:forward(output,label,sizes)\ngradOut = ctcCriterion:backward(output,label)\nprint(""----CPU----"")\nprint(""Error : "" .. err)\nprint(""Gradients :"")\nprint(gradOut) \n ctcCriterion = ctcCriterion:cuda() -- Switch to cuda implementation.\noutput = output:cuda() \n err = ctcCriterion:forward(output,label,sizes)\ngradOut = ctcCriterion:backward(output,label)\nprint(""----GPU----"")\nprint(""Error : "" .. err)\nprint(""Gradients :"")\nprint(gradOut)\n``` \n gives the output:\n```\n----CPU---- \nError : 4.9038286209106 \nGradients : \n(1,.,.) = \n  0.0117 -0.9683  0.0861  0.2341  0.6364\n  0.0117  0.0317  0.0861 -0.7659  0.6364\n[torch.FloatTensor of size 1x2x5] \n ----GPU---- \nError : 4.9038290977478 \nGradients : \n(1,.,.) = \n  0.0117 -0.9683  0.0861  0.2341  0.6364\n  0.0117  0.0317  0.0861 -0.7659  0.6364\n[torch.CudaTensor of size 1x2x5]\n```\n \n MultiSoftMax \n This Module takes 2D or 3D input and performs a softmax over the last dimension. \nIt uses the existing  SoftMax  \nCUDA/C code to do so such that the Module can be used on both GPU and CPU. \nThis can be useful for  keypoint detection . \n \n SpatialReSampling \n Applies a 2D re-sampling over an input image composed of\nseveral input planes (or channels, colors). The input tensor in  forward(input)  is \nexpected to be a 3D or 4D tensor of size :  [batchSize x] nInputPlane x width x height . \nThe number of output planes will be the same as the number of input\nplanes. \n The re-sampling is done using  bilinear interpolation . \nFor a simple nearest-neihbor upsampling, use  nn.SpatialUpSampling() ,\nand for a simple average-based down-sampling, use \n nn.SpatialDownSampling() . \n If the input image is a 3D tensor of size  nInputPlane x height x width ,\nthe output image size will be  nInputPlane x oheight x owidth  where\n owidth  and  oheight  are given to the constructor. \n Instead of  owidth  and  oheight , one can provide  rwidth  and  rheight , \nsuch that  owidth = iwidth*rwidth  and  oheight = iheight*rheight . \n As an example, we can run the following code on the famous Lenna image:\n lua\nrequire \'image\'                                                           \nrequire \'nnx\'\ninput = image.loadPNG(\'doc/image/Lenna.png\')\nl = nn.SpatialReSampling{owidth=150,oheight=150}\noutput = l:forward(input)\nimage.save(\'doc/image/Lenna-150x150-bilinear.png\', output) \n The input: \n   \n The re-sampled output: \n   \n \n QDRiemaNNLinear \n The Quasi-Diagonal Riemannian Neural Network Linear (QDRiemaNNLinear) module is an implementation\nof the quasi-diagonal reduction of metrics, used for Riemannian gradient descent.\nThe algorithm is defined in Riemannian metrics for neural networks I: feedforward networks by Yann Ollivier (http://arxiv.org/abs/1303.0818) and an efficient implementation is described in Practical Riemannian Neural Networks by Yann Ollivier and Gaetan Marceau-Caron (http://arxiv.org/abs/1602.08007).\nTo use this module, simply replace  nn.Linear(ninput,noutput)  with  nnx.QDRiemaNNLinear(ninput,noutput) .\nAs always, the step-size must be chosen accordingly.\nTwo additional arguments are also possible:\n* gamma (default=0.01): determine the update rate of the metric for a minibatch setting, i.e., (1-gamma) * oldMetric + gamma newMetric. Smaller minibatches require a smaller gamma. A default value depending on the size of the minibatches is  gamma = 1. - torch.pow(1.-1./nTraining,miniBatchSize)  where  nTraining  is the number of training examples of the dataset and  miniBatchSize  is the number of training examples per minibatch. \n* qdFlag (default=true): Whether to use the quasi-diagonal reduction (true) or only the diagonal (false). The former should be better. \n This module is a straightforward implementation of the outer product gradient descent. \n Requirements \n \n Torch7 (www.torch.ch) \n \n Installation \n \n Install Torch7 (refer to its own documentation). \n clone this project into dev directory of Torch7. \n Rebuild torch, it will include new projects too. \n \n Use the library \n First run torch, and load nnx: \n sh\n$ torch   \n ``` lua \n \n require \'nnx\'\n``` \n \n Once loaded, tab-completion will help you navigate through the\nlibrary (note that most function are added directly to nn): \n ``` lua \n \n nnx. + TAB\n...\nnn. + TAB\n``` \n \n In particular, it\'s good to verify that all modules provided pass their\ntests: \n ``` lua \n \n nnx.test_all()\nnnx.test_omp()\n``` \n \n \n Recurrent \n DEPRECATED July 6th, 2015. Use  rnn  instead.', ""This repo is a container for all my Torch7 packages. \n Note: all these packages used to be distributed into a big messy repo \ncalled XLearn. \n Retrieve all packages \n This repo is empty, and only contains references to other GIT\nrepos. You can retrieve all of them like this: \n sh\n$ git submodule init\n$ git submodule update \n Install \n 1/ Torch7 and dependencies: \n On Linux (Ubuntu > 9.04): \n sh\n$ apt-get install gcc g++ git libreadline5-dev cmake wget libqt4-core libqt4-gui libqt4-dev \n On Mac OS (Leopard, or more), using  Homebrew : \n sh\n$ brew install git readline cmake wget qt \n Then on both platforms: \n sh\n$ git clone https://github.com/andresy/torch\n$ cd torch\n$ mkdir build; cd build\n$ cmake ..\n$ make\n$ [sudo] make install \n 2/ Packages: \n Once Torch7 is installed, it comes with a package manager\nthat you can use to either install packages from the web: \n sh\n$ torch-pkg install pkg-name\n$ torch-pkg --help \n or build them locally, if you are planning to work on the \nsources: \n sh\n$ cd pkg-name\n$ torch-pkg deploy \n Use Torch7 \n First run torch, and load a package: \n sh\n$ torch   \n ``` lua \n \n require 'imgraph'\n``` \n \n Once loaded, tab-completion will help you navigate through the\nlibrary (note: tab-completion will only work if you have\nQt4 and readline): \n ``` lua \n \n imgraph. + TAB\nimgraph.colorize(           imgraph.connectcomponents( \nimgraph.graph(              imgraph.histpooling(       \nimgraph.segmentmst(         imgraph.testme(            \nimgraph.watershed(          imgraph.gradient(\n``` \n \n Most packages then provide a testme() function to quickly see\nwhat it does: \n ``` lua \n \n imgraph.testme()\n``` \n \n Checkout the demos & tutorials \n sh\n$ cd demos   \n this repo contains demos, and tutorials to get started. Looking\nat the code is the best way to get there! \n Developers \n If you would like to develop one of the submodules you should check\nout the master branch of that module:  \n sh\n$ cd nnx\n$ git checkout master\n$ git pull \n This puts you at the head of development for that submodule, and in\nthe proper branch to commit any changes you make to the git repository\nfor that module.  To check out all the submodules in developer mode we\nhave added the script : \n sh\n$ ./gitall.sh \n  a simple command to repeat a git command to all subdirectories \n syntax: \n ./gitall.sh  \n eg: \n \n switch all submodules to the master branch\n  ./gitall.sh checkout master \n pull updates for all submodules\n  ./gitall.sh pull \n other useful\n  ./gitall.sh status\n  ./gitall.sh diff \n \n WARNING: will blindly send command(s) to git in each directory"", ""imgraph: a package to create/manipulate graphs on images \n This package provides standard functions to\ncreate and manipulate edge-weighted graphs \nof images: create a graph, segment it, \ncompute its watershed, or its connected\ncomponents... \n Install \n 1/ Torch7 is required: \n Dependencies, on Linux (Ubuntu > 9.04): \n sh\n$ apt-get install gcc g++ git libreadline5-dev cmake wget libqt4-core libqt4-gui libqt4-dev libboost-all-dev \n Dependencies, on Mac OS (Leopard, or more), using  Homebrew : \n sh\n$ brew install git readline cmake wget qt \n Then on both platforms: \n sh\n$ git clone https://github.com/andresy/torch\n$ cd torch\n$ mkdir build; cd build\n$ cmake ..\n$ make\n$ [sudo] make install \n 2/ Once Torch7 is available, install this package: \n sh\n$ [sudo] torch-rocks install imgraph \n Use the library \n First run torch, and load imgraph: \n sh\n$ torch   \n ``` lua \n \n require 'imgraph'\n``` \n \n Once loaded, tab-completion will help you navigate through the\nlibrary: \n ``` lua \n \n imgraph. + TAB\nimgraph.colorize(           imgraph.connectcomponents( \nimgraph.graph(              imgraph.histpooling(       \nimgraph.segmentmst(         imgraph.testme(            \nimgraph.watershed(          imgraph.gradient(\n``` \n \n To get quickly started, run the testme() function: \n ``` lua \n \n imgraph.testme()\n``` \n \n which computes a few things on the famous image of Lena: \n"", ""neuFlow \n neuFlow  is dataflow architecture optimized for large array/tensor\ntransforms, and especially image processing operations.  More info about the\narchitecture, hardware and applications can be found\n here . \n this package \n This package is a compiler toolkit for neuFlow. It is entirely written in\n Lua , and relies on\n Torch7  to represent N-dimensional arrays\nefficiently. It also interfaces Torch7's neural-network package natively. \n how to install \n Torch7 must be install first, a task most easily accomplished using the single\nline  install script . \n or alternatively to install Torch7 and the neuFlow package by hand, you will\nneed to install a few dependencies. \n On Linux (Ubuntu): \n sh\n$ apt-get install gcc g++ git libreadline5-dev cmake wget\n$ apt-get install libqt4-core libqt4-gui libqt4-dev\n$ apt-get install ffmpeg gnuplot \n On Mac OS X (> 10.5): get  Homebrew \nand then: \n sh\n$ brew install git readline cmake wget\n$ brew install qt\n$ brew install ffmpeg gnuplot \n You're ready to install Torch7 (www.torch.ch). The most up to date instructions\ncan be found at the  Torch7 github page . \n ``` sh\n$ git clone git://github.com/andresy/torch.git\n$ cd torch\n$ mkdir build\n$ cd build \n $ cmake ..\nOR\n$ cmake .. -DCMAKE_INSTALL_PREFIX=/my/install/path\n``` \n Or if you already have a previous Torch7 installed: \n sh\n$ luarocks install torch WITH_LUA_JIT=1 # Torch7, an efficient numeric library for Lua \n You will also need additional packages: \n sh\n$ luarocks install image        # an image library for Torch7\n$ luarocks install nnx          # lots of extra neural-net modules\n$ luarocks install camera       # a camera interface for Linux/MacOS\n$ luarocks install ffmpeg       # a video decoder for most formats\n$ luarocks install inline-c     # inline C capability \n Now that Torch7 has been installed the neuflow package can be installed.\nInstalling the neuflow package requires you to download the source code\nrepository. It'll give you access to some demos, to get started: \n sh\n$ git clone https://github.com/clementfarabet/neuflow.git\n$ cd neuflow\n$ luarocks make \n how to run code on neuFlow \n Demos are located in demos/. To get started, you'll need\na standard Xilinx dev board for the Virtex 6: [the ML605 Kit]\n(http://www.xilinx.com/products/devkits/EK-V6-ML605-G.htm).\nWe provide an image of neuFlow that's pre synthesized/mapped/routed\nfor the Virtex6 VLX240T on this platform. \n To run any of the demos, follow these instructions (tested on\nUbuntu 9.04, 10.04 and Mac OS X 10.5, 10.6 and 10.7). \n ``` sh\n$ git clone https://github.com/clementfarabet/neuflow.git\n$ cd neuflow \n make Xilinx tools available (it implies you have them \n installed somewhere...) \n $ source $XILINX_INSTALL_PATH/settings**.sh \n turn on the ML605, plug the JTAG cable then load one of \n our pre-built bitfiles *: \n $ cd scripts\n$ ./get-latest-neuflow-image\n$ ./load-bitfile neuFlow-ml605.bit \n at this points, you just have wait 2 seconds that the Ethernet \n LEDs are back on (out of reset) \n run the simplest demo, a loopback client, to verify your setup **: \n $ cd ../demos\n$ sudo torch loopback.lua # on Linux\nor\n$ ./loopback.lua # on OSX \n before loading a new demo, you have to reset neuFlow: for \n now it is done by pressing the SW10 button (cpu rst) \n then you can run a typical convnet-based program, a face detector: \n $ sudo torch face-detector.lua # on Linux\nor\n$ ./face-detector.lua # on OSX\n``` \n (*) the load-bitfile script assumes that you have properly installed Xilinx's\nUSB cable driver. On RedHat and derivatives it works out of the box when\ninstalling Xilinx ISE, but on Ubuntu you'll have to follow these instructions:\nhttp://rmdir.de/~michael/xilinx/.  This is not doable on Mac OS X\nunfortunately. I usually flash the ML605 board using Ubuntu (even a virtual box\nversion works), and then run all the demos under Mac OS X. \n (**) you need to have admin privileges on your machine (sudo) to be able to\ninteract with neuFlow, as we're using a custom low-level Ethernet framing\nprotocol."", ""UNSUP \n A package for unsupervised learning in Torch. \n Provides modules that are compatible with  nn  ( LinearPsd ,  ConvPsd ,  AutoEncoder , ...),\nand self-contained algorithms ( k-means ,  PCA ). \n Requirements \n Basic dependencies: \n \n Torch7 (github.com/andresy/torch) \n kex    (github.com/koraykv/tools) \n optim  (github.cim/koraykv/optim) \n \n To run the demo scripts, you also need the following: \n \n image (github.com/clementfarabet/lua---image) \n sys   (github.com/clementfarabet/lua---sys) \n xlua  (github.com/clementfarabet/lua---xlua) \n \n Installation \n Build/Install: \n \n Install Torch7 (refer to its own documentation). \n clone all other repos (including this one) into dev directory of Torch7. \n Rebuild torch, it will include all these projects too. \n \n Alternatively, you can use torch's package manager. Once\nTorch is installed, you can install  unsup :  $ torch-pkg install unsup ."", 'para||el: a (simple) parallel computing framework for Torch \n This package provides a simple mechanism to dispatch and run Torch/Lua code\nas independant processes and communicate via ZeroMQ sockets. Processes\ncan be forked locally or on remote machines. \n Install \n Install ZeroMQ 3 : \n bash\nsudo apt-get install libzmq3-dev libzmq3 \n Install Torch7 per instructions at http://torch.ch/ . \n Download and compile this package using luarocks: \n bash\n[sudo] luarocks install parallel \n or  \n bash\ngit clone https://github.com/clementfarabet/lua---parallel.git\ncd lua---parallel\nluarocks make \n Use the library \n API, in very short: \n Load/start up package: \n lua\nrequire \'parallel\' \n Fork a new process, or N new processes, locally: \n lua\nparallel.fork()\nparallel.nfork(4) \n Fork remote processes. In that following code, we fork 4 processes on myserver.org,\nand 6 processes on myserver2.org. \n lua\nparallel.nfork( {4, ip=\'myserver.org\', protocol=\'ssh\', lua=\'/path/to/remote/torch\'},\n                {6, ip=\'myserver2.org\', protocol=\'ssh\', lua=\'/path/to/remote/torch\'} ) \n Even more flexible, a list of machines can be established first, so that \na call to sfork() [smart fork] can automatically distribute the forked processes\nonto the available machines: \n ``` lua\nparallel.addremote( {ip=\'server1.org\', cores=8, lua=\'/path/to/torch\', protocol=\'ssh -Y\'},\n                    {ip=\'server2.org\', cores=16, lua=\'/path/to/torch\', protocol=\'ssh -Y\'},\n                    {ip=\'server3.org\', cores=4, lua=\'/path/to/torch\', protocol=\'ssh -Y\'} )\nparallel.sfork(16) \n -- in this example, the 16 processes will be distributed over the 3 machines:\n-- server1.org: 6 processes\n-- server2.org: 6 processes\n-- server3.org: 4 processes\n``` \n In the spirit of  really  abstracting where the jobs are executed, calibrate() can\nbe called to estimate the compute power of each machine, so that you can distribute\nyour load accordingly. \n lua\nparallel.addremote(...)\nparallel.calibrate()\nforked = parallel.sfork(parallel.remotes.cores)  -- fork as many processes as cores available\nfor _,forked in ipairs(forked) do\n   print(\'id: \' .. forked.id .. \', speed = \' .. forked.speed)\nend\n-- the speed of each process is a number ]0..1]. A coef of 1 means that it is the\n-- fastest process available, and 0.5 for example would mean that the process is 2x\n-- slower \n Once processes have been forked, they all exist in a table: parallel.children, and\nall methods (exec,send,receive,join) work either on individual processes, or on\ngroups of processes. \n The first thing to do is to load these new processes with code. The code given\ncan either be a function, with no arguments (it won\'t have any env when executing\nin the new process), or a string. Whether it is a string or a function, both\nget serialized into strings, and reloaded on the process side, using loadstring(). \n ``` lua\n-- define process\' code:\ncode = function()\n   -- arbitrary code contained here\n   require \'torch\'\n   t = torch.Tensor(10)\n   print(t) \n -- any process can access its id, its parent\'s id [and children\'s id]\n   print(parallel.id)\n   print(parallel.parent.id)\n   if parallel.children[1] then print(parallel.children[1].id) end \n -- if arguments were passed, they\'re found in the regular ... table       \n   args = {...}   \n   print(args[1])\nend \n -- execute code in given process(es), with optional arguments:\nparallel.children:exec(code) \n -- this is equivalent to:\nfor _,child in ipairs(parallel.child) do\n    child:exec(code)\nend\n``` \n parallel implements a simple yield/join mechanism to allow a parent to sync\nand affect the behavior of its children. \n ``` lua\n-- child code:\ncode = function()\n   while true do\n      print(\'something\')\n      parallel.yield()\n   end\nend\nc = parallel.fork()\nc:exec(code) \n -- parent code\nfor i = 1,10 do\n    c:join()\nend \n -- each time join() is called, it waits for the child to yield, and vice-versa.\n-- in that example, \'something\' only gets printed when the parent joins its child\n``` \n Slightly more complex things can be implemented with yield/join: join() can take\na string as an argument, which is returned by the corresponding yield(). This\nis useful to control branching in your children: \n ``` lua\n-- child code:\ncode = function()\n   while true do\n      print(\'something\')\n      m = parallel.yield()\n      if m == \'break\' then break end\n   end\nend\nc = parallel.fork()\nc:exec(code) \n -- parent code\nc:join(\'break\')\n``` \n Sometimes you might want to wait for a process to actually terminate (die), so that\nyou can start new ones. The proper way to do this is to use the sync() function, \nwhich waits for the PID of that process to fully disappear from the OS. It also\nclears the child from the parallel.children list, and decrement parallel.nchildren. \n lua\ncode = function()\n     -- do nothing and die\nend\nparallel.nfork(1)              -- fork one process\nparallel.children:exec(code)   -- execute dummy code\nprint(parallel.nchildren)      -- prints: 1\nparallel.children:sync()       -- wait for all children (here only 1) to die\nprint(parallel.nchildren)      -- prints: 0\nparallel.nfork(2)              -- fork 2 processes\nprint(parallel.nchildren)      -- prints: 2\nprint(parallel.children[1])    -- prints: nil\nprint(parallel.children[2])    -- prints: table --- current running processes always\nprint(parallel.children[3])    -- prints: table --- exist in children[process.id] \n When creating a child (parallel.fork), a connection is established\nto transfer data between the two processes. Two functions send() and receive()\ncan be used to  efficiently  transfer data between these processes. Any Lua type, \nand all Torch7 type (tensor, storage, ...) can be transferred this way. The transmission\nis efficient for numeric data, as serialization merely involves a binary copy and\nsome extra headers for book-keeping (see serialization in Torch7\'s manual). \n ``` lua\n-- define some code for children\nsomecode = function()\n   while true do\n      -- in an infinite loop, receive objects from parent:\n      local obj = parallel.parent:receive()\n      -- print\n      parallel.print(\'received object:\', obj)\n   end\nend \n -- dispatch two processes:\nparallel.nfork(2)\nparallel.children:exec(somecode) \n -- and send them some data:\nt = {\'a table\', entry2=\'with arbitrary entries\', tensor=torch.Tensor(100,100)}\nwhile true do\n    parallel.children[1]:send(t)        -- send the whole table to child 1\n    parallel.children[2]:send(t.entry2) -- just send an entry to child 2\nend\n``` \n A convenient print function that prepends the process ID issuing the print: \n ``` lua \n \n parallel.print(\'something\') \n \n   something\n``` \n Last, but not least: always run your parent code in a protected call, to catch\npotential errors, Ctrl+C, and the likes, and terminate nicely. By terminating\nnicely, I mean: killing all remote processes that you forked... If you don\'t\ndo so, you leave you remote machines (and potentially yours) with hanging \nprocesses that are just waiting to receive data, and will not hesitate to get\nback in business the next time you run your parent code :-) \n ``` lua\nworker = function()\n       -- some worker code\nend \n parent = function()\n       -- some parent code\nend \n ok,err = pcall(parent)\nif not ok then\n   print(err)\n   parallel.close()   -- this is the key call: doing this will insure leaving a clean\n                      -- state, whatever the error was (ctrl+c, internal error, ...)\nend\n``` \n A simple complete example: \n ``` lua\n-- required libs\nrequire \'parallel\' \n -- define code for workers:\nfunction worker()\n   -- a worker starts with a blank stack, we need to reload\n   -- our libraries\n   require \'sys\'\n   require \'torch\' \n -- print from worker:\n   parallel.print(\'Im a worker, my ID is: \' .. parallel.id .. \' and my IP: \' .. parallel.ip) \n -- define a storage to receive data from top process\n   while true do\n      -- yield = allow parent to terminate me\n      m = parallel.yield()\n      if m == \'break\' then break end \n   -- receive data\n  local t = parallel.parent:receive()\n  parallel.print(\'received object with norm: \', t.data:norm())\n\n  -- send some data back\n  parallel.parent:send(\'this is my response\')\n \n end\nend \n -- define code for parent:\nfunction parent()\n   -- print from top process\n   parallel.print(\'Im the parent, my ID is: \' .. parallel.id) \n -- fork N processes\n   parallel.nfork(4) \n -- exec worker code in each process\n   parallel.children:exec(worker) \n -- create a complex object to send to workers\n   t = {name=\'my variable\', data=torch.randn(100,100)} \n -- transmit object to each worker\n   parallel.print(\'transmitting object with norm: \', t.data:norm())\n   for i = 1,1000 do\n      parallel.children:join()\n      parallel.children:send(t)\n      replies = parallel.children:receive()\n   end\n   parallel.print(\'transmitted data to all children\') \n -- sync/terminate when all workers are done\n   parallel.children:join(\'break\')\n   parallel.print(\'all processes terminated\')\nend \n -- protected execution:\nok,err = pcall(parent)\nif not ok then print(err) parallel.close() end\n``` \n License \n Copyright (c) 2011 Clement Farabet, Marco Scoffier \n Permission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n""Software""), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions: \n The above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software. \n THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.', ""LOpenGM: Lua bindings for OpenGM \n OpenGM  is a C++ library for graphical \nmodeling, and inference. The Lua\nbindings provide a simple way of describing graphs, from Lua, and then\noptimizing them with OpenGM. \n Note: this package is superseded by 'gm', a more general and\nsimple (pure Lua) package for graphical models. \n License \n LOpenGM Copyright (c) 2011 Clement Farabet (Lua Bindings) \n OpenGM  Copyright (c) 2010 by Bjoern Andres and Joerg Hendrik Kappes. \n This software was developed by Bjoern Andres and Joerg Hendrik Kappes.\nEnquiries shall be directed to: \n bjoern.andres@iwr.uni-heidelberg.de, kappes@math.uni-heidelberg.de \n All advertising materials mentioning features or use of this software must\ndisplay the following acknowledgement: ``This product includes the OpenGM\nlibrary developed by Bjoern Andres and Joerg Hendrik Kappes. Please direct\nenquiries concerning OpenGM to bjoern.andres@iwr.uni-heidelberg.de,\nkappes@math.uni-heidelberg.de''. \n Redistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met: \n \n Redistributions of source code must retain the above copyright notice,\n  this list of conditions and the following disclaimer. \n Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution. \n All advertising materials mentioning features or use of this software must\n  display the following acknowledgement: ``This product includes the OpenGM\n  library developed by Bjoern Andres and Joerg Hendrik Kappes. Please direct\n  enquiries concerning OpenGM to bjoern.andres@iwr.uni-heidelberg.de,\n  kappes@math.uni-heidelberg.de''. \n The names of the authors must not be used to endorse or promote products\n  derived from this software without specific prior written permission. \n \n THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR IMPLIED\nWARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\nMERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO\nEVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\nPROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\nOR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\nWHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\nOTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF\nADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n Install \n 1/ Torch7 is required: \n Dependencies, on Linux (Ubuntu > 9.04): \n sh\n$ apt-get install gcc g++ git libreadline5-dev cmake wget libqt4-core libqt4-gui libqt4-dev \n Dependencies, on Mac OS (Leopard, or more), using  Homebrew : \n sh\n$ brew install git readline cmake wget qt \n Then on both platforms: \n sh\n$ git clone https://github.com/andresy/torch\n$ cd torch\n$ mkdir build; cd build\n$ cmake ..\n$ make\n$ [sudo] make install \n 2/ Once Torch7 is available, install this package: \n sh\n$ [sudo] torch-pkg install opengm \n Use the library \n API, in very short: \n Load/start up package: \n lua\nrequire 'opengm' \n Construct a graph: \n lua\ng = opengm.Graph(...) \n Optimize a graph: \n lua\ng:optimize{} \n Display a graph, using Graphviz: \n lua\ng:show{} \n A simple complete example: \n ```lua\n-- load opengm\nrequire 'opengm' \n -- standard factors\nf = opengm.factors \n -- define variables\nvariables = {'car', 'person', 'building', 'street', 'vehicle'} \n -- define factors\nfactors = {-- unary factors (prior probabilities of each class):\n           {f.prior(0.9),  {'car'}},\n           {f.prior(0.01), {'person'}},\n           {f.prior(0.7),  {'building'}},\n           {f.prior(0.8),  {'street'}},\n           {f.prior(0.4),  {'vehicle'}},\n           -- Potts factors (joint probabilities):\n           {f.band(0),     {'car',      'person'}},\n           {f.band(0),     {'person',   'building'}},\n           {f.band(0),     {'building', 'street'}},\n           {f.band(0),     {'car',      'building'}},\n           {f.band(0),     {'building', 'vehicle'}},\n           {f.band(0),     {'street',   'vehicle'}},\n           {f.band(0),     {'person',   'vehicle'}},\n           {f.bimplies(1), {'car',      'vehicle'}}} \n -- create graph\ng = opengm.Graph(variables, factors) \n -- optimize graph\ng:optimize{method='a*', verbose=true} \n -- print graph\nprint(g)\n``` \n Running the script above outputs: \n <opengm> optimizing... \nstep 1: E=3.99758, c=0\nstep 2: E=3.63212, c=2.19722\nstep 3: E=3.63212, c=2.19722\n<opengm.Graph>\n  + nb of variables: 4\n  + nb of factors: 6\n  + graph is acyclic\n  + current (optimized) variable states: \n    - car [1]\n    - person [0]\n    - building [0]\n    - street [0]\n    - vehicle [1]"", ""Torch7 Library for iOS \n Torch7 provides a Matlab-like environment for state-of-the-art machine\nlearning algorithms. It is easy to use and provides a very efficient\nimplementation, thanks to an easy and fast scripting language (Lua) and a\nunderlying C implementation. \n This package has been modified (or just hacked) to fully compile\nTorch7 for iOS (iPad/iPhone) for all architectures (armv7, armv7a, arm64, i386 (simulator), x86_64 (simulator)) \n Requirements \n Torch7 needs to be installed prior to building the iOS\nversion. 'torch' needs to be available in the user's path. \n I recommend doing the easy install if you have not installed Torch7.\nhttp://torch.ch/docs/getting-started.html \n Building The Framework \n Simply run:\n$ ./generate_ios_framework \n This will build all torch's libraries as static libs, and export them\nin a single dir: framework/. The dir is ready to be included in\nan iOS project: it includes an example class to load Torch from within\nyour Objective C project. \n For examples full examples that utilize this class (Torch.m) please see \nthe ios_examples/ folder. More examples to come soon. \n Running \n When creating your Objective-C project simply import the class\nTorch.m/.h; include all the libs to the linker; add Torch.framework & Accelrate.framework\nand add all the Lua files as resources. define YOUR_FILE.lua and add it as \na resource. Run YOUR_FILE.lua using the method defined in Torch.h/.m"", ""liuflow: a wrapper around C.Liu's optical flow \n Note: this bit of code is a simple wrapper around the optical-flow\nalgorithm developped/published by C.Liu: \n C. Liu. Beyond Pixels: Exploring New Representations and Applications\nfor Motion Analysis. Doctoral Thesis. Massachusetts Institute of \nTechnology. May 2009. \n More at: http://people.csail.mit.edu/celiu/OpticalFlow/"", ""videograph: a package to create/manipulate graphs on videos \n This package provides standard functions to\ncreate and manipulate edge-weighted graphs \nof videos: create a graph, segment it, get \nits adjacency matrix, ... \n Install \n 1/ Torch7 is required: \n Dependencies, on Linux (Ubuntu > 9.04): \n sh\n$ apt-get install gcc g++ git libreadline5-dev cmake wget libqt4-core libqt4-gui libqt4-dev \n Dependencies, on Mac OS (Leopard, or more), using  Homebrew : \n sh\n$ brew install git readline cmake wget qt \n Then on both platforms: \n sh\n$ git clone https://github.com/andresy/torch\n$ cd torch\n$ mkdir build; cd build\n$ cmake ..\n$ make\n$ [sudo] make install \n 2/ Once Torch7 is available, install this package: \n sh\n$ [sudo] torch-pkg install videograph \n Use the library \n First run torch, and load videograph: \n sh\n$ torch   \n ``` lua \n \n require 'videograph'\n``` \n \n ..."", ""IPAM Graduate Summer School \n On Deep Learning, Feature Learning\nJuly 9 - 27, 2012 \n More info here.   \n Day 1: Setup \n \n \n Welcome to the Practical Sessions for the summer school \n \n \n Objectives: \n \n \n implementation-level understanding of supervised and unsupervised learning algorithms \n \n \n Many algorithms are more similar than researchers in the field\n    might have you believe! \n \n \n a sense of hyper-parameter sensitivities and run-times for various\n  algorithms \n \n \n appreciation for two approaches to programming deep learning experiments \n \n \n Code fragments for interactive exploration \n \n \n Full-blown application \n \n \n exposure to programming languages and software stacks: \n \n \n Python, NumPy, SciPy, Theano \n \n \n Lua, Torch7 \n \n \n \n \n Schedule: 1 hour on four days this first week \n \n \n Monday 12PM - 1PM:  Lua/Torch, Python/Theano, logging in to EC2 \n \n \n Tuesday 4PM - 5PM:  Supervised Learning in Lua and Python \n \n \n Wednesday 4PM - 5PM:  Unsupervised Learning in Lua and Python \n \n \n Thursday 4PM - 5PM:  TBA \n \n \n \n \n Session Structure \n \n \n Time is short for these practical sessions! \n \n \n Each day will start with two walk-throughs of things you can experiment with\n    (we'll try to be quick, to give you time afterward!) \n \n \n After the walk-throughs you can log in to an Amazon EC2 node where we've set\n    things up. \n \n \n For lack of time - you will have to choose whether to do the Lua thing or\n    the Python thing in the in-classroom time each day. \n \n \n We will negotiate with the organizers to leave the EC2 node up after the sessions \n \n \n We will be around all week - feel free to ask questions any time! \n \n \n We will be available by email after the first week. \n \n \n \n \n 10 mins crash course in Python, numpy \n \n \n intro to IPython notebook \n \n \n 10 mins crash course in Lua, Torch7 \n \n \n will review very basic Lua and Torch concepts, to get people started \n \n \n Remaining time - getting people into groups and setting them up to run the sample code\n  on laptop or EC2. Once they get it running, they can go for lunch or stick\n  around and play with things. \n \n \n Day 2: Supervised Learning \n \n \n Models: SVM, MLP, ConvNets, (Logistic Regression?) \n \n \n Data Sets: MNIST, CIFAR, Google Street View House Numbers (SVHN).\n  SVHN is an interesting new data set, very few results are available at this time \n  (and is more computer visionny that MNIST). \n \n \n Optimization Methods: SGD, ASGD, L-BFGS; batch vs. mini-batch vs. online \n \n \n Day 3: Feature Learning \n \n \n Python: Imprinting, K-Means, Autoencoder, De-noising Autoencoder, RBM,\n  (Sparse Coding?) \n \n \n Torch: Linar Autencoder, Convolutional Autoencoder, Linear and \n  Convolutional PSD (Predictive Sparse Decomposition) Autoencoder \n \n \n Day 4: To Be Decided \n \n \n Persitant Contrastive Divergence? \n \n \n Theano? \n \n \n Recurrent Neural Networks? \n \n \n GPU Programming 101? \n \n \n Torch/nn extensions: write your own modules \n \n"", 'csvigo: a package to handle CSV files (read and write). \n Install: \n First install Torch7 (www.torch.ch) then simply install this package\nusing luarocks: \n luarocks install csvigo \n Use: \n The library provides 2 high-level functions: csvigo.load and csvigo.save. To get help\non these functions, simply do: \n ``` \n \n csvigo.save()\ncsvigo.load()\n``` \n \n Loading a CSV file in \'query\' mode gives you a convenient query function that\nyou can use to query subsets of your original CSV file. To get help on this query\nfunction, simply do: \n ``` \n \n query = csvigo.load{path=\'somefile.csv\', mode=\'query\'}\nquery(\'help\')\n-- print some help\nall = query(\'all\')\nsubset = query(\'union\', {somevar=someval, someothervar={val1, val2}})\n``` \n \n Large CSV mode \n CSVigo supports efficient loading of very large CSV files into memory.\nThe loaded data structure is a read-only table with efficiency hidden under the hood. \n Loading: \n lua\nm = csvigo.load({path = ""my_large.csv"", mode = ""large""}) \n Printing by default only prints first 10 and last 10 rows\n lua\nprint(m) \n Individual element access\n lua\nprint(m[32]) \n Size of table:\n lua\nprint(#m) \n For loop over entries: \n Type 1:\n lua\nfor i=1, #m do\n    print(m[i]) -- get element\nend \n Type 2:\n lua\nfor k,v in ipairs(m) do\n    print(k)\n    print(v)\nend \n Type 3:\n lua\nfor k,v in pairs(m) do\n    print(k)\n    print(v)\nend \n Read-only table\n lua\n-- read only table, will error here:\nm[13] = \'a\'', 'json: a package to handle json (read and write). \n TAKEN From JSON4Lua, originally written for Lua 5.1. \n Wrapped for Torch7 (torch-rocks). \n JSON4Lua: JSON encoding / decoding support for the Lua language.\njson Module. \n Author: Craig Mason-Jones\nHomepage: http://json.luaforge.net/\nVersion: 0.9.40\nThis module is released under the MIT License (MIT). \n Install: \n First install Torch7 (www.torch.ch) then simply install this package\nusing torch-rocks: \n torch-rocks install json \n Usage: \n This module exposes 4 functions: \n ```\njson_string = encode(o)\n-- returns the table / string / boolean / number / nil / json.null value as a JSON-encoded string. \n o = decode(json_string)\n-- returns a Lua object populated with the data encoded in the JSON string json_string. \n save(json_file, o)\n-- saves the table / string / boolean / number / nil / json.null value as a JSON-encoded file. \n o = load(json_file)\n-- returns a Lua object populated with the data encoded in the JSON file.\n```', 'Torch Web Terminal \n This is a browser-based terminal for  Torch7 . \nThe goal of this project is to supersed the Qt4 interface, and to \nenable full graphics capabilities within the browser. \n This project is built around  Node.js , \na super lightweight asynchronous framework to build servers. In\nour case, the server is only use to connect clients (browser \nterminals) to Torch7 kernels. For now, one server instance can\nsupport an arbitrary number of clients, but each client only\nhas access to one Torch7 kernel. \n Dependencies \n You will need to install a couple of dependencies to enable\nthis web terminal: \n \n \n Node.js, which can be found  here , and\n  should also be installable with your system\'s package manager \n \n \n NPM: Node\'s package manager (sometimes comes with Node.js) \n \n \n Three Node.js packages:  ejs ,  stripcolorcodes  and  express \n    (version 2.x) \n \n \n For instance, on MacOS: \n bash\n$ brew install nodejs\n$ curl http://npmjs.org/install.sh | sh\n$ npm install express@2.x ejs stripcolorcodes \n Installation \n This project is bundled as a  torch-pkg  project, and can be \neasily installed like this: \n bash\n$ torch-pkg -local install webterm \n Or, if you have downloaded this repository on your machine, and\nyou are in its directory: \n bash\n$ torch-pkg -local deploy \n Note1: you have to deploy webterm locally (-local flag), as the node \npackages are only available to the current user. This could probably\nbe fixed, but I still don\'t know how. \n Note2: depending on the version of Node.js, you might have to do\nthe NPM install in the package directory, i.e.: \n bash\n$ cd ~/.torch/usr/share/torch/lua/webterm/\n$ npm install express@2.x ejs stripcolorcodes \n Running it \n webterm  is a standard package, so you can either require it from\na running torch instance, or start torch with it like that: \n bash\n$ torch -lwebterm \n This should produce the following output: \n text\nTry the IDE: torch -ide\nType help() for more info\nTorch 7.0  Copyright (C) 2001-2011 Idiap, NEC Labs, NYU\n==> Torch server listening on port 8080\n==> Open http://localhost:8080/ in your browser!\n=><= Torch instance started for [t7] \n At this stage, you just have to open a browser and go to \n http://localhost:8080/ . The cool thing\nof course is that you can access this adress remotely. Beware though\nthat this might open up serious security issues. \n Functions \n In the broswer, you will see a terminal, which provides full history\nand live completion. Completed entries are shown on the left pane, and\nare actual hyperlinks to documentation: \n \n One cool thing about a browser-based terminal is that all the plots\nand renderings you can generate during your session can be transparently\npiped to the console: \n \n The mechanism we use to do this is very simple: the image, or plot, is dumped\nas a png into the root of the Node.js server; and we then simply print a string\nof that form:  <img src=""dumped.png""/>  to the terminal. \n In fact, this mechanism is completely general: try doing this in the terminal: \n lua\nprint \'<h1>Some title</h1> <p>a paragraph...</a>\' \n Now even more powerful: you can really print arbitrary html there, so printing \nsomething like: \n lua\nprint \'<script> console.log(""this is javascript!"") </script>\' \n ... will just work perfectly fine! \n Multiple Users \n By default, the user is set to  t7 , which is what you should see in the terminal.\nYou can create a new user by appending the string  ?user=bob  to the URL. That\'ll\ncreate a  completely  new Torch7 kernel, which only Bob sees. \n TO DO \n \n completion is still buggy: it starts screwing up after too many nested parenthesis \n inline help (triggered by the  ?  symbol) is shitty: we should use the full html\n  help instead of the poor text-based help \n I\'d love to have notebook-like capabilities, where we can load a markdown file into\n  the browser (using the URL would be ok for now,  e.g.   ?file=myscript.md ), and \n  the text part would get rendered as html, and all the code blocks will be transformed\n  in interpretable code blocks,  ala  Mathematica/IPython. \n that last point implies that we need more flexible code entries, where we can go\n  back and forth to edit the code. \n ctrl+C: not working yet. It generates a INT signal, but it doesn\'t seem to\n  do much for now. \n', 'Taken from:\nhttp://www.steve.org.uk/Software/lua-fs/docs/index.html', ""Torch (Easy) Install \n This is just an easy install script for Torch7. Eventually, it will be folded into the main repo. \n The goal of this script is to enable one line installs. To install Torch on any machine, just do: \n curl -s https://raw.github.com/clementfarabet/torchinstall/master/install-all | bash\n \n Once installed, you should be able to run Torch with basic libraries: \n torch -lparallel -loptim -lpl -limage\n \n This script has been tested on MacOS X 10.8, and Ubuntu 12.04. It should work on earlier \nUbuntus and MacOS Xs, but other platforms are not supported. \n On Ubuntu you'll need 'sudo' privileges, as the default install is global, \nand the script needs to install dependencies. \n If you've already installed the dependencies, and don't have root privileges, you \ncan use this command to just install Torch: \n curl -s https://raw.github.com/clementfarabet/torchinstall/master/install-torch | bash\n \n By default, it will install Torch in /usr/local/ , you can override this\ndefault path by doing: \n curl -s https://raw.github.com/clementfarabet/torchinstall/master/install-torch | PREFIX=~/local bash\n \n Torch7 now ships wih Luarocks, bundlde into an executable called torch-rocks.\nYou can install new packages like this: \n torch-rocks search lua-cjson\ntorch-rocks install lua-cjson\n \n By default, torch-rocks includes a link to our own Rocks repository, hosted\n here . If you wish to publish your \npackages as rocks for Torch, simply clone this repo, add your rocks, and\nmake a pull request on Github! \n Updating from a previous version \n Note that if you are coming from a previous version you are advise to clean up the old installation\nwith the following commands \n rm -rf ~/.luarocks\nrm -rf /usr/local/lib/luarocks/\nrm -rf /usr/local/lib/lua/\nrm -rf /usr/local/share/torch/\nrm -rf /usr/local/share/lua/\nrm -rf /usr/local/lua/\nrm -rf /usr/local/etc/luarocks/"", ""nn2 \n nn2 is the successor of nn. The main thing we're trying to achieve here is: \n \n better consistency across modalities (Volumetric, Spatial, Temporal) \n better performance by packing features in memory \n \n TODO List \n 'Spatial' modules need to invert their convention, by packing the features\nin memory. Modules affected: \n \n SpatialConvolution \n SpatialConvolutionMap        \n SpatialMaxPooling \n SpatialSubSampling \n Spatial*Normalization \n SpatialLPPooling \n SpatialZeroPadding \n \n 'Volumetric' modules: \n \n VolumetricConvolution \n"", 'Lunatic: Python in Lua \n Run a python interpreter within Lua. Pass data between python and lua. \n Bug-fixed fork of  lua---python  which itself is forked from  Lunatic Page . \n See  Lunatic Page  for original documentation. \n Install \n Clone this repo locally \n $ git clone git@github.com:dylski/lua---python.git\n \n Build and install \n $ cd lua---python\n$ luarocks make python-scm-0.rockspec\n', 'torchffi \n Has moved to a more community friendly  repo .', 'XML <> Lua \n This was taken from  this site . Licensed under an MIT license. \n Simply repackaged their code for Torch.\nI also added a parse() function, which simplifies the xml->table cnoversion.', ""GraphicsMagick \n A simple Lua wrapper to  GraphicsMagick . \n Only tested on Mac OSX, with GraphicsMagick installed via Homebrew. \n gm.convert \n This is just a binding to the command line convert utility (images are not loaded\ninto Lua's memory). Examples: \n lua\ngm = require 'graphicsmagick'\ngm.convert{\n   input = '/path/to/image.png',\n   output = '/path/to/image.jpg',\n   size = '128x128',\n   quality = 95,\n   verbose = true\n} \n gm.info \n Similarly, gm.info(file) is a simple binding to the command line utility.\nIt's handy to extra the geometry of an image, as well as its exif metadata.\nOn top of it, if geolocation is found, the GPS location is nicely formatted. \n lua\ngm = require 'graphicsmagick'\ninfo = gm.info('some.jpeg')\nprint(info)\n{\n   width : 1024\n   height : 768\n   date : 2013:01:01 00:00:01\n   location :\n     {\n       longitude : W80.13\n       latitude : N25.79\n     }\n   format : JPEG\n   exif :\n     {\n        Make : Apple\n        FocalLength : 413/100\n        ...\n     }\n} \n gm.Image \n This is a full C interface to GraphicsMagick's Wand API. We expose one Class: the\nImage class, which allows loading and saving images, transforming them, and\nimporting/exporting them from/to torch Tensors. \n Load library: \n lua\ngm = require 'graphicsmagick' \n First, we provide two high-level functions to load/save directly into/form tensors: \n lua\nimg = gm.load('/path/to/image.png' [, type])    -- type = 'float' (default) | 'double' | 'byte'\ngm.save('/path/to/image.jpg' [,quality])        -- quality = 0 to 100 (for jpegs only) \n The following provide a more controlled flow for loading/saving jpegs. \n Create an image, from a file: \n lua\nimage = gm.Image('/path/to/image.png')\n-- or\nimage = gm.Image()\nimage:load('/path/to/image.png') \n Create an image, from a file, with a hint about the max size to be used: \n ```lua\nimage:load('/path/to/image.png', width [, height]) \n -- this tells the image loader that we won't need larger images than\n-- what's specified. This can speedup loading by factors of 5 to 10.\n``` \n Save an image to disk: \n ```lua\nimage:save('filename.ext') \n -- where:\n-- ext must be a know image format (jpg, JPEG, PNG, ...)\n-- (GraphicsMagick supports tons of them)\n``` \n Create an image, from a Tensor: \n ```lua\nimage = gm.Image(tensor,colorSpace,dimensions)\n-- or\nimage = gm.Image()\nimage:load(tensor,colorSpace,dimensions) \n -- where:\n-- colorSpace is: a string made of these characters: R,G,B,A,C,Y,M,K,I\n--                (for example: 'RGB', 'RGBA', 'I', or 'BGRA', ...)\n--                R: red, G: green, ... I: intensity\n--\n-- dimensions is: a string made of these characters: D,H,W\n--                (for example: 'DHW' or 'HWD')\n--                D: depth, H: height, W: width\n``` \n Export an image to a Tensor: \n ```lua\nimage = gm.Image('path.jpg')\nimage:toTensor(type,colorSpace,dimensions) \n -- where:\n-- type : 'float', 'double', or 'byte'\n-- colorSpace : same as above\n-- dimensions : same as above\n``` \n When exporting Tensors, we can specify the color space: \n ```lua\nlab = image:toTensor('float', 'LAB')\n-- equivalent to:\nimage:colorspace('LAB')\nlab = image:toTensor('float') \n -- color spaces available, for now:\n-- 'LAB', 'HSL', 'HWB' and 'YUV'\n``` \n Images can also be read/written from/to Lua strings, or binary blobs.\nThis is convenient for in memory manipulation (e.g. when downloading\nimages from the web, no need to write it to disk): \n ```lua\nblob,size = image:toBlob()\nimage:fromBlob(blob,size) \n str = image:toString()\nimage:fromString(str)\n``` \n In this library, we use a single function to read/write parameters\n(instead of the more classical get/set).  \n Here's an example of a resize: \n ```lua\n-- get dimensions:\nwidth,height = image:size() \n -- resize:\nimage:size(512,384) \n -- resize by only imposing the largest dimension:\nimage:size(512) \n -- resize by imposing the smallest dimension:\nimage:size(nil,512)\n``` \n Some basic transformations: \n lua\n-- flip or flop an image:\nimage:flip()\nimage:flop() \n Sharpen: \n lua\n-- Sharpens the image whith radius=0, sigma=0.6\nimage:sharpen(0, 0.6) \n Show an image (this makes use of Tensors, and Torch's Qt backend): \n lua\nimage:show() \n One cool thing about this library is that all the functions can be cascaded.\nHere's an example: \n lua\n-- Open, transform and save back:\ngm.Image('input.jpg'):flip():size(128):save('thumb.jpg')"", 'RestClient \n A simple client for REST APIs. This package provides a few functions \nto get and post from/to restful APIs.', ""LBFGS \n LibLBFGS (C Lib) wrapper. \n This is an FFI interface to  LibLBFGS . \n Installation \n Simply build and install  LibLBFGS  \n(with no SSE2 support, for now I don't support aligned memory blocks). \n This package can be installed with Luarocks. \n Usage \n The code in test.lua demonstrates how to use the solver. Its interface\nis 100% compatible to the solvers in  optim ."", ""CURL \n A simple interface to CURL. \n Provides two functions:  get  and  post . \n get : \n ```lua\n-- load lib:\ncurl = require 'curl' \n -- getting random pages:\nres = curl.get('http://www.google.com') \n -- with query:\nres = curl.get('http://www.google.com', {safe='off', output='search', oq='test'}) \n -- complete API:\nres = curl.get{\n    host = 'http://blogname.blogspot.com',\n    path = '/feeds/posts/default',\n    query = {\n        alt = 'json'\n    },\n    format = 'json' -- parses the output: json -> Lua table\n} \n -- Getting an image, and decoding it:\nimg = curl.get('http://www.webstandards.org/files/acid2/reference.png')\nrequire('graphicsmagick').Image():fromString(img):show()\n``` \n post : \n lua\n-- post has the same API, with a form parameter (instead of query)\nres = curl.post{\n    host = 'http://myserver.com',\n    path = '/',\n    form = {\n        username = 'bob',\n        password = 'key',\n        somefiletoupload = '@/local/path/to/file.jpg'\n    }\n}"", ""gfx.js: a browser-based graphics server \n Originally forked from the amazing  tty.js . \n The goal is to extend this project to support the creation of rich media windows,\non top of the terminal windows. \n The idea is simple: the server watches a directory, and monitors the creation &\nmodification of HTML files; upong modification / creation, it creates a new window\non the client side (browser), which simply render the HTML.  \n Clients are easy to develop: one simply needs to dump HTML into the watched\ndirectory to have it rendered by the browser. \n For now, I'm focusing on one client, written in Lua, for \n Torch7 . \n \n Check out  tty.js  for reference on the\noriginal project. Note: I'm simply extending their project, not modifying\nany of the core structure, so it should remain compatible. \n Installation \n You have to have Node.js (important, Version >= 0.10.0), NPM, and Torch7\ninstalled. With older versions of Node.js, things won't be stable. You also\nneed libgraphicsmagick-dev to be installed: \n ```sh \n OS X \n brew install graphicsmagick \n Ubuntu \n apt-get install libgraphicsmagick1-dev\napt-get install graphicsmagick\n``` \n Then simply run: \n sh\nluarocks install https://raw.github.com/clementfarabet/gfx.js/master/gfx.js-scm-0.rockspec \n Or, if you cloned the repo locally: \n sh\nluarocks make \n Execution \n Once installed, you can start/stop the server like this (I'm assuming a LuaJIT-based install): \n luajit -lgfx.start\nluajit -lgfx.stop \n And then open up a tab in your browser, at  http://localhost:8000 . \n The browser acts as a passive graphics backend. The server monitors the creation of new\nresources (charts, plots, videos, ...), and lets the browser know it should render them. \n The framework is very flexible: resources can be rendered by a client (luajit) with no\nbrowser open, and even no server listening/running. The resources generated will still\nbe saved, and can be visualized later (very useful to generate resources/charts on\na server with no X session). \n You can optionally specify a different port as an env variable, if the default (8000)\nis not available: \n PORT=4321 luajit -lgfx.start\nPORT=4321 luajit -lgfx.stop \n Also, we provide a useful PS script, which lists running servers: \n luajit -lgfx.ps \n On Mac OS, we also provide a shortcut to start the server in the background and automatically\nopen the browser at the right location: \n luajit -lgfx.go \n Alternatively, you can do things step by step: \n ```\nluajit -lgfx.start \n starts a server... \n luajit \n starts a Torch session \n ``` \n At the prompt, you can load the gfx.js client, and render things: \n lua\ngfx = require 'gfx.js'\ngfx.image(image.lena())\ngfx.image({\n   image.lena()\n   image.lena()\n   image.lena()\n   image.lena()\n   image.lena()\n   image.lena()\n   image.lena()\n   image.lena()\n}, {zoom=0.5, legends={'Image 1', 'Image 2'}}) \n This will produce this output: \n \n I've also slowly started to integrate plots from  NVD3 , and bind\nthem to Torch, so that they can seamlessly be called from the Torch repl: \n ```lua\ngfx.chart(data, {\n   chart = 'line', -- or: bar, stacked, multibar, scatter\n   width = 600,\n   height = 450,\n}) \n -- where data has the form:\ndata = {\n    {\n        key = 'Legend 1',\n        color = '#0f0',\n        values = { {x=0,y=0}, {x=1,y=1}, ... },\n    },\n    {\n        key = 'Legend 2',\n        color = '#00f',\n        values = { {x=0,y=0}, {x=1,y=1}, ... },\n    },\n} \n -- or, for a single dataset:\ndata = {\n    key = 'Legend',\n    values = { {x=0,y=0} , ... }\n} \n -- values can be provided in convenient ways:\nvalues = { {x=0,y=0[,size=0]}, ... }\nvalues = { {0,0,0}, ... }\nvalues = torch.randn(100,2)\nvalues = torch.randn(100,3)  -- the 3rd dimension is the optional size, only used by certain charts\nvalues = torch.randn(100) -- in this case, y is provided, x defaults to range(0,N-1) \n -- shortcuts are also provided for quick plots:\ngfx.chart(torch.randn(100,2), {chart='scatter'})\ngfx.chart(torch.randn(100), {chart='line'})  -- y is provided, x will be a range(1,N)\ngfx.chart({ torch.randn(100), torch.randn(100) })  -- multiple datasets\ngfx.chart({ {1,2,3,4,5,6,7,8,7,6,5,4,3,2,1}, torch.randn(100) })  -- multiple datasets, table format\n``` \n As explained above, one can generate resources/charts/figures with no server listening.\nOne can connect a server later on, and redraw the last resources generated. Here are a few\nuseful commands for that: \n lua\ngfx = require 'gfx.js'\nids = gfx.list(10) -- will list the last 10 figures generated (each figure has a unique ID)\nprint(ids[1])\n-- will print something like: dom_1212817597132847893127489\ngfx.redraw(ids[1]) -- will redraw this resource\ngfx.redraw(10) -- will redraw the last 10 resources available (sorted by descending time) \n Finally, the server gets slower as the number of resources/charts/images grows in the \nwatched directory. It's useful to sometimes clear this cache manually:\n gfx.clear()"", ""PERSIST \n A persisting table for Lua. \n Built using Redis, it's a simple abstraction that allows\none to write/read from a table that persists over sessions (the\nkey/vals are persisted in Redis). \n ```lua\n-- load lib:\np = require('persist')() \n -- write a few things to it:\np.test = 'something'\np.test2 = {\n    some = 'table',\n    nested = {is=1}\n}\n``` \n Shut down, start again: \n ```lua\n-- load lib:\np = require('persist')() \n -- still there?\nprint(p.test)\nprint(p.test2)\n``` \n The following options can be passed: \n lua\np = require('persist')({\n   url = 'localhost',\n   port = 6379,\n   verbose = false, -- this is not only used on startup\n   namespace = 'th',  -- this is the namespace in Redis\n   clear = false, -- clear all the data\n})"", 'ASyNC \n An async framework for Lua/Torch, based on  LibUV \n(using Tim Caswell\'s  luv  library). \n This lib is heavily inspired on the Node.js architecture. It\'s fun, elegant, and\nshould be extremely efficient (a lot of testing is required). \n The examples in  tests/  should provide enough documentation on the API. \n License \n MIT License \n Examples \n Starting the event loop. At the end of any program, the event loop must be started.\nNothing will be interpreted after this call, as it takes control of the runtime. \n lua\nasync.go() \n It\'s useful to have a REPL (interpreter) running asynchronously, for debugging and\nlive control of your programs: \n lua\nasync.repl()  -- fires up an asyncronous repl \n lua\nasync.repl.listen({host=\'0.0.0.0\', port=8080})   -- fires up an async repl through a TCP server\nasync.repl.connect({host=\'0.0.0.0\', port=8080})  -- connects to a remote repl through a TCP client \n Common JS like timer controls:\n lua\nasync.setInterval(millis, function()\n   print(\'printed every N millis\')\nend)\nasync.setTimeout(millis, function()\n   print(\'printed once in N millis\')\nend) \n CPU Info. Useful to know how many processors are available.\nThis is a synchronous call. \n lua\nprint(async.cpuInfo()) \n A TCP server: \n ```lua\nasync.tcp.listen({host=\'0.0.0.0\', port=8080}, function(client)\n   -- Receive:\n   client.ondata(function(chunk)\n      -- Data:\n      print(\'received: \' .. chunk) \n   -- Reply:\n  client.write(\'thanks!\')\n \n end) \n -- Done:\n   client.onend(function()\n      print(\'client gone...\')\n   end)\nend)\n``` \n A TCP client: \n ```lua\nasync.tcp.connect({host=\'127.0.0.1\', port=8080}, function(client)\n   -- Write something\n   client.write(\'something .. \' .. i) \n -- Callbacks\n   client.ondata(function(chunk)\n      print(\'received: \' .. chunk)\n      client.close()\n   end) \n -- Done:\n   client.onend(function()\n      print(\'connection closed...\')\n   end)\nend)\n``` \n File I/O. The low level interface is not complete yet, but the high-level one\nis final: \n lua\nasync.fs.readFile(\'LICENSE\', function(content)\n   print(content)\n   async.fs.writeFile(\'LICENSE.copy\', content, function(status, err)\n      print(\'==> wrote file: \' .. (status or err))\n   end)\nend) \n A lower-level interface is also available, for C-level performance. The upside:\nno copy is done, the user callback gets the raw pointer to the network buffer (read)\nand writes tap directly into the raw buffer, provided by the user. The downside:\nthe buffer returned by the ""ondata"" callback lives only for the scope of that callback,\nand must be copied by the user... \n ```lua\n-- assuming a client handle: \n local b = require \'buffer\' \n client.onrawdata(function(chunk)\n   -- chunk is a Buffer object (https://github.com/clementfarabet/buffer)\n   print(chunk) \n -- chunk will not be valid past this point, so its content must be copied,\n   -- not just referenced...\n   local safe = chunk:clone()\n   -- safe can be past around... \n -- the most common use is to copy that chunk into an existing storage,\n   -- for instance a tensor:\n   -- (assuming tensor is a torch.Tensor)\n   local dst = b(tensor)  -- creates a destination buffer on the tensor (a view, no copy)\n   dst:copy(src)\nend) \n -- write() also accepts buffers:\nclient.write( b\'this is a string saved in a buffer object\' ) \n -- last, the sync() interface can be set up in raw mode:\nclient.syncraw()\nlocal buffer = client.read()\n-- ...\n``` \n We also provide a simple async interface to CURL. \n Provides two functions:  get  and  post . \n get : \n ```lua\n-- simple URL:\nasync.curl.get(\'http://www.google.com\', function(res)\n    print(res)\nend) \n -- complete API:\nasync.curl.get({\n    host = \'http://blogname.blogspot.com\',\n    path = \'/feeds/posts/default\',\n    query = {\n        alt = \'json\'\n    },\n    format = \'json\' -- parses the output: json -> Lua table\n}, function(res)\n   print(res)\nend) \n -- Getting an image, and decoding it:\ncurl.get(\'http://www.webstandards.org/files/acid2/reference.png\', function(res)\n  local decoded = require(\'graphicsmagick\').Image():fromString(res)\nend)\n``` \n post : \n ```lua\n-- post has the same API, with a form parameter (instead of query):\nasync.curl.post({\n    host = \'http://myserver.com\',\n    path = \'/\',\n    form = {\n        username = \'bob\',\n        password = \'key\',\n        somefiletoupload = \'@/local/path/to/file.jpg\'\n    }\n}, function(res)\n   print(res)\nend) \n -- or a simple file upload:\nasync.curl.post({\n    host = \'http://myserver.com\',\n    path = \'/\',\n    file = \'@/path/to/file.png\',\n}, function(res)\n   print(res)\nend)\n```', ""Manifold \n A package to manipulate manifolds, for Torch7. \n Install \n sh\nluarocks install manifold \n Dependencies \n In order to be able to run the binaries, you need to install the package  libatlas3-base .\nOn a Ubuntu machine you can execute the following commands. \n sudo apt-get update\nsudo apt-get install libatlas3-base \n Use \n ```lua\n-- package:\nm = require 'manifold' \n -- a dataset:\nt = torch.randn(100,10) -- 100 samples, 10-dim each \n -- basic functions:\nns = m.neighbors(t) -- return the matrix of neighbors for all samples (sorted)\nds = m.distances(t) -- return the matrix of distances (L2)\nts = m.removeDuplicates(t) -- remove duplicates from dataset \n -- embeddings:\np = m.embedding.random(t, {dim=2})  -- embed samples into a 2D plane, using random projections\np = m.embedding.lle(t, {dim=2, neighbors=3})  -- embed samples into a 2D plane, using 3 neighbor (LLE)\np = m.embedding.tsne(t, {dim=2, perplexity=30})  -- embed samples into a 2D plane, using tSNE\n``` \n Demos \n To run the demos, simply type the following commands. \n sh\ncd demos\nqlua demo_swissroll.lua\nqlua demo_tsne.lua \n Below is an example of a t-SNE map produced on 5,000 MNIST digits by the  demos/demo_tsne.lua  demo. \n"", 'Buffer \n A buffer object for LuaJIT. The goal: efficient, C-speed, byte manipulation\nfor LuaJIT. \n Also provides interfaces to Torch\'s tensors and storages, for easy serialization. \n Install \n luarocks install buffer \n Simple use cases \n Load lib: \n ```lua \n \n b = require \'buffer\'\n``` \n \n Create a buffer, from a string, with a size, or from\nanother buffer: \n ```lua \n \n buf = b\'some\'\nprint(buf)\n \nbuf = b(10)\nprint(buf)\n \nbuf2 = b(buf)\nprint(buf2)\n \nbuf[1] = 10\nbuf[2] = 20\nprint(buf2)\n \n``` \n \n Creating buffers never makes copies. A buffer created from a string\nalways references the content of the string. A buffer created from\nanother buffer references the same buffer. \n Concatenating two buffers is done like it\'s done for strings: \n ```lua \n \n a = b\'some\' .. b\'thing\'\nstr = a:toString()\nprint(str)\nsomething\n``` \n \n The  toString  method simply returns a Lua string from the buffer. \nIn this case, the string is a copy, which won\'t be affected by further\nchanges of the buffer: \n ```lua \n \n a[1] = a[1] + 1\nprint(str)\nsomething\nprint(a:toString())\ntomething\n``` \n \n A slicing operator is provided: \n ```lua \n \n a = b\'testing\'\nprint(a[{1,4}])\ntest\na[{1,4}] = \'sing\'\na[{1,4}] = b\'sing\'  -- both supported\nprint(a)\nsinging\n``` \n \n A buffer can be created from a list of buffers, which provides efficient\nconcatenation: \n ```lua \n \n a1 = b\'test\'\na2 = b\'test\'\na3 = b\'again\'\na = b(a1,a2,a3)\nprint(a:toString())\ntesttestagain\nb = b( {a1,a2,a3} )\nprint(b:toString())\ntesttestagain\n``` \n \n Finally, cloning a buffer allows clean memory separation: \n ```lua \n \n a = b\'test\'\nc = a:clone()\n``` \n \n More advanced constructors are also available, to mount buffers on arbitrary\nmanaged or unmanaged chunks of memory. See tests for examples. \n Last, if Torch is available, converters are available from buffers to tensors\nand back. This is especially handy for multithreaded / multimachine environments,\nwhere exchanging tensors must be done at optimal speed (i.e. with no complex \nserialization). \n ```lua \n \n t = torch.FloatTensor(10):normal()\nbuf = b(t)\n-- buf is now a view on t\'s underlying contiguous storage\n-- buf could be transmitted over sockets / threads, as raw binary data (see async for use cases) \n \n -- from buf, new storages or tensors can be constructed like this: \n \n tt = buf:toFloatStorage()\ntt = buf:toFloatTensor()\ntt = buf:toFloatTensor(2,5)\n-- these are all views on the original storage of t.\n``` \n \n License \n Code was originally inspired from the Luvit folks. \n Copyright 2013-2014 Clement Farabet (MADBITS)\nCopyright 2012 The Luvit Authors. All Rights Reserved. \n Licensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at \n http://www.apache.org/licenses/LICENSE-2.0\n \n Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS-IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.', 'LuaForever \n Run a Lua/LuaJIT script forever. \n Example: \n ```bash\nluajit myscript.lua arg1 arg2 \n error, script crashes... \n luaforever myscript.lua arg1 arg2 \n error, script crashes... \n myscript.lua restarts automatically \n error, script crashes... \n myscript.lua restarts automatically \n ... \n ```', ""THMAP \n A simple distributed framework to map jobs/work onto multiple workers. \n The framework provides two binaries:  thmap  and  thnode .  thmap  is a controller,\nthat lets you mirror commands to multiple  thnode  instances, and absorb the output\nof all these  thnode  instances in //. \n The general philosophy is that you run a bunch of  thnode  instances on multiple machines,\nthen run  thmap  everytime you need to schedule/run new scripts onto these nodes. \n thmap  lets you git pull, and reload scripts, so that you can easily update a \ndistributed code base. \n Install \n sh\nluarocks install thmap \n Use \n On the compute machine(s), run  thnode : \n sh\nthnode \n Write a config file that lists all the available nodes ( nodes.lua ): \n lua\n-- example configuration, with two nodes:\nreturn {\n   {host='ip1', port=10001},\n   {host='ip2', port=10001}\n} \n (by default,  thnode  listens to port 10001, then increments if not available). \n Then start  thmap  to start monitoring and dispatching jobs: \n sh\nthmap --nodes nodes.lua\nip1:10001> ip2:10002> spawn('th', {'test.lua'})\n... \n The above shows you a double shell: you are connected to two  thnode  instances,\nand any command you issue will be mirrored to both. \n In its current version,  thmap  supports the following commands: \n \n spawn a job:             spawn('th', {'script.lua', 'opt1', 'opt2', ...}, {autorestart=true}) \n restart running jobs:    restart() \n list running jobs:       ps() \n exec git command:        git 'pull'  /  git 'status' \n git pull + restart:      update() \n kill all zombies:        zombies() \n"", 'Schedlua \n Basic scheduler.', ""(Note: this package is provided for compatibility with public tutorials.\n It is not maintained anymore.) \n GM: Graphical Models for Torch/LuaJIT \n This package provides standard functions to create (arbitrary) \nundirected graphical models, using adjacency matrices. \n A graph is described by an adjacency matrix, node potentials\nand edge potentials.  \n Install \n sh\n$ git clone ...\n$ [sudo] luarocks make \n Use \n First run torch, and load gm: \n sh\n$ th   \n ``` lua \n \n require 'gm'\n``` \n \n Once loaded, see and run the examples: \n ``` lua \n \n gm.examples.simple()\ngm.examples.trainMRF()\ngm.examples.trainCRF()\n``` \n"", ""nnop \n Parameter-free / operation-only Neural Network primitives\nfor torch/nn. \n Motivation, Goals \n Sometimes, it's useful to treat parameters as regular states,\nto either impose certain constraints on them, or simply\nmake weight sharing visible / straight-forward. \n The original design of  nn  treats\ntrainable parameters as special variables. This package,  nnop ,\nbuilds on  nn  and  nngraph , but separates parameters from operations. \n It introduces a new module,  nn.Parameters , which provides trainable\nparameters, but does not do any computation. Every other parameterized\nnode ( nn.Linear ,  nn.SpatialConvolution , ...) needs to be wrapped in\n nnop  to decouple trainable parameters, and become pure operation nodes. \n TODO \n \n wrap remaining parametrized nodes ( nn.SpatialConvolution , ...) \n simplify/unify auto-generated parameterNodes? \n \n Examples \n Weight sharing \n In this example, 2 modules are connected to a same set of trainable\nparameters. This is weight sharing. \n ```lua\n-- Create parameters:\nlinearWeight = nnop.Parameters(10,100)()\nlinearBias = nnop.Parameters(10)() \n -- Create multiple layers, all connected to these parameters:\ninput1 = nn.Identity()()\ninput2 = nn.Identity()()\nlinear1 = nnop.Linear()({input1, linearWeight, linearBias})\nlinear2 = nnop.Linear()({input2, linearWeight, linearBias}) \n -- Graph:\ngraph = nn.gModule({input1,input2}, {linear1,linear2}) \n -- Tests:\nres = graph:forward({torch.randn(100), torch.randn(100)})\nassert(type(res) == 'table' and #res == 2) \n input = torch.randn(100)\nres = graph:forward({input, input})\nassert(res[1]:dist( res[2] ) == 0)\n``` \n Penalty on a set of parameters \n In this example, we add an L1 penalty on a set of weight. \n When parameters are provided to the nnop.Linear constructor,\nparameter nodes are automatically created (and automatically\nconnected in the graph!). We use this in this example, this\nway we don't have to create the parameter nodes, but are still\nfree to access them and add a penalty on them. \n ```lua\n-- create base modules:\nlinear1 = nnop.Linear(10,100)\ntanh1 = nn.Tanh()\nlinear2 = nnop.Linear(100,2) \n -- bind them in a graph:\ninput = nn.Identity()()\nlayer1 = linear1(input)\nlayer2 = tanh1(layer1)\nlayer3 = linear2(layer2) \n -- get weights and impose penalty:\nweight1 = linear1.parameterNodes.weightNode\nsparse1 = nn.L1Penalty(.001)(weight1) \n -- build final model:\nmodel = nn.gModule({input}, {layer3}) \n -- train the model:\nfor i = 1,10 do\n   input = torch.rand(10)\n   output = model:forward(input)\n   gradOutput = torch.rand(2)\n   gradInput = model:updateGradInput(input, gradOutput)\n   model:accGradParameters(input, gradOutput)\nend\n```"", ""nnfunc \n Functionalize nn modules: the goal of this package is to make it\neasy to develop 3rd-party frameworks, by re-exposing nn modules\nas functions. Basically provide a functional API to nn. \n Every instantiated module becomes a simple state-less function:\ninput data and parameters must be provided as inputs to this function;\nsame thing for gradients. For convenience and efficiency, the state\nof the underlying nn module is still relied on for caching (every function\nreturned by nnfunc is a closure relying on an instantiated nn module). \n API \n Expose packages \n Any package that provides  nn.Module  children can be exposed. \n lua\nnnfunc.functionalize 'nn'   -- done by default by nnfunc\nnnfunc.functionalize 'nnx'  -- bundle new package... \n Once called, every module in the source package is available to\nuse as a function; see examples below. \n API #1 \n A single function that evaluates the module, and automatically\ncomputes gradients if  gradOutput  is provided. \n ```lua\n-- this returns a function that can be used to eval this module and\n-- its gradients:\nlayer = nnfunc.nn.Linear(10,100) \n -- compute module's output:\nprediction = layer({\n   input = torch.randn(10),\n   weight = torch.randn(100,10), bias = torch.randn(100),\n})\n-- prediction looks like this:\n-- {\n--    output = torch.Tensor(100)\n-- } \n -- output can be user-provided, optionally:\nprediction = layer({\n   input = torch.randn(10),\n   weight = torch.randn(100,10), bias = torch.randn(100),\n   output = torch.Tensor(100),\n})\n-- output is now valid \n -- compute gradients (backprop) - this happens automatically\n-- because gradOutput is provided:\ngrads = layer({\n   input = torch.randn(10),\n   weight = torch.randn(100,10), bias = torch.randn(100),\n   gradOutput = torch.randn(100),\n})\n-- grads looks like this:\n-- {\n--    gradInput = torch.Tensor(10),\n--    gradWeight = torch.Tensor(100,10),\n--    gradBias = torch.Tensor(100),\n-- } \n -- the user can also provide all the tensors for computed gradients,\n-- if her application requires that they be owned externally:\ngrads = layer({\n   input = torch.randn(10),\n   weight = torch.randn(100,10), bias = torch.randn(100),\n   gradOutput = torch.randn(100),\n   gradWeight = torch.zeros(100,10), bias = torch.zeros(100),\n   gradInput = torch.zeros(10),\n})\n-- user-provided gradInput, gradWeight and gradBias are now\n-- valid!\n``` \n API #2 \n Two separate functions: one for eval, one for gradients. This\ncan be useful when separate function pointers need to be used\nto register gradients. \n ```lua\n-- two separate functions:\nlayer,gradLayer = nnfunc.nn.Linear(10,100) \n -- compute module's output [same as API #1]:\nprediction = layer({\n   input = torch.randn(10),\n   weight = torch.randn(100,10), bias = torch.randn(100),\n}) \n -- compute gradients (backprop) [separate function for grads]:\ngrads = gradLayer({\n   input = torch.randn(10),\n   weight = torch.randn(100,10), bias = torch.randn(100),\n   gradOutput = torch.randn(100),\n})\n``` \n A hash table is also maintained to retrieve gradients associated\nto any object created: \n ```lua\n-- two separate functions:\nlayer,gradLayer = nnfunc.nn.Linear(10,100) \n -- gradLayer could be retrieve like this:\ngradLayer2 = nnfunc.gradsOf[layer]\nassert(gradLayer2 == gradLayer)\n```"", 'Something.', ""Regress \n A very simple regression test package. \n ```lua\nlocal test = require 'regress' \n test {\n   test1 = function()\n      test.mustBeTrue(a == b, 'a should == b')\n   end, \n test2 = function()\n      test.shouldBeTrue(a == b, 'a should == b')\n   end,\n}\n```""]",['repository_count'],MadbitsAI,https://avatars.githubusercontent.com/u/188959?v=4,False,2016-09-23 00:09:18,676,18,1,2010-01-24T19:09:14Z,2022-11-17T22:36:38Z,clmt,False,False,False,False,False,True,False,0,"{'C': 18, 'Lua': 45, 'Shell': 2, 'C++': 3, 'PHP': 1, 'JavaScript': 3, 'Vim Script': 1, 'Perl': 1, 'Python': 1, 'Cuda': 1}",9,Palo Alto,Mobile,18,0.19,0.93,0.17,0
ajabri,A. Jabri,PhD student,,"['pytorch-maml \n This is a PyTorch implementation of the supervised learning experiments from the paper \nModel-Agnostic Meta-Learning (MAML): https://arxiv.org/abs/1703.03400 \n Important : You will need the latest version of PyTorch, v.0.2.0 to run this code (otherwise you will get errors about \ndouble backwards not being supported). \n Currently, only the Omniglot experiments have been replicated here. The hyper-parameters are the same as those used in the original \nTensorflow implementation, except that only 1 random seed is used here. \n 5-way 1-shot training, best performance 98.9% \n \n 20-way 1-shot training, best performance 92% \n \n Note: the 20-way performance is slightly lower than that reported in the paper (they report 95.8%). If you can see why this might be,\nplease let me know. Also in this experiment, we can see evidence of overfitting to the meta-training set. \n The 5-way results are achieved by simply meta-testing the network trained on the 1-shot task on the 5-shot task (e.g. for the 5-way 5-shot result, test the 5-way 1-shot trained network with 5-shots). Again the 20-way result is lower here than reported in the paper. \n This repo also contains code for running maml experiments on permuted MNIST (tasks are created by shuffling the labels).\nThis is a nice sanity check task.', 'carml', ""Space-Time Correspondence as a Contrastive Random Walk \n  ![](https://github.com/ajabri/videowalk/raw/master/figs/teaser_animation.gif)  \n \n \n \n This is the repository for  Space-Time Correspondence as a Contrastive Random Walk , published at NeurIPS 2020.   \n [ Paper ]\n[ Project Page ]\n[ Slides ]\n[ Poster ]\n[ Talk ] \n @inproceedings{jabri2020walk,\n    Author = {Allan Jabri and Andrew Owens and Alexei A. Efros},\n    Title = {Space-Time Correspondence as a Contrastive Random Walk},\n    Booktitle = {Advances in Neural Information Processing Systems},\n    Year = {2020},\n} \nConsider citing our work or acknowledging this repository if you found this code to be helpful :) \n Requirements \n \n pytorch (>1.3) \n torchvision (0.6.0) \n cv2 \n matplotlib \n skimage \n imageio \n \n For visualization ( --visualize ):\n- wandb\n- visdom\n- sklearn \n Train \n An example training command is:\n python -W ignore train.py --data-path /path/to/kinetics/ \\\n--frame-aug grid --dropout 0.1 --clip-len 4 --temp 0.05 \\\n--model-type scratch --workers 16 --batch-size 20  \\\n--cache-dataset --data-parallel --visualize --lr 0.0001 \n This yields a model with performance on DAVIS as follows (see below for evaluation instructions), provided as  pretrained.pth :\n J&F-Mean    J-Mean  J-Recall  J-Decay    F-Mean  F-Recall   F-Decay\n  0.67606  0.645902  0.758043   0.2031  0.706219   0.83221  0.246789 \n Arguments of interest: \n \n --dropout : The rate of edge dropout (default  0.1 ). \n --clip-len : Length of video sequence. \n --temp : Softmax temperature. \n --model-type : Type of encoder. Use  scratch  or  scratch_zeropad  if training from scratch. Use  imagenet18  to load an Imagenet-pretrained network. Use  scratch  with  --resume  if reloading a checkpoint. \n --batch-size : I've managed to train models with batch sizes between 6 and 24. If you have can afford a larger batch size, consider increasing the  --lr  from 0.0001 to 0.0003. \n --frame-aug :  grid  samples a grid of patches to get nodes;  none  will just use a single image and use embeddings in the feature map as nodes. \n --visualize : Log diagonistics to  wandb  and data visualizations to  visdom . \n \n Data \n We use the official  torchvision.datasets.Kinetics400  class for training. You can find directions for downloading Kinetics  here . In particular, the code expects the path given for kinetics to contain a  train_256  subdirectory. \n You can also provide  --data-path  with a file with a list of directories of images, or a path to a directory of directory of images. In this case, clips are randomly subsampled from the directory. \n Visualization \n By default, the training script will log diagnostics to  wandb  and data visualizations to  visdom . \n Pretrained Model \n You can find the model resulting from the training command above at  pretrained.pth .\nWe are still training updated ablation models and will post them when ready. \n \n Evaluation: Label Propagation \n The label propagation algorithm is described in  test.py .  The output of  test.py  (predicted label maps) must be post-processed for evaluation. \n DAVIS \n To evaluate a trained model on the DAVIS task, clone the  davis2017-evaluation  repository, and prepare the data by downloading the  2017 dataset  and modifying the paths provided in  eval/davis_vallist.txt . Then, run: \n Label Propagation: \n python test.py --filelist /path/to/davis/vallist.txt \\\n--model-type scratch --resume ../pretrained.pth --save-path /save/path \\\n--topk 10 --videoLen 20 --radius 12  --temperature 0.05  --cropSize -1 \nThough  test.py  expects a model file created with  train.py , it can easily be modified to be used with other networks. Note that we simply use the same temperature used at training time. \n You can also run the ImageNet baseline with the command below.\n python test.py --filelist /path/to/davis/vallist.txt \\\n--model-type imagenet18 --save-path /save/path \\\n--topk 10 --videoLen 20 --radius 12  --temperature 0.05  --cropSize -1 \n Post-Process: \n``` \n Convert \n python eval/convert_davis.py --in_folder /save/path/ --out_folder /converted/path --dataset /davis/path/ \n Compute metrics \n python /path/to/davis2017-evaluation/evaluation_method.py \\\n--task semi-supervised   --results_path /converted/path --set val \\\n--davis_path /path/to/davis/\n``` \n You can generate the above commands with the script below, where removing  --dryrun  will actually run them in sequence.\n python eval/run_test.py --model-path /path/to/model --L 20 --K 10  --T 0.05 --cropSize -1 --dryrun \n Test-time Adaptation \n To do.""]",['repository_count'],UC Berkeley / BAIR,https://avatars.githubusercontent.com/u/1448621?u=29c6a12f5b1911d99562a9ac53153ef2df80941f&v=4,False,2016-10-14 23:17:29,152,12,0,2012-02-18T06:59:44Z,2022-10-03T20:31:27Z,,False,False,False,False,False,True,False,0,"{'Java': 3, 'Jupyter Notebook': 2, 'Python': 2}",1,,Machine Learning,99,0.15,0.76,0.37,0
korymath,Kory,"Research Scientist DeepMind, 

CS PhD University of Alberta

Intern Apple Special Projects Group, Google Brain, Twitter Cortex ",korymath@gmail.com,"['Gaze Vector Regression Testing \n Directory structure: \n git pull https://github.com/korymath/gazevectorregression \n Data files as *.mat files should be located in \n /data \n run_basic_exp will run on all *.mat files in the /data folder. \n run_single_test provides a single experiment and you need to set train and test data files \n Model building \n Model is built in build_single_model.m \n Can edit the way that the model is built by changing the mdl to other model builders from MATLAB. \n modelBuildingIdeas.m has several ideas to try. I think that the best option will be a bagged regression, wishing there was a more automated trial set up in matlab. \n One thing to do would be to output all the experiments to train and test directories so that we could rerun validations in Python autoML libraries, and faster c-modules to see if we could build up a solid single model. \n Notes on data from : \n all calibrations are sweep, eyes, free \n the column switches between sweep and task \n RXX - XX patient number \n Calib(C|P) - cups or pasta, when this changes, task changes, always three for each\nassume new task mean adjustment \n B  - ignore (both eye and movement) \n NN - 01, 02, 03 in sequence trials -- 1 and 2 are before then a whole bunch of trials, then 3 is the after trials calibration \n _combined_segments - eye data, \n TODO:  \n TESTING 1 on 2 and 1 on 3 compare', 'ContinuousTimeActorCritic \n Continuous Time Natural Actor Critic Reinforcement Learning \n This repository is a basic actor-critic reinforcement learning implementation based on the following papers: \n http://www.ualberta.ca/~pilarski/docs/papers/Pilarski_2011_ICORR.pdf \n and \n http://www.ualberta.ca/~pilarski/docs/papers/Pilarski_2013_ICORR_Postprint.pdf \n How can ACRL be used for multi-joint control of robotic limbs? ', 'cont-RL-order \n Experimenting with mountain car continuous task using order switching \n Basic Mountain Car Q-learning with some hot mods.  \n To plot use the plot function with the following syntax: \n python plot.py value500ep', 'Pyggy \n Artificial Improvisation \n This is a chat bot used in a live performance.', ""The Plotto Plot Machine \n A program to automate the algorithm described in  William Wallace Cook's Plotto, The Masterbook of All Plots . \n This program is written in Python for ease & legibility. \n This program relies upon the text of Plotto to generate story structure outlines. \n The text of Plotto is obtained from: https://openlibrary.org/works/OL16087095W/Plotto. \n This work is licensed under a  Creative Commons Attribution-ShareAlike 4.0 International License ."", 'runkeeper-testing \n Modified code from  @kylemath  for comparing multiple similar length trials of the same distance.  \n Details \n This matlab code works by smoothing the time derivative (smoothLength) of the GPX data and then plotting comparable runs of similar distances on the same plot. \n It also adds markers for kilometers and miles.  \n It works really well to see the comparison of two (or mulitple) runs, as can be seen right here:  \n', '#StockView \n go/StockView \n Twitter can tell us what\'s happening, but it is not great for finding business information. \n With this extension, you will be able to see live stock information right in your Twitter feed providing context for the current discussion. \n Now, when you search Twitter for cashtags like \'$TWTR\', the stock chart will be displayed so you can really know what is happening. \n \n Install \n #StockView is packaged as a chrome extension. You can download it here https://github.com/korymath/StockView/raw/master/app.crx. \n To install, you should just drag \'n\' drop the .crx file into Chrome.\nIn Chrome, open Extensions tab (chrome://extensions), drag \'n\' drop the .crx file and you are good to go. \n Amusingly ""for regular Windows users who are not skilled with computers, it is practically not possible to install and use extensions from outside the Chrome Web Store."" -- http://stackoverflow.com/questions/24577024/install-chrome-extension-not-in-the-store \n Usage \n Try searching for a single ( $TWTR ) or multiple ( $TWTR $MSFT $FB ) cashtags, using the syntax $SYMBOL. \n Development Screenshots \n Before \n Company is suggested, perhaps an account is shown and several recent tweets populate the timeline below. \n \n v1 \n Rough chart inserted nicely. \n \n v2 \n Beautiful chart comes in. \n \n v3 \n Multiple stock tickers. \n \n Technology \n #StockView uses  Highstocks  for visualization, and the data comes from the  Yahoo Stock API . ', 'brds \n A not-so-intelligent intelligent thing \n Install \n ~~~\npip install -r requirements.txt\n~~~ \n Run \n ~~~\npython decomp.py\n~~~', 'simple-chatroom \n Basic Python Chatroom', 'emrl \n emrl', 'Visual Illusions \n Can we teach a machine to categorise visual illusions and generate new ones?  \n Dataset \n \n https://github.com/robertmaxwilliams/optical-illusion-dataset \n https://www.moillusions.com/ \n http://viperlib.york.ac.uk/ \n https://twitter.com/AkiyoshiKitaoka \n http://www.psy.ritsumei.ac.jp/~akitaoka/saishin58e.html -- dating back to June 2002 (http://www.psy.ritsumei.ac.jp/~akitaoka/o1saishe.html) \n \n Details, Categories and Meta Data \n https://en.wikipedia.org/wiki/List_of_optical_illusions \n Related work \n Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images (https://arxiv.org/abs/1412.1897)', 'partitioning-groups \n An elegant way to partition groups. ', 'Talk Powerpoint Generator \n \n \n \n This program automatically generates PowerPoints about any topic.\nThese presentation slide decks can be used by improvisers for the improvisational comedy format  ""Improvised TED talk""  or  ""Powerpoint Karaoke"" .\nIn such games, the actors have to present an unseen presentation slide deck, but pretend to be an expert and explain  ""their""  slide show choices. \n Demo \n Ty out this generator on our online platform:  talkgenerator.com . \n Example \n \n Easy Install and Run \n Our program relies on certain APIs that require authentication in order to use it.\nCreate a file named  .env  (don\'t forget the period) in your project directory, and fill this with the correct API keys as described on our  wiki page about this . \n ```sh \n Make a new Python 3 virtual environment \n python3 -m venv venv; \n Activate the virtual environment \n source venv/bin/activate; \n Upgrade pip and install  requirements \n pip install --upgrade pip setuptools;\npython3 -m pip install -r requirements.txt; \n Download NLTK dependencies \n python run_nltk_download.py; \n Install the Talk Generator \n pip install -e .; \n Generate a 10 slide talk with topic peanuts \n talkgenerator --topic ""peanuts"" --num_slides 10\n``` \n Run arguments \n | Argument               | Description               |\n| ---------------------- | ------------------------- |\n|  topic  | The topic of the generator. This works best if it is a common, well-known noun. Use comma-separated words to generate a slide deck about multiple topics |\n|  slides  | The number of slides in the generated presentation ( default: 10 ) |\n|  schema  | The presentation schema to use when generating the presentation. Currently, only two modes are implemented, being  default  and  test  (for testing during development) |\n|  title  | Title of the presentation. Either  topic  or this one should to be set in order to generate a slide deck (just setting  topic  is usually more fun though)  |\n|  presenter  | The name that will be present on the first slide. Leave blank for an automatically generated name |\n|  output_folder  | The folder to output the generated presentations ( default:  ./output/ ) |\n|  save_ppt  | If this flag is true( default ), the generated powerpoint will be saved on the computer in the  output_folder |\n|  open_ppt  | If this flag is true ( default ), the generated powerpoint will automatically open after generating|\n|  parallel  | If this flag is true ( default ), the generator will generate all slides in parallel | \n Program structure \n See the  wiki  to know more about the inner implementation. \n Tests \n Test files are  tests/*.py , prefixed with  test_ . Test files use the  unittest  module.\nThey can easily be run all together when using PyCharm by right clicking on  talk-generator  and pressing  Run \'Unittests in talk-generator\' \n sh\ncoverage run -m pytest; coverage html \n Test coverage is automatically handled by  codecov . Tests are automatically run with CircleCI based on the  .yml  file in the  .circleci  directory. \n Credits \n This generator is made by\n Thomas Winters \nand  Kory Mathewson ,\nwith contributions from\n Shaun Farrugia \nand  Julian Faid . \n If you would like to refer to this project in academic work, please cite the following paper: \n Winters T., Mathewson K.W. (2019)  Automatically Generating Engaging Presentation Slide Decks . In: Ekárt A., Liapis A., Castro Pena M. (eds) Computational Intelligence in Music, Sound, Art and Design. EvoMUSART 2019. Lecture Notes in Computer Science, vol 11453. Springer, Cham \n sh\n@InProceedings{winters2019tedric,\n    author=""Winters, Thomas\n    and Mathewson, Kory W."",\n    editor=""Ek{\\\'a}rt, Anik{\\\'o}\n    and Liapis, Antonios\n    and Castro Pena, Mar{\\\'i}a Luz"",\n    title=""Automatically Generating Engaging Presentation Slide Decks"",\n    booktitle=""Computational Intelligence in Music, Sound, Art and Design"",\n    year=""2019"",\n    publisher=""Springer International Publishing"",\n    address=""Cham"",\n    pages=""127--141"",\n    isbn=""978-3-030-16667-0""\n} \n License \n MIT License. Copyright (c) 2018-2020  Kory Mathewson  and  Thomas Winters', 'jann \n \n \n \n Hi. I am  jann . I am a retrieval-based chatbot. I would make a great baseline. \n Allow me to (re)introduce myself \n I uses approximate nearest neighbor lookup using  Spotify\'s Annoy (Apache License 2.0)  library, over a distributed semantic embedding space ( Google\'s Universal Sentence Encoder (code: Apache License 2.0)  from  TensorFlow Hub . \n Objectives \n The goal of  jann  is to explicitly describes each step of the process of building a semantic similarity retrieval-based text chatbot. It is designed to be able to use diverse text source as input (e.g. Facebook messages, tweets, emails, movie lines, speeches, restaurant reviews, ...) so long as the data is collected in a single text file to be ready for processing. \n Install and configure requirements \n Note:  jann  development is tested with Python 3.8.6 on macOS 11.5.2 and Ubuntu 20.04. \n To run  jann  on your local system or a server, you will need to perform the following installation steps. \n ```sh \n OSX: Install homebrew \n /bin/bash -c ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)"" \n OSX: Install wget \n brew install wget \n Configure and activate virtual environment \n python3.8 -m venv venv\nsource venv/bin/activate \n python --version \n Ensure Python 3.8.10 \n Upgrade Pip \n pip install --upgrade pip setuptools \n Install requirements \n pip install -r requirements.txt \n Install Jann \n python setup.py install \n Set environmental variable for TensorFlow Hub \n export TFHUB_CACHE_DIR=Jann/data/module \n Make the TFHUB_CACHE_DIR \n mkdir -p ${TFHUB_CACHE_DIR} \n Download and unpack the Universal Sentence Encoder Lite model (~25 MB) \n wget ""https://tfhub.dev/google/universal-sentence-encoder-lite/2?tf-hub-format=compressed"" -O ${TFHUB_CACHE_DIR}/module_lite.tar.gz\ncd ${TFHUB_CACHE_DIR};\nmkdir -p universal-sentence-encoder-lite-2 && tar -zxvf module_lite.tar.gz -C universal-sentence-encoder-lite-2;\ncd -\n``` \n Download Cornell Movie Dialog Database \n Download the  Cornell Movie Dialog Corpus , and extract to  data/CMDC . \n ```sh \n Change directory to CMDC data subdirectory \n mkdir -p Jann/data/CMDC\ncd Jann/data/CMDC/ \n Download the corpus \n wget http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip \n Unzip the corpus and move lines and convos to the main directory \n unzip cornell_movie_dialogs_corpus.zip\nmv cornell\\ movie-dialogs\\ corpus/movie_lines.txt movie_lines.txt\nmv cornell\\ movie-dialogs\\ corpus/movie_conversations.txt movie_conversations.txt \n Change direcory to jann\'s main directory \n cd -\n``` \n As an example, we might use the first 50 lines of movie dialogue from the  Cornell Movie Dialog Corpus . \n You can set the number of lines from the corpus you want to use by changing the parameter  export NUMLINES=\'50\'  in  run_examples/run_CMDC.sh . \n Tests \n sh\npytest --cov-report=xml --cov-report=html --cov=Jann \n You should see all the tests passing. \n (simple) Run Basic Example \n ```sh\ncd Jann \n make sure that the run code is runnable \n chmod +x run_examples/run_CMDC.sh \n run it \n ./run_examples/run_CMDC.sh\n``` \n (advanced) Running Model Building \n jann  is composed of several submodules, each of which can be run in sequence as follows: \n ```sh \n Ensure that the virtual environment is activated \n source venv/bin/activate \n Change directory to Jann \n cd Jann \n Number of lines from input source to use \n export NUMTREES=\'100\' \n Number of neighbors to return \n export NUMNEIGHBORS=\'10\' \n Define the environmental variables \n export INFILE=""data/CMDC/all_lines_50.txt"" \n Embed the lines using the encoder (Universal Sentence Encoder) \n python embed_lines.py --infile=${INFILE} --verbose \n Process the embeddings and save as unique strings and numpy array \n python process_embeddings.py --infile=${INFILE} --verbose \n Index the embeddings using an approximate nearest neighbor (annoy) \n python index_embeddings.py --infile=${INFILE} --verbose --num_trees=${NUMTREES} \n Build a simple command line interaction for model testing \n python interact_with_model.py --infile=${INFILE} --verbose --num_neighbors=${NUMNEIGHBORS}\n``` \n Interaction \n For interaction with the model, the only files needed are the unique strings ( _unique_strings.csv ) and the Annoy index ( .ann ) file.  \n With the unique strings and the index file you can build a basic interaction.  \n This is demonstrated in the  interact_with_model.py  file. \n Pairs \n Conversational dialogue is composed of sequences of utterances. The sequence can be seen as pairs of utterances: inputs and responses. \n Nearest neighbours to a given input will find neighbours which are semantically related to the input. By storing input<>response pairs, rather than only inputs,  jann  can respond with a response to similar inputs. This example is shown in  run_examples/run_CMDC_pairs.sh . \n Run Web Server \n jann  is designed to run as a web service to be queried by a dialogue interface builder. For instance,  jann  is natively configured to be compatible with  Dialogflow Webhook Service . The web service runs using the Flask micro-framework and uses the performance-oriented  gunicorn  application server to launch the application with 4 workers. \n ```sh\ncd Jann \n run the pairs set up and test the interaction \n ./run_examples/run_CMDC_pairs.sh \n pairs set up will write files needed for web server deployment \n default data_key is all_lines_0 \n start development server \n python app.py \n or serve the pairs model with gunicorn and 4 workers \n gunicorn --bind 0.0.0.0:8000 app:JANN -w 4\n``` \n Monitoring \n It is helpful to see a Flask Monitoring dashboard to monitor statistics on the bot. There is a  Flask-MonitoringDashboard  which is already installed as part of Jann, see  Jann/app.py . \n To view the dashboard, navigate to  http://0.0.0.0:8000/dashboard . The default user/pass is:  admin  /  admin . \n Load / Lag Testing with Locust \n Once  jann  is running, in a new terminal window you can test the load on the server with  Locust , as defined in  Jann/tests/locustfile.py : \n sh\nsource venv/bin/activate\ncd Jann/tests\nlocust --host=http://0.0.0.0:8000 \n You can then navigate a web browser to  http://0.0.0.0:8089/ , and simulate  N  users spawning at  M  users per second and making requests to  jann . \n Testing the model by hand \n sh\ncurl --header ""Content-Type: application/json"" \\\n  --request POST \\\n  --data \'{""queryResult"": {""queryText"": ""that sounds really depressing""}}\' \\\n  http://0.0.0.0:8000/model_inference \n Response: \n sh\n{""fulfillmentText"":""Oh, come on, man. Tell me you wouldn\'t love it!""} \n Custom Datasets \n You can use any dataset you want! Format your source text with a single entry on each line, as follows: \n ```sh \n data/custom_data/example.txt \n This is the first line.\nThis is the second line, a response to the first line.\nThis is the third line.\nThis is the fourth line, a response to the third line.\n``` \n Using other Universal Sentence Encoder embedding modules \n There are  a collection of Universal Sentence Encoders  trained on a variety of data. \n Note from  TensorFlow Hub : The module performs best effort text input preprocessing, therefore it is not required to preprocess the data before applying the module. \n ```sh \n Standard Model (914 MB) \n wget \'https://tfhub.dev/google/universal-sentence-encoder/4?tf-hub-format=compressed\' -O module_standard.tar.gz\nmkdir -p universal-sentence-encoder && tar -zxvf module_standard.tar.gz -C universal-sentence-encoder\n``` \n Annoy parameters \n There are two parameters for the Approximate Nearest Neighbour: \n \n set  n_trees  as large as possible given the amount of memory you can afford, \n set  search_k  as large as possible given the time constraints you have for the queries. This parameter is a interaction tradeoff between accuracy and speed. \n \n Run details for GCP serving using nginx and uwsgi \n You will need to configure your server with the necessary software: \n ```sh\nsudo apt update\nsudo apt -y upgrade\nsudo apt install unzip python3-pip python3-dev python3-venv build-essential libssl-dev libffi-dev python3-setuptools\nsudo apt-get install nginx\ngit clone https://github.com/korymath/jann \n and follow the installation and configuration steps above \n sudo /etc/init.d/nginx start    # start nginx\n``` \n Then, you can reference a more in-depth guide  here . And here is a walkthrough on how to configure  nginx on GCP . \n You will need the uwsgi_params file, which is available in the nginx directory of the uWSGI distribution, or from  the nginx GitHub repository . \n ```sh \n uwsgi_param  QUERY_STRING       $query_string;\nuwsgi_param  REQUEST_METHOD     $request_method;\nuwsgi_param  CONTENT_TYPE       $content_type;\nuwsgi_param  CONTENT_LENGTH     $content_length; \n uwsgi_param  REQUEST_URI        $request_uri;\nuwsgi_param  PATH_INFO          $document_uri;\nuwsgi_param  DOCUMENT_ROOT      $document_root;\nuwsgi_param  SERVER_PROTOCOL    $server_protocol;\nuwsgi_param  REQUEST_SCHEME     $scheme;\nuwsgi_param  HTTPS              $https if_not_empty; \n uwsgi_param  REMOTE_ADDR        $remote_addr;\nuwsgi_param  REMOTE_PORT        $remote_port;\nuwsgi_param  SERVER_PORT        $server_port;\nuwsgi_param  SERVER_NAME        $server_name;\n``` \n Copy it into your project directory (e.g.  /home/${USER}/jann/uwsgi_params ).\nIn a moment we will tell nginx to refer to it. \n We will serve our application over HTTP on port 80, so we need to enable it: \n sh\nsudo ufw allow \'Nginx HTTP\' \n This will allow HTTP traffic on port 80, the default HTTP port. \n We can check the rule has been applied with: \n ```sh\nsudo ufw status \n Status: active \n To                         Action      From \n --                         ------      ---- \n Nginx HTTP                 ALLOW       Anywhere \n Nginx HTTP (v6)            ALLOW       Anywhere (v6) \n ``` \n Make a Systemd unit file: \n sh\n[Unit]\nDescription=JANN as a well served Flask application.\nAfter=network.target\n[Service]\nUser=korymath\nGroup=www-data\nWorkingDirectory=/home/korymath/jann/Jann\nEnvironment=""PATH=/home/korymath/jann/venv/bin""\nExecStart=/home/korymath/jann/venv/bin/uwsgi --ini wsgi.ini\n[Install]\nWantedBy=multi-user.target \n Then, copy the following into a file on your server, \nnamed:  /etc/nginx/sites-available/JANN.conf \n ```sh \n JANN.conf \n server {\n    listen      80;\n    server_name 35.209.230.155;\n    location / {\n        include     /home/korymath/jann/uwsgi_params;\n        uwsgi_pass unix:/home/korymath/jann/Jann/jann.sock;\n    }\n}\n``` \n Then, we tell nginx how to refer to the server \n ```sh \n link the site configuration to nginx enabled sites \n sudo ln -s /etc/nginx/sites-available/JANN.conf /etc/nginx/sites-enabled/ \n restart nginx \n sudo systemctl restart nginx \n restart jann \n sudo systemctl restart jann\n``` \n Common Errors/Warnings and Solutions \n sh\n/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module \'tensorflow.python.framework.fast_tensor_util\' does not match runtime version 3.6\n  return f(*args, **kwds) \n Solution (for OSX 10.13): \n sh\npip install --ignore-installed --upgrade https://github.com/lakshayg/tensorflow-build/releases/download/tf1.9.0-macos-py27-py36/tensorflow-1.9.0-cp36-cp36m-macosx_10_13_x86_64.whl \n FileNotFoundError \n sh\nFileNotFoundError: [Errno 2] No such file or directory: \'data/CMDC/movie_lines.txt\' \n Solution: \n sh\nEnsure that the input movie lines file is extracted to the correct path \n ValueError \n sh\nValueError: Signature \'spm_path\' is missing from meta graph. \n Solution \n Currently  jann  is configured to use the  universal-sentence-encoder-lite  module from TFHub as it is small, lightweight, and ready for rapid deployment. This module depends on the  SentencePiece  library and the SentencePiece model published with the module. \n You will need to make some minor code adjustments to use the heaviery modules (such as  universal-sentence-encoder \nand  universal-sentence-encoder-large . \n Start Contributing \n The guide for contributors can be found  here . It covers everything you need to know to start contributing to  jann . \n References \n \n Universal Sentence Encoder on TensorFlow Hub \n Cer, Daniel, et al. \'Universal sentence encoder.\' arXiv preprint arXiv:1803.11175 (2018). \n Danescu-Niculescu-Mizil, Cristian, and Lillian Lee. \'Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs.\' Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics. Association for Computational Linguistics, 2011. \n \n Credits \n jann  is made with love by  Kory Mathewson . \n Icon made by  Freepik  from  www.flaticon.com  is licensed by  CC 3.0 BY .', '\n dAIrector (🤖 + 📖) \n dAIrector is an automated director which collaborates with humans storytellers. \n Documentation \n Go to  https://korymath.github.io/dairector/ \n Set Up \n ```sh \n install homebrew \n /usr/bin/ruby -e ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"" \n install and upgrade portaudio, swig, git, python3 \n brew install --upgrade portaudio swig git python3 \n set up the python3 virtual environment \n virtualenv -p python3 env \n activate the virtual environment \n source env/bin/activate \n install requirements \n pip install -r requirements.txt \n in case of an error with pyaudio, may need to point to brew intall directly \n see https://stackoverflow.com/questions/33513522/when-installing-pyaudio-pip-cannot-find-portaudio-h-in-usr-local-include \n for more information \n pip install --global-option=\'build_ext\' --global-option=\'-I/Users/korymath/homebrew/Cellar/portaudio/19.6.0/include\' --global-option=\'-L/Users/korymath/homebrew/Cellar/portaudio/19.6.0/lib\' pyaudio \n get the trained model and example files \n wget https://storage.googleapis.com/api-project-941639660937.appspot.com/dairector_pretrained_examples.zip \n unpack the files \n unzip dairector_pretrained_examples.zip\n``` \n Run \n ```sh \n first ensure that your environment is activated \n source env/bin/activate \n example 1a, generate a new subgraph from the entire plotto conflict graph \n python markovgenerator.py -t outputfile.json plottoconflicts.json \n example 1b, interactive story telling using the plot subgraph and tv tropes hints \n python -W ignore storyteller.py outputfile.json tvtropes.json tvtropesmodel.bin plottomodel.bin\n``` \n Interactive Beat Generation \n The storyteller is interactive, it understands the following commands:\n* next [ cue_text ]\n* hint [ cue_text ]\n* quit \n next  uses the vector model from  plottomodel.bin  to find the next story beat based on the given cue text. \n hint  uses the  tvtropesmodel.bin  to find an appropriate trope. \n Install pocketsphinx \n sh\npip install pocketsphinx==0.1.15\npip install PyAudio==0.2.11 \n Basic Usage \n Improvisors on stage can cue the system to provide the next plot point or the next hint.\nThe improvisors provide the dialogue for each plot clause. \n Training a new model \n sh\npython topicvectors.py tvtropesmodel.bin tvtropes.json \n Cite \n sh\n@inproceedings{eger2018dairector,\n  author = {{Eger}, M. and {Mathewson}, K.~W.},\n  title = ""{dAIrector: Automatic Story Beat Generation through Knowledge Synthesis}"",\n  booktitle = {AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE18), Joint Workshop on Intelligent Narrative Technologies and Intelligent Cinematography and Editing},\n  publisher = {AAAI},\n  year = 2018,\n  address={Edmonton, Alberta, Canada},\n  month = 10,\n} \n License \n This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-sa/3.0/ or send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.', 'TEDRIC Analysis \n 1. Set up Environment \n sh\nvirtualenv -p python3 env\nsource env/bin/activate\npip install -r requirements.txt \n 2. Run \n ```sh\njupyter notebook \n then run the TEDRIC_Analysis file \n ```', '\n \n \n \n \n  PROJECT LOGO  \n \n \n \n \n \n aNAOmate \n \n    A web-based interface for controlling SoftBank\'s Aldebaran Nao (V5/V6) robots.\n     \n Explore the docs » \n \n \n Report Bug \n    ·\n     Request Feature \n \n  TABLE OF CONTENTS  \n Table of Contents \n \n About the Project \n Built With \n Getting Started \n Prerequisites \n Installation \n Usage \n Roadmap \n Contributing \n License \n Contact \n Acknowledgements \n \n  ABOUT THE PROJECT  \n About The Project \n aNAOmate  is an web-based interface for controlling SoftBank\'s Aldebaran Nao (V5/V6) robots. \n \n Slidedeck \n Built With \n \n NAOqi API \n Bootstrap \n \n  GETTING STARTED  \n Getting Started \n To get a local copy up and running follow these simple steps. \n Prerequisites \n Things you need to use the software and how to install them.\n* npm\n sh\nnpm install npm@latest -g \n Installation \n \n Clone the repo\n sh\ngit clone https:://github.com/QuinnyB/aNAOmate.git \n \n  USAGE EXAMPLES  \n Usage \n For examples, please refer to the  Documentation \n Editing TMI (Touch Move Interface for the Nao Robot) \n \n Make changes in the  TMI  directory \n Test the changes by following the quick testing instructions. \n Once satisfied, ensure that all necessary files are linked in  TMI/TMI.pml . \n Once links are complete, open the  TMI/TMI.pml  in Choregraphe \n File -> Build Application Package  and save to the  package  directory. \n Once the package is saved, install on the robot. (Note: NAOqi must be properly installed on your system for the installation script to work) \n \n sh\n/usr/bin/python package/install_pkg.py $ROBOT_IP $PACKAGE_FILE_NAME \n \n Once the package is installed, you will see:  Installation complete.  (This installs the application to  /home/nao/.local/share/PackageManager/apps/TMI  and runs the application if  autorun=""true""  in the  TMI/manifest \n The application interface is now running, and starts automatically by default. \n Navigate to  http://$ROBOT_IP/apps/TMI  to see developed the interface. \n This interface should match the tested changes in Step 2. \n \n Using the aNAOmate interface \n TODO \n Behaviours \n TODO \n Configure Development Environment \n TODO \n  ROADMAP  \n Roadmap \n See the  open issues  for a list of proposed features (and known issues). \n Authors \n \n Quinn Boser  -  Website \n Riley Dawson  -  Website \n Josh Whitney  -  Website \n Kory Mathewson  -  Website \n \n See the list of  contributors  who participated in this project. \n  CONTRIBUTING  \n Contributing \n Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are  greatly appreciated . \n \n Fork the Project \n Create your Feature Branch ( git checkout -b feature/AmazingFeature ) \n Commit your Changes ( git commit -m \'Add some AmazingFeature\' ) \n Push to the Branch ( git push origin feature/AmazingFeature ) \n Open a Pull Request \n \n  LICENSE  \n License \n Distributed under the MIT License. See  LICENSE  for more information. \n  CONTACT  \n Contact \n Josh Whitney -  @JoshJRWhitney  - joshjrwhitney@gmail.com \n Project Link:  https://github.com/QuinnyB/aNAOmate \n  ACKNOWLEDGEMENTS  \n Acknowledgements \n The authors of this project would like to graciously acknowledge to continued support of the community. \n  MARKDOWN LINKS & IMAGES  \n  https://www.markdownguide.org/basic-syntax/#reference-style-links ', '2019 Edmonton Fringe Festival Show Finder \n There is a  live demo . \n Credits \n Built by Kory Mathewson. Using show data from the Edmonton Fringe Festival Tickets  website . Based on  CSV to HTML Table  by Derek Eder.', 'Equal Groups K-Means clustering \n """"""Equal Groups K-Means clustering utlizing the scikit-learn api and related utilities."""""" \n From:\nhttps://github.com/ndanielsen/Same-Size-K-Means/blob/master/clustering/equal_groups.py \n And made to work with python3 \n """"""Equal Groups K-Means clustering\n90 percent of this is the Kmeans implmentations with the equal groups logic\nlocated in  _labels_inertia_precompute_dense()  which follows the steps laid\nout in the Elki Same-size k-Means Variation tutorial.\nhttps://elki-project.github.io/tutorial/same-size_k_means\nPlease note that this implementation only works in scikit-learn 17.X as later\nversions having breaking changes to this implementation.\nParameters \n \n n_groups : int, optional, default: 8\n    The number of clusters to form as well as the number of\n    centroids to generate.\nmax_iter : int, default: 300\n    Maximum number of iterations of the k-means algorithm for a\n    single run.\nn_init : int, default: 10\n    Number of time the k-means algorithm will be run with different\n    centroid seeds. The final results will be the best output of\n    n_init consecutive runs in terms of inertia.\ninit : {\'k-means++\', \'random\' or an ndarray}\n    Method for initialization, defaults to \'k-means++\':\n    \'k-means++\' : selects initial cluster centers for k-mean\n    clustering in a smart way to speed up convergence. See section\n    Notes in k_init for more details.\n    \'random\': choose k observations (rows) at random from data for\n    the initial centroids.\n    If an ndarray is passed, it should be of shape (n_groups, n_features)\n    and gives the initial centers.\nprecompute_distances : {\'auto\', True, False}\n    Precompute distances (faster but takes more memory).\n    \'auto\' : do not precompute distances if n_samples * n_groups > 12\n    million. This corresponds to about 100MB overhead per job using\n    double precision.\n    True : always precompute distances\n    False : never precompute distances\ntol : float, default: 1e-4\n    Relative tolerance with regards to inertia to declare convergence\nrandom_state : integer or numpy.RandomState, optional\n    The generator used to initialize the centers. If an integer is\n    given, it fixes the seed. Defaults to the global numpy random\n    number generator.\nverbose : int, default 0\n    Verbosity mode.\ncopy_x : boolean, default True\n    When pre-computing distances it is more numerically accurate to center\n    the data first.  If copy_x is True, then the original data is not\n    modified.  If False, the original data is modified, and put back before\n    the function returns, but small numerical differences may be introduced\n    by subtracting and then adding the data mean. \n Attributes \n cluster_centers_ : array, [n_groups, n_features]\n    Coordinates of cluster centers\nlabels_ :\n    Labels of each point\ninertia_ : float\n    Sum of distances of samples to their closest cluster center. \n Notes \n The k-means problem is solved using Lloyd\'s algorithm.\nThe average complexity is given by O(k n T), were n is the number of\nsamples and T is the number of iteration.\nThe worst case complexity is given by O(n^(k+2/p)) with\nn = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii,\n\'How slow is the k-means method?\' SoCG2006)\nIn practice, the k-means algorithm is very fast (one of the fastest\nclustering algorithms available), but it falls in local minima. That\'s why\nit can be useful to restart it several times.\nSee also \n \n MiniBatchKMeans:\n    Alternative online implementation that does incremental updates\n    of the centers positions using mini-batches.\n    For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n    probably much faster to than the default batch implementation.\n""""""', 'ParitBOT Dashboard', ""\n Hello, I'm Kory! \n \n \n I am a Research Scientist with  DeepMind  and a Lab Scientist with the  Creative Destruction Lab .  \n I have a Ph.D. in Computing Science from the  University of Alberta  with the  Alberta Machine Intelligence Institute .  \n My research focuses on understanding interaction between intelligent systems.  \n I am currently focusing on  🧑\u200d🦳-🤖 interfaces. \n You might also know me as an improvisational theatre performance artist with  Rapid Fire Theatre .  \n I like to fuse my interests by developing artificial intelligences to perform theatre alongside.  \n For more,  https://korymathewson.com/  and  Improbotics"", 'P5.EEGEdu \n P5.EEGEdu is an educational website to learn about coding live animations with electroencephalogram (EEG) data. It is a teaching tool that allows for students to quickly interact with their own brain waves.  \n Visit  https://p5.eegedu.com/  for the live p5 sandbox website. \n Installation for Development \n If you are interested in developing p5.EEGEdu, here are some instructions to get you started. \n Note: Currently p5.EEGEdu development requires a Mac OSX operating system.  \n To start, you will need to install  Homebrew  and  yarn . These are easy one-line installations for Mac users:  \n ```sh \n Install homebrew \n /usr/bin/ruby -e ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"" \n Install yarn \n NOTE: this will also install Node.js if it is not already installed. \n brew install yarn  \n Node.js must be version 10.x for Muse interaction \n Thus, if you are getting version issues, install n and switch versions \n sudo npm install -g n \n sudo n 10.16.0 \n ``` \n Then, in terminal, clone the git repo and enter the folder: \n sh\ngit clone https://github.com/kylemath/p5.EEGEdu\ncd p5.EEGEdu \n You then need to install the required packages for EEGEdu \n sh\nyarn install \n Local Development Environment \n Then, you can run the  Development Environment  of p5.EEGEdu: \n sh\nyarn start dev \n If it is working correctly, the p5.EEGEdu application will open in a browser window at http://localhost:3000. \n Local Production Environment \n To start the  Local Production Environment , you can use the following commands:  \n sh\nyarn run build\nserve -s build \n Deployment \n p5.EEGEdu  is running on  Firebase  and deployment happens automagically using GitHub post-commit hooks, or  Actions , as they are commonly called. You can see how the application is build and deployed by  inspecting the workflow . \n Contributing \n The guide for contributors can be found  here . It covers everything you need to know to start contributing to p5.EEGEdu. \n Development Roadmap \n References \n \n https://github.com/urish/muse-js - based toolbox for interacting with muse  \n https://github.com/NeuroJS/angular-muse - demo with streaming data in Angular, record button,  \n https://github.com/tanvach/muse-fft  - starting point react demo \n https://github.com/neurosity/eeg-pipes - easy pipable operations on eeg data from muse-js \n https://reactjs.org/  - React for web development \n https://www.chartjs.org/docs/latest/ - interactive charts \n https://github.com/urish/muse-lsl  - maybe useful to stream to LSL \n \n Credits \n p5.EEGEdu  - An Interactive Electrophysiology P5 Animation Coding Sandbox with the Interaxon Muse brought to you by Mathewson Sons \n License \n p5.EEGEdu is licensed under The MIT License (MIT)', 'iccc-ai-art', 'Just For Laughs \n \n Setup \n ```sh \n Get the code \n git clone https://github.com/korymath/justforlaughs;\ncd justforlaughs/;\npython3 -m venv venv;\nsource venv/bin/activate; \n Install requirements \n pip install --upgrade pip;\npip install -r requirements.txt;\n``` \n Run \n ```sh \n Run the web app \n python app.py; \n Run the standalone laughter detector \n python segment_laughter.py --input_audio_file example_audio.wav; \n It should output that it found 1 laugh in the example, save just the laugh cropped from the input, and the time window when laugh happened. \n Example: \n found 1 laughs. \n [{\'filename\': \'output/laugh_0.wav\', \'start\': 2.6453333333333333, \'end\': 5.261913043478261}] \n ``` \n Fixes \n For running on mac, need to ensure that the libsndfile is correclty pathed, ref: https://github.com/bastibe/python-soundfile/issues/310\n sh\nenv DYLD_LIBRARY_PATH=""/opt/homebrew/lib:$DYLD_LIBRARY_PATH"" python app.py \n Credits \n Client-sider in-browser detection from: \n \n https://github.com/tensorflow/tfjs-models/tree/master/speech-commands -  LICENSE \n \n Laughter detection model from: \n \n https://github.com/jrgillick/laughter-detection -  LICENSE \n \n Audio interface and recording adapted from: \n \n https://github.com/mattdiamond/Recorderjs -  LICENSE \n https://github.com/addpipe/simple-recorderjs-demo -  No LICENSE ,  Blog post \n', 'ELO Demo \n Setup and Install \n sh\npython3 -m venv venv;\nsource venv/bin/activate;\npip install --upgrade pip;\npip install -r requirements.txt; \n Firebase Datastore \n Ensure that  key.json  is present in root directory. For more information, see  the documentation  on deploying a similar application. \n Run \n sh\npython main.py']",['repository_count'],DeepMind,https://avatars.githubusercontent.com/u/178099?u=1d02f6838c74b227184844ff80372450b08b355a&v=4,True,2016-10-18 17:11:58,309,345,0,2010-01-07T18:23:04Z,2022-11-14T18:43:08Z,korymath,False,False,True,False,False,True,False,0,"{'MATLAB': 4, 'Jupyter Notebook': 7, 'Lua': 1, 'Python': 16, 'JavaScript': 5, 'HTML': 1, 'CSS': 1, 'TypeScript': 1}",5,"Montreal, Canada",Data Mining,56,0.16,0.87,0.67,0
nicholas-leonard,Nicholas Léonard,"TensorFlow, Python, Scala, Java, Lua/Torch, C, CUDA, deep learning, machine learning, data science",nileonard@expedia.com,"['nnx: experimental \'nn\' components \n The original neural network from Torch7,  nn , contains stable and widely\nused modules. \'nnx\' contains more experimental, unproven modules, and\noptimizations. Modules that become stable and which are proven useful make \ntheir way into \'nn\' (some already have). \n Library Documentation \n This section includes documentation for the following objects: \n \n SoftMaxTree  : a hierarchical log-softmax Module; \n TreeNLLCriterion  : a negative log-likelihood Criterion for the SoftMaxTree; \n CTCCriterion  : a Connectionist Temporal Classification Criterion based on  warp-ctc ; \n PushTable (and PullTable)  : extracts a table element and inserts it later in the network; \n MultiSoftMax  : performs a softmax over the last dimension of a 2D or 3D input; \n SpatialReSampling  : performs bilinear resampling of a 3D or 4D input image; \n [QDRiemaNNLinear] (#nnx.QDRiemaNNLinear) : quasi-diagonal reduction for Riemannian gradient descent \n Recurrent  : a generalized recurrent neural network container; \n \n \n SoftMaxTree \n A hierarchy of parameterized log-softmaxes. Used for computing the likelihood of a leaf class. \nThis Module should be used in conjunction with the  TreeNLLCriterion . \nUsing this for large vocabularies (100,000 and more) greatly accelerates training and evaluation \nof neural network language models (NNLM). \nA vocabulary hierarchy is provided via the  dp  package\'s\n BillionWords \n DataSource . \n The constructor takes 2 mandatory and 4 optional arguments : \n *  inputSize  : the number of units in the input embedding representation;\n *  hierarchy  : a Tensor mapping one  parent_id  to many  child_id  (a tree);\n *  rootId  : a number identifying the root node in the hierarchy. Defaults to  -1 ;\n *  accUpdate  : when the intent is to use  backwardUpdate  or  accUpdateGradParameters , set this to true to save memory. Defaults to false;\n *  static  : when true (the defualt), returns parameters with keys that don\'t change from batch to batch;\n *  verbose  : prints some additional information concerning the hierarchy during construction. \n The  forward  method returns an  output  Tensor of size 1D, while \n backward  returns a table  {gradInput, gradTarget} . The second \nvariable is just a Tensor of zeros , such that the  targets  can be \npropagated through  Containers  \nlike  ParallelTable . \n ```lua \n \n input = torch.randn(5,10)\ntarget = torch.IntTensor{20,24,27,10,12}\ngradOutput = torch.randn(5)\nroot_id = 29\ninput_size = 10  \nhierarchy = { \n \n [29]=torch.IntTensor{30,1,2}, [1]=torch.IntTensor{3,4,5}, \n   [2]=torch.IntTensor{6,7,8}, [3]=torch.IntTensor{9,10,11},\n   [4]=torch.IntTensor{12,13,14}, [5]=torch.IntTensor{15,16,17},\n   [6]=torch.IntTensor{18,19,20}, [7]=torch.IntTensor{21,22,23},\n   [8]=torch.IntTensor{24,25,26,27,28}\n}\nsmt = nn.SoftMaxTree(input_size, hierarchy, root_id)\nsmt:forward{input, target}\n-3.5186\n-3.8950\n-3.7433\n-3.3071\n-3.0522\n[torch.DoubleTensor of dimension 5]\nsmt:backward({input, target}, gradOutput)\n{\n  1 : DoubleTensor - size: 5x10\n  2 : IntTensor - size: 5\n} \n \n \n ``` \n \n TreeNLLCriterion \n Measures the Negative log-likelihood (NLL) for  SoftMaxTrees . \nUsed for maximizing the likelihood of SoftMaxTree outputs.\nThe SoftMaxTree Module outputs a column Tensor representing the log likelihood\nof each target in the batch. Thus SoftMaxTree requires the targets.\nSo this Criterion only computes the negative of those outputs, as \nwell as its corresponding gradients. \n \n \n PushTable (and PullTable) \n PushTable and PullTable work together. The first can be put earlier\nin a digraph of Modules such that it can communicate with a \nPullTable located later in the graph.  PushTable:forward(input)  \nfor an  input  table of Tensors to the output, excluding one, the index of which \nis specified by the  index  argument in the  PushTable(index)  constructor.\nThe Tensor identified by this  index  is communicated to one or many \nPullTables created via the  PushTable:pull(index)  factory method. \nThese can be inserted later in the digraph such that \na call to  PushTable:forward(input) , where  input  is a table or a Tensor, \nwill output a table with the previously  pushed  Tensor inserted \nat index  index . \n An example utilizing the above  SoftMaxTree  Module\nand a Linear Module demonstrates how the PushTable can be used to \nforward the  target  Tensor without any other \n Table Modules :\n```lua \n \n mlp = nn.Sequential()\nlinear = nn.Linear(50,100)\npush = nn.PushTable(2)\npull = push:pull(2)\nmlp:add(push)\nmlp:add(nn.SelectTable(1))\nmlp:add(linear)\nmlp:add(pull)\nmlp:add(smt) --smt is a SoftMaxTree instance\nmlp:forward{input, target} -- input and target are defined above\n-3.5186\n-3.8950\n-3.7433\n-3.3071\n-3.0522\n[torch.DoubleTensor of dimension 5]\nmlp:backward({input, target}, gradOutput) -- so is gradOutput\n{\n  1 : DoubleTensor - size: 5x10\n  2 : IntTensor - size: 5\n}\n The above code is equivalent to the following: lua\nmlp2 = nn.Sequential()\npara = nn.ParallelTable()\npara:add(linear)\npara:add(nn.Identity())\nmlp2:add(para)\nmlp2:add(smt)\nmlp2:forward{input, target}\n-3.5186\n-3.8950\n-3.7433\n-3.3071\n-3.0522\n[torch.DoubleTensor of dimension 5]\nmlp2:backward({input, target}, gradOutput)\n{\n  1 : DoubleTensor - size: 5x10\n  2 : IntTensor - size: 5\n}\n```\nIn some cases, this can simplify the digraph of Modules. Note that \na PushTable can be associated to many PullTables, but each PullTable \nis associated to only one PushTable. \n \n \n CTCCriterion \n criterion = nn.CTCCriterion() \nCreates a Criterion based on Baidus\'  warp-ctc  implementation.\nThis Module measures the loss between a 3D output of (batch x time x inputdim) and a target without needing alignment of inputs and labels.\nMust have installed warp-ctc which can be installed via luarocks:\n luarocks install http://raw.githubusercontent.com/baidu-research/warp-ctc/master/torch_binding/rocks/warp-ctc-scm-1.rockspec \nSupports cuda via:\n criterion = nn.CTCCriterion():cuda() \nExample:\n```\noutput = torch.Tensor({{{1,2,3,4,5},{6,7,8,9,10}}}) -- Tensor of size 1x1x5 (batch x time x inputdim).\nlabel = {{1,3}}\nsizes = torch.Tensor({2}) -- Size of each sequence (sequence-length) in the batch as a tensor\nctcCriterion = nn.CTCCriterion() \n err = ctcCriterion:forward(output,label,sizes)\ngradOut = ctcCriterion:backward(output,label)\nprint(""----CPU----"")\nprint(""Error : "" .. err)\nprint(""Gradients :"")\nprint(gradOut) \n ctcCriterion = ctcCriterion:cuda() -- Switch to cuda implementation.\noutput = output:cuda() \n err = ctcCriterion:forward(output,label,sizes)\ngradOut = ctcCriterion:backward(output,label)\nprint(""----GPU----"")\nprint(""Error : "" .. err)\nprint(""Gradients :"")\nprint(gradOut)\n``` \n gives the output:\n```\n----CPU---- \nError : 4.9038286209106 \nGradients : \n(1,.,.) = \n  0.0117 -0.9683  0.0861  0.2341  0.6364\n  0.0117  0.0317  0.0861 -0.7659  0.6364\n[torch.FloatTensor of size 1x2x5] \n ----GPU---- \nError : 4.9038290977478 \nGradients : \n(1,.,.) = \n  0.0117 -0.9683  0.0861  0.2341  0.6364\n  0.0117  0.0317  0.0861 -0.7659  0.6364\n[torch.CudaTensor of size 1x2x5]\n```\n \n MultiSoftMax \n This Module takes 2D or 3D input and performs a softmax over the last dimension. \nIt uses the existing  SoftMax  \nCUDA/C code to do so such that the Module can be used on both GPU and CPU. \nThis can be useful for  keypoint detection . \n \n SpatialReSampling \n Applies a 2D re-sampling over an input image composed of\nseveral input planes (or channels, colors). The input tensor in  forward(input)  is \nexpected to be a 3D or 4D tensor of size :  [batchSize x] nInputPlane x width x height . \nThe number of output planes will be the same as the number of input\nplanes. \n The re-sampling is done using  bilinear interpolation . \nFor a simple nearest-neihbor upsampling, use  nn.SpatialUpSampling() ,\nand for a simple average-based down-sampling, use \n nn.SpatialDownSampling() . \n If the input image is a 3D tensor of size  nInputPlane x height x width ,\nthe output image size will be  nInputPlane x oheight x owidth  where\n owidth  and  oheight  are given to the constructor. \n Instead of  owidth  and  oheight , one can provide  rwidth  and  rheight , \nsuch that  owidth = iwidth*rwidth  and  oheight = iheight*rheight . \n As an example, we can run the following code on the famous Lenna image:\n lua\nrequire \'image\'                                                           \nrequire \'nnx\'\ninput = image.loadPNG(\'doc/image/Lenna.png\')\nl = nn.SpatialReSampling{owidth=150,oheight=150}\noutput = l:forward(input)\nimage.save(\'doc/image/Lenna-150x150-bilinear.png\', output) \n The input: \n   \n The re-sampled output: \n   \n \n QDRiemaNNLinear \n The Quasi-Diagonal Riemannian Neural Network Linear (QDRiemaNNLinear) module is an implementation\nof the quasi-diagonal reduction of metrics, used for Riemannian gradient descent.\nThe algorithm is defined in Riemannian metrics for neural networks I: feedforward networks by Yann Ollivier (http://arxiv.org/abs/1303.0818) and an efficient implementation is described in Practical Riemannian Neural Networks by Yann Ollivier and Gaetan Marceau-Caron (http://arxiv.org/abs/1602.08007).\nTo use this module, simply replace  nn.Linear(ninput,noutput)  with  nnx.QDRiemaNNLinear(ninput,noutput) .\nAs always, the step-size must be chosen accordingly.\nTwo additional arguments are also possible:\n* gamma (default=0.01): determine the update rate of the metric for a minibatch setting, i.e., (1-gamma) * oldMetric + gamma newMetric. Smaller minibatches require a smaller gamma. A default value depending on the size of the minibatches is  gamma = 1. - torch.pow(1.-1./nTraining,miniBatchSize)  where  nTraining  is the number of training examples of the dataset and  miniBatchSize  is the number of training examples per minibatch. \n* qdFlag (default=true): Whether to use the quasi-diagonal reduction (true) or only the diagonal (false). The former should be better. \n This module is a straightforward implementation of the outer product gradient descent. \n Requirements \n \n Torch7 (www.torch.ch) \n \n Installation \n \n Install Torch7 (refer to its own documentation). \n clone this project into dev directory of Torch7. \n Rebuild torch, it will include new projects too. \n \n Use the library \n First run torch, and load nnx: \n sh\n$ torch   \n ``` lua \n \n require \'nnx\'\n``` \n \n Once loaded, tab-completion will help you navigate through the\nlibrary (note that most function are added directly to nn): \n ``` lua \n \n nnx. + TAB\n...\nnn. + TAB\n``` \n \n In particular, it\'s good to verify that all modules provided pass their\ntests: \n ``` lua \n \n nnx.test_all()\nnnx.test_omp()\n``` \n \n \n Recurrent \n DEPRECATED July 6th, 2015. Use  rnn  instead.', 'IFT6266 : Emotion Recognition \n This model integrates with my pylear2 fork:\nhttps://github.com/nicholas-leonard/pylearn2 \n Transfered by experimental log on github wiki: \nhttps://github.com/nicholas-leonard/ift6266/wiki', ""\n Torch-7 for Android \n \n Torch7 provides a Matlab-like environment for state-of-the-art machine\nlearning algorithms. It is easy to use and provides a very efficient\nimplementation, thanks to an easy and fast scripting language (Lua) and a\nunderlying C implementation. \n Modified to be compiled and used with Android \n Features \n \n Loading of lua packages from the apk directly. \n This is done by writing a custom package.loader\n  Reference: http://www.lua.org/manual/5.1/manual.html#pdf-package.loaders\n  The loader is in torchandroid.cpp as loader_android \n torchandroid.h and torchandroid.cpp give lots of helper functions to make life easier \n Print function overriden to redirect to logcat (only handles strings for now) \n Function to get apk assets as bytes (very useful) \n \n Requirements \n Android NDK and Android SDK \n Samples \n \n A sample project has been provided in android-demo \n android-demo/jni/torchdemo.cpp is a simple use-case \n android-demo/assets/main.lua is the file that is run \n Vinayak Ghokale from e-lab Purdue (https://github.com/e-lab) contributed a face detector demo, which showcases a fuller use-case. \n That's in the facedetector_e-lab folder. I made some changes to it to load assets etc. from apk as opposed to the sdcard, but it remains untouched otherwise. \n \n Building Torch \n \n open build.sh and modify ANDROID_NDK to your android ndk path. \n run build script\n$ sh build.sh \n \n You can use torch in your android apps. The relevant directories are\n* include - include directories\n* lib - static libs cross-compiled for armeabi-v7a\n* share - lua files \n Building Example \n \n Build Torch atleast once using the steps above. \n [Optional] Connect your android phone in debugging mode,\n              to automatically install the apk. \n Change directory into android-demo folder. \n Run build script.\n$ sh build.sh \n Run the app TorchDemo on your phone. \n"", 'common \n Repository used in common by my other projects \n Dependencies \n libpqtypes : \n    http://libpqtypes.esilo.com/\n    requires a manual linking on ubuntu\n    user version 1.5.0\nboost: \n    http://www.boost.org/\n    http://www.boost.org/doc/libs/1_51_0/libs/serialization/doc/index.html\n    mostly for serialization, i.e. archives\npostgresql:\n    http://www.postgresql.org/\n    need a working database server 8.4+ and dev header files\ngeneralhashfunctions: \n    http://www.partow.net/programming/hashfunctions/   \n    included in repository for convenience\n    use his Makefile', 'ift6085 \n Mixture of Clusters', 'delicious \n Experiments on the delicious.com dataset', 'hps \n Hyper Parameter Search: a framework for coding, training and testing pylearn2 models in PostgreSQL.', 'hypermind \n A distributed neural network framework for the brave', 'equanimity \n A distributed conditional computing experiment using neural \ndecision trees.', 'dp Package Reference Manual \n \n dp  is a  d ee p  learning library designed for streamlining \nresearch and development using the  Torch7  distribution. \nIt emphasizes flexibility through the elegant use of object-oriented \n design patterns . \n Documentation \n This package includes lots of documentations and tutorials which you \nwill find hosted on  readthedocs .\nIf you prefer, you can consult the docs using  github .', 'cunnx \n Experimental cuda nn package', 'cub \n Allows for installing CUB via luarocks', 'mydp \n A dummy repository to use as a starting point for you own tests, classes, scripts and experiments extending dp.', 'torchx \n This package contains various torch extensions:\n *  concat  : concatenates a table of tensors.\n *  find  : finds all indices of a given value.\n *  group  : sorts and groups similar tensor variables together. \n *  remap  : recursively applies a function to tables of Tensors.\n *  md5  : used for hashing strings. \n And some  paths  extensions :\n *  indexdir  : index a directory of millions of files for faster listing. \n \n [res] torch.concat([res], tensors, [dim]) \n Concatenates a table of Tensors along dimension  dim .\n *  res  is a tensor holding the concatenation of Tensors  tensor .\n *  tensors  is a table of tensors. Each tensor should have the same amount of dimensions and the same size for non- dim  dimensions.\n *  dim  is the dimension along which the tensors will be concatenated. Defaults to 1. \n Example:\n```lua \n \n res = torch.concat({torch.rand(2,3),torch.randn(2,1),torch.randn(2,2)},2)\nprint(res)\n 0.8621  0.7776  0.3284 -1.2884 -0.4939  0.6049\n 0.8404  0.8996  0.5704  0.3911 -0.0428 -1.4627\n[torch.DoubleTensor of dimension 2x6]\n``` \n \n \n [res] torch.find(tensor, val, [dim]) \n Finds all indices of a given value  val  in Tensor  tensor . \nReturns a table of these indices by traversing the tensor one row \nat a time. When  dim=2 , the only valid value for dim other than  nil  (the default),\nthe function expects a matrix and returns the row-wise indices of each found \nvalue  val  in the row. \n 1D example:\n```lua \n \n res = torch.find(torch.Tensor{1,2,3,1,1,2}, 1)\nunpack(res)\n1  4  5\n``` \n \n 2D example:\n``` \n \n tensor = torch.Tensor{{1,2,3,4,5},{5,6,0.6,0,2}}\nunpack(torch.find(tensor, 2))\n2   10 \nunpack(torch.find(tensor:t(), 2))\n3   10 \nunpack(torch.find(tensor, 2, 2))\n{2}  {5}\nunpack(torch.find(tensor:t(), 2, 2))\n{ }  {1}  { }  { }  {2}\n``` \n \n \n [res, val, idx] torch.group([val, idx], tensor, [samegrp, desc]) \n Sorts and groups similar tensor variables together.\n *  res  is a table of  {idx=torch.LongTensor,val=torch.Tensor} .\n *  val  is a Tensor of the same type as  tensor . It will be used to store and return the sorted values.\n *  idx  is a  torch.LongTensor  used to store the sorted indices.\n *  tensor  is a Tensor that will have its values sorted, and then grouped by the  samegrp  function.\n *  samegrp  is a function taking two argument :  first_val  is the first value of the current group, while  val  is the current value of the current group. When the function returns true, it is assumed that  val  is of the same group as  first_val . Defaults to  function(first_val, val) return first_val == val; end \n *  desc  is a boolean indicating whether the  tensor  gets sorted in descending order. Defaults to false. \n Example:\n```lua \n \n tensor = torch.Tensor{5,3,4,5,3,5}\nres, val, idx = torch.group(tensor)\nres\n{\n  3 : \n    {\n      idx : LongTensor - size: 2\n      val : DoubleTensor - size: 2\n    }\n  4 : \n    {\n      idx : LongTensor - size: 1\n      val : DoubleTensor - size: 1\n    }\n  5 : \n    {\n      idx : LongTensor - size: 3\n      val : DoubleTensor - size: 3\n    }\n}\n``` \n \n \n [t1, t2] torch.remap(t1, t2, f(x,y) [p1, p2]) \n Recursively applies function  f(x,y)  [to tables [of tables,...] of] Tensors\n t1  and  t2 . When prototypes  p1  or  p2  are provided, they are used \nto initialized any missing Tensors in  t1  or  t2 . \n Example:\n```lua \n \n t1 = {torch.randn(3,4), {torch.randn(3,4), torch.randn(2,4), {torch.randn(1)}}}\nt2 = {torch.randn(3,4), {torch.randn(3,4), torch.randn(2,4), {torch.randn(1)}}}\ntorch.remap(t1, t2, function(x, y) x:add(y) end)\n{\n  1 : DoubleTensor - size: 3x4\n  2 : \n    {\n      1 : DoubleTensor - size: 3x4\n      2 : DoubleTensor - size: 2x4\n      3 : \n        {\n          1 : DoubleTensor - size: 1\n        }\n    }\n}\n{\n  1 : DoubleTensor - size: 3x4\n  2 : \n    {\n      1 : DoubleTensor - size: 3x4\n      2 : DoubleTensor - size: 2x4\n      3 : \n        {\n          1 : DoubleTensor - size: 1\n        }\n    }\n}\n It also creates missing tensors: lua\nt2, t3 = torch.remap(t2, nil, function(x, y) y:resizeAs(x):copy(x) end)\nprint(t3)\n{\n  1 : DoubleTensor - size: 3x4\n  2 : \n    {\n      1 : DoubleTensor - size: 3x4\n      2 : DoubleTensor - size: 2x4\n      3 : \n        {\n          1 : DoubleTensor - size: 1\n        }\n    }\n}\n When in doubt, first tensor has priority: lua\nt4, t2 = torch.remap({torch.DoubleTensor()}, t2, function(x, y) x:resize(y:size()):copy(y) end, torch.LongTensor())\nprint(t4)\n{\n  1 : DoubleTensor - size: 3x4\n}\nt2, t5 = torch.remap(t2, {torch.DoubleTensor()}, function(x, y) y:resize(x:size()):copy(x) end, torch.LongTensor())\nprint(t5)\n{\n  1 : DoubleTensor - size: 3x4\n  2 : \n    {\n      1 : LongTensor - size: 3x4\n      2 : LongTensor - size: 2x4\n      3 : \n        {\n          1 : LongTensor - size: 1\n        }\n    }\n}\n``` \n \n \n torch.md5 \n Pure Lua module copy-pasted from  this repo  (for some reasons I can\'t get \ngit submodule to work with luarocks). The module includes two functions:\n lua\nlocal md5_as_hex   = torch.md5.sumhexa(message)   -- returns a hex string\nlocal md5_as_data  = torch.md5.sum(message)     -- returns raw bytes \nThe  torch.md5.sumhexa  function takes a string and returns another string:\n lua\ntorch.md5.sumhexa(\'helloworld!\')\n420e57b017066b44e05ea1577f6e2e12 \n \n [obj] paths.indexdir(path, [ext, use_cache, ignore]) \n lua\nfiles = paths.indexdir(""/path/to/files/"", \'png\', true)\nimages = {}\nfor i=1,files:size() do\n   local img = image.load(files:filename(i))\n   table.insert(images, img)\nend \n This function can be used to create an object indexing all files having \nextensions  ext  (a string or a list thereof) in directory  path  (string or list thereof). \nUseful for directories containing many thousands of files. The function \ncaches the resulting list to disk in  /tmp  such that it can be used \nfor later calls when  use_cache=true  (default is false). \nArgument  ignore  species a pattern to ignore (e.g. "" frame "" will ignore all files containing  ""frame"" ).', 'Torch Manual \n Torch has lots of libraries and corresponding reference documentation. \nBut this is where you can try to make some sense out of all of it. \n The Basics \n \n Introduction \n Getting Started \n Torch Ecosystem \n Programming in Lua \n Tensors \n Object-Oriented Programming \n Foreign Function Interface \n \n Neural Network Training \n \n Datasets \n Building your own dataset  \n Modules \n Writing your own torch module  \n Criterions \n Optimization \n \n Library Reference \n \n Torch  : tensors, class factory, serialization, BLAS ; \n nn  : neural network Modules and Criterions; \n optim  : SGD, LBFGS and other optimization functions ; \n gnuplot  : ploting and data visualization ; \n paths  : make directories, concatenate file paths, and other filesystem utilities ; \n image  : save, load, crop, scale, warp, translate images and such ; \n trepl  : the torch LuaJIT interpreter ; \n cwrap  : used for wrapping C/CUDA functions in Lua ; \n', 'dpnn : deep extensions to nn \n This package provides many useful features that aren\'t part of the main nn package. \nThese include  sharedClone , which allows you to clone a module and share \nparameters or gradParameters with the original module, without incuring any memory overhead.\nWe also redefined  type  such that the type-cast preserves Tensor sharing within a structure of modules.  \n The package provides the following Modules: \n \n Decorator  : abstract class to change the behaviour of an encapsulated module ; \n DontCast  : prevent encapsulated module from being casted by  Module:type()  ; \n Serial  : decorate a module makes its serialized output more compact ;  \n NaN  : decorate a module to detect the source of NaN errors ; \n Inception  : implements the Inception module of the GoogleLeNet article ; \n Collapse  : just like  nn.View(-1) ; \n Convert  : convert between different tensor types or shapes; \n ZipTable  : zip a table of tables into a table of tables; \n ZipTableOneToMany  : zip a table of element  el  and table of elements into a table of pairs of element  el  and table elements; \n CAddTensorTable  : adds a tensor to a table of tensors of the same size; \n ReverseTable  : reverse the order of elements in a table; \n PrintSize  : prints the size of inputs and gradOutputs (useful for debugging); \n Clip  : clips the inputs to a min and max value; \n Constant  : outputs a constant value given an input (which is ignored); \n SpatialUniformCrop  : uniformly crops patches from a input; \n SpatialGlimpse  : takes a fovead glimpse of an image at a given location; \n WhiteNoise  : adds isotropic Gaussian noise to the signal when in training mode; \n OneHot  : transforms a tensor of indices into  one-hot  encoding; \n Kmeans  :  Kmeans  clustering layer. Forward computes distances with respect to centroids and returns index of closest centroid. Centroids can be updated using gradient descent. Centroids could be initialized randomly or by using  kmeans++  algoirthm; \n SpatialRegionDropout  : Randomly dropouts a region (top, bottom, leftmost, rightmost) of the input image. Works with batch and any number of channels; \n FireModule  : FireModule as mentioned in the  SqueezeNet ; \n NCEModule  : optimized placeholder for a  Linear  +  SoftMax  using  noise-contrastive estimation . \n SpatialFeatNormalization  : Module for widely used preprocessing step of mean zeroing and standardization for images. \n SpatialBinaryConvolution  : Module for binary spatial convolution (Binary weights) as mentioned in  XNOR-Net . \n SimpleColorTransform  : Module for adding independent random noise to input image channels. \n PCAColorTransform  : Module for adding noise to input image using Principal Components Analysis. \n \n The following modules and criterions can be used to implement the REINFORCE algorithm : \n \n Reinforce  : abstract class for REINFORCE modules; \n ReinforceBernoulli  : samples from Bernoulli distribution; \n ReinforceNormal  : samples from Normal distribution; \n ReinforceGamma  : samples from Gamma distribution; \n ReinforceCategorical  : samples from Categorical (Multinomial with one sample) distribution; \n VRClassReward  : criterion for variance-reduced classification-based reward; \n BinaryClassReward  : criterion for variance-reduced binary classification reward (like  VRClassReward , but for binary classes); \n \n Additional differentiable criterions\n *  BinaryLogisticRegression  : criterion for binary logistic regression;\n *  SpatialBinaryLogisticRegression  : criterion for pixel wise binary logistic regression;\n *  NCECriterion  : criterion exclusively used with  NCEModule .\n *  ModuleCriterion  : adds an optional  inputModule  and  targetModule  before a decorated criterion;\n *  BinaryLogisticRegression  : criterion for binary logistic regression.\n *  SpatialBinaryLogisticRegression  : criterion for pixel wise binary logistic regression. \n A lot of the functionality implemented here was pulled from \n dp , which makes heavy use of this package. \nHowever, dpnn can be used without dp (for e.g. you can use it with optim), \nwhich is one of the main reasons why we made it. \n Tutorials \n Sagar Waghmare  wrote a nice  tutorial \non how to use dpnn with nngraph to reproduce the \n Lateral Connections in Denoising Autoencoders Support Supervised Learning .  \n A brief (1 hours) overview of Torch7, which includes some details about  dpnn , \nis available via this  NVIDIA GTC Webinar video . In any case, this presentation gives a nice overview of Logistic Regression, Multi-Layer Perceptrons, Convolutional Neural Networks and Recurrent Neural Networks using Torch7. \n \n Module \n The Module interface has been further extended with methods that facilitate \nstochastic gradient descent like  updateGradParameters  (i.e. momentum learning), \n weightDecay ,  maxParamNorm  (for regularization), and so on. \n \n Module.dpnn_parameters \n A table that specifies the name of parameter attributes. \nDefaults to  {\'weight\', \'bias\'} , which is a static variable (i.e. table exists in class namespace). \nSub-classes can define their own table statically.  \n \n Module.dpnn_gradParameters \n A table that specifies the name of gradient w.r.t. parameter attributes. \nDefaults to  {\'gradWeight\', \'gradBias\'} , which is a static variable (i.e. table exists in class namespace). \nSub-classes can define their own table statically.  \n \n [self] Module:type(type_str) \n This function converts all the parameters of a module to the given  type_str . \nThe  type_str  can be one of the types defined for  torch.Tensor \nlike  torch.DoubleTensor ,  torch.FloatTensor  and  torch.CudaTensor . \nUnlike the  type method \ndefined in  nn , this one was overriden to \nmaintain the sharing of  storage \namong Tensors. This is especially useful when cloning modules share  parameters  and  gradParameters . \n \n [clone] Module:sharedClone([shareParams, shareGradParams]) \n Similar to  clone .\nYet when  shareParams = true  (the default), the cloned module will share the parameters \nwith the original module. \nFurthermore, when  shareGradParams = true  (the default), the clone module will share \nthe gradients w.r.t. parameters with the original module.\nThis is equivalent to :\n lua\nclone = mlp:clone()\nclone:share(mlp, \'weight\', \'bias\', \'gradWeight\', \'gradBias\') \nyet it is much more efficient, especially for modules with lots of parameters, as these \nTensors aren\'t needlessly copied during the  clone .\nThis is particularly useful for  Recurrent neural networks  \nwhich require efficient copies with shared parameters and gradient w.r.t. parameters for each time-step. \n \n Module:maxParamNorm([maxOutNorm, maxInNorm]) \n This method implements a hard constraint on the upper bound of the norm of output and/or input neuron weights \n (Hinton et al. 2012, p. 2)  .\nIn a weight matrix, this is a contraint on rows ( maxOutNorm ) and/or columns ( maxInNorm ), respectively. \nHas a regularization effect analogous to  weightDecay , but with easier to optimize hyper-parameters. \nAssumes that parameters are arranged ( output dim x ... x input dim ). \nOnly affects parameters with more than one dimension.\nThe method should normally be called after  updateParameters . \nIt uses the C/CUDA optimized  torch.renorm  function.\nHint :  maxOutNorm = 2  usually does the trick.  \n \n [momGradParams] Module:momentumGradParameters() \n Returns a table of Tensors ( momGradParams ). For each element in the \ntable, a corresponding parameter ( params ) and gradient w.r.t. parameters \n( gradParams ) is returned by a call to  parameters .\nThis method is used internally by  updateGradParameters . \n \n Module:updateGradParameters(momFactor [, momDamp, momNesterov]) \n Applies classic momentum or Nesterov momentum  (Sutskever, Martens et al, 2013)  to parameter gradients. \nEach parameter Tensor ( params ) has a corresponding Tensor of the same size for gradients w.r.t. parameters ( gradParams ).\nWhen using momentum learning, another Tensor is added for each parameter Tensor ( momGradParams ).\nThis method should be called before  updateParameters \nas it affects the gradients w.r.t. parameters. \n Classic momentum is computed as follows : \n lua\nmomGradParams = momFactor*momGradParams + (1-momDamp)*gradParams\ngradParams = momGradParams   \n where  momDamp  has a default value of  momFactor . \n Nesterov momentum ( momNesterov = true ) is computed as follows (the first line is the same as classic momentum): \n lua\nmomGradParams = momFactor*momGradParams + (1-momDamp)*gradParams\ngradParams = gradParams + momFactor*momGradParams  \nThe default is to use classic momentum ( momNesterov = false ). \n \n Module:weightDecay(wdFactor [, wdMinDim]) \n Decays the weight of the parameterized models. \nImplements an L2 norm loss on parameters with dimensions greater or equal to  wdMinDim  (default is 2).\nThe resulting gradients are stored into the corresponding gradients w.r.t. parameters.\nSuch that this method should be called before  updateParameters . \n \n Module:gradParamClip(cutoffNorm [, moduleLocal]) \n Implements a contrainst on the norm of gradients w.r.t. parameters  (Pascanu et al. 2012) .\nWhen  moduleLocal = false  (the default), the norm is calculated globally to Module for which this is called.\nSo if you call it on an MLP, the norm is computed on the concatenation of all parameter Tensors.\nWhen  moduleLocal = true , the norm constraint is applied \nto the norm of all parameters in each component (non-container) module.\nThis method is useful to prevent the exploding gradient in \n Recurrent neural networks . \n \n Module:reinforce(reward) \n This method is used by Criterions that implement the REINFORCE algorithm like  VRClassReward . \nWhile vanilla backpropagation (gradient descent using the chain rule), \nREINFORCE Criterions broadcast a  reward  to all REINFORCE modules between the  forward  and the  backward .\nIn this way, when the following call to  backward  reaches the REINFORCE modules, \nthese will compute a  gradInput  using the broadcasted  reward .\nThe  reward  is broadcast to all REINFORCE modules contained \nwithin  model  by calling  model:reinforce(reward) . \nNote that the  reward  should be a 1D tensor of size  batchSize , \ni.e. each example in a batch has its own scalar reward. \n Refer to  this example \nfor a complete training script making use of the REINFORCE interface. \n \n Decorator \n lua\ndmodule = nn.Decorator(module) \n This module is an abstract class used to decorate a  module . This means \nthat method calls to  dmodule  will call the same method on the encapsulated \n module , and return its results. \n \n DontCast \n lua\ndmodule = nn.DontCast(module) \n This module is a decorator. Use it to decorate a module that you don\'t\nwant to be cast when the  type()  method is called. \n lua\nmodule = nn.DontCast(nn.Linear(3,4):float())\nmodule:double()\nth> print(module:forward(torch.FloatTensor{1,2,3}))\n 1.0927\n-1.9380\n-1.8158\n-0.0805\n[torch.FloatTensor of size 4]   \n \n Serial \n lua\ndmodule = nn.Serial(module, [tensortype])\ndmodule:[light,medium,heavy]Serial()   \n This module is a decorator that can be used to control the serialization/deserialization \nbehavior of the encapsulated module. Basically, making the resulting string or \nfile heavy (the default), medium or light in terms of size.  \n Furthermore, when specified, the  tensortype  attribute (e.g  torch.FloatTensor ,  torch.DoubleTensor  and so on.),\ndetermines what type the module will be cast to during serialization. \nNote that this will also be the type of the deserialized object.\nThe default serialization  tensortype  is  nil , i.e. the module is serialized as is.  \n The  heavySerial()  has the serialization process serialize every attribute in the module graph, \nwhich is the default behavior of nn.  \n The  mediumSerial()  has the serialization process serialize \neverything except the attributes specified in each module\'s  dpnn_mediumEmpty \ntable, which has a default value of  {\'output\', \'gradInput\', \'momGradParams\', \'dpnn_input\'} .\nDuring serialization, whether they be tables or Tensors, these attributes are emptied (no storage).\nSome modules overwrite the default  Module.dpnn_mediumEmpty  static attribute with their own. \n The  lightSerial()  has the serialization process empty \neverything a call to  mediumSerial(type)  would (so it uses  dpnn_mediumEmpty ).\nBut also empties all the parameter gradients specified by the \nattribute  dpnn_gradParameters , which defaults to  {gradWeight, gradBias} . \n We recomment using  mediumSerial()  for training, and  lightSerial()  for \nproduction (feed-forward-only models). \n \n NaN \n lua\ndmodule = nn.NaN(module, [id])   \n The  NaN  module asserts that the  output  and  gradInput  of the decorated  module  do not contain NaNs.\nThis is useful for locating the source of those pesky NaN errors. \nThe  id  defaults to automatically incremented values of  1,2,3,... . \n For example : \n lua\nlinear = nn.Linear(3,4)\nmlp = nn.Sequential()\nmlp:add(nn.NaN(nn.Identity()))\nmlp:add(nn.NaN(linear))\nmlp:add(nn.NaN(nn.Linear(4,2)))\nprint(mlp)   \n As you can see the  NaN  layers are have unique ids : \n lua\nnn.Sequential {\n  [input -> (1) -> (2) -> (3) -> output]\n  (1): nn.NaN(1) @ nn.Identity\n  (2): nn.NaN(2) @ nn.Linear(3 -> 4)\n  (3): nn.NaN(3) @ nn.Linear(4 -> 2)\n}   \n And if we fill the  bias  of the linear module with NaNs and call  forward : \n lua\nnan = math.log(math.log(0)) -- this is a nan value\nlinear.bias:fill(nan)\nmlp:forward(torch.randn(2,3))   \n We get a nice error message:\n lua\n/usr/local/share/lua/5.1/dpnn/NaN.lua:39: NaN found in parameters of module :\nnn.NaN(2) @ nn.Linear(3 -> 4)   \n For a quick one-liner to catch NaNs anywhere inside a model (for example, a  nn.Sequential  or any other  nn.Container ), we can use this with the  nn.Module.replace  function:\n lua\nmodel:replace(function(module) return nn.NaN(module) end) \n \n Inception \n References : \n \n A.  Going Deeper with Convolutions \n B.  GoogleLeNet \n \n lua\nmodule = nn.Inception(config)   \n This module uses  n +2 parallel ""columns"". \nThe original paper uses 2+2 where the first two are (but there could be more than two): \n \n 1x1 conv (reduce) -> relu -> 5x5 conv -> relu \n 1x1 conv (reduce) -> relu -> 3x3 conv -> relu  \n \n and where the other two are :  \n \n 3x3 maxpool -> 1x1 conv (reduce/project) -> relu  \n 1x1 conv (reduce) -> relu.  \n \n This module allows the first group of columns to be of any \nnumber while the last group consist of exactly two columns.\nThe 1x1 convoluations are used to reduce the number of input channels \n(or filters) such that the capacity of the network doesn\'t explode. \nWe refer to these here has  reduce . \nSince each column seems to have one and only one reduce, their initial \nconfiguration options are specified in lists of n+2 elements. \n The sole argument  config  is a table taking the following key-values : \n \n Required Arguments : \n inputSize  : number of input channels or colors, e.g. 3; \n outputSize  : numbers of filters in the non-1x1 convolution kernel sizes, e.g.  {32,48} \n reduceSize  : numbers of filters in the 1x1 convolutions (reduction) used in each column, e.g.  {48,64,32,32} . The last 2 are used respectively for the max pooling (projection) column (the last column in the paper) and the column that has nothing but a 1x1 conv (the first column in the paper). This table should have two elements more than the outputSize \n Optional Arguments : \n reduceStride  : strides of the 1x1 (reduction) convolutions. Defaults to  {1,1,...} . \n transfer  : transfer function like  nn.Tanh , nn.Sigmoid ,  nn.ReLU ,  nn.Identity , etc. It is used after each reduction (1x1 convolution) and convolution. Defaults to  nn.ReLU . \n batchNorm  : set this to  true  to use batch normalization. Defaults to  false . Note that batch normalization can be awesome \n padding  : set this to  true  to add padding to the input of the convolutions such that output width and height are same as that of the original non-padded  input . Defaults to  true . \n kernelSize  : size ( height = width ) of the non-1x1 convolution kernels. Defaults to  {5,3} . \n kernelStride  : stride of the kernels ( height = width ) of the convolution. Defaults to  {1,1} \n poolSize : size ( height = width ) of the spatial max pooling used in the next-to-last column. Defaults to 3. \n poolStride  : stride ( height = width ) of the spatial max pooling. Defaults to 1. \n \n For a complete example using this module, refer to the following :\n *  deep inception training script  ;\n *  openface facial recognition  (the model definition is  here ). \n \n Collapse \n lua\nmodule = nn.Collapse(nInputDim)   \n This module is the equivalent of:\n view = nn.View(-1)\nview:setNumInputDim(nInputDim)  \nIt collapses all non-batch dimensions. This is useful for converting \na spatial feature map to the single dimension required by a dense \nhidden layer like Linear. \n \n Convert \n lua\nmodule = nn.Convert([inputShape, outputShape])  \nModule to convert between different data formats.\nFor example, we can flatten images by using :\n lua\nmodule = nn.Convert(\'bchw\', \'bf\')  \nor equivalently\n lua\nmodule = nn.Convert(\'chw\', \'f\')  \nLets try it with an input:\n lua\nprint(module:forward(torch.randn(3,2,3,1)))\n 0.5692 -0.0190  0.5243  0.7530  0.4230  1.2483\n-0.9142  0.6013  0.5608 -1.0417 -1.4014  1.0177\n-1.5207 -0.1641 -0.4166  1.4810 -1.1725 -1.0037\n[torch.DoubleTensor of size 3x6]  \nYou could also try: \n ```lua\nmodule = nn.Convert(\'chw\', \'hwc\')\ninput = torch.randn(1,2,3,2)\ninput:select(2,1):fill(1)\ninput:select(2,2):fill(2)\nprint(input)\n(1,1,.,.) = \n  1  1\n  1  1\n  1  1\n(1,2,.,.) = \n  2  2\n  2  2\n  2  2\n[torch.DoubleTensor of size 1x2x3x2]\nprint(module:forward(input))\n(1,1,.,.) = \n  1  2\n  1  2 \n (1,2,.,.) = \n  1  2\n  1  2 \n (1,3,.,.) = \n  1  2\n  1  2\n[torch.DoubleTensor of size 1x3x2x2]\n```  \n Furthermore, it automatically converts the  input  to have the same type as  self.output \n(i.e. the type of the module).\nSo you can also just use is for automatic input type converions:\n lua\nmodule = nn.Convert()\nprint(module.output) -- type of module\n[torch.DoubleTensor with no dimension]\ninput = torch.FloatTensor{1,2,3}\nprint(module:forward(input))\n 1\n 2\n 3\n[torch.DoubleTensor of size 3] \n \n ZipTable \n lua\nmodule = nn.ZipTable() \n Zips a table of tables into a table of tables. \n Example:\n lua\nprint(module:forward{ {\'a1\',\'a2\'}, {\'b1\',\'b2\'}, {\'c1\',\'c2\'} })\n{ {\'a1\',\'b1\',\'c1\'}, {\'a2\',\'b2\',\'c2\'} } \n \n ZipTableOneToMany \n lua\nmodule = nn.ZipTableOneToMany() \n Zips a table of element  el  and table of elements  tab  into a table of tables, where the i-th table contains the element  el  and the i-th element in table  tab \n Example:\n lua\nprint(module:forward{ \'el\', {\'a\',\'b\',\'c\'} })\n{ {\'el\',\'a\'}, {\'el\',\'b\'}, {\'el\',\'c\'} } \n \n CAddTensorTable \n lua\nmodule = nn.CAddTensorTable() \n Adds the first element  el  of the input table  tab  to each tensor contained in the second element of  tab , which is itself a table \n Example:\n lua\nprint(module:forward{ (0,1,1), {(0,0,0),(1,1,1)} })\n{ (0,1,1), (1,2,2) } \n \n ReverseTable \n lua\nmodule = nn.ReverseTable() \n Reverses the order of elements in a table. \n Example: \n lua\nprint(module:forward{1,2,3,4})\n{4,3,2,1} \n \n PrintSize \n lua\nmodule = nn.PrintSize(name) \n This module is useful for debugging complicated module composites. \nIt prints the size of the  input  and  gradOutput  during  forward \nand  backward  propagation respectively.\nThe  name  is a string used to identify the module along side the printed size. \n \n Clip \n lua\nmodule = nn.Clip(minval, maxval) \n This module clips  input  values such that the output is between  minval  and  maxval . \n \n Constant \n lua\nmodule = nn.Constant(value, nInputDim) \n This module outputs a constant value given an input.\nIf  nInputDim  is specified, it uses the input to determine the size of the batch. \nThe  value  is then replicated over the batch. \nOtherwise, the  value  Tensor is output as is.\nDuring  backward , the returned  gradInput  is a zero Tensor of the same size as the  input .\nThis module has no trainable parameters.  \n You can use this with nn.ConcatTable() to append constant inputs to an input :  \n lua\nnn.ConcatTable():add(nn.Constant(v)):add(nn.Identity()) \n This is useful when you want to output a value that is independent of the \ninput to the neural network (see  this example ). \n \n SpatialUniformCrop \n lua\nmodule = nn.SpatialUniformCrop(oheight, owidth) \n During training, this module will output a cropped patch of size  oheight, owidth \nwithin the boundaries of the  input  image.\nFor each example, a location is sampled from a uniform distribution \nsuch that each possible patch has an equal probability of being sampled. \n During evaluation, the center patch is cropped and output. \n This module is commonly used at the input layer to artificially \naugment the size of the dataset to prevent overfitting. \n \n SpatialGlimpse \n Ref. A.  Recurrent Model for Visual Attention \n lua\nmodule = nn.SpatialGlimpse(size, depth, scale) \n A glimpse is the concatenation of down-scaled cropped images of \nincreasing scale around a given location in a given image.\nThe input is a pair of Tensors:  {image, location} \n location  are  (y,x)  coordinates of the center of the different scales \nof patches to be cropped from image  image . \nCoordinates are between  (-1,-1)  (top-left) and  (1,1)  (bottom-right).\nThe  output  is a batch of glimpses taken in image at location  (y,x) . \n size  can be either a scalar which specifies the  width = height  of glimpses, \nor a table of  {height, width}  to support a rectangular shape of glimpses.\n depth  is number of patches to crop per glimpse (one patch per depth).\n scale  determines the  size(t) = scale * size(t-1)  of successive cropped patches. \n So basically, this module can be used to focus the attention of the model \non a region of the input  image . \nIt is commonly used with the  RecurrentAttention  \nmodule (see  this example ). \n \n WhiteNoise \n lua\nmodule = nn.WhiteNoise([mean, stdev]) \n Useful in training [Denoising Autoencoders] (http://arxiv.org/pdf/1507.02672v1.pdf). \nTakes  mean  and  stdev  of the normal distribution as input. \nDefault values for mean and standard deviation are 0 and 0.1 respectively. \nWith  module:training() , noise is added during forward. \nDuring  backward  gradients are passed as it is. \nWith  module:evaluate()  the mean is added to the input. \n \n SpatialRegionDropout \n lua\nmodule = nn.SpatialRegionDropout(p) \nFollowing is an example of  SpatialRegionDropout  outputs on the famous lena image. \n Input \n \n Outputs \n   \n \n FireModule \n Ref: http://arxiv.org/pdf/1602.07360v1.pdf\n lua\nmodule = nn.FireModule(nInputPlane, s1x1, e1x1, e3x3, activation) \nFireModule is comprised of two submodules 1) A  squeeze  convolution module comprised of  1x1  filters followed by 2) an  expand  module that is comprised of a mix of  1x1  and  3x3  convolution filters.\nArguments:  s1x1 : number of  1x1  filters in the squeeze submodule,  e1x1 : number of  1x1  filters in the expand submodule,  e3x3 : number of  3x3  filters in the expand submodule. It is recommended that  s1x1  be less than  (e1x1+e3x3)  if you want to limit the number of input channels to the  3x3  filters in the expand submodule.\nFireModule works only with batches, for single sample convert the sample to a batch of size 1. \n \n SpatialFeatNormalization \n lua\nmodule = nn.SpatialFeatNormalization(mean, std) \nThis module normalizies each feature channel of input image based on its corresponding mean and standard deviation scalar values. This module does not learn the  mean  and  std , they are provided as arguments. \n \n SpatialBinaryConvolution \n lua\nmodule = nn.SpatialBinaryConvolution(nInputPlane, nOutputPlane, kW, kH) \nFunctioning of SpatialBinaryConvolution is similar to nn/SpatialConvolution. Only difference is that Binary weights are used for forward/backward and floating point weights are used for weight updates. Check  Binary-Weight-Network  section of  XNOR-net . \n \n SimpleColorTransform \n lua\nrange = torch.rand(inputChannels) -- Typically range is specified by user.\nmodule = nn.SimpleColorTransform(inputChannels, range) \nThis module performs a simple data augmentation technique. SimpleColorTransform module adds random noise to each color channel independently. In more advanced data augmentation technique noise is added using principal components of color channels. For that please check  PCAColorTransform \n \n PCAColorTransform \n lua\neigenVectors = torch.rand(inputChannels, inputChannels) -- Eigen Vectors\neigenValues = torch.rand(inputChannels) -- Eigen\nstd = 0.1 -- Std deviation of normal distribution with mean zero for noise.\nmodule = nn.PCAColorTransform(inputChannels, eigenVectors, eigenValues, std) \nThis module performs a data augmentation using Principal Component analysis of pixel values. When in training mode, mulitples of principal components are added to input image pixels. Magnitude of value added (noise) is dependent upon the corresponding eigen value and a random value sampled from a Gaussian distribution with mean zero and  std  (default 0.1) standard deviation. This technique was used in the famous  AlexNet  paper. \n \n OneHot \n lua\nmodule = nn.OneHot(outputSize) \n Transforms a tensor of  input  indices having integer values between 1 and  outputSize  into\na tensor of one-hot vectors of size  outputSize .  \n Forward an index to get a one-hot vector : \n ```lua \n \n module = nn.OneHot(5) -- 5 classes\nmodule:forward(torch.LongTensor{3})\n 0  0  1  0  0\n[torch.DoubleTensor of size 1x5]\n```  \n \n Forward a batch of 3 indices. Notice that these need not be stored as  torch.LongTensor  : \n ```lua \n \n module:forward(torch.Tensor{3,2,1})\n 0  0  1  0  0\n 0  1  0  0  0\n 1  0  0  0  0\n[torch.DoubleTensor of size 3x5]\n```  \n \n Forward batch of  2 x 3  indices : \n ```lua\noh:forward(torch.Tensor{{3,2,1},{1,2,3}})\n(1,.,.) = \n  0  0  1  0  0\n  0  1  0  0  0\n  1  0  0  0  0 \n (2,.,.) = \n  1  0  0  0  0\n  0  1  0  0  0\n  0  0  1  0  0\n[torch.DoubleTensor of size 2x3x5]\n```  \n \n Kmeans \n lua\nkm = nn.Kmeans(k, dim) \n k  is the number of centroids and  dim  is the dimensionality of samples.\nYou can either initialize centroids randomly from input samples or by using  kmeans++  algorithm. \n lua\nkm:initRandom(samples) -- Randomly initialize centroids from input samples.\nkm:initKmeansPlus(samples) -- Use Kmeans++ to initialize centroids.   \n Example showing how to use Kmeans module to do standard Kmeans clustering. \n ```lua\nattempts = 10\niter = 100 -- Number of iterations\nbestKm = nil\nbestLoss = math.huge\nlearningRate = 1\nfor j=1, attempts do\n   local km = nn.Kmeans(k, dim)\n   km:initKmeansPlus(samples)\n   for i=1, iter do\n      km:zeroGradParameters()\n      km:forward(samples) -- sets km.loss\n      km:backward(samples, gradOutput) -- gradOutput is ignored \n   -- Gradient Descent weight/centroids update\n  km:updateParameters(learningRate)\n \n end \n if km.loss < bestLoss then\n      bestLoss = km.loss\n      bestKm = km:clone()\n   end\nend\n `` nn.Kmeans()` module maintains loss only for the latest forward. If you want to maintain loss over the whole dataset then you who would need do it my adding the module loss for every forward. \n You can also use  nn.Kmeans()  as an auxillary layer in your network. \nA call to  forward  will generate an  output  containing the index of the nearest cluster for each sample in the batch.\nThe  gradInput  generated by  updateGradInput  will be zero.  \n \n ModuleCriterion \n lua\ncriterion = nn.ModuleCriterion(criterion [, inputModule, targetModule, castTarget])   \n This criterion decorates a  criterion  by allowing the  input  and  target  to be \nfed through an optional  inputModule  and  targetModule  before being passed to the \n criterion . The  inputModule  must not contain parameters as these would not be updated.  \n When  castTarget = true  (the default), the  targetModule  is cast along with the  inputModule  and \n criterion . Otherwise, the  targetModule  isn\'t.   \n \n NCEModule \n Ref. A  RNNLM training with NCE for Speech Recognition \n lua\nncem = nn.NCEModule(inputSize, outputSize, k, unigrams, [Z])   \n When used in conjunction with  NCECriterion , \nthe  NCEModule  implements  noise-contrastive estimation . \n The point of the NCE is to speedup computation for large  Linear  +  SoftMax  layers.\nComputing a forward/backward for  Linear(inputSize, outputSize)  for a large  outputSize  can be very expensive.\nThis is common when implementing language models having with large vocabularies of a million words.\nIn such cases, NCE can be an efficient alternative to computing the full  Linear  +  SoftMax  during training and \ncross-validation. \n The  inputSize  and  outputSize  are the same as for the  Linear  module. \nThe number of noise samples to be drawn per example is  k . A value of 25 should work well. \nIncreasing it will yield better results, while a smaller value will be more efficient to process.\nThe  unigrams  is a tensor of size  outputSize  that contains the frequencies or probability distribution over classes.\nIt is used to sample noise samples via a fast implementation of  torch.multinomial .\nThe  Z  is the normalization constant of the approximated SoftMax. \nThe default is  math.exp(9)  as specified in Ref. A. \n For inference, or measuring perplexity, the full  Linear  +  SoftMax  will need to \nbe computed. The  NCEModule  can do this by switching on the following : \n lua\nncem:evaluate()\nncem.normalized = true   \n Furthermore, to simulate  Linear  +  LogSoftMax  instead, one need only add the following to the above: \n lua\nncem.logsoftmax = true   \n An example is provided via the rnn package. \n \n NCECriterion \n lua\nncec = nn.NCECriterion()   \n This criterion only works with an  NCEModule  on the output layer.\nTogether, they implement  noise-contrastive estimation . \n \n Reinforce \n Ref A.  Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning \n Abstract class for modules that implement the REINFORCE algorithm (ref. A). \n lua\nmodule = nn.Reinforce([stochastic]) \n The  reinforce(reward)  method is called by a special Reward Criterion (e.g.  VRClassReward ).\nAfter which, when backward is called, the reward will be used to generate gradInputs. \nWhen  stochastic=true , the module is stochastic (i.e. samples from a distribution) \nduring evaluation and training.\nWhen  stochastic=false  (the default), the module is only stochastic during training. \n The REINFORCE rule for a module can be summarized as follows :\n lua\n            d ln(f(output,input))\ngradInput = ---------------------  * reward\n                  d input \nwhere the  reward  is what is provided by a Reward criterion like \n VRClassReward  via the  reinforce  method.\nThe criterion will normally be responsible for the following formula :\n lua\nreward = a*(R - b) \nwhere  a  is the alpha of the original paper, i.e. a reward scale,\n R  is the raw reward (usually 0 or 1), and  b  is the baseline reward, \nwhich is often taken to be the expected raw reward  R . \n The  output  is usually sampled from a probability distribution  f() \nparameterized by the  input . \nSee  ReinforceBernoulli  for a concrete derivation. \n Also, as you can see, the gradOutput is ignored. So within a backpropagation graph,\nthe  Reinforce  modules will replace the backpropagated gradients ( gradOutput ) \nwith their own obtained from the broadcasted  reward . \n \n ReinforceBernoulli \n Ref A.  Simple Statistical Gradient-Following Algorithms for\nConnectionist Reinforcement Learning \n lua\nmodule = nn.ReinforceBernoulli([stochastic]) \n A  Reinforce  subclass that implements the REINFORCE algorithm \n(ref. A p.230-236) for the Bernoulli probability distribution.\nInputs are bernoulli probabilities  p . \nDuring training, outputs are samples drawn from this distribution.\nDuring evaluation, when  stochastic=false , outputs are the same as the inputs.\nUses the REINFORCE algorithm (ref. A p.230-236) which is \nimplemented through the  reinforce  interface ( gradOutputs  are ignored). \n Given the following variables :  \n \n f  : bernoulli probability mass function \n x  : the sampled values (0 or 1) (i.e.  self.output ) \n p  : probability of sampling a 1 \n \n the derivative of the log bernoulli w.r.t. probability  p  is :\n d ln(f(output,input))   d ln(f(x,p))    (x - p)\n--------------------- = ------------ = ---------\n      d input               d p         p(1 - p) \n \n ReinforceNormal \n Ref A.  Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning \n lua\nmodule = nn.ReinforceNormal(stdev, [stochastic]) \n A  Reinforce  subclass that implements the REINFORCE algorithm \n(ref. A p.238-239) for a Normal (i.e. Gaussian) probability distribution.\nInputs are the means of the normal distribution.\nThe  stdev  argument specifies the standard deviation of the distribution. \nDuring training, outputs are samples drawn from this distribution.\nDuring evaluation, when  stochastic=false , outputs are the same as the inputs, i.e. the means.\nUses the REINFORCE algorithm (ref. A p.238-239) which is \nimplemented through the  reinforce  interface ( gradOutputs  are ignored). \n Given the following variables :  \n \n f  : normal probability density function \n x  : the sampled values (i.e.  self.output ) \n u  : mean ( input ) \n s  : standard deviation ( self.stdev ) \n \n the derivative of log normal w.r.t. mean  u  is :\n d ln(f(x,u,s))   (x - u)\n-------------- = -------\n     d u           s^2 \n As an example, it is used to sample locations for the  RecurrentAttention  \nmodule (see  this example ). \n \n ReinforceGamma \n Ref A.  Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning \n lua\nmodule = nn.ReinforceGamma(scale, [stochastic])   \n A  Reinforce  subclass that implements the REINFORCE algorithm \n(ref. A) for a  Gamma probability distribution  \nparametrized by shape (k) and scale (theta) variables.\nInputs are the shapes of the gamma distribution.\nDuring training, outputs are samples drawn from this distribution.\nDuring evaluation, when  stochastic=false , outputs are equal to the mean, defined as the product of\nshape and scale ie.  k*theta .\nUses the REINFORCE algorithm (ref. A) which is \nimplemented through the  reinforce  interface ( gradOutputs  are ignored). \n Given the following variables :  \n \n f  : gamma probability density function \n g  : digamma function \n x  : the sampled values (i.e.  self.output ) \n k  : shape ( input ) \n t  : scale \n \n the derivative of log gamma w.r.t. shape  k  is :\n d ln(f(x,k,t))\n-------------- = ln(x) - g(k) - ln(t)\n      d k   \n \n ReinforceCategorical \n Ref A.  Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning \n lua\nmodule = nn.ReinforceCategorical([stochastic]) \n A  Reinforce  subclass that implements the REINFORCE algorithm \n(ref. A) for a Categorical (i.e. Multinomial with one sample) probability distribution.\nInputs are the categorical probabilities of the distribution :  p[1], p[2], ..., p[k] .\nThese are usually the output of a SoftMax. \nFor  n  categories, both the  input  and  output  ares of size  batchSize x n .\nDuring training, outputs are samples drawn from this distribution.\nThe outputs are returned in one-hot encoding i.e. \nthe output for each example has exactly one category having a 1, while the remainder are zero.\nDuring evaluation, when  stochastic=false , outputs are the same as the inputs, i.e. the probabilities  p .\nUses the REINFORCE algorithm (ref. A) which is \nimplemented through the  reinforce  interface ( gradOutputs  are ignored). \n Given the following variables :  \n \n f  : categorical probability mass function \n x  : the sampled indices (one per sample) ( self.output  is the one-hot encoding of these indices) \n p  : probability vector ( p[1], p[2], ..., p[k] ) ( input ) \n \n the derivative of log categorical w.r.t. probability vector  p  is :\n d ln(f(x,p))     1/p[i]    if i = x  \n------------ =   \n    d p          0         otherwise   \n \n VRClassReward \n Ref A.  Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning \n This Reward criterion implements the REINFORCE algoritm (ref. A) for classification models.\nSpecifically, it is a Variance Reduces (VR) classification reinforcement leanring (reward-based) criterion. \n lua\nvcr = nn.VRClassReward(module [, scale, criterion])   \n While it conforms to the Criterion interface (which it inherits), \nit does not backpropagate gradients (except for the baseline  b ; see below).\nInstead, a  reward  is broadcast to the  module  via the  reinforce  method. \n The criterion implements the following formula :\n lua\nreward = a*(R - b) \nwhere  a  is the alpha described in Ref. A, i.e. a reward  scale  (defaults to 1),\n R  is the raw reward (0 for incorrect and 1 for correct classification), \nand  b  is the baseline reward, which is often taken to be the expected raw reward  R . \n The  target  of the criterion is a tensor of class indices.\nThe  input  to the criterion is a table  {y,b}  where  y  is the probability \n(or log-probability) of classes (usually the output of a SoftMax), \nand  b  is the baseline reward discussed above.  \n For each example, if  argmax(y)  is equal to the  target  class, the raw reward  R = 1 , otherwize  R = 0 . \n As for  b , its  gradInputs  are obtained from the  criterion , which defaults to  MSECriterion .\nThe  criterion \'s target is the commensurate raw reward  R .\nUsing  a*(R-b)  instead of  a*R  to obtain a  reward  is what makes this class variance reduced (VR).\nBy reducing the variance, the training can converge faster (Ref. A).\nThe predicted  b  can be nothing more than the expectation  E(R) . \n Note : for RNNs with R = 1 for last step in sequence, encapsulate it\nin  nn.ModuleCriterion(VRClassReward, nn.SelectTable(-1)) . \n For an example, this criterion is used along with the  RecurrentAttention  \nmodule to  train a recurrent model for visual attention . \n \n BinaryClassReward \n lua\nbcr = nn.BinaryClassReward(module [, scale, criterion])   \n This module implements  VRClassReward  for binary classification problems.\nSo basically, the  input  is still a table of two tensors. \nThe first input tensor is of size  batchsize  containing Bernoulli probabilities.\nThe second input tensor is the baseline prediction described in  VRClassReward .\nThe targets contain zeros and ones. \n \n BinaryLogisticRegression \n Ref A.  Learning to Segment Object Candidates \nThis criterion implements the score criterion mentioned in (ref. A). \n lua\ncriterion = nn.BinaryLogisticRegression()   \n BinaryLogisticRegression implements following cost function for binary classification. \n ``` \n log( 1 + exp( -y_k * score(x_k) ) ) \n `` \nwhere y_k is binary target score(x_k) is the corresponding prediction. y_k has value {-1, +1} and score(x_k) has value in [-1, +1]`. \n \n SpatialBinaryLogisticRegression \n Ref A.  Learning to Segment Object Candidates \n This criterion implements the spatial component of the criterion mentioned in  (ref. A). \n lua\ncriterion = nn.SpatialBinaryLogisticRegression()   \n SpatialBinaryLogisticRegression implements following cost function for binary pixel classification.\n 1\n_______ sum_ij [ log( 1 + exp( -m_ij * f_ij ) ) ]\n 2*w*h  \nwhere  m_ij  is target binary image and  f_ij  is the corresponding prediction.  m_ij  has value  {-1, +1}  and  f_ij  has value in  [-1, +1] .', 'hpo \n Hyper-parameter optimization', 'slides \n \n Boston ML \n NVIDIA GTC \n AI Camp  ( pptx ) : 12 July 2016, United Nations HQ, New York, 15min, Overview of Torch and Language Models. \n', 'rnn-examples \n Examples for the RNN library / framework', 'DrMAD \n To provide an efficient and easy-to-use hyperparameter tuning toolbox for Torch deep learning ecosystems. \n It combines Bayesian optimization (BO) and automatic differentiation (AD). For the Bayesian optimization module,\nwe will extend on  hypero ; the automatic differentiation part is based on\nDrMAD method, https://arxiv.org/abs/1601.00917. \n It is the only tool that can tune thousands of continuous hyperparameters (e.g. L2 penalties for each neuron or\nlearning rates for each layer) with a reasonable time/computational budget -- reads: outside Google. \n Current Status \n Skechy code for tuning L2 penalties and learning rates on MNIST dataset with CUDA support. \n TODO \n \n API for tuning learning rates, weight decay and momentum.  \n Experiments on ImageNet \n \n Dependencies \n \n Twitter Torch Autograd : the next version will not depend on this.  \n \n How to run \n \n drmad_mnist.lua  is for tuning L2 penalties on MNIST.  \n cuda_drmad_mnist.lua  is for tuning L2 penalties on MNIST with CUDA.  \n lr_drmad_mnist.lua  is for tuning learning rates and L2 penalties on MNIST.   \n \n Tricks \n Rally with ( Net2Net ) \n ImageNet dataset usually needs ~450,000 iterations. DrMAD may not approxiate this long trajectory well.  \n One approach would be to repeatedly initialize the weights using Net2Net, from small subsets to larget subsets\nand finally to the full dataset. \n Acceleration with Model Compression \n We will add a regression loss at every layer, which is also used in  Deep Q-Networks for Accelerating the Training of Deep Neural Networks . However, the aim here is not to compress the model, so we do not decrease the number of parameters.  \n BO and AD \n BO is a global optimization method (it can handle 20 hyperparameters at most), whereas AD can only find local solutions\n(it can handle thousands of hyperparameters because it uses gradient information). We first use BO to get some initial\naverage hyperparameters. Then we use AD method to further search for diverse local hyperparameters. \n Contact \n If you have any problems or suggestions, please contact me: jie.fu A~_~T u.nus.edu~~cation~~', 'This package is a basic Reinforcement Learning package written in LUA for Torch. It implements some simple environments and learning policies (Policy Gradient and Deep Q Learning). It also can be easily used with the OpenAI Gym package by using lutorpy (example given in the opeanaigym directory). \n Tutorials are provided in the tutorials directory \n Dependencies \n Lua: \n* Torch7\n* nn, dpnn\n* logroll, json, alewrap \n For using openAI Gym:\n* openai gym\n* lutorpy \n Installation \n \n In the torch directory: luarocks make \n Install lutorpy and open AI \n lauch the python script (example.py) \n \n WARNING : If you use an openAI Gym ATARI environment, a new sensor must be developped: it will be avaiable in the next few days (since openAI and alewrap do not store the ATARI images in the same format) \n Author: Ludovic DENOYER', 'Torch in Action \n This repository contains the code for the Torch in Action book. \n Chapter 1 : Meeting Torch\n *  facedetect : toy face detection dataset (directory with only four samples);\n *  train.lua : example face detection training script (listings 1.1, 1.2 and 1.3); \n Chapter 2 : Preparing a dataset\n *  mnist : MNIST dataset in binary format as downloaded from  yann.lecun.com ;\n *  createdataset.lua : code for serializing the MNIST dataset into  .t7  files and generating samples (section 2.3);\n *  dataloader.lua : code for listing 2.1, 2.2, 2.3 and 2.5. Defines the  DataLoader  and  TensorLoader  classes);\n *  iteratedataset.lua : code for listing 2.5. This script tests the  dataloader.lua  file by iterating through it. Only works if  createdataset.lua  was executed before hand;\n *  getmnistsample.lua : script for generating MNIST samples consolidated as a single image (used to generate figure 2.1); \n Chapter 3 : Training simple neural networks\n *  trainlogreg.lua : Training script for applying binary logistic regression on OR dataset. The model is trained using stochastic gradient descent (listing 3.1);\n *  logreg.log : log file created by running  th trainlogreg.lua > logreg.log ;\n *  trainlogreg-mnist.lua : Script for training a multinomial logistic regression model (saved as  logreg-mnist.t7 ) using SGD on the MNIST dataset. Training stops after 200 epochs where each epoch consists of 10000 samples divided into mini-batches of 32 random samples, or reaching an estimated empirical risk lower than 0.007, whichever comes first. The resulting model is evaluated on the entire training set of 50000 samples and saved to disk (listing 3.2);\n *  logreg-mnist.log : log file created by running  th trainlogreg-mnist.lua > logreg-mnist.log . The data can be used to generate a learning curve. Open the file from your favorite spreadsheet application (Microsoft Excel, LibreOffice Calc, etc.) and specify that values are separated by semicolons;\n *  backward.lua : demonstrates gradient descent through a criterion. Using the input as a parameter, the loss is minized by tacking a step in opposite direction of gradient (section 8.1.3); \n Chapter 4 : Generalizing deep neural networks\n *  tanh.xlsx : plot of the hyperbolic tangent activation function (figure 4.2);\n *  trainmlp-xor.lua : script for training an MLP with one hidden layer composed of 2 units on the XOR dataset. Used to generate  xor-mlp.xlsx  and figure 4.3;\n *  xor-mlp.xlsx : diagram outlining the boundaries of an MLP trained on the XOR dataset (figure 4.3);\n *  overfitting.xlsx : contains learing curve and model overfitting example (figure 4.4 and 4.5);\n *  trainmlp-mnist.lua : upgrades  trainlogreg-mnist.lua  by moving the definition of hyper-parameters to the cmd-line (listing 4.1 and 4.2).\n *  trainmlp-mnist-crossvalidate.lua : upgrades  trainmlp-mnist.lua  with cross-validation (listing 4.3);\n *  trainmlp-mnist-earlystop.lua : upgrades  trainmlp-mnist-crossvalidate.lua  with early-stopping (listing 4.4);\n *  trainmlp-mnist-weightdecay.lua : upgrades  trainmlp-mnist-earlystop.lua  with weight decay regularization (listing 4.5);\n *  trainmlp-mnist-hyperopt.lua : upgrades  trainmlp-mnist-weightdecay.lua  to facilitate hyper-parameter optimization (listing 4.6);\n *  hyperopt-mnist.xlsx : spreadsheet used to hyper-optimize the  trainmlp-mnist-hyperopt.lua  script (figure 4.7 and 4.8);\n *  relu.xlsx : plot of the rectified linear unit (figure 4.9).', ""rnn: recurrent neural networks \n This is a Recurrent Neural Network library that extends Torch's nn. \nYou can use it to build RNNs, LSTMs, GRUs, BRNNs, BLSTMs, and so forth and so on.\nThis library includes documentation for the following objects: \n Modules that consider successive calls to  forward  as different time-steps in a sequence :\n *  AbstractRecurrent  : an abstract class inherited by Recurrent and LSTM;\n *  Recurrent  : a generalized recurrent neural network container;\n *  LSTM  : a vanilla Long-Short Term Memory module;\n  *  FastLSTM  : a faster  LSTM  with optional support for batch normalization;\n *  GRU  : Gated Recurrent Units module;\n *  Recursor  : decorates a module to make it conform to the  AbstractRecurrent  interface;\n *  Recurrence  : decorates a module that outputs  output(t)  given  {input(t), output(t-1)} ;\n *  NormStabilizer  : implements  norm-stabilization  criterion (add this module between RNNs); \n Modules that  forward  entire sequences through a decorated  AbstractRecurrent  instance :\n *  AbstractSequencer  : an abstract class inherited by Sequencer, Repeater, RecurrentAttention, etc.;\n *  Sequencer  : applies an encapsulated module to all elements in an input sequence  (Tensor or Table);\n *  SeqLSTM  : a very fast version of  nn.Sequencer(nn.FastLSTM)  where the  input  and  output  are tensors;\n  *  SeqLSTMP  :  SeqLSTM  with a projection layer;\n *  SeqGRU  : a very fast version of  nn.Sequencer(nn.GRU)  where the  input  and  output  are tensors;\n *  SeqBRNN  : Bidirectional RNN based on SeqLSTM;\n *  BiSequencer  : used for implementing Bidirectional RNNs and LSTMs;\n *  BiSequencerLM  : used for implementing Bidirectional RNNs and LSTMs for language models;\n *  Repeater  : repeatedly applies the same input to an AbstractRecurrent instance;\n *  RecurrentAttention  : a generalized attention model for  REINFORCE modules ; \n Miscellaneous modules and criterions :\n *  MaskZero  : zeroes the  output  and  gradOutput  rows of the decorated module for commensurate  input  rows which are tensors of zeros;\n *  TrimZero  : same behavior as  MaskZero , but more efficient when  input  contains lots zero-masked rows;\n *  LookupTableMaskZero  : extends  nn.LookupTable  to support zero indexes for padding. Zero indexes are forwarded as tensors of zeros;\n *  MaskZeroCriterion  : zeros the  gradInput  and  err  rows of the decorated criterion for commensurate  input  rows which are tensors of zeros;\n *  SeqReverseSequence  : reverses an input sequence on a specific dimension; \n Criterions used for handling sequential inputs and targets :\n *  SequencerCriterion  : sequentially applies the same criterion to a sequence of inputs and targets (Tensor or Table).\n *  RepeaterCriterion  : repeatedly applies the same criterion with the same target on a sequence. \n \n Examples \n The following are example training scripts using this package : \n \n RNN/LSTM/GRU  for Penn Tree Bank dataset; \n Noise Contrastive Estimate  for training multi-layer  SeqLSTM  language models on the  Google Billion Words dataset . The example uses  MaskZero  to train independent variable length sequences using the  NCEModule  and  NCECriterion . This script is our fastest yet boasting speeds of 20,000 words/second (on NVIDIA Titan X) with a 2-layer LSTM having 250 hidden units, a batchsize of 128 and sequence length of a 100. Note that you will need to have  Torch installed with Lua instead of LuaJIT ; \n Recurrent Model for Visual Attention  for the MNIST dataset; \n Encoder-Decoder LSTM  shows you how to couple encoder and decoder  LSTMs  for sequence-to-sequence networks; \n Simple Recurrent Network  shows a simple example for building and training a simple recurrent neural network; \n Simple Sequencer Network  is a version of the above script that uses the Sequencer to decorate the  rnn  instead; \n Sequence to One  demonstrates how to do many to one sequence learning as is the case for sentiment analysis; \n Multivariate Time Series  demonstrates how train a simple RNN to do multi-variate time-series predication. \n \n External Resources \n \n rnn-benchmarks  : benchmarks comparing Torch (using this library), Theano and TensorFlow. \n Harvard Jupyter Notebook Tutorial  : an in-depth tutorial for how to use the Element-Research rnn package by Harvard University; \n dpnn  : this is a dependency of the  rnn  package. It contains useful nn extensions, modules and criterions; \n dataload  : a collection of torch dataset loaders; \n RNN/LSTM/BRNN/BLSTM training script   for Penn Tree Bank or Google Billion Words datasets; \n A brief (1 hours) overview of Torch7, which includes some details about the  rnn  packages (at the end), is available via this  NVIDIA GTC Webinar video . In any case, this presentation gives a nice overview of Logistic Regression, Multi-Layer Perceptrons, Convolutional Neural Networks and Recurrent Neural Networks using Torch7; \n Sequence to Sequence mapping using encoder-decoder RNNs  : a complete training example using synthetic data. \n ConvLSTM  is a repository for training a  Spatio-temporal video autoencoder with differentiable memory . \n An  time series example  for univariate timeseries prediction. \n \n Citation \n If you use  rnn  in your work, we'd really appreciate it if you could cite the following paper: \n Léonard, Nicholas, Sagar Waghmare, Yang Wang, and Jin-Hwa Kim.  rnn: Recurrent Library for Torch.  arXiv preprint arXiv:1511.07889 (2015). \n Any significant contributor to the library will also get added as an author to the paper.\nA  significant contributor  \nis anyone who added at least 300 lines of code to the library. \n Troubleshooting \n Most issues can be resolved by updating the various dependencies:\n bash\nluarocks install torch\nluarocks install nn\nluarocks install dpnn\nluarocks install torchx \n If you are using CUDA :\n bash\nluarocks install cutorch\nluarocks install cunn\nluarocks install cunnx \n And don't forget to update this package :\n bash\nluarocks install rnn \n If that doesn't fix it, open and issue on github. \n \n AbstractRecurrent \n An abstract class inherited by  Recurrent ,  LSTM  and  GRU .\nThe constructor takes a single argument :\n lua\nrnn = nn.AbstractRecurrent([rho]) \nArgument  rho  is the maximum number of steps to backpropagate through time (BPTT).\nSub-classes can set this to a large number like 99999 (the default) if they want to backpropagate through \nthe entire sequence whatever its length. Setting lower values of rho are \nuseful when long sequences are forward propagated, but we only whish to \nbackpropagate through the last  rho  steps, which means that the remainder \nof the sequence doesn't need to be stored (so no additional cost).  \n [recurrentModule] getStepModule(step) \n Returns a module for time-step  step . This is used internally by sub-classes \nto obtain copies of the internal  recurrentModule . These copies share \n parameters  and  gradParameters  but each have their own  output ,  gradInput  \nand any other intermediate states.  \n setOutputStep(step) \n This is a method reserved for internal use by  Recursor  \nwhen doing backward propagation. It sets the object's  output  attribute\nto point to the output at time-step  step . \nThis method was introduced to solve a very annoying bug. \n \n maskZero(nInputDim) \n Decorates the internal  recurrentModule  with  MaskZero . \nThe  output  Tensor (or table thereof) of the  recurrentModule \nwill have each row (i.e. samples) zeroed when the commensurate row of the  input  \nis a tensor of zeros.  \n The  nInputDim  argument must specify the number of non-batch dims \nin the first Tensor of the  input . In the case of an  input  table,\nthe first Tensor is the first one encountered when doing a depth-first search. \n Calling this method makes it possible to pad sequences with different lengths in the same batch with zero vectors. \n When a sample time-step is masked (i.e.  input  is a row of zeros), then \nthe hidden state is effectively reset (i.e. forgotten) for the next non-mask time-step.\nIn other words, it is possible seperate unrelated sequences with a masked element. \n trimZero(nInputDim) \n Decorates the internal  recurrentModule  with  TrimZero .  \n [output] updateOutput(input) \n Forward propagates the input for the current step. The outputs or intermediate \nstates of the previous steps are used recurrently. This is transparent to the \ncaller as the previous outputs and intermediate states are memorized. This \nmethod also increments the  step  attribute by 1. \n \n updateGradInput(input, gradOutput) \n Like  backward , this method should be called in the reverse order of \n forward  calls used to propagate a sequence. So for example : \n ```lua\nrnn = nn.LSTM(10, 10) -- AbstractRecurrent instance\nlocal outputs = {}\nfor i=1,nStep do -- forward propagate sequence\n   outputs[i] = rnn:forward(inputs[i])\nend \n for i=nStep,1,-1 do -- backward propagate sequence in reverse order\n   gradInputs[i] = rnn:backward(inputs[i], gradOutputs[i])\nend \n rnn:forget()\n```  \n The reverse order implements backpropagation through time (BPTT). \n accGradParameters(input, gradOutput, scale) \n Like  updateGradInput , but for accumulating gradients w.r.t. parameters. \n recycle(offset) \n This method goes hand in hand with  forget . It is useful when the current\ntime-step is greater than  rho , at which point it starts recycling \nthe oldest  recurrentModule   sharedClones , \nsuch that they can be reused for storing the next step. This  offset  \nis used for modules like  nn.Recurrent  that use a different module \nfor the first step. Default offset is 0. \n \n forget(offset) \n This method brings back all states to the start of the sequence buffers, \ni.e. it forgets the current sequence. It also resets the  step  attribute to 1.\nIt is highly recommended to call  forget  after each parameter update. \nOtherwise, the previous state will be used to activate the next, which \nwill often lead to instability. This is caused by the previous state being\nthe result of now changed parameters. It is also good practice to call \n forget  at the start of each new sequence. \n \n maxBPTTstep(rho) \n This method sets the maximum number of time-steps for which to perform \nbackpropagation through time (BPTT). So say you set this to  rho = 3  time-steps,\nfeed-forward for 4 steps, and then backpropgate, only the last 3 steps will be \nused for the backpropagation. If your AbstractRecurrent instance is wrapped \nby a  Sequencer , this will be handled auto-magically by the Sequencer.\nOtherwise, setting this value to a large value (i.e. 9999999), is good for most, if not all, cases. \n \n backwardOnline() \n This method was deprecated Jan 6, 2016. \nSince then, by default,  AbstractRecurrent  instances use the \nbackwardOnline behaviour. \nSee  updateGradInput  for details. \n training() \n In training mode, the network remembers all previous  rho  (number of time-steps)\nstates. This is necessary for BPTT.  \n evaluate() \n During evaluation, since their is no need to perform BPTT at a later time, \nonly the previous step is remembered. This is very efficient memory-wise, \nsuch that evaluation can be performed using potentially infinite-length \nsequence. \n \n Recurrent \n References :\n * A.  Sutsekever Thesis Sec. 2.5 and 2.8 \n * B.  Mikolov Thesis Sec. 3.2 and 3.3 \n * C.  RNN and Backpropagation Guide \n A  composite Module  for implementing Recurrent Neural Networks (RNN), excluding the output layer.  \n The  nn.Recurrent(start, input, feedback, [transfer, rho, merge])  constructor takes 6 arguments:\n *  start  : the size of the output (excluding the batch dimension), or a Module that will be inserted between the  input  Module and  transfer  module during the first step of the propagation. When  start  is a size (a number or  torch.LongTensor ), then this  start  Module will be initialized as  nn.Add(start)  (see Ref. A).\n *  input  : a Module that processes input Tensors (or Tables). Output must be of same size as  start  (or its output in the case of a  start  Module), and same size as the output of the  feedback  Module.\n *  feedback  : a Module that feedbacks the previous output Tensor (or Tables) up to the  merge  module.\n *  merge  : a  table Module  that merges the outputs of the  input  and  feedback  Module before being forwarded through the  transfer  Module.\n *  transfer  : a non-linear Module used to process the output of the  merge  module, or in the case of the first step, the output of the  start  Module.\n *  rho  : the maximum amount of backpropagation steps to take back in time. Limits the number of previous steps kept in memory. Due to the vanishing gradients effect, references A and B recommend  rho = 5  (or lower). Defaults to 99999. \n An RNN is used to process a sequence of inputs. \nEach step in the sequence should be propagated by its own  forward  (and  backward ), \none  input  (and  gradOutput ) at a time. \nEach call to  forward  keeps a log of the intermediate states (the  input  and many  Module.outputs ) \nand increments the  step  attribute by 1. \nMethod  backward  must be called in reverse order of the sequence of calls to  forward  in \norder to backpropgate through time (BPTT). This reverse order is necessary \nto return a  gradInput  for each call to  forward .  \n The  step  attribute is only reset to 1 when a call to the  forget  method is made. \nIn which case, the Module is ready to process the next sequence (or batch thereof).\nNote that the longer the sequence, the more memory that will be required to store all the \n output  and  gradInput  states (one for each time step).  \n To use this module with batches, we suggest using different \nsequences of the same size within a batch and calling  updateParameters  \nevery  rho  steps and  forget  at the end of the sequence.  \n Note that calling the  evaluate  method turns off long-term memory; \nthe RNN will only remember the previous output. This allows the RNN \nto handle long sequences without allocating any additional memory. \n For a simple concise example of how to make use of this module, please consult the \n simple-recurrent-network.lua \ntraining script. \n \n Decorate it with a Sequencer \n Note that any  AbstractRecurrent  instance can be decorated with a  Sequencer  \nsuch that an entire sequence (a table) can be presented with a single  forward/backward  call.\nThis is actually the recommended approach as it allows RNNs to be stacked and makes the \nrnn conform to the Module interface, i.e. each call to  forward  can be \nfollowed by its own immediate call to  backward  as each  input  to the \nmodel is an entire sequence, i.e. a table of tensors where each tensor represents\na time-step. \n lua\nseq = nn.Sequencer(module) \n The  simple-sequencer-network.lua  training script\nis equivalent to the above mentionned  simple-recurrent-network.lua \nscript, except that it decorates the  rnn  with a  Sequencer  which takes \na table of  inputs  and  gradOutputs  (the sequence for that batch).\nThis lets the  Sequencer  handle the looping over the sequence. \n You should only think about using the  AbstractRecurrent  modules without \na  Sequencer  if you intend to use it for real-time prediction. \nActually, you can even use an  AbstractRecurrent  instance decorated by a  Sequencer \nfor real time prediction by calling  Sequencer:remember()  and presenting each \ntime-step  input  as  {input} . \n Other decorators can be used such as the  Repeater  or  RecurrentAttention .\nThe  Sequencer  is only the most common one.  \n \n LSTM \n References :\n * A.  Speech Recognition with Deep Recurrent Neural Networks \n * B.  Long-Short Term Memory \n * C.  LSTM: A Search Space Odyssey \n * D.  nngraph LSTM implementation on github \n This is an implementation of a vanilla Long-Short Term Memory module. \nWe used Ref. A's LSTM as a blueprint for this module as it was the most concise.\nYet it is also the vanilla LSTM described in Ref. C.  \n The  nn.LSTM(inputSize, outputSize, [rho])  constructor takes 3 arguments:\n *  inputSize  : a number specifying the size of the input;\n *  outputSize  : a number specifying the size of the output;\n *  rho  : the maximum amount of backpropagation steps to take back in time. Limits the number of previous steps kept in memory. Defaults to 9999. \n   \n The actual implementation corresponds to the following algorithm:\n lua\ni[t] = σ(W[x->i]x[t] + W[h->i]h[t−1] + W[c->i]c[t−1] + b[1->i])      (1)\nf[t] = σ(W[x->f]x[t] + W[h->f]h[t−1] + W[c->f]c[t−1] + b[1->f])      (2)\nz[t] = tanh(W[x->c]x[t] + W[h->c]h[t−1] + b[1->c])                   (3)\nc[t] = f[t]c[t−1] + i[t]z[t]                                         (4)\no[t] = σ(W[x->o]x[t] + W[h->o]h[t−1] + W[c->o]c[t] + b[1->o])        (5)\nh[t] = o[t]tanh(c[t])                                                (6) \nwhere  W[s->q]  is the weight matrix from  s  to  q ,  t  indexes the time-step,\n b[1->q]  are the biases leading into  q ,  σ()  is  Sigmoid ,  x[t]  is the input,\n i[t]  is the input gate (eq. 1),  f[t]  is the forget gate (eq. 2), \n z[t]  is the input to the cell (which we call the hidden) (eq. 3), \n c[t]  is the cell (eq. 4),  o[t]  is the output gate (eq. 5), \nand  h[t]  is the output of this module (eq. 6). Also note that the \nweight matrices from cell to gate vectors are diagonal  W[c->s] , where  s  \nis  i , f , or  o . \n As you can see, unlike  Recurrent , this \nimplementation isn't generic enough that it can take arbitrary component Module\ndefinitions at construction. However, the LSTM module can easily be adapted \nthrough inheritance by overriding the different factory methods :\n  *  buildGate  : builds generic gate that is used to implement the input, forget and output gates;\n  *  buildInputGate  : builds the input gate (eq. 1). Currently calls  buildGate ;\n  *  buildForgetGate  : builds the forget gate (eq. 2). Currently calls  buildGate ;\n  *  buildHidden  : builds the hidden (eq. 3);\n  *  buildCell  : builds the cell (eq. 4);\n  *  buildOutputGate  : builds the output gate (eq. 5). Currently calls  buildGate ;\n  *  buildModel  : builds the actual LSTM model which is used internally (eq. 6). \n Note that we recommend decorating the  LSTM  with a  Sequencer  \n(refer to  this  for details). \n \n FastLSTM \n A faster version of the  LSTM . \nBasically, the input, forget and output gates, as well as the hidden state are computed at one fellswoop. \n Note that  FastLSTM  does not use peephole connections between cell and gates. The algorithm from  LSTM  changes as follows:\n lua\ni[t] = σ(W[x->i]x[t] + W[h->i]h[t−1] + b[1->i])                      (1)\nf[t] = σ(W[x->f]x[t] + W[h->f]h[t−1] + b[1->f])                      (2)\nz[t] = tanh(W[x->c]x[t] + W[h->c]h[t−1] + b[1->c])                   (3)\nc[t] = f[t]c[t−1] + i[t]z[t]                                         (4)\no[t] = σ(W[x->o]x[t] + W[h->o]h[t−1] + b[1->o])                      (5)\nh[t] = o[t]tanh(c[t])                                                (6) \ni.e. omitting the summands  W[c->i]c[t−1]  (eq. 1),  W[c->f]c[t−1]  (eq. 2), and  W[c->o]c[t]  (eq. 5). \n usenngraph \n This is a static attribute of the  FastLSTM  class. The default value is  false .\nSetting  usenngraph = true  will force all new instantiated instances of  FastLSTM  \nto use  nngraph 's  nn.gModule  to build the internal  recurrentModule  which is \ncloned for each time-step. \n \n Recurrent Batch Normalization \n This extends the  FastLSTM  class to enable faster convergence during training by zero-centering the input-to-hidden and hidden-to-hidden transformations. \nIt reduces the  internal covariate shift  between time steps. It is an implementation of Cooijmans et. al.'s  Recurrent Batch Normalization . The hidden-to-hidden transition of each LSTM cell is normalized according to \n lua\ni[t] = σ(BN(W[x->i]x[t]) + BN(W[h->i]h[t−1]) + b[1->i])                      (1)\nf[t] = σ(BN(W[x->f]x[t]) + BN(W[h->f]h[t−1]) + b[1->f])                      (2)\nz[t] = tanh(BN(W[x->c]x[t]) + BN(W[h->c]h[t−1]) + b[1->c])                   (3)\nc[t] = f[t]c[t−1] + i[t]z[t]                                                 (4)\no[t] = σ(BN(W[x->o]x[t]) + BN(W[h->o]h[t−1]) + b[1->o])                      (5)\nh[t] = o[t]tanh(c[t])                                                        (6)  \nwhere the batch normalizing transform is:                                  \n lua\n  BN(h; gamma, beta) = beta + gamma *      hd - E(hd)\n                                      ------------------\n                                       sqrt(E(σ(hd) + eps)) \nwhere  hd  is a vector of (pre)activations to be normalized,  gamma , and  beta  are model parameters that determine the mean and standard deviation of the normalized activation.  eps  is a regularization hyperparameter to keep the division numerically stable and  E(hd)  and  E(σ(hd))  are the estimates of the mean and variance in the mini-batch respectively. The authors recommend initializing  gamma  to a small value and found 0.1 to be the value that did not cause vanishing gradients.  beta , the shift parameter, is  null  by default. \n To turn on batch normalization during training, do:\n lua\nnn.FastLSTM.bn = true\nlstm = nn.FastLSTM(inputsize, outputsize, [rho, eps, momentum, affine]   \n where  momentum  is same as  gamma  in the equation above (defaults to 0.1),  eps  is defined above and  affine  is a boolean whose state determines if the learnable affine transform is turned off ( false ) or on ( true , the default). \n \n GRU \n References :\n * A.  Learning Phrase Representations Using RNN Encoder-Decoder For Statistical Machine Translation. \n * B.  Implementing a GRU/LSTM RNN with Python and Theano \n * C.  An Empirical Exploration of Recurrent Network Architectures \n * D.  Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling \n * E.  RnnDrop: A Novel Dropout for RNNs in ASR \n * F.  A Theoretically Grounded Application of Dropout in Recurrent Neural Networks \n This is an implementation of Gated Recurrent Units module.  \n The  nn.GRU(inputSize, outputSize [,rho [,p [, mono]]])  constructor takes 3 arguments likewise  nn.LSTM  or 4 arguments for dropout:\n *  inputSize  : a number specifying the size of the input;\n *  outputSize  : a number specifying the size of the output;\n *  rho  : the maximum amount of backpropagation steps to take back in time. Limits the number of previous steps kept in memory. Defaults to 9999;\n *  p  : dropout probability for inner connections of GRUs.\n *  mono  : Monotonic sample for dropouts inside GRUs. Only needed in a  TrimZero  +  BGRU (p>0) situation. \n   \n The actual implementation corresponds to the following algorithm:\n lua\nz[t] = σ(W[x->z]x[t] + W[s->z]s[t−1] + b[1->z])            (1)\nr[t] = σ(W[x->r]x[t] + W[s->r]s[t−1] + b[1->r])            (2)\nh[t] = tanh(W[x->h]x[t] + W[hr->c](s[t−1]r[t]) + b[1->h])  (3)\ns[t] = (1-z[t])h[t] + z[t]s[t-1]                           (4) \nwhere  W[s->q]  is the weight matrix from  s  to  q ,  t  indexes the time-step,  b[1->q]  are the biases leading into  q ,  σ()  is  Sigmoid ,  x[t]  is the input and  s[t]  is the output of the module (eq. 4). Note that unlike the  LSTM , the GRU has no cells. \n The GRU was benchmark on  PennTreeBank  dataset using  recurrent-language-model.lua  script. \nIt slightly outperfomed  FastLSTM , however, since LSTMs have more parameters than GRUs, \nthe dataset larger than  PennTreeBank  might change the performance result. \nDon't be too hasty to judge on which one is the better of the two (see Ref. C and D). \n Memory   examples/s\n    FastLSTM      176M        16.5K \n    GRU            92M        15.8K \n Memory  is measured by the size of  dp.Experiment  save file.  examples/s  is measured by the training speed at 1 epoch, so, it may have a disk IO bias. \n \n RNN dropout (see Ref. E and F) was benchmark on  PennTreeBank  dataset using  recurrent-language-model.lua  script, too. The details can be found in the script. In the benchmark,  GRU  utilizes a dropout after  LookupTable , while  BGRU , stands for Bayesian GRUs, uses dropouts on inner connections (naming as Ref. F), but not after  LookupTable . \n As Yarin Gal (Ref. F) mentioned, it is recommended that one may use  p = 0.25  for the first attempt. \n \n \n Recursor \n This module decorates a  module  to be used within an  AbstractSequencer  instance.\nIt does this by making the decorated module conform to the  AbstractRecurrent  interface,\nwhich like the  LSTM  and  Recurrent  classes, this class inherits.  \n lua\nrec = nn.Recursor(module[, rho]) \n For each successive call to  updateOutput  (i.e.  forward ), this \ndecorator will create a  stepClone()  of the decorated  module . \nSo for each time-step, it clones the  module . Both the clone and \noriginal share parameters and gradients w.r.t. parameters. However, for \nmodules that already conform to the  AbstractRecurrent  interface, \nthe clone and original module are one and the same (i.e. no clone). \n Examples : \n Let's assume I want to stack two LSTMs. I could use two sequencers : \n lua\nlstm = nn.Sequential()\n   :add(nn.Sequencer(nn.LSTM(100,100)))\n   :add(nn.Sequencer(nn.LSTM(100,100))) \n Using a  Recursor , I make the same model with a single  Sequencer  : \n lua\nlstm = nn.Sequencer(\n   nn.Recursor(\n      nn.Sequential()\n         :add(nn.LSTM(100,100))\n         :add(nn.LSTM(100,100))\n      )\n   ) \n Actually, the  Sequencer  will wrap any non- AbstractRecurrent  module automatically, \nso I could simplify this further to : \n lua\nlstm = nn.Sequencer(\n   nn.Sequential()\n      :add(nn.LSTM(100,100))\n      :add(nn.LSTM(100,100))\n   ) \n I can also add a  Linear  between the two  LSTM s. In this case,\na  Linear  will be cloned (and have its parameters shared) for each time-step,\nwhile the  LSTM s will do whatever cloning internally : \n lua\nlstm = nn.Sequencer(\n   nn.Sequential()\n      :add(nn.LSTM(100,100))\n      :add(nn.Linear(100,100))\n      :add(nn.LSTM(100,100))\n   )   \n AbstractRecurrent  instances like  Recursor ,  Recurrent  and  LSTM  are \nexpcted to manage time-steps internally. Non- AbstractRecurrent  instances\ncan be wrapped by a  Recursor  to have the same behavior.  \n Every call to  forward  on an  AbstractRecurrent  instance like  Recursor  \nwill increment the  self.step  attribute by 1, using a shared parameter clone\nfor each successive time-step (for a maximum of  rho  time-steps, which defaults to 9999999).\nIn this way,  backward  can be called in reverse order of the  forward  calls \nto perform backpropagation through time (BPTT). Which is exactly what \n AbstractSequencer  instances do internally.\nThe  backward  call, which is actually divided into calls to  updateGradInput  and \n accGradParameters , decrements by 1 the  self.udpateGradInputStep  and  self.accGradParametersStep \nrespectively, starting at  self.step .\nSuccessive calls to  backward  will decrement these counters and use them to \nbackpropagate through the appropriate internall step-wise shared-parameter clones. \n Anyway, in most cases, you will not have to deal with the  Recursor  object directly as\n AbstractSequencer  instances automatically decorate non- AbstractRecurrent  instances\nwith a  Recursor  in their constructors. \n For a concrete example of its use, please consult the  simple-recurrent-network.lua \ntraining script for an example of its use. \n \n Recurrence \n A extremely general container for implementing pretty much any type of recurrence. \n lua\nrnn = nn.Recurrence(recurrentModule, outputSize, nInputDim, [rho]) \n Unlike  Recurrent , this module doesn't manage a separate \nmodules like  inputModule ,  startModule ,  mergeModule  and the like.\nInstead, it only manages a single  recurrentModule , which should \noutput a Tensor or table :  output(t)  \ngiven an input table :  {input(t), output(t-1)} .\nUsing a mix of  Recursor  (say, via  Sequencer ) with  Recurrence , one can implement \npretty much any type of recurrent neural network, including LSTMs and RNNs. \n For the first step, the  Recurrence  forwards a Tensor (or table thereof)\nof zeros through the recurrent layer (like LSTM, unlike Recurrent).\nSo it needs to know the  outputSize , which is either a number or \n torch.LongStorage , or table thereof. The batch dimension should be \nexcluded from the  outputSize . Instead, the size of the batch dimension \n(i.e. number of samples) will be extrapolated from the  input  using \nthe  nInputDim  argument. For example, say that our input is a Tensor of size \n 4 x 3  where  4  is the number of samples, then  nInputDim  should be  1 .\nAs another example, if our input is a table of table [...] of tensors \nwhere the first tensor (depth first) is the same as in the previous example,\nthen our  nInputDim  is also  1 . \n As an example, let's use  Sequencer  and  Recurrence  \nto build a Simple RNN for language modeling : \n ```lua\nrho = 5\nhiddenSize = 10\noutputSize = 5 -- num classes\nnIndex = 10000 \n -- recurrent module\nrm = nn.Sequential()\n   :add(nn.ParallelTable()\n      :add(nn.LookupTable(nIndex, hiddenSize))\n      :add(nn.Linear(hiddenSize, hiddenSize)))\n   :add(nn.CAddTable())\n   :add(nn.Sigmoid()) \n rnn = nn.Sequencer(\n   nn.Sequential()\n      :add(nn.Recurrence(rm, hiddenSize, 1))\n      :add(nn.Linear(hiddenSize, outputSize))\n      :add(nn.LogSoftMax())\n)\n``` \n Note : We could very well reimplement the  LSTM  module using the\nnewer  Recursor  and  Recurrent  modules, but that would mean \nbreaking backwards compatibility for existing models saved on disk. \n \n NormStabilizer \n Ref. A :  Regularizing RNNs by Stabilizing Activations \n This module implements the  norm-stabilization  criterion: \n lua\nns = nn.NormStabilizer([beta])   \n This module regularizes the hidden states of RNNs by minimizing the difference between the\nL2-norms of consecutive steps. The cost function is defined as :\n loss = beta * 1/T sum_t( ||h[t]|| - ||h[t-1]|| )^2  \nwhere  T  is the number of time-steps. Note that we do not divide the gradient by  T \nsuch that the chosen  beta  can scale to different sequence sizes without being changed. \n The sole argument  beta  is defined in ref. A. Since we don't divide the gradients by\nthe number of time-steps, the default value of  beta=1  should be valid for most cases.  \n This module should be added between RNNs (or LSTMs or GRUs) to provide better regularization of the hidden states. \nFor example :\n lua\nlocal stepmodule = nn.Sequential()\n   :add(nn.FastLSTM(10,10))\n   :add(nn.NormStabilizer())\n   :add(nn.FastLSTM(10,10))\n   :add(nn.NormStabilizer())\nlocal rnn = nn.Sequencer(stepmodule)   \n To use it with  SeqLSTM  you can do something like this :\n lua\nlocal rnn = nn.Sequential()\n   :add(nn.SeqLSTM(10,10))\n   :add(nn.Sequencer(nn.NormStabilizer()))\n   :add(nn.SeqLSTM(10,10))\n   :add(nn.Sequencer(nn.NormStabilizer()))   \n \n AbstractSequencer \n This abstract class implements a light interface shared by \nsubclasses like :  Sequencer ,  Repeater ,  RecurrentAttention ,  BiSequencer  and so on. \n \n Sequencer \n The  nn.Sequencer(module)  constructor takes a single argument,  module , which is the module \nto be applied from left to right, on each element of the input sequence. \n lua\nseq = nn.Sequencer(module) \n This Module is a kind of  decorator  \nused to abstract away the intricacies of  AbstractRecurrent  modules. While an  AbstractRecurrent  instance \nrequires that a sequence to be presented one input at a time, each with its own call to  forward  (and  backward ),\nthe  Sequencer  forwards an  input  sequence (a table) into an  output  sequence (a table of the same length).\nIt also takes care of calling  forget  on AbstractRecurrent instances. \n Input/Output Format \n The  Sequencer  requires inputs and outputs to be of shape  seqlen x batchsize x featsize  : \n \n seqlen  is the number of time-steps that will be fed into the  Sequencer . \n batchsize  is the number of examples in the batch. Each example is its own independent sequence. \n featsize  is the size of the remaining non-batch dimensions. So this could be  1  for language models, or  c x h x w  for convolutional models, etc. \n \n \n Above is an example input sequence for a character level language model.\nIt has  seqlen  is 5 which means that it contains sequences of 5 time-steps. \nThe openning  {  and closing  }  illustrate that the time-steps are elements of a Lua table, although \nit also accepts full Tensors of shape  seqlen x batchsize x featsize . \nThe  batchsize  is 2 as their are two independent sequences :  { H, E, L, L, O }  and  { F, U, Z, Z, Y, } .\nThe  featsize  is 1 as their is only one feature dimension per character and each such character is of size 1.\nSo the input in this case is a table of  seqlen  time-steps where each time-step is represented by a  batchsize x featsize  Tensor. \n \n Above is another example of a sequence (input or output). \nIt has a  seqlen  of 4 time-steps. \nThe  batchsize  is again 2 which means there are two sequences.\nThe  featsize  is 3 as each time-step of each sequence has 3 variables.\nSo each time-step (element of the table) is represented again as a tensor\nof size  batchsize x featsize . \nNote that while in both examples the  featsize  encodes one dimension, \nit could encode more.  \n Example \n For example,  rnn  : an instance of nn.AbstractRecurrent, can forward an  input  sequence one forward at a time:\n lua\ninput = {torch.randn(3,4), torch.randn(3,4), torch.randn(3,4)}\nrnn:forward(input[1])\nrnn:forward(input[2])\nrnn:forward(input[3])   \n Equivalently, we can use a Sequencer to forward the entire  input  sequence at once: \n lua\nseq = nn.Sequencer(rnn)\nseq:forward(input)   \n We can also forward Tensors instead of Tables : \n lua\n-- seqlen x batchsize x featsize\ninput = torch.randn(3,3,4)\nseq:forward(input)   \n Details \n The  Sequencer  can also take non-recurrent Modules (i.e. non-AbstractRecurrent instances) and apply it to each \ninput to produce an output table of the same length. \nThis is especially useful for processing variable length sequences (tables). \n Internally, the  Sequencer  expects the decorated  module  to be an \n AbstractRecurrent  instance. When this is not the case, the  module  \nis automatically decorated with a  Recursor  module, which makes it \nconform to the  AbstractRecurrent  interface.  \n Note : this is due a recent update (27 Oct 2015), as before this \n AbstractRecurrent  and and non- AbstractRecurrent  instances needed to \nbe decorated by their own  Sequencer . The recent update, which introduced the \n Recursor  decorator, allows a single  Sequencer  to wrap any type of module, \n AbstractRecurrent , non- AbstractRecurrent  or a composite structure of both types.\nNevertheless, existing code shouldn't be affected by the change. \n For a concise example of its use, please consult the  simple-sequencer-network.lua \ntraining script. \n \n remember([mode]) \n When  mode='neither'  (the default behavior of the class), the Sequencer will additionally call  forget  before each call to  forward . \nWhen  mode='both'  (the default when calling this function), the Sequencer will never call  forget .\nIn which case, it is up to the user to call  forget  between independent sequences.\nThis behavior is only applicable to decorated AbstractRecurrent  modules .\nAccepted values for argument  mode  are as follows : \n \n 'eval' only affects evaluation (recommended for RNNs) \n 'train' only affects training \n 'neither' affects neither training nor evaluation (default behavior of the class) \n 'both' affects both training and evaluation (recommended for LSTMs) \n \n forget() \n Calls the decorated AbstractRecurrent module's  forget  method. \n \n SeqLSTM \n This module is a faster version of  nn.Sequencer(nn.FastLSTM(inputsize, outputsize))  : \n lua\nseqlstm = nn.SeqLSTM(inputsize, outputsize)   \n Each time-step is computed as follows (same as  FastLSTM ): \n lua\ni[t] = σ(W[x->i]x[t] + W[h->i]h[t−1] + b[1->i])                      (1)\nf[t] = σ(W[x->f]x[t] + W[h->f]h[t−1] + b[1->f])                      (2)\nz[t] = tanh(W[x->c]x[t] + W[h->c]h[t−1] + b[1->c])                   (3)\nc[t] = f[t]c[t−1] + i[t]z[t]                                         (4)\no[t] = σ(W[x->o]x[t] + W[h->o]h[t−1] + b[1->o])                      (5)\nh[t] = o[t]tanh(c[t])                                                (6)   \n A notable difference is that this module expects the  input  and  gradOutput  to \nbe tensors instead of tables. The default shape is  seqlen x batchsize x inputsize  for\nthe  input  and  seqlen x batchsize x outputsize  for the  output  : \n ```lua\ninput = torch.randn(seqlen, batchsize, inputsize)\ngradOutput = torch.randn(seqlen, batchsize, outputsize) \n output = seqlstm:forward(input)\ngradInput = seqlstm:backward(input, gradOutput)\n```  \n Note that if you prefer to transpose the first two dimension (i.e.  batchsize x seqlen  instead of the default  seqlen x batchsize )\nyou can set  seqlstm.batchfirst = true  following initialization. \n For variable length sequences, set  seqlstm.maskzero = true . \nThis is equivalent to calling  maskZero(1)  on a  FastLSTM  wrapped by a  Sequencer :\n lua\nfastlstm = nn.FastLSTM(inputsize, outputsize)\nfastlstm:maskZero(1)\nseqfastlstm = nn.Sequencer(fastlstm)   \n For  maskzero = true , input sequences are expected to be seperated by tensor of zeros for a time step. \n The  seqlstm:toFastLSTM()  method generates a  FastLSTM  instance initialized with the parameters \nof the  seqlstm  instance. Note however that the resulting parameters will not be shared (nor can they ever be). \n Like the  FastLSTM , the  SeqLSTM  does not use peephole connections between cell and gates (see  FastLSTM  for details). \n Like the  Sequencer , the  SeqLSTM  provides a  remember  method. \n Note that a  SeqLSTM  cannot replace  FastLSTM  in code that decorates it with a\n AbstractSequencer  or  Recursor  as this would be equivalent to  Sequencer(Sequencer(FastLSTM)) .\nYou have been warned. \n \n SeqLSTMP \n References:\n * A.  LSTM RNN Architectures for Large Scale Acoustic Modeling \n * B.  Exploring the Limits of Language Modeling \n lua\nlstmp = nn.SeqLSTMP(inputsize, hiddensize, outputsize)   \n The  SeqLSTMP  is a subclass of  SeqLSTM . \nIt differs in that after computing the hidden state  h[t]  (eq. 6), it is \nprojected onto  r[t]  using a simple linear transform (eq. 7). \nThe computation of the gates also uses the previous such projection  r[t-1]  (eq. 1, 2, 3, 5).\nThis differs from  SeqLSTM  which uses  h[t-1]  instead of  r[t-1] . \n The computation of a time-step outlined in  SeqLSTM  is replaced with the following:\n lua\ni[t] = σ(W[x->i]x[t] + W[r->i]r[t−1] + b[1->i])                      (1)\nf[t] = σ(W[x->f]x[t] + W[r->f]r[t−1] + b[1->f])                      (2)\nz[t] = tanh(W[x->c]x[t] + W[h->c]r[t−1] + b[1->c])                   (3)\nc[t] = f[t]c[t−1] + i[t]z[t]                                         (4)\no[t] = σ(W[x->o]x[t] + W[r->o]r[t−1] + b[1->o])                      (5)\nh[t] = o[t]tanh(c[t])                                                (6)\nr[t] = W[h->r]h[t]                                                   (7)   \n The algorithm is outlined in ref. A and benchmarked with state of the art results on the Google billion words dataset in ref. B.\n SeqLSTMP  can be used with an  hiddensize >> outputsize  such that the effective size of the memory cells  c[t]  \nand gates  i[t] ,  f[t]  and  o[t]  can be much larger than the actual input  x[t]  and output  r[t] .\nFor fixed  inputsize  and  outputsize , the  SeqLSTMP  will be able to remember much more information than the  SeqLSTM . \n \n SeqGRU \n This module is a faster version of  nn.Sequencer(nn.GRU(inputsize, outputsize))  : \n lua\nseqGRU = nn.SeqGRU(inputsize, outputsize)   \n Usage of SeqGRU differs from GRU in the same manner as SeqLSTM differs from LSTM. Therefore see  SeqLSTM  for more details. \n \n SeqBRNN \n lua\nbrnn = nn.SeqBRNN(inputSize, outputSize, [batchFirst], [merge])   \n A bi-directional RNN that uses SeqLSTM. Internally contains a 'fwd' and 'bwd' module of SeqLSTM. Expects an input shape of  seqlen x batchsize x inputsize .\nBy setting [batchFirst] to true, the input shape can be  batchsize x seqLen x inputsize . Merge module defaults to CAddTable(), summing the outputs from each\noutput layer. \n Example:\n input = torch.rand(1, 1, 5)\nbrnn = nn.SeqBRNN(5, 5)\nprint(brnn:forward(input))  \nPrints an output of a 1x1x5 tensor. \n \n BiSequencer \n Applies encapsulated  fwd  and  bwd  rnns to an input sequence in forward and reverse order.\nIt is used for implementing Bidirectional RNNs and LSTMs. \n lua\nbrnn = nn.BiSequencer(fwd, [bwd, merge]) \n The input to the module is a sequence (a table) of tensors\nand the output is a sequence (a table) of tensors of the same length.\nApplies a  fwd  rnn (an  AbstractRecurrent  instance) to each element in the sequence in\nforward order and applies the  bwd  rnn in reverse order (from last element to first element).\nThe  bwd  rnn defaults to: \n lua\nbwd = fwd:clone()\nbwd:reset() \n For each step (in the original sequence), the outputs of both rnns are merged together using\nthe  merge  module (defaults to  nn.JoinTable(1,1) ). \nIf  merge  is a number, it specifies the  JoinTable \nconstructor's  nInputDim  argument. Such that the  merge  module is then initialized as : \n lua\nmerge = nn.JoinTable(1,merge) \n Internally, the  BiSequencer  is implemented by decorating a structure of modules that makes \nuse of 3 Sequencers for the forward, backward and merge modules. \n Similarly to a  Sequencer , the sequences in a batch must have the same size.\nBut the sequence length of each batch can vary. \n Note : make sure you call  brnn:forget()  after each call to  updateParameters() . \nAlternatively, one could call  brnn.bwdSeq:forget()  so that only  bwd  rnn forgets.\nThis is the minimum requirement, as it would not make sense for the  bwd  rnn to remember future sequences. \n \n BiSequencerLM \n Applies encapsulated  fwd  and  bwd  rnns to an input sequence in forward and reverse order.\nIt is used for implementing Bidirectional RNNs and LSTMs for Language Models (LM). \n lua\nbrnn = nn.BiSequencerLM(fwd, [bwd, merge]) \n The input to the module is a sequence (a table) of tensors\nand the output is a sequence (a table) of tensors of the same length.\nApplies a  fwd  rnn (an  AbstractRecurrent  instance to the \nfirst  N-1  elements in the sequence in forward order.\nApplies the  bwd  rnn in reverse order to the last  N-1  elements (from second-to-last element to first element).\nThis is the main difference of this module with the  BiSequencer .\nThe latter cannot be used for language modeling because the  bwd  rnn would be trained to predict the input it had just be fed as input. \n \n The  bwd  rnn defaults to: \n lua\nbwd = fwd:clone()\nbwd:reset() \n While the  fwd  rnn will output representations for the last  N-1  steps,\nthe  bwd  rnn will output representations for the first  N-1  steps.\nThe missing outputs for each rnn ( the first step for the  fwd , the last step for the  bwd )\nwill be filled with zero Tensors of the same size the commensure rnn's outputs.\nThis way they can be merged. If  nn.JoinTable  is used (the default), then the first \nand last output elements will be padded with zeros for the missing  fwd  and  bwd  rnn outputs, respectively. \n For each step (in the original sequence), the outputs of both rnns are merged together using\nthe  merge  module (defaults to  nn.JoinTable(1,1) ). \nIf  merge  is a number, it specifies the  JoinTable \nconstructor's  nInputDim  argument. Such that the  merge  module is then initialized as : \n lua\nmerge = nn.JoinTable(1,merge) \n Similarly to a  Sequencer , the sequences in a batch must have the same size.\nBut the sequence length of each batch can vary. \n Note that LMs implemented with this module will not be classical LMs as they won't measure the \nprobability of a word given the previous words. Instead, they measure the probabiliy of a word\ngiven the surrounding words, i.e. context. While for mathematical reasons you may not be able to use this to measure the \nprobability of a sequence of words (like a sentence), \nyou can still measure the pseudo-likeliness of such a sequence (see  this  for a discussion). \n \n Repeater \n This Module is a  decorator  similar to  Sequencer .\nIt differs in that the sequence length is fixed before hand and the input is repeatedly forwarded \nthrough the wrapped  module  to produce an output table of length  nStep :\n lua\nr = nn.Repeater(module, nStep) \nArgument  module  should be an  AbstractRecurrent  instance.\nThis is useful for implementing models like  RCNNs ,\nwhich are repeatedly presented with the same input. \n \n RecurrentAttention \n References : \n \n A.  Recurrent Models of Visual Attention \n B.  Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning \n \n This module can be used to implement the Recurrent Attention Model (RAM) presented in Ref. A :\n lua\nram = nn.RecurrentAttention(rnn, action, nStep, hiddenSize) \n rnn  is an  AbstractRecurrent  instance. \nIts input is  {x, z}  where  x  is the input to the  ram  and  z  is an \naction sampled from the  action  module. \nThe output size of the  rnn  must be equal to  hiddenSize . \n action  is a  Module  \nthat uses a  REINFORCE module  (ref. B) like \n ReinforceNormal , \n ReinforceCategorical , or \n ReinforceBernoulli  \nto sample actions given the previous time-step's output of the  rnn . \nDuring the first time-step, the  action  module is fed with a Tensor of zeros of size  input:size(1) x hiddenSize .\nIt is important to understand that the sampled actions do not receive gradients \nbackpropagated from the training criterion. \nInstead, a reward is broadcast from a Reward Criterion like  VRClassReward  Criterion to \nthe  action 's REINFORCE module, which will backprogate graidents computed from the  output  samples \nand the  reward . \nTherefore, the  action  module's outputs are only used internally, within the RecurrentAttention module. \n nStep  is the number of actions to sample, i.e. the number of elements in the  output  table. \n hiddenSize  is the output size of the  rnn . This variable is necessary \nto generate the zero Tensor to sample an action for the first step (see above). \n A complete implementation of Ref. A is available  here . \n \n MaskZero \n This module zeroes the  output  rows of the decorated module \nfor commensurate  input  rows which are tensors of zeros. \n lua\nmz = nn.MaskZero(module, nInputDim) \n The  output  Tensor (or table thereof) of the decorated  module \nwill have each row (samples) zeroed when the commensurate row of the  input  \nis a tensor of zeros.  \n The  nInputDim  argument must specify the number of non-batch dims \nin the first Tensor of the  input . In the case of an  input  table,\nthe first Tensor is the first one encountered when doing a depth-first search. \n This decorator makes it possible to pad sequences with different lengths in the same batch with zero vectors. \n Caveat:  MaskZero  not guarantee that the  output  and  gradInput  tensors of the internal modules \nof the decorated  module  will be zeroed as well when the  input  is zero as well. \n MaskZero  only affects the immediate  gradInput  and  output  of the module that it encapsulates.\nHowever, for most modules, the gradient update for that time-step will be zero because \nbackpropagating a gradient of zeros will typically yield zeros all the way to the input.\nIn this respect, modules to avoid in encapsulating inside a  MaskZero  are  AbsractRecurrent  \ninstances as the flow of gradients between different time-steps internally. \nInstead, call the  AbstractRecurrent.maskZero  method\nto encapsulate the internal  recurrentModule . \n \n TrimZero \n WARNING : only use this module if your input contains lots of zeros. \nIn almost all cases,  MaskZero  will be faster, especially with CUDA. \n Ref. A :  TrimZero: A Torch Recurrent Module for Efficient Natural Language Processing \n The usage is the same with  MaskZero . \n lua\nmz = nn.TrimZero(module, nInputDim) \n The only difference from  MaskZero  is that it reduces computational costs by varying a batch size, if any, for the case that varying lengths are provided in the input. \nNotice that when the lengths are consistent,  MaskZero  will be faster, because  TrimZero  has an operational cost.  \n In short, the result is the same with  MaskZero 's, however,  TrimZero  is faster than  MaskZero  only when sentence lengths is costly vary. \n In practice, e.g. language model,  TrimZero  is expected to be faster than  MaskZero  about 30%. (You can test with it using  test/test_trimzero.lua .) \n \n LookupTableMaskZero \n This module extends  nn.LookupTable  to support zero indexes. Zero indexes are forwarded as zero tensors. \n lua\nlt = nn.LookupTableMaskZero(nIndex, nOutput) \n The  output  Tensor will have each row zeroed when the commensurate row of the  input  is a zero index.  \n This lookup table makes it possible to pad sequences with different lengths in the same batch with zero vectors. \n \n MaskZeroCriterion \n This criterion zeroes the  err  and  gradInput  rows of the decorated criterion \nfor commensurate  input  rows which are tensors of zeros. \n lua\nmzc = nn.MaskZeroCriterion(criterion, nInputDim) \n The  gradInput  Tensor (or table thereof) of the decorated  criterion \nwill have each row (samples) zeroed when the commensurate row of the  input  \nis a tensor of zeros. The  err  will also disregard such zero rows. \n The  nInputDim  argument must specify the number of non-batch dims \nin the first Tensor of the  input . In the case of an  input  table,\nthe first Tensor is the first one encountered when doing a depth-first search. \n This decorator makes it possible to pad sequences with different lengths in the same batch with zero vectors. \n \n SeqReverseSequence \n lua\nreverseSeq = nn.SeqReverseSequence(dim) \n Reverses an input tensor on a specified dimension. The reversal dimension can be no larger than three. \n Example:\n```\ninput = torch.Tensor({{1,2,3,4,5}, {6,7,8,9,10}})\nreverseSeq = nn.SeqReverseSequence(1)\nprint(reverseSeq:forward(input)) \n Gives us an output of torch.Tensor({{6,7,8,9,10},{1,2,3,4,5}})\n``` \n \n SequencerCriterion \n This Criterion is a  decorator : \n lua\nc = nn.SequencerCriterion(criterion, [sizeAverage])   \n Both the  input  and  target  are expected to be a sequence, either as a table or Tensor. \nFor each step in the sequence, the corresponding elements of the input and target \nwill be applied to the  criterion .\nThe output of  forward  is the sum of all individual losses in the sequence. \nThis is useful when used in conjunction with a  Sequencer . \n If  sizeAverage  is  true  (default is  false ), the  output  loss and  gradInput  is averaged over each time-step. \n \n RepeaterCriterion \n This Criterion is a  decorator : \n lua\nc = nn.RepeaterCriterion(criterion)   \n The  input  is expected to be a sequence (table or Tensor). A single  target  is \nrepeatedly applied using the same  criterion  to each element in the  input  sequence.\nThe output of  forward  is the sum of all individual losses in the sequence.\nThis is useful for implementing models like  RCNNs ,\nwhich are repeatedly presented with the same target.""]",['repository_count'],Expedia Group,https://avatars.githubusercontent.com/u/3520613?u=afd249ee477fe1c60fcb0f063f58300aa20435ef&v=4,False,2016-10-19 14:28:14,837,285,3,2013-02-09T18:54:43Z,2021-06-11T19:24:54Z,,False,False,False,False,False,True,False,0,"{'Lua': 13, 'Python': 6, 'CMake': 1, 'C++': 2, 'Cuda': 3, 'C': 1}",8,"Montreal, Canada",Data Mining,2,0.82,0.2,0.94,0
0mp,Mateusz Piotrowski,,0mp@FreeBSD.org,"[""tflclck \n TOEFL Exam Timer App \n Incredibly important note \n This is my first project I've ever put on GitHub. I keep it for nostalgic reasons. My busy-waiting implementation of a timer always makes me smile.  \n Cheers!"", 'Algorithms written in the Shakespeare Programming Language (SPL) \n This is a repo where I keep my little programs I\'ve done in the SPL. \n Bubble sort \n How to use it? \n Write 5 numbers that you would like to sort. Do not type any spaces between them!\nThe program will print the sorted array of numbers. It will neither print a newline character\nnor spaces between the numbers. A number must consist of one digit only. \n Example: \n In: \n 57490 \n Out: \n 04579 \n Stack sort \n How to use it? \n You can enter as much of nonnegative numbers as you want.\nSeparate the numbers using a newline character. The program will stop getting the\nnumbers from the input stream when the number ""-1"" is entered. \n Example: \n In: \n 420\n404\n23\n14\n290595\n1\n-1 \n Out: \n 1\n14\n23\n404\n420\n290595 \n How to compile it? \n The Shakespeare Programming Language is usually translated\nto C and then compiled. The official translator can be downloaded\nfrom  the official webpage . \n If you have any difficulties with making the translator run I do recommend\nto search thourgh the  Shakespeare Programming Language Tag  on StackOverflow. \n Long live Shakespeare!', 'SafariFerrari \n SafariFerrari is a Safari extension which allows you to navigate webpages without using a mouse.  \n Available commands \n Here are available commands which you can use in this extension ( use it without quotations marks ): \n Page navigation \n \n ""h""  - scroll left \n ""j""  - scroll up \n ""k""  - scroll down \n ""l""  - scroll right \n ""u""  - scroll up (longer step) \n ""d""  - scroll bottom (longer step) \n ""gg""  - scroll to the top of the page \n ""G""  - scroll to the bottom of the page \n \n Tab navigation \n \n ""x""  - close current tab \n ""X""  - reopen closed tab (you can use it more than just once) \n ""f""  - show links tooltips (after that you can open them in next tab by typing their ""ID"") \n ""r""  - reload tab \n ""J""  - go to the left tab \n ""K""  - go to the right tab \n "">>""  - move the current tab to the right \n ""<<""  - move the current tab to the left \n ""H""  - go back in history of the current tab \n ""F""  -  go forward in history of the current tab \n \n Miscellaneous \n \n ""q""  - surprise command \n \n Credits \n Our inspiration was an extension for Chrome called Vimium. \n History \n The extension was created during a Local Hack Day 2015 in Warsaw.', 'n0tes \n This work is licensed under a  Creative Commons Attribution 4.0 International License .', 'dotfiles \n FreeBSD \n ```sh \n dwm \n mkdir -p ${HOME}/h\ngit clone http://github.com/0mp/dwm ${HOME}/h/dwm\nmake -C ${HOME}/h/dwm clean dwm install \n makaron \n mkdir -p ${HOME}\ngit clone --recursive https://github.com/0mp/makaron ${HOME}/h/makaron\nmake -C ${HOME}/h/makaron install \n goat \n mkdir -p ${HOME}/h\ngit clone http://github.com/0mp/goat ${HOME}/h/goat\nmake -C ${HOME}/h/goat clean install\n``` \n mDNS \n sh\npkg install avahi-app nss_mdns\nsysrc avahi_daemon_enable=""YES""\nsysrc dbus_enable=""YES""\nif ! grep -q ""hosts.*:.*mdns"" /etc/nsswitch.conf; then\n    tmp=""$(mktemp)"" && \\\n    sed \'/hosts/s/$/ mdns/\' /etc/nsswitch.conf > ""$tmp"" && \\\n    cat ""$tmp"" > /etc/nsswitch.conf\n    rm ""$tmp""\nfi \n Finally, add  mdns  to the  hosts  line in  /etc/nsswitch.conf . \n Give less memory to ARC \n sysctl vfs.zfs.arc_max=$(expr -- $(sysctl -n hw.physmem) / 2) \n lagg(4) configuration \n sysrc ifconfig_em0=""up""\nsysrc wlans_iwm0=""wlan0""\nsysrc ifconfig_wlan0=""WPA powersave""\nsysrc create_args_wlan0=""wlanaddr \\$(ifconfig em0 ether | awk \'/ether/{print \\$2}\') country de""\nsysrc cloned_interfaces=""lagg0""\nsysrc ifconfig_lagg0=""up laggproto failover laggport em0 laggport wlan0 DHCP"" \n Firefox \n Microphone support: set  media.cubeb.backend  to  oss  in about:config. (https://forums.freebsd.org/threads/how-to-use-microphone-with-firefox.74292/#post-485968) \n Git \n Sample  ~/.gitconfig.local \n [includeIf ""gitdir:~/rust/""]\n    path = ~/.gitconfig-rust \n Specifying how to reconcile divergent branches \n git config pull.rebase false  # merge (the default strategy)\ngit config pull.rebase true   # rebase                      \ngit config pull.ff only       # fast-forward only', 'goat \n Overview \n \n I used to retrieve carefully-constructed  cd  commands from my history. \nBut then, I got a goat. \n ~  Jonathan Paugh  on  Google+ \n \n console\n[ ~/Pictures ] $ goat dev ~/Documents/devel # create a link to the dev directory\n[ ~/Pictures ] $ ls # see that there is no ~/Pictures/dev directory here\nseahorses wallpapers\n[ ~/Pictures ] $ cd dev # the goat framework\'s got you covered!\n[ ~/Documents/devel ] $ \n \n Oh my! This is a  POSIX-compliant  shell  movement boosting   hack  for\n real ninjas . \n posix_me_harder  # posixly_correct \n ~ 0mp \n \n Sometimes you jump around your filesystem tree a lot and you end up putting a\ncouple of ugly aliases into your shell\'s rc file. \n \n I should try it, even if it is dumb! \n ~  dse  on  What the Daily WTF?  about goat v1.1.1 \n \n With goat you can easily manage your ninja shortcuts - just type  goat p\n~/Projects  to introduce a new link and then  cd p  to jump to its destination. \n \n Rad! I can do  cd ....  now instead of performing a horse galloping-like\nwaltz with  ../  being my miserable dance floor. I\'m cloning this goat\nstraight away! \n ~ YA0mp \n \n BTW, Bash completion is now fully working with goat\'s shortcuts. \n Installation \n console\n$ make install \n Aferwards: \n \n Make sure that  ~/.local/bin  is in your  PATH : \n \n console\n  $ cat <<\'EOF\' >> ~/.bashrc\n  case ""$PATH"" in\n      *$HOME/.local/bin*) ;;\n      *) PATH=""$HOME/.local/bin:$PATH"" ;;\n  esac\n  EOF \n \n Make sure that files inside  ~/.local/etc/bash_completion.d  are actually\n  sourced by the Bash completion library: \n \n console\n  $ cat <<\'EOF\' >> ~/.bash_completion\n  if [[ -d ~/.bash_completion.d ]]\n  then\n      for f in ~/.local/etc/bash_completion.d/*\n      do\n          [[ -f $f ]] && source ""$f""\n      done\n  fi\n  EOF \n Usage overview \n ```console\nCreate a shortcut named “f” to ~/Documents/dev/freebsd (no need to use\nthe link command explicitly here): \n   $ goat f ~/Documents/dev/freebsd\n \n Follow a link to change a directory with cd(1): \n   $ cd f\n \n Take the “f” shortcut and enter its destination subdirectory with just\none command: \n   $ pwd\n  /home/0mp\n  $ cd f/ports\n  $ pwd\n  /usr/home/0mp/freebsd/ports\n \n Create a shortcut named “p” to the current directory: \n   $ goat p .\n \n Go up the filesystem tree with ... (same as the standard “cd ../../”): \n   $ cd ...\n \n List all your links: \n   $ goat list\n  dots    ->      /usr/home/0mp/.dotfiles\n  down    ->      /usr/home/0mp/Downloads\n  f       ->      /usr/home/0mp/freebsd\n  p       ->      /usr/home/0mp/freebsd/ports\n  pa      ->      /usr/home/0mp/freebsd/patches\n  src     ->      /usr/home/0mp/freebsd/svn/src\n  svn     ->      /usr/home/0mp/freebsd/svn\n \n Delete a link (or more): \n   $ goat delete f p\n \n Delete all the links which point to directories with the given prefix: \n   $ goat deleteprefix ""$HOME/Documents""\n \n ``` \n License \n Licensed under 2-Clause BSD license. Copyright © 2016-2021 Mateusz Piotrowski', 'io-touchpad \n \n \n \n \n Application \n Installation \n apt-get update\napt-get install build-essential libatlas-dev libatlas3gf-base\napt-get install python3-dev python3-setuptools python3-numpy python3-scipy python3-pip\npip3 install scikit-learn\nmake\n \n Quick start \n \n \n Go to the  app  directory. \n cd app\n \n \n \n Undertake a learning session with a trainig size of  5  for a symbol named  your-secret-symbol  which will create a  yss.txt  file in the  /tmp  directory.: \n sudo ./app.py add 5 your-secret-symbol touch /tmp/yss.txt\n \n \n \n Use the app. (Hopefully, it will recognise your symbol.) \n sudo ./app run user\n \n \n \n Run  ./app --help  if you want to learn other features of this app. \n Tests \n Installation \n apt-get update\napt-get install python3-pytest\n \n Usage \n cd app/test\npy.test-3\n \n Tools \n matrixanalyser.py \n Installation \n apt-get install python3-matplotlib\ncd app/tools\nmake\n \n Usage \n cd app\nsudo ./tools/matrixanalyser.py [--help] [--tolerance TOLERANCE] [--show]\n \n More information on the usage is avaialabe if you run  ./tools/matrixanalyser.py --help . \n All generated figures of the drawn symbols are stored inside\nthe  app/tools/data/matrixanalyser/figures  directory.', 'pocketchip', 'audisp-auditdistd \n A plugin for the Linux audit event dispatcher audispd to push audit trail logs over to a FreeBSD auditdistd daemon. \n Background \n This project is a continuation of a Google Summer of Code 2016 project for FreeBSD.\nThe original and final goal of the project is to allow a FreeBSD user to collect and process audit trails from different systems like Linux and Windows.\nAt the moment, the goal is to add an audispd plugin capable of communicating with FreeBSD auditdistd. \n Dependencies \n sh\napt install libssl-dev \n See also \n \n Non-BSM to BSM Conversion Tools project for Google Summer of Code 2016 at FreeBSD: https://wiki.freebsd.org/SummerOfCode2016/NonBSMtoBSMConversionTools \n', 'vim-robpike \n This is a minimal Vim color scheme working fine with both white and black\nterminal backgrounds. \n Build \n sh\n./build > ~/.vim/colors/robpike.vim \n Inspiration \n \n When I was a child, I used to speak like a child, think like a child,\nreason like a child; when I became a man, I did away with childish\nthings. \n Syntax highlighting is juvenile. When I was a child, I was taught\narithmetic using  colored rods . I grew up and today I\nuse monochromatic numerals. \n \n Context: https://groups.google.com/d/topic/golang-nuts/hJHCAaiL0so/discussion', '80211 \n Also \n \n https://wiki.freebsd.org/MateuszPiotrowski/Drivers \n', 'audisp-auditdistd \n Pushing audit logs from Linux over to FreeBSD using auditdistd daemons. \n Usage \n First set up \n sh\n./generate-auditdistd-conf\n./do-vagrant-up\n./do-provision\nvagrant provision linux-sender --provision-with rebuild-openbsm \n Run auditdistds \n sh\nvagrant provision freebsd-receiver --provision-with run\nvagrant provision linux-sender --provision-with run \n Detatils \n There are 3 machines: \n \n freebsd-receiver \n freebsd-sender \n linux-sender \n \n The goal is to make linux-sender work flawlessly with freebsd-receiver. \n freebsd-sender is here for debugging purposes. In order to start the freebsd-sender machine you have to run: \n sh\n./do-vagrant-up --full \n Every machine has its own OpenBSM branch. \n Dependencies \n \n rsync \n vagrant \n', 'Karta wzorów z metod numerycznych \n Instalacja pakietu LaTeX \n FreeBSD 12.0-CURRENT \n bash\npkg install texlive-full \n macOS \n Zainstaluj  pdflatex  z dystrybucji  mactex : \n bash\nbrew install brew-cask\nbrew cask install mactex \n Ubuntu 16.10 \n bash\nsudo apt-get install texlive-full \n Generowanie plików PDF \n bash\nmake \n Wygenerowany plik pojawi się w katalogu  build . \n \n Ważne \n \n https://www.freebsd.org/ \n https://joinmastodon.org/ \n', 'Metody Numeryczne MIMUW 2017 \n W skrypcie jest dużo błędów, bo do tej pory nie było żadnej kontroli jakości. Chętnie przyjmę pull requesty :-) \n Wersja live: https://www.sharelatex.com/project/59d606877ef26a08a8cf3c58 \n todo: \n \n poprawić niektóre macierze \n rozdzielić na mniejsze pliki \n poprawić błędy latexowe \n \n Karta wzorów \n https://github.com/0mp/mn-karta-wzorow', 'NAME \n sct  - screen temperature control \n SYNOPSIS \n sct \n[ temperature ] \n DESCRIPTION \n This utility can be used to change the screen temperature. \n A valid\n temperature \nvalue is between 1000 and 10000.\nIf the\n temperature \nargument is not provided or its value is invalid\nthen the screen temperature is set to the default value of 6500. \n EXAMPLES \n Campfire: \n sct 4500\n \n Dust storm on Mars: \n sct 2000\n \n Coffee-free all nighter: \n sct 8000\n \n DIAGNOSTICS \n Failed to get screen resources \nUsually those messages mean that\n sct \ncannot connect to the\nX(7)\nserver. \n Failed to open X display %s \nSee\n Failed to get screen resources . \n SEE ALSO \n redshift(1),\nxrandr(1) \n The original blog post about\n sct \nby Ted Unangst:\n https://www.tedunangst.com/flak/post/sct-set-color-temperature \n HISTORY \n The\n sct \nutility was written by\nTed Unangst < tedu@OpenBSD.org >. \n AUTHORS \n This\nmanual page was written by\nMateusz Piotrowski < 0mp@FreeBSD.org >.', 'Notatki ze wstępu do analizy sieci społecznych', 'FreeBSD Ports workshop infrastructure \n See  Wiki  for a cheat sheet and other helpful materials.', ""Bash completion for Poudriere \n To do \n \n FLAVORS  support \n grep -E -n 'TODO|XXX' ./poudriere-bash-completion \n Parse  poudriere status  in order to complete buildnames. \n \n See also \n \n Poudriere's upstream (github.com) \n"", ""coffeebreak \n This utility is a a fake FreeBSD installer written by Devin Teske. \n \n It's friday.  Do you need to pretend to be working?  Do you have FreeBSD\nsystems?  Here you go!  coffeebreak is the tool you always wanted to make\nyour machine as if it is very busy doing important stuff. \n \n Give me a break! \n coffeebreak(6) is  a part of the FreeBSD ports collection : \n pkg install coffeebreak \n Otherwise it could be installed manually with make(1): \n make install"", 'Bash completion for FreeBSD \n \n Covered software \n Bash completion functions are available for the following utilities \n \n make(1)  ( work in progress ) \n mandoc(1) \n pkg(8)  ( work in progress ) \n service(8) \n \n Installation \n Those additional Bash completion files are installed by default with the  bash-completion port : \n ```console \n pkg instal bash-completion \n ``` \n If you\'re willing to install to install this extension manually then set  DESTDIR  and  PREFIX  to match your configuration and run: \n ```console \n make install \n ``` \n Dependencies \n \n bash-completion \n \n Caveats \n Although the  pkg upstream  ships some Bash completion for pkg(8) it does not conflict with our implementation of the pkg(8) completion. The reason is that the upstream completion file is installed into  /usr/local/etc/bash_completion.d/  while our completion is installed into  /usr/local/share/bash-completion/completions/ , which is processed first according to  the bash-completion README . \n License \n BSD 2-Clause ""Simplified"" License', 'NAME \n kbfsd  - service daemon for the Keybase filesystem (KBFS) \n SYNOPSIS \n Required\nrc.conf(5)\nvariables: \n kbfsd_enable=""YES""\nkbfsd_user=""beastie""\n \n Optional\nrc.conf(5)\nvariables: \n kbfsd_keybase_username=""${kbfsd_user}""\nkbfsd_mountpoint=""/keybase""\n \n DESCRIPTION \n kbfsd \nis an\nrc(8)\ndaemon for the Keybase filesystem (KBFS).\nIts aim is to ease the process of using KBFS on\nFreeBSD.\nIt takes care of the configuration the Keybase user would have to do manually\notherwise. \n kbfsd \ndoes not start automatically even when\n kbfsd_enable \nis set to\n\' YES \'\nin\nrc.conf(5).\nSee the\n CAVEATS \nsection for more details. \n kbfsd \nhas to configure some bits of the system in order to mount KBFS as\n kbfsd_user .\n kbfsd_mountpoint \nis created and\nthe\nsysctl(8)\ntunable\n vfs.usermount \nis set to\n""1""\nso that\n kbfsd_user \ncould mount\n kbfsd_mountpoint .\nThen\n kbfsd_user \nis added to the\n""operator""\ngroup to be able to use the\n /dev/fuse \ndevice.\nFinally,\n kbfsd \nattempts to spin off the Keybase server and create required socket files.\nNote that this step requires\n kbfsd_user \nto be able to log in as\n kbfsd_keybase_username .\nThis should be possible once\n kbfsd_user \nregisters a device with\n\' keybase device add \'. \n kbfsd \nmay be controlled with the following\nrc.conf(5)\nvariables: \n kbfsd_enable \n (*bool*, default: \'`NO`\')\nEnable\n**kbfsd**.\n \n kbfsd_keybase_username \n (*str*, default: *kbfsd_user*)\nThe username used to log into Keybase.\n \n kbfsd_mountpoint \n (*str*, default: \'`/keybase`\')\nThe directory where KBFS should be mounted.\n \n kbfsd_user \n (*str*, no defaults)\nThe login name of a user, who should own\n*kbfsd_mountpoint*.\nIt cannot be empty.\n \n INSTALLATION \n The easiest way is to just install the\n kbfsd \npackage via\npkg(8)\non\nFreeBSD: \n pkg install kbfsd\n \n kbfsd \ncan be installed manually with the following command: \n make all\nmake install\n \n FILES \n /home/ ${ kbfsd_user } /.config/keybase/kbfsd. ${ kbfsd_user } .pid \n PID file.\n \n EXIT STATUS \n The\n kbfsd \ndaemon\nexits 0 on success, and >0 if an error occurs. \n SEE ALSO \n rc.conf(5),\nmount(8),\nrc(8) \n AUTHORS \n The\n kbfsd \ndaemon and its manual page were written by\nMateusz Piotrowski < 0mp@FreeBSD.org >. \n CAVEATS \n kbfsd \nis\n not \nstarted automatically together with other daemons during boot because it uses\nthe\n\' nostart \'\nKEYWORD\n(see rc(8) for details).\nThe reason is that in order to reliably mount KBFS the user has to establish\na session with the Keybase server first.\nThis is done by calling:\n\' keybase login username \'\n(where\n username \nis the same as\n kbfsd_keybase_username )\n.\nUnfortunately, this command happens to block the booting process from time to\ntime, which is unacceptable. \n BUGS \n kbfsd \nseems to kill\n kbfsfuse \ntoo rapidly for\n kbfsfuse \nto properly unmount.\nAs a workaround,\n kbfsd \ncalls\numount(8)\non the mount point in the\n poststop \nphase\n(see rc.subr(8)). \n Currently,\n kbfsd \nuses\n kbfsd_env \ninternally to set the\n HOME \nenvironmental variable to the home directory of\n kbfsd_user .\nIt is recommended to read the service file before setting\n kbfsd_env \nin\nrc.conf(5). \n On some\nFreeBSD\nversions older than 13.0 the name of\nfusefs(5)\nmight be\n""fuse""\ninstead of\n""fusefs"".\nAs a result,\n kbfsd \nmight fail to start.\nA potential workaronud is to set\n required_modules \nto an empty string in\n /usr/local/etc/rc.conf.d/kbfsd \nand then loading the FUSE kernel module differently\n(e.g., via\n kld_list \nin\nrc.conf(5))\n.', 'NAME \n moinmoincli  - edit existing MoinMoin wiki pages from a command line \n SYNOPSIS \n moinmoincli \n[ -hquv ]\n[ -f \xa0 textfile ]\n[ -n \xa0 username ]\n[ -t \xa0 target ]\n[ selector ] \n DESCRIPTION \n moinmoincli \nis an interactive utility to edit existing MoinMoin wiki pages from\na command line.\n moinmoincli \nwas only tested with the\nFreeBSD\nWiki\nwhich is using the MoinMoin engine.\nThe\nFreeBSD\nWiki address is also hard-coded into\n moinmoincli \nas the default web service the utility works with. \n moinmoincli \ncannot create new wiki pages. \n When\n moinmoincli \nis run in order to update a wiki page, a diff is presented to the\nuser to confirm the changes.\nIt is done by providing a comment describing the changes.\nNext, a summary and the action menu is displayed.\nThe summary includes the username, the target, the comment, and the\ntext file name.\nActions that could be entered at the action menu prompt are\ndescribed in the\n ACTION MENU \nsubsection.\nOnce the user confirms requested changes to be processed, the password prompt\nis presented to the user. \n Command-line options \n The options are as follows: \n -f   textfile ,  --file   textfile \n \n Point to the\nAr textfile\nthat is going to be sent to the server. \n \n -h ,  --help \n \n Show help. \n \n -n   username ,  --name   username \n \n Set the username to log in as. \n \n -q ,  --quick \n \n Activate the quick mode, which disables the action menu.\nAs a result the changes will be uploaded once a comment is entered\nby the user. \n \n -t   target ,  --target   target \n \n Provide the name of the wiki page to be updated, e.g.:\n"" /WiFi/80211ac "".\nA hyperlink is acceptable as well, e.g.:\n"" https://wiki.freebsd.org/WiFi/80211ac "". \n \n -u ,  --update \n \n Download the contents of the\n target \nwiki page and put them into the \n \n -v ,  --version \n \n Show version. \n \n selector \n \n Run\n moinmoincli \nas preconfigured for a given\n selector . \n Selectors are used if the\n select_target ()\nfunction is defined in the\nconfiguration file (defaults are listed in the\n FILES \nsection).\nA single\n selector \nis associated with a\n target \nand a\n textfile .\nAs a result it is possible to use selectors to replace\n"" moinmoincli -f ./80211ac.moin -t /WiFi/80211ac ""\nwith\n"" moinmoincli ac ""\nusing an\n ac \nselector. \n \n Action menu \n The following actions could be entered at the action menu prompt one\naction at a time: \n y ,  k \n \n Confirm changes.\nThe action menu will be closed and the changes uploaded to the server. \n \n n ,  a \n \n Abort.\nClose\n moinmoincli \nimmediately. \n \n t \n \n Mark changes as trivial so that subscribers to the modified wiki\npage are not notified. \n \n c \n \n Change the comment. \n \n d \n \n Show the diff. \n \n s \n \n Print the summary. \n \n FILES \n ~/.moinmoincli.conf \n \n The\n moinmoincli \nconfiguration file. \n \n /tmp/moinmoincli-workdir \n \n The location of temporary files. \n \n EXAMPLES \n Example 1: Downloading Wiki Pages \n Download the code of\n https://wiki.freebsd.org/Community/Dogs \nand save it under\n ./dogs.moinmoin : \n $ moinmoincli --file ./dogs.moinmoin --target /Community/Dogs --update\n \n Example 2: Uploading Wiki Pages \n Write\n ./sandbox.txt \nto\n https://wiki.freebsd.org/WikiSandBox \nas\n CharlieRoot : \n $ moinmoincli -n CharlieRoot -f ./sandbox.txt -t /WikiSandBox\n \n Example 3: Suggested Workflow \n \n \n Configure\n     ~/.moinmoincli.conf \n    so that it is not necessary to specify the username, the text file name and the\n    target website every time.\n    Use selectors for that. \n \n \n Grab the latest version of the wiki page with a selector: \n $ moinmoincli -u selectorYouHaveConfigured\n \n \n \n Edit the text file. \n \n \n Push the changes over to the wiki server with: \n $ moinmoincli selectorYouHaveConfigured\n \n \n \n Configuration File \n # The username to use when logging in.\nname=\'CharlieRoot\'\n\n# The password for the account.\npassword=\'secret\'\n\n# The default target.\n# It is used unless a selector or the -t flag is used.\ntarget=\'/WikiSandBox\'\n\n# The default textfile.\ntextfile=\'/tmp/wiki.moin\'\n\n# The select_target function, which defines the logic behind\n# selectors.\nselect_target() {\n    case ""$1"" in\n        [wW]*)\n            target=\'/WiFi\'\n            textfile=""$HOME/wifi.moin""\n            ;;\n        *)\n            printf \'%s\\n\' ""invalid selector \'$1\'"" >&2\n            ;;\n    esac\n}\n \n AUTHORS \n moinmoincli \nand its manual page was written by\nMateusz Piotrowski < 0mp@FreeBSD.org >.', ""Prometheus Documentation \n This repository contains both the content and the static-site generator code for the\nPrometheus documentation site. \n Contributing Changes \n See  CONTRIBUTING.md  for general instructions for new Prometheus contributors. \n The main documentation contents of this website are located in the  content/docs  directory. \n Documentation concerning the Prometheus server is  maintained in the Prometheus server repository  and cloned into the website at build time. \n As a guideline, please keep the documentation generally applicable and avoid use-case-specific changes. \n Prerequisites \n You need to have a working Ruby environment set up (including  bundler )\nand then install the necessary gems: \n bash\ncd docs\nmake bundle \n Building \n To generate the static site, run: \n bash\nmake build \n The resulting static site will be stored in the  output  directory. \n Optionally, you can use an API token to avoid rate limits on the API. You can get an API token from https://github.com/settings/tokens/new.\n bash\nexport GITHUB_AUTHENTICATION='-u user:token' \n Development Server \n To run a local server that displays the generated site, run: \n ```bash \n Rebuild the site whenever relevant files change: \n make guard \n Start the local development server in a separate shell: \n make serve\n``` \n You should now be able to view the generated site at\n http://localhost:3000/ . \n License \n Apache License 2.0, see  LICENSE ."", 'NAME \n vipe.sh  - portable, shell-compatible version of\nvipe(1)\nfrom moreutils \n SYNOPSIS \n vipe.sh \n|\n ... \n DESCRIPTION \n vipe.sh \nis a utility, which makes it possible to use an editor\nin pipelines of Bourne-compatible shells. \n It aims to address pipeline limitations, which result in warnings like:\n""Vim: Warning: Output is not to a terminal"". \n IMPLEMENTATION NOTES \n The\n vipe.sh \ncommand makes use of\nmktemp(1)\nto create temporary files. \n ENVIRONMENT \n The following environmental variables are recognized by\n vipe.sh : \n EDITOR  (default: ""vi"") \n \n The editor to use if\n VISUAL \nis not set. \n \n VISUAL  (default: unset) \n \n The editor to use. \n \n EXIT STATUS \n The  vipe.sh  utility exits\xa00 on success, and\xa0>0 if an error occurs. \n EXAMPLES \n The following is an example of a typical usage\nof the\n vipe.sh \ncommand: \n curl wttr.in | vipe.sh | nc termbin.com 9999 | xclip -sel clip\n \n SEE ALSO \n vipe(1),\nwhich is a part of\n moreutils . \n HISTORY \n vipe.sh \nis a reimplementation of the popular, Perl-based\nvipe(1)\nfrom the moreutils project. \n AUTHORS \n vipe.sh \nand this manual page was written by\nMateusz Piotrowski < 0mp@FreeBSD.org >.', 'The wonderful en_DK.UTF-8 locale \n Based on  the work done by Ivan Voras .', 'freebsd-ports-nix \n \n Nix package manager port for the FreeBSD Ports Collection. \n Installation \n The port has already been committed to the FreeBSD Ports Collection (see  sysutils/nix on FreshPorts ). \n The binary package can be installed with  pkg install nix . \n Community \n Join us on https://libera.chat, channel  #freebsd-nix . \n References \n Blog posts mentioning Nix on FreeBSD \n \n http://sandervanderburg.blogspot.com/2020/02/a-declarative-process-manager-agnostic.html \n \n Interesting issues, pull requests, and other related efforts \n \n https://github.com/NixOS/nixpkgs/pull/81459 \n WIP port of Nix to pkgsrc: https://wip.pkgsrc.org/cgi-bin/gitweb.cgi?p=pkgsrc-wip.git;a=tree;f=nix;hb=HEAD \n https://github.com/NixOS/nixpkgs/pull/82131 \n', 'Makaron \n', 'The Great Debugging of the  signal-cli  Daemon D-Bus Integration \n signal-cli  is a command-line client for the  Signal messaging service . \n It has been recently ported to FreeBSD ( net-im/signal-cli ). Most of its features work as expected except its integration with  D-Bus , which is necessary for UI wrappers for signal-cli like  scli . \n Expected behavior \n \n \n The service starts without any issues: \n service signal_cli start \n \n \n The user can send messages via the signal-cli daemon: \n signal-cli --dbus-system send -m ""Message"" +00123123123 \n \n \n The user is able to start scli without any issues. \n \n \n Enviroment setup \n \n Install the port. \n Link (or register) signal-cli to your Signal account. Do it as the  signal-user  from  /var/lib/signal-cli . \n \n Current challanges \n org.freedesktop.dbus.exceptions.DBusException: Failed to connect to bus Failed to auth \n console\n$ export DISPLAY=0 \n$ export JAVA_OPTS=""-Djava.library.path=/usr/local/lib""\n$ signal-cli -u +00123456789 daemon\norg.freedesktop.dbus.exceptions.DBusException: Failed to connect to bus Failed to auth\n        at org.freedesktop.dbus.DBusConnection.<init>(DBusConnection.java:304)\n        at org.freedesktop.dbus.DBusConnection.getConnection(DBusConnection.java:282)\n        at org.asamk.signal.commands.DaemonCommand.handleCommand(DaemonCommand.java:50)\n        at org.asamk.signal.Main.handleCommands(Main.java:126)\n        at org.asamk.signal.Main.main(Main.java:61) \n In  net-im/signal-cli/work/signal-cli-0.6.5/src/main/java/org/asamk/signal/commands/DaemonCommand.java  (around line 50): \n java\n        DBusConnection conn = null;\n        try {\n            try {\n                int busType;\n                if (ns.getBoolean(""system"")) {\n                    busType = DBusConnection.SYSTEM;\n                } else {\n                    busType = DBusConnection.SESSION;\n                }\n                conn = DBusConnection.getConnection(busType);\n                conn.exportObject(SIGNAL_OBJECTPATH, m);\n                conn.requestBusName(SIGNAL_BUSNAME);\n            } catch (UnsatisfiedLinkError e) {\n                System.err.println(""Missing native library dependency for dbus service: "" + e.getMessage());\n                return 1;\n            } catch (DBusException e) {\n                e.printStackTrace();\n                return 2;\n            } \n In  devel/dbus-java/work/dbus-java-2.7/org/freedesktop/dbus/DBusConnection.java  (around line 288): \n ```java\n   @SuppressWarnings(""unchecked"")\n   private DBusConnection(String address) throws DBusException\n   {\n      super(address);\n      busnames = new Vector (); \n   synchronized (_reflock) {\n     _refcount = 1; \n  }\n\n  try {\n     transport = new Transport(addr, AbstractConnection.TIMEOUT);\n        connected = true;\n  } catch (IOException IOe) {\n     if (EXCEPTION_DEBUG && Debug.debug) Debug.print(Debug.ERR, IOe);            \n     disconnect();\n     throw new DBusException(_(""Failed to connect to bus "")+IOe.getMessage());\n  } catch (ParseException Pe) {\n     if (EXCEPTION_DEBUG && Debug.debug) Debug.print(Debug.ERR, Pe);            \n     disconnect();\n     throw new DBusException(_(""Failed to connect to bus "")+Pe.getMessage());\n  }\n\n  // start listening for calls\n  listen();\n\n  // register disconnect handlers\n  DBusSigHandler h = new _sighandler();\n  addSigHandlerWithoutMatch(org.freedesktop.DBus.Local.Disconnected.class, h);\n  addSigHandlerWithoutMatch(org.freedesktop.DBus.NameAcquired.class, h);\n\n  // register ourselves\n  _dbus = getRemoteObject(""org.freedesktop.DBus"", ""/org/freedesktop/DBus"", DBus.class);\n  try {\n     busnames.add(_dbus.Hello());\n  } catch (DBusExecutionException DBEe) {\n     if (EXCEPTION_DEBUG && Debug.debug) Debug.print(Debug.ERR, DBEe);\n     throw new DBusException(DBEe.getMessage());\n  }\n \n }\n``` \n Let\'s run signal-cli with some more debug support. \n First, some preparation: \n \n Build a debug version of  devel/dbus-java  (e.g.,  cd ports/devel/dbus-java && make WITH_DEBUG=yes clean reinstall ). \n \n Modify the class path in  signal-cli  script. Replace the following JAR files: \n \n /usr/local/share/signal-cli/lib/debug-1.1.1.jar \n `/usr/local/share/signal-cli/lib/dbus-java-2.7.0.jar \n \n with: \n \n /usr/local/share/java/classes/dbus-2.7.jar \n /usr/local/share/java/classes/debug-enable.jar \n \n \n \n Add the following code to the  signal-cli  script to gain additional debugging infomation: \n DBUS_JAVA_EXCEPTION_DEBUG=yes \n See  /usr/local/share/doc/dbus-java/INSTALL  for more details. \n \n \n Create a  debug.conf  file in the directory from which you start  signal-cli : \n console\n$ echo ALL = ALL > debug.conf \n See  /usr/local/share/doc/dbus-java/INSTALL  for more details. \n \n \n Now, start  signal-cli : \n ```console \n chroot -u signal-cli / env PS1=\'\\w$ \' /bin/sh \n /$ cd /var/lib/signal-cli\n/var/lib/signal-cli$ signal-cli -u +00123456789 --config $PWD daemon --system\n[org.freedesktop.dbus.AbstractConnection. ()] Debugging of internal exceptions enabled\n[org.freedesktop.dbus.AbstractConnection. ()] Loading debug config file: debug.conf\n[org.freedesktop.dbus.DBusConnection.getConnection()] Getting bus connection for unix:path=/var/run/dbus/system_bus_socket: null\n[org.freedesktop.dbus.DBusConnection.getConnection()] Creating new bus connection to: unix:path=/var/run/dbus/system_bus_socket\n[org.freedesktop.dbus.MethodTuple. ()] new MethodTuple(Ping, )\n[org.freedesktop.dbus.Marshalling.recursiveGetDBusType()] Converted Java type: class java.lang.String to D-Bus Type: s\n[org.freedesktop.dbus.MethodTuple. ()] new MethodTuple(Introspect, )\n[org.freedesktop.dbus.EfficientQueue.shrink()] Shrinking\n[org.freedesktop.dbus.BusAddress. ()] Parsing bus address: unix:path=/var/run/dbus/system_bus_socket\n[org.freedesktop.dbus.BusAddress. ()] Transport type: unix\n[org.freedesktop.dbus.BusAddress. ()] Transport options: {path=/var/run/dbus/system_bus_socket}\n[org.freedesktop.dbus.Transport.connect()] Connecting to unix: {path=/var/run/dbus/system_bus_socket}\n[org.freedesktop.dbus.Transport$SASL.auth()] AUTH state: 0\n[org.freedesktop.dbus.Transport$SASL.send()] sending: AUTH \n [org.freedesktop.dbus.Transport$SASL.auth()] AUTH state: 1\n[org.freedesktop.dbus.Transport$SASL.receive()] received: REJECTED EXTERNAL\n[org.freedesktop.dbus.Transport$SASL$Command. ()] Creating command from: [REJECTED, EXTERNAL]\n[org.freedesktop.dbus.Transport$SASL$Command. ()] Created command: Command(3, 1, null, null)\n[org.freedesktop.dbus.Transport$SASL.send()] sending: AUTH EXTERNAL 323438 \n [org.freedesktop.dbus.Transport$SASL.auth()] AUTH state: 1\n[org.freedesktop.dbus.Transport$SASL.receive()] received: REJECTED EXTERNAL\n[org.freedesktop.dbus.Transport$SASL$Command. ()] Creating command from: [REJECTED, EXTERNAL]\n[org.freedesktop.dbus.Transport$SASL$Command. ()] Created command: Command(3, 1, null, null)\n[org.freedesktop.dbus.DBusConnection. ()] java.io.IOException: Failed to auth\n    at org.freedesktop.dbus.Transport.connect(Unknown Source)\n    at org.freedesktop.dbus.Transport. (Unknown Source)\n    at org.freedesktop.dbus.DBusConnection. (Unknown Source)\n    at org.freedesktop.dbus.DBusConnection.getConnection(Unknown Source)\n    at org.asamk.signal.commands.DaemonCommand.handleCommand(DaemonCommand.java:50)\n    at org.asamk.signal.Main.handleCommands(Main.java:126)\n    at org.asamk.signal.Main.main(Main.java:61)\n[org.freedesktop.dbus.DBusConnection.disconnect()] Disconnecting DBusConnection\n[org.freedesktop.dbus.Message. ()] Creating message with serial 1\n[org.freedesktop.dbus.Message.append()] Appending sig: yyyy data: [66, 3, 0, 1]\n[org.freedesktop.dbus.Message.append()] Appending item: 0 y 0\n[org.freedesktop.dbus.Message.appendone()] 4\n[org.freedesktop.dbus.Message.appendone()] Appending type: y value: 66\n[org.freedesktop.dbus.Message.pad()] padding for y\n[org.freedesktop.dbus.Message.pad()] 4 0 4 1\n[org.freedesktop.dbus.Message.append()] Appending item: 1 y 1\n[org.freedesktop.dbus.Message.appendone()] 4\n[org.freedesktop.dbus.Message.appendone()] Appending type: y value: 3\n[org.freedesktop.dbus.Message.pad()] padding for y\n[org.freedesktop.dbus.Message.pad()] 3 1 4 1\n[org.freedesktop.dbus.Message.append()] Appending item: 2 y 2\n[org.freedesktop.dbus.Message.appendone()] 4\n[org.freedesktop.dbus.Message.appendone()] Appending type: y value: 0\n[org.freedesktop.dbus.Message.pad()] padding for y\n[org.freedesktop.dbus.Message.pad()] 2 2 4 1\n[org.freedesktop.dbus.Message.append()] Appending item: 3 y 3\n[org.freedesktop.dbus.Message.appendone()] 4\n[org.freedesktop.dbus.Message.appendone()] Appending type: y value: 1\n[org.freedesktop.dbus.Message.pad()] padding for y\n[org.freedesktop.dbus.Message.pad()] 1 3 4 1\n[org.freedesktop.dbus.Message.append()] Appending sig: ua(yv) data: [1, [[4, [s, org.freedesktop.DBus.Local.Disconnected]], [5, [u, 0]], [6, [s, org.freedesktop.DBus.Local]], [8, [g, s]]]]\n[org.freedesktop.dbus.Message.append()] Appending item: 0 u 0\n[org.freedesktop.dbus.Message.appendone()] 8\n[org.freedesktop.dbus.Message.appendone()] Appending type: u value: 1\n[org.freedesktop.dbus.Message.pad()] padding for u\n[org.freedesktop.dbus.Message.pad()] 0 4 8 4\n[org.freedesktop.dbus.Message.marshallint()] Marshalled int 1 to 00 00 00 01 \n[org.freedesktop.dbus.Message.append()] Appending item: 1 a 1\n[org.freedesktop.dbus.Message.appendone()] 12\n[org.freedesktop.dbus.Message.appendone()] Appending type: a value: [Ljava.lang.Object;@5d0a1059\n[org.freedesktop.dbus.Message.pad()] padding for a\n[org.freedesktop.dbus.Message.pad()] 0 4 12 4\n[org.freedesktop.dbus.Message.appendone()] Appending array: [[4, [s, org.freedesktop.DBus.Local.Disconnected]], [5, [u, 0]], [6, [s, org.freedesktop.DBus.Local]], [8, [g, s]]]\n[org.freedesktop.dbus.Message.pad()] padding for (\n[org.freedesktop.dbus.Message.pad()] 0 4 16 8\n[org.freedesktop.dbus.Message.appendone()] 16\n[org.freedesktop.dbus.Message.appendone()] Appending type: ( value: [Ljava.lang.Object;@485966cc\n[org.freedesktop.dbus.Message.pad()] padding for (\n[org.freedesktop.dbus.Message.pad()] 0 4 16 8\n[org.freedesktop.dbus.Message.appendone()] 16\n[org.freedesktop.dbus.Message.appendone()] Appending type: y value: 4\n[org.freedesktop.dbus.Message.pad()] padding for y\n[org.freedesktop.dbus.Message.pad()] 0 4 16 1\n[org.freedesktop.dbus.Message.appendone()] 17\n[org.freedesktop.dbus.Message.appendone()] Appending type: v value: [Ljava.lang.Object;@1de76cc7\n[org.freedesktop.dbus.Message.pad()] padding for v\n[org.freedesktop.dbus.Message.pad()] 0 4 17 1\n[org.freedesktop.dbus.Message.appendone()] 17\n[org.freedesktop.dbus.Message.appendone()] Appending type: g value: s\n[org.freedesktop.dbus.Message.pad()] padding for g\n[org.freedesktop.dbus.Message.pad()] 0 4 17 1\n[org.freedesktop.dbus.Message.appendone()] 20\n[org.freedesktop.dbus.Message.appendone()] Appending type: s value: org.freedesktop.DBus.Local.Disconnected\n[org.freedesktop.dbus.Message.pad()] padding for s\n[org.freedesktop.dbus.Message.pad()] 0 3 20 4\n[org.freedesktop.dbus.Message.appendone()] Appending String of length 39\n[org.freedesktop.dbus.Message.marshallint()] Marshalled int 39 to 00 00 00 27 \n[org.freedesktop.dbus.Message.appendone()] 64\n[org.freedesktop.dbus.Message.appendone()] Appending type: ( value: [Ljava.lang.Object;@54bff557\n[org.freedesktop.dbus.Message.pad()] padding for (\n[org.freedesktop.dbus.Message.pad()] 0 3 64 8\n[org.freedesktop.dbus.Message.appendone()] 64\n[org.freedesktop.dbus.Message.appendone()] Appending type: y value: 5\n[org.freedesktop.dbus.Message.pad()] padding for y\n[org.freedesktop.dbus.Message.pad()] 0 3 64 1\n[org.freedesktop.dbus.Message.appendone()] 65\n[org.freedesktop.dbus.Message.appendone()] Appending type: v value: [Ljava.lang.Object;@593aaf41\n[org.freedesktop.dbus.Message.pad()] padding for v\n[org.freedesktop.dbus.Message.pad()] 0 3 65 1\n[org.freedesktop.dbus.Message.appendone()] 65\n[org.freedesktop.dbus.Message.appendone()] Appending type: g value: u\n[org.freedesktop.dbus.Message.pad()] padding for g\n[org.freedesktop.dbus.Message.pad()] 0 3 65 1\n[org.freedesktop.dbus.Message.appendone()] 68\n[org.freedesktop.dbus.Message.appendone()] Appending type: u value: 0\n[org.freedesktop.dbus.Message.pad()] padding for u\n[org.freedesktop.dbus.Message.pad()] 0 3 68 4\n[org.freedesktop.dbus.Message.marshallint()] Marshalled int 0 to 00 00 00 00 \n[org.freedesktop.dbus.Message.appendone()] 72\n[org.freedesktop.dbus.Message.appendone()] Appending type: ( value: [Ljava.lang.Object;@5a56cdac\n[org.freedesktop.dbus.Message.pad()] padding for (\n[org.freedesktop.dbus.Message.pad()] 0 3 72 8\n[org.freedesktop.dbus.Message.appendone()] 72\n[org.freedesktop.dbus.Message.appendone()] Appending type: y value: 6\n[org.freedesktop.dbus.Message.pad()] padding for y\n[org.freedesktop.dbus.Message.pad()] 0 3 72 1\n[org.freedesktop.dbus.Message.appendone()] 73\n[org.freedesktop.dbus.Message.appendone()] Appending type: v value: [Ljava.lang.Object;@7c711375\n[org.freedesktop.dbus.Message.pad()] padding for v\n[org.freedesktop.dbus.Message.pad()] 0 3 73 1\n[org.freedesktop.dbus.Message.appendone()] 73\n[org.freedesktop.dbus.Message.appendone()] Appending type: g value: s\n[org.freedesktop.dbus.Message.pad()] padding for g\n[org.freedesktop.dbus.Message.pad()] 0 3 73 1\n[org.freedesktop.dbus.Message.appendone()] 76\n[org.freedesktop.dbus.Message.appendone()] Appending type: s value: org.freedesktop.DBus.Local\n[org.freedesktop.dbus.Message.pad()] padding for s\n[org.freedesktop.dbus.Message.pad()] 0 3 76 4\n[org.freedesktop.dbus.Message.appendone()] Appending String of length 26\n[org.freedesktop.dbus.Message.marshallint()] Marshalled int 26 to 00 00 00 1a \n[org.freedesktop.dbus.Message.appendone()] 107\n[org.freedesktop.dbus.Message.appendone()] Appending type: ( value: [Ljava.lang.Object;@57cf54e1\n[org.freedesktop.dbus.Message.pad()] padding for (\n[org.freedesktop.dbus.Message.pad()] 0 3 107 8\n[org.freedesktop.dbus.Message.pad()] 0 3 112 5\n[org.freedesktop.dbus.Message.ensureBuffers()] Resizing 18\n[org.freedesktop.dbus.Message.appendone()] 112\n[org.freedesktop.dbus.Message.appendone()] Appending type: y value: 8\n[org.freedesktop.dbus.Message.pad()] padding for y\n[org.freedesktop.dbus.Message.pad()] 0 3 112 1\n[org.freedesktop.dbus.Message.appendone()] 113\n[org.freedesktop.dbus.Message.appendone()] Appending type: v value: [Ljava.lang.Object;@5b03b9fe\n[org.freedesktop.dbus.Message.pad()] padding for v\n[org.freedesktop.dbus.Message.pad()] 0 3 113 1\n[org.freedesktop.dbus.Message.appendone()] 113\n[org.freedesktop.dbus.Message.appendone()] Appending type: g value: g\n[org.freedesktop.dbus.Message.pad()] padding for g\n[org.freedesktop.dbus.Message.pad()] 0 3 113 1\n[org.freedesktop.dbus.Message.appendone()] 116\n[org.freedesktop.dbus.Message.appendone()] Appending type: g value: s\n[org.freedesktop.dbus.Message.pad()] padding for g\n[org.freedesktop.dbus.Message.pad()] 0 3 116 1\n[org.freedesktop.dbus.Message.appendone()] start: 16 end: 119 length: 103\n[org.freedesktop.dbus.Message.marshallint()] Marshalled int 103 to 00 00 00 67 \n[org.freedesktop.dbus.Message.pad()] padding for \n[org.freedesktop.dbus.Message.pad()] 0 3 119 8\n[org.freedesktop.dbus.Message.pad()] 0 3 120 1\n[org.freedesktop.dbus.Message.append()] Appending sig: s data: [Disconnected]\n[org.freedesktop.dbus.Message.append()] Appending item: 0 s 0\n[org.freedesktop.dbus.Message.appendone()] 120\n[org.freedesktop.dbus.Message.appendone()] Appending type: s value: Disconnected\n[org.freedesktop.dbus.Message.pad()] padding for s\n[org.freedesktop.dbus.Message.pad()] 0 3 120 4\n[org.freedesktop.dbus.Message.appendone()] Appending String of length 12\n[org.freedesktop.dbus.Message.marshallint()] Marshalled int 12 to 00 00 00 0c \n[org.freedesktop.dbus.Message.marshallint()] Marshalled int 17 to 00 00 00 11 \n[org.freedesktop.dbus.AbstractConnection.disconnect()] Sending disconnected signal\n[org.freedesktop.dbus.Message. ()] Creating message with serial 2\n[org.freedesktop.dbus.Message.append()] Appending sig: yyyy data: [66, 4, 0, 1]\n[org.freedesktop.dbus.Message.append()] Appending item: 0 y 0\n[org.freedesktop.dbus.Message.appendone()] 4\n[org.freedesktop.dbus.Message.appendone()] Appending type: y value: 66\n[org.freedesktop.dbus.Message.pad()] padding for y\n[org.freedesktop.dbus.Message.pad()] 4 0 4 1\n[org.freedesktop.dbus.Message.append()] Appending item: 1 y 1\n[org.freedesktop.dbus.Message.appendone()] 4\n[org.freedesktop.dbus.Message.appendone()] Appending type: y value: 4\n[org.freedesktop.dbus.Message.pad()] padding for y\n[org.freedesktop.dbus.Message.pad()] 3 1 4 1\n[org.freedesktop.dbus.Message.append()] Appending item: 2 y 2\n[org.freedesktop.dbus.Message.appendone()] 4\n[org.freedesktop.dbus.Message.appendone()] Appending type: y value: 0\n[org.freedesktop.dbus.Message.pad()] padding for y\n[org.freedesktop.dbus.Message.pad()] 2 2 4 1\n[org.freedesktop.dbus.Message.append()] Appending item: 3 y 3\n[org.freedesktop.dbus.Message.appendone()] 4\n[org.freedesktop.dbus.Message.appendone()] Appending type: y value: 1\n[org.freedesktop.dbus.Message.pad()] padding for y\n[org.freedesktop.dbus.Message.pad()] 1 3 4 1\n[org.freedesktop.dbus.Message.append()] Appending sig: ua(yv) data: [3, [[1, [o, /]], [2, [s, org.freedesktop.DBus.Local]], [3, [s, Disconnected]]]]\n[org.freedesktop.dbus.Message.append()] Appending item: 0 u 0\n[org.freedesktop.dbus.Message.appendone()] 8\n[org.freedesktop.dbus.Message.appendone()] Appending type: u value: 3\n[org.freedesktop.dbus.Message.pad()] padding for u\n[org.freedesktop.dbus.Message.pad()] 0 4 8 4\n[org.freedesktop.dbus.Message.marshallint()] Marshalled int 3 to 00 00 00 03 \n[org.freedesktop.dbus.Message.append()] Appending item: 1 a 1\n[org.freedesktop.dbus.Message.appendone()] 12\n[org.freedesktop.dbus.Message.appendone()] Appending type: a value: [Ljava.lang.Object;@3c7f66c4\n[org.freedesktop.dbus.Message.pad()] padding for a\n[org.freedesktop.dbus.Message.pad()] 0 4 12 4\n[org.freedesktop.dbus.Message.appendone()] Appending array: [[1, [o, /]], [2, [s, org.freedesktop.DBus.Local]], [3, [s, Disconnected]]]\n[org.freedesktop.dbus.Message.pad()] padding for (\n[org.freedesktop.dbus.Message.pad()] 0 4 16 8\n[org.freedesktop.dbus.Message.appendone()] 16\n[org.freedesktop.dbus.Message.appendone()] Appending type: ( value: [Ljava.lang.Object;@194bcebf\n[org.freedesktop.dbus.Message.pad()] padding for (\n[org.freedesktop.dbus.Message.pad()] 0 4 16 8\n[org.freedesktop.dbus.Message.appendone()] 16\n[org.freedesktop.dbus.Message.appendone()] Appending type: y value: 1\n[org.freedesktop.dbus.Message.pad()] padding for y\n[org.freedesktop.dbus.Message.pad()] 0 4 16 1\n[org.freedesktop.dbus.Message.appendone()] 17\n[org.freedesktop.dbus.Message.appendone()] Appending type: v value: [Ljava.lang.Object;@17497425\n[org.freedesktop.dbus.Message.pad()] padding for v\n[org.freedesktop.dbus.Message.pad()] 0 4 17 1\n[org.freedesktop.dbus.Message.appendone()] 17\n[org.freedesktop.dbus.Message.appendone()] Appending type: g value: o\n[org.freedesktop.dbus.Message.pad()] padding for g\n[org.freedesktop.dbus.Message.pad()] 0 4 17 1\n[org.freedesktop.dbus.Message.appendone()] 20\n[org.freedesktop.dbus.Message.appendone()] Appending type: o value: /\n[org.freedesktop.dbus.Message.pad()] padding for o\n[org.freedesktop.dbus.Message.pad()] 0 3 20 4\n[org.freedesktop.dbus.Message.appendone()] Appending String of length 1\n[org.freedesktop.dbus.Message.marshallint()] Marshalled int 1 to 00 00 00 01 \n[org.freedesktop.dbus.Message.appendone()] 26\n[org.freedesktop.dbus.Message.appendone()] Appending type: ( value: [Ljava.lang.Object;@f0da945\n[org.freedesktop.dbus.Message.pad()] padding for (\n[org.freedesktop.dbus.Message.pad()] 0 3 26 8\n[org.freedesktop.dbus.Message.pad()] 0 3 32 6\n[org.freedesktop.dbus.Message.appendone()] 32\n[org.freedesktop.dbus.Message.appendone()] Appending type: y value: 2\n[org.freedesktop.dbus.Message.pad()] padding for y\n[org.freedesktop.dbus.Message.pad()] 0 3 32 1\n[org.freedesktop.dbus.Message.appendone()] 33\n[org.freedesktop.dbus.Message.appendone()] Appending type: v value: [Ljava.lang.Object;@4803b726\n[org.freedesktop.dbus.Message.pad()] padding for v\n[org.freedesktop.dbus.Message.pad()] 0 3 33 1\n[org.freedesktop.dbus.Message.appendone()] 33\n[org.freedesktop.dbus.Message.appendone()] Appending type: g value: s\n[org.freedesktop.dbus.Message.pad()] padding for g\n[org.freedesktop.dbus.Message.pad()] 0 3 33 1\n[org.freedesktop.dbus.Message.appendone()] 36\n[org.freedesktop.dbus.Message.appendone()] Appending type: s value: org.freedesktop.DBus.Local\n[org.freedesktop.dbus.Message.pad()] padding for s\n[org.freedesktop.dbus.Message.pad()] 0 3 36 4\n[org.freedesktop.dbus.Message.appendone()] Appending String of length 26\n[org.freedesktop.dbus.Message.marshallint()] Marshalled int 26 to 00 00 00 1a \n[org.freedesktop.dbus.Message.appendone()] 67\n[org.freedesktop.dbus.Message.appendone()] Appending type: ( value: [Ljava.lang.Object;@ffaa6af\n[org.freedesktop.dbus.Message.pad()] padding for (\n[org.freedesktop.dbus.Message.pad()] 0 3 67 8\n[org.freedesktop.dbus.Message.pad()] 0 3 72 5\n[org.freedesktop.dbus.Message.ensureBuffers()] Resizing 16\n[org.freedesktop.dbus.Message.appendone()] 72\n[org.freedesktop.dbus.Message.appendone()] Appending type: y value: 3\n[org.freedesktop.dbus.Message.pad()] padding for y\n[org.freedesktop.dbus.Message.pad()] 0 3 72 1\n[org.freedesktop.dbus.Message.appendone()] 73\n[org.freedesktop.dbus.Message.appendone()] Appending type: v value: [Ljava.lang.Object;@53ce1329\n[org.freedesktop.dbus.Message.pad()] padding for v\n[org.freedesktop.dbus.Message.pad()] 0 3 73 1\n[org.freedesktop.dbus.Message.appendone()] 73\n[org.freedesktop.dbus.Message.appendone()] Appending type: g value: s\n[org.freedesktop.dbus.Message.pad()] padding for g\n[org.freedesktop.dbus.Message.pad()] 0 3 73 1\n[org.freedesktop.dbus.Message.appendone()] 76\n[org.freedesktop.dbus.Message.appendone()] Appending type: s value: Disconnected\n[org.freedesktop.dbus.Message.pad()] padding for s\n[org.freedesktop.dbus.Message.pad()] 0 3 76 4\n[org.freedesktop.dbus.Message.appendone()] Appending String of length 12\n[org.freedesktop.dbus.Message.marshallint()] Marshalled int 12 to 00 00 00 0c \n[org.freedesktop.dbus.Message.appendone()] start: 16 end: 93 length: 77\n[org.freedesktop.dbus.Message.marshallint()] Marshalled int 77 to 00 00 00 4d \n[org.freedesktop.dbus.Message.pad()] padding for \n[org.freedesktop.dbus.Message.pad()] 0 3 93 8\n[org.freedesktop.dbus.Message.pad()] 0 3 96 3\n[org.freedesktop.dbus.AbstractConnection.handleMessage()] Handling incoming signal: Disconnected(0,3) { Path=>/, Interface=>org.freedesktop.DBus.Local, Member=>Disconnected } { }\n[org.freedesktop.dbus.AbstractConnection.disconnect()] Disconnecting Abstract Connection\norg.freedesktop.dbus.exceptions.DBusException: Failed to connect to bus Failed to auth\n    at org.freedesktop.dbus.DBusConnection. (Unknown Source)\n    at org.freedesktop.dbus.DBusConnection.getConnection(Unknown Source)\n    at org.asamk.signal.commands.DaemonCommand.handleCommand(DaemonCommand.java:50)\n    at org.asamk.signal.Main.handleCommands(Main.java:126)\n    at org.asamk.signal.Main.main(Main.java:61)\n``` \n Troubleshooting Java Exceptions \n org.freedesktop.dbus.exceptions.DBusException: Cannot Resolve Session Bus Address \n console\n$ signal-cli -u +00123456789 daemon\norg.freedesktop.dbus.exceptions.DBusException: Cannot Resolve Session Bus Address\n        at org.freedesktop.dbus.DBusConnection.getConnection(DBusConnection.java:267)\n        at org.asamk.signal.commands.DaemonCommand.handleCommand(DaemonCommand.java:50)\n        at org.asamk.signal.Main.handleCommands(Main.java:126)\n        at org.asamk.signal.Main.main(Main.java:61) \n Solution \n The DISPLAY environment variable has to omit the leading "":"", e.g., ""0"" instead of "":0"". Otherwise, signal-cli tries to open an invalid D-BUS session bus file, e.g.,  ~/.dbus/session-bus/0123456789abcdef0123456789abcdef-:0  instead of  ~/.dbus/session-bus/0123456789abcdef0123456789abcdef-0 . \n org.freedesktop.dbus.exceptions.DBusException: Failed to connect to bus unknown address type \'unix \n console\n$ DISPLAY=0 signal-cli -u +00123456789 daemon\norg.freedesktop.dbus.exceptions.DBusException: Failed to connect to bus unknown address type \'unix\n        at org.freedesktop.dbus.DBusConnection.<init>(DBusConnection.java:304)\n        at org.freedesktop.dbus.DBusConnection.getConnection(DBusConnection.java:282)\n        at org.asamk.signal.commands.DaemonCommand.handleCommand(DaemonCommand.java:50)\n        at org.asamk.signal.Main.handleCommands(Main.java:126)\n        at org.asamk.signal.Main.main(Main.java:61) \n Solution \n Unquote the value of the  DBUS_SESSION_BUS_ADDRESS  variable in  ~/.dbus/session-bus/0123456789abcdef0123456789abcdef-0 . For example: \n DBUS_SESSION_BUS_ADDRESS=\'unix:path=/tmp/dbus-ABCDEFGHIJ,guid=fedcba9876543210fedcba9876543210\' \n should be \n DBUS_SESSION_BUS_ADDRESS=unix:path=/tmp/dbus-ABCDEFGHIJ,guid=fedcba9876543210fedcba9876543210 \n Missing native library dependency for dbus service: no unix-java in java.library.path \n console\n$ export DISPLAY=0 \n$ signal-cli -u +00123456789 daemon\nMissing native library dependency for dbus service: no unix-java in java.library.path \n signal-cli  cannot find the share library provided by libmatthew. A proper path can be set via  JAVA_OPTS : \n sh\nexport JAVA_OPTS=""-Djava.library.path=/usr/local/lib"" \n General notes \n Use the following command to list all available message buses: \n ``` \n Source: https://unix.stackexchange.com/a/46309 \n dbus-send --print-reply --dest=org.freedesktop.DBus /org/freedesktop/DBus org.freedesktop.DBus.ListNames\n``` \n Send a Signal message via the signal-cli D-Bus daemon: \n dbus-send --session --type=method_call --print-reply --dest=""org.asamk.Signal"" /org/asamk/Signal org.asamk.Signal.sendMessage string:MessageText array:string: string:RECIPIENT \\`\\`\\` \n qdbus  (from  devel/qt5-qdbus ) can be used to view avaiable interfaces to D-Bus. \n Resources \n \n https://github.com/AsamK/signal-cli/wiki/DBus-service \n https://unix.stackexchange.com/questions/194308/d-bus-authentication-and-authorization/200104 \n D-Bus service files: \n http://kkaempf.blogspot.com/2009/03/d-bus-service-on-demand.html \n https://stackoverflow.com/questions/19453507/how-to-create-a-dbus-service \n \n \n https://gitlab.freedesktop.org/dbus/dbus-java/commit/129eed36e2cc5e55c2a2ab057e2eccbe0e23c395 \n \n  vim: softtabstop=8 shiftwidth=8 tabstop=8 noexpandtab\n', ""Jekyll::AssetPipeline \n This gem is an external asset pipeline for Jekyll projects. It supports  Sass  for CSS, and ES6 for JavaScript (via  Babel ). It also runs  PurgeCSS  to remove unnecessary CSS and  Uglify  to compress JavaScript. \n Installation \n Add this line to your application's  Gemfile : \n rb\ngem 'jekyll-asset-pipeline', git: 'https://github.com/crdschurch/jekyll-asset-pipeline', tag: '0.0.1' \n And then execute: \n $ bundle exec jekyll-asset-pipeline install\n \n This script does the following: \n \n Installs  purgecss  globally via NPM. \n Installs local JS package dependencies, which will create a  package.json  file if it doesn't already exit. \n Copies  gulpfile.js  into the project root. This is the configuration for the build process, which you're welcome to customize as necessary. \n Copies  purgecss.config.json  into the project root. This is the configuration for PurgeCSS, which you are also welcome to customize as necessary. \n \n The process will also likely create a  package-lock.json  file and a  node_modules  directory. It is recommended that you add the  node_modules  directory to your  .gitignore  file. \n Introduction \n The build process has three primary components: \n \n Gulp.js : The build process. The build logic and configuration can be found in  gulpfile.js , which is copied into the root of your project during installation. \n A series of Jekyll hooks that control when (and whether or not) to run the asset build when the jekyll build is run. \n Jekyll tags to support resolving the appropriate filename for your  <link>  and  <script>  tags. \n \n Usage \n The build is run via Gulp.js (which is run via an NPM script). This occurs automatically as part of the Jekyll build process ( jekyll build  or  jekyll serve ). \n The build uses  _assets/stylesheets  as the source directory for (S)CSS files and  _assets/javascripts  as the source for JS files. (More on each of these in their respective sections, below.) \n The build will run if any of the following conditions are true: \n \n BUILD_ASSETS  environment variable is set to  true  (i.e.  BUILD_ASSETS=true jekyll [build/serve] ). \n There are no  .js  or  .css  files in the build directory. \n A  .js  or  .scss  file within the source directory has been modified since the last time the build was run. \n \n Liquid/HTML Tags \n Within a Jekyll view (HTML file), you can use the custom tags to load the appropriate file(s): \n liquid\n{% javascript_link_tag application %}\n{% stylesheet_link_tag application %} \n Notice the lack of file extension. \n The  javascript_link_tag  accepts a second argument for which you can add an  async  or  defer  attribute to the script tag. \n Configuration \n Aside from the environment variable mentioned above, you have the option to adjust one value in your site's  _config.yml  file. \n \n asset_dest  (default:  assets ): The directory within your build directory in which to house the built assets. \n \n CSS \n The CSS builds one sass source file ( _assets/stylesheets/application.scss ) and puts the compiled output in  _site/assets/ . There is nothing to configure, as Sass supports importing partials by default. \n JavaScript \n JavaScript is more configurable that the CSS. All JS build configuration can be found in  _assets/javascripts/config.js . This file is to export an array of config objects, where each object represents a built file with the following options: \n \n name  (Required): The name of the file (sans  .js  extension). \n deps : An array of vendors files (dependencies, sans  .js  extension) to prepend to the built file. \n files : An array of files (sans  .js  extension) to process with Babel and then append to the built file. \n \n The resulting file(s) will be placed in  _site/assets/ . \n Take the following example: \n js\nmodule.exports = [\n  {\n    name: 'application',\n    deps: [\n      'vendor/jquery.min',\n      'vendor/lodash.min'\n    ],\n    files: [\n      'components/header'\n    ]\n  }\n] \n Given the config above,  _assets/javascripts/vendor/jquery.min.js  (notice  .js  extension is automatically added) and  _assets/javascripts/vendor/lodash.min.js  will be prepended to a temporary file, while  _assets/javascripts/components/header.js  will be processed with Babel (to support older browsers), minified, and appended to the same file. This file will eventually become named  application.js  (because of the  name  option in the config) and will be placed in  _site/assets/ . \n Troubleshooting \n If you start the Jekyll server and there are missing styles or your scripts are working, it's likely that the Jekyll asset tags are looking for a different filename than what exists in your build directory ( _site , by default). There are two quick options to fix: \n \n Delete the build directory and restart the server (or re-run the build). \n Save a file in your assets source directory. The next time the project builds (which would be instantaneously if the server is already running) the assets will regenerate. \n \n Contributing \n Bug reports and pull requests are welcome on GitHub at https://github.com/crdschurch/jekyll-asset-pipeline. This project is intended to be a safe, welcoming space for collaboration, and contributors are expected to adhere to the  Contributor Covenant  code of conduct. \n Code of Conduct \n Everyone interacting in the Jekyll::Asset::Pipeline project’s codebases, issue trackers, chat rooms and mailing lists is expected to follow the  code of conduct ."", 'pyspark-test', 'mpds-orange \n Overall system design \n Autoscaling system \n There are 2 Kafka topics used by the autoscaling system: \n \n Metric  for message rate metrics \n Prediction  for output results of the prediction models \n \n Metrics \n See the  Prometheus Postman collection . \n \n Example of a Prometheus API request and response: \n\nHere\'s the request. It is not a single HTTP request, but a separate request for each metric. Although it is technically possible to get all the metrics at once (as per [the documentation](https://prometheus.io/docs/prometheus/latest/querying/basics/)), we would have to do aggregations on our own. Having multiple queries is not a problem, however. We can simply specify a `time` field in the HTTP request to retrieve a consistent set of metrics.\n\n```sh\nrequest() {\n    address=""http://prometheus:30090""\n    endpoint=""/api/v1/query""\n    url=""${address}${endpoint}""\n    time=""2021-02-08T10:10:51.781Z""\n\n    curl -Ss -X POST -F query=""$1"" -F time=""$time"" ""$url"" | jq\n}\n\nrequest ""avg(avg by (operator_id) (flink_taskmanager_job_latency_source_id_operator_id_operator_subtask_index_latency{quantile=\\""0.95\\""}))""\nrequest ""avg(kafka_server_brokertopicmetrics_total_messagesinpersec_count)""\nrequest kafka_controller_kafkacontroller_controllerstate_value\n```\n\nThe response:\n\n```\n{\n  ""status"": ""success"",\n  ""data"": {\n    ""resultType"": ""vector"",\n    ""result"": [\n      {\n        ""metric"": {},\n        ""value"": [\n          1612779051.781,\n          ""181.43333333333334""\n        ]\n      }\n    ]\n  }\n}\n{\n  ""status"": ""success"",\n  ""data"": {\n    ""resultType"": ""vector"",\n    ""result"": [\n      {\n        ""metric"": {},\n        ""value"": [\n          1612779051.781,\n          ""112629192.33333334""\n        ]\n      }\n    ]\n  }\n}\n{\n  ""status"": ""success"",\n  ""data"": {\n    ""resultType"": ""vector"",\n    ""result"": [\n      {\n        ""metric"": {\n          ""__name__"": ""kafka_controller_kafkacontroller_controllerstate_value"",\n          ""app_kubernetes_io_component"": ""kafka"",\n          ""app_kubernetes_io_instance"": ""mpds"",\n          ""app_kubernetes_io_managed_by"": ""Helm"",\n          ""app_kubernetes_io_name"": ""kafka"",\n          ""controller_revision_hash"": ""kafka-7dc6cd8b54"",\n          ""helm_sh_chart"": ""kafka-11.8.2"",\n          ""instance"": ""10.1.0.10:5556"",\n          ""job"": ""kubernetes-pods"",\n          ""kubernetes_namespace"": ""default"",\n          ""kubernetes_pod_name"": ""kafka-1"",\n          ""statefulset_kubernetes_io_pod_name"": ""kafka-1""\n        },\n        ""value"": [\n          1612779051.781,\n          ""0""\n        ]\n      },\n      {\n        ""metric"": {\n          ""__name__"": ""kafka_controller_kafkacontroller_controllerstate_value"",\n          ""app_kubernetes_io_component"": ""kafka"",\n          ""app_kubernetes_io_instance"": ""mpds"",\n          ""app_kubernetes_io_managed_by"": ""Helm"",\n          ""app_kubernetes_io_name"": ""kafka"",\n          ""controller_revision_hash"": ""kafka-7dc6cd8b54"",\n          ""helm_sh_chart"": ""kafka-11.8.2"",\n          ""instance"": ""10.1.1.6:5556"",\n          ""job"": ""kubernetes-pods"",\n          ""kubernetes_namespace"": ""default"",\n          ""kubernetes_pod_name"": ""kafka-0"",\n          ""statefulset_kubernetes_io_pod_name"": ""kafka-0""\n        },\n        ""value"": [\n          1612779051.781,\n          ""0""\n        ]\n      },\n      {\n        ""metric"": {\n          ""__name__"": ""kafka_controller_kafkacontroller_controllerstate_value"",\n          ""app_kubernetes_io_component"": ""kafka"",\n          ""app_kubernetes_io_instance"": ""mpds"",\n          ""app_kubernetes_io_managed_by"": ""Helm"",\n          ""app_kubernetes_io_name"": ""kafka"",\n          ""controller_revision_hash"": ""kafka-7dc6cd8b54"",\n          ""helm_sh_chart"": ""kafka-11.8.2"",\n          ""instance"": ""10.1.2.6:5556"",\n          ""job"": ""kubernetes-pods"",\n          ""kubernetes_namespace"": ""default"",\n          ""kubernetes_pod_name"": ""kafka-2"",\n          ""statefulset_kubernetes_io_pod_name"": ""kafka-2""\n        },\n        ""value"": [\n          1612779051.781,\n          ""0""\n        ]\n      }\n    ]\n  }\n}\n```\n \n The following metrics are potentially interesting, but are not available available at this moment in Prometheus:\n- Network in/out (to do)\n- CPU Utilization (to do, probably available via Kubernetes API\'s)\n- Memory Usage (to do, probably avialable via Kubernetes API\'s) \n Prediction models \n There are two prediction models available at the moment: a long-term one, and a short-term one. Both models are predicting the load based on the Kafka message rates.', 'OpenConnect FreeBSD Daemon \n A service daemon for the FreeBSD rc(8) framework. It lets the user configure\nOpenConnect VPN in rc.conf(5) and use the standard FreeBSD tools to control the\ndaemon. \n Features:\n- Support for starting multiple OpenConnect services.\n- Support for running arbitrary commands for OTP-based authentication. \n See the service files for usage details. \n Installation \n ```console\n% make\n% su \n make install \n ``` \n Examples \n Configure and connect to a VPN with OpenConnect: \n ```console \n sysrc openconnect_myvpn_enable=""YES"" \n sysrc openconnect_myvpn_username=""charlie.root"" \n sysrc openconnect_myvpn_server=""vpn.example.org"" \n service openconnect setpassword myvpn \n Password (openconnect_myvpn): \n service openconnect start myvpn \n ``` \n Enable ""myvpn"" OpenConnect service to run on boot: \n ```console \n sysrc openconnect_services+=""myvpn"" \n ``` \n Enable verbose output (e.g., for debugging): \n ```console \n sysrc openconnect_myvpn_args+=""--verbose"" \n ``` \n Set the password manually: \n ```console \n mkdir -p /usr/local/etc/openconnect/passwords \n (umask 077 && echo ""password"" > /usr/local/etc/openconnect/passwords/myvpn) \n ``` \n License \n The 2-Clause BSD license.', 'Mantra \n Mantra is a CLI for previewing  mandoc  manual pages with live auto-reload in a pager. \n Installation \n Dependencies: \n \n entr  (for watching file changes) \n less  (Less is the only supported pager at the moment) \n tmux  (for scripting pager interactions) \n \n sh\nmake all\nPREFIX=/usr/local make install \n Usage \n sh\nmantra style.9 \n License \n BSD 2-Clause ""Simplified"" License']",['repository_count'],@FreeBSD,https://avatars.githubusercontent.com/u/7978161?u=67ecd37eba9a42e26d07274b4d4ed75d3cbc6460&v=4,True,2016-11-01 17:46:53,169,451,2,2014-06-24T17:52:23Z,2022-11-25T09:48:26Z,,False,False,False,False,False,True,False,0,"{'C++': 2, 'JavaScript': 2, 'HTML': 1, 'Shell': 15, 'Lua': 1, 'Python': 4, 'C': 6, 'TeX': 3, 'Roff': 3, 'CSS': 1, 'Erlang': 1, 'Makefile': 5, 'Ruby': 1, 'Java': 2, 'Jupyter Notebook': 1}",3,"Berlin, Germany",Data Analytics,4,0.06,0.56,0.06,0
Maratyszcza,Marat Dukhan,,maratek@gmail.com,"['BLISBench \n Benchmark of matrix-matrix multiplication implementations for Web browsers \n This project visualizes performance of matrix-matrix multiplication (GEMM functions in BLAS) for the following implementations: \n \n  Naive  DGEMM / ZGEMM  in JavaScript with JS Arrays \n  Naive  SGEMM / DGEMM / CGEMM / ZGEMM  in JavaScript with Typed Arrays \n  Naive  SGEMM / DGEMM / CGEMM / ZGEMM  in Asm.js (compiled from C with Emscripten) \n  Naive  SGEMM / DGEMM / CGEMM / ZGEMM  in Portable Native Client (compiled from C) \n  BLIS-provided  SGEMM / DGEMM / CGEMM / ZGEMM  in Asm.js (compiled from C with Emscripten) \n  BLIS-provided  SGEMM / DGEMM / CGEMM / ZGEMM  in Portable Native Client (compiled from C) \n \n Additional Resources \n BLIS for the Web: HPC in a Web browser  presentation on  BLIS Retreat 2014 \n References \n \n BLIS  library for basic linear algebra operations. \n Emscripten  C/C++-to-JavaScript compiler. \n Portable Native Client  technology for running C/C++ code in a browser. \n Asm.js  subset of JavaScript. \n', ""Linear Algebra - Foundations to Frontiers Demos \n Live demos for Robert van de Geijn's and Maggie Myers'  Linear Algebra - Foundations to Frontiers  MOOC course. \n \n Live benchmark of matrix-matrix multiplication  demonstrates performance patterns of naive vs blocked vs state-of-the-art ( BLIS ) matrix-matrix multiplication implementations. \n Live pointer-chasing benchmark  measures random access time for arrays of different size. When arrays are too big to fit into cache, the access time dramatically increases. \n"", 'pthreadpool \n \n \n pthreadpool  is a portable and efficient thread pool implementation.\nIt provides similar functionality to  #pragma omp parallel for , but with additional features. \n Features: \n \n C interface (C++-compatible). \n 1D-6D loops with step parameters. \n Run on user-specified or auto-detected number of threads. \n Work-stealing scheduling for efficient work balancing. \n Wait-free synchronization of work items. \n Compatible with Linux (including Android), macOS, iOS, Windows, Emscripten environments. \n 100% unit tests coverage. \n Throughput and latency microbenchmarks. \n \n Example \n The following example demonstates using the thread pool for parallel addition of two arrays: \n ```c\nstatic void add_arrays(struct array_addition_context* context, size_t i) {\n  context->sum[i] = context->augend[i] + context->addend[i];\n} \n define ARRAY_SIZE 4 \n int main() {\n  double augend[ARRAY_SIZE] = { 1.0, 2.0, 4.0, -5.0 };\n  double addend[ARRAY_SIZE] = { 0.25, -1.75, 0.0, 0.5 };\n  double sum[ARRAY_SIZE]; \n pthreadpool_t threadpool = pthreadpool_create(0);\n  assert(threadpool != NULL); \n const size_t threads_count = pthreadpool_get_threads_count(threadpool);\n  printf(""Created thread pool with %zu threads\\n"", threads_count); \n struct array_addition_context context = { augend, addend, sum };\n  pthreadpool_parallelize_1d(threadpool,\n    (pthreadpool_task_1d_t) add_arrays,\n    (void ) &context,\n    ARRAY_SIZE,\n    PTHREADPOOL_FLAG_DISABLE_DENORMALS /  flags */); \n pthreadpool_destroy(threadpool);\n  threadpool = NULL; \n printf(""%8s\\t%.2lf\\t%.2lf\\t%.2lf\\t%.2lf\\n"", ""Augend"",\n    augend[0], augend[1], augend[2], augend[3]);\n  printf(""%8s\\t%.2lf\\t%.2lf\\t%.2lf\\t%.2lf\\n"", ""Addend"",\n    addend[0], addend[1], addend[2], addend[3]);\n  printf(""%8s\\t%.2lf\\t%.2lf\\t%.2lf\\t%.2lf\\n"", ""Sum"",\n    sum[0], sum[1], sum[2], sum[3]); \n return 0;\n}\n```', 'WebRunner (PeachPy.IO backend) \n WebRunner is a service to execute user-supplied untrusted machine code on your server without compromising its security. \n Key features: \n \n REST API (i.e. you communicate with the service through stateless HTTP requests) \n Built-in loader for ELF object files \n Sandboxing of untrusted code through  seccomp-bpf  mechanism \n Benchmarking and analyzing the code with hardware event counters. \n Self-check command to support automation of service downtime  \n Extendable set of supported kernels \n \n WebRunner dependencies \n Required dependecies \n \n Linux kernel >= 3.17 \n Python 2.7 \n Ninja  build system ( sudo apt-get install ninja-build ) \n ninja-syntax  module ( sudo pip install ninja-syntax ) \n \n Recommended dependecies \n \n systemd (WebRunner includes service configuration only for systemd) \n Ubuntu 15.10 (WebRunner was tested only on this distribution) \n \n Optional dependecies \n \n PeachPy  (required to run  the example ) \n \n Building WebRunner \n Configure and compile: \n bash\n./configure.py\nninja \n Recommended:  install WebRunner to  /usr/sbin/webrunner  and register as a  systemd  service: \n bash\nsudo ninja install \n After installation you can start the service with  sudo ninja start  and terminate it with  sudo ninja stop \n Alternative:  run WebRunner without installation: \n bash\n./webrunner # webrunner -h to list options \n REST API \n WebRunner commands must follow the pattern  http://server[:port]/machine-id/command[?query] \n \n machine-id  is an arbitrary string. It is parsed, but ignored by the WebRunner. \n command  is one of the supported commands ( monitor  or  run ). \n query  is an optional query string with command parameters. \n \n monitor  command \n The  monitor  command is used to check server status. \n HTTP request \n \n \n Method:  HEAD \n \n \n URL:  http://server[:port]/machine-id/monitor \n \n \n HTTP response \n A server would respond HTTP status ok 200 (OK) to this command. \n Example \n bash\ncurl --head ""http://localhost:8081/local/monitor"" \n run  command \n The  run  command is used to benchmark and analyze a function in an ELF object. The ELF object must be sent in the request body. \n HTTP request \n \n \n Method:  POST \n \n \n Content-Type:  application/octet-stream \n \n \n URL:  http://server[:port]/machine-id/run?kernel=kernel-name&[param1=value1&param2=value2&...] \n \n \n The  kernel  parameter specifies kernel type. Query parameters after it depend on the kernel type and specify parameters of the kernel run. Look at XML specifications in the  /src/kernels  directory for permitted kernel types and their parameters. \n HTTP response \n The server would respond with a line of names of hardware performance counters and their values (one per line) \n Example \n bash\nwget --header=""Content-Type:application/octet-stream"" --post-file=sdot.o \\\n  ""http://localhost:8081/local/run?kernel=sdot&n=10000&incx=1&incy=2""', 'Caffe \n \n \n Caffe is a deep learning framework made with expression, speed, and modularity in mind.\nIt is developed by the Berkeley Vision and Learning Center ( BVLC ) and community contributors. \n Check out the  project site  for all the details like \n \n DIY Deep Learning for Vision with Caffe \n Tutorial Documentation \n BVLC reference models  and the  community model zoo \n Installation instructions \n \n and step-by-step examples. \n \n Please join the  caffe-users group  or  gitter chat  to ask questions and talk about methods and models.\nFramework development discussions and thorough bug reports are collected on  Issues . \n Happy brewing! \n License and Citation \n Caffe is released under the  BSD 2-Clause license .\nThe BVLC reference models are released for unrestricted use. \n Please cite Caffe in your publications if it helps your research: \n @article{jia2014caffe,\n  Author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},\n  Journal = {arXiv preprint arXiv:1408.5093},\n  Title = {Caffe: Convolutional Architecture for Fast Feature Embedding},\n  Year = {2014}\n}\n', 'FPplus \n Scientific library for high-precision computations and research \n FPplus was originally developed for a research project on instructions to accelerate high-precision computations, but it is also useful as a general-purpose library. FPplus features: \n \n Header-only library for error-free transforms and double-double computations \n Implements error-free addition, multiplication, and fused multiply-add \n Implements double-double addition and multiplication in multiple variants \n Compatible with C99, C++, OpenCL, and CUDA \n Special versions of error-free transforms in SIMD intrinsics: \n x86 SIMD (128-bit and 256-bit AVX + FMA, 512-bit wide MIC and AVX-512) \n IBM VSX (POWER 7 and POWER 8) and QPX (Blue Gene/Q) \n ARMv8 SIMD \n Extensive documentation with references to scientific literature \n Testsuite based on  MPFR  and  Google Test \n Examples and code-generators for high-precision algorithms: \n Polynomial evaluation with compensated Horner scheme \n Compensated dot product algorithm \n Inner kernel of matrix multiplication (GEMM) operation in double-double precision \n \n Requirements \n CPU targets: \n \n gcc-compatible compiler (tested on gcc, clang and icc) \n Hardware FMA support \n Precise floating-point semantics \n No  -ffast-math  option when compiling with  gcc  or  clang \n -fp-model precise  when compiling with  icc \n \n OpenCL targets: \n \n cl_khr_fp64 ,  cl_amd_fp64 , or  cl_APPLE_fp64_basic_ops  extension \n Hardware FMA support ( FP_FAST_FMA  must be defined by OpenCL compiler) \n Precise floating-point semantics \n No  -cl-fast-relaxed-math  option \n \n CUDA targets: \n \n Compute capability 2.0 or higher \n \n Using FPplus \n ```c \n include  \n ``` \n Publications \n Marat Dukhan, Richard Vuduc and Jason Riedy  ""Wanted: Floating-Point Add Round-off Error instruction"" . arXiv preprint 1603.00491 (2016) \n Acknowledgements \n \n \n The library was developed by  Marat Dukhan  as a research project at  Richard Vuduc \'s HPC Garage lab in the Georgia Institute of Technology, College of Computing, School of Computational Science and Engineering. FPplus is based on algorithms in  Handbook of Floating-Point Arithmetics ,  David Bailey \'s QD library, the works of  Jonathan Shewchuk ,  Theodorus Dekker ,  Donald Knuth , and  Sylvie Boldo and Jean-Michel Muller . We thank  Jason Riedy  for his feedback and support. \n This material is based upon work supported by the U.S. National Science Foundation (NSF) Award Number 1339745 and the U.S. Dept. of Energy (DOE), Office of Science, Advanced Scientific Computing Research under award DE-FC02-10ER26006/DE-SC0004915. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of NSF or DOE.', ""\n NNPACK \n \n \n NNPACK is an acceleration package for neural network computations. NNPACK aims to provide high-performance implementations of convnet layers for multi-core CPUs. \n NNPACK is not intended to be directly used by machine learning researchers; instead it provides low-level performance primitives leveraged in leading deep learning frameworks, such as  PyTorch ,  Caffe2 ,  MXNet , \n tiny-dnn ,  Caffe ,  Torch , and  Darknet . \n Platforms and requirements \n | Environment  | Architecture  | CPU requirements                 |\n| ------------ | ------------- | -------------------------------- |\n| Linux        | x86-64        | AVX2 and 3-level cache hierarchy |\n| Linux        | ARM           | NEON                             |\n| Linux        | ARM64         |                                  |\n| macOS        | x86-64        | AVX2 and 3-level cache hierarchy |\n| Android      | ARM           | NEON                             |\n| Android      | ARM64         |                                  |\n| Android      | x86           |                                  |\n| Android      | x86-64        |                                  |\n| iOS          | ARM           |                                  |\n| iOS          | ARM64         |                                  |\n| Emscripten   | Asm.js        |                                  |\n| Emscripten   | WebAssembly   |                                  | \n Features \n \n Multiple algorithms for convolutional layers: \n Fast convolution based on Fourier transform (for kernels up to 16x16 without stride) \n Fast convolution based on Winograd transform (for 3x3 kernels without stride) \n Implicit matrix-matrix multiplication algorithm (no limitations) \n Direct convolution algorithm (for 1x1 kernels without stride) \n Multi-threaded SIMD-aware implementations of neural network layers \n Implemented in C99 and Python without external dependencies \n Extensive coverage with unit tests \n \n Layers \n \n Convolutional layer \n Inference-optimized forward propagation ( nnp_convolution_inference ) \n Training-optimized forward propagation ( nnp_convolution_output ) \n Training-optimized backward input gradient update ( nnp_convolution_input_gradient ) \n Training-optimized backward kernel gradient update ( nnp_convolution_kernel_gradient ) \n Fully-connected layer \n Inference-optimized forward propagation ( nnp_fully_connected_inference  and  nnp_fully_connected_inference_f16f32  version for FP16 weights) \n Training-optimized forward propagation ( nnp_fully_connected_output ) \n Max pooling layer \n Forward propagation, both for training and inference, ( nnp_max_pooling_output ) \n ReLU layer (with parametrized negative slope) \n Forward propagation, both for training and inference, optionally in-place, ( nnp_relu_output ) \n Backward input gradient update ( nnp_relu_input_gradient ) \n Softmax layer \n Forward propagation, both for training and inference, optionally in-place ( nnp_softmax_output ) \n \n Building \n For most users, the recommended way to build NNPACK is through CMake: \n bash\nmkdir build\ncd build\ncmake -G Ninja ..\nninja \n Note: if  ninja  is not available on your system, configure without  -G Ninja , and use  make  instead of  ninja . \n Building NNPACK - Using vcpkg \n You can download and install NNPACK using the  vcpkg  dependency manager: \n git clone https://github.com/Microsoft/vcpkg.git\ncd vcpkg\n./bootstrap-vcpkg.sh\n./vcpkg integrate install\n./vcpkg install NNPACK\n \n The NNPACK port in vcpkg is kept up to date by Microsoft team members and community contributors. If the version is out of date, please  create an issue or pull request  on the vcpkg repository. \n Cross-compilation for Android \n To cross-compile for Android, add extra configuration options for  cmake :  -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake  (where  $ANDROID_NDK  is the path to Android NDK directorory, e.g.  /opt/android-ndk-r15c )  AND  arguments from the table below \n | ABI         | Extra cmake args                                    | Restrictions               |\n| ----------- | --------------------------------------------------- | -------------------------- |\n| armeabi     |  -DANDROID_ABI=armeabi -DANDROID_TOOLCHAIN=gcc      | Requires CPU with ARM NEON |\n| armeabi-v7a |  -DANDROID_ABI=armeabi-v7a -DANDROID_TOOLCHAIN=gcc  | Requires CPU with ARM NEON |\n| arm64-v8a   |  -DANDROID_ABI=arm64-v8a -DANDROID_TOOLCHAIN=clang  | Requires clang toolchain   |\n| x86         |  -DANDROID_ABI=x86                                  |                            |\n| x86_64      |  -DANDROID_ABI=x86_64                               |                            | \n Notes:\n- On  armeabi  and  armeabi-v7a   nnp_initialize  will fail with  nnp_status_unsupported_hardware  if the mobile CPU does not support ARM NEON. Don't set  -DANDROID_ARM_NEON=1  for NNPACK compilation as it can make  nnp_initialize  crash on CPUs without ARM NEON.\n- NNPACK builds for  armeabi  and  armeabi-v7a  are up to 2x slower if you use  clang  toolchain.\n-  mips  and  mips64  are not supported, and we have no plans to add it (pull request would be welcome, though)\n-  x86_64  build will use generic 128-bit (SSE2) micro-kernels rather than AVX2 micro-kernels in native build \n Ecosystem \n Deep Learning Frameworks \n \n PyTorch  supports NNPACK on mobile for inference in convolutional layers. \n TVM  supports NNPACK for inference in convolutional layers. See  these instructions  to enable NNPACK in TVM. \n MXNet  supports NNPACK for inference in convolutional layers, fully-connected, and max-pooling layers. See  MXNet wiki  for configuration instructions and performance benchmarks). \n Caffe2  supports NNPACK for inference in convolutional layers. \n darknet-nnpack  - fork of  Darknet  framework with NNPACK support. \n tiny-dnn  - header-only deep learning framework in C++11, which natively supports NNPACK. \n Maratyszcza/caffe  - up-to-date integration of NNPACK (convolutional, fully-connected, max-pooling, and ReLU layers) into Caffe based on  nnpack-pr  branch in  ajtulloch/caffe . \n Maratyszcza/caffe-nnpack  - older and unmaintained integration of NNPACK (convolutional layers only) into Caffe. \n szagoruyko/nnpack.torch  - integration of NNPACK into Lua Torch via ffi \n See also discussion in  Issue #1 \n \n Languages and Environments \n \n nnpack-windows  - unofficial port for Windows \n node-nnpack  - Node.js bindings \n peterhj/libnnpack  - Rust bindings \n \n Users \n \n Facebook  uses NNPACK in production. \n Prisma  uses NNPACK in the mobile app. \n \n Acknowledgements \n \n \n The library is developed by  Marat Dukhan  of Georgia Tech with extensive advice from  Nicolas Vasilache  and  Soumith Chintala  of Facebook Artificial Intelligence Research.  Andrew Tulloch  of Facebook Artificial Intelligence Research contributed Caffe integration. We thank  Andrew Lavin  for fruitful discussions on Winograd transform-based implementations. NNPACK is a research project at  Richard Vuduc 's HPC Garage lab in the Georgia Institute of Technology, College of Computing, School of Computational Science and Engineering. \n This material is based upon work supported by the U.S. National Science Foundation (NSF) Award Number 1339745. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of NSF."", 'FXdiv \n \n \n Header-only library for division via fixed-point multiplication by inverse \n On modern CPUs and GPUs integer division is several times slower than multiplication. FXdiv implements an algorithm to replace an integer division with a multiplication and two shifts. This algorithm improves performance when an application performs repeated divisions by the same divisor. \n Features \n \n Integer division for  uint32_t ,  uint64_t , and  size_t \n Header-only library, no installation or build required \n Compatible with C99, C++, OpenCL, and CUDA \n Uses platform-specific compiler intrinsics for optimal performance \n Covered with unit tests and microbenchmarks \n \n Example \n ```c \n include  \n /  Division of array by a constant: reference implementation  /\nvoid divide_array_c(size_t length, uint32_t array[], uint32_t divisor) {\n  for (size_t i = 0; i < length; i++) {\n    array[i] /= divisor;\n  }\n} \n /  Division of array by a constant: implementation with FXdiv  /\nvoid divide_array_fxdiv(size_t length, uint32_t array[], uint32_t divisor) {\n  const struct fxdiv_divisor_uint32_t precomputed_divisor =\n    fxdiv_init_uint32_t(divisor);\n  for (size_t i = 0; i < length; i++) {\n    array[i] = fxdiv_quotient_uint32_t(array[i], precomputed_divisor);\n  }\n}\n``` \n Status \n Currently working features: \n | Platform        | uint32_t | uint64_t | size_t   |\n| --------------- |:--------:|:--------:|:--------:|\n| x86-64 gcc      | Works    | Works    | Works    |\n| x86-64 clang    | Works    | Works    | Works    |\n| x86-64 MSVC     | Works    | Works    | Works    |\n| x86 gcc         | Works    | Works    | Works    |\n| x86 clang       | Works    | Works    | Works    |\n| x86 MSVC        | Works    | Works    | Works    |\n| ARMv7 gcc       | Works    | Works    | Works    |\n| ARMv7 clang     | Works    | Works    | Works    |\n| ARMv7 MSVC      | Compiles | Compiles | Compiles |\n| ARM64 gcc       | Works    | Works    | Works    |\n| ARM64 clang     | Works    | Works    | Works    |\n| ARM64 MSVC      | Compiles | Compiles | Compiles |\n| PPC64 gcc       | Works    | Works    | Works    |\n| WAsm clang      | Works    | Works    | Works    |\n| Asm.js clang    | Works    | Works    | Works    |\n| PNaCl clang     | Works    | Works    | Works    |\n| CUDA            | Untested | Untested | Untested |\n| OpenCL          | Untested | Untested | Untested | \n *ARMv7 and ARM64 builds with MSVC are presumed to work, but were only verified to compile successfully \n References \n \n Granlund, Torbjörn, and Peter L. Montgomery. ""Division by invariant integers using multiplication."" In ACM SIGPLAN Notices, vol. 29, no. 6, pp. 61-72. ACM, 1994. Available:  gmplib.org/~tege/divcnst-pldi94.pdf \n', ""FP16 \n Header-only library for conversion to/from half-precision floating point formats \n Features \n \n Supports IEEE and ARM alternative half-precision floating-point format \n Property converts infinities and NaNs \n Properly converts denormal numbers, even on systems without denormal support \n \n \n Header-only library, no installation or build required \n Compatible with C99 and C++11 \n Fully covered with unit tests and microbenchmarks \n \n Acknowledgements \n \n \n The library is developed by  Marat Dukhan  of Georgia Tech. FP16 is a research project at  Richard Vuduc 's HPC Garage lab in the Georgia Institute of Technology, College of Computing, School of Computational Science and Engineering. \n This material is based upon work supported by the U.S. National Science Foundation (NSF) Award Number 1339745. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of NSF."", 'P(ortable) SIMD \n Portable 128-bit SIMD intrinsics', 'CPU INFOrmation library \n \n \n \n cpuinfo is a library to detect essential for performance optimization information about host CPU. \n Features \n \n Cross-platform  availability: \n Linux, Windows, macOS, Android, and iOS operating systems \n x86, x86-64, ARM, and ARM64 architectures \n Modern  C/C++ interface \n Thread-safe \n No memory allocation after initialization \n No exceptions thrown \n Detection of  supported instruction sets , up to AVX512 (x86) and ARMv8.3 extensions \n Detection of SoC and core information: \n Processor (SoC) name \n Vendor and  microarchitecture  for each CPU core \n ID ( MIDR  on ARM,  CPUID  leaf 1 EAX value on x86) for each CPU core \n Detection of  cache information : \n Cache type (instruction/data/unified), size and line size \n Cache associativity \n Cores and logical processors (hyper-threads) sharing the cache \n Detection of  topology information  (relative between logical processors, cores, and processor packages) \n Well-tested  production-quality  code: \n 60+ mock tests based on data from real devices \n Includes work-arounds for common bugs in hardware and OS kernels \n Supports systems with heterogenous cores, such as  big.LITTLE  and Max.Med.Min \n Permissive  open-source  license (Simplified BSD) \n \n Examples \n Log processor name: \n c\ncpuinfo_initialize();\nprintf(""Running on %s CPU\\n"", cpuinfo_get_package(0)->name); \n Detect if target is a 32-bit or 64-bit ARM system: \n ```c \n if CPUINFO_ARCH_ARM || CPUINFO_ARCH_ARM64 \n /* 32-bit ARM-specific code here */\n \n endif \n ``` \n Check if the host CPU supports ARM NEON \n c\ncpuinfo_initialize();\nif (cpuinfo_has_arm_neon()) {\n    neon_implementation(arguments);\n} \n Check if the host CPU supports x86 AVX \n c\ncpuinfo_initialize();\nif (cpuinfo_has_x86_avx()) {\n    avx_implementation(arguments);\n} \n Check if the thread runs on a Cortex-A53 core \n c\ncpuinfo_initialize();\nswitch (cpuinfo_get_current_core()->uarch) {\n    case cpuinfo_uarch_cortex_a53:\n        cortex_a53_implementation(arguments);\n        break;\n    default:\n        generic_implementation(arguments);\n        break;\n} \n Get the size of level 1 data cache on the fastest core in the processor (e.g. big core in big.LITTLE ARM systems): \n c\ncpuinfo_initialize();\nconst size_t l1_size = cpuinfo_get_processor(0)->cache.l1d->size; \n Pin thread to cores sharing L2 cache with the current core (Linux or Android) \n c\ncpuinfo_initialize();\ncpu_set_t cpu_set;\nCPU_ZERO(&cpu_set);\nconst struct cpuinfo_cache* current_l2 = cpuinfo_get_current_processor()->cache.l2;\nfor (uint32_t i = 0; i < current_l2->processor_count; i++) {\n    CPU_SET(cpuinfo_get_processor(current_l2->processor_start + i)->linux_id, &cpu_set);\n}\npthread_setaffinity_np(pthread_self(), sizeof(cpu_set_t), &cpu_set); \n Use via pkg-config \n If you would like to provide your project\'s build environment with the necessary compiler and linker flags in a portable manner, the library by default when built enables  CPUINFO_BUILD_PKG_CONFIG  and will generate a  pkg-config  manifest ( libcpuinfo.pc ). Here are several examples of how to use it: \n Command Line \n If you used your distro\'s package manager to install the library, you can verify that it is available to your build environment like so: \n console\n$ pkg-config --cflags --libs libcpuinfo\n-I/usr/include/x86_64-linux-gnu/ -L/lib/x86_64-linux-gnu/ -lcpuinfo \n If you have installed the library from source into a non-standard prefix, pkg-config may need help finding it: \n console\n$ PKG_CONFIG_PATH=""/home/me/projects/cpuinfo/prefix/lib/pkgconfig/:$PKG_CONFIG_PATH"" pkg-config --cflags --libs libcpuinfo\n-I/home/me/projects/cpuinfo/prefix/include -L/home/me/projects/cpuinfo/prefix/lib -lcpuinfo \n GNU Autotools \n To  use  with the GNU Autotools include the following snippet in your project\'s  configure.ac : \n ```makefile \n CPU INFOrmation library... \n PKG_CHECK_MODULES(\n    [libcpuinfo], [libcpuinfo], [],\n    [AC_MSG_ERROR([libcpuinfo missing...])])\nYOURPROJECT_CXXFLAGS=""$YOURPROJECT_CXXFLAGS $libcpuinfo_CFLAGS""\nYOURPROJECT_LIBS=""$YOURPROJECT_LIBS $libcpuinfo_LIBS""\n``` \n Meson \n To use with Meson you just need to add  dependency(\'libcpuinfo\')  as a dependency for your executable. \n ```meson\nproject(\n    \'MyCpuInfoProject\',\n    \'cpp\',\n    meson_version: \'>=0.55.0\'\n) \n executable(\n    \'MyCpuInfoExecutable\',\n    sources: \'main.cpp\',\n    dependencies: dependency(\'libcpuinfo\')\n)\n``` \n Bazel \n This project can be built using  Bazel .  \n You can also use this library as a dependency to your Bazel project. Add to the  WORKSPACE  file: \n ```python\nload(""@bazel_tools//tools/build_defs/repo:git.bzl"", ""git_repository"") \n git_repository(\n    name = ""org_pytorch_cpuinfo"",\n    branch = ""master"",\n    remote = ""https://github.com/Vertexwahn/cpuinfo.git"",\n)\n``` \n And to your  BUILD  file: \n python\ncc_binary(\n    name = ""cpuinfo_test"",\n    srcs = [\n        # ...\n    ],\n    deps = [\n        ""@org_pytorch_cpuinfo//:cpuinfo"",\n    ],\n) \n CMake \n To use with CMake use the  FindPkgConfig  module. Here is an example: \n ```cmake\ncmake_minimum_required(VERSION 3.6)\nproject(""MyCpuInfoProject"") \n find_package(PkgConfig)\npkg_check_modules(CpuInfo REQUIRED IMPORTED_TARGET libcpuinfo) \n add_executable(${PROJECT_NAME} main.cpp)\ntarget_link_libraries(${PROJECT_NAME} PkgConfig::CpuInfo)\n``` \n Makefile \n To use within a vanilla makefile, you can call pkg-config directly to supply compiler and linker flags using shell substitution. \n makefile\nCFLAGS=-g3 -Wall -Wextra -Werror ...\nLDFLAGS=-lfoo ...\n...\nCFLAGS+= $(pkg-config --cflags libcpuinfo)\nLDFLAGS+= $(pkg-config --libs libcpuinfo) \n Exposed information \n \n [x] Processor (SoC) name \n [x] Microarchitecture \n [x] Usable instruction sets \n [ ] CPU frequency \n [x] Cache \n [x] Size \n [x] Associativity \n [x] Line size \n [x] Number of partitions \n [x] Flags (unified, inclusive, complex hash function) \n [x] Topology (logical processors that share this cache level) \n [ ] TLB \n [ ] Number of entries \n [ ] Associativity \n [ ] Covered page types (instruction, data) \n [ ] Covered page sizes \n [x] Topology information \n [x] Logical processors \n [x] Cores \n [x] Packages (sockets) \n \n Supported environments: \n \n [x] Android \n [x] x86 ABI \n [x] x86_64 ABI \n [x] armeabi ABI \n [x] armeabiv7-a ABI \n [x] arm64-v8a ABI \n [ ] ~~mips ABI~~ \n [ ] ~~mips64 ABI~~ \n [x] Linux \n [x] x86 \n [x] x86-64 \n [x] 32-bit ARM (ARMv5T and later) \n [x] ARM64 \n [ ] PowerPC64 \n [x] iOS \n [x] x86 (iPhone simulator) \n [x] x86-64 (iPhone simulator) \n [x] ARMv7 \n [x] ARM64 \n [x] macOS \n [x] x86 \n [x] x86-64 \n [x] ARM64 (Apple silicon) \n [x] Windows \n [x] x86 \n [x] x86-64 \n [x] arm64 \n \n Methods \n \n Processor (SoC) name detection \n [x] Using CPUID leaves 0x80000002–0x80000004 on x86/x86-64 \n [x] Using  /proc/cpuinfo  on ARM \n [x] Using  ro.chipname ,  ro.board.platform ,  ro.product.board ,  ro.mediatek.platform ,  ro.arch  properties (Android) \n [ ] Using kernel log ( dmesg ) on ARM Linux \n [x] Using Windows registry on ARM64 Windows \n Vendor and microarchitecture detection \n [x] Intel-designed x86/x86-64 cores (up to Sunny Cove, Goldmont Plus, and Knights Mill) \n [x] AMD-designed x86/x86-64 cores (up to Puma/Jaguar and Zen 2) \n [ ] VIA-designed x86/x86-64 cores \n [ ] Other x86 cores (DM&P, RDC, Transmeta, Cyrix, Rise) \n [x] ARM-designed ARM cores (up to Cortex-A55, Cortex-A77, and Neoverse E1/N1/V1/N2) \n [x] Qualcomm-designed ARM cores (Scorpion, Krait, and Kryo) \n [x] Nvidia-designed ARM cores (Denver and Carmel) \n [x] Samsung-designed ARM cores (Exynos) \n [x] Intel-designed ARM cores (XScale up to 3rd-gen) \n [x] Apple-designed ARM cores (up to Lightning and Thunder) \n [x] Cavium-designed ARM cores (ThunderX) \n [x] AppliedMicro-designed ARM cores (X-Gene) \n Instruction set detection \n [x] Using CPUID (x86/x86-64) \n [x] Using  /proc/cpuinfo  on 32-bit ARM EABI (Linux) \n [x] Using microarchitecture heuristics on (32-bit ARM) \n [x] Using  FPSID  and  WCID  registers (32-bit ARM) \n [x] Using  getauxval  (Linux/ARM) \n [x] Using  /proc/self/auxv  (Android/ARM) \n [ ] Using instruction probing on ARM (Linux) \n [ ] Using CPUID registers on ARM64 (Linux) \n [x] Using IsProcessorFeaturePresent on ARM64 Windows \n Cache detection \n [x] Using CPUID leaf 0x00000002 (x86/x86-64) \n [x] Using CPUID leaf 0x00000004 (non-AMD x86/x86-64) \n [ ] Using CPUID leaves 0x80000005-0x80000006 (AMD x86/x86-64) \n [x] Using CPUID leaf 0x8000001D (AMD x86/x86-64) \n [x] Using  /proc/cpuinfo  (Linux/pre-ARMv7) \n [x] Using microarchitecture heuristics (ARM) \n [x] Using chipset name (ARM) \n [x] Using  sysctlbyname  (Mach) \n [x] Using sysfs  typology  directories (ARM/Linux) \n [ ] Using sysfs  cache  directories (Linux) \n [x] Using  GetLogicalProcessorInformationEx  on ARM64 Windows \n TLB detection \n [x] Using CPUID leaf 0x00000002 (x86/x86-64) \n [ ] Using CPUID leaves 0x80000005-0x80000006 and 0x80000019 (AMD x86/x86-64) \n [x] Using microarchitecture heuristics (ARM) \n Topology detection \n [x] Using CPUID leaf 0x00000001 on x86/x86-64 (legacy APIC ID) \n [x] Using CPUID leaf 0x0000000B on x86/x86-64 (Intel APIC ID) \n [ ] Using CPUID leaf 0x8000001E on x86/x86-64 (AMD APIC ID) \n [x] Using  /proc/cpuinfo  (Linux) \n [x] Using  host_info  (Mach) \n [x] Using  GetLogicalProcessorInformationEx  (Windows) \n [x] Using sysfs (Linux) \n [x] Using chipset name (ARM/Linux) \n', ""TensorFlow.js \n TensorFlow.js is an open-source hardware-accelerated JavaScript library for\ntraining and deploying machine learning models. \n Develop ML in the Browser   \nUse flexible and intuitive APIs to build models from scratch using the low-level\nJavaScript linear algebra library or the high-level layers API. \n Develop ML in Node.js   \nExecute native TensorFlow with the same TensorFlow.js API under the Node.js\nruntime. \n Run Existing models   \nUse TensorFlow.js model converters to run pre-existing TensorFlow models right\nin the browser. \n Retrain Existing models   \nRetrain pre-existing ML models using sensor data connected to the browser or\nother client-side data. \n About this repo \n This repository contains the logic and scripts that combine\nseveral packages. \n APIs:\n-  TensorFlow.js Core ,\n  a flexible low-level API for neural networks and numerical computation.\n-  TensorFlow.js Layers ,\n  a high-level API which implements functionality similar to\n   Keras .\n-  TensorFlow.js Data ,\n  a simple API to load and prepare data analogous to\n   tf.data .\n-  TensorFlow.js Converter ,\n  tools to import a TensorFlow SavedModel to TensorFlow.js\n-  TensorFlow.js Vis ,\n  in-browser visualization for TensorFlow.js models\n-  TensorFlow.js AutoML ,\n  Set of APIs to load and run models produced by\n   AutoML Edge . \n Backends/Platforms:\n-  TensorFlow.js Node , Node backend via TensorFlow C++\n-  TensorFlow.js WASM , WebAssembly backend\n-  TensorFlow.js React Native , Support for React Native.\n-  TensorFlow.js WebGPU , WebGPU backend. \n If you care about bundle size, you can import those packages individually. \n If you are looking for Node.js support, check out the  TensorFlow.js Node directory . \n Examples \n Check out our\n examples repository \nand our  tutorials . \n Gallery \n Be sure to check out  the gallery  of all projects related to TensorFlow.js. \n Pre-trained models \n Be sure to also check out our  models repository  where we host pre-trained models\non NPM. \n Getting started \n There are two main ways to get TensorFlow.js in your JavaScript project:\nvia  script tags   or  by installing it from  NPM \nand using a build tool like  Parcel ,\n WebPack , or  Rollup . \n via Script Tag \n Add the following code to an HTML file: \n ```html\n\n   \n  Load TensorFlow.js  \n   \n <!-- Place your code in the script tag below. You can also use an external .js file -->\n<script>\n  // Notice there is no 'import' statement. 'tf' is available on the index-page\n  // because of the script tag above.\n\n  // Define a model for linear regression.\n  const model = tf.sequential();\n  model.add(tf.layers.dense({units: 1, inputShape: [1]}));\n\n  // Prepare the model for training: Specify the loss and the optimizer.\n  model.compile({loss: 'meanSquaredError', optimizer: 'sgd'});\n\n  // Generate some synthetic data for training.\n  const xs = tf.tensor2d([1, 2, 3, 4], [4, 1]);\n  const ys = tf.tensor2d([1, 3, 5, 7], [4, 1]);\n\n  // Train the model using the data.\n  model.fit(xs, ys).then(() => {\n    // Use the model to do inference on a data point the model hasn't seen before:\n    // Open the browser devtools to see the output\n    model.predict(tf.tensor2d([5], [1, 1])).print();\n  });\n</script>\n \n \n \n``` \n Open up that HTML file in your browser, and the code should run! \n via NPM \n Add TensorFlow.js to your project using  yarn   or   npm .  Note:  Because\nwe use ES2017 syntax (such as  import ), this workflow assumes you are using a modern browser or a bundler/transpiler\nto convert your code to something older browsers understand. See our\n examples \nto see how we use  Parcel  to build\nour code. However, you are free to use any build tool that you prefer. \n ```js\nimport * as tf from '@tensorflow/tfjs'; \n // Define a model for linear regression.\nconst model = tf.sequential();\nmodel.add(tf.layers.dense({units: 1, inputShape: [1]})); \n // Prepare the model for training: Specify the loss and the optimizer.\nmodel.compile({loss: 'meanSquaredError', optimizer: 'sgd'}); \n // Generate some synthetic data for training.\nconst xs = tf.tensor2d([1, 2, 3, 4], [4, 1]);\nconst ys = tf.tensor2d([1, 3, 5, 7], [4, 1]); \n // Train the model using the data.\nmodel.fit(xs, ys).then(() => {\n  // Use the model to do inference on a data point the model hasn't seen before:\n  model.predict(tf.tensor2d([5], [1, 1])).print();\n});\n``` \n See our  tutorials ,  examples \nand  documentation  for more details. \n Importing pre-trained models \n We support porting pre-trained models from:\n-  TensorFlow SavedModel \n-  Keras \n Find out more \n TensorFlow.js  is a part of the\n TensorFlow  ecosystem. For more info:\n- For help from the community, use  tensorflow.js  tag on Stack Overflow.\n-  js.tensorflow.org \n-  Tutorials \n-  API reference \n-  Discussion mailing list \n Thanks,  BrowserStack , for providing testing support."", 'XNNPACK \n XNNPACK is a highly optimized library of floating-point neural network inference operators for ARM, WebAssembly, and x86 platforms. XNNPACK is not intended for direct use by deep learning practitioners and researchers; instead it provides low-level performance primitives for accelerating high-level machine learning frameworks, such as  TensorFlow Lite ,  TensorFlow.js ,  PyTorch ,  ONNX Runtime , and  MediaPipe . \n Supported Architectures \n \n ARM64 on Android, Linux, macOS, and iOS (including WatchOS and tvOS) \n ARMv7 (with NEON) on Android \n ARMv6 (with VFPv2) on Linux \n x86 and x86-64 (up to AVX512) on Windows, Linux, macOS, Android, and iOS simulator \n WebAssembly MVP \n WebAssembly SIMD \n RISC-V (RV32GV and RV64GC) \n \n Operator Coverage \n XNNPACK implements the following neural network operators: \n \n 2D Convolution (including grouped and depthwise) \n 2D Deconvolution (AKA Transposed Convolution) \n 2D Average Pooling \n 2D Max Pooling \n 2D ArgMax Pooling (Max Pooling + indices) \n 2D Unpooling \n 2D Bilinear Resize \n 2D Depth-to-Space (AKA Pixel Shuffle) \n Add (including broadcasting, two inputs only) \n Subtract (including broadcasting) \n Divide (including broadcasting) \n Maximum (including broadcasting) \n Minimum (including broadcasting) \n Multiply (including broadcasting) \n Squared Difference (including broadcasting) \n Global Average Pooling \n Channel Shuffle \n Fully Connected \n Abs (absolute value) \n Bankers\' Rounding (rounding to nearest, ties to even) \n Ceiling (rounding to integer above) \n Clamp (includes ReLU and ReLU6) \n Convert (includes fixed-point and half-precision quantization and\n  dequantization) \n Copy \n ELU \n Floor (rounding to integer below) \n HardSwish \n Leaky ReLU \n Negate \n Sigmoid \n Softmax \n Square \n Transpose \n Truncation (rounding to integer towards zero) \n PReLU \n \n All operators in XNNPACK support NHWC layout, but additionally allow custom stride along the  C hannel dimension. Thus, operators can consume a subset of channels in the input tensor, and produce a subset of channels in the output tensor, providing a zero-cost Channel Split and Channel Concatenation operations. \n Performance \n Mobile phones \n The table below presents  single-threaded  performance of XNNPACK library on three generations of MobileNet models and three generations of Pixel phones. \n | Model                   | Pixel, ms | Pixel 2, ms | Pixel 3a, ms |\n| ----------------------- | :-------: | :---------: | :----------: |\n| FP32 MobileNet v1 1.0X  |    82     |      86     |      88      |\n| FP32 MobileNet v2 1.0X  |    49     |      53     |      55      |\n| FP32 MobileNet v3 Large |    39     |      42     |      44      |\n| FP32 MobileNet v3 Small |    12     |      14     |      14      | \n The following table presents  multi-threaded  (using as many threads as there are big cores) performance of XNNPACK library on three generations of MobileNet models and three generations of Pixel phones. \n | Model                   | Pixel, ms | Pixel 2, ms | Pixel 3a, ms |\n| ----------------------- | :-------: | :---------: | :----------: |\n| FP32 MobileNet v1 1.0X  |    43     |      27     |      46      |\n| FP32 MobileNet v2 1.0X  |    26     |      18     |      28      |\n| FP32 MobileNet v3 Large |    22     |      16     |      24      |\n| FP32 MobileNet v3 Small |     7     |       6     |       8      | \n Benchmarked on March 27, 2020 with  end2end_bench --benchmark_min_time=5  on an Android/ARM64 build with Android NDK r21 ( bazel build -c opt --config android_arm64 :end2end_bench ) and neural network models with randomized weights and inputs. \n Raspberry Pi \n The table below presents  multi-threaded  performance of XNNPACK library on three generations of MobileNet models and three generations of Raspberry Pi boards. \n | Model                   | RPi Zero W (BCM2835), ms | RPi 2 (BCM2836), ms | RPi 3+ (BCM2837B0), ms | RPi 4 (BCM2711), ms | RPi 4 (BCM2711, ARM64), ms |\n| ----------------------- | :----------------------: | :-----------------: | :--------------------: | :-----------------: | :------------------------: |\n| FP32 MobileNet v1 1.0X  |          3919            |         302         |          114           |          72         |             77             |\n| FP32 MobileNet v2 1.0X  |          1987            |         191         |           79           |          41         |             46             |\n| FP32 MobileNet v3 Large |          1658            |         161         |           67           |          38         |             40             |\n| FP32 MobileNet v3 Small |           474            |          50         |           22           |          13         |             15             |\n| INT8 MobileNet v1 1.0X  |          2589            |         128         |           46           |          29         |             24             |\n| INT8 MobileNet v2 1.0X  |          1495            |          82         |           30           |          20         |             17             | \n Benchmarked on Feb 8, 2022 with  end2end-bench --benchmark_min_time=5  on a Raspbian Buster build with CMake ( ./scripts/build-local.sh ) and neural network models with randomized weights and inputs. INT8 inference was evaluated on per-channel quantization schema. \n Publications \n \n Marat Dukhan ""The Indirect Convolution Algorithm"". Presented on  Efficient Deep Learning for Compute Vision (ECV) 2019  workshop ( slides ,  paper on ArXiv ). \n Erich Elsen, Marat Dukhan, Trevor Gale, Karen Simonyan ""Fast Sparse ConvNets"".\n   Paper on ArXiv ,  pre-trained sparse\n  models . \n Marat Dukhan, Artsiom Ablavatski ""The Two-Pass Softmax Algorithm"".\n   Paper on ArXiv . \n Yury Pisarchyk, Juhyun Lee ""Efficient Memory Management for Deep Neural Net Inference"".\n   Paper on ArXiv . \n \n Ecosystem \n Machine Learning Frameworks \n \n TensorFlow Lite . \n TensorFlow.js WebAssembly backend . \n PyTorch Mobile . \n MediaPipe for the Web . \n Alibaba HALO (Heterogeneity-Aware Lowering and Optimization) \n Samsung ONE (On-device Neural Engine) \n \n Acknowledgements \n XNNPACK is a based on  QNNPACK  library. Over time its codebase diverged a lot, and XNNPACK API is no longer compatible with QNNPACK.']",['repository_count'],@google,https://avatars.githubusercontent.com/u/1093985?u=8d30dde7394ec1b87643270479da5ca63fbf89b1&v=4,True,2016-11-02 19:20:19,609,2,1,2011-09-30T23:56:32Z,2022-11-18T05:26:22Z,,False,False,False,False,False,True,False,0,"{'C++': 6, 'R': 1, 'C': 7, 'Python': 5, 'JavaScript': 1, 'Shell': 1, 'TypeScript': 1, 'WebAssembly': 1}",2,"Silicon Valley, CA",Computer Vision,82,0.56,0.14,0.09,0
cvondrick,Carl Vondrick,,,"['Please Note \n Intel has created an excellent annotation tool with the latest technologies. https://github.com/opencv/cvat  \n The project below is archived, and no further updates are expected. \n 2009 to 2020 \n VATIC - Video Annotation Tool from Irvine, California \n VATIC is an online video annotation tool for computer vision research that\ncrowdsources work to Amazon\'s Mechanical Turk. Our tool makes it easy to build\nmassive, affordable video data sets.  \n \n INSTALLATION \n Note: VATIC has only been tested on Ubuntu with Apache 2.2 HTTP server and a\nMySQL server. This document will describe installation on this platform,\nhowever it should work any operating system and with any server. \n Download \n You can download and extract VATIC from our website. Note: do NOT run the \ninstaller as root.  \n $ wget http://mit.edu/vondrick/vatic/vatic-install.sh\n$ chmod +x vatic-install.sh\n$ ./vatic-install.sh\n$ cd vatic\n \n HTTP Server Configuration \n Open the Apache configuration file. On Ubuntu, this file is located at: \n /etc/apache2/sites-enabled/000-default\n \n If you do not use Apache on this computer for any other purpose, replace the\ncontents of the file with: \n WSGIDaemonProcess www-data\nWSGIProcessGroup www-data\n\n<VirtualHost *:80>\n    ServerName vatic.domain.edu\n    DocumentRoot /path/to/vatic/public\n\n    WSGIScriptAlias /server /path/to/vatic/server.py\n    CustomLog /var/log/apache2/access.log combined\n</VirtualHost>\n \n updating ServerName with your domain name, DocumentRoot with the path to\nthe public directory in VATIC, and WSGIScriptAlias to VATIC\'s server.py file. \n If you do use Apache for other purposes, you will have to setup a new virtual\nhost with the correct document root and script alias, as shown above. \n Make sure you have the mod_headers module enabled: \n $ sudo cp /etc/apache2/mods-available/headers.load /etc/apache2/mods-enabled\n \n After making these changes, restart Apache: \n $ sudo apache2ctl graceful\n \n SQL Server Configuration \n We recommend creating a separate database specifically for VATIC: \n $ mysql -u root\nmysql> create database vatic;\n \n The next section will automatically create the necessary tables. \n Setup \n Inside the vatic directory, copy config.py-example to config.py: \n $ cp config.py-example config.py\n \n Then open config.py and make changes to the following variables in order to\nconfigure VATIC: \n signature       Amazon Mechanical Turk AWS signature (secret access key)\naccesskey       Amazon Mechanical Turk AWS access key (access key ID)\nsandbox         If true, put into Mturk sandbox mode. For debugging.\nlocalhost       The local HTTP address: http://vatic.domain.edu/ so it\n                matches the ServerName in Apache.\ndatabase        Database connection string: for example,\n                mysql://user:pass@localhost/vatic\ngeolocation     API key from ipinfodb.com for geolocation services\n \n If you do not plan on using VATIC on Mechcanical Turk (offlien mode only), you\ncan leave the signature and accesskey empty. \n After saving results, you can then initialize the database: \n $ turkic setup --database\n \n Note: if you want to reset the database, you can do this with: \n $ turkic setup --database --reset\n \n which will require confirmation to reset in order to prevent data loss. \n Finally, you must also allow VATIC to access turkic, a major dependency: \n $ turkic setup --public-symlink\n \n ANNOTATION \n Before you continue, you should verify that the installation was correct. You\ncan verify this with: \n $ turkic status --verify\n \n If you receive any error messages, it means the installation was not complete\nand you should review the previous section. Note: If you do not plan on\nusing Mechanical Turk, you can safely ignore any errors caused by Mechanical\nTurk. \n Frame Extraction \n Our system requires that videos are extracted into JPEG frames. Our tool can \ndo this automatically for you: \n $ mkdir /path/to/output/directory\n$ turkic extract /path/to/video.mp4 /path/to/output/directory\n \n By default, our tool will resize the frames to fit within a 720x480 rectangle.\nWe believe this resolution is ideal for online video viewing. You can change \nresolution with options: \n $ turkic extract /path/to/video.mp4 /path/to/output/directory\n  --width 1000 --height 1000\n \n or \n $ turkic extract /path/to/video.mp4 /path/to/output/directory\n  --no-resize\n \n The tool will maintain aspect ratio in all cases. \n Alternatively, if you have already extracted frames, you can use the\nformatframes command to format the video into a format that VATIC understands: \n $ turkic formatframes /path/to/frames/ /path/to/output/directory\n \n The above command will read all the images in /path/to/frames and create\nhard links (soft copy) in /path/to/output/directory. \n Importing a Video \n After extracting frames, the video can be imported into our tool for \nannotation. The general syntax for this operation is: \n $ turkic load identifier /path/to/output/directory Label1 Label2 LabelN\n \n where identifier is a unique string that you will use to refer to this video,\n/path/to/output/directory is the directory of frames, and LabelX are class\nlabels that you want annotated (e.g., Person, Car, Bicycle). You can have as\nmany class labels as you wish, but you must have at least one. \n When a video is imported, it is broken into small segments typically of only a\nfew seconds. When all the segments are annotated, the annotations are merged\nacross segments because each segment overlaps another by a small margin. \n The above command specifies all of the required options, but there are many\noptions available as well. We recommend using these options. \n MTurk Options\n    --title         The title that MTurk workers see\n    --description   The description that MTurk workers see\n    --duration      Time in seconds that a worker has to complete the task\n    --lifetime      Time in seconds that the task is online\n    --keywords      Keywords that MTurk workers can search on\n    --offline       Disable MTurk and use for self annotation only\n\nCompensation Options\n    --cost                  The price advertised to MTurk workers\n    --per-object-bonus      A bonus in dollars paid for each object\n    --completion-bonus      A bonus in dollars paid for completing the task\n\nQualification Options\n    --min-approved-percent  Minimum percent of tasks the worker must have\n                            approved before they can work for you\n    --min-approved-amount   Minimum number of tasks that the worker must \n                            have completed before they can work for you\n\nVideo Options\n    --length        The length of each segment for this video in frames\n    --overlap       The overlap between segments in frames\n    --use-frames    When splitting into segments, only the frame intervals\n                    specified in this file. Each line should contain a\n                    start frame, followed by a space, then the stop frame.\n                    Frames outside the intervals in this file will be\n                    ignored.\n    --skip          If specified, request annotations only every N frames.\n    --blow-radius   When a user marks an annotation, blow away all other\n                    annotations within this many frames. If you want to\n                    allow the user to make fine-grained annotations, set\n                    this number to a small integer, or 0 to disable. By\n                    default, this is 5, which we recommend.\n \n You can also specify temporal attributes that each object label can take on.\nFor example, you may have a person object with attributes ""walking"", ""running"",\nor ""sitting"". You can specify attributes the same way as labels, except you\nprepend an ~ before the text, which bind the attribute to the previous label: \n $ turkic load identifier /path/to/output/directory Label1 ~Attr1A ~Attr1B\n  Label2 ~Attr2A ~Attr2B ~Attr2C Label3\n \n In the above example, Label1 will have attributes Attr1A and Attr1B, Label2\nwill have attributes Attr2B, Attr2B, and Attr2C and Label3 will have no \nattributes. Specifying attributes is optional. \n Gold Standard Training \n It turns out that video annotation is extremely challenging and most MTurk\nworkers lack the necessary patience. For this reason, we recommend requiring\nworkers to pass a ""gold standard"" video. When a new worker visits the task,\nthey will be redirected to a video for which the annotations are already known.\nIn order to move on to the true annotations, the worker must correctly annotate\nthe gold standard video first. We have found that this approach significantly\nimproves the quality of the annotations. \n To use this feature, import a video to be used as the gold standard: \n $ turkic load identifier-train /path/to/frames Label1 Label2 LabelN\n  --for-training --for-training-start 0 --for-training-stop 500\n  --for-training-overlap 0.5 --for-training-tolerance 0.1\n  --for-training-mistakes 1\n \n You can also use any of the options described above. Explanations for the new\noptions are as follows: \n --for-training              Specifies that this video is gold standard\n--for-training-start        Specifies the first frame to use\n--for-training-stop         Specifies the last frame to use\n--for-training-overlap      Percent overlap that worker\'s boxes must match \n--for-training-tolerance    Percent that annotations must agree temporally\n--for-training-mistakes     The number of completely wrong annotations \n                            allowed. We recommend setting this to a small,\n                            nonzero integer.\n \n After running the above command, it will provide you with an URL for you to\ninput the ground truth annotation. You must make this ground truth annotation\nas careful as possible, as it will be used to evaluate future workers. \n You can now specify that a video should use a gold standard video: \n $ turkic load identifier /path/to/output/directory Label1 Label2 LabelN\n  --train-with identifier-train\n \n When a not-yet-seen worker visits this video, they will now be redirected to\nto the training video and be required to pass the evaluation test first. \n Publishing Tasks \n When you are ready for the MTurk workers to annotate, you must publish the \ntasks, which will allow workers to start annotating: \n $ turkic publish\n \n You can limit the number of tasks that are published: \n $ turkic publish --limit 100\n \n Running above command repeatedly will launch tasks in batches of 100. You can\nalso disable all pending tasks: \n $ turkic publish --disable\n \n which will ""unpublish"" tasks that have not yet been completed. \n If you have videos that are offline only, you can see their access URLs with\nthe command: \n $ turkic publish --offline\n \n Note: for the above command to work, you must have loaded the video with the\n--offline parameter as well:  \n $ turkic load identifier /path/to/frames Person --offline\n \n Checking the Status \n You can check the status of the video annotation server with the command: \n $ turkic status\n \n This will list various statistics about the server, such as number of jobs\npublished and how many are completed. You can get even more statistics by\nrequesting additional information from Amazon: \n $ turkic status --turk\n \n which will output how much money is left in your account, among other\nstatistics. \n When all the videos are annotated, the last line will read: \n Server is offline.\n \n Retrieving Annotations \n You can get all the annotations for a video with the command: \n $ turkic dump identifier -o output.txt\n \n which will write the file ""output.txt"" where each line contains one\nannotation. Each line contains 10+ columns, separated by spaces. The\ndefinition of these columns are: \n 1   Track ID. All rows with the same ID belong to the same path.\n2   xmin. The top left x-coordinate of the bounding box.\n3   ymin. The top left y-coordinate of the bounding box.\n4   xmax. The bottom right x-coordinate of the bounding box.\n5   ymax. The bottom right y-coordinate of the bounding box.\n6   frame. The frame that this annotation represents.\n7   lost. If 1, the annotation is outside of the view screen.\n8   occluded. If 1, the annotation is occluded.\n9   generated. If 1, the annotation was automatically interpolated.\n10  label. The label for this annotation, enclosed in quotation marks.\n11+ attributes. Each column after this is an attribute.\n \n By default, the above command will not attempt to merge annotations across\nshot segments. You can request merging with the command: \n $ turkic dump identifier -o output.txt --merge --merge-threshold 0.5\n \n The --merge-threshold option is optional, but it is a number between 0 and 1\nthat represents much the paths must agree in order to merge. 1 specifies a\nperfect match and 0 specifies no match. In practice, 0.5 is sufficient. Merging\nis done using the Hungarian algorithm. \n You can also scale annotations by a factor, which is useful for when the\nvideos have been downsampled: \n $ turkic dump identifier -o output.txt -s 2.8\n \n or force it to fit within a max dimension: \n $ turkic dump identifier -o output.txt --dimensions 400x200\n \n or force it to fit within the dimensions of the original video: \n $ turkic dump identifier -o output.txt --original-video /path/to/video.mp4\n \n The command can also output to many different formats. Available formats are: \n --xml       Use XML\n--json      Use JSON\n--matlab    Use MATLAB\n--pickle    Use Python\'s Pickle\n--labelme   Use LabelMe video\'s XML format\n--pascal    Use PASCAL VOC format, treating each frame as an image\n \n The specifications for these formats should be self explanatory. \n Visualizing Videos \n You can preview the annotations by visualizing the results: \n $ turkic visualize identifier /tmp --merge\n \n which will output frames to /tmp with the bounding boxes with the file name\nas the frame number. The visualization will contain some meta information\nthat can help you identify bad workers. You can remove this meta information\nwith the option: \n $ turkic visualize identifer /tmp --merge --no-augment\n \n If you want to make a video of the visualization (e.g., with ffmpeg), it is\nuseful to renumber the frames so that they start counting at 0 and do not\nhave any gaps: \n $ turkic visualize identifier /tmp --merge --renumber\n \n If you wish to display the class label and their attributes next to the box,\nspecify the --labels option: \n $ turkic visualize identifier /tmp --labels\n \n Compensating Workers \n When you are ready, you can compensate workers: \n $ turkic compensate --default accept\n \n which will pay all workers for all outstanding tasks. We strongly recommend\npaying all workers regardless of their quality. You should attempt to pay\nworkers at least once per day. \n Finding Jobs \n If you have found a small mistake in a video and want to make\nthe correction yourself, you can start an annotation session initialized with\nthe MTurk workers annotations: \n $ turkic find --id identifier\n$ turkic find --id identifier --frame frame\n \n where identifier is the identifier for the video and frame is the frame number\nthat the error occurs. In most cases, this command will return one URL for you\nto make the corrections. If it outputs two URLs, it means the frame number\noccurs in two overlapping segments, and so you may have to make changes to both\nof the segments. You can also omit the frame argument, in which case it will\noutput all URLs for that video. \n If you want to find the HIT id, assignment ID, or worker ID for a particular\nvideo, specify the --ids parameter to the vet command: \n $ turkic find --id identifer --ids\n$ turkic find --id identifer --frame frame --ids\n \n will print a list of all the IDs for the video. If the corresponding segment\nhas been published and completed, it will list three strings: the HIT ID,\nassignment ID, and the worker ID. If the job has been published but not\nfinished, it will just list the HIT ID. If the job has not yet been published,\nit prints ""(not published)"". \n Additionally, if you want to find the job that corresponds to a particular\nHIT ID, you can use the find command: \n $ turkic find --hitid HITID\n \n Quality Control \n The gold standard does a ""pretty good"" job of weeding out bad workers.\nNonetheless, there will always be bad workers that we must identify and\ninvalidate. Our tool provides a method to sample the annotations provided by\nworkers, which you can then manually verify for correctness: \n $ turkic sample /tmp\n \n which by default will pick 3 random videos that the worker has completed, and\npick 4 random frames from each of those videos, and write visualiations to a\nfile in /tmp. You can tweak the number of videos and the number of frames with\nthe options: \n $ turkic sample /tmp --number 3 --frames 4\n \n Moreover, you can only look at work from a certain date: \n $ turkic sample /tmp --since ""yesterday""\n \n The filename will follow the format of WORKERID-JOBID.jpg. Once you have\nidentified a mallicious worker, you can block them, invalidate ALL of their\nwork, and respawn their jobs with the command: \n $ turkic invalidate workerid\n \n The options are also available: \n --no-block      invalidate and respawn, but don\'t block\n--no-publish    block and invalidate, but don\'t respawn\n \n You can also invalidate and respawn individual jobs with the command: \n $ turkic invalidate --hit hitid\n \n Listing all Videos \n You can retrieve a list of all videos in the system with: \n $ turkic list\n \n If you want just the videos that have been published: \n $ turkic list --published\n \n If you want just the videos that have been worked on: \n $ turkic list --completed\n \n If you instead want the videos that are used for gold standard: \n $ turkic list --training\n \n Finally, if you just want to count how many videos are in the system, use the\n--count option, in combination with any of the above: \n $ turkic list --count\n$ turkic list --published --count\n \n If you want statistics about each video, then give the --stats option: \n $ turkic list --stats\n \n Managing Workers \n You can list all known workers with the command: \n $ turkic workers\n \n which will dump every worker with the number of jobs they have completed. You\ncan also use this command to block and unblock workers: \n $ turkic workers --block workerid\n$ turkic workers --unblock workerid\n \n You can also search for workers by the first few letters of their ID: \n $ turkic workers --search A3M\n \n Deleting a Video \n You can delete a video at any time with: \n $ turkic delete identifier\n \n If the video has already been annotated (even partially), this command will \nwarn you and abort. You can force deletion with: \n $ turkic delete identifier --force\n \n which will REMOVE ALL DATA AND CANNOT BE UNDONE. \n REFERENCES \n When using our system, please cite: \n Carl Vondrick, Donald Patterson, Deva Ramanan. ""Efficiently Scaling Up\nCrowdsourced Video Annotation"" International Journal of Computer Vision\n(IJCV). June 2012.\n \n FEEDBACK AND BUGS \n Please direct all comments and report all bugs to: \n Carl Vondrick\nvondrick@mit.edu\n \n Thanks for using our system!', 'iHOG: Inverting Histograms of Oriented Gradients \n This software package contains tools to invert and visualize HOG features.\nIt implements the Paired Dictionary Learning algorithm described in our\npaper ""HOGgles: Visualizing Object Detection Features"" [1]. \n \n Installation \n Before you can use this tool, you must compile iHOG. Execute the \'compile\'\nscript in MATLAB to compile the HOG feature extraction code and sparse coding\nSPAMS toolbox: \n $ cd /path/to/ihog\n$ matlab\n>> compile\n \n If you run into trouble compiling the SPAMS code, you might try opening \nthe file  /path/to/ihog/spams/compile.m  and adjusting the settings for\nyour computer. \n Remember to also adjust your path so MATLAB can find iHOG: \n >> addpath(genpath(\'/path/to/ihog\'))\n \n If you want to use iHOG in your own project, you can simply drop the iHOG\ndirectory into the root of your project. \n In order to use iHOG, you must have a learned paired dictionary. By default,\niHOG will attempt to download a pretrained one from MIT for you on the first\nexecution. If you wish to download it manually, simply do: \n $ wget http://people.csail.mit.edu/vondrick/pd.mat\n \n Inverting HOG \n To invert a HOG point, use the \'invertHOG()\' function: \n >> feat = features(im, 8);\n>> ihog = invertHOG(feat);\n>> imagesc(ihog); axis image;\n \n Computing the inverse should take no longer than a second for a typical sized\nimage on a modern computer. (It may slower the first time you invoke it as it\ncaches the paired dictionary from disk.) \n Learning \n We provide a prelearned dictionary in \'pd.mat\', but you can learn your own if\nyou wish. Simply call the \'learnpairdict()\' function and pass it a directory of\nimages: \n >> pd = learnpairdict(\'/path/to/images/\', 1000000, 1000, 5, 5);\n \n The above learns a 5x5 HOG patch paired dictionary with 1000 elements and a\ntraining set size of one million window patches. Depending on the size of the\nproblem, it may take minutes or hours to complete. \n Bundled Libraries \n The iHOG package contains source code from the SPAMS sparse coding toolbox\n(http://spams-devel.gforge.inria.fr/). We have modified their code to better\nsupport 64 bit machines. \n In addition, we have included a select few files from the discriminatively\ntrained deformable parts model (http://people.cs.uchicago.edu/~rbg/latent/).\nWe use their HOG computation code and glyph visualization code. \n Questions and Comments \n If you have any feedback, please write to Carl Vondrick at  vondrick@mit.edu . \n References \n If you use our software, please cite our conference paper: \n [1] Carl Vondrick, Aditya Khosla, Tomasz Malisiewicz, Antonio Torralba.\n""HOGgles: Visualizing Object Detection Features""  International Conference\non Computer Vision (ICCV), Sydney, Australia, December 2013.', 'tmux-parallel \n This is a simple recipe for launching jobs on a cluster. \n It is mostly meant for clusters that lack proper queueing systems (such as the vision cluster at MIT).  \n Installation \n \n Install tmux \n Add this line to your  .tmux.conf : \n   bind-key y set-window-option synchronize-panes \n Put  crunch.sh  into your path \n Modify  crunch.sh  to change .csail.mit.edu domains to your domain \n \n Usage \n \n Start tmux \n Run command:  crunch N machine1 machine2 machine3  where  N  is the number of replicas you want, and the rest are the hostnames of the machines you want to connect. This will cause tmux to open N*number_of_hosts panes, with SSH connections into each machine. \n Press  Ctrl+b y  to synchronize all the panes. \n Start typing, and your input will be broadcast to all machines. \n \n Best practices \n If you write your scripts in the right way, you can use the above workflow to process data in parallel. Below is a psuedo-code that shows the basic idea. It assumes you are using a networked file system (such as NFS). \n workload = [""file1.json"", ""file2.json"", ...]\n\nrandom.shuffle(workload) # in matlab, you must remember to set the random seed\n\nfor workitem in workload:\n  in_path = ""input/"" + workitem\n  out_path = ""output/"" + workitem\n  lock_path = out_path + "".lock""\n\n  if exist(out_path) or exist(lock_path):\n    continue\n  mkdir(lock_path) # create lock file\n\n  # do heavy lifting here that eventually writes out_path\n\n  rmdir(lock_path) # remove lock file\n \n CSAIL instructions \n Remember, everytime you connect to a CSAIL machine, you must authenticate, otherwise your jobs will be killed eventually. To get around this, you can start your session with  longscreen , and start tmux inside the longscreen. ', ""Torch Starter \n This is a simple Torch7 starter package. It can be used  as a simplified kickoff point for a Torch project. \n \n I pieced together this package largely from  Torch7 resources online . I mostly just copied the code, and stripped a lot of extra functionality out, to make it easier to hack on.  \n If something is not clear, or could be made more simple, please let me know. The goal is to be simple. \n Installation \n If you are at CSAIL, you can use my Torch installation:\n bash\n. /data/vision/torralba/commonsense/torch/install/bin/torch-activate\nexport LD_LIBRARY_PATH=/data/vision/torralba/commonsense/cudnnv5/cuda/lib64:$LD_LIBRARY_PATH \n Otherwise, installation is fairly simple. You need to install:\n-  Torch7 \n-  cunn  for training on GPU\n-  cudnn  for faster training on GPU\n-  tds  for some data structures\n-  display  for graphs  \n You can install all of these with the commands:\n```bash \n install torch first \n git clone https://github.com/torch/distro.git ~/torch --recursive\ncd ~/torch; bash install-deps;\n./install.sh \n install libraries \n luarocks install cunn\nluarocks install cudnn\nluarocks install tds\nluarocks install https://raw.githubusercontent.com/szym/display/master/display-scm-0.rockspec\n``` \n Learning Resources \n \n Torch Cheat Sheet \n 60 minute blitz \n \n Model \n I trained an AlexNet-esque network on  Places365 \nwith this code, which you can  download here . This model obtains\n50% top-1 accuracy on the validation set. This is slightly worse than the published result because we didn't do averaging over 10 crops. \n If you use this model, please cite the Places2 paper (of which I am not\naffiliated).  Note this model is slightly different from the AlexNet in Caffe.\nNotable differences: no groups in the convolutions, no\nspatial normalization, batch normalizaiton, trained with Adam instead of SGD,\nand sampling with replacement. It is unclear to me whether these changes have a\nsignificant impact on performance.  \n Data Setup \n By default, we assume you have a text file that lists your dataset. This text does not store your dataset; it just lists filepaths to it, and any meta data. Each line in this text file represents one training example, and its associated category ID. The syntax of the line should be: \n <filename><tab><number> \nFor example:\n bedroom/IMG_050283.jpg    5\nbedroom/IMG_237761.jpg    5\noffice/IMG_838222.jpg     10 \nThe  <number>  should start counting at 1.  \n After you create this file, open  main.lua  and change  data_list  to point to this file. You can specify a  data_root  too, which will be prepended to each filename.  \n Training \n Define your model in the  net  variable. By default, it is AlexNet. To learn more about the modules you can use, see  nn . You can also adjust your loss with the  criterion  variable.  \n Remember to also adjust any options in  opt , such as the learning rate and the number of classes. Setting these hyperparameters is a bit of an art, but generally it is recommended to use a learning rate of  0.001  and batch size of at least  64 , but  128  or  256  may be better if you have the memory. For a systematic study, see  this paper . \n Finally, to start training, just do: \n bash\n$ CUDA_VISIBLE_DEVICES=0 th main.lua \nwhere you replace the number after  CUDA_VISIBLE_DEVICES  with the GPU you want to run on. \nYou can find which GPU to use with  $ nvidia-smi  on our GPU cluster. Note: this number is 0-indexed, unlike the rest of Torch! \n During training, it will dump snapshots to the  checkpoints/  directory every epoch. Each time you start a new experiment, you should change the  name  (in  opt ), to avoid overwriting previous experiments. The code will not warn you about this (to keep things simple). \n Evaluation \n To evaluate your model, you can use the  eval.lua  script. It mostly follows the same format as  main.lua . It reads your validation/testing dataset from a file similar to before, and sequentially runs through it, calculating both the top-1 and top-5 accuracy.  \n Graphics, Logs \n If you want to see graphics and the loss over time, in a different shell on the same machine, run this command:\n bash\n$ th -ldisplay.start 8000 0.0.0.0 \nthen navigate to  http://HOST.csail.mit.edu:8000  in your browser. Every 10th iteration it will\npush graphs.  \n On the CSAIL vision cluster, you can run this code out-of-the-box, and it will start to train\nAlexNet on the Places2 database. "", ""torch-ffmpeg \n This is a simple wrapper for FFmpeg in Torch7. There are a couple of other wrappers for FFmpeg already, but I found them difficult to install. \n This wrapper:\n- talks to FFmpeg via Unix pipes so it is easy to install\n- it is a single Lua file (and only 100 lines), so easy to modify \n- it doesn't write to disk, so it is reasonably fast \n Usage \n The  demo.lua  code shows how to use it. It's pretty easy: \n require 'torch-ffmpeg'\nvid = FFmpeg('video.mp4')\nframes = vid:read(10)\nvid:close()\n \n frames  will be a T x 3 x W x H tensor, where T is number of frames read, and W and H is width and height. In the example above, T = 10. \n Options \n If you want to specify different options, such as a different starting point or change the frame rate, you can pass additional options to FFmpeg like so: \n vid = FFmpeg('video.mp4', '-r 10') -- 10 fps\nvid = FFmpeg('video.mp4', '-ss 00:00:07') -- seek to 7sec mark\nvid = FFmpeg('video.mp4', '-s 100x100') -- downsample resolution to 100x100\nvid = FFmpeg('video.mp4', '-r 10 -s 100x100') -- frame rate and downsample\n \n Note that seeking is approximate, but fast."", ""Generating Videos with Scene Dynamics \n This repository contains an implementation of  Generating Videos with Scene Dynamics  by Carl Vondrick, Hamed Pirsiavash, Antonio Torralba, to appear at NIPS 2016. The model learns to generate tiny videos using adversarial networks. \n Example Generations \n Below are some selected videos that are generated by our model. These videos are not real; they are hallucinated by a generative video model. While they are not photo-realistic, the motions are fairly reasonable for the scene category they are trained on. \n \n Beach \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Golf \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Train Station \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Baby \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Training \n The code requires a Torch7 installation.  \n To train a generator for video, see main.lua. This file will construct the networks, start many threads to load data, and train the networks. \n For the conditional version, see main_conditional.lua. This is similar to main.lua, except the input to the model is a static image. \n To generate videos, see generate.lua. This file will also output intermediate layers,\nsuch as the mask and background image, which you can inspect manually. \n Data \n The data loading is designed assuming videos have been stabilized and flattened\ninto JPEG images. We do this for efficiency. Stabilization is computationally slow and\nmust be done offline, and reading one file per video is more efficient on NFS. \n For our stabilization code, see the 'extra' directory.\nEssentially, this will convert each video into an image of vertically\nconcatenated frames. After doing this, you create a text file listing\nall the frames, which you pass into the data loader. \n Models \n You can download our pre-trained models  here  (1 GB ZIP file). \n Notes \n The code is based on  DCGAN  and our  starter code  in  Torch7 . \n If you find this useful for your research, please consider citing our NIPS\npaper. \n License \n MIT"", 'SoundNet \n Code for paper "" SoundNet: Learning Sound Representations from Unlabeled Video "" by Yusuf Aytar, Carl Vondrick, Antonio Torralba. NIPS 2016 \n We learn rich natural sound representations by capitalizing on large amounts of unlabeled sound data collected in the wild. We leverage the natural synchronization between vision and sound to learn an acoustic representation using two-million unlabeled videos. We propose a student-teacher training procedure which transfers discriminative visual knowledge from well established visual models (e.g. ImageNet and PlacesCNN) into the sound modality using unlabeled video as a bridge. \n \n Visualization of learned conv1 filters:\n \n Requirements \n \n torch7 \n torch7 audio (and sox) \n torch7 hdf5 (only for feature extraction) \n probably a GPU \n \n Pretrained Model \n We provide pre-trained models that are trained over 2,000,000 unlabeled videos. You can download the 8 layer and 5 layer models  here . We recommend the 8 layer network. \n Recognize Categories \n You can use SoundNet to recognize sounds or as features (see next section). To recognize objects and scenes, you can use our provided script. First, create a text file where each line lists an audio file you wish to process. We use MP3 files, but most audio formats should be supported. Then, extract predictions into HDF5 files like so: \n bash\n$ list=data.txt th extract_predictions.lua \n where  data.txt  is this text file. It will write HDF5 files to the location of the input files with the scores of each category. To map the dimension index back to the category name, use the files  categories/categories_places2.txt  for scenes and  categories/categories_imagenet.txt  for objects. \n The script will also output the top scoring object and scene category. E \n Feature Extraction \n Using our network, you can extract good features for natural sounds. You can use our provided script to extract features. First, create a text file where each line lists an audio file you wish to process. We use MP3 files, but most audio formats should be supported. Then, extract features into HDF5 files like so: \n bash\n$ list=data.txt th extract_feat.lua \n where  data.txt  is this text file. It will write HDF5 files to the location of the input files with the features. You can then do anything you wish with these features.  \n By default, it will extract the  conv7  layer. You can extract other layers like so: \n ```bash\n$ list=data.txt layer=24 th extract_feat.lua\n```` \n The model will predict a matrix of size TIME x DIM where DIM is the feature dimensionality and TIME is relative to the length of the sound file. If you wish a fixed length vector, you can average-pool or max-pool over time. \n Advanced \n If you want to write your own feature extraction code, it is very easy in Torch: \n ```lua\nsound = audio.load(\'file.mp3\'):select(2,1):clone():mul(2^-23):view(1,1-1,1):cuda() \n net = torch.load(\'soundnet8_final.t7\')\nnet:forward(sound)\nfeatures = net.modules[24].output:float()\n``` \n Finetuning \n If you want to fine-tune SoundNet on your own dataset, you can use  main_finetune.lua . Create a text file that lists your MP3 files and their corresponding categories as integers. Each line lists one file in the format: filename [space] integer. For example:\n /path/to/file1.mp3 1\n/path/to/file2.mp3 5\n/path/to/file3.mp3 2 \nThe integer at the end of the text file specifies the category. Note they should start counting at 1 (not zero). \n Then, you can finetune SoundNet with the command:\n finetune=models/soundnet8_final.t7 data_list=dataset.txt data_root=/ nClasses=5 name=mynet1 th main_finetune.lua \nwhere the  data_list  variable points to your text file you created above, and  nClasses  specifies the number of categories you have. \n Note you may have to modify  main_finetune.lua  depending on your needs. This is meant to just get you started. In particular, you may need to adjust the  fineSize  and  loadSize , which specify how many samples from the waveform to use. \n Training \n The code for training is in  main_train.lua , which will train the 8 layer SoundNet model. You can also use  main_train_small.lua  to train the 5 layer SoundNet model. To start training, just do: \n bash\n$ CUDA_VISIBLE_DEVICES=0 th main_train.lua \n The code for loading the data is in  data/donkey_audio.lua . The training code will launch several threads. Each thread reads a different subset of the dataset. It will read MP3 files into a raw waveform. For the labels, it reads a large binary file that stores the class probabilities computed from ImageNet and Places networks. \n Data \n We plan to release 2 million MP3s and their corresponding class probabilities soon. Stay tuned. \n References \n If you use SoundNet in your research, please cite our paper: \n SoundNet: Learning Sound Representations from Unlabeled Video \nYusuf Aytar, Carl Vondrick, Antonio Torralba\nNIPS 2016\n']",['repository_count'],,https://avatars.githubusercontent.com/u/316755?u=c63e4c4f1abd8bee4963ee0d698b876db95a183a&v=4,False,2016-11-03 00:37:23,686,52,0,2010-06-28T18:58:15Z,2022-11-05T23:33:17Z,,False,False,False,False,False,True,False,0,"{'HTML': 2, 'Python': 5, 'C++': 1, 'MATLAB': 1, 'Lua': 5, 'Shell': 1}",10,,Computer Vision,97,0.65,0.55,0.7,0
MaximumEntropy,Sandeep Subramanian,"Researcher @NVIDIA, Formerly Mila | CMU | MSR | FAIR | Element AI",,"[""SanDeepLearn \n MNIST Multilayer Perceptron \n ```\ntrain_x, train_y, dev_x, dev_y, test_x, test_y = get_data(dataset='mnist') \n x = T.fmatrix()\ny = T.imatrix()\nfc1 = FullyConnectedLayer(\n    input_dim=train_x.shape[1],\n    output_dim=500,\n    activation='tanh'\n)\nfc2 = FullyConnectedLayer(\n    input_dim=500,\n    output_dim=500,\n    activation='tanh'\n)\nfc3 = FullyConnectedLayer(\n    input_dim=500,\n    output_dim=10,\n    activation='softmax'\n) \n params = fc1.params + fc2.params + fc3.params\nact1 = fc1.fprop(x)\nact2 = fc2.fprop(act1)\nact3 = fc3.fprop(act2)\nloss = T.nnet.categorical_crossentropy(\n    act3,\n    y\n).mean() \n print 'Compiling optimization method ...'\nupdates = Optimizer().sgd(\n    loss,\n    params,\n    lr=0.01\n) \n print 'Compiling train function ...'\nf_train = theano.function(\n    inputs=[x, y],\n    outputs=loss,\n    updates=updates\n) \n print 'Compiling evaluation function ...'\nf_eval = theano.function(\n    inputs=[x],\n    outputs=act3\n) \n print 'Training network ...'\nfor epoch in xrange(10):\n    costs = []\n    for batch in xrange(0, train_x.shape[0], 24):\n        cost = f_train(\n            train_x[batch:batch + 24],\n            train_y[batch:batch + 24]\n        )\n        costs.append(cost)\n    print 'Epoch %d Training Loss : %.3f ' % (epoch, np.mean(costs)) \n dev_predictions = f_eval(dev_x)\ntest_predictions = f_eval(test_x)\nprint 'Accuracy on dev : %.3f%% ' % (\n    100. * (np.argmax(dev_predictions, axis=1) == dev_y).mean()\n)\nprint 'Accuracy on test : %.3f%% ' % (\n    100. * (np.argmax(test_predictions, axis=1) == test_y).mean()\n) \n ``` \n MNIST Le-Net 5 \n ```\ntrain_x, train_y, dev_x, dev_y, test_x, test_y = get_data(dataset='mnist') \n train_x = train_x.reshape(\n    train_x.shape[0],\n    1,\n    int(np.sqrt(train_x.shape[1])),\n    int(np.sqrt(train_x.shape[1]))\n) \n dev_x = dev_x.reshape(\n    dev_x.shape[0],\n    1,\n    int(np.sqrt(dev_x.shape[1])),\n    int(np.sqrt(dev_x.shape[1]))\n)\ntest_x = test_x.reshape(\n    test_x.shape[0],\n    1,\n    int(np.sqrt(test_x.shape[1])),\n    int(np.sqrt(test_x.shape[1]))\n) \n x = T.tensor4()\ny = T.imatrix() \n convolution_layer0 = Convolution2DLayer(\n    input_height=train_x.shape[2],\n    input_width=train_x.shape[3],\n    filter_width=5,\n    filter_height=5,\n    num_filters=20,\n    num_feature_maps=1,\n    flatten=False,\n    wide=False\n) \n convolution_layer1 = Convolution2DLayer(\n    input_height=convolution_layer0.output_height_shape,\n    input_width=convolution_layer0.output_width_shape,\n    filter_width=5,\n    filter_height=5,\n    num_filters=50,\n    num_feature_maps=20,\n    flatten=True,\n    wide=False\n) \n fc1 = FullyConnectedLayer(800, 500, activation='tanh')\nfc2 = FullyConnectedLayer(500, 10, activation='softmax') \n params = convolution_layer0.params + convolution_layer1.params + \\\n    fc1.params + fc2.params\nact1 = convolution_layer0.fprop(x)\nact2 = convolution_layer1.fprop(act1)\nact3 = fc1.fprop(act2)\nact4 = fc2.fprop(act3)\nloss = T.nnet.categorical_crossentropy(\n    act4,\n    y\n).mean() \n print 'Compiling optimization method ...'\nupdates = Optimizer().sgd(\n    loss,\n    params,\n    lr=0.01\n) \n print 'Compiling train function ...'\nf_train = theano.function(\n    inputs=[x, y],\n    outputs=loss,\n    updates=updates\n) \n print 'Compiling evaluation function ...'\nf_eval = theano.function(\n    inputs=[x],\n    outputs=act4\n) \n print 'Training network ...'\nfor epoch in xrange(10):\n    costs = []\n    for batch in xrange(0, train_x.shape[0], 24):\n        cost = f_train(\n            train_x[batch:batch + 24],\n            train_y[batch:batch + 24]\n        )\n        costs.append(cost)\n    print 'Epoch %d Training Loss : %.3f ' % (epoch, np.mean(costs)) \n dev_predictions = f_eval(dev_x)\ntest_predictions = f_eval(test_x)\nprint 'Accuracy on dev : %.3f%% ' % (\n    100. * (np.argmax(dev_predictions, axis=1) == dev_y).mean()\n)\nprint 'Accuracy on test : %.3f%% ' % (\n    100. * (np.argmax(test_predictions, axis=1) == test_y).mean()\n) \n ``` \n Transmembrane helix prediction using Recurrent Neural Networks \n ```\nx = T.ivector()\ny = T.ivector() \n emb = EmbeddingLayer(20, 50, name='embedding')\nif network == 'rnn':\n    rnn = RNN(50, 50, name='rnn')\nelif network == 'lstm':\n    rnn = LSTM(50, 50, name='lstm')\nfc1 = FullyConnectedLayer(50, 1, name='fc') \n params = emb.params + rnn.params + \\\n    fc1.params\nembs = emb.fprop(x)\nact1 = rnn.fprop(embs)\nact2 = fc1.fprop(act1)\nloss = ((act2.transpose() - y) ** 2).mean() \n print 'Compiling optimization method ...'\nupdates = Optimizer().sgd(\n    loss,\n    params,\n    lr=0.01\n) \n print 'Compiling train function ...'\nf_train = theano.function(\n    inputs=[x, y],\n    outputs=loss,\n    updates=updates\n) \n print 'Compiling evaluation function ...'\nf_eval = theano.function(\n    inputs=[x],\n    outputs=act2\n) \n print 'Training network ...'\nfor epoch in xrange(10):\n    costs = []\n    for data_point, labels in zip(train_x, train_y):\n        cost = f_train(\n            data_point,\n            labels\n        )\n    costs.append(cost) \n print 'Epoch %d Training Loss : %f ' % (epoch, np.mean(costs))\n \n accs = []\nfor data_point, labels in zip(test_x, test_y):\n    preds = f_eval(data_point).squeeze()\n    preds = [1 if pred > 0.5 else 0 for pred in preds]\n    acc = sum([True if a == b else False for a, b in zip(preds, labels)]) \\\n        / float(len(preds))\n    accs.append(acc)\nprint 'Testing Accuracy : %f%% ' % (np.mean(accs) * 100.) \n ```"", 'This is an experimental incremental seq-seq MT system', 'wedding-template', 'NeuralMT', 'cudnn-rnn-benchmarks \n All benchmarks are reported for a host with the following specifications : \n * NVIDIA GeForce GTX TITAN X GPU\n\n* Intel(R) Core(TM) i7-5930K CPU @ 3.50GHz\n\n* CUDA 8.0, cudnnv5105\n \n These benchmarks are aimed at understanding the performance gains with using the cuDNN RNN implementation (https://devblogs.nvidia.com/parallelforall/optimizing-recurrent-neural-networks-cudnn-5/) in theano. \n The benchmarks are evaluated similar to https://github.com/glample/rnn-benchmarks that compares RNN implementations in different deep learning frameworks. Results will be integrated into the above repository eventually. \n Note: Results on regular RNNs cannot be compared as is between the two repositories as this benchmark uses the new theano GPU backend libgpuarray https://github.com/Theano/libgpuarray and different hardware specifications. \n The Recurrent Networks take as input a 3D Tensor  batch_size x seq_length x hidden_size \nand output all hidden states, compute a MSE loss at each step and compute the gradients of error with respect to each parameter.\nThe  hidden_size  specifies the size of the output and input layer of the networks. \n The code of the scripts we ran are available.\nThe code for the regular theano RNN implementations were borrowed from the rnn-benchmarks repository. \n The reported  Train  time is the average time needed to run (forward, backward) for a single training example, the smaller the better. \n A more exhaustive grid search will be done soon. \n Note: The compile times, although not reported are much faster for the cuDNN implementation.  \n LSTM - cuDNN LSTM and GRU vs FastLSTM in rnn.py \n This LSTM implementation used for these benchmarks does not use peephole connections between cell and gates. \n Depth 1 \n Batch Size 32 x Seq Len 30 \n Hidden Size 128 \n | Version | Train (µs) | Forward only (µs) |\n| ------------- | ------------- | ------------- |\n| Theano LSTM | 204.5 | 57.1 |\n| cuDNN Theano LSTM | 118.8 | 59.5 |\n| cuDNN Theano GRU | 117.4 | 57.6 | \n Hidden Size 512 \n | Version | Train (µs) | Forward only (µs) |\n| ------------- | ------------- | ------------- |\n| Theano LSTM | 530.9 | 148.1 |\n| cuDNN Theano LSTM | 223.2 | 102.4 |\n| cuDNN Theano GRU | 184.6 | 77.6 | \n Hidden Size 1024 \n | Version | Train (µs) | Forward only (µs) |\n| ------------- | ------------- | ------------- |\n| Theano LSTM | 1102.0 | 294.0 |\n| cuDNN Theano LSTM | 601.8 | 161.1 |\n| cuDNN Theano GRU | 394.8 | 136.2 | \n Batch Size 128 x Seq Len 30 \n Hidden Size 128 \n | Version | Train (µs) | Forward only (µs) |\n| ------------- | ------------- | ------------- |\n| Theano LSTM | 200.8 | 52.8 |\n| cuDNN Theano LSTM | 33.4 | 15.0 |\n| cuDNN Theano GRU | 32.2 | 14.4 | \n Hidden Size 512 \n | Version | Train (µs) | Forward only (µs) |\n| ------------- | ------------- | ------------- |\n| Theano LSTM | 491.0 | 138.2 |\n| cuDNN Theano LSTM | 100.8 | 31.7 |\n| cuDNN Theano GRU | 83.3 | 26.5 | \n Hidden Size 1024 \n | Version | Train (µs) | Forward only (µs) |\n| ------------- | ------------- | ------------- |\n| Theano LSTM | 1000.1 | 291.8 |\n| cuDNN Theano LSTM | 221.2 | 69.0 |\n| cuDNN Theano GRU | 181.3 | 59.1 | \n Depth 3 \n Batch Size 128 x Seq Len 30 \n Hidden Size 512 \n | Version | Train (µs) | Forward only (µs) |\n| ------------- | ------------- | ------------- |\n| Theano LSTM | 778.3 | 418.3 |\n| cuDNN Theano LSTM | 244.9 | 70.2 |\n| cuDNN Theano GRU | 197.1 | 55.7 | \n Hidden Size 1024 \n | Version | Train (µs) | Forward only (µs) |\n| ------------- | ------------- | ------------- |\n| Theano LSTM | 1592.8 | 882.7 |\n| cuDNN Theano LSTM | 820.6 | 256.8 |\n| cuDNN Theano GRU | 639.5 | 195.2 | \n Batch Size 128 x Seq Len 200 \n Hidden Size 512 \n | Version | Train (µs) | Forward only (µs) |\n| ------------- | ------------- | ------------- |\n| Theano LSTM | 2196.6 | 1168.1 |\n| cuDNN Theano LSTM | 1539.5 | 485.9 |\n| cuDNN Theano GRU | 1253.8 | 386.4 | \n Hidden Size 1024 \n | Version | Train (µs) | Forward only (µs) |\n| ------------- | ------------- | ------------- |\n| Theano LSTM | 5711.1 | 3427.9 |\n| cuDNN Theano LSTM | 5342.5 | 1692.1 |\n| cuDNN Theano GRU | 4163.4 | 1274.5 |', 'personal_website', ""Sequence to Sequence models with PyTorch \n This repository contains implementations of Sequence to Sequence (Seq2Seq) models in PyTorch \n At present it has implementations for :  \n * Vanilla Sequence to Sequence models\n\n* Attention based Sequence to Sequence models from https://arxiv.org/abs/1409.0473 and https://arxiv.org/abs/1508.04025\n\n* Faster attention mechanisms using dot products between the **final** encoder and decoder hidden states\n\n* Sequence to Sequence autoencoders (experimental)\n \n Sequence to Sequence models \n A vanilla sequence to sequence model presented in https://arxiv.org/abs/1409.3215, https://arxiv.org/abs/1406.1078 consits of using a recurrent neural network such as an LSTM (http://dl.acm.org/citation.cfm?id=1246450) or GRU (https://arxiv.org/abs/1412.3555) to encode a sequence of words or characters in a  source  language into a fixed length vector representation and then deocoding from that representation using another RNN in the  target  language. \n \n An extension of sequence to sequence models that incorporate an attention mechanism was presented in https://arxiv.org/abs/1409.0473 that uses information from the RNN hidden states in the source language at each time step in the deocder RNN. This attention mechanism significantly improves performance on tasks like machine translation. A few variants of the attention model for the task of machine translation have been presented in https://arxiv.org/abs/1508.04025. \n \n The repository also contains a simpler and faster variant of the attention mechanism that doesn't attend over the hidden states of the encoder at each time step in the deocder. Instead, it computes the a single batched dot product between all the hidden states of the decoder and encoder once after the decoder has processed all inputs in the target. This however comes at a minor cost in model performance. One advantage of this model is that it is possible to use the cuDNN LSTM in the attention based decoder as well since the attention is computed after running through all the inputs in the decoder. \n Results on English - French WMT14 \n The following presents the model architecture and results obtained when training on the WMT14 English - French dataset. The training data is the english-french bitext from Europral-v7. The validation dataset is newstest2011 \n The model was trained with following configuration \n * Source and target word embedding dimensions - 512\n\n* Source and target LSTM hidden dimensions - 1024\n\n* Encoder - 2 Layer Bidirectional LSTM\n\n* Decoder - 1 Layer LSTM\n\n* Optimization - ADAM with a learning rate of 0.0001 and batch size of 80\n\n* Decoding - Greedy decoding (argmax)\n \n | Model | BLEU | Train Time Per Epoch |\n| ------------- | ------------- | ------------- |\n| Seq2Seq | 11.82 | 2h 50min |\n| Seq2Seq FastAttention | 18.89 | 3h 45min |\n| Seq2Seq Attention | 22.60 | 4h 47min | \n Times reported are using a Pre 2016 Nvidia GeForce Titan X \n Running \n To run, edit the config file and execute python nmt.py --config  \n NOTE: This only runs on a GPU for now."", 'IFT-6266 Class Project \n Conditional Image Generation - Inpainting with MSCOCO \n Introduction \n I took a pretty standard approach to solving the problem of inpainting an image. Given a 64 x 64 image from MSCOCO, with it\'s center (32 x 32) masked out, I built a fully convolutional architecture that attempts to predict the center with an L2 reconstruction loss + a Wasserstein GAN (WGAN) objective. Some relevant literature that I used when building this model was \n \n Convolutional autoencoders with an L2 reconstruction objective - https://pdfs.semanticscholar.org/1c6d/990c80e60aa0b0059415444cdf94b3574f0f.pdf \n Mixing an L2 reconstruction objective with a GAN objective was presented in Context Encoders: Feature Learning by Inpainting - https://arxiv.org/abs/1604.07379.  \n Wasserstein GANs - https://arxiv.org/abs/1701.07875 \n Pix2Pix (Image-to-Image Translation with Conditional Adversarial Networks) - Fully convolutional model with an adverserial objective - https://arxiv.org/abs/1611.07004 \n DCGAN (Deep Convolutional Generative Adverserial Networks) - https://arxiv.org/abs/1511.06434 \n DenseNet (Denseley Connected Convolutional Networks) - https://arxiv.org/abs/1608.06993 \n UNet - https://arxiv.org/abs/1505.04597 \n Sentence Embeddings (Simple but tough to beat baseline for sentence embeddings) - https://openreview.net/pdf?id=SyK00v5xx \n Adaptation of Word2Vec (Two/Too Simple Adaptations of Word2Vec for Syntax Problems) - http://www.cs.cmu.edu/~lingwang/papers/naacl2015.pdf \n \n An example of masked image  and it\'s corresponding ground-truth is shown in the image below. \n \n Architecture \n I used a fully convolutional architecture for the generator, that is very similar to the DCGAN generator with strided convolution and transpose convolution layers, batch-normalization and ReLU activations. The difference between my generator and the DCGAN generator is that I don\'t have the initial linear transformation that they use to reshape the noise prior. The discriminator is also similar to the DCGAN discriminator with strided convolutions, batch normalization and ReLUs. \n As in the pix2pix paper, I added skip connections between the feature maps produced by the regular convolutions and the transpose convoltion feature maps. The feature maps are concatenated along the channel axis. \n Generator \n | Layer | Filters | Input Shape | Output Shape | Kernel Size |\n| ------------- | ------------- | ------------- | ------------ | ------------ |\n| Conv1 | 32 | 3 x 64 x 64 | 32 x 32 x 32 | 4 x 4 |\n| Conv2 | 64 | 32 x 32 x 32 | 64 x 16 x 16 | 4 x 4 |\n| Conv3 | 96 | 64 x 16 x 16 | 96 x 8 x 8 | 4 x 4 |\n| Conv4 | 128 | 96 x 8 x 8 | 128 x 4 x 4 | 4 x 4 |\n| TConv1 | 96 | 128 x 4 x 4 | 96 x 8 x 8 | 4 x 4 |\n| TConv2 | 64 | (96 + 96) x 8 x 8 | 64 x 16 x 16 | 4 x 4 |\n| TConv3 | 1 | (64 + 64) x 16 x 16 | 1 x 32 x 32 | 4 x 4 | \n The table above presents the generator\'s architecture where Conv  refers to a regular convolution with a stride of (2, 2) and TConv  refers to a transpose convolution, also with a stride of (2, 2). \n Discriminator \n | Layer | Filters | Input Shape | Output Shape | Kernel Size |\n| ------------- | ------------- | ------------- | ------------ | ------------ |\n| Conv1 | 32 | 3 x 32 x 32 | 32 x 16 x 16 | 4 x 4 |\n| Conv2 | 64 | 32 x 16 x 16 | 64 x 8 x 8 | 4 x 4 |\n| Conv3 | 96 | 64 x 8 x 8 | 96 x 4 x 4 | 4 x 4 |\n| Conv4 | 128 | 96 x 4 x 4 | 128 x 2 x 2 | 4 x 4 |\n| Conv5 | 256 | 128 x 2 x 2 | 256 x 1 x 1 | 2 x 2 | \n Training \n As evident from the Generator architecture, the model takes a 3 channel 64 x 64 image with the center masked out and tries to predict the center 32 x 32 square. The discriminator (when using the adverserial objective) takes ""fake"" 32 x 32 samples from our Generator and ""real"" 32 x 32 samples from our training data and is trained to distinguish between them. The generator is trained to fool the discriminator. This is formulated as a minimax game as described in Ian Goodfellow\'s original GAN paper. The L2 reconstruction objective tries to minimize the squared error between the predicted image center and the ground truth. The L2 + GAN objective simply adds these two losses together and backpropagates it through the network. In this work, I used a WGAN instead of a regular GAN which from an implementation perspective, simply removes  log  from the GAN objective and clamps the weights of the discriminator. \n Adding Image Captions \n The MSCOCO dataset contains several human written captions of each image. These written descriptions could improve the ability of our model to predict the masked out center. For example, in the image presented above, the sheep are masked out leaving only the green fields in the background visible. The model could therefore inpaint in many possible ways - with an empty field, a deer or a bear etc. However, if the model was present with a caption of the image saying ""Three sheep in a field of grass"", this could reduce it\'s search space. \n In this project, I explored using a fixed representation of a caption using the ""Simple but tough to beat baseline for sentence embeddings"". In a nutshell, the paper uses a weighted average of the individual word embeddings for each word in the caption. The word embeddings for each word are trained on a large external corpus. In this paper, I used an adapted word2vec implementation (Two/Too Simple Adaptations of Word2Vec for Syntax Problems) trained on the English Gigaword 5 corpus. The weights of each word are inversely proportional to their word frequency in the text8 corpus. I then used the approach described in the paper to extract the first principal component on the sentence embeddings of the MSCOCO training captions and applied the described transformation on the training and dev set. \n When using captions, I modified my generator to downsample all the way to 1 x 1 and then contatenated the embedding of the image\'s caption before upsampling with transpose convolutions. \n Caption Generator \n | Layer | Filters | Input Shape | Output Shape | Kernel Size |\n| ------------- | ------------- | ------------- | ------------ | ------------ |\n| Conv1 | 32 | 3 x 64 x 64 | 32 x 32 x 32 | 4 x 4 |\n| Conv2 | 64 | 32 x 32 x 32 | 64 x 16 x 16 | 4 x 4 |\n| Conv3 | 96 | 64 x 16 x 16 | 96 x 8 x 8 | 4 x 4 |\n| Conv4 | 128 | 96 x 8 x 8 | 128 x 4 x 4 | 4 x 4 |\n| Conv5 | 256 | 128 x 4 x 4 | 256 x 2 x 2 | 4 x 4 |\n| Conv6 | 256 | 256 x 2 x 2 | 256 x 1 x 1 | 2 x 2 |\n| TConv1 | 128 | (256 + 300emb) x 1 x 1 | 128 x 2 x 2 | 4 x 4 |\n| TConv2 | 96 | (128 + 256) x 2 x 2 | 96 x 4 x 4 | 4 x 4 |\n| TConv3 | 64 | (96 + 128) x 4 x 4 | 64 x 8 x 8 | 4 x 4 |\n| TConv4 | 32 | (64 + 96) x 8 x 8 | 32 x 16 x 16 | 4 x 4 |\n| TConv5 | 1 | (32 + 64) x 16 x 16 | 1 x 32 x 32 | 4 x 4 | \n Hyperparameters \n \n Optimizer - ADAM for both the generator and discriminator with a learning rate of 2e-3 \n Discriminator weight clamping -  (-0.03, 0.03) if discriminator is trained 5 times for every generator update else (-0.05, 0.05).  \n Batch size - 32 \n \n Results \n In this section, I will present some of my cherry-picked inpainted images. All samples are inpaintings of the MSCOCO dev set. \n L2 \n \n \n \n \n \n \n L2 + WGAN \n \n \n \n \n \n \n L2 + WGAN + Caption \n \n \n \n \n \n \n Conclusion & Discussion \n I experimented with a simple method to solve the inpainting problem using a convolutional autoencoder with a WGAN objective. I also added information from image captions using a simple baseline for sentence embeddings. \n The convolutional autoencoder with an L2 reconstruction objective yields reasonable results and appears to mostly predict the center as a smoothened interpolation of its surrounding. Adding the WGAN objective appeared to yield some structure to the predictions especially in the images that required inpainting the foreground and not just the background. Further, adding captions, seemed to help a little more in the quality of samples and also accelerated training (50-60 epochs for L2 + WGAN vs 30-40 epochs with captions). It also appeared to focus on drawing actual objects instead of just smoothing images with captions where a woman\'s face (first row 5th column) can be seen (if you squint really hard).', 'welcome_tutorials \n Various tutorials given for welcoming new students at MILA.', '[WIP] PyTorch Notebooks \n A collection of Jupyter Notebooks to serve as an introduction to PyTorch. \n This repository is a basically a clone of https://github.com/mila-udem/welcome_tutorials/tree/master/pytorch that I wrote a year ago, with modifications for PyTorch 1.0. \n It contains \n \n An Introduction to the Torch Tensor Library and examples of basic operations \n Autograd \n Utilities for implementing Neural Networks and Optimization Algorithms \n Image classification example - MNIST & CIFAR-10 \n Neural Machine Translation with Sequence to Sequence models \n Generative Adversarial Networks - MNIST \n']",['repository_count'],NVIDIA,https://avatars.githubusercontent.com/u/9114321?u=5a21fdc2ace44b69d5ac2d6401715bb3de1a64d0&v=4,False,2016-11-18 14:50:39,208,11,0,2014-10-09T15:07:22Z,2022-11-15T01:28:27Z,,False,False,False,False,False,True,False,0,"{'Java': 2, 'Python': 14, 'CSS': 1, 'Jupyter Notebook': 4, 'HTML': 2, 'JavaScript': 1}",2,"San Fransisco, Bay Area",AI for Finance,33,0.61,0.69,0.19,0
