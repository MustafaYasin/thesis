username,fullName,bio,email,repository_count,company,avatar_url,isHireable,star_time,followers,following,organizations,repositories,createdAt,updatedAt,twitterUsername,isGitHubStar,isCampusExpert,isDeveloperProgramMember,isSiteAdmin,isViewer,anyPinnableItems,viewerIsFollowing,sponsors,primary_language,yearsofExperience,location,domainofExpertise
alexbw,Alex Wiltschko,,alex.bw@gmail.com,"['Sampling Inference for Bayesian HSMMs and HMMs \n This is a Python library for approximate unsupervised sampling inference in\nBayesian Hidden Markov Models (HMMs) and explicit-duration Hidden semi-Markov\nModels (HSMMs), focusing on the Bayesian Nonparametric extensions, the HDP-HMM\nand HDP-HSMM, via the weak-limit approximation. \n There are also some plugins to extend the functionality: \n \n factorial models \n autoregressive models \n collapsed HDP sampling inference . \n \n The inference can be run in parallel over multiple cores and/or multiple\nmachines (even on EC2!) using  ipython \'s\nexcellent  ipython.parallel  module. Someday I might even document how to do\nit! \n Installing \n You can clone this library and its dependencies with \n git clone --recursive git://github.com/mattjj/pyhsmm.git \n The library depends on  numpy ,  scipy , and, for visualization,  matplotlib . \n Disabling assertions may speed things up; to disable assertions, run your\nCPython interpreter with the  -O  flag. \n A Simple Demonstration \n Here\'s how to draw from the HDP-HSMM posterior over HSMMs given a sequence of\nobservations. (The same example, along with the code to generate the synthetic\ndata loaded in this example, can be found in  examples/basic.py .) \n Let\'s say we have some 2D data in a data.txt file: \n bash\n head -5 data.txt\n-3.711962552600095444e-02 1.456401745267922598e-01\n7.553818775915704942e-02 2.457422192223903679e-01\n-2.465977987699214502e+00 5.537627981813508793e-01\n-7.031638516485749779e-01 1.536468304146855757e-01\n-9.224669847039665971e-01 3.680035337673161489e-01 \n In Python, we can plot the data in a 2D plot, collapsing out the time dimension: \n ```python\nimport numpy as np\nfrom matplotlib import pyplot as plt \n data = np.loadtxt(\'data.txt\')\nplt.plot(data[:,0],data[:,1],\'kx\')\n``` \n \n We can also make a plot of time versus the first principal component: \n python\nfrom pyhsmm.util.plot import pca_project_data\nplt.plot(pca_project_data(data,1)) \n \n To learn an HSMM, we\'ll use  pyhsmm  to create an  HSMM  instance using some\nreasonable hyperparameters. We\'ll ask this model to infer the number of states\nas well (since an HDP-HSMM is instantiated by default), so we\'ll give it an\n Nmax  parameter: \n ```python\nimport pyhsmm\nimport pyhsmm.basic.distributions as distributions \n obs_dim = 2\nNmax = 25 \n obs_hypparams = {\'mu_0\':np.zeros(obs_dim),\n                \'sigma_0\':np.eye(obs_dim),\n                \'kappa_0\':0.3,\n                \'nu_0\':obs_dim+5}\ndur_hypparams = {\'alpha_0\':2*30,\n                 \'beta_0\':2} \n obs_distns = [distributions.Gaussian( obs_hypparams) for state in range(Nmax)]\ndur_distns = [distributions.PoissonDuration( dur_hypparams) for state in range(Nmax)] \n posteriormodel = pyhsmm.models.HSMM(\n        alpha=6.,gamma=6., # better to sample over these; see concentration-resampling.py\n        init_state_concentration=6., # pretty inconsequential\n        obs_distns=obs_distns,\n        dur_distns=dur_distns,\n        trunc=60) # duration truncation speeds things up when it\'s possible\n``` \n (The first two arguments set the ""new-table"" proportionality constant for the\nmeta-Chinese Restaurant Process and the other CRPs, respectively, in the HDP\nprior on transition matrices. For this example, they really don\'t matter at\nall, but on real data it\'s much better to infer these parameters, as in\n examples/concentration_resampling.py .) \n The  trunc  parameter is an optional argument that can speed up inference: it\nsets a truncation limit on the maximum duration for any state. If you don\'t\npass in the  trunc  argument, no truncation is used and all possible state\nduration lengths are considered. \n Then, we add the data we want to condition on: \n python\nposteriormodel.add_data(data) \n (If we had multiple observation sequences to learn from, we could add them to the\nmodel just by calling  add_data()  for each observation sequence.) \n Now we run a resampling loop. For each iteration of the loop, all the latent\nvariables of the model will be resampled by Gibbs sampling steps, including the\ntransition matrix, the observation means and covariances, the duration\nparameters, and the hidden state sequence. We\'ll also copy some samples so that\nwe can plot them. \n python\nmodels = []\nfor idx in progprint_xrange(150):\n    posteriormodel.resample_model()\n    if (idx+1) % 10 == 0:\n        models.append(copy.deepcopy(posteriormodel)) \n Now we can plot our saved samples: \n python\nfig = plt.figure()\nfor idx, model in enumerate(models):\n    plt.clf()\n    model.plot()\n    plt.gcf().suptitle(\'HDP-HSMM sampled after %d iterations\' % (10*(idx+1)))\n    plt.savefig(\'iter_%.3d.png\' % (10*(idx+1))) \n \n I generated these data from an HSMM that looked like this: \n \n So the posterior samples look pretty good! \n Speed \n HSMMs constitute a much more powerful model class than plain-old HMMs, and that\nenhanced power comes with a computational price: each sampling iteration for an\nHSMM is much slower than that of an HMM. But that price is often worthwhile if\nyou want to place priors on state durations or have the model learn duration\nstructure present in the data. (In the example, strong duration structure is\nwhat made the inference algorithm latch onto the correct explanation so\neasily.) In addition, the increased cost of each iteration often pays for\nitself, since HSMM samplers empirically seem to take fewer iterations to\nconverge than comparable HMM samplers. \n Using my nothing-special i7-920 desktop machine and a NumPy/SciPy built against\nIntel\'s MKL BLAS (which generally outperforms ATLAS for vectorized operations)\nalong with the Eigen-backed classes, here\'s how long the demo iterations took: \n $ python examples/hsmm.py\n.........................  [  25/100,    0.05sec avg,    3.95sec ETA ]\n.........................  [  50/100,    0.05sec avg,    2.64sec ETA ]\n.........................  [  75/100,    0.05sec avg,    1.34sec ETA ]\n.........................  [ 100/100,    0.05sec avg,    0.05sec ETA ]\n   0.05sec avg,    5.21sec total \n Extending the Code \n To add your own observation or duration distributions, implement the interfaces\ndefined in  basic/abstractions.py . Also see the plugins. To get a flavor of\nthe style, see  pybasicbayes . \n Contributors \n Contributions by Chia-ying Lee. \n References \n \n \n Matthew J. Johnson and Alan S. Willsky.  Bayesian Nonparametric Hidden\n  Semi-Markov Models .\n  Journal of Machine Learning Research (JMLR), 14:673â€“701, February 2013. \n \n \n Matthew J. Johnson and Alan S. Willsky,  The Hierarchical Dirichlet Process\n  Hidden Semi-Markov Model . 26th\n  Conference on Uncertainty in Artificial Intelligence (UAI 2010), Avalon,\n  California, July 2010. \n \n \n bibtex\n@article{johnson2013hdphsmm,\n    title={Bayesian Nonparametric Hidden Semi-Markov Models},\n    author={Johnson, Matthew J. and Willsky, Alan S.},\n    journal={Journal of Machine Learning Research},\n    pages={673--701},\n    volume={14},\n    month={February},\n    year={2013},\n}', 'An analgesic for high-performance audio on iOS and OSX. \n Really fast audio in iOS and Mac OS X using Audio Units is hard, and will leave you scarred and bloody. What used to take days can now be done with just a few lines of code. \n Getting Audio \n objective-c\nNovocaine *audioManager = [Novocaine audioManager];\n[audioManager setInputBlock:^(float *newAudio, UInt32 numSamples, UInt32 numChannels) {\n    // Now you\'re getting audio from the microphone every 20 milliseconds or so. How\'s that for easy?\n    // Audio comes in interleaved, so,\n    // if numChannels = 2, newAudio[0] is channel 1, newAudio[1] is channel 2, newAudio[2] is channel 1, etc.\n}];\n[audioManager play]; \n Playing Audio \n objective-c\nNovocaine *audioManager = [Novocaine audioManager];\n[audioManager setOutputBlock:^(float *audioToPlay, UInt32 numSamples, UInt32 numChannels) {\n    // All you have to do is put your audio into ""audioToPlay"".\n}];\n[audioManager play]; \n Does anybody actually use it? \n Yep. Novocaine is result of three years of work on the audio engine of  Octave ,  Fourier  and  oScope , a powerful suite of audio analysis apps. Please do check them out! \n A thing to note: \n The RingBuffer class is written in C++ to make things extra zippy, so the classes that use it will have to be Objective-C++. Change all the files that use RingBuffer from MyClass.m to MyClass.mm. \n Want some examples? \n Inside of ViewController.mm are a bunch of tiny little examples I wrote. Uncomment one and see how it sounds. \nDo note, however, for examples involving play-through, that you should be using headphones. Having the \nmic and speaker close to each other will produce some gnarly feedback.   \n Want to learn the nitty-gritty of Core Audio? \n If you want to get down and dirty, if you want to get brave and get close to the hardware, I can only point you to the places where I learned how to do this stuff. Chris Adamson and Michael Tyson are two giants in the field of iOS audio, and they each wrote indispensable blog posts ( this is Chris\'s ,  this is Michael\'s ). Also, Chris Adamson now has a  whole gosh-darned BOOK on Core Audio . I would have done unspeakable things to get my hands on this when I was first starting. \n', 'My Code for the Netflix Prize \n I\'m not aware of folks having published their code for the Netflix Prize. Here\'s mine. \nUnder the team name ""Hi!"", I competed alone in college. I did it mostly for fun, and to learn modern machine learning techniques. It was an incredibly valuable, but strenuous, time. Well worth it on all fronts, though.  \nI peaked out at #45 or so, and then dropped out to work on my senior thesis, and came in #145 or so.   \nWhat I learned in the process was that smarter wasn\'t always better -- make an algorithm, and then scale it up, and then make a dozen tweaks to it, and then average all of the results together. That\'s how you climbed the leaderboard.    \n Anyhoo, I haven\'t touched this code in awhile, but perhaps it\'ll be useful to folks interested in competitive data mining. \nSpecifically, the lessons I learned: \n \n Get the raw data into a saved and manageable format  fast . The easier it is to load your data in and start mutating it, the better. \n If doing simple pivots on your data is hard, and slows you down from visualizing whats in your data, spend time making data structures which make that easy.   \n Generalize. Iterate. If you have a method you think will work, but it has a lot of knobs, and you don\'t know the best way to set those knobs, make it easy for you to try  every possible iteration . There is often not a good way to figure out what the  best  approach is. You will have to try many of them in order to build up an intuition. Specifically, that means (for me) a pluggable architecture. If there\'s ten ways to try a particular step, make sure you write your overarching algorithm so that it takes a function that you can pass to it, as opposed to having a method hardwired in the code. That way, you can hotswap all your ideas.   \n Speed is a feature. Of course you make sure it works first. But your goal is to see if something works. If an algorithm takes a day to run, but you can spend five hours making it run in 1/3 of a day, do it. You\'ll be running it over and over again, and you\'ll learn more if you can iterate.  \n \n As for the technical nitty-gritty, everything that\'s speed sensitive is written in Cython, which was the best balance of speed and convenience in 2009. If I were to do it al again, I would use (Numba)[http://github.com/numba/numba].   \n The original data is gone, I believe, but I might have it stored somewhere. I\'ll look for that.  ', ""NURBS - Non Uniform Rational B-Splines. \n This python module is a revival and update of Runar Tenfjord's NURBS toolbox, which itself\nis a port of Mark Spink's SCILAB/MATLAB tolbox to python with help of numpy. \n Dependency \n Python 2.0 or above \nNumPy \nDislin -> optional but recomended \nPyOpenGL -> optional   \n Installation \n Just run   python setup.py install   \n License \n Runar Tenfjord originally relased this package under the GPL Version 2 license,  \nso I suppose that means it has to stay that way.    \n Originally by: Runar Tenfjord, runten@netcom.no \nMinor updates to work with NumPy by Alex Wiltschko  "", 'cuda-tests \n Gotta learn some CUDA stuff', 'pypatent \n Scrape patents from the USPTO', 'airruler \n A ruler. Made of air. ', 'paralleltools \n A summary of parallelizing moderate amounts of work in Python', 'nsgt \n Non-stationary Gabor transforms (GitHub mirror of http://grrrr.org/research/software/nsgt/)', 'IDA \n This code accompanies the publication from the Whitesides lab to Lab on a Chip, concerning automated analysis of red blood cell health using affordable field tests. Specifically, this code implements the automated extraction of scanned AMPS tubes from flatbed scanner images, the distillation of those images into 1D data traces, and then the automated identification of the anemic disease state of the blood in those 1D data traces, as well as the prediction of continuously-varying red blood cell (RBC) parameters. \n Installation \n This code requires Python, as well as some extra 3rd-party libraries. All routines have only been tested on Mac OS, but should work on Linux. No guarantees for Windows.\n1) First, we recommend using the ""Anaconda"" Python distribution, specifically the Python 2.7 version.  Download and install here .\n1) With anaconda installed, you will need one extra library, to read TIFF files.\n pip install imageio \n1) You should be able to run the notebooks now. Inside this code repository, start up an IPython notebook:\n jupyter notebook \n1) You should now be able to browser around the ""extraction"" and ""analysis"" folders, which contain the relevant code. \n Running extraction \n The first step required will be to prepare the raw data of TIFF file scans of 4x3=12 tubes from a flatbed scanner. We will also have to associate metadata for each patient that the blood in a tube was drawn from.\nOur end goal will be a 1D array for each tube, along with the corresponding patient data (anemic state & RBC parameters). \n The extraction algorithm is explained step-by-step in the notebook  extraction/Explaining the Extraction Algorithm.ipynb .\nThe implementation we use in the paper (which also fuses patient metadata with AMPS image data) can be found in  extraction/Data Extraction Pipeline.ipynb . Most of the code in this notebook is specific to the particular Excel file format that we used to record patient metadata, and may have to be largely rewritten for reuse. \n Running analysis \n There are two sets of analyses done in the paper: classification and regression. \n Running classification analysis \n The goal of this analysis is to predict disease state only from the 1D data trace extracted from each AMPS tube. We use logistic regression, a linear classifier, and transform the 1D data representation using PCA to remove redundancy and constrain the input dimensionality of the problem. We also used Bayesian Optimization (bayesopt) to tune the hyperparameters of the problem, including the specific output dimension of PCA, the regularization parameter of logistic regression, and some image preprocessing parameters. Unfortunately, the service we used for bayesopt is no longer available. If you would like to automatically tune these parameters, we recommend either using random search (a surprisingly effective hueristic), or the open source library ""Spearmint"", upon which the now-defunct service we used was based, or products from the company SigOpt, which also implements bayesopt. The file we used to automatically tune these hypers is  classify.py .\nThe best set of hyperparameters is stored in the notebook  Analyzing best classification.ipynb .\nThis notebook examines ROC performance of the classifier for discriminating different anemia types, as well as the effect of centrifugation time of the tube on IDA classification AUC performance. \n Running regression analysis. \n Similar to above, we used a defunct bayesopt service to automatically tune some parameters of this algorithm. The original file is in  regress.py . The original methodology will work well, even without automated tuning.\nThe best set of hyperparameters is stored in the notebook  Analyzing best regression.ipynb . \n License \n See the LICENSE file. It is under a GPL license, and this code may only be used for academic purposes.', ""adabayes \n To do: \n \n [x] Find candidate last layer features (DeCAF, Overfeat) \n [x] Find AlexNet code (5 convolutional layers with max-pooling, then three fully connected layers) \n [X]  Find download script for MNIST dataset \n [X]  Find download script for CIFAR10 dataset \n [x] Fork torch-dist repo \n [x] Update torch-dist repo for OS X 10.10 install \n [x] Add new required submodules to distro (nnx, iTorch, ccn2, cunnx, cudnn, sdl2, cutorch) \n [x] Figure out model serialization and loading \n Built-in model serialization.  Loading  and  saving . \n There is also facebook's  Thrift library , which I haven't seen many examples for. \n \n \n [ ] Get the data out of the  DataSource  models that we're using \n [ ] Train and Whetlab a net on MNIST (to get the whole workflow going) \n [ ] Train and Whetlab a net on CIFAR10 \n [ ] Train and Whetlab a net on STL10 \n [ ] Build dumb ensemble on CIFAR10 \n [ ] Build dumb ensemble on STL10 \n [ ] Grok boosting criterion \n [ ] Whetlab a net with progressive ensembling on CIFAR10 \n [ ] Whetlab a net with progressive ensembling on STL10 \n [ ] Whetlab a net with progressive ensembling on the last-layer features of ImageNet \n [ ] Whetlab a net with progressive ensembling and tuned class weights on CIFAR10 \n [ ] Whetlab a net with progressive ensembling and tuned class weights on STL10 \n [ ] Extract last-layer ImageNet features \n [ ] Host last-layer ImageNet features \n [ ] Build download script for last-layer ImageNet features \n [ ] Train and Whetlab a net on last-layer features on ImageNet \n [ ] Build a dumb ensemble on ImageNet \n [ ] Whetlab a net with progressive ensembling and tuned class weights on the last-layer features of ImageNet \n \n Collecting some resources \n AlexNet:\nhttps://github.com/soumith/convnet-benchmarks/blob/master/torch7/imagenet_winners/alexnet.lua \n I think this is NiN:\nhttps://github.com/soumith/convnet-benchmarks/blob/master/torch7/imagenet_winners/googlenet.lua \n OverFeat:\nhttps://github.com/facebook/fbcunn/blob/master/examples/imagenet/models/overfeat_cunn.lua \n Multiclass criterion:\nhttps://github.com/torch/nn/blob/master/doc/criterion.md#classnllcriterion \n Some other interesting nets:\nhttps://github.com/culurciello/profiling \n Bunch of demos, not all nets:\nhttps://github.com/torch/demos \n Overfeat features:\nhttp://cilvr.nyu.edu/doku.php?id=software:overfeat:start \n Decaf ImageNet submission:\nhttps://github.com/UCB-ICSI-Vision-Group/decaf-release/wiki/imagenet \n Some other net implementations:\nhttps://github.com/eladhoffer/ImageNet-Training"", ""Conda recipes for installing packages from the Torch ecosystem. \n NOTE: No longer maintained. \n To install packages \n ``` \n Install anaconda if you don't have it (instructions here for OS X) \n wget http://repo.continuum.io/miniconda/Miniconda-latest-MacOSX-x86_64.sh\nsh Miniconda-latest-MacOSX-x86_64.sh -b -p $HOME/anaconda \n Add anaconda to your $PATH \n export PATH=$HOME/anaconda/bin:$PATH \n Install Lua & Torch \n conda install lua=5.2 lua-science -c alexbw \n Available versions of Lua: 2.0, 5.1, 5.2, 5.3 \n 2.0 is LuaJIT \n ``` \n To build packages \n ``` \n Install anaconda if you don't have it (instructions here for OS X) \n wget http://repo.continuum.io/miniconda/Miniconda-latest-MacOSX-x86_64.sh\nsh Miniconda-latest-MacOSX-x86_64.sh -b -p $HOME/anaconda \n Add anaconda to your $PATH \n export PATH=$HOME/anaconda/bin:$PATH \n Get the newest version of conda, as well as some conda build tools \n conda update conda -y\nconda install conda-build anaconda-client -y \n Build all packages \n sh build_all.sh \n Ideally, all you have to do to install everything is this \n conda install lua=5.2 lua-science\n``` \n TODO:\n - https://github.com/AlexMili/torch-dataframe\n - https://github.com/twitter/torch-ipc\n - https://github.com/twitter/torch-distlearn\n - https://github.com/twitter/torch-dataset \n Resources: \n \n \n Making packages relocatable (LuaJIT hard-codes path) \n \n \n Build instructions for luarocks \n \n \n Build instructions for Lua \n \n \n Patching files with git diffs is finicky \n \n \n Linking against readline  (need a few extra flags, and link against -lncursesw, not -lncurses) \n \n \n Upgrading old Debian .  Also this . \n \n \n Misc notes:\n metadata:ns_cfg() â€” defines for YAML directives\nmain_build: â€”\xa0defines version numbers \nenviron:get_lua_include_dir() â€”\xa0uses version numbers to locate the include directory\nconfig:Config._get_lua â€”\xa0uses version numbers to locate the binary\nThis is where the linked package name is converted into what is used"", ""validata \n Continuous integration for your data \n We do continuous integration on code. Why not data?\nValidata is a small package to run basic sanity checks on your data.\nI haven't found anything that aggregates all of these checks and tricks in one place. \n There is only one method which is exposed,  check(data,labels)  (optionally taking data or labels).\nIf any data check fails, it throws a well-named error, as well as hints for how you might fix the problem -- data covariance matrix ill-conditioned? Try whitening. \n Initially, this will be a Python/NumPy only package running basic checks, but hopefully it becomes a resource of data sanity and sanitation checks.\nStill very much a work in progress. \n Examples (some implemented, some not) include: \n \n If your labels are one-hot, are you using all slots? \n Is the covariance matrix of your data ill-conditioned? \n Do you have any constant variables? \n Can you train a classifier to distinguish train and test data, using whether they are in train or test as a label? Indicates different data distributions. \n If you're using integer labels, are the unique labels contiguous? \n Do you have just one unique label? \n Is the data under different labels statistically separable? \n If you have an old dataset and a new dataset (or two halves of the same dataset), is the distribution of each dimension stationary? Check for divergence with a KS test. \n What else? I end up applying these tricks in a very ad hoc fashion, whenever a subtle bug pops up, and not rigorously before each project I tackle. I'd like to stuff all these tricks in one place, and run them like a unit test, or a continuous integration test, on data that I start working with. \n \n Should probably also think about engarde"", ""bayarea-dl-summerschool \n Torch notebooks and slides for the Bay Area Deep Learning Summer School \n Installation Instructions \n Install anaconda if you don't have it (instructions here for OS X) \n wget http://repo.continuum.io/miniconda/Miniconda-latest-MacOSX-x86_64.sh\nsh Miniconda-latest-MacOSX-x86_64.sh -b -p $HOME/anaconda \n Add anaconda to your $PATH \n export PATH=$HOME/anaconda/bin:$PATH \n Install Lua & Torch \n ```\nconda install lua=5.2 lua-science -c alexbw \n Although, you could install other Lua versions like 2.0 (LuaJIT), 5.1, 5.2 and 5.3 \n ``` \n Clone this repository and start the notebook server \n ```\ngit clone https://github.com/alexbw/bayarea-dl-summerschool.git\ncd bayarea-dl-summerschool\nitorch notebook \n Will open a browser tab, then you can navigate to the notebooks \n ```""]",['repository_count'],Google,https://avatars.githubusercontent.com/u/161935?u=f722a589176ab3dce6285bcce174117e3c103ec3&v=4,False,2016-08-15 18:39:55,396,332,0,2009-12-04T14:50:40Z,2022-08-23T18:31:52Z,,False,False,False,False,False,True,False,0,"{'C': 5, 'Objective-C': 5, 'C++': 1, 'Python': 14, 'JavaScript': 3, 'C#': 1, 'Shell': 2, 'Jupyter Notebook': 5, 'Lua': 4}",5,"Boston, MA",Data Governance,47,0.17,0.77,0.22,0
