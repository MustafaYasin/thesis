USERNAME,README
alexbw,"Sampling Inference for Bayesian HSMMs and HMMs 
 This is a Python library for approximate unsupervised sampling inference in
Bayesian Hidden Markov Models (HMMs) and explicit-duration Hidden semi-Markov
Models (HSMMs), focusing on the Bayesian Nonparametric extensions, the HDP-HMM
and HDP-HSMM, via the weak-limit approximation. 
 There are also some plugins to extend the functionality: 
 
 factorial models 
 autoregressive models 
 collapsed HDP sampling inference . 
 
 The inference can be run in parallel over multiple cores and/or multiple
machines (even on EC2!) using  ipython 's
excellent  ipython.parallel  module. Someday I might even document how to do
it! 
 Installing 
 You can clone this library and its dependencies with 
 git clone --recursive git://github.com/mattjj/pyhsmm.git 
 The library depends on  numpy ,  scipy , and, for visualization,  matplotlib . 
 Disabling assertions may speed things up; to disable assertions, run your
CPython interpreter with the  -O  flag. 
 A Simple Demonstration 
 Here's how to draw from the HDP-HSMM posterior over HSMMs given a sequence of
observations. (The same example, along with the code to generate the synthetic
data loaded in this example, can be found in  examples/basic.py .) 
 Let's say we have some 2D data in a data.txt file: 
 bash
 head -5 data.txt
-3.711962552600095444e-02 1.456401745267922598e-01
7.553818775915704942e-02 2.457422192223903679e-01
-2.465977987699214502e+00 5.537627981813508793e-01
-7.031638516485749779e-01 1.536468304146855757e-01
-9.224669847039665971e-01 3.680035337673161489e-01 
 In Python, we can plot the data in a 2D plot, collapsing out the time dimension: 
 ```python
import numpy as np
from matplotlib import pyplot as plt 
 data = np.loadtxt('data.txt')
plt.plot(data[:,0],data[:,1],'kx')
``` 
 
 We can also make a plot of time versus the first principal component: 
 python
from pyhsmm.util.plot import pca_project_data
plt.plot(pca_project_data(data,1)) 
 
 To learn an HSMM, we'll use  pyhsmm  to create an  HSMM  instance using some
reasonable hyperparameters. We'll ask this model to infer the number of states
as well (since an HDP-HSMM is instantiated by default), so we'll give it an
 Nmax  parameter: 
 ```python
import pyhsmm
import pyhsmm.basic.distributions as distributions 
 obs_dim = 2
Nmax = 25 
 obs_hypparams = {'mu_0':np.zeros(obs_dim),
                'sigma_0':np.eye(obs_dim),
                'kappa_0':0.3,
                'nu_0':obs_dim+5}
dur_hypparams = {'alpha_0':2*30,
                 'beta_0':2} 
 obs_distns = [distributions.Gaussian( obs_hypparams) for state in range(Nmax)]
dur_distns = [distributions.PoissonDuration( dur_hypparams) for state in range(Nmax)] 
 posteriormodel = pyhsmm.models.HSMM(
        alpha=6.,gamma=6., # better to sample over these; see concentration-resampling.py
        init_state_concentration=6., # pretty inconsequential
        obs_distns=obs_distns,
        dur_distns=dur_distns,
        trunc=60) # duration truncation speeds things up when it's possible
``` 
 (The first two arguments set the ""new-table"" proportionality constant for the
meta-Chinese Restaurant Process and the other CRPs, respectively, in the HDP
prior on transition matrices. For this example, they really don't matter at
all, but on real data it's much better to infer these parameters, as in
 examples/concentration_resampling.py .) 
 The  trunc  parameter is an optional argument that can speed up inference: it
sets a truncation limit on the maximum duration for any state. If you don't
pass in the  trunc  argument, no truncation is used and all possible state
duration lengths are considered. 
 Then, we add the data we want to condition on: 
 python
posteriormodel.add_data(data) 
 (If we had multiple observation sequences to learn from, we could add them to the
model just by calling  add_data()  for each observation sequence.) 
 Now we run a resampling loop. For each iteration of the loop, all the latent
variables of the model will be resampled by Gibbs sampling steps, including the
transition matrix, the observation means and covariances, the duration
parameters, and the hidden state sequence. We'll also copy some samples so that
we can plot them. 
 python
models = []
for idx in progprint_xrange(150):
    posteriormodel.resample_model()
    if (idx+1) % 10 == 0:
        models.append(copy.deepcopy(posteriormodel)) 
 Now we can plot our saved samples: 
 python
fig = plt.figure()
for idx, model in enumerate(models):
    plt.clf()
    model.plot()
    plt.gcf().suptitle('HDP-HSMM sampled after %d iterations' % (10*(idx+1)))
    plt.savefig('iter_%.3d.png' % (10*(idx+1))) 
 
 I generated these data from an HSMM that looked like this: 
 
 So the posterior samples look pretty good! 
 Speed 
 HSMMs constitute a much more powerful model class than plain-old HMMs, and that
enhanced power comes with a computational price: each sampling iteration for an
HSMM is much slower than that of an HMM. But that price is often worthwhile if
you want to place priors on state durations or have the model learn duration
structure present in the data. (In the example, strong duration structure is
what made the inference algorithm latch onto the correct explanation so
easily.) In addition, the increased cost of each iteration often pays for
itself, since HSMM samplers empirically seem to take fewer iterations to
converge than comparable HMM samplers. 
 Using my nothing-special i7-920 desktop machine and a NumPy/SciPy built against
Intel's MKL BLAS (which generally outperforms ATLAS for vectorized operations)
along with the Eigen-backed classes, here's how long the demo iterations took: 
 $ python examples/hsmm.py
.........................  [  25/100,    0.05sec avg,    3.95sec ETA ]
.........................  [  50/100,    0.05sec avg,    2.64sec ETA ]
.........................  [  75/100,    0.05sec avg,    1.34sec ETA ]
.........................  [ 100/100,    0.05sec avg,    0.05sec ETA ]
   0.05sec avg,    5.21sec total 
 Extending the Code 
 To add your own observation or duration distributions, implement the interfaces
defined in  basic/abstractions.py . Also see the plugins. To get a flavor of
the style, see  pybasicbayes . 
 Contributors 
 Contributions by Chia-ying Lee. 
 References 
 
 
 Matthew J. Johnson and Alan S. Willsky.  Bayesian Nonparametric Hidden
  Semi-Markov Models .
  Journal of Machine Learning Research (JMLR), 14:673–701, February 2013. 
 
 
 Matthew J. Johnson and Alan S. Willsky,  The Hierarchical Dirichlet Process
  Hidden Semi-Markov Model . 26th
  Conference on Uncertainty in Artificial Intelligence (UAI 2010), Avalon,
  California, July 2010. 
 
 
 bibtex
@article{johnson2013hdphsmm,
    title={Bayesian Nonparametric Hidden Semi-Markov Models},
    author={Johnson, Matthew J. and Willsky, Alan S.},
    journal={Journal of Machine Learning Research},
    pages={673--701},
    volume={14},
    month={February},
    year={2013},
} An analgesic for high-performance audio on iOS and OSX. 
 Really fast audio in iOS and Mac OS X using Audio Units is hard, and will leave you scarred and bloody. What used to take days can now be done with just a few lines of code. 
 Getting Audio 
 objective-c
Novocaine *audioManager = [Novocaine audioManager];
[audioManager setInputBlock:^(float *newAudio, UInt32 numSamples, UInt32 numChannels) {
    // Now you're getting audio from the microphone every 20 milliseconds or so. How's that for easy?
    // Audio comes in interleaved, so,
    // if numChannels = 2, newAudio[0] is channel 1, newAudio[1] is channel 2, newAudio[2] is channel 1, etc.
}];
[audioManager play]; 
 Playing Audio 
 objective-c
Novocaine *audioManager = [Novocaine audioManager];
[audioManager setOutputBlock:^(float *audioToPlay, UInt32 numSamples, UInt32 numChannels) {
    // All you have to do is put your audio into ""audioToPlay"".
}];
[audioManager play]; 
 Does anybody actually use it? 
 Yep. Novocaine is result of three years of work on the audio engine of  Octave ,  Fourier  and  oScope , a powerful suite of audio analysis apps. Please do check them out! 
 A thing to note: 
 The RingBuffer class is written in C++ to make things extra zippy, so the classes that use it will have to be Objective-C++. Change all the files that use RingBuffer from MyClass.m to MyClass.mm. 
 Want some examples? 
 Inside of ViewController.mm are a bunch of tiny little examples I wrote. Uncomment one and see how it sounds. 
Do note, however, for examples involving play-through, that you should be using headphones. Having the 
mic and speaker close to each other will produce some gnarly feedback.   
 Want to learn the nitty-gritty of Core Audio? 
 If you want to get down and dirty, if you want to get brave and get close to the hardware, I can only point you to the places where I learned how to do this stuff. Chris Adamson and Michael Tyson are two giants in the field of iOS audio, and they each wrote indispensable blog posts ( this is Chris's ,  this is Michael's ). Also, Chris Adamson now has a  whole gosh-darned BOOK on Core Audio . I would have done unspeakable things to get my hands on this when I was first starting. 
 My Code for the Netflix Prize 
 I'm not aware of folks having published their code for the Netflix Prize. Here's mine. 
Under the team name ""Hi!"", I competed alone in college. I did it mostly for fun, and to learn modern machine learning techniques. It was an incredibly valuable, but strenuous, time. Well worth it on all fronts, though.  
I peaked out at #45 or so, and then dropped out to work on my senior thesis, and came in #145 or so.   
What I learned in the process was that smarter wasn't always better -- make an algorithm, and then scale it up, and then make a dozen tweaks to it, and then average all of the results together. That's how you climbed the leaderboard.    
 Anyhoo, I haven't touched this code in awhile, but perhaps it'll be useful to folks interested in competitive data mining. 
Specifically, the lessons I learned: 
 
 Get the raw data into a saved and manageable format  fast . The easier it is to load your data in and start mutating it, the better. 
 If doing simple pivots on your data is hard, and slows you down from visualizing whats in your data, spend time making data structures which make that easy.   
 Generalize. Iterate. If you have a method you think will work, but it has a lot of knobs, and you don't know the best way to set those knobs, make it easy for you to try  every possible iteration . There is often not a good way to figure out what the  best  approach is. You will have to try many of them in order to build up an intuition. Specifically, that means (for me) a pluggable architecture. If there's ten ways to try a particular step, make sure you write your overarching algorithm so that it takes a function that you can pass to it, as opposed to having a method hardwired in the code. That way, you can hotswap all your ideas.   
 Speed is a feature. Of course you make sure it works first. But your goal is to see if something works. If an algorithm takes a day to run, but you can spend five hours making it run in 1/3 of a day, do it. You'll be running it over and over again, and you'll learn more if you can iterate.  
 
 As for the technical nitty-gritty, everything that's speed sensitive is written in Cython, which was the best balance of speed and convenience in 2009. If I were to do it al again, I would use (Numba)[http://github.com/numba/numba].   
 The original data is gone, I believe, but I might have it stored somewhere. I'll look for that.   NURBS - Non Uniform Rational B-Splines. 
 This python module is a revival and update of Runar Tenfjord's NURBS toolbox, which itself
is a port of Mark Spink's SCILAB/MATLAB tolbox to python with help of numpy. 
 Dependency 
 Python 2.0 or above 
NumPy 
Dislin -> optional but recomended 
PyOpenGL -> optional   
 Installation 
 Just run   python setup.py install   
 License 
 Runar Tenfjord originally relased this package under the GPL Version 2 license,  
so I suppose that means it has to stay that way.    
 Originally by: Runar Tenfjord, runten@netcom.no 
Minor updates to work with NumPy by Alex Wiltschko   cuda-tests 
 Gotta learn some CUDA stuff pypatent 
 Scrape patents from the USPTO airruler 
 A ruler. Made of air.  paralleltools 
 A summary of parallelizing moderate amounts of work in Python nsgt 
 Non-stationary Gabor transforms (GitHub mirror of http://grrrr.org/research/software/nsgt/) IDA 
 This code accompanies the publication from the Whitesides lab to Lab on a Chip, concerning automated analysis of red blood cell health using affordable field tests. Specifically, this code implements the automated extraction of scanned AMPS tubes from flatbed scanner images, the distillation of those images into 1D data traces, and then the automated identification of the anemic disease state of the blood in those 1D data traces, as well as the prediction of continuously-varying red blood cell (RBC) parameters. 
 Installation 
 This code requires Python, as well as some extra 3rd-party libraries. All routines have only been tested on Mac OS, but should work on Linux. No guarantees for Windows.
1) First, we recommend using the ""Anaconda"" Python distribution, specifically the Python 2.7 version.  Download and install here .
1) With anaconda installed, you will need one extra library, to read TIFF files.
 pip install imageio 
1) You should be able to run the notebooks now. Inside this code repository, start up an IPython notebook:
 jupyter notebook 
1) You should now be able to browser around the ""extraction"" and ""analysis"" folders, which contain the relevant code. 
 Running extraction 
 The first step required will be to prepare the raw data of TIFF file scans of 4x3=12 tubes from a flatbed scanner. We will also have to associate metadata for each patient that the blood in a tube was drawn from.
Our end goal will be a 1D array for each tube, along with the corresponding patient data (anemic state & RBC parameters). 
 The extraction algorithm is explained step-by-step in the notebook  extraction/Explaining the Extraction Algorithm.ipynb .
The implementation we use in the paper (which also fuses patient metadata with AMPS image data) can be found in  extraction/Data Extraction Pipeline.ipynb . Most of the code in this notebook is specific to the particular Excel file format that we used to record patient metadata, and may have to be largely rewritten for reuse. 
 Running analysis 
 There are two sets of analyses done in the paper: classification and regression. 
 Running classification analysis 
 The goal of this analysis is to predict disease state only from the 1D data trace extracted from each AMPS tube. We use logistic regression, a linear classifier, and transform the 1D data representation using PCA to remove redundancy and constrain the input dimensionality of the problem. We also used Bayesian Optimization (bayesopt) to tune the hyperparameters of the problem, including the specific output dimension of PCA, the regularization parameter of logistic regression, and some image preprocessing parameters. Unfortunately, the service we used for bayesopt is no longer available. If you would like to automatically tune these parameters, we recommend either using random search (a surprisingly effective hueristic), or the open source library ""Spearmint"", upon which the now-defunct service we used was based, or products from the company SigOpt, which also implements bayesopt. The file we used to automatically tune these hypers is  classify.py .
The best set of hyperparameters is stored in the notebook  Analyzing best classification.ipynb .
This notebook examines ROC performance of the classifier for discriminating different anemia types, as well as the effect of centrifugation time of the tube on IDA classification AUC performance. 
 Running regression analysis. 
 Similar to above, we used a defunct bayesopt service to automatically tune some parameters of this algorithm. The original file is in  regress.py . The original methodology will work well, even without automated tuning.
The best set of hyperparameters is stored in the notebook  Analyzing best regression.ipynb . 
 License 
 See the LICENSE file. It is under a GPL license, and this code may only be used for academic purposes. adabayes 
 To do: 
 
 [x] Find candidate last layer features (DeCAF, Overfeat) 
 [x] Find AlexNet code (5 convolutional layers with max-pooling, then three fully connected layers) 
 [X]  Find download script for MNIST dataset 
 [X]  Find download script for CIFAR10 dataset 
 [x] Fork torch-dist repo 
 [x] Update torch-dist repo for OS X 10.10 install 
 [x] Add new required submodules to distro (nnx, iTorch, ccn2, cunnx, cudnn, sdl2, cutorch) 
 [x] Figure out model serialization and loading 
 Built-in model serialization.  Loading  and  saving . 
 There is also facebook's  Thrift library , which I haven't seen many examples for. 
 
 
 [ ] Get the data out of the  DataSource  models that we're using 
 [ ] Train and Whetlab a net on MNIST (to get the whole workflow going) 
 [ ] Train and Whetlab a net on CIFAR10 
 [ ] Train and Whetlab a net on STL10 
 [ ] Build dumb ensemble on CIFAR10 
 [ ] Build dumb ensemble on STL10 
 [ ] Grok boosting criterion 
 [ ] Whetlab a net with progressive ensembling on CIFAR10 
 [ ] Whetlab a net with progressive ensembling on STL10 
 [ ] Whetlab a net with progressive ensembling on the last-layer features of ImageNet 
 [ ] Whetlab a net with progressive ensembling and tuned class weights on CIFAR10 
 [ ] Whetlab a net with progressive ensembling and tuned class weights on STL10 
 [ ] Extract last-layer ImageNet features 
 [ ] Host last-layer ImageNet features 
 [ ] Build download script for last-layer ImageNet features 
 [ ] Train and Whetlab a net on last-layer features on ImageNet 
 [ ] Build a dumb ensemble on ImageNet 
 [ ] Whetlab a net with progressive ensembling and tuned class weights on the last-layer features of ImageNet 
 
 Collecting some resources 
 AlexNet:
https://github.com/soumith/convnet-benchmarks/blob/master/torch7/imagenet_winners/alexnet.lua 
 I think this is NiN:
https://github.com/soumith/convnet-benchmarks/blob/master/torch7/imagenet_winners/googlenet.lua 
 OverFeat:
https://github.com/facebook/fbcunn/blob/master/examples/imagenet/models/overfeat_cunn.lua 
 Multiclass criterion:
https://github.com/torch/nn/blob/master/doc/criterion.md#classnllcriterion 
 Some other interesting nets:
https://github.com/culurciello/profiling 
 Bunch of demos, not all nets:
https://github.com/torch/demos 
 Overfeat features:
http://cilvr.nyu.edu/doku.php?id=software:overfeat:start 
 Decaf ImageNet submission:
https://github.com/UCB-ICSI-Vision-Group/decaf-release/wiki/imagenet 
 Some other net implementations:
https://github.com/eladhoffer/ImageNet-Training Conda recipes for installing packages from the Torch ecosystem. 
 NOTE: No longer maintained. 
 To install packages 
 ``` 
 Install anaconda if you don't have it (instructions here for OS X) 
 wget http://repo.continuum.io/miniconda/Miniconda-latest-MacOSX-x86_64.sh
sh Miniconda-latest-MacOSX-x86_64.sh -b -p $HOME/anaconda 
 Add anaconda to your $PATH 
 export PATH=$HOME/anaconda/bin:$PATH 
 Install Lua & Torch 
 conda install lua=5.2 lua-science -c alexbw 
 Available versions of Lua: 2.0, 5.1, 5.2, 5.3 
 2.0 is LuaJIT 
 ``` 
 To build packages 
 ``` 
 Install anaconda if you don't have it (instructions here for OS X) 
 wget http://repo.continuum.io/miniconda/Miniconda-latest-MacOSX-x86_64.sh
sh Miniconda-latest-MacOSX-x86_64.sh -b -p $HOME/anaconda 
 Add anaconda to your $PATH 
 export PATH=$HOME/anaconda/bin:$PATH 
 Get the newest version of conda, as well as some conda build tools 
 conda update conda -y
conda install conda-build anaconda-client -y 
 Build all packages 
 sh build_all.sh 
 Ideally, all you have to do to install everything is this 
 conda install lua=5.2 lua-science
``` 
 TODO:
 - https://github.com/AlexMili/torch-dataframe
 - https://github.com/twitter/torch-ipc
 - https://github.com/twitter/torch-distlearn
 - https://github.com/twitter/torch-dataset 
 Resources: 
 
 
 Making packages relocatable (LuaJIT hard-codes path) 
 
 
 Build instructions for luarocks 
 
 
 Build instructions for Lua 
 
 
 Patching files with git diffs is finicky 
 
 
 Linking against readline  (need a few extra flags, and link against -lncursesw, not -lncurses) 
 
 
 Upgrading old Debian .  Also this . 
 
 
 Misc notes:
 metadata:ns_cfg() — defines for YAML directives
main_build: — defines version numbers 
environ:get_lua_include_dir() — uses version numbers to locate the include directory
config:Config._get_lua — uses version numbers to locate the binary
This is where the linked package name is converted into what is used validata 
 Continuous integration for your data 
 We do continuous integration on code. Why not data?
Validata is a small package to run basic sanity checks on your data.
I haven't found anything that aggregates all of these checks and tricks in one place. 
 There is only one method which is exposed,  check(data,labels)  (optionally taking data or labels).
If any data check fails, it throws a well-named error, as well as hints for how you might fix the problem -- data covariance matrix ill-conditioned? Try whitening. 
 Initially, this will be a Python/NumPy only package running basic checks, but hopefully it becomes a resource of data sanity and sanitation checks.
Still very much a work in progress. 
 Examples (some implemented, some not) include: 
 
 If your labels are one-hot, are you using all slots? 
 Is the covariance matrix of your data ill-conditioned? 
 Do you have any constant variables? 
 Can you train a classifier to distinguish train and test data, using whether they are in train or test as a label? Indicates different data distributions. 
 If you're using integer labels, are the unique labels contiguous? 
 Do you have just one unique label? 
 Is the data under different labels statistically separable? 
 If you have an old dataset and a new dataset (or two halves of the same dataset), is the distribution of each dimension stationary? Check for divergence with a KS test. 
 What else? I end up applying these tricks in a very ad hoc fashion, whenever a subtle bug pops up, and not rigorously before each project I tackle. I'd like to stuff all these tricks in one place, and run them like a unit test, or a continuous integration test, on data that I start working with. 
 
 Should probably also think about engarde bayarea-dl-summerschool 
 Torch notebooks and slides for the Bay Area Deep Learning Summer School 
 Installation Instructions 
 Install anaconda if you don't have it (instructions here for OS X) 
 wget http://repo.continuum.io/miniconda/Miniconda-latest-MacOSX-x86_64.sh
sh Miniconda-latest-MacOSX-x86_64.sh -b -p $HOME/anaconda 
 Add anaconda to your $PATH 
 export PATH=$HOME/anaconda/bin:$PATH 
 Install Lua & Torch 
 ```
conda install lua=5.2 lua-science -c alexbw 
 Although, you could install other Lua versions like 2.0 (LuaJIT), 5.1, 5.2 and 5.3 
 ``` 
 Clone this repository and start the notebook server 
 ```
git clone https://github.com/alexbw/bayarea-dl-summerschool.git
cd bayarea-dl-summerschool
itorch notebook 
 Will open a browser tab, then you can navigate to the notebooks 
 ```"
iassael,"DEARanking 
 Proposing a hybrid DEA/Polynomial Interpolation (DEA/PI) algorithm for the raking of protected areas: An application in Greece Find The Word (Τηλεκύβος Cheat) 
 Find the greek word from the given letters. Application to cheat and solve games like ""Τηλεκύβος"" and ""Βρες τη λέξη"" csoxcal 
 Oxford University, Computer Science Calendar Filter for Google Calendar use A Hybrid Parallel Implementation of the Aho-Corasick and Wu-Manber Algorithms Using NVIDIA CUDA and MPI Evaluated on a Biological Sequence Database. 
 Charalampos S. Kouzinopoulos, Yannis M. Assael, Themistoklis K. Pyrgiotis, Konstantinos G. Margaritis 
 Multiple matching algorithms are used to locate the occurrences of patterns from a finite pattern set in a large input string. Aho-Corasick and Wu-Manber, two of the most well known algorithms for multiple matching require an increased computing power, particularly in cases where large-size datasets must be processed, as is common in computational biology applications. Over the past years, Graphics Processing Units (GPUs) have evolved to powerful parallel processors outperforming Central Processing Units (CPUs) in scientific calculations. Moreover, multiple GPUs can be used in parallel, forming hybrid computer cluster configurations to achieve an even higher processing throughput. This paper evaluates the speedup of the parallel implementation of the Aho-Corasick and Wu-Manber algorithms on a hybrid GPU cluster, when used to process a snapshot of the Expressed Sequence Tags of the human genome and for different problem parameters. 
 Links 
 arXiv pre-print 
 Bibtex 
 @article{kouzinopoulos2015hybrid,
  title={A Hybrid Parallel Implementation of the Aho-Corasick and Wu-Manber Algorithms Using NVIDIA CUDA and MPI Evaluated on a Biological Sequence Database},
  author={Kouzinopoulos, Charalampos S. and Assael, Yannis M. and Pyrgiotis, Themistoklis K. and Margaritis, Konstantinos G.},
  journal={International Journal on Artificial Intelligence Tools},
  volume={24},
  number={1},
  pages={1540001},
  year={2015},
  publisher={World Scientific}
} 
 License 
 Code licensed under the GNU General Public License v3.0. RKHS Function 
 Description 
 Synthetic heteroscedastic 1D function generated from 2 Squared Exponential kernels for Bayesian Optimization method evaluation tasks  
 Input Space 
 x ∈ [0, 1] 
 Global Maximum 
 x=0.89235, f(x)=5.73839 
 Authors 
 Copyright (C) 2014 Ziyu Wang, John-Alexander Assael, Nando de Freitas
published under GPLv3 license 
 Screenshot 
 Decomposition module for Torch7 
 
 
 Principal Component Analysis (PCA) 
 
 
 Whitened Principal Component Analysis (W-PCA) 
 
 
 Linear Discriminant Analysis (LDA) 
 
 
 Locality Preserving Projections (LPP) 
 
 
 Neighbourhood Preserving Projections (NPP) 
 
 
 Fast Independent Component Analysis (FastICA) 
 
 
 by John-Alexander Assael 
 http://www.johnassael.com 
 https://github.com/iassael/torch7-decomposition 
 Installation 
 Clone this repository or download the source code. 
 Usage 
 Call  decomposition = require ""decomposition"" 
and then any of the following: 
 
 
 decomposition.pca(x) , 
 
 
 decomposition.lda(x, y) , 
 
 
 decomposition.lpp(x) , 
 
 
 decomposition.npp(x) , 
 
 
 decomposition.fastica(x) . 
 
 
 Alternativly, you can use iTorch notebook and open  decomposition.ipynb . 
 Contributing 
 
 Fork it! 
 Create your feature branch:  git checkout -b my-new-feature 
 Commit your changes:  git commit -am 'Add some feature' 
 Push to the branch:  git push origin my-new-feature 
 Submit a pull request 
 
 Notes 
 The implementations were developed in terms of learning and may not be optimal. 
 License 
 Copyright (C) 2015 John-Alexander Assael (www.johnassael.com)
https://github.com/iassael/torch7-decomposition 
 The MIT License (MIT) 
 Permission is hereby granted, free of charge, to any person obtaining a copy of
this software and associated documentation files (the ""Software""), to deal in
the Software without restriction, including without limitation the rights to
use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies
of the Software, and to permit persons to whom the Software is furnished to do
so, subject to the following conditions: 
 The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software. 
 THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE. Torch7 impementation of: 
 Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images 
 by Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, Martin Riedmiller (http://arxiv.org/abs/1506.07365) 
 Implemented by John-Alexander M. Assael (iassael@gmail.com) and Marc P. Deisenroth. 
 The MIT License (MIT)

Permission is hereby granted, free of charge, to any person obtaining a copy of
this software and associated documentation files (the ""Software""), to deal in
the Software without restriction, including without limitation the rights to
use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies
of the Software, and to permit persons to whom the Software is furnished to do
so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
 Regularization of Neural Networks using DropConnect 
 Li Wan, Matthew Zeiler, Sixin Zhang, Yann LeCun, Rob Fergus 
 Dept. of Computer Science, Courant Institute of Mathematical Science, New York University 
 Torch7 implementation by John-Alexander M. Assael Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) 
 Djork-Arné Clevert, Thomas Unterthiner, Sepp Hochreiter 
 Torch7 implementation by John-Alexander M. Assael Deep Exploration via Bootstrapped DQN 
 Ian Osband, Charles Blundell, Alexander Pritzel, Benjamin Van Roy 
 Usage 
 nn.Bootstrap(nn.Linear(size_in, size_out), 10, 0.08) 
 Implemented by Yannis M. Assael (www.yannisassael.com) Recurrent Batch Normalization 
 Batch-Normalized LSTMs 
 Tim Cooijmans, Nicolas Ballas, César Laurent, Çağlar Gülçehre, Aaron Courville 
 http://arxiv.org/abs/1603.09025 
 Usage 
 local rnn = LSTM(input_size, rnn_size, n, dropout, bn) 
 n = number of layers (1-N) 
 dropout = probability of dropping a neuron (0-1) 
 bn = batch normalization (true, false) 
 Example 
 https://github.com/iassael/char-rnn 
 Performance 
 Validation scores on char-rnn with default options 
 
 Implemented in Torch by Yannis M. Assael (www.yannisassael.com) Grid World DQN using torch7 
 This is a naive DQN example implemented in torch7 to help future research. 
 The environment is based on  rlenvs  of  Kaixhin  and the model makes use of ""Increasing the Action Gap"" ( http://arxiv.org/abs/1512.04860 ). 
 Implemented by Yannis M. Assael ( yannisassael.com ) Single pendulum Deterministic Policy Gradient example using torch7 
 Continuous Control with Deep Reinforcement Learning 
 Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra 
 http://arxiv.org/abs/1509.02971 
 Dependecies 
 luarocks install Math-RungeKutta
luarocks install csvigo
luarocks install image
luarocks install hdf5 
 Implemented by Yannis M. Assael ( yannisassael.com ) Using deep Q-learning to understand the tax evasion behavior of risk-averse firms 
 Links 
 
 arXiv preprint 
 Expert Systems with Applications 
 
 Execution 
 $ # Requirements: torch
$ th tax-dqn.lua 
 Bibtex 
 @article{goumagias2018using,
  title={Using deep Q-learning to understand the tax evasion behavior of risk-averse firms},
  author={Goumagias, Nikolaos D and Hristu-Varsakelis, Dimitrios and Assael, Yannis M},
  journal={Expert Systems with Applications},
  volume={101},
  pages={258--270},
  year={2018},
  publisher={Elsevier}
}
 
 License 
 Code licensed under the Apache License v2.0 Restoring ancient text using deep learning 
 A case study on Greek epigraphy 
 Yannis Assael * , Thea Sommerschield * , Jonathan Prag 
 
 Ancient History relies on disciplines such as Epigraphy, the study of ancient inscribed texts, for evidence of the recorded past. However, these texts, ""inscriptions"", are often damaged over the centuries, and illegible parts of the text must be restored by specialists, known as epigraphists.
This work presents a novel assistive method for providing text restorations using deep neural networks.
To the best of our knowledge, Pythia is the first ancient text restoration model that recovers missing characters from a damaged text input.
Its architecture is carefully designed to handle long-term context information, and deal efficiently with missing or corrupted character and word representations. 
To train it, we wrote a non-trivial pipeline to convert PHI, the largest digital corpus of ancient Greek inscriptions, to machine actionable text, which we call PHI-ML.
On PHI-ML, Pythia's predictions achieve a 30.1% character error rate, compared to the 57.3% of human epigraphists. Moreover, in 73.5% of cases the ground-truth sequence was among the Top-20 hypotheses of Pythia, which effectively demonstrates the impact of such an assistive method on the field of digital epigraphy, and sets the state-of-the-art in ancient text restoration. 
 
 
Pythia-Bi-Word processing the phrase μηδέν ἄγαν (mēdén ágan) ""nothing in excess"", a fabled maxim inscribed on Apollo's temple in Delphi. The letters ""γα"" are missing, and annotated with ""?"". Since word ἄ??ν contains missing characters, its embedding is treated as unknown (""unk""). The decoder outputs correctly ""γα"".
 
 References 
 
 arXiv pre-print 
 EMNLP-IJCNLP 2019 
 Digital Classicist Wiki 
 DeepMind research blog 
 University of Oxford news blog 
 
 When using any of this project's source code, please cite:
 @inproceedings{assael2019restoring,
  title={Restoring ancient text using deep learning: a case study on {Greek} epigraphy},
  author={Assael, Yannis and Sommerschield, Thea and Prag, Jonathan},
  booktitle={Empirical Methods in Natural Language Processing},
  pages={6369--6376},
  year={2019}
} 
 Pythia online 
 To aid further research in the field we created an online interactive python notebook, where researchers can query one of our models to get text restorations and visualise the attention weights. 
 
 Google Colab 
 
 Pythia offline 
 The following snippets provide references for regenerating PHI-ML and training new models offline. 
 Dependencies 
 pip install -r requirements.txt && \
python -m nltk.downloader punkt 
 PHI-ML dataset generation 
 ``` 
 Download PHI (this will take a while) 
 python -c 'import pythia.data.phi_download; pythia.data.phi_download.main()' 
 Process and generate PHI-ML 
 python -c 'import pythia.data.phi_process; pythia.data.phi_process.main()'
```
Preprocessed PHI-ML uploaded by @Holger.Danske800:  link 
 Training 
 python -c 'import pythia.train; pythia.train.main()' 
 Evaluation 
 python -c 'import pythia.test; pythia.test.main()' --load_checkpoint=""your_model_path/"" 
 Docker execution 
 ./build.sh
./run.sh <GPU_ID> python -c 'import pythia.train; pythia.train.main()' 
 License 
 Apache License, Version 2.0 
 
 
Damaged inscription: a decree concerning the Acropolis of Athens (485/4 BCE).  IG  I 3  4B. (CC BY-SA 3.0, WikiMedia)
 Vivechrom RGB color matcher 
 Yannis Assael ( www.assael.gr ) 
 Description 
 This colaboratory notebook is used to find the closest vivechrom.gr paint code given an RGB color. 
 Instructions 
 
 Open this  Google Colab Notebook . 
 To edit and run the code, you need to click 'Open in Playground Mode' if you can see it at the top of the page, or save a local copy somewhere on your computer. 
 The cells need to be run one by one by pressing shift+enter or by using the play button. 
 Fill the  target_color_rgb  value with your target RGB color. 
 Run the cell to get suggestions ordered by color similarity. 
 
 License 
 ```
Copyright 2020 Yannis Assael 
 Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at 
 http://www.apache.org/licenses/LICENSE-2.0
 
 Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
``` 
 
 
 Restoring and attributing ancient texts using deep neural networks 
 Yannis Assael 1,* , Thea Sommerschield 2,3,* , Brendan Shillingford 1 , Mahyar Bordbar 1 , John Pavlopoulos 4 ,
Marita Chatzipanagiotou 4 , Ion Androutsopoulos 4 , Jonathan Prag 3 , Nando de Freitas 1 
 1  DeepMind, United Kingdom 
 2  Ca’ Foscari University of Venice, Italy 
 3  University of Oxford, United Kingdom 
 4  Athens University of Economics and Business, Greece 
 *  Authors contributed equally to this work 
 
 
 Ancient History relies on disciplines such as Epigraphy, the study of inscribed
texts known as ""inscriptions"", for evidence of the thought, language, society
and history of past civilizations. However, over the centuries many inscriptions
have been damaged to the point of illegibility, transported far from their
original location, and their date of writing is steeped in uncertainty. We
present Ithaca, the first Deep Neural Network for the textual restoration,
geographical and chronological attribution of ancient Greek inscriptions. Ithaca
is designed to assist and expand the historian’s workflow: its architecture
focuses on collaboration, decision support, and interpretability. 
 
 
 Restoration of damaged inscription: this inscription ( IG  I 3  4B) records a decree concerning the Acropolis of Athens and dates 485/4 BCE. (CC BY-SA 3.0, WikiMedia) 
 
 While Ithaca alone achieves 62% accuracy when restoring damaged texts, as soon
as historians use Ithaca their performance leaps from 25% to 72%, confirming
this synergistic research aid’s impact. Ithaca can attribute inscriptions to
their original location with 71% accuracy and can date them with a distance of
less than 30 years from ground-truth ranges, redating key texts of Classical
Athens and contributing to topical debates in Ancient History. This work shows
how models like Ithaca can unlock the cooperative potential between AI and
historians, transformationally impacting the way we study and write about one of
the most significant periods in human history. 
 
 
 Ithaca's architecture processing the phrase ""δήμο το αθηναίων"" (""the people of Athens""). The first 3 characters of the phrase were hidden and their restoration is proposed. In tandem, Ithaca also predicts the inscription’s region and date. 
 
 References 
 
 Nature article 
 DeepMind blog 
 
 When using any of this project's source code, please cite: 
 @article{asssome2022restoring,
  title = {Restoring and attributing ancient texts using deep neural networks},
  author = {Assael*, Yannis and Sommerschield*, Thea and Shillingford, Brendan and Bordbar, Mahyar and Pavlopoulos, John and Chatzipanagiotou, Marita and Androutsopoulos, Ion and Prag, Jonathan and de Freitas, Nando},
  doi = {10.1038/s41586-022-04448-z},
  journal = {Nature},
  year = {2022}
} 
 Ithaca inference online 
 To aid further research in the field we created an online interactive python notebook, where researchers can query one of our trained models to get text restorations, visualise attention weights, and more. 
 
 Ithaca Interactive Interface 
 Google Colab for using Ithaca for your research 
 
 Ithaca inference offline 
 Advanced users who want to perform inference using the trained model may want
to do so manually using the  ithaca  library directly. 
 First, to install the  ithaca  library and its dependencies, run:
 sh
pip install . 
 Then, download the model via
 sh
curl --output checkpoint.pkl https://storage.googleapis.com/ithaca-resources/models/checkpoint_v1.pkl 
 An example of using the library can be run via
 sh
python inference_example.py --input_file=example_input.txt 
which will run restoration and attribution on
the text in  example_input.txt . 
 To run it with different input text, run
```sh
python inference_example.py --input=""..."" 
 or using text in a UTF-8 encoded text file: 
 python inference_example.py --input_file=some_other_input_file.txt
``` 
 The restoration or attribution JSON can be saved to a file:
 sh
python inference_example.py \
  --input_file=example_input.txt \
  --attribute_json=attribute.json \
  --restore_json=restore.json 
 For full help, run:
 sh
python inference_example.py --help 
 Dataset generation 
 Ithaca was trained on The Packard Humanities Institute’s
"" Searchable Greek Inscriptions "" public
dataset. The processing workflow for generating the machine-actionable text and
metadata, as well as further details on the train, validation and test splits
are available at  I.PHI dataset . 
 Training Ithaca 
 See  train/README.md  for instructions. 
 License 
 Apache License, Version 2.0"
bshillingford,"id3tag-fix codehackathon2014 
 Team members:
Brendan Shillingford
Daisy Shih
Kevin Lim 
 http://data.gc.ca/eng/canadian-open-data-experience-code pycmdline-template 
 Template for making your one-off Python utilities more user-friendly. 
 Add  @arg  and  @cmdline  decorators to a main function as below; this provides an easier interface to the  logging  and  optparse  modules. 
 Examples 
 Example 1: simple program 
 This program accepts 2 arguments: one a flag passed as  --count=COUNT , or  -n COUNT ; another is a position argument, listed without a dash-prefixed flag name before it; and a default help flag  --help / -h  printing a usage page. 
 The arguments to the decorators are the same as in  optparse . 
 ```python 
 !/usr/bin/env python 
 from simpleargs import cmdline, arg 
 @arg('argument', help=""pass ARG to program"", metavar=""ARG"") # simple positional argument
@arg('--count', '-n', help=""number of bars to foo"", type=int, default=1)
@cmdline(description=""This program foos bars a given number of times."")
def main(argument, count): 
 print(""Hello there."")

print(""Your Argument: %s"" % argument)

for i in range(count):
    print(""Fooing bar #%d"" % i)
print(""Fooed %d bars."" % count)
 
 if  name  == ' main ':
    main()
``` 
 Example 2: simple program + logging 
 ``` 
 !/usr/bin/env python 
 import logging as L
from simpleargs import cmdline, arg, loglevel 
 @arg('argument', help=""pass ARG to program"", metavar=""ARG"") # simple positional argument
@arg('--count', '-n', type=int, default=1)
@loglevel()                                                 # if on, log level DEBUG, else INFO
@cmdline(description=""This program foos bars a given number of times."")
def main(argument, count): 
 print ""Hello there.""

L.info(""You passed an argument."")
L.debug(""Your Argument: %s"" % argument)

for i in range(count):
    L.debug(""Fooing bar #%d"" % i)
L.info(""Fooed %d bars."" % count)
 
 if  name  == ' main ':
    main()
``` Starbucks WiFi autologin 
 Automatically accept ToS when connecting. 
 Barely tested. 
 This tool is purely for educational purposes only, and should 
not be used in a real situation. 
 Dependencies: 
 
 Python 2 
 requests 
 BeautifulSoup 
 
 Installation: 
 Can just execute starbuckswifi.py directly, but install if you want. 
 Install via:
``` 
 python2 setup.py install 
 When you connect to an accesspoint, run 
$ python2 -m starbuckswifi
``` REST and MQTT for IFTTT 
 What's IFTTT? 
 IFTTT is a service that enables users to  connect different web applications  (e.g., Facebook, Evernote, Weather, Dropbox, etc.) together through simple conditional statements known as ""Recipes"". [Source: Wikipedia] 
 A recipe is a (trigger, action) pair. However, IFTTT doesn't support fully custom actions. However, it does support publishing to any Wordpress blog. This app emulates Wordpress's XML-RPC API so that you can send a  GET / POST / PUT  REST request, or publish a MQTT message, all by choosing the ""create post"" action in IFTTT. 
 Install 
 Simply clone this repository and upload as a Heroku app, or host it however you want. 
 For local testing, you can use  flaskrun.py . If desired, use a  virtualenv , and install all the packages in  requirements.txt . 
 Usage 
 Set the  title  and  body  of the post. Everything else is ignored. 
 Examples: 
 
 title =  GET http://httpbin.org/get?some=stuff&here 
 title =  POST http://example.com/something , body =  whatever data you want to post  (similarly for  PUT ) 
 title =  MQTT-PUB mqtt://iot.eclipse.org/topic/name/goes/here?qos=1&retain=T , body =  the payload of your request 
 
 Notice the URI format used to encode MQTT settings. 
 MQTT-PUB syntax 
 The URI is parsed as follows:  mqtt://{hostname}[:{port}]/{topic-name}[?{settings}] . The post's body is used as the payload and may be empty, but  note IFTTT may remove newlines or mess with whitespace . Hence, I suggest using JSON if possible. 
 Settings must be URI-encoded as usual; the following are valid:
*  qos  =  0  or  1  or  2 , default is  0  (see MQTT specifications for semantics)
*  retain  =  true  or  false , default is  false  ( 1 ,  T ,  t , and  tRUe  are all accepted as true)
*  protocol  =  MQTTv31  or  MQTTv311 , latter is default autobw 
 autobw  is a simple library for automatically performing  a backwards pass, given only a forwards pass,  in Torch. 
 A major advantage of this is that the neural network's  structure need not be fixed before runtime . This allows for easy implementation of structures such as recurrent networks. See the example below. 
 Backpropagation is often described as a method for propagating gradients through a computational graph. One way to implement it for graphs is to explicitly construct a graph given by the user, then evaluate the computational nodes in the order specified in the forward pass, then again but in reverse for the backward pass.  
 Install 
 luarocks install https://raw.githubusercontent.com/bshillingford/autobw.torch/master/autobw-scm-1.rockspec 
 Details 
 A method that's closer to how one may reason about a neural network is to explicitly write down a forward pass while recording the statements as they are being executed, then execute the statements' derivative computations (aka adjoint) in reverse. This is equivalent to specifying a computation graph, but more explicit, and allows the user to use  control-flow such as for loops and conditionals . 
 This is similar to the approach taken by implementations of reverse-mode automatic differentiation, see e.g.  http://arxiv.org/abs/1502.05767 . 
 Examples: 
 A simple example of computing  linear(x1) + x2 * sigmoid(x3) , but  randomly  replacing  sigmoid(x3)  with  x3  sometimes:
```lua
lin = nn.Linear(5,5)
add = nn.CAddTable()
mul = nn.CMulTable()
sigm = nn.Sigmoid() 
 tape = autobw.Tape() 
 -------------- START OF FORWARD PASS --------------
-- records the sequence of operations
tape:begin()
coin_flip = torch.rand(1)[1]
val1 = lin:forward(x1) 
 if coin_flip > 0.5 then
  maybe_sigmoid = sigm:forward(x3)
else
  maybe_sigmoid = x3
end 
 result = add:forward{val1, mul:forward{x2, maybe_sigmoid}}
tape:stop()
-------------- END OF FORWARD PASS -------------- 
 -- Play it back in reverse:
tape:backward() 
 -- Now, the gradients are in the four nn.Module objects as usual.
``` 
 Note: I don't actually use the gradients at all here, and I don't set them to zero first, just to keep the example simple.
See also  our nngraph practical  for the equivalent in  nngraph . 
 RNN Example 
 See the  examples folder  for a  fully functional rnn example  with toy data. 
 LSTM example 
 The LSTM example  https://github.com/oxford-cs-ml-2015/practical6  can easily be shortened by using this. We delete the backward pass, and simply play it back from the recorded forward pass:
```lua
-- setup autodiff
tape = Tape() -- TODO: local 
 -- do fwd/bwd and return loss, grad_params
function feval(x)
    if x ~= params then
        params:copy(x)
    end
    grad_params:zero() 
 ------------------ get minibatch -------------------
local x, y = loader:next_batch()

------------------- forward pass -------------------
tape:begin() -----------------
local embeddings = {}            -- input embeddings
local lstm_c = {[0]=initstate_c} -- internal cell states of LSTM
local lstm_h = {[0]=initstate_h} -- output values of LSTM
local predictions = {}           -- softmax outputs
local loss = 0

for t=1,opt.seq_length do
    embeddings[t] = clones.embed[t]:forward(x[{{}, t}])

    -- we're feeding the *correct* things in here, alternatively
    -- we could sample from the previous timestep and embed that, but that's
    -- more commonly done for LSTM encoder-decoder models
    lstm_c[t], lstm_h[t] = unpack(clones.lstm[t]:forward{embeddings[t], lstm_c[t-1], lstm_h[t-1]})

    predictions[t] = clones.softmax[t]:forward(lstm_h[t])
    loss = loss + clones.criterion[t]:forward(predictions[t], y[{{}, t}])
end
tape:stop() -----------------

------------------ backward pass -------------------
tape:backward()

------------------------ misc ----------------------
-- transfer final state to initial state (BPTT)
initstate_c:copy(lstm_c[#lstm_c])
initstate_h:copy(lstm_h[#lstm_h])

-- clip gradient element-wise
grad_params:clamp(-5, 5)

return loss, grad_params
 
 end
``` gzfile 
 Conveniently (and with decent performance) read and write data from  gzip  
files directly. Useful for text or very large files. 
 Implemented as an FFI wrapper for zlib, including wrappers for C  FILE  
functions like  fscanf  and  fwrite . More can be easily added. 
 Install:
 luarocks install https://raw.githubusercontent.com/bshillingford/lua_gzipfile/master/gzfile-scm-1.rockspec 
 Example: 
 Read 200 floats from a gzipped file directly into a torch tensor:
```lua
require 'torch'
local GZFile = require 'gzfile.GZFile' 
 local tensor = torch.FloatTensor(200)
local f = GZFile('floats.gz', 'rb')
f:readbuf(tensor:data(), 200 4)  -- sizeof(float)=4; read 200 4 bytes
f:close()
-- now do stuff with the tensor
```
Note: compressing floats can be useful for neural nets, since similar values at
similar orders of magnitude will often results in repeated byte patterns. 
 To access the underlying  FILE*  handle, use the  handle  property of  GZFile . 
 Functions implemented: 
 
 Constructor:  file = GZFile(filename, mode) 
    Opens the file for reading or writing using the given mode.
    See zlib gzopen() documentation for supported modes. 
 :close()  Closes the file. Later operations will fail. 
 :write(str) 
    Writes a lua string. Doesn't allocate memory, just casts using ffi.
    Calls fwrite and returns its ret val, i.e. number of bytes written. 
 :read(nbytes) 
    Reads to a buffer then turns into a lua string.
    Allocates memory on each call, so slightly inefficient if you do many
    reads. 
 :writebuf(ptr, nbytes) 
    Writes data from the given buffer to the file.
    Returns number of bytes written. 
 :readbuf(ptr, nbytes) 
    Reads into the given location in memory.
    Returns number of bytes read. 
 :flush() 
 :peek() 
 :tell() 
    Returns the position in the file using ftell. 
 :seek(offset, origin) 
    Seek using fseek, relative to beginning of file.
    Note the argument order matches C's rather than Lua's io.
    Returns new position from ftell. 
 :getc() 
 :scanf(fmt, typestring) 
    Calls fscanf, but only for reading a single field e.g.  %s .
    Allocates memory automatically, this memory will be gc'd by lua.
    Typestring examples:  'float[1]' ,  'uint8_t[1]' ,  'char[16]' . 
 Class visualizer for lua/torch 
 Class hierarchy visualizer for torch classes, and classic classes (see github.com/deepmind/classic) 
 Currently generates a d3 visualization, easily adapted to graphviz. 
 Example of viewing the classes in  nn 
 th generate.lua htmld3tree nn > output.html 
Then open  output.html  in your browser. 
 Usage: 
 sh
th [-lclassic] generate.lua output_mode package1 [package2 [package3 ...] ] 
where  output_mode  is  htmld3force  or  htmld3tree . See e.g.  htmld3tree.lua ; more can be added. 
 
 Output is to stdout; redirect it to a file like  output.html . 
 Use  -lclassic  to monitor classic classes instead. (see  https://github.com/deepmind/classic ) 
 
 Useful extensions/TODOs: 
 
 Output to graphviz 
 Parse libraries for documentation and include it in the visualization? 
 At the least, actually do something useful when clicking on the classes. 
 
 Internals 
 For  torch.class  classes, it simply monkeypatches this function to monitor created classes when
the packages get  require d. For classic classes, it uses a central (private) registry storing 
the classes and their parents. nnquery : query large neural network graph structures in Torch 
 NN modules in Torch are often complex graph structures, like  nn.Container s and its subclasses and  nn.gModules  ( nngraph ), arbitrarily nested. This makes it tedious to extract nn modules when debugging, monitoring training progress, or testing. 
 nnquery  provides a facility to query these arbitrarily complex DAGs. XPath and CSS are designed to handle trees, whereas this library supports querying DAGs like neural nets.
The API is loosely inspired by a mix of XPath, CSS queries, and .NET's LINQ. 
 See below for a simple example, and a more complete example of extracting things from an LSTM. 
 Installation 
 Install  nnquery :
 luarocks install https://raw.githubusercontent.com/bshillingford/nnquery/master/rocks/nnquery-scm-1.rockspec 
Totem is optional, and used for unit tests. 
 Usage 
 There are two important base classes that nearly everything is derived from: 
 
 Element  (full name:  nnquery.Element ) 
 ElementList 
 
 Every object you wish to query is wrapped in an  Element , and sequences/collections of these
are represented using  ElementList s. 
 To wrap an object in an  Element  so you can query it:
```lua
local nnq = require 'nnquery'
local seq = nn.Sequential()
    :add(nn.Tanh())
    :add(nn.ReLU()) 
 local tanh = nnq(seq):children():first()
```
On the last line, 
 
 nnq(seq)  wraps  seq  into an  Element ; 
 :children()  returns an  ElementList  of two  Elements  for  seq 's children; 
 :first()  returns the first  Element  in the  ElementList . 
 
 Realistic example with an LSTM: 
 This is an example of using various functions in  Element  and  ElementList :
```lua
require 'nn'
require 'nngraph'
local nnq = require 'nnquery' 
 -- nngraph implementation of LSTM timestep, from Oxford course's practical #6
function create_lstm(opt)
  local x = nn.Identity()()
  local prev_c = nn.Identity()()
  local prev_h = nn.Identity()() 
 function new_input_sum()
    -- transforms input
    local i2h            = nn.Linear(opt.rnn_size, opt.rnn_size)(x)
    -- transforms previous timestep's output
    local h2h            = nn.Linear(opt.rnn_size, opt.rnn_size)(prev_h)
    return nn.CAddTable()({i2h, h2h})
  end 
 local in_gate          = nn.Sigmoid()(new_input_sum())
  local forget_gate      = nn.Sigmoid()(new_input_sum())
  local out_gate         = nn.Sigmoid()(new_input_sum())
  local in_transform     = nn.Tanh()(new_input_sum()) 
 local next_c           = nn.CAddTable()({
      nn.CMulTable()({forget_gate, prev_c}),
      nn.CMulTable()({in_gate,     in_transform})
  })
  local next_h           = nn.CMulTable()({out_gate, nn.Tanh()(next_c)}) 
 nngraph.annotateNodes()
  local mod = nn.gModule({x, prev_c, prev_h}, {next_c, next_h})
  mod.name = ""LSTM""
  return mod
end 
 -- Example network
local foo = nn.Sequential()
    :add(nn.Module())
    :add(create_lstm{rnn_size=3})
    :add(nn.ReLU())
    :add(nn.ReLU())
    :add(nn.Linear(3, 4)) 
 -- Find the LSTM in a few different ways:
local lstm = nnq(foo)   -- Wrap the module in an Element object using the default context
                        -- which allows querying nn containers and nngraph's gmodules.
    :descendants()      -- Get all descendants below this node in the graph
    :where(function(e)  -- Filter Elements by the given predicate
      return e:classIs(nnq.NNGraphGModuleElement)
    end)
    :only()             -- Returns the first element in the returned sequence, and
                        -- asserts that it is the only element in the sequence.
                        -- (shortcut for list:first() and assert(list:count() == 1))
local lstm2 = nnq(foo)
    :children()         -- Returns the contained modules of the nn.Sequential object as an
                        -- ElementList
    :nth(2)             -- Grabs the 2nd child of the nn.Sequential
                        -- (alternate shorthand syntax: nnq(foo):children()[2])
local lstm3 = nnq(foo)
    :descendants()      --  
    :attr{name='LSTM'}  -- Get only the objects with a name attribute set to 'LSTM',
                        -- where it'll check both raw attributes and attempt to call
                        -- the function assuming it's a getter method, i.e. check 
                        -- module:name() == 'LSTM'.
    :only()
assert(lstm:val() == lstm2:val() and lstm2:val() == lstm3:val(),
    'they should all return the same LSTM gmodule') 
 -- Get the output nodes of the nngraph gmodule as an ElementList:
local outputs = lstm:outputs()
-- Two ways to get the count for an ElementList:
print('The LSTM gmodule has '..outputs:count()..' outputs, they are:' outputs)
print('The LSTM gmodule has '..#outputs..' outputs, they are:', outputs)
assert(outputs:first():name() == 'next_c')  -- :name() is available on NNGraphNodeElements,
                                            -- as a shortcut for:
assert(outputs:first():val().data.annotations.name == 'next_c')  
 -- Let's find the forget gate:
local forget_gate = lstm:descendants():attr{name='forget_gate'}:only()
print(forget_gate)
-- But it's the sigmoid, not the gate's pre-activations, so let's get the sum:
local input_sum = forget_gate:parent() -- This is an alias for :parents():only().
                                       -- Note: nngraph nodes can have multiple parents (i.e.
                                       -- inputs 
assert(torch.isTypeOf(input_sum:val().data.module, nn.CAddTable))
assert(torch.isTypeOf(input_sum:module(), nn.CAddTable)) -- alias for :val().data.module
``` 
 Further details: 
 Wrapping objects into elements and similar operations only make sense relative to a  context , an instance of  nnquery.Context , which contains a list of  Element  types and conditions on which to instantiate depending on what type is provided to it. Additionally, the context caches  Element s, so that wrapping the same object twice returns the same instance of the  Element  subclass.
 nnquery/init.lua  contains the construction of a default context (accessible as  nnquery.default ) that contains all the implemented  Element  types, similarly to this:
 lua
local ctx = nnq.Context()
ctx:reg(nnq.NNGraphGModuleElement, nnq.NNGraphGModuleElement.isGmodule)
ctx:reg(nnq.NNGraphNodeElement, nnq.NNGraphNodeElement.isNode)
ctx:reg(nnq.ContainerElement, nnq.ContainerElement.isContainer) -- after since gModule IS_A Container
ctx:default(nnq.ChildlessElement) 
 Note that there is no true ""root"" node, unlike an XML/HTML document; the root is simply the place where the query begins. Therefore, one cannot[*] search for the root's parents, even if the root module is contained in (for example) a container. 
 [*] Usually. Unless an element's parents are pre-populated from a previous query. 
 Documentation 
 Further documentation can be found in doc comment style before class definitions and method definitions in the code itself. 
 TODO: extract these into markdown format and put links here 
 Developing 
 Extending 
 You may have your own  nn  modules that are not handled by the existing handlers. In this case,
you can implement your own  Element  object (see the existing ones for examples), and create your own context that adds a handler for this  Element . See the default context (see above) for details. 
 Contributing 
 Bug reports are appreciated, preferably with a pull request for a test that breaks existing code and a patch that fixes it. If you do, please adhere to the (informal) code style in the existing code where appropriate. Torch serialization reader for Python 
 
 
 Mostly direct port of the torch7 Lua and C serialization implementation to 
Python, depending only on  numpy  (and the standard library:  array  
and  struct ). Sharing of objects including  torch.Tensor s is preserved. 
 python
import torchfile
stuff = torchfile.load('a_bunch_of_stuff.t7') 
 Installation: 
 Install from  PyPI :
 sh
pip install torchfile 
or clone this repository, then:
 sh
python setup.py install 
 Supports Python 2.7, 3.4, 3.5, 3.6. Probably others too. 
 More examples: 
 Write from torch, read from Python: 
 Lua:
 lua
+th> torch.save('/tmp/test.t7', {hello=123, world=torch.rand(1,2,3)}) 
Python:
 python
In [3]: o = torchfile.load('/tmp/test.t7')
In [4]: print o['world'].shape
(1, 2, 3)
In [5]: o
Out[5]: 
{'hello': 123, 'world': array([[[ 0.52291083,  0.29261517,  0.11113465],
         [ 0.01017287,  0.21466237,  0.26572137]]])} 
 Arbitary torch classes supported: 
 ```python
In [1]: import torchfile 
 In [2]: o = torchfile.load('testfiles_x86_64/gmodule_with_linear_identity.t7') 
 In [3]: o.forwardnodes[3].data.module
Out[3]: TorchObject(nn.Identity, {'output': array([], dtype=float64), 'gradInput': array([], dtype=float64)}) 
 In [4]: for node in o.forwardnodes: print(repr(node.data.module))                                                                                                           
None
None
None
TorchObject(nn.Identity, {'output': array([], dtype=float64), 'gradInput': array([], dtype=float64)})
None
TorchObject(nn.Identity, {'output': array([], dtype=float64), 'gradInput': array([], dtype=float64)})
TorchObject(nn.Linear, {'weight': array([[-0.0248373 ],
       [ 0.17503954]]), 'gradInput': array([], dtype=float64), 'gradWeight': array([[  1.22317168e-312],
       [  1.22317168e-312]]), 'bias': array([ 0.05159848, -0.25367146]), 'gradBias': array([  1.22317168e-312,   1.22317168e-312]), 'output': array([], dtype=float64)})
TorchObject(nn.CAddTable, {'output': array([], dtype=float64), 'gradInput': []})
None 
 In [5]: o.forwardnodes[6].data.module.weight
Out[5]: 
array([[-0.0248373 ],
       [ 0.17503954]]) 
 In [6]: o.forwardnodes[6].data.module.bias
Out[6]: array([ 0.05159848, -0.25367146])
``` 
 More complex writing from torch: 
 Lua:
 lua
+th> f = torch.DiskFile('/tmp/test.t7', 'w'):binary()
+th> f:writeBool(false)
+th> f:writeObject({hello=123})
+th> f:writeInt(456)
+th> f:close() 
Python:
 python
In [1]: import torchfile
In [2]: with open('/tmp/test.t7','rb') as f:
   ...:     r = torchfile.T7Reader(f)
   ...:     print(r.read_boolean())
   ...:     print(r.read_obj())
   ...:     print(r.read_int())
   ...: 
False
{'hello': 123}
456 
 Supported types: 
 
 nil  to Python  None 
 numbers to Python floats, or by default a heuristic changes them to ints or
   longs if they are integral 
 booleans 
 strings: read as byte strings (Python 3) or normal strings (Python 2), like
   lua strings which don't support unicode, and that can contain null chars 
 tables converted to a special dict (*); if they are list-like (i.e. have
   numeric keys from 1 through n) they become a python list by default 
 Torch classes: supports Tensors and Storages, and most classes such as 
   modules. Trivially extensible much like the Torch serialization code.
   Trivial torch classes like most  nn.Module  subclasses become 
    TorchObject s. The  torch_readers  dict contains the mapping from class
   names to reading functions. 
 functions: loaded into the  LuaFunction   namedtuple ,
   which simply wraps the raw serialized data, i.e. upvalues and code.
   These are mostly useless, but exist so you can deserialize anything. 
 tds.Hash, tds.Vec 
 
 (*) Since Lua allows you to index a table with a table but Python does not, we 
    replace dicts with a subclass that is hashable, and change its
    equality comparison behaviour to compare by reference.
    See  hashable_uniq_dict . 
 Test files demonstrating various features: 
 ```python
In [1]: import torchfile 
 In [2]: torchfile.load('testfiles_x86_64/list_table.t7')
Out[2]: ['hello', 'world', 'third item', 123] 
 In [3]: torchfile.load('testfiles_x86_64/doubletensor.t7')
Out[3]: 
array([[ 1. ,  2. ,  3. ],
       [ 4. ,  5. ,  6.9]]) 
 ...also other files demonstrating various types. 
 ``` 
 The example  t7  files will work on any modern Intel or AMD 64-bit CPU, but the
code will use the native byte ordering etc. Currently, the implementation 
assumes the system-dependent binary Torch format, but minor refactoring can 
give support for the ascii format as well. Linux Spotify ad muter 
 Listens to track changes with dbus. Assumes pulseaudio, and mutes master during ads (i.e. not just spotify's stream! [1]). 
 License: BSD. Based on  https://muffinresearch.co.uk/linux-spotify-track-notifier-with-added-d-bus-love/ . 
 [1] TODO: only mute spotify stream, read pavucontrol source code to figure out how. Currently mutes pulseaudio using  amixer -D pulse sset Master {off|on} . Fork of  fb.debugger : dependency-free 
 fb.debugger  is an excellent debugger for torch and lua in general, and can be found at  https://github.com/facebook/fblualib .
However,  fblualib  and its dependencies are quite heavy but you may just want the debugger. This repository is a fork of  fb.debugger  with dependencies integrated and/or removed.  
 To run a script and drop into the debugger on an error, simply do:
 sh
fbdbg-run script_name.lua your_arg1 your_arg2 ... 
 Install: 
 luarocks install https://raw.githubusercontent.com/bshillingford/fbdebugger-minimal/master/fbdebugger-standalone-1.rockspec 
 Dependencies: 
 
 penlight >= 1.3.1 
 libedit  installed in your system ( libedit.so  anywhere in the library search path). 
 
 
 Original README: 
 fb-debugger: A source-level Lua debugger 
 This package implements a source-level Lua debugger. 
 Usage 
 You may enter the debugger in two different ways:
* explicitly: at the point of interest, do
 lua
local debugger = require('fb.debugger')
debugger.enter() 
  and you will be dropped in the debugger
* automatically when you hit an (uncaught) error: if using
   fb.trepl , you may set the environment variable
   LUA_DEBUG_ON_ERROR  to  1 , and you'll be dropped in the debugger
  whenever your code raises an uncaught error. 
 Debugger commands 
 help  will give you a list of commands, inspired by
 gdb . The following commands exist and behave
similarly to their gdb counterparts:
*  help  displays help
*  where  /  backtrace  /  bt  displays the current stack trace (with a
  marker for the currently selected frame)
*  frame  selects a given frame
*  up  /  down  moves the currently selected frame up / down one
*  b  /  break  sets a breakpoint at a given location (specified either as
   <file>:<line_number>  or  <function_name> ; the function name is looked up
  in the scope of the current frame)
*  info breakpoints  lists breakpoints
*  enable ,  disable ,  delete  enable, disable, and delete a breakpoint,
  respectively
*  next  /  n  single-steps one line, skipping over function calls
*  step  /  s  single-steps one line, descending into function calls
*  finish  continues execution until the function in the currently selected
  frame returns
*  continue  /  c  continues program execution until the next breakpoint,
  or until the next time the debugger is reentered (via  debugger.enter()  or
  automatically in case of error)
*  locals  /  vlocals  shows locals in scope in the current frame;  vlocals 
  also shows values (verbose)
*  globals  /  vglobals  shows all globals
*  upvalues  /  vupvalues  shows the current function's upvalues
*  exec  /  e  executes code in the scope of the current frame
*  print  /  p  evaluates an expression in the scope of the current frame and
  prints the result
*  list  /  l  lists source code (if available); by default it lists the
  function in the current frame, but it accepts a location argument just like
   break ; just like gdb, repeating  l  without arguments continues listing
  the same file
*  quit  /  q  quits the debugger; the program is resumed. 
 Note that  locals ,  globals , or  upvalues  will occasionally show a
synthetic name for a variable (such as  _dbgl_tmp_4 ). These indicate variables
that have been shadowed in the current scope (and so their original name
now refers to something else) or internal Lua temporaries (modifying those
is ill-advised). LipNet: End-to-End Sentence-level Lipreading. 
 Yannis M. Assael, Brendan Shillingford, Shimon Whiteson, Nando de Freitas 
 Links 
 -  arXiv pre-print 
 Bibtex 
 @article{assael2016lipnet,
  title={LipNet: End-to-End Sentence-level Lipreading},
  author={Assael, Yannis M and Shillingford, Brendan and Whiteson, Shimon and de Freitas, Nando},
  journal={GPU Technology Conference},
  year={2017}
}
 
 License 
 Code licensed under the Apache License v2.0. num2word 
 Line-for-line Lua port of  http://stackoverflow.com/questions/25150316/convert-numbers-to-english-strings 
 Python version fixed a ""zero thousand"" bug. Reasonably high quality for numbers smaller than a billion, but a few odd spellings left uncorrected e.g. 18. 
 Test of equivalent implementation 
 bash
luajit test_num2word.lua > lua.txt
python test_num2word.py > py.txt
diff lua.txt py.txt || echo ""they are different"" Wrapper for CUDA profiler start/stop API functions. Zero dependencies. 
 Example:
```python
import cudaprofile 
 cudaprofile.start() 
 ... do expensive cuda stuff ... 
 cudaprofile.stop()
 ``
and run the script from nvprof or nvvp`. 
 You may want to use  nvprof  with  --profile-from-start-off  and only call  start()  when desired. wifi-locate (Python) 
 Locates the Wi-Fi-enabled machine using nearby Wi-Fi access points' relative signal strengths. Uses Google's API. 
 To use, call  linux_scan  or  osx_scan , then give the result to  locate  which returns  (accuracy, (lat,lng)) . 
 Pretty useful for  xflux  or fetching weather. 
 Quick start: 
 Install:
 bash
pip install git+https://github.com/bshillingford/wifi-locate 
 Example:
 python
from wifilocate import locate, linux_scan
accuracy, latlng = locate(linux_scan(device=""wlan0"", iwlist_path='/sbin/iwlist'))
print(accuracy, latlng)  # e.g. 25, (50.1234567, -1.234567) 
 Details 
 Calls Google's API (most likely used in Firefox, based on the URL). The module supports Python 2 and 3, and only depends on  requests . If you don't yet have requests, consider my dependency on it a favour. It's great. 
 In Linux this uses  iwlist , and in OS X it uses a little-known but built-in utility called  airport . This is a fork of the excellent extension Better Google Tasks: 
 
 https://chrome.google.com/webstore/detail/better-google-tasks/denjcdefjebbmlihdoojnebochnkgcin?hl=en-GB 
 http://richwells.me/blog/better-google-tasks/ 
 
 I added functionality for handling URLs to a specific task list, like this:
 https://mail.google.com/tasks/canvas#List Name Here 
and changing the default task list to open via the extension's options dialog. sharearray 
 Have you worried about creating large identical numpy arrays across processes due to RAM wastage, e.g. datasets that are big enough to fit in RAM but large enough to cause concern when running multiple jobs using the same data?
 sharearray  efficiently caches numpy arrays in RAM (using shared memory in  /dev/shm , no root needed) locally on a machine. 
 Usage is simple, using the  cache  function or  decorator  decorator.
A first call saves the result of the call into the built-in RAM disk, and
returns a read-only memory-mapped view into it.
Since it's in RAM, there's no performance penalty.
Any subsequent calls with the same ID will return an identical read-only memory mapped view,
even across processes. The IDs are  global . 
 Installation:
 pip install git+https://github.com/bshillingford/python-sharearray 
or
 git clone https://github.com/bshillingford/python-sharearray
python setup.py install 
 Usage 
 Using  decorator : 
 ```python
@sharearray.decorator('some_unique_id', verbose=False)
def get_training_data():
    # create largeish / expensive-to-generate data
    return my_array # some instance of np.ndarray 
 first call, across all processes, creates the array 
 arr_view = get_training_data() 
 all further calls are cached/memoized: we return a view into memory 
 arr_view_2 = get_training_data()
``` 
 Using the  cache  function: 
 ```python
import sharearray
import numpy as np
arr = sharearray.cache('my_global_id', lambda: create_large_array()) 
 or: 
 arr = sharearray.cache('my_global_id', lambda: create_large_array())
 ``
where, for instance, create_large_array` returns a large training set, potentially performing expensive feature transformations or data augmentations first. 
 By default, the file is at  /dev/shm/sharearray_my_global_id.npy , and to avoid concurrency
issues when first generating the array, and to avoid duplicated computation,  
 For futher details, read the docstrings. You may be interested in the  timeout ,  verbose , and  log_func  arguments (to either  cache  or  decorator ). 
 PyTorch 
 Since PyTorch does not yet support memmapped files (at time of writing), we can instead just create torch Tensors that point to the memory mapped by numpy:
 python
data_numpy = get_training_data()          # numpy.ndarray
data_torch = torch.from_numpy(data_numpy) # torch.Tensor 
 Notes 
 TODO: support returning multiple arrays (e.g. as a tuple or dict) from the callback / decorated function 
 There exist similar libraries in Python already, but this just makes it easier to do as a memoization-style API. Also, this module is a single file, and does not write anything in C."
hmansell,
clementfarabet,"sys 
 Has moved to a more community friendly  repo . xlua 
 Has moved to a more community friendly  repo . image 
 Has moved to a more community friendly  repo . nnx: experimental 'nn' components 
 The original neural network from Torch7,  nn , contains stable and widely
used modules. 'nnx' contains more experimental, unproven modules, and
optimizations. Modules that become stable and which are proven useful make 
their way into 'nn' (some already have). 
 Library Documentation 
 This section includes documentation for the following objects: 
 
 SoftMaxTree  : a hierarchical log-softmax Module; 
 TreeNLLCriterion  : a negative log-likelihood Criterion for the SoftMaxTree; 
 CTCCriterion  : a Connectionist Temporal Classification Criterion based on  warp-ctc ; 
 PushTable (and PullTable)  : extracts a table element and inserts it later in the network; 
 MultiSoftMax  : performs a softmax over the last dimension of a 2D or 3D input; 
 SpatialReSampling  : performs bilinear resampling of a 3D or 4D input image; 
 [QDRiemaNNLinear] (#nnx.QDRiemaNNLinear) : quasi-diagonal reduction for Riemannian gradient descent 
 Recurrent  : a generalized recurrent neural network container; 
 
 
 SoftMaxTree 
 A hierarchy of parameterized log-softmaxes. Used for computing the likelihood of a leaf class. 
This Module should be used in conjunction with the  TreeNLLCriterion . 
Using this for large vocabularies (100,000 and more) greatly accelerates training and evaluation 
of neural network language models (NNLM). 
A vocabulary hierarchy is provided via the  dp  package's
 BillionWords 
 DataSource . 
 The constructor takes 2 mandatory and 4 optional arguments : 
 *  inputSize  : the number of units in the input embedding representation;
 *  hierarchy  : a Tensor mapping one  parent_id  to many  child_id  (a tree);
 *  rootId  : a number identifying the root node in the hierarchy. Defaults to  -1 ;
 *  accUpdate  : when the intent is to use  backwardUpdate  or  accUpdateGradParameters , set this to true to save memory. Defaults to false;
 *  static  : when true (the defualt), returns parameters with keys that don't change from batch to batch;
 *  verbose  : prints some additional information concerning the hierarchy during construction. 
 The  forward  method returns an  output  Tensor of size 1D, while 
 backward  returns a table  {gradInput, gradTarget} . The second 
variable is just a Tensor of zeros , such that the  targets  can be 
propagated through  Containers  
like  ParallelTable . 
 ```lua 
 
 input = torch.randn(5,10)
target = torch.IntTensor{20,24,27,10,12}
gradOutput = torch.randn(5)
root_id = 29
input_size = 10  
hierarchy = { 
 
 [29]=torch.IntTensor{30,1,2}, [1]=torch.IntTensor{3,4,5}, 
   [2]=torch.IntTensor{6,7,8}, [3]=torch.IntTensor{9,10,11},
   [4]=torch.IntTensor{12,13,14}, [5]=torch.IntTensor{15,16,17},
   [6]=torch.IntTensor{18,19,20}, [7]=torch.IntTensor{21,22,23},
   [8]=torch.IntTensor{24,25,26,27,28}
}
smt = nn.SoftMaxTree(input_size, hierarchy, root_id)
smt:forward{input, target}
-3.5186
-3.8950
-3.7433
-3.3071
-3.0522
[torch.DoubleTensor of dimension 5]
smt:backward({input, target}, gradOutput)
{
  1 : DoubleTensor - size: 5x10
  2 : IntTensor - size: 5
} 
 
 
 ``` 
 
 TreeNLLCriterion 
 Measures the Negative log-likelihood (NLL) for  SoftMaxTrees . 
Used for maximizing the likelihood of SoftMaxTree outputs.
The SoftMaxTree Module outputs a column Tensor representing the log likelihood
of each target in the batch. Thus SoftMaxTree requires the targets.
So this Criterion only computes the negative of those outputs, as 
well as its corresponding gradients. 
 
 
 PushTable (and PullTable) 
 PushTable and PullTable work together. The first can be put earlier
in a digraph of Modules such that it can communicate with a 
PullTable located later in the graph.  PushTable:forward(input)  
for an  input  table of Tensors to the output, excluding one, the index of which 
is specified by the  index  argument in the  PushTable(index)  constructor.
The Tensor identified by this  index  is communicated to one or many 
PullTables created via the  PushTable:pull(index)  factory method. 
These can be inserted later in the digraph such that 
a call to  PushTable:forward(input) , where  input  is a table or a Tensor, 
will output a table with the previously  pushed  Tensor inserted 
at index  index . 
 An example utilizing the above  SoftMaxTree  Module
and a Linear Module demonstrates how the PushTable can be used to 
forward the  target  Tensor without any other 
 Table Modules :
```lua 
 
 mlp = nn.Sequential()
linear = nn.Linear(50,100)
push = nn.PushTable(2)
pull = push:pull(2)
mlp:add(push)
mlp:add(nn.SelectTable(1))
mlp:add(linear)
mlp:add(pull)
mlp:add(smt) --smt is a SoftMaxTree instance
mlp:forward{input, target} -- input and target are defined above
-3.5186
-3.8950
-3.7433
-3.3071
-3.0522
[torch.DoubleTensor of dimension 5]
mlp:backward({input, target}, gradOutput) -- so is gradOutput
{
  1 : DoubleTensor - size: 5x10
  2 : IntTensor - size: 5
}
 The above code is equivalent to the following: lua
mlp2 = nn.Sequential()
para = nn.ParallelTable()
para:add(linear)
para:add(nn.Identity())
mlp2:add(para)
mlp2:add(smt)
mlp2:forward{input, target}
-3.5186
-3.8950
-3.7433
-3.3071
-3.0522
[torch.DoubleTensor of dimension 5]
mlp2:backward({input, target}, gradOutput)
{
  1 : DoubleTensor - size: 5x10
  2 : IntTensor - size: 5
}
```
In some cases, this can simplify the digraph of Modules. Note that 
a PushTable can be associated to many PullTables, but each PullTable 
is associated to only one PushTable. 
 
 
 CTCCriterion 
 criterion = nn.CTCCriterion() 
Creates a Criterion based on Baidus'  warp-ctc  implementation.
This Module measures the loss between a 3D output of (batch x time x inputdim) and a target without needing alignment of inputs and labels.
Must have installed warp-ctc which can be installed via luarocks:
 luarocks install http://raw.githubusercontent.com/baidu-research/warp-ctc/master/torch_binding/rocks/warp-ctc-scm-1.rockspec 
Supports cuda via:
 criterion = nn.CTCCriterion():cuda() 
Example:
```
output = torch.Tensor({{{1,2,3,4,5},{6,7,8,9,10}}}) -- Tensor of size 1x1x5 (batch x time x inputdim).
label = {{1,3}}
sizes = torch.Tensor({2}) -- Size of each sequence (sequence-length) in the batch as a tensor
ctcCriterion = nn.CTCCriterion() 
 err = ctcCriterion:forward(output,label,sizes)
gradOut = ctcCriterion:backward(output,label)
print(""----CPU----"")
print(""Error : "" .. err)
print(""Gradients :"")
print(gradOut) 
 ctcCriterion = ctcCriterion:cuda() -- Switch to cuda implementation.
output = output:cuda() 
 err = ctcCriterion:forward(output,label,sizes)
gradOut = ctcCriterion:backward(output,label)
print(""----GPU----"")
print(""Error : "" .. err)
print(""Gradients :"")
print(gradOut)
``` 
 gives the output:
```
----CPU---- 
Error : 4.9038286209106 
Gradients : 
(1,.,.) = 
  0.0117 -0.9683  0.0861  0.2341  0.6364
  0.0117  0.0317  0.0861 -0.7659  0.6364
[torch.FloatTensor of size 1x2x5] 
 ----GPU---- 
Error : 4.9038290977478 
Gradients : 
(1,.,.) = 
  0.0117 -0.9683  0.0861  0.2341  0.6364
  0.0117  0.0317  0.0861 -0.7659  0.6364
[torch.CudaTensor of size 1x2x5]
```
 
 MultiSoftMax 
 This Module takes 2D or 3D input and performs a softmax over the last dimension. 
It uses the existing  SoftMax  
CUDA/C code to do so such that the Module can be used on both GPU and CPU. 
This can be useful for  keypoint detection . 
 
 SpatialReSampling 
 Applies a 2D re-sampling over an input image composed of
several input planes (or channels, colors). The input tensor in  forward(input)  is 
expected to be a 3D or 4D tensor of size :  [batchSize x] nInputPlane x width x height . 
The number of output planes will be the same as the number of input
planes. 
 The re-sampling is done using  bilinear interpolation . 
For a simple nearest-neihbor upsampling, use  nn.SpatialUpSampling() ,
and for a simple average-based down-sampling, use 
 nn.SpatialDownSampling() . 
 If the input image is a 3D tensor of size  nInputPlane x height x width ,
the output image size will be  nInputPlane x oheight x owidth  where
 owidth  and  oheight  are given to the constructor. 
 Instead of  owidth  and  oheight , one can provide  rwidth  and  rheight , 
such that  owidth = iwidth*rwidth  and  oheight = iheight*rheight . 
 As an example, we can run the following code on the famous Lenna image:
 lua
require 'image'                                                           
require 'nnx'
input = image.loadPNG('doc/image/Lenna.png')
l = nn.SpatialReSampling{owidth=150,oheight=150}
output = l:forward(input)
image.save('doc/image/Lenna-150x150-bilinear.png', output) 
 The input: 
   
 The re-sampled output: 
   
 
 QDRiemaNNLinear 
 The Quasi-Diagonal Riemannian Neural Network Linear (QDRiemaNNLinear) module is an implementation
of the quasi-diagonal reduction of metrics, used for Riemannian gradient descent.
The algorithm is defined in Riemannian metrics for neural networks I: feedforward networks by Yann Ollivier (http://arxiv.org/abs/1303.0818) and an efficient implementation is described in Practical Riemannian Neural Networks by Yann Ollivier and Gaetan Marceau-Caron (http://arxiv.org/abs/1602.08007).
To use this module, simply replace  nn.Linear(ninput,noutput)  with  nnx.QDRiemaNNLinear(ninput,noutput) .
As always, the step-size must be chosen accordingly.
Two additional arguments are also possible:
* gamma (default=0.01): determine the update rate of the metric for a minibatch setting, i.e., (1-gamma) * oldMetric + gamma newMetric. Smaller minibatches require a smaller gamma. A default value depending on the size of the minibatches is  gamma = 1. - torch.pow(1.-1./nTraining,miniBatchSize)  where  nTraining  is the number of training examples of the dataset and  miniBatchSize  is the number of training examples per minibatch. 
* qdFlag (default=true): Whether to use the quasi-diagonal reduction (true) or only the diagonal (false). The former should be better. 
 This module is a straightforward implementation of the outer product gradient descent. 
 Requirements 
 
 Torch7 (www.torch.ch) 
 
 Installation 
 
 Install Torch7 (refer to its own documentation). 
 clone this project into dev directory of Torch7. 
 Rebuild torch, it will include new projects too. 
 
 Use the library 
 First run torch, and load nnx: 
 sh
$ torch   
 ``` lua 
 
 require 'nnx'
``` 
 
 Once loaded, tab-completion will help you navigate through the
library (note that most function are added directly to nn): 
 ``` lua 
 
 nnx. + TAB
...
nn. + TAB
``` 
 
 In particular, it's good to verify that all modules provided pass their
tests: 
 ``` lua 
 
 nnx.test_all()
nnx.test_omp()
``` 
 
 
 Recurrent 
 DEPRECATED July 6th, 2015. Use  rnn  instead. This repo is a container for all my Torch7 packages. 
 Note: all these packages used to be distributed into a big messy repo 
called XLearn. 
 Retrieve all packages 
 This repo is empty, and only contains references to other GIT
repos. You can retrieve all of them like this: 
 sh
$ git submodule init
$ git submodule update 
 Install 
 1/ Torch7 and dependencies: 
 On Linux (Ubuntu > 9.04): 
 sh
$ apt-get install gcc g++ git libreadline5-dev cmake wget libqt4-core libqt4-gui libqt4-dev 
 On Mac OS (Leopard, or more), using  Homebrew : 
 sh
$ brew install git readline cmake wget qt 
 Then on both platforms: 
 sh
$ git clone https://github.com/andresy/torch
$ cd torch
$ mkdir build; cd build
$ cmake ..
$ make
$ [sudo] make install 
 2/ Packages: 
 Once Torch7 is installed, it comes with a package manager
that you can use to either install packages from the web: 
 sh
$ torch-pkg install pkg-name
$ torch-pkg --help 
 or build them locally, if you are planning to work on the 
sources: 
 sh
$ cd pkg-name
$ torch-pkg deploy 
 Use Torch7 
 First run torch, and load a package: 
 sh
$ torch   
 ``` lua 
 
 require 'imgraph'
``` 
 
 Once loaded, tab-completion will help you navigate through the
library (note: tab-completion will only work if you have
Qt4 and readline): 
 ``` lua 
 
 imgraph. + TAB
imgraph.colorize(           imgraph.connectcomponents( 
imgraph.graph(              imgraph.histpooling(       
imgraph.segmentmst(         imgraph.testme(            
imgraph.watershed(          imgraph.gradient(
``` 
 
 Most packages then provide a testme() function to quickly see
what it does: 
 ``` lua 
 
 imgraph.testme()
``` 
 
 Checkout the demos & tutorials 
 sh
$ cd demos   
 this repo contains demos, and tutorials to get started. Looking
at the code is the best way to get there! 
 Developers 
 If you would like to develop one of the submodules you should check
out the master branch of that module:  
 sh
$ cd nnx
$ git checkout master
$ git pull 
 This puts you at the head of development for that submodule, and in
the proper branch to commit any changes you make to the git repository
for that module.  To check out all the submodules in developer mode we
have added the script : 
 sh
$ ./gitall.sh 
  a simple command to repeat a git command to all subdirectories 
 syntax: 
 ./gitall.sh  
 eg: 
 
 switch all submodules to the master branch
  ./gitall.sh checkout master 
 pull updates for all submodules
  ./gitall.sh pull 
 other useful
  ./gitall.sh status
  ./gitall.sh diff 
 
 WARNING: will blindly send command(s) to git in each directory imgraph: a package to create/manipulate graphs on images 
 This package provides standard functions to
create and manipulate edge-weighted graphs 
of images: create a graph, segment it, 
compute its watershed, or its connected
components... 
 Install 
 1/ Torch7 is required: 
 Dependencies, on Linux (Ubuntu > 9.04): 
 sh
$ apt-get install gcc g++ git libreadline5-dev cmake wget libqt4-core libqt4-gui libqt4-dev libboost-all-dev 
 Dependencies, on Mac OS (Leopard, or more), using  Homebrew : 
 sh
$ brew install git readline cmake wget qt 
 Then on both platforms: 
 sh
$ git clone https://github.com/andresy/torch
$ cd torch
$ mkdir build; cd build
$ cmake ..
$ make
$ [sudo] make install 
 2/ Once Torch7 is available, install this package: 
 sh
$ [sudo] torch-rocks install imgraph 
 Use the library 
 First run torch, and load imgraph: 
 sh
$ torch   
 ``` lua 
 
 require 'imgraph'
``` 
 
 Once loaded, tab-completion will help you navigate through the
library: 
 ``` lua 
 
 imgraph. + TAB
imgraph.colorize(           imgraph.connectcomponents( 
imgraph.graph(              imgraph.histpooling(       
imgraph.segmentmst(         imgraph.testme(            
imgraph.watershed(          imgraph.gradient(
``` 
 
 To get quickly started, run the testme() function: 
 ``` lua 
 
 imgraph.testme()
``` 
 
 which computes a few things on the famous image of Lena: 
 neuFlow 
 neuFlow  is dataflow architecture optimized for large array/tensor
transforms, and especially image processing operations.  More info about the
architecture, hardware and applications can be found
 here . 
 this package 
 This package is a compiler toolkit for neuFlow. It is entirely written in
 Lua , and relies on
 Torch7  to represent N-dimensional arrays
efficiently. It also interfaces Torch7's neural-network package natively. 
 how to install 
 Torch7 must be install first, a task most easily accomplished using the single
line  install script . 
 or alternatively to install Torch7 and the neuFlow package by hand, you will
need to install a few dependencies. 
 On Linux (Ubuntu): 
 sh
$ apt-get install gcc g++ git libreadline5-dev cmake wget
$ apt-get install libqt4-core libqt4-gui libqt4-dev
$ apt-get install ffmpeg gnuplot 
 On Mac OS X (> 10.5): get  Homebrew 
and then: 
 sh
$ brew install git readline cmake wget
$ brew install qt
$ brew install ffmpeg gnuplot 
 You're ready to install Torch7 (www.torch.ch). The most up to date instructions
can be found at the  Torch7 github page . 
 ``` sh
$ git clone git://github.com/andresy/torch.git
$ cd torch
$ mkdir build
$ cd build 
 $ cmake ..
OR
$ cmake .. -DCMAKE_INSTALL_PREFIX=/my/install/path
``` 
 Or if you already have a previous Torch7 installed: 
 sh
$ luarocks install torch WITH_LUA_JIT=1 # Torch7, an efficient numeric library for Lua 
 You will also need additional packages: 
 sh
$ luarocks install image        # an image library for Torch7
$ luarocks install nnx          # lots of extra neural-net modules
$ luarocks install camera       # a camera interface for Linux/MacOS
$ luarocks install ffmpeg       # a video decoder for most formats
$ luarocks install inline-c     # inline C capability 
 Now that Torch7 has been installed the neuflow package can be installed.
Installing the neuflow package requires you to download the source code
repository. It'll give you access to some demos, to get started: 
 sh
$ git clone https://github.com/clementfarabet/neuflow.git
$ cd neuflow
$ luarocks make 
 how to run code on neuFlow 
 Demos are located in demos/. To get started, you'll need
a standard Xilinx dev board for the Virtex 6: [the ML605 Kit]
(http://www.xilinx.com/products/devkits/EK-V6-ML605-G.htm).
We provide an image of neuFlow that's pre synthesized/mapped/routed
for the Virtex6 VLX240T on this platform. 
 To run any of the demos, follow these instructions (tested on
Ubuntu 9.04, 10.04 and Mac OS X 10.5, 10.6 and 10.7). 
 ``` sh
$ git clone https://github.com/clementfarabet/neuflow.git
$ cd neuflow 
 make Xilinx tools available (it implies you have them 
 installed somewhere...) 
 $ source $XILINX_INSTALL_PATH/settings**.sh 
 turn on the ML605, plug the JTAG cable then load one of 
 our pre-built bitfiles *: 
 $ cd scripts
$ ./get-latest-neuflow-image
$ ./load-bitfile neuFlow-ml605.bit 
 at this points, you just have wait 2 seconds that the Ethernet 
 LEDs are back on (out of reset) 
 run the simplest demo, a loopback client, to verify your setup **: 
 $ cd ../demos
$ sudo torch loopback.lua # on Linux
or
$ ./loopback.lua # on OSX 
 before loading a new demo, you have to reset neuFlow: for 
 now it is done by pressing the SW10 button (cpu rst) 
 then you can run a typical convnet-based program, a face detector: 
 $ sudo torch face-detector.lua # on Linux
or
$ ./face-detector.lua # on OSX
``` 
 (*) the load-bitfile script assumes that you have properly installed Xilinx's
USB cable driver. On RedHat and derivatives it works out of the box when
installing Xilinx ISE, but on Ubuntu you'll have to follow these instructions:
http://rmdir.de/~michael/xilinx/.  This is not doable on Mac OS X
unfortunately. I usually flash the ML605 board using Ubuntu (even a virtual box
version works), and then run all the demos under Mac OS X. 
 (**) you need to have admin privileges on your machine (sudo) to be able to
interact with neuFlow, as we're using a custom low-level Ethernet framing
protocol. UNSUP 
 A package for unsupervised learning in Torch. 
 Provides modules that are compatible with  nn  ( LinearPsd ,  ConvPsd ,  AutoEncoder , ...),
and self-contained algorithms ( k-means ,  PCA ). 
 Requirements 
 Basic dependencies: 
 
 Torch7 (github.com/andresy/torch) 
 kex    (github.com/koraykv/tools) 
 optim  (github.cim/koraykv/optim) 
 
 To run the demo scripts, you also need the following: 
 
 image (github.com/clementfarabet/lua---image) 
 sys   (github.com/clementfarabet/lua---sys) 
 xlua  (github.com/clementfarabet/lua---xlua) 
 
 Installation 
 Build/Install: 
 
 Install Torch7 (refer to its own documentation). 
 clone all other repos (including this one) into dev directory of Torch7. 
 Rebuild torch, it will include all these projects too. 
 
 Alternatively, you can use torch's package manager. Once
Torch is installed, you can install  unsup :  $ torch-pkg install unsup . para||el: a (simple) parallel computing framework for Torch 
 This package provides a simple mechanism to dispatch and run Torch/Lua code
as independant processes and communicate via ZeroMQ sockets. Processes
can be forked locally or on remote machines. 
 Install 
 Install ZeroMQ 3 : 
 bash
sudo apt-get install libzmq3-dev libzmq3 
 Install Torch7 per instructions at http://torch.ch/ . 
 Download and compile this package using luarocks: 
 bash
[sudo] luarocks install parallel 
 or  
 bash
git clone https://github.com/clementfarabet/lua---parallel.git
cd lua---parallel
luarocks make 
 Use the library 
 API, in very short: 
 Load/start up package: 
 lua
require 'parallel' 
 Fork a new process, or N new processes, locally: 
 lua
parallel.fork()
parallel.nfork(4) 
 Fork remote processes. In that following code, we fork 4 processes on myserver.org,
and 6 processes on myserver2.org. 
 lua
parallel.nfork( {4, ip='myserver.org', protocol='ssh', lua='/path/to/remote/torch'},
                {6, ip='myserver2.org', protocol='ssh', lua='/path/to/remote/torch'} ) 
 Even more flexible, a list of machines can be established first, so that 
a call to sfork() [smart fork] can automatically distribute the forked processes
onto the available machines: 
 ``` lua
parallel.addremote( {ip='server1.org', cores=8, lua='/path/to/torch', protocol='ssh -Y'},
                    {ip='server2.org', cores=16, lua='/path/to/torch', protocol='ssh -Y'},
                    {ip='server3.org', cores=4, lua='/path/to/torch', protocol='ssh -Y'} )
parallel.sfork(16) 
 -- in this example, the 16 processes will be distributed over the 3 machines:
-- server1.org: 6 processes
-- server2.org: 6 processes
-- server3.org: 4 processes
``` 
 In the spirit of  really  abstracting where the jobs are executed, calibrate() can
be called to estimate the compute power of each machine, so that you can distribute
your load accordingly. 
 lua
parallel.addremote(...)
parallel.calibrate()
forked = parallel.sfork(parallel.remotes.cores)  -- fork as many processes as cores available
for _,forked in ipairs(forked) do
   print('id: ' .. forked.id .. ', speed = ' .. forked.speed)
end
-- the speed of each process is a number ]0..1]. A coef of 1 means that it is the
-- fastest process available, and 0.5 for example would mean that the process is 2x
-- slower 
 Once processes have been forked, they all exist in a table: parallel.children, and
all methods (exec,send,receive,join) work either on individual processes, or on
groups of processes. 
 The first thing to do is to load these new processes with code. The code given
can either be a function, with no arguments (it won't have any env when executing
in the new process), or a string. Whether it is a string or a function, both
get serialized into strings, and reloaded on the process side, using loadstring(). 
 ``` lua
-- define process' code:
code = function()
   -- arbitrary code contained here
   require 'torch'
   t = torch.Tensor(10)
   print(t) 
 -- any process can access its id, its parent's id [and children's id]
   print(parallel.id)
   print(parallel.parent.id)
   if parallel.children[1] then print(parallel.children[1].id) end 
 -- if arguments were passed, they're found in the regular ... table       
   args = {...}   
   print(args[1])
end 
 -- execute code in given process(es), with optional arguments:
parallel.children:exec(code) 
 -- this is equivalent to:
for _,child in ipairs(parallel.child) do
    child:exec(code)
end
``` 
 parallel implements a simple yield/join mechanism to allow a parent to sync
and affect the behavior of its children. 
 ``` lua
-- child code:
code = function()
   while true do
      print('something')
      parallel.yield()
   end
end
c = parallel.fork()
c:exec(code) 
 -- parent code
for i = 1,10 do
    c:join()
end 
 -- each time join() is called, it waits for the child to yield, and vice-versa.
-- in that example, 'something' only gets printed when the parent joins its child
``` 
 Slightly more complex things can be implemented with yield/join: join() can take
a string as an argument, which is returned by the corresponding yield(). This
is useful to control branching in your children: 
 ``` lua
-- child code:
code = function()
   while true do
      print('something')
      m = parallel.yield()
      if m == 'break' then break end
   end
end
c = parallel.fork()
c:exec(code) 
 -- parent code
c:join('break')
``` 
 Sometimes you might want to wait for a process to actually terminate (die), so that
you can start new ones. The proper way to do this is to use the sync() function, 
which waits for the PID of that process to fully disappear from the OS. It also
clears the child from the parallel.children list, and decrement parallel.nchildren. 
 lua
code = function()
     -- do nothing and die
end
parallel.nfork(1)              -- fork one process
parallel.children:exec(code)   -- execute dummy code
print(parallel.nchildren)      -- prints: 1
parallel.children:sync()       -- wait for all children (here only 1) to die
print(parallel.nchildren)      -- prints: 0
parallel.nfork(2)              -- fork 2 processes
print(parallel.nchildren)      -- prints: 2
print(parallel.children[1])    -- prints: nil
print(parallel.children[2])    -- prints: table --- current running processes always
print(parallel.children[3])    -- prints: table --- exist in children[process.id] 
 When creating a child (parallel.fork), a connection is established
to transfer data between the two processes. Two functions send() and receive()
can be used to  efficiently  transfer data between these processes. Any Lua type, 
and all Torch7 type (tensor, storage, ...) can be transferred this way. The transmission
is efficient for numeric data, as serialization merely involves a binary copy and
some extra headers for book-keeping (see serialization in Torch7's manual). 
 ``` lua
-- define some code for children
somecode = function()
   while true do
      -- in an infinite loop, receive objects from parent:
      local obj = parallel.parent:receive()
      -- print
      parallel.print('received object:', obj)
   end
end 
 -- dispatch two processes:
parallel.nfork(2)
parallel.children:exec(somecode) 
 -- and send them some data:
t = {'a table', entry2='with arbitrary entries', tensor=torch.Tensor(100,100)}
while true do
    parallel.children[1]:send(t)        -- send the whole table to child 1
    parallel.children[2]:send(t.entry2) -- just send an entry to child 2
end
``` 
 A convenient print function that prepends the process ID issuing the print: 
 ``` lua 
 
 parallel.print('something') 
 
   something
``` 
 Last, but not least: always run your parent code in a protected call, to catch
potential errors, Ctrl+C, and the likes, and terminate nicely. By terminating
nicely, I mean: killing all remote processes that you forked... If you don't
do so, you leave you remote machines (and potentially yours) with hanging 
processes that are just waiting to receive data, and will not hesitate to get
back in business the next time you run your parent code :-) 
 ``` lua
worker = function()
       -- some worker code
end 
 parent = function()
       -- some parent code
end 
 ok,err = pcall(parent)
if not ok then
   print(err)
   parallel.close()   -- this is the key call: doing this will insure leaving a clean
                      -- state, whatever the error was (ctrl+c, internal error, ...)
end
``` 
 A simple complete example: 
 ``` lua
-- required libs
require 'parallel' 
 -- define code for workers:
function worker()
   -- a worker starts with a blank stack, we need to reload
   -- our libraries
   require 'sys'
   require 'torch' 
 -- print from worker:
   parallel.print('Im a worker, my ID is: ' .. parallel.id .. ' and my IP: ' .. parallel.ip) 
 -- define a storage to receive data from top process
   while true do
      -- yield = allow parent to terminate me
      m = parallel.yield()
      if m == 'break' then break end 
   -- receive data
  local t = parallel.parent:receive()
  parallel.print('received object with norm: ', t.data:norm())

  -- send some data back
  parallel.parent:send('this is my response')
 
 end
end 
 -- define code for parent:
function parent()
   -- print from top process
   parallel.print('Im the parent, my ID is: ' .. parallel.id) 
 -- fork N processes
   parallel.nfork(4) 
 -- exec worker code in each process
   parallel.children:exec(worker) 
 -- create a complex object to send to workers
   t = {name='my variable', data=torch.randn(100,100)} 
 -- transmit object to each worker
   parallel.print('transmitting object with norm: ', t.data:norm())
   for i = 1,1000 do
      parallel.children:join()
      parallel.children:send(t)
      replies = parallel.children:receive()
   end
   parallel.print('transmitted data to all children') 
 -- sync/terminate when all workers are done
   parallel.children:join('break')
   parallel.print('all processes terminated')
end 
 -- protected execution:
ok,err = pcall(parent)
if not ok then print(err) parallel.close() end
``` 
 License 
 Copyright (c) 2011 Clement Farabet, Marco Scoffier 
 Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
""Software""), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions: 
 The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software. 
 THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. LOpenGM: Lua bindings for OpenGM 
 OpenGM  is a C++ library for graphical 
modeling, and inference. The Lua
bindings provide a simple way of describing graphs, from Lua, and then
optimizing them with OpenGM. 
 Note: this package is superseded by 'gm', a more general and
simple (pure Lua) package for graphical models. 
 License 
 LOpenGM Copyright (c) 2011 Clement Farabet (Lua Bindings) 
 OpenGM  Copyright (c) 2010 by Bjoern Andres and Joerg Hendrik Kappes. 
 This software was developed by Bjoern Andres and Joerg Hendrik Kappes.
Enquiries shall be directed to: 
 bjoern.andres@iwr.uni-heidelberg.de, kappes@math.uni-heidelberg.de 
 All advertising materials mentioning features or use of this software must
display the following acknowledgement: ``This product includes the OpenGM
library developed by Bjoern Andres and Joerg Hendrik Kappes. Please direct
enquiries concerning OpenGM to bjoern.andres@iwr.uni-heidelberg.de,
kappes@math.uni-heidelberg.de''. 
 Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met: 
 
 Redistributions of source code must retain the above copyright notice,
  this list of conditions and the following disclaimer. 
 Redistributions in binary form must reproduce the above copyright notice,
  this list of conditions and the following disclaimer in the documentation
  and/or other materials provided with the distribution. 
 All advertising materials mentioning features or use of this software must
  display the following acknowledgement: ``This product includes the OpenGM
  library developed by Bjoern Andres and Joerg Hendrik Kappes. Please direct
  enquiries concerning OpenGM to bjoern.andres@iwr.uni-heidelberg.de,
  kappes@math.uni-heidelberg.de''. 
 The names of the authors must not be used to endorse or promote products
  derived from this software without specific prior written permission. 
 
 THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR IMPLIED
WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. 
 Install 
 1/ Torch7 is required: 
 Dependencies, on Linux (Ubuntu > 9.04): 
 sh
$ apt-get install gcc g++ git libreadline5-dev cmake wget libqt4-core libqt4-gui libqt4-dev 
 Dependencies, on Mac OS (Leopard, or more), using  Homebrew : 
 sh
$ brew install git readline cmake wget qt 
 Then on both platforms: 
 sh
$ git clone https://github.com/andresy/torch
$ cd torch
$ mkdir build; cd build
$ cmake ..
$ make
$ [sudo] make install 
 2/ Once Torch7 is available, install this package: 
 sh
$ [sudo] torch-pkg install opengm 
 Use the library 
 API, in very short: 
 Load/start up package: 
 lua
require 'opengm' 
 Construct a graph: 
 lua
g = opengm.Graph(...) 
 Optimize a graph: 
 lua
g:optimize{} 
 Display a graph, using Graphviz: 
 lua
g:show{} 
 A simple complete example: 
 ```lua
-- load opengm
require 'opengm' 
 -- standard factors
f = opengm.factors 
 -- define variables
variables = {'car', 'person', 'building', 'street', 'vehicle'} 
 -- define factors
factors = {-- unary factors (prior probabilities of each class):
           {f.prior(0.9),  {'car'}},
           {f.prior(0.01), {'person'}},
           {f.prior(0.7),  {'building'}},
           {f.prior(0.8),  {'street'}},
           {f.prior(0.4),  {'vehicle'}},
           -- Potts factors (joint probabilities):
           {f.band(0),     {'car',      'person'}},
           {f.band(0),     {'person',   'building'}},
           {f.band(0),     {'building', 'street'}},
           {f.band(0),     {'car',      'building'}},
           {f.band(0),     {'building', 'vehicle'}},
           {f.band(0),     {'street',   'vehicle'}},
           {f.band(0),     {'person',   'vehicle'}},
           {f.bimplies(1), {'car',      'vehicle'}}} 
 -- create graph
g = opengm.Graph(variables, factors) 
 -- optimize graph
g:optimize{method='a*', verbose=true} 
 -- print graph
print(g)
``` 
 Running the script above outputs: 
 <opengm> optimizing... 
step 1: E=3.99758, c=0
step 2: E=3.63212, c=2.19722
step 3: E=3.63212, c=2.19722
<opengm.Graph>
  + nb of variables: 4
  + nb of factors: 6
  + graph is acyclic
  + current (optimized) variable states: 
    - car [1]
    - person [0]
    - building [0]
    - street [0]
    - vehicle [1] Torch7 Library for iOS 
 Torch7 provides a Matlab-like environment for state-of-the-art machine
learning algorithms. It is easy to use and provides a very efficient
implementation, thanks to an easy and fast scripting language (Lua) and a
underlying C implementation. 
 This package has been modified (or just hacked) to fully compile
Torch7 for iOS (iPad/iPhone) for all architectures (armv7, armv7a, arm64, i386 (simulator), x86_64 (simulator)) 
 Requirements 
 Torch7 needs to be installed prior to building the iOS
version. 'torch' needs to be available in the user's path. 
 I recommend doing the easy install if you have not installed Torch7.
http://torch.ch/docs/getting-started.html 
 Building The Framework 
 Simply run:
$ ./generate_ios_framework 
 This will build all torch's libraries as static libs, and export them
in a single dir: framework/. The dir is ready to be included in
an iOS project: it includes an example class to load Torch from within
your Objective C project. 
 For examples full examples that utilize this class (Torch.m) please see 
the ios_examples/ folder. More examples to come soon. 
 Running 
 When creating your Objective-C project simply import the class
Torch.m/.h; include all the libs to the linker; add Torch.framework & Accelrate.framework
and add all the Lua files as resources. define YOUR_FILE.lua and add it as 
a resource. Run YOUR_FILE.lua using the method defined in Torch.h/.m liuflow: a wrapper around C.Liu's optical flow 
 Note: this bit of code is a simple wrapper around the optical-flow
algorithm developped/published by C.Liu: 
 C. Liu. Beyond Pixels: Exploring New Representations and Applications
for Motion Analysis. Doctoral Thesis. Massachusetts Institute of 
Technology. May 2009. 
 More at: http://people.csail.mit.edu/celiu/OpticalFlow/ videograph: a package to create/manipulate graphs on videos 
 This package provides standard functions to
create and manipulate edge-weighted graphs 
of videos: create a graph, segment it, get 
its adjacency matrix, ... 
 Install 
 1/ Torch7 is required: 
 Dependencies, on Linux (Ubuntu > 9.04): 
 sh
$ apt-get install gcc g++ git libreadline5-dev cmake wget libqt4-core libqt4-gui libqt4-dev 
 Dependencies, on Mac OS (Leopard, or more), using  Homebrew : 
 sh
$ brew install git readline cmake wget qt 
 Then on both platforms: 
 sh
$ git clone https://github.com/andresy/torch
$ cd torch
$ mkdir build; cd build
$ cmake ..
$ make
$ [sudo] make install 
 2/ Once Torch7 is available, install this package: 
 sh
$ [sudo] torch-pkg install videograph 
 Use the library 
 First run torch, and load videograph: 
 sh
$ torch   
 ``` lua 
 
 require 'videograph'
``` 
 
 ... IPAM Graduate Summer School 
 On Deep Learning, Feature Learning
July 9 - 27, 2012 
 More info here.   
 Day 1: Setup 
 
 
 Welcome to the Practical Sessions for the summer school 
 
 
 Objectives: 
 
 
 implementation-level understanding of supervised and unsupervised learning algorithms 
 
 
 Many algorithms are more similar than researchers in the field
    might have you believe! 
 
 
 a sense of hyper-parameter sensitivities and run-times for various
  algorithms 
 
 
 appreciation for two approaches to programming deep learning experiments 
 
 
 Code fragments for interactive exploration 
 
 
 Full-blown application 
 
 
 exposure to programming languages and software stacks: 
 
 
 Python, NumPy, SciPy, Theano 
 
 
 Lua, Torch7 
 
 
 
 
 Schedule: 1 hour on four days this first week 
 
 
 Monday 12PM - 1PM:  Lua/Torch, Python/Theano, logging in to EC2 
 
 
 Tuesday 4PM - 5PM:  Supervised Learning in Lua and Python 
 
 
 Wednesday 4PM - 5PM:  Unsupervised Learning in Lua and Python 
 
 
 Thursday 4PM - 5PM:  TBA 
 
 
 
 
 Session Structure 
 
 
 Time is short for these practical sessions! 
 
 
 Each day will start with two walk-throughs of things you can experiment with
    (we'll try to be quick, to give you time afterward!) 
 
 
 After the walk-throughs you can log in to an Amazon EC2 node where we've set
    things up. 
 
 
 For lack of time - you will have to choose whether to do the Lua thing or
    the Python thing in the in-classroom time each day. 
 
 
 We will negotiate with the organizers to leave the EC2 node up after the sessions 
 
 
 We will be around all week - feel free to ask questions any time! 
 
 
 We will be available by email after the first week. 
 
 
 
 
 10 mins crash course in Python, numpy 
 
 
 intro to IPython notebook 
 
 
 10 mins crash course in Lua, Torch7 
 
 
 will review very basic Lua and Torch concepts, to get people started 
 
 
 Remaining time - getting people into groups and setting them up to run the sample code
  on laptop or EC2. Once they get it running, they can go for lunch or stick
  around and play with things. 
 
 
 Day 2: Supervised Learning 
 
 
 Models: SVM, MLP, ConvNets, (Logistic Regression?) 
 
 
 Data Sets: MNIST, CIFAR, Google Street View House Numbers (SVHN).
  SVHN is an interesting new data set, very few results are available at this time 
  (and is more computer visionny that MNIST). 
 
 
 Optimization Methods: SGD, ASGD, L-BFGS; batch vs. mini-batch vs. online 
 
 
 Day 3: Feature Learning 
 
 
 Python: Imprinting, K-Means, Autoencoder, De-noising Autoencoder, RBM,
  (Sparse Coding?) 
 
 
 Torch: Linar Autencoder, Convolutional Autoencoder, Linear and 
  Convolutional PSD (Predictive Sparse Decomposition) Autoencoder 
 
 
 Day 4: To Be Decided 
 
 
 Persitant Contrastive Divergence? 
 
 
 Theano? 
 
 
 Recurrent Neural Networks? 
 
 
 GPU Programming 101? 
 
 
 Torch/nn extensions: write your own modules 
 
 csvigo: a package to handle CSV files (read and write). 
 Install: 
 First install Torch7 (www.torch.ch) then simply install this package
using luarocks: 
 luarocks install csvigo 
 Use: 
 The library provides 2 high-level functions: csvigo.load and csvigo.save. To get help
on these functions, simply do: 
 ``` 
 
 csvigo.save()
csvigo.load()
``` 
 
 Loading a CSV file in 'query' mode gives you a convenient query function that
you can use to query subsets of your original CSV file. To get help on this query
function, simply do: 
 ``` 
 
 query = csvigo.load{path='somefile.csv', mode='query'}
query('help')
-- print some help
all = query('all')
subset = query('union', {somevar=someval, someothervar={val1, val2}})
``` 
 
 Large CSV mode 
 CSVigo supports efficient loading of very large CSV files into memory.
The loaded data structure is a read-only table with efficiency hidden under the hood. 
 Loading: 
 lua
m = csvigo.load({path = ""my_large.csv"", mode = ""large""}) 
 Printing by default only prints first 10 and last 10 rows
 lua
print(m) 
 Individual element access
 lua
print(m[32]) 
 Size of table:
 lua
print(#m) 
 For loop over entries: 
 Type 1:
 lua
for i=1, #m do
    print(m[i]) -- get element
end 
 Type 2:
 lua
for k,v in ipairs(m) do
    print(k)
    print(v)
end 
 Type 3:
 lua
for k,v in pairs(m) do
    print(k)
    print(v)
end 
 Read-only table
 lua
-- read only table, will error here:
m[13] = 'a' json: a package to handle json (read and write). 
 TAKEN From JSON4Lua, originally written for Lua 5.1. 
 Wrapped for Torch7 (torch-rocks). 
 JSON4Lua: JSON encoding / decoding support for the Lua language.
json Module. 
 Author: Craig Mason-Jones
Homepage: http://json.luaforge.net/
Version: 0.9.40
This module is released under the MIT License (MIT). 
 Install: 
 First install Torch7 (www.torch.ch) then simply install this package
using torch-rocks: 
 torch-rocks install json 
 Usage: 
 This module exposes 4 functions: 
 ```
json_string = encode(o)
-- returns the table / string / boolean / number / nil / json.null value as a JSON-encoded string. 
 o = decode(json_string)
-- returns a Lua object populated with the data encoded in the JSON string json_string. 
 save(json_file, o)
-- saves the table / string / boolean / number / nil / json.null value as a JSON-encoded file. 
 o = load(json_file)
-- returns a Lua object populated with the data encoded in the JSON file.
``` Torch Web Terminal 
 This is a browser-based terminal for  Torch7 . 
The goal of this project is to supersed the Qt4 interface, and to 
enable full graphics capabilities within the browser. 
 This project is built around  Node.js , 
a super lightweight asynchronous framework to build servers. In
our case, the server is only use to connect clients (browser 
terminals) to Torch7 kernels. For now, one server instance can
support an arbitrary number of clients, but each client only
has access to one Torch7 kernel. 
 Dependencies 
 You will need to install a couple of dependencies to enable
this web terminal: 
 
 
 Node.js, which can be found  here , and
  should also be installable with your system's package manager 
 
 
 NPM: Node's package manager (sometimes comes with Node.js) 
 
 
 Three Node.js packages:  ejs ,  stripcolorcodes  and  express 
    (version 2.x) 
 
 
 For instance, on MacOS: 
 bash
$ brew install nodejs
$ curl http://npmjs.org/install.sh | sh
$ npm install express@2.x ejs stripcolorcodes 
 Installation 
 This project is bundled as a  torch-pkg  project, and can be 
easily installed like this: 
 bash
$ torch-pkg -local install webterm 
 Or, if you have downloaded this repository on your machine, and
you are in its directory: 
 bash
$ torch-pkg -local deploy 
 Note1: you have to deploy webterm locally (-local flag), as the node 
packages are only available to the current user. This could probably
be fixed, but I still don't know how. 
 Note2: depending on the version of Node.js, you might have to do
the NPM install in the package directory, i.e.: 
 bash
$ cd ~/.torch/usr/share/torch/lua/webterm/
$ npm install express@2.x ejs stripcolorcodes 
 Running it 
 webterm  is a standard package, so you can either require it from
a running torch instance, or start torch with it like that: 
 bash
$ torch -lwebterm 
 This should produce the following output: 
 text
Try the IDE: torch -ide
Type help() for more info
Torch 7.0  Copyright (C) 2001-2011 Idiap, NEC Labs, NYU
==> Torch server listening on port 8080
==> Open http://localhost:8080/ in your browser!
=><= Torch instance started for [t7] 
 At this stage, you just have to open a browser and go to 
 http://localhost:8080/ . The cool thing
of course is that you can access this adress remotely. Beware though
that this might open up serious security issues. 
 Functions 
 In the broswer, you will see a terminal, which provides full history
and live completion. Completed entries are shown on the left pane, and
are actual hyperlinks to documentation: 
 
 One cool thing about a browser-based terminal is that all the plots
and renderings you can generate during your session can be transparently
piped to the console: 
 
 The mechanism we use to do this is very simple: the image, or plot, is dumped
as a png into the root of the Node.js server; and we then simply print a string
of that form:  <img src=""dumped.png""/>  to the terminal. 
 In fact, this mechanism is completely general: try doing this in the terminal: 
 lua
print '<h1>Some title</h1> <p>a paragraph...</a>' 
 Now even more powerful: you can really print arbitrary html there, so printing 
something like: 
 lua
print '<script> console.log(""this is javascript!"") </script>' 
 ... will just work perfectly fine! 
 Multiple Users 
 By default, the user is set to  t7 , which is what you should see in the terminal.
You can create a new user by appending the string  ?user=bob  to the URL. That'll
create a  completely  new Torch7 kernel, which only Bob sees. 
 TO DO 
 
 completion is still buggy: it starts screwing up after too many nested parenthesis 
 inline help (triggered by the  ?  symbol) is shitty: we should use the full html
  help instead of the poor text-based help 
 I'd love to have notebook-like capabilities, where we can load a markdown file into
  the browser (using the URL would be ok for now,  e.g.   ?file=myscript.md ), and 
  the text part would get rendered as html, and all the code blocks will be transformed
  in interpretable code blocks,  ala  Mathematica/IPython. 
 that last point implies that we need more flexible code entries, where we can go
  back and forth to edit the code. 
 ctrl+C: not working yet. It generates a INT signal, but it doesn't seem to
  do much for now. 
 Taken from:
http://www.steve.org.uk/Software/lua-fs/docs/index.html Torch (Easy) Install 
 This is just an easy install script for Torch7. Eventually, it will be folded into the main repo. 
 The goal of this script is to enable one line installs. To install Torch on any machine, just do: 
 curl -s https://raw.github.com/clementfarabet/torchinstall/master/install-all | bash
 
 Once installed, you should be able to run Torch with basic libraries: 
 torch -lparallel -loptim -lpl -limage
 
 This script has been tested on MacOS X 10.8, and Ubuntu 12.04. It should work on earlier 
Ubuntus and MacOS Xs, but other platforms are not supported. 
 On Ubuntu you'll need 'sudo' privileges, as the default install is global, 
and the script needs to install dependencies. 
 If you've already installed the dependencies, and don't have root privileges, you 
can use this command to just install Torch: 
 curl -s https://raw.github.com/clementfarabet/torchinstall/master/install-torch | bash
 
 By default, it will install Torch in /usr/local/ , you can override this
default path by doing: 
 curl -s https://raw.github.com/clementfarabet/torchinstall/master/install-torch | PREFIX=~/local bash
 
 Torch7 now ships wih Luarocks, bundlde into an executable called torch-rocks.
You can install new packages like this: 
 torch-rocks search lua-cjson
torch-rocks install lua-cjson
 
 By default, torch-rocks includes a link to our own Rocks repository, hosted
 here . If you wish to publish your 
packages as rocks for Torch, simply clone this repo, add your rocks, and
make a pull request on Github! 
 Updating from a previous version 
 Note that if you are coming from a previous version you are advise to clean up the old installation
with the following commands 
 rm -rf ~/.luarocks
rm -rf /usr/local/lib/luarocks/
rm -rf /usr/local/lib/lua/
rm -rf /usr/local/share/torch/
rm -rf /usr/local/share/lua/
rm -rf /usr/local/lua/
rm -rf /usr/local/etc/luarocks/ nn2 
 nn2 is the successor of nn. The main thing we're trying to achieve here is: 
 
 better consistency across modalities (Volumetric, Spatial, Temporal) 
 better performance by packing features in memory 
 
 TODO List 
 'Spatial' modules need to invert their convention, by packing the features
in memory. Modules affected: 
 
 SpatialConvolution 
 SpatialConvolutionMap        
 SpatialMaxPooling 
 SpatialSubSampling 
 Spatial*Normalization 
 SpatialLPPooling 
 SpatialZeroPadding 
 
 'Volumetric' modules: 
 
 VolumetricConvolution 
 Lunatic: Python in Lua 
 Run a python interpreter within Lua. Pass data between python and lua. 
 Bug-fixed fork of  lua---python  which itself is forked from  Lunatic Page . 
 See  Lunatic Page  for original documentation. 
 Install 
 Clone this repo locally 
 $ git clone git@github.com:dylski/lua---python.git
 
 Build and install 
 $ cd lua---python
$ luarocks make python-scm-0.rockspec
 torchffi 
 Has moved to a more community friendly  repo . XML <> Lua 
 This was taken from  this site . Licensed under an MIT license. 
 Simply repackaged their code for Torch.
I also added a parse() function, which simplifies the xml->table cnoversion. GraphicsMagick 
 A simple Lua wrapper to  GraphicsMagick . 
 Only tested on Mac OSX, with GraphicsMagick installed via Homebrew. 
 gm.convert 
 This is just a binding to the command line convert utility (images are not loaded
into Lua's memory). Examples: 
 lua
gm = require 'graphicsmagick'
gm.convert{
   input = '/path/to/image.png',
   output = '/path/to/image.jpg',
   size = '128x128',
   quality = 95,
   verbose = true
} 
 gm.info 
 Similarly, gm.info(file) is a simple binding to the command line utility.
It's handy to extra the geometry of an image, as well as its exif metadata.
On top of it, if geolocation is found, the GPS location is nicely formatted. 
 lua
gm = require 'graphicsmagick'
info = gm.info('some.jpeg')
print(info)
{
   width : 1024
   height : 768
   date : 2013:01:01 00:00:01
   location :
     {
       longitude : W80.13
       latitude : N25.79
     }
   format : JPEG
   exif :
     {
        Make : Apple
        FocalLength : 413/100
        ...
     }
} 
 gm.Image 
 This is a full C interface to GraphicsMagick's Wand API. We expose one Class: the
Image class, which allows loading and saving images, transforming them, and
importing/exporting them from/to torch Tensors. 
 Load library: 
 lua
gm = require 'graphicsmagick' 
 First, we provide two high-level functions to load/save directly into/form tensors: 
 lua
img = gm.load('/path/to/image.png' [, type])    -- type = 'float' (default) | 'double' | 'byte'
gm.save('/path/to/image.jpg' [,quality])        -- quality = 0 to 100 (for jpegs only) 
 The following provide a more controlled flow for loading/saving jpegs. 
 Create an image, from a file: 
 lua
image = gm.Image('/path/to/image.png')
-- or
image = gm.Image()
image:load('/path/to/image.png') 
 Create an image, from a file, with a hint about the max size to be used: 
 ```lua
image:load('/path/to/image.png', width [, height]) 
 -- this tells the image loader that we won't need larger images than
-- what's specified. This can speedup loading by factors of 5 to 10.
``` 
 Save an image to disk: 
 ```lua
image:save('filename.ext') 
 -- where:
-- ext must be a know image format (jpg, JPEG, PNG, ...)
-- (GraphicsMagick supports tons of them)
``` 
 Create an image, from a Tensor: 
 ```lua
image = gm.Image(tensor,colorSpace,dimensions)
-- or
image = gm.Image()
image:load(tensor,colorSpace,dimensions) 
 -- where:
-- colorSpace is: a string made of these characters: R,G,B,A,C,Y,M,K,I
--                (for example: 'RGB', 'RGBA', 'I', or 'BGRA', ...)
--                R: red, G: green, ... I: intensity
--
-- dimensions is: a string made of these characters: D,H,W
--                (for example: 'DHW' or 'HWD')
--                D: depth, H: height, W: width
``` 
 Export an image to a Tensor: 
 ```lua
image = gm.Image('path.jpg')
image:toTensor(type,colorSpace,dimensions) 
 -- where:
-- type : 'float', 'double', or 'byte'
-- colorSpace : same as above
-- dimensions : same as above
``` 
 When exporting Tensors, we can specify the color space: 
 ```lua
lab = image:toTensor('float', 'LAB')
-- equivalent to:
image:colorspace('LAB')
lab = image:toTensor('float') 
 -- color spaces available, for now:
-- 'LAB', 'HSL', 'HWB' and 'YUV'
``` 
 Images can also be read/written from/to Lua strings, or binary blobs.
This is convenient for in memory manipulation (e.g. when downloading
images from the web, no need to write it to disk): 
 ```lua
blob,size = image:toBlob()
image:fromBlob(blob,size) 
 str = image:toString()
image:fromString(str)
``` 
 In this library, we use a single function to read/write parameters
(instead of the more classical get/set).  
 Here's an example of a resize: 
 ```lua
-- get dimensions:
width,height = image:size() 
 -- resize:
image:size(512,384) 
 -- resize by only imposing the largest dimension:
image:size(512) 
 -- resize by imposing the smallest dimension:
image:size(nil,512)
``` 
 Some basic transformations: 
 lua
-- flip or flop an image:
image:flip()
image:flop() 
 Sharpen: 
 lua
-- Sharpens the image whith radius=0, sigma=0.6
image:sharpen(0, 0.6) 
 Show an image (this makes use of Tensors, and Torch's Qt backend): 
 lua
image:show() 
 One cool thing about this library is that all the functions can be cascaded.
Here's an example: 
 lua
-- Open, transform and save back:
gm.Image('input.jpg'):flip():size(128):save('thumb.jpg') RestClient 
 A simple client for REST APIs. This package provides a few functions 
to get and post from/to restful APIs. LBFGS 
 LibLBFGS (C Lib) wrapper. 
 This is an FFI interface to  LibLBFGS . 
 Installation 
 Simply build and install  LibLBFGS  
(with no SSE2 support, for now I don't support aligned memory blocks). 
 This package can be installed with Luarocks. 
 Usage 
 The code in test.lua demonstrates how to use the solver. Its interface
is 100% compatible to the solvers in  optim . CURL 
 A simple interface to CURL. 
 Provides two functions:  get  and  post . 
 get : 
 ```lua
-- load lib:
curl = require 'curl' 
 -- getting random pages:
res = curl.get('http://www.google.com') 
 -- with query:
res = curl.get('http://www.google.com', {safe='off', output='search', oq='test'}) 
 -- complete API:
res = curl.get{
    host = 'http://blogname.blogspot.com',
    path = '/feeds/posts/default',
    query = {
        alt = 'json'
    },
    format = 'json' -- parses the output: json -> Lua table
} 
 -- Getting an image, and decoding it:
img = curl.get('http://www.webstandards.org/files/acid2/reference.png')
require('graphicsmagick').Image():fromString(img):show()
``` 
 post : 
 lua
-- post has the same API, with a form parameter (instead of query)
res = curl.post{
    host = 'http://myserver.com',
    path = '/',
    form = {
        username = 'bob',
        password = 'key',
        somefiletoupload = '@/local/path/to/file.jpg'
    }
} gfx.js: a browser-based graphics server 
 Originally forked from the amazing  tty.js . 
 The goal is to extend this project to support the creation of rich media windows,
on top of the terminal windows. 
 The idea is simple: the server watches a directory, and monitors the creation &
modification of HTML files; upong modification / creation, it creates a new window
on the client side (browser), which simply render the HTML.  
 Clients are easy to develop: one simply needs to dump HTML into the watched
directory to have it rendered by the browser. 
 For now, I'm focusing on one client, written in Lua, for 
 Torch7 . 
 
 Check out  tty.js  for reference on the
original project. Note: I'm simply extending their project, not modifying
any of the core structure, so it should remain compatible. 
 Installation 
 You have to have Node.js (important, Version >= 0.10.0), NPM, and Torch7
installed. With older versions of Node.js, things won't be stable. You also
need libgraphicsmagick-dev to be installed: 
 ```sh 
 OS X 
 brew install graphicsmagick 
 Ubuntu 
 apt-get install libgraphicsmagick1-dev
apt-get install graphicsmagick
``` 
 Then simply run: 
 sh
luarocks install https://raw.github.com/clementfarabet/gfx.js/master/gfx.js-scm-0.rockspec 
 Or, if you cloned the repo locally: 
 sh
luarocks make 
 Execution 
 Once installed, you can start/stop the server like this (I'm assuming a LuaJIT-based install): 
 luajit -lgfx.start
luajit -lgfx.stop 
 And then open up a tab in your browser, at  http://localhost:8000 . 
 The browser acts as a passive graphics backend. The server monitors the creation of new
resources (charts, plots, videos, ...), and lets the browser know it should render them. 
 The framework is very flexible: resources can be rendered by a client (luajit) with no
browser open, and even no server listening/running. The resources generated will still
be saved, and can be visualized later (very useful to generate resources/charts on
a server with no X session). 
 You can optionally specify a different port as an env variable, if the default (8000)
is not available: 
 PORT=4321 luajit -lgfx.start
PORT=4321 luajit -lgfx.stop 
 Also, we provide a useful PS script, which lists running servers: 
 luajit -lgfx.ps 
 On Mac OS, we also provide a shortcut to start the server in the background and automatically
open the browser at the right location: 
 luajit -lgfx.go 
 Alternatively, you can do things step by step: 
 ```
luajit -lgfx.start 
 starts a server... 
 luajit 
 starts a Torch session 
 ``` 
 At the prompt, you can load the gfx.js client, and render things: 
 lua
gfx = require 'gfx.js'
gfx.image(image.lena())
gfx.image({
   image.lena()
   image.lena()
   image.lena()
   image.lena()
   image.lena()
   image.lena()
   image.lena()
   image.lena()
}, {zoom=0.5, legends={'Image 1', 'Image 2'}}) 
 This will produce this output: 
 
 I've also slowly started to integrate plots from  NVD3 , and bind
them to Torch, so that they can seamlessly be called from the Torch repl: 
 ```lua
gfx.chart(data, {
   chart = 'line', -- or: bar, stacked, multibar, scatter
   width = 600,
   height = 450,
}) 
 -- where data has the form:
data = {
    {
        key = 'Legend 1',
        color = '#0f0',
        values = { {x=0,y=0}, {x=1,y=1}, ... },
    },
    {
        key = 'Legend 2',
        color = '#00f',
        values = { {x=0,y=0}, {x=1,y=1}, ... },
    },
} 
 -- or, for a single dataset:
data = {
    key = 'Legend',
    values = { {x=0,y=0} , ... }
} 
 -- values can be provided in convenient ways:
values = { {x=0,y=0[,size=0]}, ... }
values = { {0,0,0}, ... }
values = torch.randn(100,2)
values = torch.randn(100,3)  -- the 3rd dimension is the optional size, only used by certain charts
values = torch.randn(100) -- in this case, y is provided, x defaults to range(0,N-1) 
 -- shortcuts are also provided for quick plots:
gfx.chart(torch.randn(100,2), {chart='scatter'})
gfx.chart(torch.randn(100), {chart='line'})  -- y is provided, x will be a range(1,N)
gfx.chart({ torch.randn(100), torch.randn(100) })  -- multiple datasets
gfx.chart({ {1,2,3,4,5,6,7,8,7,6,5,4,3,2,1}, torch.randn(100) })  -- multiple datasets, table format
``` 
 As explained above, one can generate resources/charts/figures with no server listening.
One can connect a server later on, and redraw the last resources generated. Here are a few
useful commands for that: 
 lua
gfx = require 'gfx.js'
ids = gfx.list(10) -- will list the last 10 figures generated (each figure has a unique ID)
print(ids[1])
-- will print something like: dom_1212817597132847893127489
gfx.redraw(ids[1]) -- will redraw this resource
gfx.redraw(10) -- will redraw the last 10 resources available (sorted by descending time) 
 Finally, the server gets slower as the number of resources/charts/images grows in the 
watched directory. It's useful to sometimes clear this cache manually:
 gfx.clear() PERSIST 
 A persisting table for Lua. 
 Built using Redis, it's a simple abstraction that allows
one to write/read from a table that persists over sessions (the
key/vals are persisted in Redis). 
 ```lua
-- load lib:
p = require('persist')() 
 -- write a few things to it:
p.test = 'something'
p.test2 = {
    some = 'table',
    nested = {is=1}
}
``` 
 Shut down, start again: 
 ```lua
-- load lib:
p = require('persist')() 
 -- still there?
print(p.test)
print(p.test2)
``` 
 The following options can be passed: 
 lua
p = require('persist')({
   url = 'localhost',
   port = 6379,
   verbose = false, -- this is not only used on startup
   namespace = 'th',  -- this is the namespace in Redis
   clear = false, -- clear all the data
}) ASyNC 
 An async framework for Lua/Torch, based on  LibUV 
(using Tim Caswell's  luv  library). 
 This lib is heavily inspired on the Node.js architecture. It's fun, elegant, and
should be extremely efficient (a lot of testing is required). 
 The examples in  tests/  should provide enough documentation on the API. 
 License 
 MIT License 
 Examples 
 Starting the event loop. At the end of any program, the event loop must be started.
Nothing will be interpreted after this call, as it takes control of the runtime. 
 lua
async.go() 
 It's useful to have a REPL (interpreter) running asynchronously, for debugging and
live control of your programs: 
 lua
async.repl()  -- fires up an asyncronous repl 
 lua
async.repl.listen({host='0.0.0.0', port=8080})   -- fires up an async repl through a TCP server
async.repl.connect({host='0.0.0.0', port=8080})  -- connects to a remote repl through a TCP client 
 Common JS like timer controls:
 lua
async.setInterval(millis, function()
   print('printed every N millis')
end)
async.setTimeout(millis, function()
   print('printed once in N millis')
end) 
 CPU Info. Useful to know how many processors are available.
This is a synchronous call. 
 lua
print(async.cpuInfo()) 
 A TCP server: 
 ```lua
async.tcp.listen({host='0.0.0.0', port=8080}, function(client)
   -- Receive:
   client.ondata(function(chunk)
      -- Data:
      print('received: ' .. chunk) 
   -- Reply:
  client.write('thanks!')
 
 end) 
 -- Done:
   client.onend(function()
      print('client gone...')
   end)
end)
``` 
 A TCP client: 
 ```lua
async.tcp.connect({host='127.0.0.1', port=8080}, function(client)
   -- Write something
   client.write('something .. ' .. i) 
 -- Callbacks
   client.ondata(function(chunk)
      print('received: ' .. chunk)
      client.close()
   end) 
 -- Done:
   client.onend(function()
      print('connection closed...')
   end)
end)
``` 
 File I/O. The low level interface is not complete yet, but the high-level one
is final: 
 lua
async.fs.readFile('LICENSE', function(content)
   print(content)
   async.fs.writeFile('LICENSE.copy', content, function(status, err)
      print('==> wrote file: ' .. (status or err))
   end)
end) 
 A lower-level interface is also available, for C-level performance. The upside:
no copy is done, the user callback gets the raw pointer to the network buffer (read)
and writes tap directly into the raw buffer, provided by the user. The downside:
the buffer returned by the ""ondata"" callback lives only for the scope of that callback,
and must be copied by the user... 
 ```lua
-- assuming a client handle: 
 local b = require 'buffer' 
 client.onrawdata(function(chunk)
   -- chunk is a Buffer object (https://github.com/clementfarabet/buffer)
   print(chunk) 
 -- chunk will not be valid past this point, so its content must be copied,
   -- not just referenced...
   local safe = chunk:clone()
   -- safe can be past around... 
 -- the most common use is to copy that chunk into an existing storage,
   -- for instance a tensor:
   -- (assuming tensor is a torch.Tensor)
   local dst = b(tensor)  -- creates a destination buffer on the tensor (a view, no copy)
   dst:copy(src)
end) 
 -- write() also accepts buffers:
client.write( b'this is a string saved in a buffer object' ) 
 -- last, the sync() interface can be set up in raw mode:
client.syncraw()
local buffer = client.read()
-- ...
``` 
 We also provide a simple async interface to CURL. 
 Provides two functions:  get  and  post . 
 get : 
 ```lua
-- simple URL:
async.curl.get('http://www.google.com', function(res)
    print(res)
end) 
 -- complete API:
async.curl.get({
    host = 'http://blogname.blogspot.com',
    path = '/feeds/posts/default',
    query = {
        alt = 'json'
    },
    format = 'json' -- parses the output: json -> Lua table
}, function(res)
   print(res)
end) 
 -- Getting an image, and decoding it:
curl.get('http://www.webstandards.org/files/acid2/reference.png', function(res)
  local decoded = require('graphicsmagick').Image():fromString(res)
end)
``` 
 post : 
 ```lua
-- post has the same API, with a form parameter (instead of query):
async.curl.post({
    host = 'http://myserver.com',
    path = '/',
    form = {
        username = 'bob',
        password = 'key',
        somefiletoupload = '@/local/path/to/file.jpg'
    }
}, function(res)
   print(res)
end) 
 -- or a simple file upload:
async.curl.post({
    host = 'http://myserver.com',
    path = '/',
    file = '@/path/to/file.png',
}, function(res)
   print(res)
end)
``` Manifold 
 A package to manipulate manifolds, for Torch7. 
 Install 
 sh
luarocks install manifold 
 Dependencies 
 In order to be able to run the binaries, you need to install the package  libatlas3-base .
On a Ubuntu machine you can execute the following commands. 
 sudo apt-get update
sudo apt-get install libatlas3-base 
 Use 
 ```lua
-- package:
m = require 'manifold' 
 -- a dataset:
t = torch.randn(100,10) -- 100 samples, 10-dim each 
 -- basic functions:
ns = m.neighbors(t) -- return the matrix of neighbors for all samples (sorted)
ds = m.distances(t) -- return the matrix of distances (L2)
ts = m.removeDuplicates(t) -- remove duplicates from dataset 
 -- embeddings:
p = m.embedding.random(t, {dim=2})  -- embed samples into a 2D plane, using random projections
p = m.embedding.lle(t, {dim=2, neighbors=3})  -- embed samples into a 2D plane, using 3 neighbor (LLE)
p = m.embedding.tsne(t, {dim=2, perplexity=30})  -- embed samples into a 2D plane, using tSNE
``` 
 Demos 
 To run the demos, simply type the following commands. 
 sh
cd demos
qlua demo_swissroll.lua
qlua demo_tsne.lua 
 Below is an example of a t-SNE map produced on 5,000 MNIST digits by the  demos/demo_tsne.lua  demo. 
 Buffer 
 A buffer object for LuaJIT. The goal: efficient, C-speed, byte manipulation
for LuaJIT. 
 Also provides interfaces to Torch's tensors and storages, for easy serialization. 
 Install 
 luarocks install buffer 
 Simple use cases 
 Load lib: 
 ```lua 
 
 b = require 'buffer'
``` 
 
 Create a buffer, from a string, with a size, or from
another buffer: 
 ```lua 
 
 buf = b'some'
print(buf)
 
buf = b(10)
print(buf)
 
buf2 = b(buf)
print(buf2)
 
buf[1] = 10
buf[2] = 20
print(buf2)
 
``` 
 
 Creating buffers never makes copies. A buffer created from a string
always references the content of the string. A buffer created from
another buffer references the same buffer. 
 Concatenating two buffers is done like it's done for strings: 
 ```lua 
 
 a = b'some' .. b'thing'
str = a:toString()
print(str)
something
``` 
 
 The  toString  method simply returns a Lua string from the buffer. 
In this case, the string is a copy, which won't be affected by further
changes of the buffer: 
 ```lua 
 
 a[1] = a[1] + 1
print(str)
something
print(a:toString())
tomething
``` 
 
 A slicing operator is provided: 
 ```lua 
 
 a = b'testing'
print(a[{1,4}])
test
a[{1,4}] = 'sing'
a[{1,4}] = b'sing'  -- both supported
print(a)
singing
``` 
 
 A buffer can be created from a list of buffers, which provides efficient
concatenation: 
 ```lua 
 
 a1 = b'test'
a2 = b'test'
a3 = b'again'
a = b(a1,a2,a3)
print(a:toString())
testtestagain
b = b( {a1,a2,a3} )
print(b:toString())
testtestagain
``` 
 
 Finally, cloning a buffer allows clean memory separation: 
 ```lua 
 
 a = b'test'
c = a:clone()
``` 
 
 More advanced constructors are also available, to mount buffers on arbitrary
managed or unmanaged chunks of memory. See tests for examples. 
 Last, if Torch is available, converters are available from buffers to tensors
and back. This is especially handy for multithreaded / multimachine environments,
where exchanging tensors must be done at optimal speed (i.e. with no complex 
serialization). 
 ```lua 
 
 t = torch.FloatTensor(10):normal()
buf = b(t)
-- buf is now a view on t's underlying contiguous storage
-- buf could be transmitted over sockets / threads, as raw binary data (see async for use cases) 
 
 -- from buf, new storages or tensors can be constructed like this: 
 
 tt = buf:toFloatStorage()
tt = buf:toFloatTensor()
tt = buf:toFloatTensor(2,5)
-- these are all views on the original storage of t.
``` 
 
 License 
 Code was originally inspired from the Luvit folks. 
 Copyright 2013-2014 Clement Farabet (MADBITS)
Copyright 2012 The Luvit Authors. All Rights Reserved. 
 Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at 
 http://www.apache.org/licenses/LICENSE-2.0
 
 Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS-IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License. LuaForever 
 Run a Lua/LuaJIT script forever. 
 Example: 
 ```bash
luajit myscript.lua arg1 arg2 
 error, script crashes... 
 luaforever myscript.lua arg1 arg2 
 error, script crashes... 
 myscript.lua restarts automatically 
 error, script crashes... 
 myscript.lua restarts automatically 
 ... 
 ``` THMAP 
 A simple distributed framework to map jobs/work onto multiple workers. 
 The framework provides two binaries:  thmap  and  thnode .  thmap  is a controller,
that lets you mirror commands to multiple  thnode  instances, and absorb the output
of all these  thnode  instances in //. 
 The general philosophy is that you run a bunch of  thnode  instances on multiple machines,
then run  thmap  everytime you need to schedule/run new scripts onto these nodes. 
 thmap  lets you git pull, and reload scripts, so that you can easily update a 
distributed code base. 
 Install 
 sh
luarocks install thmap 
 Use 
 On the compute machine(s), run  thnode : 
 sh
thnode 
 Write a config file that lists all the available nodes ( nodes.lua ): 
 lua
-- example configuration, with two nodes:
return {
   {host='ip1', port=10001},
   {host='ip2', port=10001}
} 
 (by default,  thnode  listens to port 10001, then increments if not available). 
 Then start  thmap  to start monitoring and dispatching jobs: 
 sh
thmap --nodes nodes.lua
ip1:10001> ip2:10002> spawn('th', {'test.lua'})
... 
 The above shows you a double shell: you are connected to two  thnode  instances,
and any command you issue will be mirrored to both. 
 In its current version,  thmap  supports the following commands: 
 
 spawn a job:             spawn('th', {'script.lua', 'opt1', 'opt2', ...}, {autorestart=true}) 
 restart running jobs:    restart() 
 list running jobs:       ps() 
 exec git command:        git 'pull'  /  git 'status' 
 git pull + restart:      update() 
 kill all zombies:        zombies() 
 Schedlua 
 Basic scheduler. (Note: this package is provided for compatibility with public tutorials.
 It is not maintained anymore.) 
 GM: Graphical Models for Torch/LuaJIT 
 This package provides standard functions to create (arbitrary) 
undirected graphical models, using adjacency matrices. 
 A graph is described by an adjacency matrix, node potentials
and edge potentials.  
 Install 
 sh
$ git clone ...
$ [sudo] luarocks make 
 Use 
 First run torch, and load gm: 
 sh
$ th   
 ``` lua 
 
 require 'gm'
``` 
 
 Once loaded, see and run the examples: 
 ``` lua 
 
 gm.examples.simple()
gm.examples.trainMRF()
gm.examples.trainCRF()
``` 
 nnop 
 Parameter-free / operation-only Neural Network primitives
for torch/nn. 
 Motivation, Goals 
 Sometimes, it's useful to treat parameters as regular states,
to either impose certain constraints on them, or simply
make weight sharing visible / straight-forward. 
 The original design of  nn  treats
trainable parameters as special variables. This package,  nnop ,
builds on  nn  and  nngraph , but separates parameters from operations. 
 It introduces a new module,  nn.Parameters , which provides trainable
parameters, but does not do any computation. Every other parameterized
node ( nn.Linear ,  nn.SpatialConvolution , ...) needs to be wrapped in
 nnop  to decouple trainable parameters, and become pure operation nodes. 
 TODO 
 
 wrap remaining parametrized nodes ( nn.SpatialConvolution , ...) 
 simplify/unify auto-generated parameterNodes? 
 
 Examples 
 Weight sharing 
 In this example, 2 modules are connected to a same set of trainable
parameters. This is weight sharing. 
 ```lua
-- Create parameters:
linearWeight = nnop.Parameters(10,100)()
linearBias = nnop.Parameters(10)() 
 -- Create multiple layers, all connected to these parameters:
input1 = nn.Identity()()
input2 = nn.Identity()()
linear1 = nnop.Linear()({input1, linearWeight, linearBias})
linear2 = nnop.Linear()({input2, linearWeight, linearBias}) 
 -- Graph:
graph = nn.gModule({input1,input2}, {linear1,linear2}) 
 -- Tests:
res = graph:forward({torch.randn(100), torch.randn(100)})
assert(type(res) == 'table' and #res == 2) 
 input = torch.randn(100)
res = graph:forward({input, input})
assert(res[1]:dist( res[2] ) == 0)
``` 
 Penalty on a set of parameters 
 In this example, we add an L1 penalty on a set of weight. 
 When parameters are provided to the nnop.Linear constructor,
parameter nodes are automatically created (and automatically
connected in the graph!). We use this in this example, this
way we don't have to create the parameter nodes, but are still
free to access them and add a penalty on them. 
 ```lua
-- create base modules:
linear1 = nnop.Linear(10,100)
tanh1 = nn.Tanh()
linear2 = nnop.Linear(100,2) 
 -- bind them in a graph:
input = nn.Identity()()
layer1 = linear1(input)
layer2 = tanh1(layer1)
layer3 = linear2(layer2) 
 -- get weights and impose penalty:
weight1 = linear1.parameterNodes.weightNode
sparse1 = nn.L1Penalty(.001)(weight1) 
 -- build final model:
model = nn.gModule({input}, {layer3}) 
 -- train the model:
for i = 1,10 do
   input = torch.rand(10)
   output = model:forward(input)
   gradOutput = torch.rand(2)
   gradInput = model:updateGradInput(input, gradOutput)
   model:accGradParameters(input, gradOutput)
end
``` nnfunc 
 Functionalize nn modules: the goal of this package is to make it
easy to develop 3rd-party frameworks, by re-exposing nn modules
as functions. Basically provide a functional API to nn. 
 Every instantiated module becomes a simple state-less function:
input data and parameters must be provided as inputs to this function;
same thing for gradients. For convenience and efficiency, the state
of the underlying nn module is still relied on for caching (every function
returned by nnfunc is a closure relying on an instantiated nn module). 
 API 
 Expose packages 
 Any package that provides  nn.Module  children can be exposed. 
 lua
nnfunc.functionalize 'nn'   -- done by default by nnfunc
nnfunc.functionalize 'nnx'  -- bundle new package... 
 Once called, every module in the source package is available to
use as a function; see examples below. 
 API #1 
 A single function that evaluates the module, and automatically
computes gradients if  gradOutput  is provided. 
 ```lua
-- this returns a function that can be used to eval this module and
-- its gradients:
layer = nnfunc.nn.Linear(10,100) 
 -- compute module's output:
prediction = layer({
   input = torch.randn(10),
   weight = torch.randn(100,10), bias = torch.randn(100),
})
-- prediction looks like this:
-- {
--    output = torch.Tensor(100)
-- } 
 -- output can be user-provided, optionally:
prediction = layer({
   input = torch.randn(10),
   weight = torch.randn(100,10), bias = torch.randn(100),
   output = torch.Tensor(100),
})
-- output is now valid 
 -- compute gradients (backprop) - this happens automatically
-- because gradOutput is provided:
grads = layer({
   input = torch.randn(10),
   weight = torch.randn(100,10), bias = torch.randn(100),
   gradOutput = torch.randn(100),
})
-- grads looks like this:
-- {
--    gradInput = torch.Tensor(10),
--    gradWeight = torch.Tensor(100,10),
--    gradBias = torch.Tensor(100),
-- } 
 -- the user can also provide all the tensors for computed gradients,
-- if her application requires that they be owned externally:
grads = layer({
   input = torch.randn(10),
   weight = torch.randn(100,10), bias = torch.randn(100),
   gradOutput = torch.randn(100),
   gradWeight = torch.zeros(100,10), bias = torch.zeros(100),
   gradInput = torch.zeros(10),
})
-- user-provided gradInput, gradWeight and gradBias are now
-- valid!
``` 
 API #2 
 Two separate functions: one for eval, one for gradients. This
can be useful when separate function pointers need to be used
to register gradients. 
 ```lua
-- two separate functions:
layer,gradLayer = nnfunc.nn.Linear(10,100) 
 -- compute module's output [same as API #1]:
prediction = layer({
   input = torch.randn(10),
   weight = torch.randn(100,10), bias = torch.randn(100),
}) 
 -- compute gradients (backprop) [separate function for grads]:
grads = gradLayer({
   input = torch.randn(10),
   weight = torch.randn(100,10), bias = torch.randn(100),
   gradOutput = torch.randn(100),
})
``` 
 A hash table is also maintained to retrieve gradients associated
to any object created: 
 ```lua
-- two separate functions:
layer,gradLayer = nnfunc.nn.Linear(10,100) 
 -- gradLayer could be retrieve like this:
gradLayer2 = nnfunc.gradsOf[layer]
assert(gradLayer2 == gradLayer)
``` Something. Regress 
 A very simple regression test package. 
 ```lua
local test = require 'regress' 
 test {
   test1 = function()
      test.mustBeTrue(a == b, 'a should == b')
   end, 
 test2 = function()
      test.shouldBeTrue(a == b, 'a should == b')
   end,
}
```"
ajabri,"pytorch-maml 
 This is a PyTorch implementation of the supervised learning experiments from the paper 
Model-Agnostic Meta-Learning (MAML): https://arxiv.org/abs/1703.03400 
 Important : You will need the latest version of PyTorch, v.0.2.0 to run this code (otherwise you will get errors about 
double backwards not being supported). 
 Currently, only the Omniglot experiments have been replicated here. The hyper-parameters are the same as those used in the original 
Tensorflow implementation, except that only 1 random seed is used here. 
 5-way 1-shot training, best performance 98.9% 
 
 20-way 1-shot training, best performance 92% 
 
 Note: the 20-way performance is slightly lower than that reported in the paper (they report 95.8%). If you can see why this might be,
please let me know. Also in this experiment, we can see evidence of overfitting to the meta-training set. 
 The 5-way results are achieved by simply meta-testing the network trained on the 1-shot task on the 5-shot task (e.g. for the 5-way 5-shot result, test the 5-way 1-shot trained network with 5-shots). Again the 20-way result is lower here than reported in the paper. 
 This repo also contains code for running maml experiments on permuted MNIST (tasks are created by shuffling the labels).
This is a nice sanity check task. carml Space-Time Correspondence as a Contrastive Random Walk 
  ![](https://github.com/ajabri/videowalk/raw/master/figs/teaser_animation.gif)  
 
 
 
 This is the repository for  Space-Time Correspondence as a Contrastive Random Walk , published at NeurIPS 2020.   
 [ Paper ]
[ Project Page ]
[ Slides ]
[ Poster ]
[ Talk ] 
 @inproceedings{jabri2020walk,
    Author = {Allan Jabri and Andrew Owens and Alexei A. Efros},
    Title = {Space-Time Correspondence as a Contrastive Random Walk},
    Booktitle = {Advances in Neural Information Processing Systems},
    Year = {2020},
} 
Consider citing our work or acknowledging this repository if you found this code to be helpful :) 
 Requirements 
 
 pytorch (>1.3) 
 torchvision (0.6.0) 
 cv2 
 matplotlib 
 skimage 
 imageio 
 
 For visualization ( --visualize ):
- wandb
- visdom
- sklearn 
 Train 
 An example training command is:
 python -W ignore train.py --data-path /path/to/kinetics/ \
--frame-aug grid --dropout 0.1 --clip-len 4 --temp 0.05 \
--model-type scratch --workers 16 --batch-size 20  \
--cache-dataset --data-parallel --visualize --lr 0.0001 
 This yields a model with performance on DAVIS as follows (see below for evaluation instructions), provided as  pretrained.pth :
 J&F-Mean    J-Mean  J-Recall  J-Decay    F-Mean  F-Recall   F-Decay
  0.67606  0.645902  0.758043   0.2031  0.706219   0.83221  0.246789 
 Arguments of interest: 
 
 --dropout : The rate of edge dropout (default  0.1 ). 
 --clip-len : Length of video sequence. 
 --temp : Softmax temperature. 
 --model-type : Type of encoder. Use  scratch  or  scratch_zeropad  if training from scratch. Use  imagenet18  to load an Imagenet-pretrained network. Use  scratch  with  --resume  if reloading a checkpoint. 
 --batch-size : I've managed to train models with batch sizes between 6 and 24. If you have can afford a larger batch size, consider increasing the  --lr  from 0.0001 to 0.0003. 
 --frame-aug :  grid  samples a grid of patches to get nodes;  none  will just use a single image and use embeddings in the feature map as nodes. 
 --visualize : Log diagonistics to  wandb  and data visualizations to  visdom . 
 
 Data 
 We use the official  torchvision.datasets.Kinetics400  class for training. You can find directions for downloading Kinetics  here . In particular, the code expects the path given for kinetics to contain a  train_256  subdirectory. 
 You can also provide  --data-path  with a file with a list of directories of images, or a path to a directory of directory of images. In this case, clips are randomly subsampled from the directory. 
 Visualization 
 By default, the training script will log diagnostics to  wandb  and data visualizations to  visdom . 
 Pretrained Model 
 You can find the model resulting from the training command above at  pretrained.pth .
We are still training updated ablation models and will post them when ready. 
 
 Evaluation: Label Propagation 
 The label propagation algorithm is described in  test.py .  The output of  test.py  (predicted label maps) must be post-processed for evaluation. 
 DAVIS 
 To evaluate a trained model on the DAVIS task, clone the  davis2017-evaluation  repository, and prepare the data by downloading the  2017 dataset  and modifying the paths provided in  eval/davis_vallist.txt . Then, run: 
 Label Propagation: 
 python test.py --filelist /path/to/davis/vallist.txt \
--model-type scratch --resume ../pretrained.pth --save-path /save/path \
--topk 10 --videoLen 20 --radius 12  --temperature 0.05  --cropSize -1 
Though  test.py  expects a model file created with  train.py , it can easily be modified to be used with other networks. Note that we simply use the same temperature used at training time. 
 You can also run the ImageNet baseline with the command below.
 python test.py --filelist /path/to/davis/vallist.txt \
--model-type imagenet18 --save-path /save/path \
--topk 10 --videoLen 20 --radius 12  --temperature 0.05  --cropSize -1 
 Post-Process: 
``` 
 Convert 
 python eval/convert_davis.py --in_folder /save/path/ --out_folder /converted/path --dataset /davis/path/ 
 Compute metrics 
 python /path/to/davis2017-evaluation/evaluation_method.py \
--task semi-supervised   --results_path /converted/path --set val \
--davis_path /path/to/davis/
``` 
 You can generate the above commands with the script below, where removing  --dryrun  will actually run them in sequence.
 python eval/run_test.py --model-path /path/to/model --L 20 --K 10  --T 0.05 --cropSize -1 --dryrun 
 Test-time Adaptation 
 To do."
korymath,"Gaze Vector Regression Testing 
 Directory structure: 
 git pull https://github.com/korymath/gazevectorregression 
 Data files as *.mat files should be located in 
 /data 
 run_basic_exp will run on all *.mat files in the /data folder. 
 run_single_test provides a single experiment and you need to set train and test data files 
 Model building 
 Model is built in build_single_model.m 
 Can edit the way that the model is built by changing the mdl to other model builders from MATLAB. 
 modelBuildingIdeas.m has several ideas to try. I think that the best option will be a bagged regression, wishing there was a more automated trial set up in matlab. 
 One thing to do would be to output all the experiments to train and test directories so that we could rerun validations in Python autoML libraries, and faster c-modules to see if we could build up a solid single model. 
 Notes on data from : 
 all calibrations are sweep, eyes, free 
 the column switches between sweep and task 
 RXX - XX patient number 
 Calib(C|P) - cups or pasta, when this changes, task changes, always three for each
assume new task mean adjustment 
 B  - ignore (both eye and movement) 
 NN - 01, 02, 03 in sequence trials -- 1 and 2 are before then a whole bunch of trials, then 3 is the after trials calibration 
 _combined_segments - eye data, 
 TODO:  
 TESTING 1 on 2 and 1 on 3 compare ContinuousTimeActorCritic 
 Continuous Time Natural Actor Critic Reinforcement Learning 
 This repository is a basic actor-critic reinforcement learning implementation based on the following papers: 
 http://www.ualberta.ca/~pilarski/docs/papers/Pilarski_2011_ICORR.pdf 
 and 
 http://www.ualberta.ca/~pilarski/docs/papers/Pilarski_2013_ICORR_Postprint.pdf 
 How can ACRL be used for multi-joint control of robotic limbs?  cont-RL-order 
 Experimenting with mountain car continuous task using order switching 
 Basic Mountain Car Q-learning with some hot mods.  
 To plot use the plot function with the following syntax: 
 python plot.py value500ep Pyggy 
 Artificial Improvisation 
 This is a chat bot used in a live performance. The Plotto Plot Machine 
 A program to automate the algorithm described in  William Wallace Cook's Plotto, The Masterbook of All Plots . 
 This program is written in Python for ease & legibility. 
 This program relies upon the text of Plotto to generate story structure outlines. 
 The text of Plotto is obtained from: https://openlibrary.org/works/OL16087095W/Plotto. 
 This work is licensed under a  Creative Commons Attribution-ShareAlike 4.0 International License . runkeeper-testing 
 Modified code from  @kylemath  for comparing multiple similar length trials of the same distance.  
 Details 
 This matlab code works by smoothing the time derivative (smoothLength) of the GPX data and then plotting comparable runs of similar distances on the same plot. 
 It also adds markers for kilometers and miles.  
 It works really well to see the comparison of two (or mulitple) runs, as can be seen right here:  
 #StockView 
 go/StockView 
 Twitter can tell us what's happening, but it is not great for finding business information. 
 With this extension, you will be able to see live stock information right in your Twitter feed providing context for the current discussion. 
 Now, when you search Twitter for cashtags like '$TWTR', the stock chart will be displayed so you can really know what is happening. 
 
 Install 
 #StockView is packaged as a chrome extension. You can download it here https://github.com/korymath/StockView/raw/master/app.crx. 
 To install, you should just drag 'n' drop the .crx file into Chrome.
In Chrome, open Extensions tab (chrome://extensions), drag 'n' drop the .crx file and you are good to go. 
 Amusingly ""for regular Windows users who are not skilled with computers, it is practically not possible to install and use extensions from outside the Chrome Web Store."" -- http://stackoverflow.com/questions/24577024/install-chrome-extension-not-in-the-store 
 Usage 
 Try searching for a single ( $TWTR ) or multiple ( $TWTR $MSFT $FB ) cashtags, using the syntax $SYMBOL. 
 Development Screenshots 
 Before 
 Company is suggested, perhaps an account is shown and several recent tweets populate the timeline below. 
 
 v1 
 Rough chart inserted nicely. 
 
 v2 
 Beautiful chart comes in. 
 
 v3 
 Multiple stock tickers. 
 
 Technology 
 #StockView uses  Highstocks  for visualization, and the data comes from the  Yahoo Stock API .  brds 
 A not-so-intelligent intelligent thing 
 Install 
 ~~~
pip install -r requirements.txt
~~~ 
 Run 
 ~~~
python decomp.py
~~~ simple-chatroom 
 Basic Python Chatroom emrl 
 emrl Visual Illusions 
 Can we teach a machine to categorise visual illusions and generate new ones?  
 Dataset 
 
 https://github.com/robertmaxwilliams/optical-illusion-dataset 
 https://www.moillusions.com/ 
 http://viperlib.york.ac.uk/ 
 https://twitter.com/AkiyoshiKitaoka 
 http://www.psy.ritsumei.ac.jp/~akitaoka/saishin58e.html -- dating back to June 2002 (http://www.psy.ritsumei.ac.jp/~akitaoka/o1saishe.html) 
 
 Details, Categories and Meta Data 
 https://en.wikipedia.org/wiki/List_of_optical_illusions 
 Related work 
 Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images (https://arxiv.org/abs/1412.1897) partitioning-groups 
 An elegant way to partition groups.  Talk Powerpoint Generator 
 
 
 
 This program automatically generates PowerPoints about any topic.
These presentation slide decks can be used by improvisers for the improvisational comedy format  ""Improvised TED talk""  or  ""Powerpoint Karaoke"" .
In such games, the actors have to present an unseen presentation slide deck, but pretend to be an expert and explain  ""their""  slide show choices. 
 Demo 
 Ty out this generator on our online platform:  talkgenerator.com . 
 Example 
 
 Easy Install and Run 
 Our program relies on certain APIs that require authentication in order to use it.
Create a file named  .env  (don't forget the period) in your project directory, and fill this with the correct API keys as described on our  wiki page about this . 
 ```sh 
 Make a new Python 3 virtual environment 
 python3 -m venv venv; 
 Activate the virtual environment 
 source venv/bin/activate; 
 Upgrade pip and install  requirements 
 pip install --upgrade pip setuptools;
python3 -m pip install -r requirements.txt; 
 Download NLTK dependencies 
 python run_nltk_download.py; 
 Install the Talk Generator 
 pip install -e .; 
 Generate a 10 slide talk with topic peanuts 
 talkgenerator --topic ""peanuts"" --num_slides 10
``` 
 Run arguments 
 | Argument               | Description               |
| ---------------------- | ------------------------- |
|  topic  | The topic of the generator. This works best if it is a common, well-known noun. Use comma-separated words to generate a slide deck about multiple topics |
|  slides  | The number of slides in the generated presentation ( default: 10 ) |
|  schema  | The presentation schema to use when generating the presentation. Currently, only two modes are implemented, being  default  and  test  (for testing during development) |
|  title  | Title of the presentation. Either  topic  or this one should to be set in order to generate a slide deck (just setting  topic  is usually more fun though)  |
|  presenter  | The name that will be present on the first slide. Leave blank for an automatically generated name |
|  output_folder  | The folder to output the generated presentations ( default:  ./output/ ) |
|  save_ppt  | If this flag is true( default ), the generated powerpoint will be saved on the computer in the  output_folder |
|  open_ppt  | If this flag is true ( default ), the generated powerpoint will automatically open after generating|
|  parallel  | If this flag is true ( default ), the generator will generate all slides in parallel | 
 Program structure 
 See the  wiki  to know more about the inner implementation. 
 Tests 
 Test files are  tests/*.py , prefixed with  test_ . Test files use the  unittest  module.
They can easily be run all together when using PyCharm by right clicking on  talk-generator  and pressing  Run 'Unittests in talk-generator' 
 sh
coverage run -m pytest; coverage html 
 Test coverage is automatically handled by  codecov . Tests are automatically run with CircleCI based on the  .yml  file in the  .circleci  directory. 
 Credits 
 This generator is made by
 Thomas Winters 
and  Kory Mathewson ,
with contributions from
 Shaun Farrugia 
and  Julian Faid . 
 If you would like to refer to this project in academic work, please cite the following paper: 
 Winters T., Mathewson K.W. (2019)  Automatically Generating Engaging Presentation Slide Decks . In: Ekárt A., Liapis A., Castro Pena M. (eds) Computational Intelligence in Music, Sound, Art and Design. EvoMUSART 2019. Lecture Notes in Computer Science, vol 11453. Springer, Cham 
 sh
@InProceedings{winters2019tedric,
    author=""Winters, Thomas
    and Mathewson, Kory W."",
    editor=""Ek{\'a}rt, Anik{\'o}
    and Liapis, Antonios
    and Castro Pena, Mar{\'i}a Luz"",
    title=""Automatically Generating Engaging Presentation Slide Decks"",
    booktitle=""Computational Intelligence in Music, Sound, Art and Design"",
    year=""2019"",
    publisher=""Springer International Publishing"",
    address=""Cham"",
    pages=""127--141"",
    isbn=""978-3-030-16667-0""
} 
 License 
 MIT License. Copyright (c) 2018-2020  Kory Mathewson  and  Thomas Winters jann 
 
 
 
 Hi. I am  jann . I am a retrieval-based chatbot. I would make a great baseline. 
 Allow me to (re)introduce myself 
 I uses approximate nearest neighbor lookup using  Spotify's Annoy (Apache License 2.0)  library, over a distributed semantic embedding space ( Google's Universal Sentence Encoder (code: Apache License 2.0)  from  TensorFlow Hub . 
 Objectives 
 The goal of  jann  is to explicitly describes each step of the process of building a semantic similarity retrieval-based text chatbot. It is designed to be able to use diverse text source as input (e.g. Facebook messages, tweets, emails, movie lines, speeches, restaurant reviews, ...) so long as the data is collected in a single text file to be ready for processing. 
 Install and configure requirements 
 Note:  jann  development is tested with Python 3.8.6 on macOS 11.5.2 and Ubuntu 20.04. 
 To run  jann  on your local system or a server, you will need to perform the following installation steps. 
 ```sh 
 OSX: Install homebrew 
 /bin/bash -c ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)"" 
 OSX: Install wget 
 brew install wget 
 Configure and activate virtual environment 
 python3.8 -m venv venv
source venv/bin/activate 
 python --version 
 Ensure Python 3.8.10 
 Upgrade Pip 
 pip install --upgrade pip setuptools 
 Install requirements 
 pip install -r requirements.txt 
 Install Jann 
 python setup.py install 
 Set environmental variable for TensorFlow Hub 
 export TFHUB_CACHE_DIR=Jann/data/module 
 Make the TFHUB_CACHE_DIR 
 mkdir -p ${TFHUB_CACHE_DIR} 
 Download and unpack the Universal Sentence Encoder Lite model (~25 MB) 
 wget ""https://tfhub.dev/google/universal-sentence-encoder-lite/2?tf-hub-format=compressed"" -O ${TFHUB_CACHE_DIR}/module_lite.tar.gz
cd ${TFHUB_CACHE_DIR};
mkdir -p universal-sentence-encoder-lite-2 && tar -zxvf module_lite.tar.gz -C universal-sentence-encoder-lite-2;
cd -
``` 
 Download Cornell Movie Dialog Database 
 Download the  Cornell Movie Dialog Corpus , and extract to  data/CMDC . 
 ```sh 
 Change directory to CMDC data subdirectory 
 mkdir -p Jann/data/CMDC
cd Jann/data/CMDC/ 
 Download the corpus 
 wget http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip 
 Unzip the corpus and move lines and convos to the main directory 
 unzip cornell_movie_dialogs_corpus.zip
mv cornell\ movie-dialogs\ corpus/movie_lines.txt movie_lines.txt
mv cornell\ movie-dialogs\ corpus/movie_conversations.txt movie_conversations.txt 
 Change direcory to jann's main directory 
 cd -
``` 
 As an example, we might use the first 50 lines of movie dialogue from the  Cornell Movie Dialog Corpus . 
 You can set the number of lines from the corpus you want to use by changing the parameter  export NUMLINES='50'  in  run_examples/run_CMDC.sh . 
 Tests 
 sh
pytest --cov-report=xml --cov-report=html --cov=Jann 
 You should see all the tests passing. 
 (simple) Run Basic Example 
 ```sh
cd Jann 
 make sure that the run code is runnable 
 chmod +x run_examples/run_CMDC.sh 
 run it 
 ./run_examples/run_CMDC.sh
``` 
 (advanced) Running Model Building 
 jann  is composed of several submodules, each of which can be run in sequence as follows: 
 ```sh 
 Ensure that the virtual environment is activated 
 source venv/bin/activate 
 Change directory to Jann 
 cd Jann 
 Number of lines from input source to use 
 export NUMTREES='100' 
 Number of neighbors to return 
 export NUMNEIGHBORS='10' 
 Define the environmental variables 
 export INFILE=""data/CMDC/all_lines_50.txt"" 
 Embed the lines using the encoder (Universal Sentence Encoder) 
 python embed_lines.py --infile=${INFILE} --verbose 
 Process the embeddings and save as unique strings and numpy array 
 python process_embeddings.py --infile=${INFILE} --verbose 
 Index the embeddings using an approximate nearest neighbor (annoy) 
 python index_embeddings.py --infile=${INFILE} --verbose --num_trees=${NUMTREES} 
 Build a simple command line interaction for model testing 
 python interact_with_model.py --infile=${INFILE} --verbose --num_neighbors=${NUMNEIGHBORS}
``` 
 Interaction 
 For interaction with the model, the only files needed are the unique strings ( _unique_strings.csv ) and the Annoy index ( .ann ) file.  
 With the unique strings and the index file you can build a basic interaction.  
 This is demonstrated in the  interact_with_model.py  file. 
 Pairs 
 Conversational dialogue is composed of sequences of utterances. The sequence can be seen as pairs of utterances: inputs and responses. 
 Nearest neighbours to a given input will find neighbours which are semantically related to the input. By storing input<>response pairs, rather than only inputs,  jann  can respond with a response to similar inputs. This example is shown in  run_examples/run_CMDC_pairs.sh . 
 Run Web Server 
 jann  is designed to run as a web service to be queried by a dialogue interface builder. For instance,  jann  is natively configured to be compatible with  Dialogflow Webhook Service . The web service runs using the Flask micro-framework and uses the performance-oriented  gunicorn  application server to launch the application with 4 workers. 
 ```sh
cd Jann 
 run the pairs set up and test the interaction 
 ./run_examples/run_CMDC_pairs.sh 
 pairs set up will write files needed for web server deployment 
 default data_key is all_lines_0 
 start development server 
 python app.py 
 or serve the pairs model with gunicorn and 4 workers 
 gunicorn --bind 0.0.0.0:8000 app:JANN -w 4
``` 
 Monitoring 
 It is helpful to see a Flask Monitoring dashboard to monitor statistics on the bot. There is a  Flask-MonitoringDashboard  which is already installed as part of Jann, see  Jann/app.py . 
 To view the dashboard, navigate to  http://0.0.0.0:8000/dashboard . The default user/pass is:  admin  /  admin . 
 Load / Lag Testing with Locust 
 Once  jann  is running, in a new terminal window you can test the load on the server with  Locust , as defined in  Jann/tests/locustfile.py : 
 sh
source venv/bin/activate
cd Jann/tests
locust --host=http://0.0.0.0:8000 
 You can then navigate a web browser to  http://0.0.0.0:8089/ , and simulate  N  users spawning at  M  users per second and making requests to  jann . 
 Testing the model by hand 
 sh
curl --header ""Content-Type: application/json"" \
  --request POST \
  --data '{""queryResult"": {""queryText"": ""that sounds really depressing""}}' \
  http://0.0.0.0:8000/model_inference 
 Response: 
 sh
{""fulfillmentText"":""Oh, come on, man. Tell me you wouldn't love it!""} 
 Custom Datasets 
 You can use any dataset you want! Format your source text with a single entry on each line, as follows: 
 ```sh 
 data/custom_data/example.txt 
 This is the first line.
This is the second line, a response to the first line.
This is the third line.
This is the fourth line, a response to the third line.
``` 
 Using other Universal Sentence Encoder embedding modules 
 There are  a collection of Universal Sentence Encoders  trained on a variety of data. 
 Note from  TensorFlow Hub : The module performs best effort text input preprocessing, therefore it is not required to preprocess the data before applying the module. 
 ```sh 
 Standard Model (914 MB) 
 wget 'https://tfhub.dev/google/universal-sentence-encoder/4?tf-hub-format=compressed' -O module_standard.tar.gz
mkdir -p universal-sentence-encoder && tar -zxvf module_standard.tar.gz -C universal-sentence-encoder
``` 
 Annoy parameters 
 There are two parameters for the Approximate Nearest Neighbour: 
 
 set  n_trees  as large as possible given the amount of memory you can afford, 
 set  search_k  as large as possible given the time constraints you have for the queries. This parameter is a interaction tradeoff between accuracy and speed. 
 
 Run details for GCP serving using nginx and uwsgi 
 You will need to configure your server with the necessary software: 
 ```sh
sudo apt update
sudo apt -y upgrade
sudo apt install unzip python3-pip python3-dev python3-venv build-essential libssl-dev libffi-dev python3-setuptools
sudo apt-get install nginx
git clone https://github.com/korymath/jann 
 and follow the installation and configuration steps above 
 sudo /etc/init.d/nginx start    # start nginx
``` 
 Then, you can reference a more in-depth guide  here . And here is a walkthrough on how to configure  nginx on GCP . 
 You will need the uwsgi_params file, which is available in the nginx directory of the uWSGI distribution, or from  the nginx GitHub repository . 
 ```sh 
 uwsgi_param  QUERY_STRING       $query_string;
uwsgi_param  REQUEST_METHOD     $request_method;
uwsgi_param  CONTENT_TYPE       $content_type;
uwsgi_param  CONTENT_LENGTH     $content_length; 
 uwsgi_param  REQUEST_URI        $request_uri;
uwsgi_param  PATH_INFO          $document_uri;
uwsgi_param  DOCUMENT_ROOT      $document_root;
uwsgi_param  SERVER_PROTOCOL    $server_protocol;
uwsgi_param  REQUEST_SCHEME     $scheme;
uwsgi_param  HTTPS              $https if_not_empty; 
 uwsgi_param  REMOTE_ADDR        $remote_addr;
uwsgi_param  REMOTE_PORT        $remote_port;
uwsgi_param  SERVER_PORT        $server_port;
uwsgi_param  SERVER_NAME        $server_name;
``` 
 Copy it into your project directory (e.g.  /home/${USER}/jann/uwsgi_params ).
In a moment we will tell nginx to refer to it. 
 We will serve our application over HTTP on port 80, so we need to enable it: 
 sh
sudo ufw allow 'Nginx HTTP' 
 This will allow HTTP traffic on port 80, the default HTTP port. 
 We can check the rule has been applied with: 
 ```sh
sudo ufw status 
 Status: active 
 To                         Action      From 
 --                         ------      ---- 
 Nginx HTTP                 ALLOW       Anywhere 
 Nginx HTTP (v6)            ALLOW       Anywhere (v6) 
 ``` 
 Make a Systemd unit file: 
 sh
[Unit]
Description=JANN as a well served Flask application.
After=network.target
[Service]
User=korymath
Group=www-data
WorkingDirectory=/home/korymath/jann/Jann
Environment=""PATH=/home/korymath/jann/venv/bin""
ExecStart=/home/korymath/jann/venv/bin/uwsgi --ini wsgi.ini
[Install]
WantedBy=multi-user.target 
 Then, copy the following into a file on your server, 
named:  /etc/nginx/sites-available/JANN.conf 
 ```sh 
 JANN.conf 
 server {
    listen      80;
    server_name 35.209.230.155;
    location / {
        include     /home/korymath/jann/uwsgi_params;
        uwsgi_pass unix:/home/korymath/jann/Jann/jann.sock;
    }
}
``` 
 Then, we tell nginx how to refer to the server 
 ```sh 
 link the site configuration to nginx enabled sites 
 sudo ln -s /etc/nginx/sites-available/JANN.conf /etc/nginx/sites-enabled/ 
 restart nginx 
 sudo systemctl restart nginx 
 restart jann 
 sudo systemctl restart jann
``` 
 Common Errors/Warnings and Solutions 
 sh
/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
  return f(*args, **kwds) 
 Solution (for OSX 10.13): 
 sh
pip install --ignore-installed --upgrade https://github.com/lakshayg/tensorflow-build/releases/download/tf1.9.0-macos-py27-py36/tensorflow-1.9.0-cp36-cp36m-macosx_10_13_x86_64.whl 
 FileNotFoundError 
 sh
FileNotFoundError: [Errno 2] No such file or directory: 'data/CMDC/movie_lines.txt' 
 Solution: 
 sh
Ensure that the input movie lines file is extracted to the correct path 
 ValueError 
 sh
ValueError: Signature 'spm_path' is missing from meta graph. 
 Solution 
 Currently  jann  is configured to use the  universal-sentence-encoder-lite  module from TFHub as it is small, lightweight, and ready for rapid deployment. This module depends on the  SentencePiece  library and the SentencePiece model published with the module. 
 You will need to make some minor code adjustments to use the heaviery modules (such as  universal-sentence-encoder 
and  universal-sentence-encoder-large . 
 Start Contributing 
 The guide for contributors can be found  here . It covers everything you need to know to start contributing to  jann . 
 References 
 
 Universal Sentence Encoder on TensorFlow Hub 
 Cer, Daniel, et al. 'Universal sentence encoder.' arXiv preprint arXiv:1803.11175 (2018). 
 Danescu-Niculescu-Mizil, Cristian, and Lillian Lee. 'Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs.' Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics. Association for Computational Linguistics, 2011. 
 
 Credits 
 jann  is made with love by  Kory Mathewson . 
 Icon made by  Freepik  from  www.flaticon.com  is licensed by  CC 3.0 BY . 
 dAIrector (🤖 + 📖) 
 dAIrector is an automated director which collaborates with humans storytellers. 
 Documentation 
 Go to  https://korymath.github.io/dairector/ 
 Set Up 
 ```sh 
 install homebrew 
 /usr/bin/ruby -e ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"" 
 install and upgrade portaudio, swig, git, python3 
 brew install --upgrade portaudio swig git python3 
 set up the python3 virtual environment 
 virtualenv -p python3 env 
 activate the virtual environment 
 source env/bin/activate 
 install requirements 
 pip install -r requirements.txt 
 in case of an error with pyaudio, may need to point to brew intall directly 
 see https://stackoverflow.com/questions/33513522/when-installing-pyaudio-pip-cannot-find-portaudio-h-in-usr-local-include 
 for more information 
 pip install --global-option='build_ext' --global-option='-I/Users/korymath/homebrew/Cellar/portaudio/19.6.0/include' --global-option='-L/Users/korymath/homebrew/Cellar/portaudio/19.6.0/lib' pyaudio 
 get the trained model and example files 
 wget https://storage.googleapis.com/api-project-941639660937.appspot.com/dairector_pretrained_examples.zip 
 unpack the files 
 unzip dairector_pretrained_examples.zip
``` 
 Run 
 ```sh 
 first ensure that your environment is activated 
 source env/bin/activate 
 example 1a, generate a new subgraph from the entire plotto conflict graph 
 python markovgenerator.py -t outputfile.json plottoconflicts.json 
 example 1b, interactive story telling using the plot subgraph and tv tropes hints 
 python -W ignore storyteller.py outputfile.json tvtropes.json tvtropesmodel.bin plottomodel.bin
``` 
 Interactive Beat Generation 
 The storyteller is interactive, it understands the following commands:
* next [ cue_text ]
* hint [ cue_text ]
* quit 
 next  uses the vector model from  plottomodel.bin  to find the next story beat based on the given cue text. 
 hint  uses the  tvtropesmodel.bin  to find an appropriate trope. 
 Install pocketsphinx 
 sh
pip install pocketsphinx==0.1.15
pip install PyAudio==0.2.11 
 Basic Usage 
 Improvisors on stage can cue the system to provide the next plot point or the next hint.
The improvisors provide the dialogue for each plot clause. 
 Training a new model 
 sh
python topicvectors.py tvtropesmodel.bin tvtropes.json 
 Cite 
 sh
@inproceedings{eger2018dairector,
  author = {{Eger}, M. and {Mathewson}, K.~W.},
  title = ""{dAIrector: Automatic Story Beat Generation through Knowledge Synthesis}"",
  booktitle = {AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE18), Joint Workshop on Intelligent Narrative Technologies and Intelligent Cinematography and Editing},
  publisher = {AAAI},
  year = 2018,
  address={Edmonton, Alberta, Canada},
  month = 10,
} 
 License 
 This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-sa/3.0/ or send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA. TEDRIC Analysis 
 1. Set up Environment 
 sh
virtualenv -p python3 env
source env/bin/activate
pip install -r requirements.txt 
 2. Run 
 ```sh
jupyter notebook 
 then run the TEDRIC_Analysis file 
 ``` 
 
 
 
 
  PROJECT LOGO  
 
 
 
 
 
 aNAOmate 
 
    A web-based interface for controlling SoftBank's Aldebaran Nao (V5/V6) robots.
     
 Explore the docs » 
 
 
 Report Bug 
    ·
     Request Feature 
 
  TABLE OF CONTENTS  
 Table of Contents 
 
 About the Project 
 Built With 
 Getting Started 
 Prerequisites 
 Installation 
 Usage 
 Roadmap 
 Contributing 
 License 
 Contact 
 Acknowledgements 
 
  ABOUT THE PROJECT  
 About The Project 
 aNAOmate  is an web-based interface for controlling SoftBank's Aldebaran Nao (V5/V6) robots. 
 
 Slidedeck 
 Built With 
 
 NAOqi API 
 Bootstrap 
 
  GETTING STARTED  
 Getting Started 
 To get a local copy up and running follow these simple steps. 
 Prerequisites 
 Things you need to use the software and how to install them.
* npm
 sh
npm install npm@latest -g 
 Installation 
 
 Clone the repo
 sh
git clone https:://github.com/QuinnyB/aNAOmate.git 
 
  USAGE EXAMPLES  
 Usage 
 For examples, please refer to the  Documentation 
 Editing TMI (Touch Move Interface for the Nao Robot) 
 
 Make changes in the  TMI  directory 
 Test the changes by following the quick testing instructions. 
 Once satisfied, ensure that all necessary files are linked in  TMI/TMI.pml . 
 Once links are complete, open the  TMI/TMI.pml  in Choregraphe 
 File -> Build Application Package  and save to the  package  directory. 
 Once the package is saved, install on the robot. (Note: NAOqi must be properly installed on your system for the installation script to work) 
 
 sh
/usr/bin/python package/install_pkg.py $ROBOT_IP $PACKAGE_FILE_NAME 
 
 Once the package is installed, you will see:  Installation complete.  (This installs the application to  /home/nao/.local/share/PackageManager/apps/TMI  and runs the application if  autorun=""true""  in the  TMI/manifest 
 The application interface is now running, and starts automatically by default. 
 Navigate to  http://$ROBOT_IP/apps/TMI  to see developed the interface. 
 This interface should match the tested changes in Step 2. 
 
 Using the aNAOmate interface 
 TODO 
 Behaviours 
 TODO 
 Configure Development Environment 
 TODO 
  ROADMAP  
 Roadmap 
 See the  open issues  for a list of proposed features (and known issues). 
 Authors 
 
 Quinn Boser  -  Website 
 Riley Dawson  -  Website 
 Josh Whitney  -  Website 
 Kory Mathewson  -  Website 
 
 See the list of  contributors  who participated in this project. 
  CONTRIBUTING  
 Contributing 
 Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are  greatly appreciated . 
 
 Fork the Project 
 Create your Feature Branch ( git checkout -b feature/AmazingFeature ) 
 Commit your Changes ( git commit -m 'Add some AmazingFeature' ) 
 Push to the Branch ( git push origin feature/AmazingFeature ) 
 Open a Pull Request 
 
  LICENSE  
 License 
 Distributed under the MIT License. See  LICENSE  for more information. 
  CONTACT  
 Contact 
 Josh Whitney -  @JoshJRWhitney  - joshjrwhitney@gmail.com 
 Project Link:  https://github.com/QuinnyB/aNAOmate 
  ACKNOWLEDGEMENTS  
 Acknowledgements 
 The authors of this project would like to graciously acknowledge to continued support of the community. 
  MARKDOWN LINKS & IMAGES  
  https://www.markdownguide.org/basic-syntax/#reference-style-links  2019 Edmonton Fringe Festival Show Finder 
 There is a  live demo . 
 Credits 
 Built by Kory Mathewson. Using show data from the Edmonton Fringe Festival Tickets  website . Based on  CSV to HTML Table  by Derek Eder. Equal Groups K-Means clustering 
 """"""Equal Groups K-Means clustering utlizing the scikit-learn api and related utilities."""""" 
 From:
https://github.com/ndanielsen/Same-Size-K-Means/blob/master/clustering/equal_groups.py 
 And made to work with python3 
 """"""Equal Groups K-Means clustering
90 percent of this is the Kmeans implmentations with the equal groups logic
located in  _labels_inertia_precompute_dense()  which follows the steps laid
out in the Elki Same-size k-Means Variation tutorial.
https://elki-project.github.io/tutorial/same-size_k_means
Please note that this implementation only works in scikit-learn 17.X as later
versions having breaking changes to this implementation.
Parameters 
 
 n_groups : int, optional, default: 8
    The number of clusters to form as well as the number of
    centroids to generate.
max_iter : int, default: 300
    Maximum number of iterations of the k-means algorithm for a
    single run.
n_init : int, default: 10
    Number of time the k-means algorithm will be run with different
    centroid seeds. The final results will be the best output of
    n_init consecutive runs in terms of inertia.
init : {'k-means++', 'random' or an ndarray}
    Method for initialization, defaults to 'k-means++':
    'k-means++' : selects initial cluster centers for k-mean
    clustering in a smart way to speed up convergence. See section
    Notes in k_init for more details.
    'random': choose k observations (rows) at random from data for
    the initial centroids.
    If an ndarray is passed, it should be of shape (n_groups, n_features)
    and gives the initial centers.
precompute_distances : {'auto', True, False}
    Precompute distances (faster but takes more memory).
    'auto' : do not precompute distances if n_samples * n_groups > 12
    million. This corresponds to about 100MB overhead per job using
    double precision.
    True : always precompute distances
    False : never precompute distances
tol : float, default: 1e-4
    Relative tolerance with regards to inertia to declare convergence
random_state : integer or numpy.RandomState, optional
    The generator used to initialize the centers. If an integer is
    given, it fixes the seed. Defaults to the global numpy random
    number generator.
verbose : int, default 0
    Verbosity mode.
copy_x : boolean, default True
    When pre-computing distances it is more numerically accurate to center
    the data first.  If copy_x is True, then the original data is not
    modified.  If False, the original data is modified, and put back before
    the function returns, but small numerical differences may be introduced
    by subtracting and then adding the data mean. 
 Attributes 
 cluster_centers_ : array, [n_groups, n_features]
    Coordinates of cluster centers
labels_ :
    Labels of each point
inertia_ : float
    Sum of distances of samples to their closest cluster center. 
 Notes 
 The k-means problem is solved using Lloyd's algorithm.
The average complexity is given by O(k n T), were n is the number of
samples and T is the number of iteration.
The worst case complexity is given by O(n^(k+2/p)) with
n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii,
'How slow is the k-means method?' SoCG2006)
In practice, the k-means algorithm is very fast (one of the fastest
clustering algorithms available), but it falls in local minima. That's why
it can be useful to restart it several times.
See also 
 
 MiniBatchKMeans:
    Alternative online implementation that does incremental updates
    of the centers positions using mini-batches.
    For large scale learning (say n_samples > 10k) MiniBatchKMeans is
    probably much faster to than the default batch implementation.
"""""" ParitBOT Dashboard 
 Hello, I'm Kory! 
 
 
 I am a Research Scientist with  DeepMind  and a Lab Scientist with the  Creative Destruction Lab .  
 I have a Ph.D. in Computing Science from the  University of Alberta  with the  Alberta Machine Intelligence Institute .  
 My research focuses on understanding interaction between intelligent systems.  
 I am currently focusing on  🧑‍🦳-🤖 interfaces. 
 You might also know me as an improvisational theatre performance artist with  Rapid Fire Theatre .  
 I like to fuse my interests by developing artificial intelligences to perform theatre alongside.  
 For more,  https://korymathewson.com/  and  Improbotics P5.EEGEdu 
 P5.EEGEdu is an educational website to learn about coding live animations with electroencephalogram (EEG) data. It is a teaching tool that allows for students to quickly interact with their own brain waves.  
 Visit  https://p5.eegedu.com/  for the live p5 sandbox website. 
 Installation for Development 
 If you are interested in developing p5.EEGEdu, here are some instructions to get you started. 
 Note: Currently p5.EEGEdu development requires a Mac OSX operating system.  
 To start, you will need to install  Homebrew  and  yarn . These are easy one-line installations for Mac users:  
 ```sh 
 Install homebrew 
 /usr/bin/ruby -e ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"" 
 Install yarn 
 NOTE: this will also install Node.js if it is not already installed. 
 brew install yarn  
 Node.js must be version 10.x for Muse interaction 
 Thus, if you are getting version issues, install n and switch versions 
 sudo npm install -g n 
 sudo n 10.16.0 
 ``` 
 Then, in terminal, clone the git repo and enter the folder: 
 sh
git clone https://github.com/kylemath/p5.EEGEdu
cd p5.EEGEdu 
 You then need to install the required packages for EEGEdu 
 sh
yarn install 
 Local Development Environment 
 Then, you can run the  Development Environment  of p5.EEGEdu: 
 sh
yarn start dev 
 If it is working correctly, the p5.EEGEdu application will open in a browser window at http://localhost:3000. 
 Local Production Environment 
 To start the  Local Production Environment , you can use the following commands:  
 sh
yarn run build
serve -s build 
 Deployment 
 p5.EEGEdu  is running on  Firebase  and deployment happens automagically using GitHub post-commit hooks, or  Actions , as they are commonly called. You can see how the application is build and deployed by  inspecting the workflow . 
 Contributing 
 The guide for contributors can be found  here . It covers everything you need to know to start contributing to p5.EEGEdu. 
 Development Roadmap 
 References 
 
 https://github.com/urish/muse-js - based toolbox for interacting with muse  
 https://github.com/NeuroJS/angular-muse - demo with streaming data in Angular, record button,  
 https://github.com/tanvach/muse-fft  - starting point react demo 
 https://github.com/neurosity/eeg-pipes - easy pipable operations on eeg data from muse-js 
 https://reactjs.org/  - React for web development 
 https://www.chartjs.org/docs/latest/ - interactive charts 
 https://github.com/urish/muse-lsl  - maybe useful to stream to LSL 
 
 Credits 
 p5.EEGEdu  - An Interactive Electrophysiology P5 Animation Coding Sandbox with the Interaxon Muse brought to you by Mathewson Sons 
 License 
 p5.EEGEdu is licensed under The MIT License (MIT) iccc-ai-art Just For Laughs 
 
 Setup 
 ```sh 
 Get the code 
 git clone https://github.com/korymath/justforlaughs;
cd justforlaughs/;
python3 -m venv venv;
source venv/bin/activate; 
 Install requirements 
 pip install --upgrade pip;
pip install -r requirements.txt;
``` 
 Run 
 ```sh 
 Run the web app 
 python app.py; 
 Run the standalone laughter detector 
 python segment_laughter.py --input_audio_file example_audio.wav; 
 It should output that it found 1 laugh in the example, save just the laugh cropped from the input, and the time window when laugh happened. 
 Example: 
 found 1 laughs. 
 [{'filename': 'output/laugh_0.wav', 'start': 2.6453333333333333, 'end': 5.261913043478261}] 
 ``` 
 Fixes 
 For running on mac, need to ensure that the libsndfile is correclty pathed, ref: https://github.com/bastibe/python-soundfile/issues/310
 sh
env DYLD_LIBRARY_PATH=""/opt/homebrew/lib:$DYLD_LIBRARY_PATH"" python app.py 
 Credits 
 Client-sider in-browser detection from: 
 
 https://github.com/tensorflow/tfjs-models/tree/master/speech-commands -  LICENSE 
 
 Laughter detection model from: 
 
 https://github.com/jrgillick/laughter-detection -  LICENSE 
 
 Audio interface and recording adapted from: 
 
 https://github.com/mattdiamond/Recorderjs -  LICENSE 
 https://github.com/addpipe/simple-recorderjs-demo -  No LICENSE ,  Blog post 
 ELO Demo 
 Setup and Install 
 sh
python3 -m venv venv;
source venv/bin/activate;
pip install --upgrade pip;
pip install -r requirements.txt; 
 Firebase Datastore 
 Ensure that  key.json  is present in root directory. For more information, see  the documentation  on deploying a similar application. 
 Run 
 sh
python main.py"
nicholas-leonard,"nnx: experimental 'nn' components 
 The original neural network from Torch7,  nn , contains stable and widely
used modules. 'nnx' contains more experimental, unproven modules, and
optimizations. Modules that become stable and which are proven useful make 
their way into 'nn' (some already have). 
 Library Documentation 
 This section includes documentation for the following objects: 
 
 SoftMaxTree  : a hierarchical log-softmax Module; 
 TreeNLLCriterion  : a negative log-likelihood Criterion for the SoftMaxTree; 
 CTCCriterion  : a Connectionist Temporal Classification Criterion based on  warp-ctc ; 
 PushTable (and PullTable)  : extracts a table element and inserts it later in the network; 
 MultiSoftMax  : performs a softmax over the last dimension of a 2D or 3D input; 
 SpatialReSampling  : performs bilinear resampling of a 3D or 4D input image; 
 [QDRiemaNNLinear] (#nnx.QDRiemaNNLinear) : quasi-diagonal reduction for Riemannian gradient descent 
 Recurrent  : a generalized recurrent neural network container; 
 
 
 SoftMaxTree 
 A hierarchy of parameterized log-softmaxes. Used for computing the likelihood of a leaf class. 
This Module should be used in conjunction with the  TreeNLLCriterion . 
Using this for large vocabularies (100,000 and more) greatly accelerates training and evaluation 
of neural network language models (NNLM). 
A vocabulary hierarchy is provided via the  dp  package's
 BillionWords 
 DataSource . 
 The constructor takes 2 mandatory and 4 optional arguments : 
 *  inputSize  : the number of units in the input embedding representation;
 *  hierarchy  : a Tensor mapping one  parent_id  to many  child_id  (a tree);
 *  rootId  : a number identifying the root node in the hierarchy. Defaults to  -1 ;
 *  accUpdate  : when the intent is to use  backwardUpdate  or  accUpdateGradParameters , set this to true to save memory. Defaults to false;
 *  static  : when true (the defualt), returns parameters with keys that don't change from batch to batch;
 *  verbose  : prints some additional information concerning the hierarchy during construction. 
 The  forward  method returns an  output  Tensor of size 1D, while 
 backward  returns a table  {gradInput, gradTarget} . The second 
variable is just a Tensor of zeros , such that the  targets  can be 
propagated through  Containers  
like  ParallelTable . 
 ```lua 
 
 input = torch.randn(5,10)
target = torch.IntTensor{20,24,27,10,12}
gradOutput = torch.randn(5)
root_id = 29
input_size = 10  
hierarchy = { 
 
 [29]=torch.IntTensor{30,1,2}, [1]=torch.IntTensor{3,4,5}, 
   [2]=torch.IntTensor{6,7,8}, [3]=torch.IntTensor{9,10,11},
   [4]=torch.IntTensor{12,13,14}, [5]=torch.IntTensor{15,16,17},
   [6]=torch.IntTensor{18,19,20}, [7]=torch.IntTensor{21,22,23},
   [8]=torch.IntTensor{24,25,26,27,28}
}
smt = nn.SoftMaxTree(input_size, hierarchy, root_id)
smt:forward{input, target}
-3.5186
-3.8950
-3.7433
-3.3071
-3.0522
[torch.DoubleTensor of dimension 5]
smt:backward({input, target}, gradOutput)
{
  1 : DoubleTensor - size: 5x10
  2 : IntTensor - size: 5
} 
 
 
 ``` 
 
 TreeNLLCriterion 
 Measures the Negative log-likelihood (NLL) for  SoftMaxTrees . 
Used for maximizing the likelihood of SoftMaxTree outputs.
The SoftMaxTree Module outputs a column Tensor representing the log likelihood
of each target in the batch. Thus SoftMaxTree requires the targets.
So this Criterion only computes the negative of those outputs, as 
well as its corresponding gradients. 
 
 
 PushTable (and PullTable) 
 PushTable and PullTable work together. The first can be put earlier
in a digraph of Modules such that it can communicate with a 
PullTable located later in the graph.  PushTable:forward(input)  
for an  input  table of Tensors to the output, excluding one, the index of which 
is specified by the  index  argument in the  PushTable(index)  constructor.
The Tensor identified by this  index  is communicated to one or many 
PullTables created via the  PushTable:pull(index)  factory method. 
These can be inserted later in the digraph such that 
a call to  PushTable:forward(input) , where  input  is a table or a Tensor, 
will output a table with the previously  pushed  Tensor inserted 
at index  index . 
 An example utilizing the above  SoftMaxTree  Module
and a Linear Module demonstrates how the PushTable can be used to 
forward the  target  Tensor without any other 
 Table Modules :
```lua 
 
 mlp = nn.Sequential()
linear = nn.Linear(50,100)
push = nn.PushTable(2)
pull = push:pull(2)
mlp:add(push)
mlp:add(nn.SelectTable(1))
mlp:add(linear)
mlp:add(pull)
mlp:add(smt) --smt is a SoftMaxTree instance
mlp:forward{input, target} -- input and target are defined above
-3.5186
-3.8950
-3.7433
-3.3071
-3.0522
[torch.DoubleTensor of dimension 5]
mlp:backward({input, target}, gradOutput) -- so is gradOutput
{
  1 : DoubleTensor - size: 5x10
  2 : IntTensor - size: 5
}
 The above code is equivalent to the following: lua
mlp2 = nn.Sequential()
para = nn.ParallelTable()
para:add(linear)
para:add(nn.Identity())
mlp2:add(para)
mlp2:add(smt)
mlp2:forward{input, target}
-3.5186
-3.8950
-3.7433
-3.3071
-3.0522
[torch.DoubleTensor of dimension 5]
mlp2:backward({input, target}, gradOutput)
{
  1 : DoubleTensor - size: 5x10
  2 : IntTensor - size: 5
}
```
In some cases, this can simplify the digraph of Modules. Note that 
a PushTable can be associated to many PullTables, but each PullTable 
is associated to only one PushTable. 
 
 
 CTCCriterion 
 criterion = nn.CTCCriterion() 
Creates a Criterion based on Baidus'  warp-ctc  implementation.
This Module measures the loss between a 3D output of (batch x time x inputdim) and a target without needing alignment of inputs and labels.
Must have installed warp-ctc which can be installed via luarocks:
 luarocks install http://raw.githubusercontent.com/baidu-research/warp-ctc/master/torch_binding/rocks/warp-ctc-scm-1.rockspec 
Supports cuda via:
 criterion = nn.CTCCriterion():cuda() 
Example:
```
output = torch.Tensor({{{1,2,3,4,5},{6,7,8,9,10}}}) -- Tensor of size 1x1x5 (batch x time x inputdim).
label = {{1,3}}
sizes = torch.Tensor({2}) -- Size of each sequence (sequence-length) in the batch as a tensor
ctcCriterion = nn.CTCCriterion() 
 err = ctcCriterion:forward(output,label,sizes)
gradOut = ctcCriterion:backward(output,label)
print(""----CPU----"")
print(""Error : "" .. err)
print(""Gradients :"")
print(gradOut) 
 ctcCriterion = ctcCriterion:cuda() -- Switch to cuda implementation.
output = output:cuda() 
 err = ctcCriterion:forward(output,label,sizes)
gradOut = ctcCriterion:backward(output,label)
print(""----GPU----"")
print(""Error : "" .. err)
print(""Gradients :"")
print(gradOut)
``` 
 gives the output:
```
----CPU---- 
Error : 4.9038286209106 
Gradients : 
(1,.,.) = 
  0.0117 -0.9683  0.0861  0.2341  0.6364
  0.0117  0.0317  0.0861 -0.7659  0.6364
[torch.FloatTensor of size 1x2x5] 
 ----GPU---- 
Error : 4.9038290977478 
Gradients : 
(1,.,.) = 
  0.0117 -0.9683  0.0861  0.2341  0.6364
  0.0117  0.0317  0.0861 -0.7659  0.6364
[torch.CudaTensor of size 1x2x5]
```
 
 MultiSoftMax 
 This Module takes 2D or 3D input and performs a softmax over the last dimension. 
It uses the existing  SoftMax  
CUDA/C code to do so such that the Module can be used on both GPU and CPU. 
This can be useful for  keypoint detection . 
 
 SpatialReSampling 
 Applies a 2D re-sampling over an input image composed of
several input planes (or channels, colors). The input tensor in  forward(input)  is 
expected to be a 3D or 4D tensor of size :  [batchSize x] nInputPlane x width x height . 
The number of output planes will be the same as the number of input
planes. 
 The re-sampling is done using  bilinear interpolation . 
For a simple nearest-neihbor upsampling, use  nn.SpatialUpSampling() ,
and for a simple average-based down-sampling, use 
 nn.SpatialDownSampling() . 
 If the input image is a 3D tensor of size  nInputPlane x height x width ,
the output image size will be  nInputPlane x oheight x owidth  where
 owidth  and  oheight  are given to the constructor. 
 Instead of  owidth  and  oheight , one can provide  rwidth  and  rheight , 
such that  owidth = iwidth*rwidth  and  oheight = iheight*rheight . 
 As an example, we can run the following code on the famous Lenna image:
 lua
require 'image'                                                           
require 'nnx'
input = image.loadPNG('doc/image/Lenna.png')
l = nn.SpatialReSampling{owidth=150,oheight=150}
output = l:forward(input)
image.save('doc/image/Lenna-150x150-bilinear.png', output) 
 The input: 
   
 The re-sampled output: 
   
 
 QDRiemaNNLinear 
 The Quasi-Diagonal Riemannian Neural Network Linear (QDRiemaNNLinear) module is an implementation
of the quasi-diagonal reduction of metrics, used for Riemannian gradient descent.
The algorithm is defined in Riemannian metrics for neural networks I: feedforward networks by Yann Ollivier (http://arxiv.org/abs/1303.0818) and an efficient implementation is described in Practical Riemannian Neural Networks by Yann Ollivier and Gaetan Marceau-Caron (http://arxiv.org/abs/1602.08007).
To use this module, simply replace  nn.Linear(ninput,noutput)  with  nnx.QDRiemaNNLinear(ninput,noutput) .
As always, the step-size must be chosen accordingly.
Two additional arguments are also possible:
* gamma (default=0.01): determine the update rate of the metric for a minibatch setting, i.e., (1-gamma) * oldMetric + gamma newMetric. Smaller minibatches require a smaller gamma. A default value depending on the size of the minibatches is  gamma = 1. - torch.pow(1.-1./nTraining,miniBatchSize)  where  nTraining  is the number of training examples of the dataset and  miniBatchSize  is the number of training examples per minibatch. 
* qdFlag (default=true): Whether to use the quasi-diagonal reduction (true) or only the diagonal (false). The former should be better. 
 This module is a straightforward implementation of the outer product gradient descent. 
 Requirements 
 
 Torch7 (www.torch.ch) 
 
 Installation 
 
 Install Torch7 (refer to its own documentation). 
 clone this project into dev directory of Torch7. 
 Rebuild torch, it will include new projects too. 
 
 Use the library 
 First run torch, and load nnx: 
 sh
$ torch   
 ``` lua 
 
 require 'nnx'
``` 
 
 Once loaded, tab-completion will help you navigate through the
library (note that most function are added directly to nn): 
 ``` lua 
 
 nnx. + TAB
...
nn. + TAB
``` 
 
 In particular, it's good to verify that all modules provided pass their
tests: 
 ``` lua 
 
 nnx.test_all()
nnx.test_omp()
``` 
 
 
 Recurrent 
 DEPRECATED July 6th, 2015. Use  rnn  instead. IFT6266 : Emotion Recognition 
 This model integrates with my pylear2 fork:
https://github.com/nicholas-leonard/pylearn2 
 Transfered by experimental log on github wiki: 
https://github.com/nicholas-leonard/ift6266/wiki 
 Torch-7 for Android 
 
 Torch7 provides a Matlab-like environment for state-of-the-art machine
learning algorithms. It is easy to use and provides a very efficient
implementation, thanks to an easy and fast scripting language (Lua) and a
underlying C implementation. 
 Modified to be compiled and used with Android 
 Features 
 
 Loading of lua packages from the apk directly. 
 This is done by writing a custom package.loader
  Reference: http://www.lua.org/manual/5.1/manual.html#pdf-package.loaders
  The loader is in torchandroid.cpp as loader_android 
 torchandroid.h and torchandroid.cpp give lots of helper functions to make life easier 
 Print function overriden to redirect to logcat (only handles strings for now) 
 Function to get apk assets as bytes (very useful) 
 
 Requirements 
 Android NDK and Android SDK 
 Samples 
 
 A sample project has been provided in android-demo 
 android-demo/jni/torchdemo.cpp is a simple use-case 
 android-demo/assets/main.lua is the file that is run 
 Vinayak Ghokale from e-lab Purdue (https://github.com/e-lab) contributed a face detector demo, which showcases a fuller use-case. 
 That's in the facedetector_e-lab folder. I made some changes to it to load assets etc. from apk as opposed to the sdcard, but it remains untouched otherwise. 
 
 Building Torch 
 
 open build.sh and modify ANDROID_NDK to your android ndk path. 
 run build script
$ sh build.sh 
 
 You can use torch in your android apps. The relevant directories are
* include - include directories
* lib - static libs cross-compiled for armeabi-v7a
* share - lua files 
 Building Example 
 
 Build Torch atleast once using the steps above. 
 [Optional] Connect your android phone in debugging mode,
              to automatically install the apk. 
 Change directory into android-demo folder. 
 Run build script.
$ sh build.sh 
 Run the app TorchDemo on your phone. 
 common 
 Repository used in common by my other projects 
 Dependencies 
 libpqtypes : 
    http://libpqtypes.esilo.com/
    requires a manual linking on ubuntu
    user version 1.5.0
boost: 
    http://www.boost.org/
    http://www.boost.org/doc/libs/1_51_0/libs/serialization/doc/index.html
    mostly for serialization, i.e. archives
postgresql:
    http://www.postgresql.org/
    need a working database server 8.4+ and dev header files
generalhashfunctions: 
    http://www.partow.net/programming/hashfunctions/   
    included in repository for convenience
    use his Makefile ift6085 
 Mixture of Clusters delicious 
 Experiments on the delicious.com dataset hps 
 Hyper Parameter Search: a framework for coding, training and testing pylearn2 models in PostgreSQL. hypermind 
 A distributed neural network framework for the brave equanimity 
 A distributed conditional computing experiment using neural 
decision trees. dp Package Reference Manual 
 
 dp  is a  d ee p  learning library designed for streamlining 
research and development using the  Torch7  distribution. 
It emphasizes flexibility through the elegant use of object-oriented 
 design patterns . 
 Documentation 
 This package includes lots of documentations and tutorials which you 
will find hosted on  readthedocs .
If you prefer, you can consult the docs using  github . cunnx 
 Experimental cuda nn package cub 
 Allows for installing CUB via luarocks mydp 
 A dummy repository to use as a starting point for you own tests, classes, scripts and experiments extending dp. torchx 
 This package contains various torch extensions:
 *  concat  : concatenates a table of tensors.
 *  find  : finds all indices of a given value.
 *  group  : sorts and groups similar tensor variables together. 
 *  remap  : recursively applies a function to tables of Tensors.
 *  md5  : used for hashing strings. 
 And some  paths  extensions :
 *  indexdir  : index a directory of millions of files for faster listing. 
 
 [res] torch.concat([res], tensors, [dim]) 
 Concatenates a table of Tensors along dimension  dim .
 *  res  is a tensor holding the concatenation of Tensors  tensor .
 *  tensors  is a table of tensors. Each tensor should have the same amount of dimensions and the same size for non- dim  dimensions.
 *  dim  is the dimension along which the tensors will be concatenated. Defaults to 1. 
 Example:
```lua 
 
 res = torch.concat({torch.rand(2,3),torch.randn(2,1),torch.randn(2,2)},2)
print(res)
 0.8621  0.7776  0.3284 -1.2884 -0.4939  0.6049
 0.8404  0.8996  0.5704  0.3911 -0.0428 -1.4627
[torch.DoubleTensor of dimension 2x6]
``` 
 
 
 [res] torch.find(tensor, val, [dim]) 
 Finds all indices of a given value  val  in Tensor  tensor . 
Returns a table of these indices by traversing the tensor one row 
at a time. When  dim=2 , the only valid value for dim other than  nil  (the default),
the function expects a matrix and returns the row-wise indices of each found 
value  val  in the row. 
 1D example:
```lua 
 
 res = torch.find(torch.Tensor{1,2,3,1,1,2}, 1)
unpack(res)
1  4  5
``` 
 
 2D example:
``` 
 
 tensor = torch.Tensor{{1,2,3,4,5},{5,6,0.6,0,2}}
unpack(torch.find(tensor, 2))
2   10 
unpack(torch.find(tensor:t(), 2))
3   10 
unpack(torch.find(tensor, 2, 2))
{2}  {5}
unpack(torch.find(tensor:t(), 2, 2))
{ }  {1}  { }  { }  {2}
``` 
 
 
 [res, val, idx] torch.group([val, idx], tensor, [samegrp, desc]) 
 Sorts and groups similar tensor variables together.
 *  res  is a table of  {idx=torch.LongTensor,val=torch.Tensor} .
 *  val  is a Tensor of the same type as  tensor . It will be used to store and return the sorted values.
 *  idx  is a  torch.LongTensor  used to store the sorted indices.
 *  tensor  is a Tensor that will have its values sorted, and then grouped by the  samegrp  function.
 *  samegrp  is a function taking two argument :  first_val  is the first value of the current group, while  val  is the current value of the current group. When the function returns true, it is assumed that  val  is of the same group as  first_val . Defaults to  function(first_val, val) return first_val == val; end 
 *  desc  is a boolean indicating whether the  tensor  gets sorted in descending order. Defaults to false. 
 Example:
```lua 
 
 tensor = torch.Tensor{5,3,4,5,3,5}
res, val, idx = torch.group(tensor)
res
{
  3 : 
    {
      idx : LongTensor - size: 2
      val : DoubleTensor - size: 2
    }
  4 : 
    {
      idx : LongTensor - size: 1
      val : DoubleTensor - size: 1
    }
  5 : 
    {
      idx : LongTensor - size: 3
      val : DoubleTensor - size: 3
    }
}
``` 
 
 
 [t1, t2] torch.remap(t1, t2, f(x,y) [p1, p2]) 
 Recursively applies function  f(x,y)  [to tables [of tables,...] of] Tensors
 t1  and  t2 . When prototypes  p1  or  p2  are provided, they are used 
to initialized any missing Tensors in  t1  or  t2 . 
 Example:
```lua 
 
 t1 = {torch.randn(3,4), {torch.randn(3,4), torch.randn(2,4), {torch.randn(1)}}}
t2 = {torch.randn(3,4), {torch.randn(3,4), torch.randn(2,4), {torch.randn(1)}}}
torch.remap(t1, t2, function(x, y) x:add(y) end)
{
  1 : DoubleTensor - size: 3x4
  2 : 
    {
      1 : DoubleTensor - size: 3x4
      2 : DoubleTensor - size: 2x4
      3 : 
        {
          1 : DoubleTensor - size: 1
        }
    }
}
{
  1 : DoubleTensor - size: 3x4
  2 : 
    {
      1 : DoubleTensor - size: 3x4
      2 : DoubleTensor - size: 2x4
      3 : 
        {
          1 : DoubleTensor - size: 1
        }
    }
}
 It also creates missing tensors: lua
t2, t3 = torch.remap(t2, nil, function(x, y) y:resizeAs(x):copy(x) end)
print(t3)
{
  1 : DoubleTensor - size: 3x4
  2 : 
    {
      1 : DoubleTensor - size: 3x4
      2 : DoubleTensor - size: 2x4
      3 : 
        {
          1 : DoubleTensor - size: 1
        }
    }
}
 When in doubt, first tensor has priority: lua
t4, t2 = torch.remap({torch.DoubleTensor()}, t2, function(x, y) x:resize(y:size()):copy(y) end, torch.LongTensor())
print(t4)
{
  1 : DoubleTensor - size: 3x4
}
t2, t5 = torch.remap(t2, {torch.DoubleTensor()}, function(x, y) y:resize(x:size()):copy(x) end, torch.LongTensor())
print(t5)
{
  1 : DoubleTensor - size: 3x4
  2 : 
    {
      1 : LongTensor - size: 3x4
      2 : LongTensor - size: 2x4
      3 : 
        {
          1 : LongTensor - size: 1
        }
    }
}
``` 
 
 
 torch.md5 
 Pure Lua module copy-pasted from  this repo  (for some reasons I can't get 
git submodule to work with luarocks). The module includes two functions:
 lua
local md5_as_hex   = torch.md5.sumhexa(message)   -- returns a hex string
local md5_as_data  = torch.md5.sum(message)     -- returns raw bytes 
The  torch.md5.sumhexa  function takes a string and returns another string:
 lua
torch.md5.sumhexa('helloworld!')
420e57b017066b44e05ea1577f6e2e12 
 
 [obj] paths.indexdir(path, [ext, use_cache, ignore]) 
 lua
files = paths.indexdir(""/path/to/files/"", 'png', true)
images = {}
for i=1,files:size() do
   local img = image.load(files:filename(i))
   table.insert(images, img)
end 
 This function can be used to create an object indexing all files having 
extensions  ext  (a string or a list thereof) in directory  path  (string or list thereof). 
Useful for directories containing many thousands of files. The function 
caches the resulting list to disk in  /tmp  such that it can be used 
for later calls when  use_cache=true  (default is false). 
Argument  ignore  species a pattern to ignore (e.g. "" frame "" will ignore all files containing  ""frame"" ). Torch Manual 
 Torch has lots of libraries and corresponding reference documentation. 
But this is where you can try to make some sense out of all of it. 
 The Basics 
 
 Introduction 
 Getting Started 
 Torch Ecosystem 
 Programming in Lua 
 Tensors 
 Object-Oriented Programming 
 Foreign Function Interface 
 
 Neural Network Training 
 
 Datasets 
 Building your own dataset  
 Modules 
 Writing your own torch module  
 Criterions 
 Optimization 
 
 Library Reference 
 
 Torch  : tensors, class factory, serialization, BLAS ; 
 nn  : neural network Modules and Criterions; 
 optim  : SGD, LBFGS and other optimization functions ; 
 gnuplot  : ploting and data visualization ; 
 paths  : make directories, concatenate file paths, and other filesystem utilities ; 
 image  : save, load, crop, scale, warp, translate images and such ; 
 trepl  : the torch LuaJIT interpreter ; 
 cwrap  : used for wrapping C/CUDA functions in Lua ; 
 dpnn : deep extensions to nn 
 This package provides many useful features that aren't part of the main nn package. 
These include  sharedClone , which allows you to clone a module and share 
parameters or gradParameters with the original module, without incuring any memory overhead.
We also redefined  type  such that the type-cast preserves Tensor sharing within a structure of modules.  
 The package provides the following Modules: 
 
 Decorator  : abstract class to change the behaviour of an encapsulated module ; 
 DontCast  : prevent encapsulated module from being casted by  Module:type()  ; 
 Serial  : decorate a module makes its serialized output more compact ;  
 NaN  : decorate a module to detect the source of NaN errors ; 
 Inception  : implements the Inception module of the GoogleLeNet article ; 
 Collapse  : just like  nn.View(-1) ; 
 Convert  : convert between different tensor types or shapes; 
 ZipTable  : zip a table of tables into a table of tables; 
 ZipTableOneToMany  : zip a table of element  el  and table of elements into a table of pairs of element  el  and table elements; 
 CAddTensorTable  : adds a tensor to a table of tensors of the same size; 
 ReverseTable  : reverse the order of elements in a table; 
 PrintSize  : prints the size of inputs and gradOutputs (useful for debugging); 
 Clip  : clips the inputs to a min and max value; 
 Constant  : outputs a constant value given an input (which is ignored); 
 SpatialUniformCrop  : uniformly crops patches from a input; 
 SpatialGlimpse  : takes a fovead glimpse of an image at a given location; 
 WhiteNoise  : adds isotropic Gaussian noise to the signal when in training mode; 
 OneHot  : transforms a tensor of indices into  one-hot  encoding; 
 Kmeans  :  Kmeans  clustering layer. Forward computes distances with respect to centroids and returns index of closest centroid. Centroids can be updated using gradient descent. Centroids could be initialized randomly or by using  kmeans++  algoirthm; 
 SpatialRegionDropout  : Randomly dropouts a region (top, bottom, leftmost, rightmost) of the input image. Works with batch and any number of channels; 
 FireModule  : FireModule as mentioned in the  SqueezeNet ; 
 NCEModule  : optimized placeholder for a  Linear  +  SoftMax  using  noise-contrastive estimation . 
 SpatialFeatNormalization  : Module for widely used preprocessing step of mean zeroing and standardization for images. 
 SpatialBinaryConvolution  : Module for binary spatial convolution (Binary weights) as mentioned in  XNOR-Net . 
 SimpleColorTransform  : Module for adding independent random noise to input image channels. 
 PCAColorTransform  : Module for adding noise to input image using Principal Components Analysis. 
 
 The following modules and criterions can be used to implement the REINFORCE algorithm : 
 
 Reinforce  : abstract class for REINFORCE modules; 
 ReinforceBernoulli  : samples from Bernoulli distribution; 
 ReinforceNormal  : samples from Normal distribution; 
 ReinforceGamma  : samples from Gamma distribution; 
 ReinforceCategorical  : samples from Categorical (Multinomial with one sample) distribution; 
 VRClassReward  : criterion for variance-reduced classification-based reward; 
 BinaryClassReward  : criterion for variance-reduced binary classification reward (like  VRClassReward , but for binary classes); 
 
 Additional differentiable criterions
 *  BinaryLogisticRegression  : criterion for binary logistic regression;
 *  SpatialBinaryLogisticRegression  : criterion for pixel wise binary logistic regression;
 *  NCECriterion  : criterion exclusively used with  NCEModule .
 *  ModuleCriterion  : adds an optional  inputModule  and  targetModule  before a decorated criterion;
 *  BinaryLogisticRegression  : criterion for binary logistic regression.
 *  SpatialBinaryLogisticRegression  : criterion for pixel wise binary logistic regression. 
 A lot of the functionality implemented here was pulled from 
 dp , which makes heavy use of this package. 
However, dpnn can be used without dp (for e.g. you can use it with optim), 
which is one of the main reasons why we made it. 
 Tutorials 
 Sagar Waghmare  wrote a nice  tutorial 
on how to use dpnn with nngraph to reproduce the 
 Lateral Connections in Denoising Autoencoders Support Supervised Learning .  
 A brief (1 hours) overview of Torch7, which includes some details about  dpnn , 
is available via this  NVIDIA GTC Webinar video . In any case, this presentation gives a nice overview of Logistic Regression, Multi-Layer Perceptrons, Convolutional Neural Networks and Recurrent Neural Networks using Torch7. 
 
 Module 
 The Module interface has been further extended with methods that facilitate 
stochastic gradient descent like  updateGradParameters  (i.e. momentum learning), 
 weightDecay ,  maxParamNorm  (for regularization), and so on. 
 
 Module.dpnn_parameters 
 A table that specifies the name of parameter attributes. 
Defaults to  {'weight', 'bias'} , which is a static variable (i.e. table exists in class namespace). 
Sub-classes can define their own table statically.  
 
 Module.dpnn_gradParameters 
 A table that specifies the name of gradient w.r.t. parameter attributes. 
Defaults to  {'gradWeight', 'gradBias'} , which is a static variable (i.e. table exists in class namespace). 
Sub-classes can define their own table statically.  
 
 [self] Module:type(type_str) 
 This function converts all the parameters of a module to the given  type_str . 
The  type_str  can be one of the types defined for  torch.Tensor 
like  torch.DoubleTensor ,  torch.FloatTensor  and  torch.CudaTensor . 
Unlike the  type method 
defined in  nn , this one was overriden to 
maintain the sharing of  storage 
among Tensors. This is especially useful when cloning modules share  parameters  and  gradParameters . 
 
 [clone] Module:sharedClone([shareParams, shareGradParams]) 
 Similar to  clone .
Yet when  shareParams = true  (the default), the cloned module will share the parameters 
with the original module. 
Furthermore, when  shareGradParams = true  (the default), the clone module will share 
the gradients w.r.t. parameters with the original module.
This is equivalent to :
 lua
clone = mlp:clone()
clone:share(mlp, 'weight', 'bias', 'gradWeight', 'gradBias') 
yet it is much more efficient, especially for modules with lots of parameters, as these 
Tensors aren't needlessly copied during the  clone .
This is particularly useful for  Recurrent neural networks  
which require efficient copies with shared parameters and gradient w.r.t. parameters for each time-step. 
 
 Module:maxParamNorm([maxOutNorm, maxInNorm]) 
 This method implements a hard constraint on the upper bound of the norm of output and/or input neuron weights 
 (Hinton et al. 2012, p. 2)  .
In a weight matrix, this is a contraint on rows ( maxOutNorm ) and/or columns ( maxInNorm ), respectively. 
Has a regularization effect analogous to  weightDecay , but with easier to optimize hyper-parameters. 
Assumes that parameters are arranged ( output dim x ... x input dim ). 
Only affects parameters with more than one dimension.
The method should normally be called after  updateParameters . 
It uses the C/CUDA optimized  torch.renorm  function.
Hint :  maxOutNorm = 2  usually does the trick.  
 
 [momGradParams] Module:momentumGradParameters() 
 Returns a table of Tensors ( momGradParams ). For each element in the 
table, a corresponding parameter ( params ) and gradient w.r.t. parameters 
( gradParams ) is returned by a call to  parameters .
This method is used internally by  updateGradParameters . 
 
 Module:updateGradParameters(momFactor [, momDamp, momNesterov]) 
 Applies classic momentum or Nesterov momentum  (Sutskever, Martens et al, 2013)  to parameter gradients. 
Each parameter Tensor ( params ) has a corresponding Tensor of the same size for gradients w.r.t. parameters ( gradParams ).
When using momentum learning, another Tensor is added for each parameter Tensor ( momGradParams ).
This method should be called before  updateParameters 
as it affects the gradients w.r.t. parameters. 
 Classic momentum is computed as follows : 
 lua
momGradParams = momFactor*momGradParams + (1-momDamp)*gradParams
gradParams = momGradParams   
 where  momDamp  has a default value of  momFactor . 
 Nesterov momentum ( momNesterov = true ) is computed as follows (the first line is the same as classic momentum): 
 lua
momGradParams = momFactor*momGradParams + (1-momDamp)*gradParams
gradParams = gradParams + momFactor*momGradParams  
The default is to use classic momentum ( momNesterov = false ). 
 
 Module:weightDecay(wdFactor [, wdMinDim]) 
 Decays the weight of the parameterized models. 
Implements an L2 norm loss on parameters with dimensions greater or equal to  wdMinDim  (default is 2).
The resulting gradients are stored into the corresponding gradients w.r.t. parameters.
Such that this method should be called before  updateParameters . 
 
 Module:gradParamClip(cutoffNorm [, moduleLocal]) 
 Implements a contrainst on the norm of gradients w.r.t. parameters  (Pascanu et al. 2012) .
When  moduleLocal = false  (the default), the norm is calculated globally to Module for which this is called.
So if you call it on an MLP, the norm is computed on the concatenation of all parameter Tensors.
When  moduleLocal = true , the norm constraint is applied 
to the norm of all parameters in each component (non-container) module.
This method is useful to prevent the exploding gradient in 
 Recurrent neural networks . 
 
 Module:reinforce(reward) 
 This method is used by Criterions that implement the REINFORCE algorithm like  VRClassReward . 
While vanilla backpropagation (gradient descent using the chain rule), 
REINFORCE Criterions broadcast a  reward  to all REINFORCE modules between the  forward  and the  backward .
In this way, when the following call to  backward  reaches the REINFORCE modules, 
these will compute a  gradInput  using the broadcasted  reward .
The  reward  is broadcast to all REINFORCE modules contained 
within  model  by calling  model:reinforce(reward) . 
Note that the  reward  should be a 1D tensor of size  batchSize , 
i.e. each example in a batch has its own scalar reward. 
 Refer to  this example 
for a complete training script making use of the REINFORCE interface. 
 
 Decorator 
 lua
dmodule = nn.Decorator(module) 
 This module is an abstract class used to decorate a  module . This means 
that method calls to  dmodule  will call the same method on the encapsulated 
 module , and return its results. 
 
 DontCast 
 lua
dmodule = nn.DontCast(module) 
 This module is a decorator. Use it to decorate a module that you don't
want to be cast when the  type()  method is called. 
 lua
module = nn.DontCast(nn.Linear(3,4):float())
module:double()
th> print(module:forward(torch.FloatTensor{1,2,3}))
 1.0927
-1.9380
-1.8158
-0.0805
[torch.FloatTensor of size 4]   
 
 Serial 
 lua
dmodule = nn.Serial(module, [tensortype])
dmodule:[light,medium,heavy]Serial()   
 This module is a decorator that can be used to control the serialization/deserialization 
behavior of the encapsulated module. Basically, making the resulting string or 
file heavy (the default), medium or light in terms of size.  
 Furthermore, when specified, the  tensortype  attribute (e.g  torch.FloatTensor ,  torch.DoubleTensor  and so on.),
determines what type the module will be cast to during serialization. 
Note that this will also be the type of the deserialized object.
The default serialization  tensortype  is  nil , i.e. the module is serialized as is.  
 The  heavySerial()  has the serialization process serialize every attribute in the module graph, 
which is the default behavior of nn.  
 The  mediumSerial()  has the serialization process serialize 
everything except the attributes specified in each module's  dpnn_mediumEmpty 
table, which has a default value of  {'output', 'gradInput', 'momGradParams', 'dpnn_input'} .
During serialization, whether they be tables or Tensors, these attributes are emptied (no storage).
Some modules overwrite the default  Module.dpnn_mediumEmpty  static attribute with their own. 
 The  lightSerial()  has the serialization process empty 
everything a call to  mediumSerial(type)  would (so it uses  dpnn_mediumEmpty ).
But also empties all the parameter gradients specified by the 
attribute  dpnn_gradParameters , which defaults to  {gradWeight, gradBias} . 
 We recomment using  mediumSerial()  for training, and  lightSerial()  for 
production (feed-forward-only models). 
 
 NaN 
 lua
dmodule = nn.NaN(module, [id])   
 The  NaN  module asserts that the  output  and  gradInput  of the decorated  module  do not contain NaNs.
This is useful for locating the source of those pesky NaN errors. 
The  id  defaults to automatically incremented values of  1,2,3,... . 
 For example : 
 lua
linear = nn.Linear(3,4)
mlp = nn.Sequential()
mlp:add(nn.NaN(nn.Identity()))
mlp:add(nn.NaN(linear))
mlp:add(nn.NaN(nn.Linear(4,2)))
print(mlp)   
 As you can see the  NaN  layers are have unique ids : 
 lua
nn.Sequential {
  [input -> (1) -> (2) -> (3) -> output]
  (1): nn.NaN(1) @ nn.Identity
  (2): nn.NaN(2) @ nn.Linear(3 -> 4)
  (3): nn.NaN(3) @ nn.Linear(4 -> 2)
}   
 And if we fill the  bias  of the linear module with NaNs and call  forward : 
 lua
nan = math.log(math.log(0)) -- this is a nan value
linear.bias:fill(nan)
mlp:forward(torch.randn(2,3))   
 We get a nice error message:
 lua
/usr/local/share/lua/5.1/dpnn/NaN.lua:39: NaN found in parameters of module :
nn.NaN(2) @ nn.Linear(3 -> 4)   
 For a quick one-liner to catch NaNs anywhere inside a model (for example, a  nn.Sequential  or any other  nn.Container ), we can use this with the  nn.Module.replace  function:
 lua
model:replace(function(module) return nn.NaN(module) end) 
 
 Inception 
 References : 
 
 A.  Going Deeper with Convolutions 
 B.  GoogleLeNet 
 
 lua
module = nn.Inception(config)   
 This module uses  n +2 parallel ""columns"". 
The original paper uses 2+2 where the first two are (but there could be more than two): 
 
 1x1 conv (reduce) -> relu -> 5x5 conv -> relu 
 1x1 conv (reduce) -> relu -> 3x3 conv -> relu  
 
 and where the other two are :  
 
 3x3 maxpool -> 1x1 conv (reduce/project) -> relu  
 1x1 conv (reduce) -> relu.  
 
 This module allows the first group of columns to be of any 
number while the last group consist of exactly two columns.
The 1x1 convoluations are used to reduce the number of input channels 
(or filters) such that the capacity of the network doesn't explode. 
We refer to these here has  reduce . 
Since each column seems to have one and only one reduce, their initial 
configuration options are specified in lists of n+2 elements. 
 The sole argument  config  is a table taking the following key-values : 
 
 Required Arguments : 
 inputSize  : number of input channels or colors, e.g. 3; 
 outputSize  : numbers of filters in the non-1x1 convolution kernel sizes, e.g.  {32,48} 
 reduceSize  : numbers of filters in the 1x1 convolutions (reduction) used in each column, e.g.  {48,64,32,32} . The last 2 are used respectively for the max pooling (projection) column (the last column in the paper) and the column that has nothing but a 1x1 conv (the first column in the paper). This table should have two elements more than the outputSize 
 Optional Arguments : 
 reduceStride  : strides of the 1x1 (reduction) convolutions. Defaults to  {1,1,...} . 
 transfer  : transfer function like  nn.Tanh , nn.Sigmoid ,  nn.ReLU ,  nn.Identity , etc. It is used after each reduction (1x1 convolution) and convolution. Defaults to  nn.ReLU . 
 batchNorm  : set this to  true  to use batch normalization. Defaults to  false . Note that batch normalization can be awesome 
 padding  : set this to  true  to add padding to the input of the convolutions such that output width and height are same as that of the original non-padded  input . Defaults to  true . 
 kernelSize  : size ( height = width ) of the non-1x1 convolution kernels. Defaults to  {5,3} . 
 kernelStride  : stride of the kernels ( height = width ) of the convolution. Defaults to  {1,1} 
 poolSize : size ( height = width ) of the spatial max pooling used in the next-to-last column. Defaults to 3. 
 poolStride  : stride ( height = width ) of the spatial max pooling. Defaults to 1. 
 
 For a complete example using this module, refer to the following :
 *  deep inception training script  ;
 *  openface facial recognition  (the model definition is  here ). 
 
 Collapse 
 lua
module = nn.Collapse(nInputDim)   
 This module is the equivalent of:
 view = nn.View(-1)
view:setNumInputDim(nInputDim)  
It collapses all non-batch dimensions. This is useful for converting 
a spatial feature map to the single dimension required by a dense 
hidden layer like Linear. 
 
 Convert 
 lua
module = nn.Convert([inputShape, outputShape])  
Module to convert between different data formats.
For example, we can flatten images by using :
 lua
module = nn.Convert('bchw', 'bf')  
or equivalently
 lua
module = nn.Convert('chw', 'f')  
Lets try it with an input:
 lua
print(module:forward(torch.randn(3,2,3,1)))
 0.5692 -0.0190  0.5243  0.7530  0.4230  1.2483
-0.9142  0.6013  0.5608 -1.0417 -1.4014  1.0177
-1.5207 -0.1641 -0.4166  1.4810 -1.1725 -1.0037
[torch.DoubleTensor of size 3x6]  
You could also try: 
 ```lua
module = nn.Convert('chw', 'hwc')
input = torch.randn(1,2,3,2)
input:select(2,1):fill(1)
input:select(2,2):fill(2)
print(input)
(1,1,.,.) = 
  1  1
  1  1
  1  1
(1,2,.,.) = 
  2  2
  2  2
  2  2
[torch.DoubleTensor of size 1x2x3x2]
print(module:forward(input))
(1,1,.,.) = 
  1  2
  1  2 
 (1,2,.,.) = 
  1  2
  1  2 
 (1,3,.,.) = 
  1  2
  1  2
[torch.DoubleTensor of size 1x3x2x2]
```  
 Furthermore, it automatically converts the  input  to have the same type as  self.output 
(i.e. the type of the module).
So you can also just use is for automatic input type converions:
 lua
module = nn.Convert()
print(module.output) -- type of module
[torch.DoubleTensor with no dimension]
input = torch.FloatTensor{1,2,3}
print(module:forward(input))
 1
 2
 3
[torch.DoubleTensor of size 3] 
 
 ZipTable 
 lua
module = nn.ZipTable() 
 Zips a table of tables into a table of tables. 
 Example:
 lua
print(module:forward{ {'a1','a2'}, {'b1','b2'}, {'c1','c2'} })
{ {'a1','b1','c1'}, {'a2','b2','c2'} } 
 
 ZipTableOneToMany 
 lua
module = nn.ZipTableOneToMany() 
 Zips a table of element  el  and table of elements  tab  into a table of tables, where the i-th table contains the element  el  and the i-th element in table  tab 
 Example:
 lua
print(module:forward{ 'el', {'a','b','c'} })
{ {'el','a'}, {'el','b'}, {'el','c'} } 
 
 CAddTensorTable 
 lua
module = nn.CAddTensorTable() 
 Adds the first element  el  of the input table  tab  to each tensor contained in the second element of  tab , which is itself a table 
 Example:
 lua
print(module:forward{ (0,1,1), {(0,0,0),(1,1,1)} })
{ (0,1,1), (1,2,2) } 
 
 ReverseTable 
 lua
module = nn.ReverseTable() 
 Reverses the order of elements in a table. 
 Example: 
 lua
print(module:forward{1,2,3,4})
{4,3,2,1} 
 
 PrintSize 
 lua
module = nn.PrintSize(name) 
 This module is useful for debugging complicated module composites. 
It prints the size of the  input  and  gradOutput  during  forward 
and  backward  propagation respectively.
The  name  is a string used to identify the module along side the printed size. 
 
 Clip 
 lua
module = nn.Clip(minval, maxval) 
 This module clips  input  values such that the output is between  minval  and  maxval . 
 
 Constant 
 lua
module = nn.Constant(value, nInputDim) 
 This module outputs a constant value given an input.
If  nInputDim  is specified, it uses the input to determine the size of the batch. 
The  value  is then replicated over the batch. 
Otherwise, the  value  Tensor is output as is.
During  backward , the returned  gradInput  is a zero Tensor of the same size as the  input .
This module has no trainable parameters.  
 You can use this with nn.ConcatTable() to append constant inputs to an input :  
 lua
nn.ConcatTable():add(nn.Constant(v)):add(nn.Identity()) 
 This is useful when you want to output a value that is independent of the 
input to the neural network (see  this example ). 
 
 SpatialUniformCrop 
 lua
module = nn.SpatialUniformCrop(oheight, owidth) 
 During training, this module will output a cropped patch of size  oheight, owidth 
within the boundaries of the  input  image.
For each example, a location is sampled from a uniform distribution 
such that each possible patch has an equal probability of being sampled. 
 During evaluation, the center patch is cropped and output. 
 This module is commonly used at the input layer to artificially 
augment the size of the dataset to prevent overfitting. 
 
 SpatialGlimpse 
 Ref. A.  Recurrent Model for Visual Attention 
 lua
module = nn.SpatialGlimpse(size, depth, scale) 
 A glimpse is the concatenation of down-scaled cropped images of 
increasing scale around a given location in a given image.
The input is a pair of Tensors:  {image, location} 
 location  are  (y,x)  coordinates of the center of the different scales 
of patches to be cropped from image  image . 
Coordinates are between  (-1,-1)  (top-left) and  (1,1)  (bottom-right).
The  output  is a batch of glimpses taken in image at location  (y,x) . 
 size  can be either a scalar which specifies the  width = height  of glimpses, 
or a table of  {height, width}  to support a rectangular shape of glimpses.
 depth  is number of patches to crop per glimpse (one patch per depth).
 scale  determines the  size(t) = scale * size(t-1)  of successive cropped patches. 
 So basically, this module can be used to focus the attention of the model 
on a region of the input  image . 
It is commonly used with the  RecurrentAttention  
module (see  this example ). 
 
 WhiteNoise 
 lua
module = nn.WhiteNoise([mean, stdev]) 
 Useful in training [Denoising Autoencoders] (http://arxiv.org/pdf/1507.02672v1.pdf). 
Takes  mean  and  stdev  of the normal distribution as input. 
Default values for mean and standard deviation are 0 and 0.1 respectively. 
With  module:training() , noise is added during forward. 
During  backward  gradients are passed as it is. 
With  module:evaluate()  the mean is added to the input. 
 
 SpatialRegionDropout 
 lua
module = nn.SpatialRegionDropout(p) 
Following is an example of  SpatialRegionDropout  outputs on the famous lena image. 
 Input 
 
 Outputs 
   
 
 FireModule 
 Ref: http://arxiv.org/pdf/1602.07360v1.pdf
 lua
module = nn.FireModule(nInputPlane, s1x1, e1x1, e3x3, activation) 
FireModule is comprised of two submodules 1) A  squeeze  convolution module comprised of  1x1  filters followed by 2) an  expand  module that is comprised of a mix of  1x1  and  3x3  convolution filters.
Arguments:  s1x1 : number of  1x1  filters in the squeeze submodule,  e1x1 : number of  1x1  filters in the expand submodule,  e3x3 : number of  3x3  filters in the expand submodule. It is recommended that  s1x1  be less than  (e1x1+e3x3)  if you want to limit the number of input channels to the  3x3  filters in the expand submodule.
FireModule works only with batches, for single sample convert the sample to a batch of size 1. 
 
 SpatialFeatNormalization 
 lua
module = nn.SpatialFeatNormalization(mean, std) 
This module normalizies each feature channel of input image based on its corresponding mean and standard deviation scalar values. This module does not learn the  mean  and  std , they are provided as arguments. 
 
 SpatialBinaryConvolution 
 lua
module = nn.SpatialBinaryConvolution(nInputPlane, nOutputPlane, kW, kH) 
Functioning of SpatialBinaryConvolution is similar to nn/SpatialConvolution. Only difference is that Binary weights are used for forward/backward and floating point weights are used for weight updates. Check  Binary-Weight-Network  section of  XNOR-net . 
 
 SimpleColorTransform 
 lua
range = torch.rand(inputChannels) -- Typically range is specified by user.
module = nn.SimpleColorTransform(inputChannels, range) 
This module performs a simple data augmentation technique. SimpleColorTransform module adds random noise to each color channel independently. In more advanced data augmentation technique noise is added using principal components of color channels. For that please check  PCAColorTransform 
 
 PCAColorTransform 
 lua
eigenVectors = torch.rand(inputChannels, inputChannels) -- Eigen Vectors
eigenValues = torch.rand(inputChannels) -- Eigen
std = 0.1 -- Std deviation of normal distribution with mean zero for noise.
module = nn.PCAColorTransform(inputChannels, eigenVectors, eigenValues, std) 
This module performs a data augmentation using Principal Component analysis of pixel values. When in training mode, mulitples of principal components are added to input image pixels. Magnitude of value added (noise) is dependent upon the corresponding eigen value and a random value sampled from a Gaussian distribution with mean zero and  std  (default 0.1) standard deviation. This technique was used in the famous  AlexNet  paper. 
 
 OneHot 
 lua
module = nn.OneHot(outputSize) 
 Transforms a tensor of  input  indices having integer values between 1 and  outputSize  into
a tensor of one-hot vectors of size  outputSize .  
 Forward an index to get a one-hot vector : 
 ```lua 
 
 module = nn.OneHot(5) -- 5 classes
module:forward(torch.LongTensor{3})
 0  0  1  0  0
[torch.DoubleTensor of size 1x5]
```  
 
 Forward a batch of 3 indices. Notice that these need not be stored as  torch.LongTensor  : 
 ```lua 
 
 module:forward(torch.Tensor{3,2,1})
 0  0  1  0  0
 0  1  0  0  0
 1  0  0  0  0
[torch.DoubleTensor of size 3x5]
```  
 
 Forward batch of  2 x 3  indices : 
 ```lua
oh:forward(torch.Tensor{{3,2,1},{1,2,3}})
(1,.,.) = 
  0  0  1  0  0
  0  1  0  0  0
  1  0  0  0  0 
 (2,.,.) = 
  1  0  0  0  0
  0  1  0  0  0
  0  0  1  0  0
[torch.DoubleTensor of size 2x3x5]
```  
 
 Kmeans 
 lua
km = nn.Kmeans(k, dim) 
 k  is the number of centroids and  dim  is the dimensionality of samples.
You can either initialize centroids randomly from input samples or by using  kmeans++  algorithm. 
 lua
km:initRandom(samples) -- Randomly initialize centroids from input samples.
km:initKmeansPlus(samples) -- Use Kmeans++ to initialize centroids.   
 Example showing how to use Kmeans module to do standard Kmeans clustering. 
 ```lua
attempts = 10
iter = 100 -- Number of iterations
bestKm = nil
bestLoss = math.huge
learningRate = 1
for j=1, attempts do
   local km = nn.Kmeans(k, dim)
   km:initKmeansPlus(samples)
   for i=1, iter do
      km:zeroGradParameters()
      km:forward(samples) -- sets km.loss
      km:backward(samples, gradOutput) -- gradOutput is ignored 
   -- Gradient Descent weight/centroids update
  km:updateParameters(learningRate)
 
 end 
 if km.loss < bestLoss then
      bestLoss = km.loss
      bestKm = km:clone()
   end
end
 `` nn.Kmeans()` module maintains loss only for the latest forward. If you want to maintain loss over the whole dataset then you who would need do it my adding the module loss for every forward. 
 You can also use  nn.Kmeans()  as an auxillary layer in your network. 
A call to  forward  will generate an  output  containing the index of the nearest cluster for each sample in the batch.
The  gradInput  generated by  updateGradInput  will be zero.  
 
 ModuleCriterion 
 lua
criterion = nn.ModuleCriterion(criterion [, inputModule, targetModule, castTarget])   
 This criterion decorates a  criterion  by allowing the  input  and  target  to be 
fed through an optional  inputModule  and  targetModule  before being passed to the 
 criterion . The  inputModule  must not contain parameters as these would not be updated.  
 When  castTarget = true  (the default), the  targetModule  is cast along with the  inputModule  and 
 criterion . Otherwise, the  targetModule  isn't.   
 
 NCEModule 
 Ref. A  RNNLM training with NCE for Speech Recognition 
 lua
ncem = nn.NCEModule(inputSize, outputSize, k, unigrams, [Z])   
 When used in conjunction with  NCECriterion , 
the  NCEModule  implements  noise-contrastive estimation . 
 The point of the NCE is to speedup computation for large  Linear  +  SoftMax  layers.
Computing a forward/backward for  Linear(inputSize, outputSize)  for a large  outputSize  can be very expensive.
This is common when implementing language models having with large vocabularies of a million words.
In such cases, NCE can be an efficient alternative to computing the full  Linear  +  SoftMax  during training and 
cross-validation. 
 The  inputSize  and  outputSize  are the same as for the  Linear  module. 
The number of noise samples to be drawn per example is  k . A value of 25 should work well. 
Increasing it will yield better results, while a smaller value will be more efficient to process.
The  unigrams  is a tensor of size  outputSize  that contains the frequencies or probability distribution over classes.
It is used to sample noise samples via a fast implementation of  torch.multinomial .
The  Z  is the normalization constant of the approximated SoftMax. 
The default is  math.exp(9)  as specified in Ref. A. 
 For inference, or measuring perplexity, the full  Linear  +  SoftMax  will need to 
be computed. The  NCEModule  can do this by switching on the following : 
 lua
ncem:evaluate()
ncem.normalized = true   
 Furthermore, to simulate  Linear  +  LogSoftMax  instead, one need only add the following to the above: 
 lua
ncem.logsoftmax = true   
 An example is provided via the rnn package. 
 
 NCECriterion 
 lua
ncec = nn.NCECriterion()   
 This criterion only works with an  NCEModule  on the output layer.
Together, they implement  noise-contrastive estimation . 
 
 Reinforce 
 Ref A.  Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning 
 Abstract class for modules that implement the REINFORCE algorithm (ref. A). 
 lua
module = nn.Reinforce([stochastic]) 
 The  reinforce(reward)  method is called by a special Reward Criterion (e.g.  VRClassReward ).
After which, when backward is called, the reward will be used to generate gradInputs. 
When  stochastic=true , the module is stochastic (i.e. samples from a distribution) 
during evaluation and training.
When  stochastic=false  (the default), the module is only stochastic during training. 
 The REINFORCE rule for a module can be summarized as follows :
 lua
            d ln(f(output,input))
gradInput = ---------------------  * reward
                  d input 
where the  reward  is what is provided by a Reward criterion like 
 VRClassReward  via the  reinforce  method.
The criterion will normally be responsible for the following formula :
 lua
reward = a*(R - b) 
where  a  is the alpha of the original paper, i.e. a reward scale,
 R  is the raw reward (usually 0 or 1), and  b  is the baseline reward, 
which is often taken to be the expected raw reward  R . 
 The  output  is usually sampled from a probability distribution  f() 
parameterized by the  input . 
See  ReinforceBernoulli  for a concrete derivation. 
 Also, as you can see, the gradOutput is ignored. So within a backpropagation graph,
the  Reinforce  modules will replace the backpropagated gradients ( gradOutput ) 
with their own obtained from the broadcasted  reward . 
 
 ReinforceBernoulli 
 Ref A.  Simple Statistical Gradient-Following Algorithms for
Connectionist Reinforcement Learning 
 lua
module = nn.ReinforceBernoulli([stochastic]) 
 A  Reinforce  subclass that implements the REINFORCE algorithm 
(ref. A p.230-236) for the Bernoulli probability distribution.
Inputs are bernoulli probabilities  p . 
During training, outputs are samples drawn from this distribution.
During evaluation, when  stochastic=false , outputs are the same as the inputs.
Uses the REINFORCE algorithm (ref. A p.230-236) which is 
implemented through the  reinforce  interface ( gradOutputs  are ignored). 
 Given the following variables :  
 
 f  : bernoulli probability mass function 
 x  : the sampled values (0 or 1) (i.e.  self.output ) 
 p  : probability of sampling a 1 
 
 the derivative of the log bernoulli w.r.t. probability  p  is :
 d ln(f(output,input))   d ln(f(x,p))    (x - p)
--------------------- = ------------ = ---------
      d input               d p         p(1 - p) 
 
 ReinforceNormal 
 Ref A.  Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning 
 lua
module = nn.ReinforceNormal(stdev, [stochastic]) 
 A  Reinforce  subclass that implements the REINFORCE algorithm 
(ref. A p.238-239) for a Normal (i.e. Gaussian) probability distribution.
Inputs are the means of the normal distribution.
The  stdev  argument specifies the standard deviation of the distribution. 
During training, outputs are samples drawn from this distribution.
During evaluation, when  stochastic=false , outputs are the same as the inputs, i.e. the means.
Uses the REINFORCE algorithm (ref. A p.238-239) which is 
implemented through the  reinforce  interface ( gradOutputs  are ignored). 
 Given the following variables :  
 
 f  : normal probability density function 
 x  : the sampled values (i.e.  self.output ) 
 u  : mean ( input ) 
 s  : standard deviation ( self.stdev ) 
 
 the derivative of log normal w.r.t. mean  u  is :
 d ln(f(x,u,s))   (x - u)
-------------- = -------
     d u           s^2 
 As an example, it is used to sample locations for the  RecurrentAttention  
module (see  this example ). 
 
 ReinforceGamma 
 Ref A.  Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning 
 lua
module = nn.ReinforceGamma(scale, [stochastic])   
 A  Reinforce  subclass that implements the REINFORCE algorithm 
(ref. A) for a  Gamma probability distribution  
parametrized by shape (k) and scale (theta) variables.
Inputs are the shapes of the gamma distribution.
During training, outputs are samples drawn from this distribution.
During evaluation, when  stochastic=false , outputs are equal to the mean, defined as the product of
shape and scale ie.  k*theta .
Uses the REINFORCE algorithm (ref. A) which is 
implemented through the  reinforce  interface ( gradOutputs  are ignored). 
 Given the following variables :  
 
 f  : gamma probability density function 
 g  : digamma function 
 x  : the sampled values (i.e.  self.output ) 
 k  : shape ( input ) 
 t  : scale 
 
 the derivative of log gamma w.r.t. shape  k  is :
 d ln(f(x,k,t))
-------------- = ln(x) - g(k) - ln(t)
      d k   
 
 ReinforceCategorical 
 Ref A.  Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning 
 lua
module = nn.ReinforceCategorical([stochastic]) 
 A  Reinforce  subclass that implements the REINFORCE algorithm 
(ref. A) for a Categorical (i.e. Multinomial with one sample) probability distribution.
Inputs are the categorical probabilities of the distribution :  p[1], p[2], ..., p[k] .
These are usually the output of a SoftMax. 
For  n  categories, both the  input  and  output  ares of size  batchSize x n .
During training, outputs are samples drawn from this distribution.
The outputs are returned in one-hot encoding i.e. 
the output for each example has exactly one category having a 1, while the remainder are zero.
During evaluation, when  stochastic=false , outputs are the same as the inputs, i.e. the probabilities  p .
Uses the REINFORCE algorithm (ref. A) which is 
implemented through the  reinforce  interface ( gradOutputs  are ignored). 
 Given the following variables :  
 
 f  : categorical probability mass function 
 x  : the sampled indices (one per sample) ( self.output  is the one-hot encoding of these indices) 
 p  : probability vector ( p[1], p[2], ..., p[k] ) ( input ) 
 
 the derivative of log categorical w.r.t. probability vector  p  is :
 d ln(f(x,p))     1/p[i]    if i = x  
------------ =   
    d p          0         otherwise   
 
 VRClassReward 
 Ref A.  Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning 
 This Reward criterion implements the REINFORCE algoritm (ref. A) for classification models.
Specifically, it is a Variance Reduces (VR) classification reinforcement leanring (reward-based) criterion. 
 lua
vcr = nn.VRClassReward(module [, scale, criterion])   
 While it conforms to the Criterion interface (which it inherits), 
it does not backpropagate gradients (except for the baseline  b ; see below).
Instead, a  reward  is broadcast to the  module  via the  reinforce  method. 
 The criterion implements the following formula :
 lua
reward = a*(R - b) 
where  a  is the alpha described in Ref. A, i.e. a reward  scale  (defaults to 1),
 R  is the raw reward (0 for incorrect and 1 for correct classification), 
and  b  is the baseline reward, which is often taken to be the expected raw reward  R . 
 The  target  of the criterion is a tensor of class indices.
The  input  to the criterion is a table  {y,b}  where  y  is the probability 
(or log-probability) of classes (usually the output of a SoftMax), 
and  b  is the baseline reward discussed above.  
 For each example, if  argmax(y)  is equal to the  target  class, the raw reward  R = 1 , otherwize  R = 0 . 
 As for  b , its  gradInputs  are obtained from the  criterion , which defaults to  MSECriterion .
The  criterion 's target is the commensurate raw reward  R .
Using  a*(R-b)  instead of  a*R  to obtain a  reward  is what makes this class variance reduced (VR).
By reducing the variance, the training can converge faster (Ref. A).
The predicted  b  can be nothing more than the expectation  E(R) . 
 Note : for RNNs with R = 1 for last step in sequence, encapsulate it
in  nn.ModuleCriterion(VRClassReward, nn.SelectTable(-1)) . 
 For an example, this criterion is used along with the  RecurrentAttention  
module to  train a recurrent model for visual attention . 
 
 BinaryClassReward 
 lua
bcr = nn.BinaryClassReward(module [, scale, criterion])   
 This module implements  VRClassReward  for binary classification problems.
So basically, the  input  is still a table of two tensors. 
The first input tensor is of size  batchsize  containing Bernoulli probabilities.
The second input tensor is the baseline prediction described in  VRClassReward .
The targets contain zeros and ones. 
 
 BinaryLogisticRegression 
 Ref A.  Learning to Segment Object Candidates 
This criterion implements the score criterion mentioned in (ref. A). 
 lua
criterion = nn.BinaryLogisticRegression()   
 BinaryLogisticRegression implements following cost function for binary classification. 
 ``` 
 log( 1 + exp( -y_k * score(x_k) ) ) 
 `` 
where y_k is binary target score(x_k) is the corresponding prediction. y_k has value {-1, +1} and score(x_k) has value in [-1, +1]`. 
 
 SpatialBinaryLogisticRegression 
 Ref A.  Learning to Segment Object Candidates 
 This criterion implements the spatial component of the criterion mentioned in  (ref. A). 
 lua
criterion = nn.SpatialBinaryLogisticRegression()   
 SpatialBinaryLogisticRegression implements following cost function for binary pixel classification.
 1
_______ sum_ij [ log( 1 + exp( -m_ij * f_ij ) ) ]
 2*w*h  
where  m_ij  is target binary image and  f_ij  is the corresponding prediction.  m_ij  has value  {-1, +1}  and  f_ij  has value in  [-1, +1] . hpo 
 Hyper-parameter optimization slides 
 
 Boston ML 
 NVIDIA GTC 
 AI Camp  ( pptx ) : 12 July 2016, United Nations HQ, New York, 15min, Overview of Torch and Language Models. 
 rnn-examples 
 Examples for the RNN library / framework DrMAD 
 To provide an efficient and easy-to-use hyperparameter tuning toolbox for Torch deep learning ecosystems. 
 It combines Bayesian optimization (BO) and automatic differentiation (AD). For the Bayesian optimization module,
we will extend on  hypero ; the automatic differentiation part is based on
DrMAD method, https://arxiv.org/abs/1601.00917. 
 It is the only tool that can tune thousands of continuous hyperparameters (e.g. L2 penalties for each neuron or
learning rates for each layer) with a reasonable time/computational budget -- reads: outside Google. 
 Current Status 
 Skechy code for tuning L2 penalties and learning rates on MNIST dataset with CUDA support. 
 TODO 
 
 API for tuning learning rates, weight decay and momentum.  
 Experiments on ImageNet 
 
 Dependencies 
 
 Twitter Torch Autograd : the next version will not depend on this.  
 
 How to run 
 
 drmad_mnist.lua  is for tuning L2 penalties on MNIST.  
 cuda_drmad_mnist.lua  is for tuning L2 penalties on MNIST with CUDA.  
 lr_drmad_mnist.lua  is for tuning learning rates and L2 penalties on MNIST.   
 
 Tricks 
 Rally with ( Net2Net ) 
 ImageNet dataset usually needs ~450,000 iterations. DrMAD may not approxiate this long trajectory well.  
 One approach would be to repeatedly initialize the weights using Net2Net, from small subsets to larget subsets
and finally to the full dataset. 
 Acceleration with Model Compression 
 We will add a regression loss at every layer, which is also used in  Deep Q-Networks for Accelerating the Training of Deep Neural Networks . However, the aim here is not to compress the model, so we do not decrease the number of parameters.  
 BO and AD 
 BO is a global optimization method (it can handle 20 hyperparameters at most), whereas AD can only find local solutions
(it can handle thousands of hyperparameters because it uses gradient information). We first use BO to get some initial
average hyperparameters. Then we use AD method to further search for diverse local hyperparameters. 
 Contact 
 If you have any problems or suggestions, please contact me: jie.fu A~_~T u.nus.edu~~cation~~ This package is a basic Reinforcement Learning package written in LUA for Torch. It implements some simple environments and learning policies (Policy Gradient and Deep Q Learning). It also can be easily used with the OpenAI Gym package by using lutorpy (example given in the opeanaigym directory). 
 Tutorials are provided in the tutorials directory 
 Dependencies 
 Lua: 
* Torch7
* nn, dpnn
* logroll, json, alewrap 
 For using openAI Gym:
* openai gym
* lutorpy 
 Installation 
 
 In the torch directory: luarocks make 
 Install lutorpy and open AI 
 lauch the python script (example.py) 
 
 WARNING : If you use an openAI Gym ATARI environment, a new sensor must be developped: it will be avaiable in the next few days (since openAI and alewrap do not store the ATARI images in the same format) 
 Author: Ludovic DENOYER Torch in Action 
 This repository contains the code for the Torch in Action book. 
 Chapter 1 : Meeting Torch
 *  facedetect : toy face detection dataset (directory with only four samples);
 *  train.lua : example face detection training script (listings 1.1, 1.2 and 1.3); 
 Chapter 2 : Preparing a dataset
 *  mnist : MNIST dataset in binary format as downloaded from  yann.lecun.com ;
 *  createdataset.lua : code for serializing the MNIST dataset into  .t7  files and generating samples (section 2.3);
 *  dataloader.lua : code for listing 2.1, 2.2, 2.3 and 2.5. Defines the  DataLoader  and  TensorLoader  classes);
 *  iteratedataset.lua : code for listing 2.5. This script tests the  dataloader.lua  file by iterating through it. Only works if  createdataset.lua  was executed before hand;
 *  getmnistsample.lua : script for generating MNIST samples consolidated as a single image (used to generate figure 2.1); 
 Chapter 3 : Training simple neural networks
 *  trainlogreg.lua : Training script for applying binary logistic regression on OR dataset. The model is trained using stochastic gradient descent (listing 3.1);
 *  logreg.log : log file created by running  th trainlogreg.lua > logreg.log ;
 *  trainlogreg-mnist.lua : Script for training a multinomial logistic regression model (saved as  logreg-mnist.t7 ) using SGD on the MNIST dataset. Training stops after 200 epochs where each epoch consists of 10000 samples divided into mini-batches of 32 random samples, or reaching an estimated empirical risk lower than 0.007, whichever comes first. The resulting model is evaluated on the entire training set of 50000 samples and saved to disk (listing 3.2);
 *  logreg-mnist.log : log file created by running  th trainlogreg-mnist.lua > logreg-mnist.log . The data can be used to generate a learning curve. Open the file from your favorite spreadsheet application (Microsoft Excel, LibreOffice Calc, etc.) and specify that values are separated by semicolons;
 *  backward.lua : demonstrates gradient descent through a criterion. Using the input as a parameter, the loss is minized by tacking a step in opposite direction of gradient (section 8.1.3); 
 Chapter 4 : Generalizing deep neural networks
 *  tanh.xlsx : plot of the hyperbolic tangent activation function (figure 4.2);
 *  trainmlp-xor.lua : script for training an MLP with one hidden layer composed of 2 units on the XOR dataset. Used to generate  xor-mlp.xlsx  and figure 4.3;
 *  xor-mlp.xlsx : diagram outlining the boundaries of an MLP trained on the XOR dataset (figure 4.3);
 *  overfitting.xlsx : contains learing curve and model overfitting example (figure 4.4 and 4.5);
 *  trainmlp-mnist.lua : upgrades  trainlogreg-mnist.lua  by moving the definition of hyper-parameters to the cmd-line (listing 4.1 and 4.2).
 *  trainmlp-mnist-crossvalidate.lua : upgrades  trainmlp-mnist.lua  with cross-validation (listing 4.3);
 *  trainmlp-mnist-earlystop.lua : upgrades  trainmlp-mnist-crossvalidate.lua  with early-stopping (listing 4.4);
 *  trainmlp-mnist-weightdecay.lua : upgrades  trainmlp-mnist-earlystop.lua  with weight decay regularization (listing 4.5);
 *  trainmlp-mnist-hyperopt.lua : upgrades  trainmlp-mnist-weightdecay.lua  to facilitate hyper-parameter optimization (listing 4.6);
 *  hyperopt-mnist.xlsx : spreadsheet used to hyper-optimize the  trainmlp-mnist-hyperopt.lua  script (figure 4.7 and 4.8);
 *  relu.xlsx : plot of the rectified linear unit (figure 4.9). rnn: recurrent neural networks 
 This is a Recurrent Neural Network library that extends Torch's nn. 
You can use it to build RNNs, LSTMs, GRUs, BRNNs, BLSTMs, and so forth and so on.
This library includes documentation for the following objects: 
 Modules that consider successive calls to  forward  as different time-steps in a sequence :
 *  AbstractRecurrent  : an abstract class inherited by Recurrent and LSTM;
 *  Recurrent  : a generalized recurrent neural network container;
 *  LSTM  : a vanilla Long-Short Term Memory module;
  *  FastLSTM  : a faster  LSTM  with optional support for batch normalization;
 *  GRU  : Gated Recurrent Units module;
 *  Recursor  : decorates a module to make it conform to the  AbstractRecurrent  interface;
 *  Recurrence  : decorates a module that outputs  output(t)  given  {input(t), output(t-1)} ;
 *  NormStabilizer  : implements  norm-stabilization  criterion (add this module between RNNs); 
 Modules that  forward  entire sequences through a decorated  AbstractRecurrent  instance :
 *  AbstractSequencer  : an abstract class inherited by Sequencer, Repeater, RecurrentAttention, etc.;
 *  Sequencer  : applies an encapsulated module to all elements in an input sequence  (Tensor or Table);
 *  SeqLSTM  : a very fast version of  nn.Sequencer(nn.FastLSTM)  where the  input  and  output  are tensors;
  *  SeqLSTMP  :  SeqLSTM  with a projection layer;
 *  SeqGRU  : a very fast version of  nn.Sequencer(nn.GRU)  where the  input  and  output  are tensors;
 *  SeqBRNN  : Bidirectional RNN based on SeqLSTM;
 *  BiSequencer  : used for implementing Bidirectional RNNs and LSTMs;
 *  BiSequencerLM  : used for implementing Bidirectional RNNs and LSTMs for language models;
 *  Repeater  : repeatedly applies the same input to an AbstractRecurrent instance;
 *  RecurrentAttention  : a generalized attention model for  REINFORCE modules ; 
 Miscellaneous modules and criterions :
 *  MaskZero  : zeroes the  output  and  gradOutput  rows of the decorated module for commensurate  input  rows which are tensors of zeros;
 *  TrimZero  : same behavior as  MaskZero , but more efficient when  input  contains lots zero-masked rows;
 *  LookupTableMaskZero  : extends  nn.LookupTable  to support zero indexes for padding. Zero indexes are forwarded as tensors of zeros;
 *  MaskZeroCriterion  : zeros the  gradInput  and  err  rows of the decorated criterion for commensurate  input  rows which are tensors of zeros;
 *  SeqReverseSequence  : reverses an input sequence on a specific dimension; 
 Criterions used for handling sequential inputs and targets :
 *  SequencerCriterion  : sequentially applies the same criterion to a sequence of inputs and targets (Tensor or Table).
 *  RepeaterCriterion  : repeatedly applies the same criterion with the same target on a sequence. 
 
 Examples 
 The following are example training scripts using this package : 
 
 RNN/LSTM/GRU  for Penn Tree Bank dataset; 
 Noise Contrastive Estimate  for training multi-layer  SeqLSTM  language models on the  Google Billion Words dataset . The example uses  MaskZero  to train independent variable length sequences using the  NCEModule  and  NCECriterion . This script is our fastest yet boasting speeds of 20,000 words/second (on NVIDIA Titan X) with a 2-layer LSTM having 250 hidden units, a batchsize of 128 and sequence length of a 100. Note that you will need to have  Torch installed with Lua instead of LuaJIT ; 
 Recurrent Model for Visual Attention  for the MNIST dataset; 
 Encoder-Decoder LSTM  shows you how to couple encoder and decoder  LSTMs  for sequence-to-sequence networks; 
 Simple Recurrent Network  shows a simple example for building and training a simple recurrent neural network; 
 Simple Sequencer Network  is a version of the above script that uses the Sequencer to decorate the  rnn  instead; 
 Sequence to One  demonstrates how to do many to one sequence learning as is the case for sentiment analysis; 
 Multivariate Time Series  demonstrates how train a simple RNN to do multi-variate time-series predication. 
 
 External Resources 
 
 rnn-benchmarks  : benchmarks comparing Torch (using this library), Theano and TensorFlow. 
 Harvard Jupyter Notebook Tutorial  : an in-depth tutorial for how to use the Element-Research rnn package by Harvard University; 
 dpnn  : this is a dependency of the  rnn  package. It contains useful nn extensions, modules and criterions; 
 dataload  : a collection of torch dataset loaders; 
 RNN/LSTM/BRNN/BLSTM training script   for Penn Tree Bank or Google Billion Words datasets; 
 A brief (1 hours) overview of Torch7, which includes some details about the  rnn  packages (at the end), is available via this  NVIDIA GTC Webinar video . In any case, this presentation gives a nice overview of Logistic Regression, Multi-Layer Perceptrons, Convolutional Neural Networks and Recurrent Neural Networks using Torch7; 
 Sequence to Sequence mapping using encoder-decoder RNNs  : a complete training example using synthetic data. 
 ConvLSTM  is a repository for training a  Spatio-temporal video autoencoder with differentiable memory . 
 An  time series example  for univariate timeseries prediction. 
 
 Citation 
 If you use  rnn  in your work, we'd really appreciate it if you could cite the following paper: 
 Léonard, Nicholas, Sagar Waghmare, Yang Wang, and Jin-Hwa Kim.  rnn: Recurrent Library for Torch.  arXiv preprint arXiv:1511.07889 (2015). 
 Any significant contributor to the library will also get added as an author to the paper.
A  significant contributor  
is anyone who added at least 300 lines of code to the library. 
 Troubleshooting 
 Most issues can be resolved by updating the various dependencies:
 bash
luarocks install torch
luarocks install nn
luarocks install dpnn
luarocks install torchx 
 If you are using CUDA :
 bash
luarocks install cutorch
luarocks install cunn
luarocks install cunnx 
 And don't forget to update this package :
 bash
luarocks install rnn 
 If that doesn't fix it, open and issue on github. 
 
 AbstractRecurrent 
 An abstract class inherited by  Recurrent ,  LSTM  and  GRU .
The constructor takes a single argument :
 lua
rnn = nn.AbstractRecurrent([rho]) 
Argument  rho  is the maximum number of steps to backpropagate through time (BPTT).
Sub-classes can set this to a large number like 99999 (the default) if they want to backpropagate through 
the entire sequence whatever its length. Setting lower values of rho are 
useful when long sequences are forward propagated, but we only whish to 
backpropagate through the last  rho  steps, which means that the remainder 
of the sequence doesn't need to be stored (so no additional cost).  
 [recurrentModule] getStepModule(step) 
 Returns a module for time-step  step . This is used internally by sub-classes 
to obtain copies of the internal  recurrentModule . These copies share 
 parameters  and  gradParameters  but each have their own  output ,  gradInput  
and any other intermediate states.  
 setOutputStep(step) 
 This is a method reserved for internal use by  Recursor  
when doing backward propagation. It sets the object's  output  attribute
to point to the output at time-step  step . 
This method was introduced to solve a very annoying bug. 
 
 maskZero(nInputDim) 
 Decorates the internal  recurrentModule  with  MaskZero . 
The  output  Tensor (or table thereof) of the  recurrentModule 
will have each row (i.e. samples) zeroed when the commensurate row of the  input  
is a tensor of zeros.  
 The  nInputDim  argument must specify the number of non-batch dims 
in the first Tensor of the  input . In the case of an  input  table,
the first Tensor is the first one encountered when doing a depth-first search. 
 Calling this method makes it possible to pad sequences with different lengths in the same batch with zero vectors. 
 When a sample time-step is masked (i.e.  input  is a row of zeros), then 
the hidden state is effectively reset (i.e. forgotten) for the next non-mask time-step.
In other words, it is possible seperate unrelated sequences with a masked element. 
 trimZero(nInputDim) 
 Decorates the internal  recurrentModule  with  TrimZero .  
 [output] updateOutput(input) 
 Forward propagates the input for the current step. The outputs or intermediate 
states of the previous steps are used recurrently. This is transparent to the 
caller as the previous outputs and intermediate states are memorized. This 
method also increments the  step  attribute by 1. 
 
 updateGradInput(input, gradOutput) 
 Like  backward , this method should be called in the reverse order of 
 forward  calls used to propagate a sequence. So for example : 
 ```lua
rnn = nn.LSTM(10, 10) -- AbstractRecurrent instance
local outputs = {}
for i=1,nStep do -- forward propagate sequence
   outputs[i] = rnn:forward(inputs[i])
end 
 for i=nStep,1,-1 do -- backward propagate sequence in reverse order
   gradInputs[i] = rnn:backward(inputs[i], gradOutputs[i])
end 
 rnn:forget()
```  
 The reverse order implements backpropagation through time (BPTT). 
 accGradParameters(input, gradOutput, scale) 
 Like  updateGradInput , but for accumulating gradients w.r.t. parameters. 
 recycle(offset) 
 This method goes hand in hand with  forget . It is useful when the current
time-step is greater than  rho , at which point it starts recycling 
the oldest  recurrentModule   sharedClones , 
such that they can be reused for storing the next step. This  offset  
is used for modules like  nn.Recurrent  that use a different module 
for the first step. Default offset is 0. 
 
 forget(offset) 
 This method brings back all states to the start of the sequence buffers, 
i.e. it forgets the current sequence. It also resets the  step  attribute to 1.
It is highly recommended to call  forget  after each parameter update. 
Otherwise, the previous state will be used to activate the next, which 
will often lead to instability. This is caused by the previous state being
the result of now changed parameters. It is also good practice to call 
 forget  at the start of each new sequence. 
 
 maxBPTTstep(rho) 
 This method sets the maximum number of time-steps for which to perform 
backpropagation through time (BPTT). So say you set this to  rho = 3  time-steps,
feed-forward for 4 steps, and then backpropgate, only the last 3 steps will be 
used for the backpropagation. If your AbstractRecurrent instance is wrapped 
by a  Sequencer , this will be handled auto-magically by the Sequencer.
Otherwise, setting this value to a large value (i.e. 9999999), is good for most, if not all, cases. 
 
 backwardOnline() 
 This method was deprecated Jan 6, 2016. 
Since then, by default,  AbstractRecurrent  instances use the 
backwardOnline behaviour. 
See  updateGradInput  for details. 
 training() 
 In training mode, the network remembers all previous  rho  (number of time-steps)
states. This is necessary for BPTT.  
 evaluate() 
 During evaluation, since their is no need to perform BPTT at a later time, 
only the previous step is remembered. This is very efficient memory-wise, 
such that evaluation can be performed using potentially infinite-length 
sequence. 
 
 Recurrent 
 References :
 * A.  Sutsekever Thesis Sec. 2.5 and 2.8 
 * B.  Mikolov Thesis Sec. 3.2 and 3.3 
 * C.  RNN and Backpropagation Guide 
 A  composite Module  for implementing Recurrent Neural Networks (RNN), excluding the output layer.  
 The  nn.Recurrent(start, input, feedback, [transfer, rho, merge])  constructor takes 6 arguments:
 *  start  : the size of the output (excluding the batch dimension), or a Module that will be inserted between the  input  Module and  transfer  module during the first step of the propagation. When  start  is a size (a number or  torch.LongTensor ), then this  start  Module will be initialized as  nn.Add(start)  (see Ref. A).
 *  input  : a Module that processes input Tensors (or Tables). Output must be of same size as  start  (or its output in the case of a  start  Module), and same size as the output of the  feedback  Module.
 *  feedback  : a Module that feedbacks the previous output Tensor (or Tables) up to the  merge  module.
 *  merge  : a  table Module  that merges the outputs of the  input  and  feedback  Module before being forwarded through the  transfer  Module.
 *  transfer  : a non-linear Module used to process the output of the  merge  module, or in the case of the first step, the output of the  start  Module.
 *  rho  : the maximum amount of backpropagation steps to take back in time. Limits the number of previous steps kept in memory. Due to the vanishing gradients effect, references A and B recommend  rho = 5  (or lower). Defaults to 99999. 
 An RNN is used to process a sequence of inputs. 
Each step in the sequence should be propagated by its own  forward  (and  backward ), 
one  input  (and  gradOutput ) at a time. 
Each call to  forward  keeps a log of the intermediate states (the  input  and many  Module.outputs ) 
and increments the  step  attribute by 1. 
Method  backward  must be called in reverse order of the sequence of calls to  forward  in 
order to backpropgate through time (BPTT). This reverse order is necessary 
to return a  gradInput  for each call to  forward .  
 The  step  attribute is only reset to 1 when a call to the  forget  method is made. 
In which case, the Module is ready to process the next sequence (or batch thereof).
Note that the longer the sequence, the more memory that will be required to store all the 
 output  and  gradInput  states (one for each time step).  
 To use this module with batches, we suggest using different 
sequences of the same size within a batch and calling  updateParameters  
every  rho  steps and  forget  at the end of the sequence.  
 Note that calling the  evaluate  method turns off long-term memory; 
the RNN will only remember the previous output. This allows the RNN 
to handle long sequences without allocating any additional memory. 
 For a simple concise example of how to make use of this module, please consult the 
 simple-recurrent-network.lua 
training script. 
 
 Decorate it with a Sequencer 
 Note that any  AbstractRecurrent  instance can be decorated with a  Sequencer  
such that an entire sequence (a table) can be presented with a single  forward/backward  call.
This is actually the recommended approach as it allows RNNs to be stacked and makes the 
rnn conform to the Module interface, i.e. each call to  forward  can be 
followed by its own immediate call to  backward  as each  input  to the 
model is an entire sequence, i.e. a table of tensors where each tensor represents
a time-step. 
 lua
seq = nn.Sequencer(module) 
 The  simple-sequencer-network.lua  training script
is equivalent to the above mentionned  simple-recurrent-network.lua 
script, except that it decorates the  rnn  with a  Sequencer  which takes 
a table of  inputs  and  gradOutputs  (the sequence for that batch).
This lets the  Sequencer  handle the looping over the sequence. 
 You should only think about using the  AbstractRecurrent  modules without 
a  Sequencer  if you intend to use it for real-time prediction. 
Actually, you can even use an  AbstractRecurrent  instance decorated by a  Sequencer 
for real time prediction by calling  Sequencer:remember()  and presenting each 
time-step  input  as  {input} . 
 Other decorators can be used such as the  Repeater  or  RecurrentAttention .
The  Sequencer  is only the most common one.  
 
 LSTM 
 References :
 * A.  Speech Recognition with Deep Recurrent Neural Networks 
 * B.  Long-Short Term Memory 
 * C.  LSTM: A Search Space Odyssey 
 * D.  nngraph LSTM implementation on github 
 This is an implementation of a vanilla Long-Short Term Memory module. 
We used Ref. A's LSTM as a blueprint for this module as it was the most concise.
Yet it is also the vanilla LSTM described in Ref. C.  
 The  nn.LSTM(inputSize, outputSize, [rho])  constructor takes 3 arguments:
 *  inputSize  : a number specifying the size of the input;
 *  outputSize  : a number specifying the size of the output;
 *  rho  : the maximum amount of backpropagation steps to take back in time. Limits the number of previous steps kept in memory. Defaults to 9999. 
   
 The actual implementation corresponds to the following algorithm:
 lua
i[t] = σ(W[x->i]x[t] + W[h->i]h[t−1] + W[c->i]c[t−1] + b[1->i])      (1)
f[t] = σ(W[x->f]x[t] + W[h->f]h[t−1] + W[c->f]c[t−1] + b[1->f])      (2)
z[t] = tanh(W[x->c]x[t] + W[h->c]h[t−1] + b[1->c])                   (3)
c[t] = f[t]c[t−1] + i[t]z[t]                                         (4)
o[t] = σ(W[x->o]x[t] + W[h->o]h[t−1] + W[c->o]c[t] + b[1->o])        (5)
h[t] = o[t]tanh(c[t])                                                (6) 
where  W[s->q]  is the weight matrix from  s  to  q ,  t  indexes the time-step,
 b[1->q]  are the biases leading into  q ,  σ()  is  Sigmoid ,  x[t]  is the input,
 i[t]  is the input gate (eq. 1),  f[t]  is the forget gate (eq. 2), 
 z[t]  is the input to the cell (which we call the hidden) (eq. 3), 
 c[t]  is the cell (eq. 4),  o[t]  is the output gate (eq. 5), 
and  h[t]  is the output of this module (eq. 6). Also note that the 
weight matrices from cell to gate vectors are diagonal  W[c->s] , where  s  
is  i , f , or  o . 
 As you can see, unlike  Recurrent , this 
implementation isn't generic enough that it can take arbitrary component Module
definitions at construction. However, the LSTM module can easily be adapted 
through inheritance by overriding the different factory methods :
  *  buildGate  : builds generic gate that is used to implement the input, forget and output gates;
  *  buildInputGate  : builds the input gate (eq. 1). Currently calls  buildGate ;
  *  buildForgetGate  : builds the forget gate (eq. 2). Currently calls  buildGate ;
  *  buildHidden  : builds the hidden (eq. 3);
  *  buildCell  : builds the cell (eq. 4);
  *  buildOutputGate  : builds the output gate (eq. 5). Currently calls  buildGate ;
  *  buildModel  : builds the actual LSTM model which is used internally (eq. 6). 
 Note that we recommend decorating the  LSTM  with a  Sequencer  
(refer to  this  for details). 
 
 FastLSTM 
 A faster version of the  LSTM . 
Basically, the input, forget and output gates, as well as the hidden state are computed at one fellswoop. 
 Note that  FastLSTM  does not use peephole connections between cell and gates. The algorithm from  LSTM  changes as follows:
 lua
i[t] = σ(W[x->i]x[t] + W[h->i]h[t−1] + b[1->i])                      (1)
f[t] = σ(W[x->f]x[t] + W[h->f]h[t−1] + b[1->f])                      (2)
z[t] = tanh(W[x->c]x[t] + W[h->c]h[t−1] + b[1->c])                   (3)
c[t] = f[t]c[t−1] + i[t]z[t]                                         (4)
o[t] = σ(W[x->o]x[t] + W[h->o]h[t−1] + b[1->o])                      (5)
h[t] = o[t]tanh(c[t])                                                (6) 
i.e. omitting the summands  W[c->i]c[t−1]  (eq. 1),  W[c->f]c[t−1]  (eq. 2), and  W[c->o]c[t]  (eq. 5). 
 usenngraph 
 This is a static attribute of the  FastLSTM  class. The default value is  false .
Setting  usenngraph = true  will force all new instantiated instances of  FastLSTM  
to use  nngraph 's  nn.gModule  to build the internal  recurrentModule  which is 
cloned for each time-step. 
 
 Recurrent Batch Normalization 
 This extends the  FastLSTM  class to enable faster convergence during training by zero-centering the input-to-hidden and hidden-to-hidden transformations. 
It reduces the  internal covariate shift  between time steps. It is an implementation of Cooijmans et. al.'s  Recurrent Batch Normalization . The hidden-to-hidden transition of each LSTM cell is normalized according to 
 lua
i[t] = σ(BN(W[x->i]x[t]) + BN(W[h->i]h[t−1]) + b[1->i])                      (1)
f[t] = σ(BN(W[x->f]x[t]) + BN(W[h->f]h[t−1]) + b[1->f])                      (2)
z[t] = tanh(BN(W[x->c]x[t]) + BN(W[h->c]h[t−1]) + b[1->c])                   (3)
c[t] = f[t]c[t−1] + i[t]z[t]                                                 (4)
o[t] = σ(BN(W[x->o]x[t]) + BN(W[h->o]h[t−1]) + b[1->o])                      (5)
h[t] = o[t]tanh(c[t])                                                        (6)  
where the batch normalizing transform is:                                  
 lua
  BN(h; gamma, beta) = beta + gamma *      hd - E(hd)
                                      ------------------
                                       sqrt(E(σ(hd) + eps)) 
where  hd  is a vector of (pre)activations to be normalized,  gamma , and  beta  are model parameters that determine the mean and standard deviation of the normalized activation.  eps  is a regularization hyperparameter to keep the division numerically stable and  E(hd)  and  E(σ(hd))  are the estimates of the mean and variance in the mini-batch respectively. The authors recommend initializing  gamma  to a small value and found 0.1 to be the value that did not cause vanishing gradients.  beta , the shift parameter, is  null  by default. 
 To turn on batch normalization during training, do:
 lua
nn.FastLSTM.bn = true
lstm = nn.FastLSTM(inputsize, outputsize, [rho, eps, momentum, affine]   
 where  momentum  is same as  gamma  in the equation above (defaults to 0.1),  eps  is defined above and  affine  is a boolean whose state determines if the learnable affine transform is turned off ( false ) or on ( true , the default). 
 
 GRU 
 References :
 * A.  Learning Phrase Representations Using RNN Encoder-Decoder For Statistical Machine Translation. 
 * B.  Implementing a GRU/LSTM RNN with Python and Theano 
 * C.  An Empirical Exploration of Recurrent Network Architectures 
 * D.  Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling 
 * E.  RnnDrop: A Novel Dropout for RNNs in ASR 
 * F.  A Theoretically Grounded Application of Dropout in Recurrent Neural Networks 
 This is an implementation of Gated Recurrent Units module.  
 The  nn.GRU(inputSize, outputSize [,rho [,p [, mono]]])  constructor takes 3 arguments likewise  nn.LSTM  or 4 arguments for dropout:
 *  inputSize  : a number specifying the size of the input;
 *  outputSize  : a number specifying the size of the output;
 *  rho  : the maximum amount of backpropagation steps to take back in time. Limits the number of previous steps kept in memory. Defaults to 9999;
 *  p  : dropout probability for inner connections of GRUs.
 *  mono  : Monotonic sample for dropouts inside GRUs. Only needed in a  TrimZero  +  BGRU (p>0) situation. 
   
 The actual implementation corresponds to the following algorithm:
 lua
z[t] = σ(W[x->z]x[t] + W[s->z]s[t−1] + b[1->z])            (1)
r[t] = σ(W[x->r]x[t] + W[s->r]s[t−1] + b[1->r])            (2)
h[t] = tanh(W[x->h]x[t] + W[hr->c](s[t−1]r[t]) + b[1->h])  (3)
s[t] = (1-z[t])h[t] + z[t]s[t-1]                           (4) 
where  W[s->q]  is the weight matrix from  s  to  q ,  t  indexes the time-step,  b[1->q]  are the biases leading into  q ,  σ()  is  Sigmoid ,  x[t]  is the input and  s[t]  is the output of the module (eq. 4). Note that unlike the  LSTM , the GRU has no cells. 
 The GRU was benchmark on  PennTreeBank  dataset using  recurrent-language-model.lua  script. 
It slightly outperfomed  FastLSTM , however, since LSTMs have more parameters than GRUs, 
the dataset larger than  PennTreeBank  might change the performance result. 
Don't be too hasty to judge on which one is the better of the two (see Ref. C and D). 
 Memory   examples/s
    FastLSTM      176M        16.5K 
    GRU            92M        15.8K 
 Memory  is measured by the size of  dp.Experiment  save file.  examples/s  is measured by the training speed at 1 epoch, so, it may have a disk IO bias. 
 
 RNN dropout (see Ref. E and F) was benchmark on  PennTreeBank  dataset using  recurrent-language-model.lua  script, too. The details can be found in the script. In the benchmark,  GRU  utilizes a dropout after  LookupTable , while  BGRU , stands for Bayesian GRUs, uses dropouts on inner connections (naming as Ref. F), but not after  LookupTable . 
 As Yarin Gal (Ref. F) mentioned, it is recommended that one may use  p = 0.25  for the first attempt. 
 
 
 Recursor 
 This module decorates a  module  to be used within an  AbstractSequencer  instance.
It does this by making the decorated module conform to the  AbstractRecurrent  interface,
which like the  LSTM  and  Recurrent  classes, this class inherits.  
 lua
rec = nn.Recursor(module[, rho]) 
 For each successive call to  updateOutput  (i.e.  forward ), this 
decorator will create a  stepClone()  of the decorated  module . 
So for each time-step, it clones the  module . Both the clone and 
original share parameters and gradients w.r.t. parameters. However, for 
modules that already conform to the  AbstractRecurrent  interface, 
the clone and original module are one and the same (i.e. no clone). 
 Examples : 
 Let's assume I want to stack two LSTMs. I could use two sequencers : 
 lua
lstm = nn.Sequential()
   :add(nn.Sequencer(nn.LSTM(100,100)))
   :add(nn.Sequencer(nn.LSTM(100,100))) 
 Using a  Recursor , I make the same model with a single  Sequencer  : 
 lua
lstm = nn.Sequencer(
   nn.Recursor(
      nn.Sequential()
         :add(nn.LSTM(100,100))
         :add(nn.LSTM(100,100))
      )
   ) 
 Actually, the  Sequencer  will wrap any non- AbstractRecurrent  module automatically, 
so I could simplify this further to : 
 lua
lstm = nn.Sequencer(
   nn.Sequential()
      :add(nn.LSTM(100,100))
      :add(nn.LSTM(100,100))
   ) 
 I can also add a  Linear  between the two  LSTM s. In this case,
a  Linear  will be cloned (and have its parameters shared) for each time-step,
while the  LSTM s will do whatever cloning internally : 
 lua
lstm = nn.Sequencer(
   nn.Sequential()
      :add(nn.LSTM(100,100))
      :add(nn.Linear(100,100))
      :add(nn.LSTM(100,100))
   )   
 AbstractRecurrent  instances like  Recursor ,  Recurrent  and  LSTM  are 
expcted to manage time-steps internally. Non- AbstractRecurrent  instances
can be wrapped by a  Recursor  to have the same behavior.  
 Every call to  forward  on an  AbstractRecurrent  instance like  Recursor  
will increment the  self.step  attribute by 1, using a shared parameter clone
for each successive time-step (for a maximum of  rho  time-steps, which defaults to 9999999).
In this way,  backward  can be called in reverse order of the  forward  calls 
to perform backpropagation through time (BPTT). Which is exactly what 
 AbstractSequencer  instances do internally.
The  backward  call, which is actually divided into calls to  updateGradInput  and 
 accGradParameters , decrements by 1 the  self.udpateGradInputStep  and  self.accGradParametersStep 
respectively, starting at  self.step .
Successive calls to  backward  will decrement these counters and use them to 
backpropagate through the appropriate internall step-wise shared-parameter clones. 
 Anyway, in most cases, you will not have to deal with the  Recursor  object directly as
 AbstractSequencer  instances automatically decorate non- AbstractRecurrent  instances
with a  Recursor  in their constructors. 
 For a concrete example of its use, please consult the  simple-recurrent-network.lua 
training script for an example of its use. 
 
 Recurrence 
 A extremely general container for implementing pretty much any type of recurrence. 
 lua
rnn = nn.Recurrence(recurrentModule, outputSize, nInputDim, [rho]) 
 Unlike  Recurrent , this module doesn't manage a separate 
modules like  inputModule ,  startModule ,  mergeModule  and the like.
Instead, it only manages a single  recurrentModule , which should 
output a Tensor or table :  output(t)  
given an input table :  {input(t), output(t-1)} .
Using a mix of  Recursor  (say, via  Sequencer ) with  Recurrence , one can implement 
pretty much any type of recurrent neural network, including LSTMs and RNNs. 
 For the first step, the  Recurrence  forwards a Tensor (or table thereof)
of zeros through the recurrent layer (like LSTM, unlike Recurrent).
So it needs to know the  outputSize , which is either a number or 
 torch.LongStorage , or table thereof. The batch dimension should be 
excluded from the  outputSize . Instead, the size of the batch dimension 
(i.e. number of samples) will be extrapolated from the  input  using 
the  nInputDim  argument. For example, say that our input is a Tensor of size 
 4 x 3  where  4  is the number of samples, then  nInputDim  should be  1 .
As another example, if our input is a table of table [...] of tensors 
where the first tensor (depth first) is the same as in the previous example,
then our  nInputDim  is also  1 . 
 As an example, let's use  Sequencer  and  Recurrence  
to build a Simple RNN for language modeling : 
 ```lua
rho = 5
hiddenSize = 10
outputSize = 5 -- num classes
nIndex = 10000 
 -- recurrent module
rm = nn.Sequential()
   :add(nn.ParallelTable()
      :add(nn.LookupTable(nIndex, hiddenSize))
      :add(nn.Linear(hiddenSize, hiddenSize)))
   :add(nn.CAddTable())
   :add(nn.Sigmoid()) 
 rnn = nn.Sequencer(
   nn.Sequential()
      :add(nn.Recurrence(rm, hiddenSize, 1))
      :add(nn.Linear(hiddenSize, outputSize))
      :add(nn.LogSoftMax())
)
``` 
 Note : We could very well reimplement the  LSTM  module using the
newer  Recursor  and  Recurrent  modules, but that would mean 
breaking backwards compatibility for existing models saved on disk. 
 
 NormStabilizer 
 Ref. A :  Regularizing RNNs by Stabilizing Activations 
 This module implements the  norm-stabilization  criterion: 
 lua
ns = nn.NormStabilizer([beta])   
 This module regularizes the hidden states of RNNs by minimizing the difference between the
L2-norms of consecutive steps. The cost function is defined as :
 loss = beta * 1/T sum_t( ||h[t]|| - ||h[t-1]|| )^2  
where  T  is the number of time-steps. Note that we do not divide the gradient by  T 
such that the chosen  beta  can scale to different sequence sizes without being changed. 
 The sole argument  beta  is defined in ref. A. Since we don't divide the gradients by
the number of time-steps, the default value of  beta=1  should be valid for most cases.  
 This module should be added between RNNs (or LSTMs or GRUs) to provide better regularization of the hidden states. 
For example :
 lua
local stepmodule = nn.Sequential()
   :add(nn.FastLSTM(10,10))
   :add(nn.NormStabilizer())
   :add(nn.FastLSTM(10,10))
   :add(nn.NormStabilizer())
local rnn = nn.Sequencer(stepmodule)   
 To use it with  SeqLSTM  you can do something like this :
 lua
local rnn = nn.Sequential()
   :add(nn.SeqLSTM(10,10))
   :add(nn.Sequencer(nn.NormStabilizer()))
   :add(nn.SeqLSTM(10,10))
   :add(nn.Sequencer(nn.NormStabilizer()))   
 
 AbstractSequencer 
 This abstract class implements a light interface shared by 
subclasses like :  Sequencer ,  Repeater ,  RecurrentAttention ,  BiSequencer  and so on. 
 
 Sequencer 
 The  nn.Sequencer(module)  constructor takes a single argument,  module , which is the module 
to be applied from left to right, on each element of the input sequence. 
 lua
seq = nn.Sequencer(module) 
 This Module is a kind of  decorator  
used to abstract away the intricacies of  AbstractRecurrent  modules. While an  AbstractRecurrent  instance 
requires that a sequence to be presented one input at a time, each with its own call to  forward  (and  backward ),
the  Sequencer  forwards an  input  sequence (a table) into an  output  sequence (a table of the same length).
It also takes care of calling  forget  on AbstractRecurrent instances. 
 Input/Output Format 
 The  Sequencer  requires inputs and outputs to be of shape  seqlen x batchsize x featsize  : 
 
 seqlen  is the number of time-steps that will be fed into the  Sequencer . 
 batchsize  is the number of examples in the batch. Each example is its own independent sequence. 
 featsize  is the size of the remaining non-batch dimensions. So this could be  1  for language models, or  c x h x w  for convolutional models, etc. 
 
 
 Above is an example input sequence for a character level language model.
It has  seqlen  is 5 which means that it contains sequences of 5 time-steps. 
The openning  {  and closing  }  illustrate that the time-steps are elements of a Lua table, although 
it also accepts full Tensors of shape  seqlen x batchsize x featsize . 
The  batchsize  is 2 as their are two independent sequences :  { H, E, L, L, O }  and  { F, U, Z, Z, Y, } .
The  featsize  is 1 as their is only one feature dimension per character and each such character is of size 1.
So the input in this case is a table of  seqlen  time-steps where each time-step is represented by a  batchsize x featsize  Tensor. 
 
 Above is another example of a sequence (input or output). 
It has a  seqlen  of 4 time-steps. 
The  batchsize  is again 2 which means there are two sequences.
The  featsize  is 3 as each time-step of each sequence has 3 variables.
So each time-step (element of the table) is represented again as a tensor
of size  batchsize x featsize . 
Note that while in both examples the  featsize  encodes one dimension, 
it could encode more.  
 Example 
 For example,  rnn  : an instance of nn.AbstractRecurrent, can forward an  input  sequence one forward at a time:
 lua
input = {torch.randn(3,4), torch.randn(3,4), torch.randn(3,4)}
rnn:forward(input[1])
rnn:forward(input[2])
rnn:forward(input[3])   
 Equivalently, we can use a Sequencer to forward the entire  input  sequence at once: 
 lua
seq = nn.Sequencer(rnn)
seq:forward(input)   
 We can also forward Tensors instead of Tables : 
 lua
-- seqlen x batchsize x featsize
input = torch.randn(3,3,4)
seq:forward(input)   
 Details 
 The  Sequencer  can also take non-recurrent Modules (i.e. non-AbstractRecurrent instances) and apply it to each 
input to produce an output table of the same length. 
This is especially useful for processing variable length sequences (tables). 
 Internally, the  Sequencer  expects the decorated  module  to be an 
 AbstractRecurrent  instance. When this is not the case, the  module  
is automatically decorated with a  Recursor  module, which makes it 
conform to the  AbstractRecurrent  interface.  
 Note : this is due a recent update (27 Oct 2015), as before this 
 AbstractRecurrent  and and non- AbstractRecurrent  instances needed to 
be decorated by their own  Sequencer . The recent update, which introduced the 
 Recursor  decorator, allows a single  Sequencer  to wrap any type of module, 
 AbstractRecurrent , non- AbstractRecurrent  or a composite structure of both types.
Nevertheless, existing code shouldn't be affected by the change. 
 For a concise example of its use, please consult the  simple-sequencer-network.lua 
training script. 
 
 remember([mode]) 
 When  mode='neither'  (the default behavior of the class), the Sequencer will additionally call  forget  before each call to  forward . 
When  mode='both'  (the default when calling this function), the Sequencer will never call  forget .
In which case, it is up to the user to call  forget  between independent sequences.
This behavior is only applicable to decorated AbstractRecurrent  modules .
Accepted values for argument  mode  are as follows : 
 
 'eval' only affects evaluation (recommended for RNNs) 
 'train' only affects training 
 'neither' affects neither training nor evaluation (default behavior of the class) 
 'both' affects both training and evaluation (recommended for LSTMs) 
 
 forget() 
 Calls the decorated AbstractRecurrent module's  forget  method. 
 
 SeqLSTM 
 This module is a faster version of  nn.Sequencer(nn.FastLSTM(inputsize, outputsize))  : 
 lua
seqlstm = nn.SeqLSTM(inputsize, outputsize)   
 Each time-step is computed as follows (same as  FastLSTM ): 
 lua
i[t] = σ(W[x->i]x[t] + W[h->i]h[t−1] + b[1->i])                      (1)
f[t] = σ(W[x->f]x[t] + W[h->f]h[t−1] + b[1->f])                      (2)
z[t] = tanh(W[x->c]x[t] + W[h->c]h[t−1] + b[1->c])                   (3)
c[t] = f[t]c[t−1] + i[t]z[t]                                         (4)
o[t] = σ(W[x->o]x[t] + W[h->o]h[t−1] + b[1->o])                      (5)
h[t] = o[t]tanh(c[t])                                                (6)   
 A notable difference is that this module expects the  input  and  gradOutput  to 
be tensors instead of tables. The default shape is  seqlen x batchsize x inputsize  for
the  input  and  seqlen x batchsize x outputsize  for the  output  : 
 ```lua
input = torch.randn(seqlen, batchsize, inputsize)
gradOutput = torch.randn(seqlen, batchsize, outputsize) 
 output = seqlstm:forward(input)
gradInput = seqlstm:backward(input, gradOutput)
```  
 Note that if you prefer to transpose the first two dimension (i.e.  batchsize x seqlen  instead of the default  seqlen x batchsize )
you can set  seqlstm.batchfirst = true  following initialization. 
 For variable length sequences, set  seqlstm.maskzero = true . 
This is equivalent to calling  maskZero(1)  on a  FastLSTM  wrapped by a  Sequencer :
 lua
fastlstm = nn.FastLSTM(inputsize, outputsize)
fastlstm:maskZero(1)
seqfastlstm = nn.Sequencer(fastlstm)   
 For  maskzero = true , input sequences are expected to be seperated by tensor of zeros for a time step. 
 The  seqlstm:toFastLSTM()  method generates a  FastLSTM  instance initialized with the parameters 
of the  seqlstm  instance. Note however that the resulting parameters will not be shared (nor can they ever be). 
 Like the  FastLSTM , the  SeqLSTM  does not use peephole connections between cell and gates (see  FastLSTM  for details). 
 Like the  Sequencer , the  SeqLSTM  provides a  remember  method. 
 Note that a  SeqLSTM  cannot replace  FastLSTM  in code that decorates it with a
 AbstractSequencer  or  Recursor  as this would be equivalent to  Sequencer(Sequencer(FastLSTM)) .
You have been warned. 
 
 SeqLSTMP 
 References:
 * A.  LSTM RNN Architectures for Large Scale Acoustic Modeling 
 * B.  Exploring the Limits of Language Modeling 
 lua
lstmp = nn.SeqLSTMP(inputsize, hiddensize, outputsize)   
 The  SeqLSTMP  is a subclass of  SeqLSTM . 
It differs in that after computing the hidden state  h[t]  (eq. 6), it is 
projected onto  r[t]  using a simple linear transform (eq. 7). 
The computation of the gates also uses the previous such projection  r[t-1]  (eq. 1, 2, 3, 5).
This differs from  SeqLSTM  which uses  h[t-1]  instead of  r[t-1] . 
 The computation of a time-step outlined in  SeqLSTM  is replaced with the following:
 lua
i[t] = σ(W[x->i]x[t] + W[r->i]r[t−1] + b[1->i])                      (1)
f[t] = σ(W[x->f]x[t] + W[r->f]r[t−1] + b[1->f])                      (2)
z[t] = tanh(W[x->c]x[t] + W[h->c]r[t−1] + b[1->c])                   (3)
c[t] = f[t]c[t−1] + i[t]z[t]                                         (4)
o[t] = σ(W[x->o]x[t] + W[r->o]r[t−1] + b[1->o])                      (5)
h[t] = o[t]tanh(c[t])                                                (6)
r[t] = W[h->r]h[t]                                                   (7)   
 The algorithm is outlined in ref. A and benchmarked with state of the art results on the Google billion words dataset in ref. B.
 SeqLSTMP  can be used with an  hiddensize >> outputsize  such that the effective size of the memory cells  c[t]  
and gates  i[t] ,  f[t]  and  o[t]  can be much larger than the actual input  x[t]  and output  r[t] .
For fixed  inputsize  and  outputsize , the  SeqLSTMP  will be able to remember much more information than the  SeqLSTM . 
 
 SeqGRU 
 This module is a faster version of  nn.Sequencer(nn.GRU(inputsize, outputsize))  : 
 lua
seqGRU = nn.SeqGRU(inputsize, outputsize)   
 Usage of SeqGRU differs from GRU in the same manner as SeqLSTM differs from LSTM. Therefore see  SeqLSTM  for more details. 
 
 SeqBRNN 
 lua
brnn = nn.SeqBRNN(inputSize, outputSize, [batchFirst], [merge])   
 A bi-directional RNN that uses SeqLSTM. Internally contains a 'fwd' and 'bwd' module of SeqLSTM. Expects an input shape of  seqlen x batchsize x inputsize .
By setting [batchFirst] to true, the input shape can be  batchsize x seqLen x inputsize . Merge module defaults to CAddTable(), summing the outputs from each
output layer. 
 Example:
 input = torch.rand(1, 1, 5)
brnn = nn.SeqBRNN(5, 5)
print(brnn:forward(input))  
Prints an output of a 1x1x5 tensor. 
 
 BiSequencer 
 Applies encapsulated  fwd  and  bwd  rnns to an input sequence in forward and reverse order.
It is used for implementing Bidirectional RNNs and LSTMs. 
 lua
brnn = nn.BiSequencer(fwd, [bwd, merge]) 
 The input to the module is a sequence (a table) of tensors
and the output is a sequence (a table) of tensors of the same length.
Applies a  fwd  rnn (an  AbstractRecurrent  instance) to each element in the sequence in
forward order and applies the  bwd  rnn in reverse order (from last element to first element).
The  bwd  rnn defaults to: 
 lua
bwd = fwd:clone()
bwd:reset() 
 For each step (in the original sequence), the outputs of both rnns are merged together using
the  merge  module (defaults to  nn.JoinTable(1,1) ). 
If  merge  is a number, it specifies the  JoinTable 
constructor's  nInputDim  argument. Such that the  merge  module is then initialized as : 
 lua
merge = nn.JoinTable(1,merge) 
 Internally, the  BiSequencer  is implemented by decorating a structure of modules that makes 
use of 3 Sequencers for the forward, backward and merge modules. 
 Similarly to a  Sequencer , the sequences in a batch must have the same size.
But the sequence length of each batch can vary. 
 Note : make sure you call  brnn:forget()  after each call to  updateParameters() . 
Alternatively, one could call  brnn.bwdSeq:forget()  so that only  bwd  rnn forgets.
This is the minimum requirement, as it would not make sense for the  bwd  rnn to remember future sequences. 
 
 BiSequencerLM 
 Applies encapsulated  fwd  and  bwd  rnns to an input sequence in forward and reverse order.
It is used for implementing Bidirectional RNNs and LSTMs for Language Models (LM). 
 lua
brnn = nn.BiSequencerLM(fwd, [bwd, merge]) 
 The input to the module is a sequence (a table) of tensors
and the output is a sequence (a table) of tensors of the same length.
Applies a  fwd  rnn (an  AbstractRecurrent  instance to the 
first  N-1  elements in the sequence in forward order.
Applies the  bwd  rnn in reverse order to the last  N-1  elements (from second-to-last element to first element).
This is the main difference of this module with the  BiSequencer .
The latter cannot be used for language modeling because the  bwd  rnn would be trained to predict the input it had just be fed as input. 
 
 The  bwd  rnn defaults to: 
 lua
bwd = fwd:clone()
bwd:reset() 
 While the  fwd  rnn will output representations for the last  N-1  steps,
the  bwd  rnn will output representations for the first  N-1  steps.
The missing outputs for each rnn ( the first step for the  fwd , the last step for the  bwd )
will be filled with zero Tensors of the same size the commensure rnn's outputs.
This way they can be merged. If  nn.JoinTable  is used (the default), then the first 
and last output elements will be padded with zeros for the missing  fwd  and  bwd  rnn outputs, respectively. 
 For each step (in the original sequence), the outputs of both rnns are merged together using
the  merge  module (defaults to  nn.JoinTable(1,1) ). 
If  merge  is a number, it specifies the  JoinTable 
constructor's  nInputDim  argument. Such that the  merge  module is then initialized as : 
 lua
merge = nn.JoinTable(1,merge) 
 Similarly to a  Sequencer , the sequences in a batch must have the same size.
But the sequence length of each batch can vary. 
 Note that LMs implemented with this module will not be classical LMs as they won't measure the 
probability of a word given the previous words. Instead, they measure the probabiliy of a word
given the surrounding words, i.e. context. While for mathematical reasons you may not be able to use this to measure the 
probability of a sequence of words (like a sentence), 
you can still measure the pseudo-likeliness of such a sequence (see  this  for a discussion). 
 
 Repeater 
 This Module is a  decorator  similar to  Sequencer .
It differs in that the sequence length is fixed before hand and the input is repeatedly forwarded 
through the wrapped  module  to produce an output table of length  nStep :
 lua
r = nn.Repeater(module, nStep) 
Argument  module  should be an  AbstractRecurrent  instance.
This is useful for implementing models like  RCNNs ,
which are repeatedly presented with the same input. 
 
 RecurrentAttention 
 References : 
 
 A.  Recurrent Models of Visual Attention 
 B.  Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning 
 
 This module can be used to implement the Recurrent Attention Model (RAM) presented in Ref. A :
 lua
ram = nn.RecurrentAttention(rnn, action, nStep, hiddenSize) 
 rnn  is an  AbstractRecurrent  instance. 
Its input is  {x, z}  where  x  is the input to the  ram  and  z  is an 
action sampled from the  action  module. 
The output size of the  rnn  must be equal to  hiddenSize . 
 action  is a  Module  
that uses a  REINFORCE module  (ref. B) like 
 ReinforceNormal , 
 ReinforceCategorical , or 
 ReinforceBernoulli  
to sample actions given the previous time-step's output of the  rnn . 
During the first time-step, the  action  module is fed with a Tensor of zeros of size  input:size(1) x hiddenSize .
It is important to understand that the sampled actions do not receive gradients 
backpropagated from the training criterion. 
Instead, a reward is broadcast from a Reward Criterion like  VRClassReward  Criterion to 
the  action 's REINFORCE module, which will backprogate graidents computed from the  output  samples 
and the  reward . 
Therefore, the  action  module's outputs are only used internally, within the RecurrentAttention module. 
 nStep  is the number of actions to sample, i.e. the number of elements in the  output  table. 
 hiddenSize  is the output size of the  rnn . This variable is necessary 
to generate the zero Tensor to sample an action for the first step (see above). 
 A complete implementation of Ref. A is available  here . 
 
 MaskZero 
 This module zeroes the  output  rows of the decorated module 
for commensurate  input  rows which are tensors of zeros. 
 lua
mz = nn.MaskZero(module, nInputDim) 
 The  output  Tensor (or table thereof) of the decorated  module 
will have each row (samples) zeroed when the commensurate row of the  input  
is a tensor of zeros.  
 The  nInputDim  argument must specify the number of non-batch dims 
in the first Tensor of the  input . In the case of an  input  table,
the first Tensor is the first one encountered when doing a depth-first search. 
 This decorator makes it possible to pad sequences with different lengths in the same batch with zero vectors. 
 Caveat:  MaskZero  not guarantee that the  output  and  gradInput  tensors of the internal modules 
of the decorated  module  will be zeroed as well when the  input  is zero as well. 
 MaskZero  only affects the immediate  gradInput  and  output  of the module that it encapsulates.
However, for most modules, the gradient update for that time-step will be zero because 
backpropagating a gradient of zeros will typically yield zeros all the way to the input.
In this respect, modules to avoid in encapsulating inside a  MaskZero  are  AbsractRecurrent  
instances as the flow of gradients between different time-steps internally. 
Instead, call the  AbstractRecurrent.maskZero  method
to encapsulate the internal  recurrentModule . 
 
 TrimZero 
 WARNING : only use this module if your input contains lots of zeros. 
In almost all cases,  MaskZero  will be faster, especially with CUDA. 
 Ref. A :  TrimZero: A Torch Recurrent Module for Efficient Natural Language Processing 
 The usage is the same with  MaskZero . 
 lua
mz = nn.TrimZero(module, nInputDim) 
 The only difference from  MaskZero  is that it reduces computational costs by varying a batch size, if any, for the case that varying lengths are provided in the input. 
Notice that when the lengths are consistent,  MaskZero  will be faster, because  TrimZero  has an operational cost.  
 In short, the result is the same with  MaskZero 's, however,  TrimZero  is faster than  MaskZero  only when sentence lengths is costly vary. 
 In practice, e.g. language model,  TrimZero  is expected to be faster than  MaskZero  about 30%. (You can test with it using  test/test_trimzero.lua .) 
 
 LookupTableMaskZero 
 This module extends  nn.LookupTable  to support zero indexes. Zero indexes are forwarded as zero tensors. 
 lua
lt = nn.LookupTableMaskZero(nIndex, nOutput) 
 The  output  Tensor will have each row zeroed when the commensurate row of the  input  is a zero index.  
 This lookup table makes it possible to pad sequences with different lengths in the same batch with zero vectors. 
 
 MaskZeroCriterion 
 This criterion zeroes the  err  and  gradInput  rows of the decorated criterion 
for commensurate  input  rows which are tensors of zeros. 
 lua
mzc = nn.MaskZeroCriterion(criterion, nInputDim) 
 The  gradInput  Tensor (or table thereof) of the decorated  criterion 
will have each row (samples) zeroed when the commensurate row of the  input  
is a tensor of zeros. The  err  will also disregard such zero rows. 
 The  nInputDim  argument must specify the number of non-batch dims 
in the first Tensor of the  input . In the case of an  input  table,
the first Tensor is the first one encountered when doing a depth-first search. 
 This decorator makes it possible to pad sequences with different lengths in the same batch with zero vectors. 
 
 SeqReverseSequence 
 lua
reverseSeq = nn.SeqReverseSequence(dim) 
 Reverses an input tensor on a specified dimension. The reversal dimension can be no larger than three. 
 Example:
```
input = torch.Tensor({{1,2,3,4,5}, {6,7,8,9,10}})
reverseSeq = nn.SeqReverseSequence(1)
print(reverseSeq:forward(input)) 
 Gives us an output of torch.Tensor({{6,7,8,9,10},{1,2,3,4,5}})
``` 
 
 SequencerCriterion 
 This Criterion is a  decorator : 
 lua
c = nn.SequencerCriterion(criterion, [sizeAverage])   
 Both the  input  and  target  are expected to be a sequence, either as a table or Tensor. 
For each step in the sequence, the corresponding elements of the input and target 
will be applied to the  criterion .
The output of  forward  is the sum of all individual losses in the sequence. 
This is useful when used in conjunction with a  Sequencer . 
 If  sizeAverage  is  true  (default is  false ), the  output  loss and  gradInput  is averaged over each time-step. 
 
 RepeaterCriterion 
 This Criterion is a  decorator : 
 lua
c = nn.RepeaterCriterion(criterion)   
 The  input  is expected to be a sequence (table or Tensor). A single  target  is 
repeatedly applied using the same  criterion  to each element in the  input  sequence.
The output of  forward  is the sum of all individual losses in the sequence.
This is useful for implementing models like  RCNNs ,
which are repeatedly presented with the same target."
0mp,"tflclck 
 TOEFL Exam Timer App 
 Incredibly important note 
 This is my first project I've ever put on GitHub. I keep it for nostalgic reasons. My busy-waiting implementation of a timer always makes me smile.  
 Cheers! Algorithms written in the Shakespeare Programming Language (SPL) 
 This is a repo where I keep my little programs I've done in the SPL. 
 Bubble sort 
 How to use it? 
 Write 5 numbers that you would like to sort. Do not type any spaces between them!
The program will print the sorted array of numbers. It will neither print a newline character
nor spaces between the numbers. A number must consist of one digit only. 
 Example: 
 In: 
 57490 
 Out: 
 04579 
 Stack sort 
 How to use it? 
 You can enter as much of nonnegative numbers as you want.
Separate the numbers using a newline character. The program will stop getting the
numbers from the input stream when the number ""-1"" is entered. 
 Example: 
 In: 
 420
404
23
14
290595
1
-1 
 Out: 
 1
14
23
404
420
290595 
 How to compile it? 
 The Shakespeare Programming Language is usually translated
to C and then compiled. The official translator can be downloaded
from  the official webpage . 
 If you have any difficulties with making the translator run I do recommend
to search thourgh the  Shakespeare Programming Language Tag  on StackOverflow. 
 Long live Shakespeare! SafariFerrari 
 SafariFerrari is a Safari extension which allows you to navigate webpages without using a mouse.  
 Available commands 
 Here are available commands which you can use in this extension ( use it without quotations marks ): 
 Page navigation 
 
 ""h""  - scroll left 
 ""j""  - scroll up 
 ""k""  - scroll down 
 ""l""  - scroll right 
 ""u""  - scroll up (longer step) 
 ""d""  - scroll bottom (longer step) 
 ""gg""  - scroll to the top of the page 
 ""G""  - scroll to the bottom of the page 
 
 Tab navigation 
 
 ""x""  - close current tab 
 ""X""  - reopen closed tab (you can use it more than just once) 
 ""f""  - show links tooltips (after that you can open them in next tab by typing their ""ID"") 
 ""r""  - reload tab 
 ""J""  - go to the left tab 
 ""K""  - go to the right tab 
 "">>""  - move the current tab to the right 
 ""<<""  - move the current tab to the left 
 ""H""  - go back in history of the current tab 
 ""F""  -  go forward in history of the current tab 
 
 Miscellaneous 
 
 ""q""  - surprise command 
 
 Credits 
 Our inspiration was an extension for Chrome called Vimium. 
 History 
 The extension was created during a Local Hack Day 2015 in Warsaw. n0tes 
 This work is licensed under a  Creative Commons Attribution 4.0 International License . dotfiles 
 FreeBSD 
 ```sh 
 dwm 
 mkdir -p ${HOME}/h
git clone http://github.com/0mp/dwm ${HOME}/h/dwm
make -C ${HOME}/h/dwm clean dwm install 
 makaron 
 mkdir -p ${HOME}
git clone --recursive https://github.com/0mp/makaron ${HOME}/h/makaron
make -C ${HOME}/h/makaron install 
 goat 
 mkdir -p ${HOME}/h
git clone http://github.com/0mp/goat ${HOME}/h/goat
make -C ${HOME}/h/goat clean install
``` 
 mDNS 
 sh
pkg install avahi-app nss_mdns
sysrc avahi_daemon_enable=""YES""
sysrc dbus_enable=""YES""
if ! grep -q ""hosts.*:.*mdns"" /etc/nsswitch.conf; then
    tmp=""$(mktemp)"" && \
    sed '/hosts/s/$/ mdns/' /etc/nsswitch.conf > ""$tmp"" && \
    cat ""$tmp"" > /etc/nsswitch.conf
    rm ""$tmp""
fi 
 Finally, add  mdns  to the  hosts  line in  /etc/nsswitch.conf . 
 Give less memory to ARC 
 sysctl vfs.zfs.arc_max=$(expr -- $(sysctl -n hw.physmem) / 2) 
 lagg(4) configuration 
 sysrc ifconfig_em0=""up""
sysrc wlans_iwm0=""wlan0""
sysrc ifconfig_wlan0=""WPA powersave""
sysrc create_args_wlan0=""wlanaddr \$(ifconfig em0 ether | awk '/ether/{print \$2}') country de""
sysrc cloned_interfaces=""lagg0""
sysrc ifconfig_lagg0=""up laggproto failover laggport em0 laggport wlan0 DHCP"" 
 Firefox 
 Microphone support: set  media.cubeb.backend  to  oss  in about:config. (https://forums.freebsd.org/threads/how-to-use-microphone-with-firefox.74292/#post-485968) 
 Git 
 Sample  ~/.gitconfig.local 
 [includeIf ""gitdir:~/rust/""]
    path = ~/.gitconfig-rust 
 Specifying how to reconcile divergent branches 
 git config pull.rebase false  # merge (the default strategy)
git config pull.rebase true   # rebase                      
git config pull.ff only       # fast-forward only goat 
 Overview 
 
 I used to retrieve carefully-constructed  cd  commands from my history. 
But then, I got a goat. 
 ~  Jonathan Paugh  on  Google+ 
 
 console
[ ~/Pictures ] $ goat dev ~/Documents/devel # create a link to the dev directory
[ ~/Pictures ] $ ls # see that there is no ~/Pictures/dev directory here
seahorses wallpapers
[ ~/Pictures ] $ cd dev # the goat framework's got you covered!
[ ~/Documents/devel ] $ 
 
 Oh my! This is a  POSIX-compliant  shell  movement boosting   hack  for
 real ninjas . 
 posix_me_harder  # posixly_correct 
 ~ 0mp 
 
 Sometimes you jump around your filesystem tree a lot and you end up putting a
couple of ugly aliases into your shell's rc file. 
 
 I should try it, even if it is dumb! 
 ~  dse  on  What the Daily WTF?  about goat v1.1.1 
 
 With goat you can easily manage your ninja shortcuts - just type  goat p
~/Projects  to introduce a new link and then  cd p  to jump to its destination. 
 
 Rad! I can do  cd ....  now instead of performing a horse galloping-like
waltz with  ../  being my miserable dance floor. I'm cloning this goat
straight away! 
 ~ YA0mp 
 
 BTW, Bash completion is now fully working with goat's shortcuts. 
 Installation 
 console
$ make install 
 Aferwards: 
 
 Make sure that  ~/.local/bin  is in your  PATH : 
 
 console
  $ cat <<'EOF' >> ~/.bashrc
  case ""$PATH"" in
      *$HOME/.local/bin*) ;;
      *) PATH=""$HOME/.local/bin:$PATH"" ;;
  esac
  EOF 
 
 Make sure that files inside  ~/.local/etc/bash_completion.d  are actually
  sourced by the Bash completion library: 
 
 console
  $ cat <<'EOF' >> ~/.bash_completion
  if [[ -d ~/.bash_completion.d ]]
  then
      for f in ~/.local/etc/bash_completion.d/*
      do
          [[ -f $f ]] && source ""$f""
      done
  fi
  EOF 
 Usage overview 
 ```console
Create a shortcut named “f” to ~/Documents/dev/freebsd (no need to use
the link command explicitly here): 
   $ goat f ~/Documents/dev/freebsd
 
 Follow a link to change a directory with cd(1): 
   $ cd f
 
 Take the “f” shortcut and enter its destination subdirectory with just
one command: 
   $ pwd
  /home/0mp
  $ cd f/ports
  $ pwd
  /usr/home/0mp/freebsd/ports
 
 Create a shortcut named “p” to the current directory: 
   $ goat p .
 
 Go up the filesystem tree with ... (same as the standard “cd ../../”): 
   $ cd ...
 
 List all your links: 
   $ goat list
  dots    ->      /usr/home/0mp/.dotfiles
  down    ->      /usr/home/0mp/Downloads
  f       ->      /usr/home/0mp/freebsd
  p       ->      /usr/home/0mp/freebsd/ports
  pa      ->      /usr/home/0mp/freebsd/patches
  src     ->      /usr/home/0mp/freebsd/svn/src
  svn     ->      /usr/home/0mp/freebsd/svn
 
 Delete a link (or more): 
   $ goat delete f p
 
 Delete all the links which point to directories with the given prefix: 
   $ goat deleteprefix ""$HOME/Documents""
 
 ``` 
 License 
 Licensed under 2-Clause BSD license. Copyright © 2016-2021 Mateusz Piotrowski io-touchpad 
 
 
 
 
 Application 
 Installation 
 apt-get update
apt-get install build-essential libatlas-dev libatlas3gf-base
apt-get install python3-dev python3-setuptools python3-numpy python3-scipy python3-pip
pip3 install scikit-learn
make
 
 Quick start 
 
 
 Go to the  app  directory. 
 cd app
 
 
 
 Undertake a learning session with a trainig size of  5  for a symbol named  your-secret-symbol  which will create a  yss.txt  file in the  /tmp  directory.: 
 sudo ./app.py add 5 your-secret-symbol touch /tmp/yss.txt
 
 
 
 Use the app. (Hopefully, it will recognise your symbol.) 
 sudo ./app run user
 
 
 
 Run  ./app --help  if you want to learn other features of this app. 
 Tests 
 Installation 
 apt-get update
apt-get install python3-pytest
 
 Usage 
 cd app/test
py.test-3
 
 Tools 
 matrixanalyser.py 
 Installation 
 apt-get install python3-matplotlib
cd app/tools
make
 
 Usage 
 cd app
sudo ./tools/matrixanalyser.py [--help] [--tolerance TOLERANCE] [--show]
 
 More information on the usage is avaialabe if you run  ./tools/matrixanalyser.py --help . 
 All generated figures of the drawn symbols are stored inside
the  app/tools/data/matrixanalyser/figures  directory. pocketchip audisp-auditdistd 
 A plugin for the Linux audit event dispatcher audispd to push audit trail logs over to a FreeBSD auditdistd daemon. 
 Background 
 This project is a continuation of a Google Summer of Code 2016 project for FreeBSD.
The original and final goal of the project is to allow a FreeBSD user to collect and process audit trails from different systems like Linux and Windows.
At the moment, the goal is to add an audispd plugin capable of communicating with FreeBSD auditdistd. 
 Dependencies 
 sh
apt install libssl-dev 
 See also 
 
 Non-BSM to BSM Conversion Tools project for Google Summer of Code 2016 at FreeBSD: https://wiki.freebsd.org/SummerOfCode2016/NonBSMtoBSMConversionTools 
 vim-robpike 
 This is a minimal Vim color scheme working fine with both white and black
terminal backgrounds. 
 Build 
 sh
./build > ~/.vim/colors/robpike.vim 
 Inspiration 
 
 When I was a child, I used to speak like a child, think like a child,
reason like a child; when I became a man, I did away with childish
things. 
 Syntax highlighting is juvenile. When I was a child, I was taught
arithmetic using  colored rods . I grew up and today I
use monochromatic numerals. 
 
 Context: https://groups.google.com/d/topic/golang-nuts/hJHCAaiL0so/discussion 80211 
 Also 
 
 https://wiki.freebsd.org/MateuszPiotrowski/Drivers 
 audisp-auditdistd 
 Pushing audit logs from Linux over to FreeBSD using auditdistd daemons. 
 Usage 
 First set up 
 sh
./generate-auditdistd-conf
./do-vagrant-up
./do-provision
vagrant provision linux-sender --provision-with rebuild-openbsm 
 Run auditdistds 
 sh
vagrant provision freebsd-receiver --provision-with run
vagrant provision linux-sender --provision-with run 
 Detatils 
 There are 3 machines: 
 
 freebsd-receiver 
 freebsd-sender 
 linux-sender 
 
 The goal is to make linux-sender work flawlessly with freebsd-receiver. 
 freebsd-sender is here for debugging purposes. In order to start the freebsd-sender machine you have to run: 
 sh
./do-vagrant-up --full 
 Every machine has its own OpenBSM branch. 
 Dependencies 
 
 rsync 
 vagrant 
 Karta wzorów z metod numerycznych 
 Instalacja pakietu LaTeX 
 FreeBSD 12.0-CURRENT 
 bash
pkg install texlive-full 
 macOS 
 Zainstaluj  pdflatex  z dystrybucji  mactex : 
 bash
brew install brew-cask
brew cask install mactex 
 Ubuntu 16.10 
 bash
sudo apt-get install texlive-full 
 Generowanie plików PDF 
 bash
make 
 Wygenerowany plik pojawi się w katalogu  build . 
 
 Ważne 
 
 https://www.freebsd.org/ 
 https://joinmastodon.org/ 
 Metody Numeryczne MIMUW 2017 
 W skrypcie jest dużo błędów, bo do tej pory nie było żadnej kontroli jakości. Chętnie przyjmę pull requesty :-) 
 Wersja live: https://www.sharelatex.com/project/59d606877ef26a08a8cf3c58 
 todo: 
 
 poprawić niektóre macierze 
 rozdzielić na mniejsze pliki 
 poprawić błędy latexowe 
 
 Karta wzorów 
 https://github.com/0mp/mn-karta-wzorow NAME 
 sct  - screen temperature control 
 SYNOPSIS 
 sct 
[ temperature ] 
 DESCRIPTION 
 This utility can be used to change the screen temperature. 
 A valid
 temperature 
value is between 1000 and 10000.
If the
 temperature 
argument is not provided or its value is invalid
then the screen temperature is set to the default value of 6500. 
 EXAMPLES 
 Campfire: 
 sct 4500
 
 Dust storm on Mars: 
 sct 2000
 
 Coffee-free all nighter: 
 sct 8000
 
 DIAGNOSTICS 
 Failed to get screen resources 
Usually those messages mean that
 sct 
cannot connect to the
X(7)
server. 
 Failed to open X display %s 
See
 Failed to get screen resources . 
 SEE ALSO 
 redshift(1),
xrandr(1) 
 The original blog post about
 sct 
by Ted Unangst:
 https://www.tedunangst.com/flak/post/sct-set-color-temperature 
 HISTORY 
 The
 sct 
utility was written by
Ted Unangst < tedu@OpenBSD.org >. 
 AUTHORS 
 This
manual page was written by
Mateusz Piotrowski < 0mp@FreeBSD.org >. Notatki ze wstępu do analizy sieci społecznych FreeBSD Ports workshop infrastructure 
 See  Wiki  for a cheat sheet and other helpful materials. Bash completion for Poudriere 
 To do 
 
 FLAVORS  support 
 grep -E -n 'TODO|XXX' ./poudriere-bash-completion 
 Parse  poudriere status  in order to complete buildnames. 
 
 See also 
 
 Poudriere's upstream (github.com) 
 coffeebreak 
 This utility is a a fake FreeBSD installer written by Devin Teske. 
 
 It's friday.  Do you need to pretend to be working?  Do you have FreeBSD
systems?  Here you go!  coffeebreak is the tool you always wanted to make
your machine as if it is very busy doing important stuff. 
 
 Give me a break! 
 coffeebreak(6) is  a part of the FreeBSD ports collection : 
 pkg install coffeebreak 
 Otherwise it could be installed manually with make(1): 
 make install Bash completion for FreeBSD 
 
 Covered software 
 Bash completion functions are available for the following utilities 
 
 make(1)  ( work in progress ) 
 mandoc(1) 
 pkg(8)  ( work in progress ) 
 service(8) 
 
 Installation 
 Those additional Bash completion files are installed by default with the  bash-completion port : 
 ```console 
 pkg instal bash-completion 
 ``` 
 If you're willing to install to install this extension manually then set  DESTDIR  and  PREFIX  to match your configuration and run: 
 ```console 
 make install 
 ``` 
 Dependencies 
 
 bash-completion 
 
 Caveats 
 Although the  pkg upstream  ships some Bash completion for pkg(8) it does not conflict with our implementation of the pkg(8) completion. The reason is that the upstream completion file is installed into  /usr/local/etc/bash_completion.d/  while our completion is installed into  /usr/local/share/bash-completion/completions/ , which is processed first according to  the bash-completion README . 
 License 
 BSD 2-Clause ""Simplified"" License NAME 
 kbfsd  - service daemon for the Keybase filesystem (KBFS) 
 SYNOPSIS 
 Required
rc.conf(5)
variables: 
 kbfsd_enable=""YES""
kbfsd_user=""beastie""
 
 Optional
rc.conf(5)
variables: 
 kbfsd_keybase_username=""${kbfsd_user}""
kbfsd_mountpoint=""/keybase""
 
 DESCRIPTION 
 kbfsd 
is an
rc(8)
daemon for the Keybase filesystem (KBFS).
Its aim is to ease the process of using KBFS on
FreeBSD.
It takes care of the configuration the Keybase user would have to do manually
otherwise. 
 kbfsd 
does not start automatically even when
 kbfsd_enable 
is set to
' YES '
in
rc.conf(5).
See the
 CAVEATS 
section for more details. 
 kbfsd 
has to configure some bits of the system in order to mount KBFS as
 kbfsd_user .
 kbfsd_mountpoint 
is created and
the
sysctl(8)
tunable
 vfs.usermount 
is set to
""1""
so that
 kbfsd_user 
could mount
 kbfsd_mountpoint .
Then
 kbfsd_user 
is added to the
""operator""
group to be able to use the
 /dev/fuse 
device.
Finally,
 kbfsd 
attempts to spin off the Keybase server and create required socket files.
Note that this step requires
 kbfsd_user 
to be able to log in as
 kbfsd_keybase_username .
This should be possible once
 kbfsd_user 
registers a device with
' keybase device add '. 
 kbfsd 
may be controlled with the following
rc.conf(5)
variables: 
 kbfsd_enable 
 (*bool*, default: '`NO`')
Enable
**kbfsd**.
 
 kbfsd_keybase_username 
 (*str*, default: *kbfsd_user*)
The username used to log into Keybase.
 
 kbfsd_mountpoint 
 (*str*, default: '`/keybase`')
The directory where KBFS should be mounted.
 
 kbfsd_user 
 (*str*, no defaults)
The login name of a user, who should own
*kbfsd_mountpoint*.
It cannot be empty.
 
 INSTALLATION 
 The easiest way is to just install the
 kbfsd 
package via
pkg(8)
on
FreeBSD: 
 pkg install kbfsd
 
 kbfsd 
can be installed manually with the following command: 
 make all
make install
 
 FILES 
 /home/ ${ kbfsd_user } /.config/keybase/kbfsd. ${ kbfsd_user } .pid 
 PID file.
 
 EXIT STATUS 
 The
 kbfsd 
daemon
exits 0 on success, and >0 if an error occurs. 
 SEE ALSO 
 rc.conf(5),
mount(8),
rc(8) 
 AUTHORS 
 The
 kbfsd 
daemon and its manual page were written by
Mateusz Piotrowski < 0mp@FreeBSD.org >. 
 CAVEATS 
 kbfsd 
is
 not 
started automatically together with other daemons during boot because it uses
the
' nostart '
KEYWORD
(see rc(8) for details).
The reason is that in order to reliably mount KBFS the user has to establish
a session with the Keybase server first.
This is done by calling:
' keybase login username '
(where
 username 
is the same as
 kbfsd_keybase_username )
.
Unfortunately, this command happens to block the booting process from time to
time, which is unacceptable. 
 BUGS 
 kbfsd 
seems to kill
 kbfsfuse 
too rapidly for
 kbfsfuse 
to properly unmount.
As a workaround,
 kbfsd 
calls
umount(8)
on the mount point in the
 poststop 
phase
(see rc.subr(8)). 
 Currently,
 kbfsd 
uses
 kbfsd_env 
internally to set the
 HOME 
environmental variable to the home directory of
 kbfsd_user .
It is recommended to read the service file before setting
 kbfsd_env 
in
rc.conf(5). 
 On some
FreeBSD
versions older than 13.0 the name of
fusefs(5)
might be
""fuse""
instead of
""fusefs"".
As a result,
 kbfsd 
might fail to start.
A potential workaronud is to set
 required_modules 
to an empty string in
 /usr/local/etc/rc.conf.d/kbfsd 
and then loading the FUSE kernel module differently
(e.g., via
 kld_list 
in
rc.conf(5))
. NAME 
 moinmoincli  - edit existing MoinMoin wiki pages from a command line 
 SYNOPSIS 
 moinmoincli 
[ -hquv ]
[ -f   textfile ]
[ -n   username ]
[ -t   target ]
[ selector ] 
 DESCRIPTION 
 moinmoincli 
is an interactive utility to edit existing MoinMoin wiki pages from
a command line.
 moinmoincli 
was only tested with the
FreeBSD
Wiki
which is using the MoinMoin engine.
The
FreeBSD
Wiki address is also hard-coded into
 moinmoincli 
as the default web service the utility works with. 
 moinmoincli 
cannot create new wiki pages. 
 When
 moinmoincli 
is run in order to update a wiki page, a diff is presented to the
user to confirm the changes.
It is done by providing a comment describing the changes.
Next, a summary and the action menu is displayed.
The summary includes the username, the target, the comment, and the
text file name.
Actions that could be entered at the action menu prompt are
described in the
 ACTION MENU 
subsection.
Once the user confirms requested changes to be processed, the password prompt
is presented to the user. 
 Command-line options 
 The options are as follows: 
 -f   textfile ,  --file   textfile 
 
 Point to the
Ar textfile
that is going to be sent to the server. 
 
 -h ,  --help 
 
 Show help. 
 
 -n   username ,  --name   username 
 
 Set the username to log in as. 
 
 -q ,  --quick 
 
 Activate the quick mode, which disables the action menu.
As a result the changes will be uploaded once a comment is entered
by the user. 
 
 -t   target ,  --target   target 
 
 Provide the name of the wiki page to be updated, e.g.:
"" /WiFi/80211ac "".
A hyperlink is acceptable as well, e.g.:
"" https://wiki.freebsd.org/WiFi/80211ac "". 
 
 -u ,  --update 
 
 Download the contents of the
 target 
wiki page and put them into the 
 
 -v ,  --version 
 
 Show version. 
 
 selector 
 
 Run
 moinmoincli 
as preconfigured for a given
 selector . 
 Selectors are used if the
 select_target ()
function is defined in the
configuration file (defaults are listed in the
 FILES 
section).
A single
 selector 
is associated with a
 target 
and a
 textfile .
As a result it is possible to use selectors to replace
"" moinmoincli -f ./80211ac.moin -t /WiFi/80211ac ""
with
"" moinmoincli ac ""
using an
 ac 
selector. 
 
 Action menu 
 The following actions could be entered at the action menu prompt one
action at a time: 
 y ,  k 
 
 Confirm changes.
The action menu will be closed and the changes uploaded to the server. 
 
 n ,  a 
 
 Abort.
Close
 moinmoincli 
immediately. 
 
 t 
 
 Mark changes as trivial so that subscribers to the modified wiki
page are not notified. 
 
 c 
 
 Change the comment. 
 
 d 
 
 Show the diff. 
 
 s 
 
 Print the summary. 
 
 FILES 
 ~/.moinmoincli.conf 
 
 The
 moinmoincli 
configuration file. 
 
 /tmp/moinmoincli-workdir 
 
 The location of temporary files. 
 
 EXAMPLES 
 Example 1: Downloading Wiki Pages 
 Download the code of
 https://wiki.freebsd.org/Community/Dogs 
and save it under
 ./dogs.moinmoin : 
 $ moinmoincli --file ./dogs.moinmoin --target /Community/Dogs --update
 
 Example 2: Uploading Wiki Pages 
 Write
 ./sandbox.txt 
to
 https://wiki.freebsd.org/WikiSandBox 
as
 CharlieRoot : 
 $ moinmoincli -n CharlieRoot -f ./sandbox.txt -t /WikiSandBox
 
 Example 3: Suggested Workflow 
 
 
 Configure
     ~/.moinmoincli.conf 
    so that it is not necessary to specify the username, the text file name and the
    target website every time.
    Use selectors for that. 
 
 
 Grab the latest version of the wiki page with a selector: 
 $ moinmoincli -u selectorYouHaveConfigured
 
 
 
 Edit the text file. 
 
 
 Push the changes over to the wiki server with: 
 $ moinmoincli selectorYouHaveConfigured
 
 
 
 Configuration File 
 # The username to use when logging in.
name='CharlieRoot'

# The password for the account.
password='secret'

# The default target.
# It is used unless a selector or the -t flag is used.
target='/WikiSandBox'

# The default textfile.
textfile='/tmp/wiki.moin'

# The select_target function, which defines the logic behind
# selectors.
select_target() {
    case ""$1"" in
        [wW]*)
            target='/WiFi'
            textfile=""$HOME/wifi.moin""
            ;;
        *)
            printf '%s\n' ""invalid selector '$1'"" >&2
            ;;
    esac
}
 
 AUTHORS 
 moinmoincli 
and its manual page was written by
Mateusz Piotrowski < 0mp@FreeBSD.org >. Prometheus Documentation 
 This repository contains both the content and the static-site generator code for the
Prometheus documentation site. 
 Contributing Changes 
 See  CONTRIBUTING.md  for general instructions for new Prometheus contributors. 
 The main documentation contents of this website are located in the  content/docs  directory. 
 Documentation concerning the Prometheus server is  maintained in the Prometheus server repository  and cloned into the website at build time. 
 As a guideline, please keep the documentation generally applicable and avoid use-case-specific changes. 
 Prerequisites 
 You need to have a working Ruby environment set up (including  bundler )
and then install the necessary gems: 
 bash
cd docs
make bundle 
 Building 
 To generate the static site, run: 
 bash
make build 
 The resulting static site will be stored in the  output  directory. 
 Optionally, you can use an API token to avoid rate limits on the API. You can get an API token from https://github.com/settings/tokens/new.
 bash
export GITHUB_AUTHENTICATION='-u user:token' 
 Development Server 
 To run a local server that displays the generated site, run: 
 ```bash 
 Rebuild the site whenever relevant files change: 
 make guard 
 Start the local development server in a separate shell: 
 make serve
``` 
 You should now be able to view the generated site at
 http://localhost:3000/ . 
 License 
 Apache License 2.0, see  LICENSE . NAME 
 vipe.sh  - portable, shell-compatible version of
vipe(1)
from moreutils 
 SYNOPSIS 
 vipe.sh 
|
 ... 
 DESCRIPTION 
 vipe.sh 
is a utility, which makes it possible to use an editor
in pipelines of Bourne-compatible shells. 
 It aims to address pipeline limitations, which result in warnings like:
""Vim: Warning: Output is not to a terminal"". 
 IMPLEMENTATION NOTES 
 The
 vipe.sh 
command makes use of
mktemp(1)
to create temporary files. 
 ENVIRONMENT 
 The following environmental variables are recognized by
 vipe.sh : 
 EDITOR  (default: ""vi"") 
 
 The editor to use if
 VISUAL 
is not set. 
 
 VISUAL  (default: unset) 
 
 The editor to use. 
 
 EXIT STATUS 
 The  vipe.sh  utility exits 0 on success, and >0 if an error occurs. 
 EXAMPLES 
 The following is an example of a typical usage
of the
 vipe.sh 
command: 
 curl wttr.in | vipe.sh | nc termbin.com 9999 | xclip -sel clip
 
 SEE ALSO 
 vipe(1),
which is a part of
 moreutils . 
 HISTORY 
 vipe.sh 
is a reimplementation of the popular, Perl-based
vipe(1)
from the moreutils project. 
 AUTHORS 
 vipe.sh 
and this manual page was written by
Mateusz Piotrowski < 0mp@FreeBSD.org >. The wonderful en_DK.UTF-8 locale 
 Based on  the work done by Ivan Voras . freebsd-ports-nix 
 
 Nix package manager port for the FreeBSD Ports Collection. 
 Installation 
 The port has already been committed to the FreeBSD Ports Collection (see  sysutils/nix on FreshPorts ). 
 The binary package can be installed with  pkg install nix . 
 Community 
 Join us on https://libera.chat, channel  #freebsd-nix . 
 References 
 Blog posts mentioning Nix on FreeBSD 
 
 http://sandervanderburg.blogspot.com/2020/02/a-declarative-process-manager-agnostic.html 
 
 Interesting issues, pull requests, and other related efforts 
 
 https://github.com/NixOS/nixpkgs/pull/81459 
 WIP port of Nix to pkgsrc: https://wip.pkgsrc.org/cgi-bin/gitweb.cgi?p=pkgsrc-wip.git;a=tree;f=nix;hb=HEAD 
 https://github.com/NixOS/nixpkgs/pull/82131 
 Makaron 
 The Great Debugging of the  signal-cli  Daemon D-Bus Integration 
 signal-cli  is a command-line client for the  Signal messaging service . 
 It has been recently ported to FreeBSD ( net-im/signal-cli ). Most of its features work as expected except its integration with  D-Bus , which is necessary for UI wrappers for signal-cli like  scli . 
 Expected behavior 
 
 
 The service starts without any issues: 
 service signal_cli start 
 
 
 The user can send messages via the signal-cli daemon: 
 signal-cli --dbus-system send -m ""Message"" +00123123123 
 
 
 The user is able to start scli without any issues. 
 
 
 Enviroment setup 
 
 Install the port. 
 Link (or register) signal-cli to your Signal account. Do it as the  signal-user  from  /var/lib/signal-cli . 
 
 Current challanges 
 org.freedesktop.dbus.exceptions.DBusException: Failed to connect to bus Failed to auth 
 console
$ export DISPLAY=0 
$ export JAVA_OPTS=""-Djava.library.path=/usr/local/lib""
$ signal-cli -u +00123456789 daemon
org.freedesktop.dbus.exceptions.DBusException: Failed to connect to bus Failed to auth
        at org.freedesktop.dbus.DBusConnection.<init>(DBusConnection.java:304)
        at org.freedesktop.dbus.DBusConnection.getConnection(DBusConnection.java:282)
        at org.asamk.signal.commands.DaemonCommand.handleCommand(DaemonCommand.java:50)
        at org.asamk.signal.Main.handleCommands(Main.java:126)
        at org.asamk.signal.Main.main(Main.java:61) 
 In  net-im/signal-cli/work/signal-cli-0.6.5/src/main/java/org/asamk/signal/commands/DaemonCommand.java  (around line 50): 
 java
        DBusConnection conn = null;
        try {
            try {
                int busType;
                if (ns.getBoolean(""system"")) {
                    busType = DBusConnection.SYSTEM;
                } else {
                    busType = DBusConnection.SESSION;
                }
                conn = DBusConnection.getConnection(busType);
                conn.exportObject(SIGNAL_OBJECTPATH, m);
                conn.requestBusName(SIGNAL_BUSNAME);
            } catch (UnsatisfiedLinkError e) {
                System.err.println(""Missing native library dependency for dbus service: "" + e.getMessage());
                return 1;
            } catch (DBusException e) {
                e.printStackTrace();
                return 2;
            } 
 In  devel/dbus-java/work/dbus-java-2.7/org/freedesktop/dbus/DBusConnection.java  (around line 288): 
 ```java
   @SuppressWarnings(""unchecked"")
   private DBusConnection(String address) throws DBusException
   {
      super(address);
      busnames = new Vector (); 
   synchronized (_reflock) {
     _refcount = 1; 
  }

  try {
     transport = new Transport(addr, AbstractConnection.TIMEOUT);
        connected = true;
  } catch (IOException IOe) {
     if (EXCEPTION_DEBUG && Debug.debug) Debug.print(Debug.ERR, IOe);            
     disconnect();
     throw new DBusException(_(""Failed to connect to bus "")+IOe.getMessage());
  } catch (ParseException Pe) {
     if (EXCEPTION_DEBUG && Debug.debug) Debug.print(Debug.ERR, Pe);            
     disconnect();
     throw new DBusException(_(""Failed to connect to bus "")+Pe.getMessage());
  }

  // start listening for calls
  listen();

  // register disconnect handlers
  DBusSigHandler h = new _sighandler();
  addSigHandlerWithoutMatch(org.freedesktop.DBus.Local.Disconnected.class, h);
  addSigHandlerWithoutMatch(org.freedesktop.DBus.NameAcquired.class, h);

  // register ourselves
  _dbus = getRemoteObject(""org.freedesktop.DBus"", ""/org/freedesktop/DBus"", DBus.class);
  try {
     busnames.add(_dbus.Hello());
  } catch (DBusExecutionException DBEe) {
     if (EXCEPTION_DEBUG && Debug.debug) Debug.print(Debug.ERR, DBEe);
     throw new DBusException(DBEe.getMessage());
  }
 
 }
``` 
 Let's run signal-cli with some more debug support. 
 First, some preparation: 
 
 Build a debug version of  devel/dbus-java  (e.g.,  cd ports/devel/dbus-java && make WITH_DEBUG=yes clean reinstall ). 
 
 Modify the class path in  signal-cli  script. Replace the following JAR files: 
 
 /usr/local/share/signal-cli/lib/debug-1.1.1.jar 
 `/usr/local/share/signal-cli/lib/dbus-java-2.7.0.jar 
 
 with: 
 
 /usr/local/share/java/classes/dbus-2.7.jar 
 /usr/local/share/java/classes/debug-enable.jar 
 
 
 
 Add the following code to the  signal-cli  script to gain additional debugging infomation: 
 DBUS_JAVA_EXCEPTION_DEBUG=yes 
 See  /usr/local/share/doc/dbus-java/INSTALL  for more details. 
 
 
 Create a  debug.conf  file in the directory from which you start  signal-cli : 
 console
$ echo ALL = ALL > debug.conf 
 See  /usr/local/share/doc/dbus-java/INSTALL  for more details. 
 
 
 Now, start  signal-cli : 
 ```console 
 chroot -u signal-cli / env PS1='\w$ ' /bin/sh 
 /$ cd /var/lib/signal-cli
/var/lib/signal-cli$ signal-cli -u +00123456789 --config $PWD daemon --system
[org.freedesktop.dbus.AbstractConnection. ()] Debugging of internal exceptions enabled
[org.freedesktop.dbus.AbstractConnection. ()] Loading debug config file: debug.conf
[org.freedesktop.dbus.DBusConnection.getConnection()] Getting bus connection for unix:path=/var/run/dbus/system_bus_socket: null
[org.freedesktop.dbus.DBusConnection.getConnection()] Creating new bus connection to: unix:path=/var/run/dbus/system_bus_socket
[org.freedesktop.dbus.MethodTuple. ()] new MethodTuple(Ping, )
[org.freedesktop.dbus.Marshalling.recursiveGetDBusType()] Converted Java type: class java.lang.String to D-Bus Type: s
[org.freedesktop.dbus.MethodTuple. ()] new MethodTuple(Introspect, )
[org.freedesktop.dbus.EfficientQueue.shrink()] Shrinking
[org.freedesktop.dbus.BusAddress. ()] Parsing bus address: unix:path=/var/run/dbus/system_bus_socket
[org.freedesktop.dbus.BusAddress. ()] Transport type: unix
[org.freedesktop.dbus.BusAddress. ()] Transport options: {path=/var/run/dbus/system_bus_socket}
[org.freedesktop.dbus.Transport.connect()] Connecting to unix: {path=/var/run/dbus/system_bus_socket}
[org.freedesktop.dbus.Transport$SASL.auth()] AUTH state: 0
[org.freedesktop.dbus.Transport$SASL.send()] sending: AUTH 
 [org.freedesktop.dbus.Transport$SASL.auth()] AUTH state: 1
[org.freedesktop.dbus.Transport$SASL.receive()] received: REJECTED EXTERNAL
[org.freedesktop.dbus.Transport$SASL$Command. ()] Creating command from: [REJECTED, EXTERNAL]
[org.freedesktop.dbus.Transport$SASL$Command. ()] Created command: Command(3, 1, null, null)
[org.freedesktop.dbus.Transport$SASL.send()] sending: AUTH EXTERNAL 323438 
 [org.freedesktop.dbus.Transport$SASL.auth()] AUTH state: 1
[org.freedesktop.dbus.Transport$SASL.receive()] received: REJECTED EXTERNAL
[org.freedesktop.dbus.Transport$SASL$Command. ()] Creating command from: [REJECTED, EXTERNAL]
[org.freedesktop.dbus.Transport$SASL$Command. ()] Created command: Command(3, 1, null, null)
[org.freedesktop.dbus.DBusConnection. ()] java.io.IOException: Failed to auth
    at org.freedesktop.dbus.Transport.connect(Unknown Source)
    at org.freedesktop.dbus.Transport. (Unknown Source)
    at org.freedesktop.dbus.DBusConnection. (Unknown Source)
    at org.freedesktop.dbus.DBusConnection.getConnection(Unknown Source)
    at org.asamk.signal.commands.DaemonCommand.handleCommand(DaemonCommand.java:50)
    at org.asamk.signal.Main.handleCommands(Main.java:126)
    at org.asamk.signal.Main.main(Main.java:61)
[org.freedesktop.dbus.DBusConnection.disconnect()] Disconnecting DBusConnection
[org.freedesktop.dbus.Message. ()] Creating message with serial 1
[org.freedesktop.dbus.Message.append()] Appending sig: yyyy data: [66, 3, 0, 1]
[org.freedesktop.dbus.Message.append()] Appending item: 0 y 0
[org.freedesktop.dbus.Message.appendone()] 4
[org.freedesktop.dbus.Message.appendone()] Appending type: y value: 66
[org.freedesktop.dbus.Message.pad()] padding for y
[org.freedesktop.dbus.Message.pad()] 4 0 4 1
[org.freedesktop.dbus.Message.append()] Appending item: 1 y 1
[org.freedesktop.dbus.Message.appendone()] 4
[org.freedesktop.dbus.Message.appendone()] Appending type: y value: 3
[org.freedesktop.dbus.Message.pad()] padding for y
[org.freedesktop.dbus.Message.pad()] 3 1 4 1
[org.freedesktop.dbus.Message.append()] Appending item: 2 y 2
[org.freedesktop.dbus.Message.appendone()] 4
[org.freedesktop.dbus.Message.appendone()] Appending type: y value: 0
[org.freedesktop.dbus.Message.pad()] padding for y
[org.freedesktop.dbus.Message.pad()] 2 2 4 1
[org.freedesktop.dbus.Message.append()] Appending item: 3 y 3
[org.freedesktop.dbus.Message.appendone()] 4
[org.freedesktop.dbus.Message.appendone()] Appending type: y value: 1
[org.freedesktop.dbus.Message.pad()] padding for y
[org.freedesktop.dbus.Message.pad()] 1 3 4 1
[org.freedesktop.dbus.Message.append()] Appending sig: ua(yv) data: [1, [[4, [s, org.freedesktop.DBus.Local.Disconnected]], [5, [u, 0]], [6, [s, org.freedesktop.DBus.Local]], [8, [g, s]]]]
[org.freedesktop.dbus.Message.append()] Appending item: 0 u 0
[org.freedesktop.dbus.Message.appendone()] 8
[org.freedesktop.dbus.Message.appendone()] Appending type: u value: 1
[org.freedesktop.dbus.Message.pad()] padding for u
[org.freedesktop.dbus.Message.pad()] 0 4 8 4
[org.freedesktop.dbus.Message.marshallint()] Marshalled int 1 to 00 00 00 01 
[org.freedesktop.dbus.Message.append()] Appending item: 1 a 1
[org.freedesktop.dbus.Message.appendone()] 12
[org.freedesktop.dbus.Message.appendone()] Appending type: a value: [Ljava.lang.Object;@5d0a1059
[org.freedesktop.dbus.Message.pad()] padding for a
[org.freedesktop.dbus.Message.pad()] 0 4 12 4
[org.freedesktop.dbus.Message.appendone()] Appending array: [[4, [s, org.freedesktop.DBus.Local.Disconnected]], [5, [u, 0]], [6, [s, org.freedesktop.DBus.Local]], [8, [g, s]]]
[org.freedesktop.dbus.Message.pad()] padding for (
[org.freedesktop.dbus.Message.pad()] 0 4 16 8
[org.freedesktop.dbus.Message.appendone()] 16
[org.freedesktop.dbus.Message.appendone()] Appending type: ( value: [Ljava.lang.Object;@485966cc
[org.freedesktop.dbus.Message.pad()] padding for (
[org.freedesktop.dbus.Message.pad()] 0 4 16 8
[org.freedesktop.dbus.Message.appendone()] 16
[org.freedesktop.dbus.Message.appendone()] Appending type: y value: 4
[org.freedesktop.dbus.Message.pad()] padding for y
[org.freedesktop.dbus.Message.pad()] 0 4 16 1
[org.freedesktop.dbus.Message.appendone()] 17
[org.freedesktop.dbus.Message.appendone()] Appending type: v value: [Ljava.lang.Object;@1de76cc7
[org.freedesktop.dbus.Message.pad()] padding for v
[org.freedesktop.dbus.Message.pad()] 0 4 17 1
[org.freedesktop.dbus.Message.appendone()] 17
[org.freedesktop.dbus.Message.appendone()] Appending type: g value: s
[org.freedesktop.dbus.Message.pad()] padding for g
[org.freedesktop.dbus.Message.pad()] 0 4 17 1
[org.freedesktop.dbus.Message.appendone()] 20
[org.freedesktop.dbus.Message.appendone()] Appending type: s value: org.freedesktop.DBus.Local.Disconnected
[org.freedesktop.dbus.Message.pad()] padding for s
[org.freedesktop.dbus.Message.pad()] 0 3 20 4
[org.freedesktop.dbus.Message.appendone()] Appending String of length 39
[org.freedesktop.dbus.Message.marshallint()] Marshalled int 39 to 00 00 00 27 
[org.freedesktop.dbus.Message.appendone()] 64
[org.freedesktop.dbus.Message.appendone()] Appending type: ( value: [Ljava.lang.Object;@54bff557
[org.freedesktop.dbus.Message.pad()] padding for (
[org.freedesktop.dbus.Message.pad()] 0 3 64 8
[org.freedesktop.dbus.Message.appendone()] 64
[org.freedesktop.dbus.Message.appendone()] Appending type: y value: 5
[org.freedesktop.dbus.Message.pad()] padding for y
[org.freedesktop.dbus.Message.pad()] 0 3 64 1
[org.freedesktop.dbus.Message.appendone()] 65
[org.freedesktop.dbus.Message.appendone()] Appending type: v value: [Ljava.lang.Object;@593aaf41
[org.freedesktop.dbus.Message.pad()] padding for v
[org.freedesktop.dbus.Message.pad()] 0 3 65 1
[org.freedesktop.dbus.Message.appendone()] 65
[org.freedesktop.dbus.Message.appendone()] Appending type: g value: u
[org.freedesktop.dbus.Message.pad()] padding for g
[org.freedesktop.dbus.Message.pad()] 0 3 65 1
[org.freedesktop.dbus.Message.appendone()] 68
[org.freedesktop.dbus.Message.appendone()] Appending type: u value: 0
[org.freedesktop.dbus.Message.pad()] padding for u
[org.freedesktop.dbus.Message.pad()] 0 3 68 4
[org.freedesktop.dbus.Message.marshallint()] Marshalled int 0 to 00 00 00 00 
[org.freedesktop.dbus.Message.appendone()] 72
[org.freedesktop.dbus.Message.appendone()] Appending type: ( value: [Ljava.lang.Object;@5a56cdac
[org.freedesktop.dbus.Message.pad()] padding for (
[org.freedesktop.dbus.Message.pad()] 0 3 72 8
[org.freedesktop.dbus.Message.appendone()] 72
[org.freedesktop.dbus.Message.appendone()] Appending type: y value: 6
[org.freedesktop.dbus.Message.pad()] padding for y
[org.freedesktop.dbus.Message.pad()] 0 3 72 1
[org.freedesktop.dbus.Message.appendone()] 73
[org.freedesktop.dbus.Message.appendone()] Appending type: v value: [Ljava.lang.Object;@7c711375
[org.freedesktop.dbus.Message.pad()] padding for v
[org.freedesktop.dbus.Message.pad()] 0 3 73 1
[org.freedesktop.dbus.Message.appendone()] 73
[org.freedesktop.dbus.Message.appendone()] Appending type: g value: s
[org.freedesktop.dbus.Message.pad()] padding for g
[org.freedesktop.dbus.Message.pad()] 0 3 73 1
[org.freedesktop.dbus.Message.appendone()] 76
[org.freedesktop.dbus.Message.appendone()] Appending type: s value: org.freedesktop.DBus.Local
[org.freedesktop.dbus.Message.pad()] padding for s
[org.freedesktop.dbus.Message.pad()] 0 3 76 4
[org.freedesktop.dbus.Message.appendone()] Appending String of length 26
[org.freedesktop.dbus.Message.marshallint()] Marshalled int 26 to 00 00 00 1a 
[org.freedesktop.dbus.Message.appendone()] 107
[org.freedesktop.dbus.Message.appendone()] Appending type: ( value: [Ljava.lang.Object;@57cf54e1
[org.freedesktop.dbus.Message.pad()] padding for (
[org.freedesktop.dbus.Message.pad()] 0 3 107 8
[org.freedesktop.dbus.Message.pad()] 0 3 112 5
[org.freedesktop.dbus.Message.ensureBuffers()] Resizing 18
[org.freedesktop.dbus.Message.appendone()] 112
[org.freedesktop.dbus.Message.appendone()] Appending type: y value: 8
[org.freedesktop.dbus.Message.pad()] padding for y
[org.freedesktop.dbus.Message.pad()] 0 3 112 1
[org.freedesktop.dbus.Message.appendone()] 113
[org.freedesktop.dbus.Message.appendone()] Appending type: v value: [Ljava.lang.Object;@5b03b9fe
[org.freedesktop.dbus.Message.pad()] padding for v
[org.freedesktop.dbus.Message.pad()] 0 3 113 1
[org.freedesktop.dbus.Message.appendone()] 113
[org.freedesktop.dbus.Message.appendone()] Appending type: g value: g
[org.freedesktop.dbus.Message.pad()] padding for g
[org.freedesktop.dbus.Message.pad()] 0 3 113 1
[org.freedesktop.dbus.Message.appendone()] 116
[org.freedesktop.dbus.Message.appendone()] Appending type: g value: s
[org.freedesktop.dbus.Message.pad()] padding for g
[org.freedesktop.dbus.Message.pad()] 0 3 116 1
[org.freedesktop.dbus.Message.appendone()] start: 16 end: 119 length: 103
[org.freedesktop.dbus.Message.marshallint()] Marshalled int 103 to 00 00 00 67 
[org.freedesktop.dbus.Message.pad()] padding for 
[org.freedesktop.dbus.Message.pad()] 0 3 119 8
[org.freedesktop.dbus.Message.pad()] 0 3 120 1
[org.freedesktop.dbus.Message.append()] Appending sig: s data: [Disconnected]
[org.freedesktop.dbus.Message.append()] Appending item: 0 s 0
[org.freedesktop.dbus.Message.appendone()] 120
[org.freedesktop.dbus.Message.appendone()] Appending type: s value: Disconnected
[org.freedesktop.dbus.Message.pad()] padding for s
[org.freedesktop.dbus.Message.pad()] 0 3 120 4
[org.freedesktop.dbus.Message.appendone()] Appending String of length 12
[org.freedesktop.dbus.Message.marshallint()] Marshalled int 12 to 00 00 00 0c 
[org.freedesktop.dbus.Message.marshallint()] Marshalled int 17 to 00 00 00 11 
[org.freedesktop.dbus.AbstractConnection.disconnect()] Sending disconnected signal
[org.freedesktop.dbus.Message. ()] Creating message with serial 2
[org.freedesktop.dbus.Message.append()] Appending sig: yyyy data: [66, 4, 0, 1]
[org.freedesktop.dbus.Message.append()] Appending item: 0 y 0
[org.freedesktop.dbus.Message.appendone()] 4
[org.freedesktop.dbus.Message.appendone()] Appending type: y value: 66
[org.freedesktop.dbus.Message.pad()] padding for y
[org.freedesktop.dbus.Message.pad()] 4 0 4 1
[org.freedesktop.dbus.Message.append()] Appending item: 1 y 1
[org.freedesktop.dbus.Message.appendone()] 4
[org.freedesktop.dbus.Message.appendone()] Appending type: y value: 4
[org.freedesktop.dbus.Message.pad()] padding for y
[org.freedesktop.dbus.Message.pad()] 3 1 4 1
[org.freedesktop.dbus.Message.append()] Appending item: 2 y 2
[org.freedesktop.dbus.Message.appendone()] 4
[org.freedesktop.dbus.Message.appendone()] Appending type: y value: 0
[org.freedesktop.dbus.Message.pad()] padding for y
[org.freedesktop.dbus.Message.pad()] 2 2 4 1
[org.freedesktop.dbus.Message.append()] Appending item: 3 y 3
[org.freedesktop.dbus.Message.appendone()] 4
[org.freedesktop.dbus.Message.appendone()] Appending type: y value: 1
[org.freedesktop.dbus.Message.pad()] padding for y
[org.freedesktop.dbus.Message.pad()] 1 3 4 1
[org.freedesktop.dbus.Message.append()] Appending sig: ua(yv) data: [3, [[1, [o, /]], [2, [s, org.freedesktop.DBus.Local]], [3, [s, Disconnected]]]]
[org.freedesktop.dbus.Message.append()] Appending item: 0 u 0
[org.freedesktop.dbus.Message.appendone()] 8
[org.freedesktop.dbus.Message.appendone()] Appending type: u value: 3
[org.freedesktop.dbus.Message.pad()] padding for u
[org.freedesktop.dbus.Message.pad()] 0 4 8 4
[org.freedesktop.dbus.Message.marshallint()] Marshalled int 3 to 00 00 00 03 
[org.freedesktop.dbus.Message.append()] Appending item: 1 a 1
[org.freedesktop.dbus.Message.appendone()] 12
[org.freedesktop.dbus.Message.appendone()] Appending type: a value: [Ljava.lang.Object;@3c7f66c4
[org.freedesktop.dbus.Message.pad()] padding for a
[org.freedesktop.dbus.Message.pad()] 0 4 12 4
[org.freedesktop.dbus.Message.appendone()] Appending array: [[1, [o, /]], [2, [s, org.freedesktop.DBus.Local]], [3, [s, Disconnected]]]
[org.freedesktop.dbus.Message.pad()] padding for (
[org.freedesktop.dbus.Message.pad()] 0 4 16 8
[org.freedesktop.dbus.Message.appendone()] 16
[org.freedesktop.dbus.Message.appendone()] Appending type: ( value: [Ljava.lang.Object;@194bcebf
[org.freedesktop.dbus.Message.pad()] padding for (
[org.freedesktop.dbus.Message.pad()] 0 4 16 8
[org.freedesktop.dbus.Message.appendone()] 16
[org.freedesktop.dbus.Message.appendone()] Appending type: y value: 1
[org.freedesktop.dbus.Message.pad()] padding for y
[org.freedesktop.dbus.Message.pad()] 0 4 16 1
[org.freedesktop.dbus.Message.appendone()] 17
[org.freedesktop.dbus.Message.appendone()] Appending type: v value: [Ljava.lang.Object;@17497425
[org.freedesktop.dbus.Message.pad()] padding for v
[org.freedesktop.dbus.Message.pad()] 0 4 17 1
[org.freedesktop.dbus.Message.appendone()] 17
[org.freedesktop.dbus.Message.appendone()] Appending type: g value: o
[org.freedesktop.dbus.Message.pad()] padding for g
[org.freedesktop.dbus.Message.pad()] 0 4 17 1
[org.freedesktop.dbus.Message.appendone()] 20
[org.freedesktop.dbus.Message.appendone()] Appending type: o value: /
[org.freedesktop.dbus.Message.pad()] padding for o
[org.freedesktop.dbus.Message.pad()] 0 3 20 4
[org.freedesktop.dbus.Message.appendone()] Appending String of length 1
[org.freedesktop.dbus.Message.marshallint()] Marshalled int 1 to 00 00 00 01 
[org.freedesktop.dbus.Message.appendone()] 26
[org.freedesktop.dbus.Message.appendone()] Appending type: ( value: [Ljava.lang.Object;@f0da945
[org.freedesktop.dbus.Message.pad()] padding for (
[org.freedesktop.dbus.Message.pad()] 0 3 26 8
[org.freedesktop.dbus.Message.pad()] 0 3 32 6
[org.freedesktop.dbus.Message.appendone()] 32
[org.freedesktop.dbus.Message.appendone()] Appending type: y value: 2
[org.freedesktop.dbus.Message.pad()] padding for y
[org.freedesktop.dbus.Message.pad()] 0 3 32 1
[org.freedesktop.dbus.Message.appendone()] 33
[org.freedesktop.dbus.Message.appendone()] Appending type: v value: [Ljava.lang.Object;@4803b726
[org.freedesktop.dbus.Message.pad()] padding for v
[org.freedesktop.dbus.Message.pad()] 0 3 33 1
[org.freedesktop.dbus.Message.appendone()] 33
[org.freedesktop.dbus.Message.appendone()] Appending type: g value: s
[org.freedesktop.dbus.Message.pad()] padding for g
[org.freedesktop.dbus.Message.pad()] 0 3 33 1
[org.freedesktop.dbus.Message.appendone()] 36
[org.freedesktop.dbus.Message.appendone()] Appending type: s value: org.freedesktop.DBus.Local
[org.freedesktop.dbus.Message.pad()] padding for s
[org.freedesktop.dbus.Message.pad()] 0 3 36 4
[org.freedesktop.dbus.Message.appendone()] Appending String of length 26
[org.freedesktop.dbus.Message.marshallint()] Marshalled int 26 to 00 00 00 1a 
[org.freedesktop.dbus.Message.appendone()] 67
[org.freedesktop.dbus.Message.appendone()] Appending type: ( value: [Ljava.lang.Object;@ffaa6af
[org.freedesktop.dbus.Message.pad()] padding for (
[org.freedesktop.dbus.Message.pad()] 0 3 67 8
[org.freedesktop.dbus.Message.pad()] 0 3 72 5
[org.freedesktop.dbus.Message.ensureBuffers()] Resizing 16
[org.freedesktop.dbus.Message.appendone()] 72
[org.freedesktop.dbus.Message.appendone()] Appending type: y value: 3
[org.freedesktop.dbus.Message.pad()] padding for y
[org.freedesktop.dbus.Message.pad()] 0 3 72 1
[org.freedesktop.dbus.Message.appendone()] 73
[org.freedesktop.dbus.Message.appendone()] Appending type: v value: [Ljava.lang.Object;@53ce1329
[org.freedesktop.dbus.Message.pad()] padding for v
[org.freedesktop.dbus.Message.pad()] 0 3 73 1
[org.freedesktop.dbus.Message.appendone()] 73
[org.freedesktop.dbus.Message.appendone()] Appending type: g value: s
[org.freedesktop.dbus.Message.pad()] padding for g
[org.freedesktop.dbus.Message.pad()] 0 3 73 1
[org.freedesktop.dbus.Message.appendone()] 76
[org.freedesktop.dbus.Message.appendone()] Appending type: s value: Disconnected
[org.freedesktop.dbus.Message.pad()] padding for s
[org.freedesktop.dbus.Message.pad()] 0 3 76 4
[org.freedesktop.dbus.Message.appendone()] Appending String of length 12
[org.freedesktop.dbus.Message.marshallint()] Marshalled int 12 to 00 00 00 0c 
[org.freedesktop.dbus.Message.appendone()] start: 16 end: 93 length: 77
[org.freedesktop.dbus.Message.marshallint()] Marshalled int 77 to 00 00 00 4d 
[org.freedesktop.dbus.Message.pad()] padding for 
[org.freedesktop.dbus.Message.pad()] 0 3 93 8
[org.freedesktop.dbus.Message.pad()] 0 3 96 3
[org.freedesktop.dbus.AbstractConnection.handleMessage()] Handling incoming signal: Disconnected(0,3) { Path=>/, Interface=>org.freedesktop.DBus.Local, Member=>Disconnected } { }
[org.freedesktop.dbus.AbstractConnection.disconnect()] Disconnecting Abstract Connection
org.freedesktop.dbus.exceptions.DBusException: Failed to connect to bus Failed to auth
    at org.freedesktop.dbus.DBusConnection. (Unknown Source)
    at org.freedesktop.dbus.DBusConnection.getConnection(Unknown Source)
    at org.asamk.signal.commands.DaemonCommand.handleCommand(DaemonCommand.java:50)
    at org.asamk.signal.Main.handleCommands(Main.java:126)
    at org.asamk.signal.Main.main(Main.java:61)
``` 
 Troubleshooting Java Exceptions 
 org.freedesktop.dbus.exceptions.DBusException: Cannot Resolve Session Bus Address 
 console
$ signal-cli -u +00123456789 daemon
org.freedesktop.dbus.exceptions.DBusException: Cannot Resolve Session Bus Address
        at org.freedesktop.dbus.DBusConnection.getConnection(DBusConnection.java:267)
        at org.asamk.signal.commands.DaemonCommand.handleCommand(DaemonCommand.java:50)
        at org.asamk.signal.Main.handleCommands(Main.java:126)
        at org.asamk.signal.Main.main(Main.java:61) 
 Solution 
 The DISPLAY environment variable has to omit the leading "":"", e.g., ""0"" instead of "":0"". Otherwise, signal-cli tries to open an invalid D-BUS session bus file, e.g.,  ~/.dbus/session-bus/0123456789abcdef0123456789abcdef-:0  instead of  ~/.dbus/session-bus/0123456789abcdef0123456789abcdef-0 . 
 org.freedesktop.dbus.exceptions.DBusException: Failed to connect to bus unknown address type 'unix 
 console
$ DISPLAY=0 signal-cli -u +00123456789 daemon
org.freedesktop.dbus.exceptions.DBusException: Failed to connect to bus unknown address type 'unix
        at org.freedesktop.dbus.DBusConnection.<init>(DBusConnection.java:304)
        at org.freedesktop.dbus.DBusConnection.getConnection(DBusConnection.java:282)
        at org.asamk.signal.commands.DaemonCommand.handleCommand(DaemonCommand.java:50)
        at org.asamk.signal.Main.handleCommands(Main.java:126)
        at org.asamk.signal.Main.main(Main.java:61) 
 Solution 
 Unquote the value of the  DBUS_SESSION_BUS_ADDRESS  variable in  ~/.dbus/session-bus/0123456789abcdef0123456789abcdef-0 . For example: 
 DBUS_SESSION_BUS_ADDRESS='unix:path=/tmp/dbus-ABCDEFGHIJ,guid=fedcba9876543210fedcba9876543210' 
 should be 
 DBUS_SESSION_BUS_ADDRESS=unix:path=/tmp/dbus-ABCDEFGHIJ,guid=fedcba9876543210fedcba9876543210 
 Missing native library dependency for dbus service: no unix-java in java.library.path 
 console
$ export DISPLAY=0 
$ signal-cli -u +00123456789 daemon
Missing native library dependency for dbus service: no unix-java in java.library.path 
 signal-cli  cannot find the share library provided by libmatthew. A proper path can be set via  JAVA_OPTS : 
 sh
export JAVA_OPTS=""-Djava.library.path=/usr/local/lib"" 
 General notes 
 Use the following command to list all available message buses: 
 ``` 
 Source: https://unix.stackexchange.com/a/46309 
 dbus-send --print-reply --dest=org.freedesktop.DBus /org/freedesktop/DBus org.freedesktop.DBus.ListNames
``` 
 Send a Signal message via the signal-cli D-Bus daemon: 
 dbus-send --session --type=method_call --print-reply --dest=""org.asamk.Signal"" /org/asamk/Signal org.asamk.Signal.sendMessage string:MessageText array:string: string:RECIPIENT \`\`\` 
 qdbus  (from  devel/qt5-qdbus ) can be used to view avaiable interfaces to D-Bus. 
 Resources 
 
 https://github.com/AsamK/signal-cli/wiki/DBus-service 
 https://unix.stackexchange.com/questions/194308/d-bus-authentication-and-authorization/200104 
 D-Bus service files: 
 http://kkaempf.blogspot.com/2009/03/d-bus-service-on-demand.html 
 https://stackoverflow.com/questions/19453507/how-to-create-a-dbus-service 
 
 
 https://gitlab.freedesktop.org/dbus/dbus-java/commit/129eed36e2cc5e55c2a2ab057e2eccbe0e23c395 
 
  vim: softtabstop=8 shiftwidth=8 tabstop=8 noexpandtab
 Jekyll::AssetPipeline 
 This gem is an external asset pipeline for Jekyll projects. It supports  Sass  for CSS, and ES6 for JavaScript (via  Babel ). It also runs  PurgeCSS  to remove unnecessary CSS and  Uglify  to compress JavaScript. 
 Installation 
 Add this line to your application's  Gemfile : 
 rb
gem 'jekyll-asset-pipeline', git: 'https://github.com/crdschurch/jekyll-asset-pipeline', tag: '0.0.1' 
 And then execute: 
 $ bundle exec jekyll-asset-pipeline install
 
 This script does the following: 
 
 Installs  purgecss  globally via NPM. 
 Installs local JS package dependencies, which will create a  package.json  file if it doesn't already exit. 
 Copies  gulpfile.js  into the project root. This is the configuration for the build process, which you're welcome to customize as necessary. 
 Copies  purgecss.config.json  into the project root. This is the configuration for PurgeCSS, which you are also welcome to customize as necessary. 
 
 The process will also likely create a  package-lock.json  file and a  node_modules  directory. It is recommended that you add the  node_modules  directory to your  .gitignore  file. 
 Introduction 
 The build process has three primary components: 
 
 Gulp.js : The build process. The build logic and configuration can be found in  gulpfile.js , which is copied into the root of your project during installation. 
 A series of Jekyll hooks that control when (and whether or not) to run the asset build when the jekyll build is run. 
 Jekyll tags to support resolving the appropriate filename for your  <link>  and  <script>  tags. 
 
 Usage 
 The build is run via Gulp.js (which is run via an NPM script). This occurs automatically as part of the Jekyll build process ( jekyll build  or  jekyll serve ). 
 The build uses  _assets/stylesheets  as the source directory for (S)CSS files and  _assets/javascripts  as the source for JS files. (More on each of these in their respective sections, below.) 
 The build will run if any of the following conditions are true: 
 
 BUILD_ASSETS  environment variable is set to  true  (i.e.  BUILD_ASSETS=true jekyll [build/serve] ). 
 There are no  .js  or  .css  files in the build directory. 
 A  .js  or  .scss  file within the source directory has been modified since the last time the build was run. 
 
 Liquid/HTML Tags 
 Within a Jekyll view (HTML file), you can use the custom tags to load the appropriate file(s): 
 liquid
{% javascript_link_tag application %}
{% stylesheet_link_tag application %} 
 Notice the lack of file extension. 
 The  javascript_link_tag  accepts a second argument for which you can add an  async  or  defer  attribute to the script tag. 
 Configuration 
 Aside from the environment variable mentioned above, you have the option to adjust one value in your site's  _config.yml  file. 
 
 asset_dest  (default:  assets ): The directory within your build directory in which to house the built assets. 
 
 CSS 
 The CSS builds one sass source file ( _assets/stylesheets/application.scss ) and puts the compiled output in  _site/assets/ . There is nothing to configure, as Sass supports importing partials by default. 
 JavaScript 
 JavaScript is more configurable that the CSS. All JS build configuration can be found in  _assets/javascripts/config.js . This file is to export an array of config objects, where each object represents a built file with the following options: 
 
 name  (Required): The name of the file (sans  .js  extension). 
 deps : An array of vendors files (dependencies, sans  .js  extension) to prepend to the built file. 
 files : An array of files (sans  .js  extension) to process with Babel and then append to the built file. 
 
 The resulting file(s) will be placed in  _site/assets/ . 
 Take the following example: 
 js
module.exports = [
  {
    name: 'application',
    deps: [
      'vendor/jquery.min',
      'vendor/lodash.min'
    ],
    files: [
      'components/header'
    ]
  }
] 
 Given the config above,  _assets/javascripts/vendor/jquery.min.js  (notice  .js  extension is automatically added) and  _assets/javascripts/vendor/lodash.min.js  will be prepended to a temporary file, while  _assets/javascripts/components/header.js  will be processed with Babel (to support older browsers), minified, and appended to the same file. This file will eventually become named  application.js  (because of the  name  option in the config) and will be placed in  _site/assets/ . 
 Troubleshooting 
 If you start the Jekyll server and there are missing styles or your scripts are working, it's likely that the Jekyll asset tags are looking for a different filename than what exists in your build directory ( _site , by default). There are two quick options to fix: 
 
 Delete the build directory and restart the server (or re-run the build). 
 Save a file in your assets source directory. The next time the project builds (which would be instantaneously if the server is already running) the assets will regenerate. 
 
 Contributing 
 Bug reports and pull requests are welcome on GitHub at https://github.com/crdschurch/jekyll-asset-pipeline. This project is intended to be a safe, welcoming space for collaboration, and contributors are expected to adhere to the  Contributor Covenant  code of conduct. 
 Code of Conduct 
 Everyone interacting in the Jekyll::Asset::Pipeline project’s codebases, issue trackers, chat rooms and mailing lists is expected to follow the  code of conduct . pyspark-test mpds-orange 
 Overall system design 
 Autoscaling system 
 There are 2 Kafka topics used by the autoscaling system: 
 
 Metric  for message rate metrics 
 Prediction  for output results of the prediction models 
 
 Metrics 
 See the  Prometheus Postman collection . 
 
 Example of a Prometheus API request and response: 

Here's the request. It is not a single HTTP request, but a separate request for each metric. Although it is technically possible to get all the metrics at once (as per [the documentation](https://prometheus.io/docs/prometheus/latest/querying/basics/)), we would have to do aggregations on our own. Having multiple queries is not a problem, however. We can simply specify a `time` field in the HTTP request to retrieve a consistent set of metrics.

```sh
request() {
    address=""http://prometheus:30090""
    endpoint=""/api/v1/query""
    url=""${address}${endpoint}""
    time=""2021-02-08T10:10:51.781Z""

    curl -Ss -X POST -F query=""$1"" -F time=""$time"" ""$url"" | jq
}

request ""avg(avg by (operator_id) (flink_taskmanager_job_latency_source_id_operator_id_operator_subtask_index_latency{quantile=\""0.95\""}))""
request ""avg(kafka_server_brokertopicmetrics_total_messagesinpersec_count)""
request kafka_controller_kafkacontroller_controllerstate_value
```

The response:

```
{
  ""status"": ""success"",
  ""data"": {
    ""resultType"": ""vector"",
    ""result"": [
      {
        ""metric"": {},
        ""value"": [
          1612779051.781,
          ""181.43333333333334""
        ]
      }
    ]
  }
}
{
  ""status"": ""success"",
  ""data"": {
    ""resultType"": ""vector"",
    ""result"": [
      {
        ""metric"": {},
        ""value"": [
          1612779051.781,
          ""112629192.33333334""
        ]
      }
    ]
  }
}
{
  ""status"": ""success"",
  ""data"": {
    ""resultType"": ""vector"",
    ""result"": [
      {
        ""metric"": {
          ""__name__"": ""kafka_controller_kafkacontroller_controllerstate_value"",
          ""app_kubernetes_io_component"": ""kafka"",
          ""app_kubernetes_io_instance"": ""mpds"",
          ""app_kubernetes_io_managed_by"": ""Helm"",
          ""app_kubernetes_io_name"": ""kafka"",
          ""controller_revision_hash"": ""kafka-7dc6cd8b54"",
          ""helm_sh_chart"": ""kafka-11.8.2"",
          ""instance"": ""10.1.0.10:5556"",
          ""job"": ""kubernetes-pods"",
          ""kubernetes_namespace"": ""default"",
          ""kubernetes_pod_name"": ""kafka-1"",
          ""statefulset_kubernetes_io_pod_name"": ""kafka-1""
        },
        ""value"": [
          1612779051.781,
          ""0""
        ]
      },
      {
        ""metric"": {
          ""__name__"": ""kafka_controller_kafkacontroller_controllerstate_value"",
          ""app_kubernetes_io_component"": ""kafka"",
          ""app_kubernetes_io_instance"": ""mpds"",
          ""app_kubernetes_io_managed_by"": ""Helm"",
          ""app_kubernetes_io_name"": ""kafka"",
          ""controller_revision_hash"": ""kafka-7dc6cd8b54"",
          ""helm_sh_chart"": ""kafka-11.8.2"",
          ""instance"": ""10.1.1.6:5556"",
          ""job"": ""kubernetes-pods"",
          ""kubernetes_namespace"": ""default"",
          ""kubernetes_pod_name"": ""kafka-0"",
          ""statefulset_kubernetes_io_pod_name"": ""kafka-0""
        },
        ""value"": [
          1612779051.781,
          ""0""
        ]
      },
      {
        ""metric"": {
          ""__name__"": ""kafka_controller_kafkacontroller_controllerstate_value"",
          ""app_kubernetes_io_component"": ""kafka"",
          ""app_kubernetes_io_instance"": ""mpds"",
          ""app_kubernetes_io_managed_by"": ""Helm"",
          ""app_kubernetes_io_name"": ""kafka"",
          ""controller_revision_hash"": ""kafka-7dc6cd8b54"",
          ""helm_sh_chart"": ""kafka-11.8.2"",
          ""instance"": ""10.1.2.6:5556"",
          ""job"": ""kubernetes-pods"",
          ""kubernetes_namespace"": ""default"",
          ""kubernetes_pod_name"": ""kafka-2"",
          ""statefulset_kubernetes_io_pod_name"": ""kafka-2""
        },
        ""value"": [
          1612779051.781,
          ""0""
        ]
      }
    ]
  }
}
```
 
 The following metrics are potentially interesting, but are not available available at this moment in Prometheus:
- Network in/out (to do)
- CPU Utilization (to do, probably available via Kubernetes API's)
- Memory Usage (to do, probably avialable via Kubernetes API's) 
 Prediction models 
 There are two prediction models available at the moment: a long-term one, and a short-term one. Both models are predicting the load based on the Kafka message rates. OpenConnect FreeBSD Daemon 
 A service daemon for the FreeBSD rc(8) framework. It lets the user configure
OpenConnect VPN in rc.conf(5) and use the standard FreeBSD tools to control the
daemon. 
 Features:
- Support for starting multiple OpenConnect services.
- Support for running arbitrary commands for OTP-based authentication. 
 See the service files for usage details. 
 Installation 
 ```console
% make
% su 
 make install 
 ``` 
 Examples 
 Configure and connect to a VPN with OpenConnect: 
 ```console 
 sysrc openconnect_myvpn_enable=""YES"" 
 sysrc openconnect_myvpn_username=""charlie.root"" 
 sysrc openconnect_myvpn_server=""vpn.example.org"" 
 service openconnect setpassword myvpn 
 Password (openconnect_myvpn): 
 service openconnect start myvpn 
 ``` 
 Enable ""myvpn"" OpenConnect service to run on boot: 
 ```console 
 sysrc openconnect_services+=""myvpn"" 
 ``` 
 Enable verbose output (e.g., for debugging): 
 ```console 
 sysrc openconnect_myvpn_args+=""--verbose"" 
 ``` 
 Set the password manually: 
 ```console 
 mkdir -p /usr/local/etc/openconnect/passwords 
 (umask 077 && echo ""password"" > /usr/local/etc/openconnect/passwords/myvpn) 
 ``` 
 License 
 The 2-Clause BSD license. Mantra 
 Mantra is a CLI for previewing  mandoc  manual pages with live auto-reload in a pager. 
 Installation 
 Dependencies: 
 
 entr  (for watching file changes) 
 less  (Less is the only supported pager at the moment) 
 tmux  (for scripting pager interactions) 
 
 sh
make all
PREFIX=/usr/local make install 
 Usage 
 sh
mantra style.9 
 License 
 BSD 2-Clause ""Simplified"" License"
Maratyszcza,"BLISBench 
 Benchmark of matrix-matrix multiplication implementations for Web browsers 
 This project visualizes performance of matrix-matrix multiplication (GEMM functions in BLAS) for the following implementations: 
 
  Naive  DGEMM / ZGEMM  in JavaScript with JS Arrays 
  Naive  SGEMM / DGEMM / CGEMM / ZGEMM  in JavaScript with Typed Arrays 
  Naive  SGEMM / DGEMM / CGEMM / ZGEMM  in Asm.js (compiled from C with Emscripten) 
  Naive  SGEMM / DGEMM / CGEMM / ZGEMM  in Portable Native Client (compiled from C) 
  BLIS-provided  SGEMM / DGEMM / CGEMM / ZGEMM  in Asm.js (compiled from C with Emscripten) 
  BLIS-provided  SGEMM / DGEMM / CGEMM / ZGEMM  in Portable Native Client (compiled from C) 
 
 Additional Resources 
 BLIS for the Web: HPC in a Web browser  presentation on  BLIS Retreat 2014 
 References 
 
 BLIS  library for basic linear algebra operations. 
 Emscripten  C/C++-to-JavaScript compiler. 
 Portable Native Client  technology for running C/C++ code in a browser. 
 Asm.js  subset of JavaScript. 
 Linear Algebra - Foundations to Frontiers Demos 
 Live demos for Robert van de Geijn's and Maggie Myers'  Linear Algebra - Foundations to Frontiers  MOOC course. 
 
 Live benchmark of matrix-matrix multiplication  demonstrates performance patterns of naive vs blocked vs state-of-the-art ( BLIS ) matrix-matrix multiplication implementations. 
 Live pointer-chasing benchmark  measures random access time for arrays of different size. When arrays are too big to fit into cache, the access time dramatically increases. 
 pthreadpool 
 
 
 pthreadpool  is a portable and efficient thread pool implementation.
It provides similar functionality to  #pragma omp parallel for , but with additional features. 
 Features: 
 
 C interface (C++-compatible). 
 1D-6D loops with step parameters. 
 Run on user-specified or auto-detected number of threads. 
 Work-stealing scheduling for efficient work balancing. 
 Wait-free synchronization of work items. 
 Compatible with Linux (including Android), macOS, iOS, Windows, Emscripten environments. 
 100% unit tests coverage. 
 Throughput and latency microbenchmarks. 
 
 Example 
 The following example demonstates using the thread pool for parallel addition of two arrays: 
 ```c
static void add_arrays(struct array_addition_context* context, size_t i) {
  context->sum[i] = context->augend[i] + context->addend[i];
} 
 define ARRAY_SIZE 4 
 int main() {
  double augend[ARRAY_SIZE] = { 1.0, 2.0, 4.0, -5.0 };
  double addend[ARRAY_SIZE] = { 0.25, -1.75, 0.0, 0.5 };
  double sum[ARRAY_SIZE]; 
 pthreadpool_t threadpool = pthreadpool_create(0);
  assert(threadpool != NULL); 
 const size_t threads_count = pthreadpool_get_threads_count(threadpool);
  printf(""Created thread pool with %zu threads\n"", threads_count); 
 struct array_addition_context context = { augend, addend, sum };
  pthreadpool_parallelize_1d(threadpool,
    (pthreadpool_task_1d_t) add_arrays,
    (void ) &context,
    ARRAY_SIZE,
    PTHREADPOOL_FLAG_DISABLE_DENORMALS /  flags */); 
 pthreadpool_destroy(threadpool);
  threadpool = NULL; 
 printf(""%8s\t%.2lf\t%.2lf\t%.2lf\t%.2lf\n"", ""Augend"",
    augend[0], augend[1], augend[2], augend[3]);
  printf(""%8s\t%.2lf\t%.2lf\t%.2lf\t%.2lf\n"", ""Addend"",
    addend[0], addend[1], addend[2], addend[3]);
  printf(""%8s\t%.2lf\t%.2lf\t%.2lf\t%.2lf\n"", ""Sum"",
    sum[0], sum[1], sum[2], sum[3]); 
 return 0;
}
``` WebRunner (PeachPy.IO backend) 
 WebRunner is a service to execute user-supplied untrusted machine code on your server without compromising its security. 
 Key features: 
 
 REST API (i.e. you communicate with the service through stateless HTTP requests) 
 Built-in loader for ELF object files 
 Sandboxing of untrusted code through  seccomp-bpf  mechanism 
 Benchmarking and analyzing the code with hardware event counters. 
 Self-check command to support automation of service downtime  
 Extendable set of supported kernels 
 
 WebRunner dependencies 
 Required dependecies 
 
 Linux kernel >= 3.17 
 Python 2.7 
 Ninja  build system ( sudo apt-get install ninja-build ) 
 ninja-syntax  module ( sudo pip install ninja-syntax ) 
 
 Recommended dependecies 
 
 systemd (WebRunner includes service configuration only for systemd) 
 Ubuntu 15.10 (WebRunner was tested only on this distribution) 
 
 Optional dependecies 
 
 PeachPy  (required to run  the example ) 
 
 Building WebRunner 
 Configure and compile: 
 bash
./configure.py
ninja 
 Recommended:  install WebRunner to  /usr/sbin/webrunner  and register as a  systemd  service: 
 bash
sudo ninja install 
 After installation you can start the service with  sudo ninja start  and terminate it with  sudo ninja stop 
 Alternative:  run WebRunner without installation: 
 bash
./webrunner # webrunner -h to list options 
 REST API 
 WebRunner commands must follow the pattern  http://server[:port]/machine-id/command[?query] 
 
 machine-id  is an arbitrary string. It is parsed, but ignored by the WebRunner. 
 command  is one of the supported commands ( monitor  or  run ). 
 query  is an optional query string with command parameters. 
 
 monitor  command 
 The  monitor  command is used to check server status. 
 HTTP request 
 
 
 Method:  HEAD 
 
 
 URL:  http://server[:port]/machine-id/monitor 
 
 
 HTTP response 
 A server would respond HTTP status ok 200 (OK) to this command. 
 Example 
 bash
curl --head ""http://localhost:8081/local/monitor"" 
 run  command 
 The  run  command is used to benchmark and analyze a function in an ELF object. The ELF object must be sent in the request body. 
 HTTP request 
 
 
 Method:  POST 
 
 
 Content-Type:  application/octet-stream 
 
 
 URL:  http://server[:port]/machine-id/run?kernel=kernel-name&[param1=value1&param2=value2&...] 
 
 
 The  kernel  parameter specifies kernel type. Query parameters after it depend on the kernel type and specify parameters of the kernel run. Look at XML specifications in the  /src/kernels  directory for permitted kernel types and their parameters. 
 HTTP response 
 The server would respond with a line of names of hardware performance counters and their values (one per line) 
 Example 
 bash
wget --header=""Content-Type:application/octet-stream"" --post-file=sdot.o \
  ""http://localhost:8081/local/run?kernel=sdot&n=10000&incx=1&incy=2"" Caffe 
 
 
 Caffe is a deep learning framework made with expression, speed, and modularity in mind.
It is developed by the Berkeley Vision and Learning Center ( BVLC ) and community contributors. 
 Check out the  project site  for all the details like 
 
 DIY Deep Learning for Vision with Caffe 
 Tutorial Documentation 
 BVLC reference models  and the  community model zoo 
 Installation instructions 
 
 and step-by-step examples. 
 
 Please join the  caffe-users group  or  gitter chat  to ask questions and talk about methods and models.
Framework development discussions and thorough bug reports are collected on  Issues . 
 Happy brewing! 
 License and Citation 
 Caffe is released under the  BSD 2-Clause license .
The BVLC reference models are released for unrestricted use. 
 Please cite Caffe in your publications if it helps your research: 
 @article{jia2014caffe,
  Author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
  Journal = {arXiv preprint arXiv:1408.5093},
  Title = {Caffe: Convolutional Architecture for Fast Feature Embedding},
  Year = {2014}
}
 FPplus 
 Scientific library for high-precision computations and research 
 FPplus was originally developed for a research project on instructions to accelerate high-precision computations, but it is also useful as a general-purpose library. FPplus features: 
 
 Header-only library for error-free transforms and double-double computations 
 Implements error-free addition, multiplication, and fused multiply-add 
 Implements double-double addition and multiplication in multiple variants 
 Compatible with C99, C++, OpenCL, and CUDA 
 Special versions of error-free transforms in SIMD intrinsics: 
 x86 SIMD (128-bit and 256-bit AVX + FMA, 512-bit wide MIC and AVX-512) 
 IBM VSX (POWER 7 and POWER 8) and QPX (Blue Gene/Q) 
 ARMv8 SIMD 
 Extensive documentation with references to scientific literature 
 Testsuite based on  MPFR  and  Google Test 
 Examples and code-generators for high-precision algorithms: 
 Polynomial evaluation with compensated Horner scheme 
 Compensated dot product algorithm 
 Inner kernel of matrix multiplication (GEMM) operation in double-double precision 
 
 Requirements 
 CPU targets: 
 
 gcc-compatible compiler (tested on gcc, clang and icc) 
 Hardware FMA support 
 Precise floating-point semantics 
 No  -ffast-math  option when compiling with  gcc  or  clang 
 -fp-model precise  when compiling with  icc 
 
 OpenCL targets: 
 
 cl_khr_fp64 ,  cl_amd_fp64 , or  cl_APPLE_fp64_basic_ops  extension 
 Hardware FMA support ( FP_FAST_FMA  must be defined by OpenCL compiler) 
 Precise floating-point semantics 
 No  -cl-fast-relaxed-math  option 
 
 CUDA targets: 
 
 Compute capability 2.0 or higher 
 
 Using FPplus 
 ```c 
 include  
 ``` 
 Publications 
 Marat Dukhan, Richard Vuduc and Jason Riedy  ""Wanted: Floating-Point Add Round-off Error instruction"" . arXiv preprint 1603.00491 (2016) 
 Acknowledgements 
 
 
 The library was developed by  Marat Dukhan  as a research project at  Richard Vuduc 's HPC Garage lab in the Georgia Institute of Technology, College of Computing, School of Computational Science and Engineering. FPplus is based on algorithms in  Handbook of Floating-Point Arithmetics ,  David Bailey 's QD library, the works of  Jonathan Shewchuk ,  Theodorus Dekker ,  Donald Knuth , and  Sylvie Boldo and Jean-Michel Muller . We thank  Jason Riedy  for his feedback and support. 
 This material is based upon work supported by the U.S. National Science Foundation (NSF) Award Number 1339745 and the U.S. Dept. of Energy (DOE), Office of Science, Advanced Scientific Computing Research under award DE-FC02-10ER26006/DE-SC0004915. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of NSF or DOE. 
 NNPACK 
 
 
 NNPACK is an acceleration package for neural network computations. NNPACK aims to provide high-performance implementations of convnet layers for multi-core CPUs. 
 NNPACK is not intended to be directly used by machine learning researchers; instead it provides low-level performance primitives leveraged in leading deep learning frameworks, such as  PyTorch ,  Caffe2 ,  MXNet , 
 tiny-dnn ,  Caffe ,  Torch , and  Darknet . 
 Platforms and requirements 
 | Environment  | Architecture  | CPU requirements                 |
| ------------ | ------------- | -------------------------------- |
| Linux        | x86-64        | AVX2 and 3-level cache hierarchy |
| Linux        | ARM           | NEON                             |
| Linux        | ARM64         |                                  |
| macOS        | x86-64        | AVX2 and 3-level cache hierarchy |
| Android      | ARM           | NEON                             |
| Android      | ARM64         |                                  |
| Android      | x86           |                                  |
| Android      | x86-64        |                                  |
| iOS          | ARM           |                                  |
| iOS          | ARM64         |                                  |
| Emscripten   | Asm.js        |                                  |
| Emscripten   | WebAssembly   |                                  | 
 Features 
 
 Multiple algorithms for convolutional layers: 
 Fast convolution based on Fourier transform (for kernels up to 16x16 without stride) 
 Fast convolution based on Winograd transform (for 3x3 kernels without stride) 
 Implicit matrix-matrix multiplication algorithm (no limitations) 
 Direct convolution algorithm (for 1x1 kernels without stride) 
 Multi-threaded SIMD-aware implementations of neural network layers 
 Implemented in C99 and Python without external dependencies 
 Extensive coverage with unit tests 
 
 Layers 
 
 Convolutional layer 
 Inference-optimized forward propagation ( nnp_convolution_inference ) 
 Training-optimized forward propagation ( nnp_convolution_output ) 
 Training-optimized backward input gradient update ( nnp_convolution_input_gradient ) 
 Training-optimized backward kernel gradient update ( nnp_convolution_kernel_gradient ) 
 Fully-connected layer 
 Inference-optimized forward propagation ( nnp_fully_connected_inference  and  nnp_fully_connected_inference_f16f32  version for FP16 weights) 
 Training-optimized forward propagation ( nnp_fully_connected_output ) 
 Max pooling layer 
 Forward propagation, both for training and inference, ( nnp_max_pooling_output ) 
 ReLU layer (with parametrized negative slope) 
 Forward propagation, both for training and inference, optionally in-place, ( nnp_relu_output ) 
 Backward input gradient update ( nnp_relu_input_gradient ) 
 Softmax layer 
 Forward propagation, both for training and inference, optionally in-place ( nnp_softmax_output ) 
 
 Building 
 For most users, the recommended way to build NNPACK is through CMake: 
 bash
mkdir build
cd build
cmake -G Ninja ..
ninja 
 Note: if  ninja  is not available on your system, configure without  -G Ninja , and use  make  instead of  ninja . 
 Building NNPACK - Using vcpkg 
 You can download and install NNPACK using the  vcpkg  dependency manager: 
 git clone https://github.com/Microsoft/vcpkg.git
cd vcpkg
./bootstrap-vcpkg.sh
./vcpkg integrate install
./vcpkg install NNPACK
 
 The NNPACK port in vcpkg is kept up to date by Microsoft team members and community contributors. If the version is out of date, please  create an issue or pull request  on the vcpkg repository. 
 Cross-compilation for Android 
 To cross-compile for Android, add extra configuration options for  cmake :  -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake  (where  $ANDROID_NDK  is the path to Android NDK directorory, e.g.  /opt/android-ndk-r15c )  AND  arguments from the table below 
 | ABI         | Extra cmake args                                    | Restrictions               |
| ----------- | --------------------------------------------------- | -------------------------- |
| armeabi     |  -DANDROID_ABI=armeabi -DANDROID_TOOLCHAIN=gcc      | Requires CPU with ARM NEON |
| armeabi-v7a |  -DANDROID_ABI=armeabi-v7a -DANDROID_TOOLCHAIN=gcc  | Requires CPU with ARM NEON |
| arm64-v8a   |  -DANDROID_ABI=arm64-v8a -DANDROID_TOOLCHAIN=clang  | Requires clang toolchain   |
| x86         |  -DANDROID_ABI=x86                                  |                            |
| x86_64      |  -DANDROID_ABI=x86_64                               |                            | 
 Notes:
- On  armeabi  and  armeabi-v7a   nnp_initialize  will fail with  nnp_status_unsupported_hardware  if the mobile CPU does not support ARM NEON. Don't set  -DANDROID_ARM_NEON=1  for NNPACK compilation as it can make  nnp_initialize  crash on CPUs without ARM NEON.
- NNPACK builds for  armeabi  and  armeabi-v7a  are up to 2x slower if you use  clang  toolchain.
-  mips  and  mips64  are not supported, and we have no plans to add it (pull request would be welcome, though)
-  x86_64  build will use generic 128-bit (SSE2) micro-kernels rather than AVX2 micro-kernels in native build 
 Ecosystem 
 Deep Learning Frameworks 
 
 PyTorch  supports NNPACK on mobile for inference in convolutional layers. 
 TVM  supports NNPACK for inference in convolutional layers. See  these instructions  to enable NNPACK in TVM. 
 MXNet  supports NNPACK for inference in convolutional layers, fully-connected, and max-pooling layers. See  MXNet wiki  for configuration instructions and performance benchmarks). 
 Caffe2  supports NNPACK for inference in convolutional layers. 
 darknet-nnpack  - fork of  Darknet  framework with NNPACK support. 
 tiny-dnn  - header-only deep learning framework in C++11, which natively supports NNPACK. 
 Maratyszcza/caffe  - up-to-date integration of NNPACK (convolutional, fully-connected, max-pooling, and ReLU layers) into Caffe based on  nnpack-pr  branch in  ajtulloch/caffe . 
 Maratyszcza/caffe-nnpack  - older and unmaintained integration of NNPACK (convolutional layers only) into Caffe. 
 szagoruyko/nnpack.torch  - integration of NNPACK into Lua Torch via ffi 
 See also discussion in  Issue #1 
 
 Languages and Environments 
 
 nnpack-windows  - unofficial port for Windows 
 node-nnpack  - Node.js bindings 
 peterhj/libnnpack  - Rust bindings 
 
 Users 
 
 Facebook  uses NNPACK in production. 
 Prisma  uses NNPACK in the mobile app. 
 
 Acknowledgements 
 
 
 The library is developed by  Marat Dukhan  of Georgia Tech with extensive advice from  Nicolas Vasilache  and  Soumith Chintala  of Facebook Artificial Intelligence Research.  Andrew Tulloch  of Facebook Artificial Intelligence Research contributed Caffe integration. We thank  Andrew Lavin  for fruitful discussions on Winograd transform-based implementations. NNPACK is a research project at  Richard Vuduc 's HPC Garage lab in the Georgia Institute of Technology, College of Computing, School of Computational Science and Engineering. 
 This material is based upon work supported by the U.S. National Science Foundation (NSF) Award Number 1339745. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of NSF. FXdiv 
 
 
 Header-only library for division via fixed-point multiplication by inverse 
 On modern CPUs and GPUs integer division is several times slower than multiplication. FXdiv implements an algorithm to replace an integer division with a multiplication and two shifts. This algorithm improves performance when an application performs repeated divisions by the same divisor. 
 Features 
 
 Integer division for  uint32_t ,  uint64_t , and  size_t 
 Header-only library, no installation or build required 
 Compatible with C99, C++, OpenCL, and CUDA 
 Uses platform-specific compiler intrinsics for optimal performance 
 Covered with unit tests and microbenchmarks 
 
 Example 
 ```c 
 include  
 /  Division of array by a constant: reference implementation  /
void divide_array_c(size_t length, uint32_t array[], uint32_t divisor) {
  for (size_t i = 0; i < length; i++) {
    array[i] /= divisor;
  }
} 
 /  Division of array by a constant: implementation with FXdiv  /
void divide_array_fxdiv(size_t length, uint32_t array[], uint32_t divisor) {
  const struct fxdiv_divisor_uint32_t precomputed_divisor =
    fxdiv_init_uint32_t(divisor);
  for (size_t i = 0; i < length; i++) {
    array[i] = fxdiv_quotient_uint32_t(array[i], precomputed_divisor);
  }
}
``` 
 Status 
 Currently working features: 
 | Platform        | uint32_t | uint64_t | size_t   |
| --------------- |:--------:|:--------:|:--------:|
| x86-64 gcc      | Works    | Works    | Works    |
| x86-64 clang    | Works    | Works    | Works    |
| x86-64 MSVC     | Works    | Works    | Works    |
| x86 gcc         | Works    | Works    | Works    |
| x86 clang       | Works    | Works    | Works    |
| x86 MSVC        | Works    | Works    | Works    |
| ARMv7 gcc       | Works    | Works    | Works    |
| ARMv7 clang     | Works    | Works    | Works    |
| ARMv7 MSVC      | Compiles | Compiles | Compiles |
| ARM64 gcc       | Works    | Works    | Works    |
| ARM64 clang     | Works    | Works    | Works    |
| ARM64 MSVC      | Compiles | Compiles | Compiles |
| PPC64 gcc       | Works    | Works    | Works    |
| WAsm clang      | Works    | Works    | Works    |
| Asm.js clang    | Works    | Works    | Works    |
| PNaCl clang     | Works    | Works    | Works    |
| CUDA            | Untested | Untested | Untested |
| OpenCL          | Untested | Untested | Untested | 
 *ARMv7 and ARM64 builds with MSVC are presumed to work, but were only verified to compile successfully 
 References 
 
 Granlund, Torbjörn, and Peter L. Montgomery. ""Division by invariant integers using multiplication."" In ACM SIGPLAN Notices, vol. 29, no. 6, pp. 61-72. ACM, 1994. Available:  gmplib.org/~tege/divcnst-pldi94.pdf 
 FP16 
 Header-only library for conversion to/from half-precision floating point formats 
 Features 
 
 Supports IEEE and ARM alternative half-precision floating-point format 
 Property converts infinities and NaNs 
 Properly converts denormal numbers, even on systems without denormal support 
 
 
 Header-only library, no installation or build required 
 Compatible with C99 and C++11 
 Fully covered with unit tests and microbenchmarks 
 
 Acknowledgements 
 
 
 The library is developed by  Marat Dukhan  of Georgia Tech. FP16 is a research project at  Richard Vuduc 's HPC Garage lab in the Georgia Institute of Technology, College of Computing, School of Computational Science and Engineering. 
 This material is based upon work supported by the U.S. National Science Foundation (NSF) Award Number 1339745. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of NSF. P(ortable) SIMD 
 Portable 128-bit SIMD intrinsics CPU INFOrmation library 
 
 
 
 cpuinfo is a library to detect essential for performance optimization information about host CPU. 
 Features 
 
 Cross-platform  availability: 
 Linux, Windows, macOS, Android, and iOS operating systems 
 x86, x86-64, ARM, and ARM64 architectures 
 Modern  C/C++ interface 
 Thread-safe 
 No memory allocation after initialization 
 No exceptions thrown 
 Detection of  supported instruction sets , up to AVX512 (x86) and ARMv8.3 extensions 
 Detection of SoC and core information: 
 Processor (SoC) name 
 Vendor and  microarchitecture  for each CPU core 
 ID ( MIDR  on ARM,  CPUID  leaf 1 EAX value on x86) for each CPU core 
 Detection of  cache information : 
 Cache type (instruction/data/unified), size and line size 
 Cache associativity 
 Cores and logical processors (hyper-threads) sharing the cache 
 Detection of  topology information  (relative between logical processors, cores, and processor packages) 
 Well-tested  production-quality  code: 
 60+ mock tests based on data from real devices 
 Includes work-arounds for common bugs in hardware and OS kernels 
 Supports systems with heterogenous cores, such as  big.LITTLE  and Max.Med.Min 
 Permissive  open-source  license (Simplified BSD) 
 
 Examples 
 Log processor name: 
 c
cpuinfo_initialize();
printf(""Running on %s CPU\n"", cpuinfo_get_package(0)->name); 
 Detect if target is a 32-bit or 64-bit ARM system: 
 ```c 
 if CPUINFO_ARCH_ARM || CPUINFO_ARCH_ARM64 
 /* 32-bit ARM-specific code here */
 
 endif 
 ``` 
 Check if the host CPU supports ARM NEON 
 c
cpuinfo_initialize();
if (cpuinfo_has_arm_neon()) {
    neon_implementation(arguments);
} 
 Check if the host CPU supports x86 AVX 
 c
cpuinfo_initialize();
if (cpuinfo_has_x86_avx()) {
    avx_implementation(arguments);
} 
 Check if the thread runs on a Cortex-A53 core 
 c
cpuinfo_initialize();
switch (cpuinfo_get_current_core()->uarch) {
    case cpuinfo_uarch_cortex_a53:
        cortex_a53_implementation(arguments);
        break;
    default:
        generic_implementation(arguments);
        break;
} 
 Get the size of level 1 data cache on the fastest core in the processor (e.g. big core in big.LITTLE ARM systems): 
 c
cpuinfo_initialize();
const size_t l1_size = cpuinfo_get_processor(0)->cache.l1d->size; 
 Pin thread to cores sharing L2 cache with the current core (Linux or Android) 
 c
cpuinfo_initialize();
cpu_set_t cpu_set;
CPU_ZERO(&cpu_set);
const struct cpuinfo_cache* current_l2 = cpuinfo_get_current_processor()->cache.l2;
for (uint32_t i = 0; i < current_l2->processor_count; i++) {
    CPU_SET(cpuinfo_get_processor(current_l2->processor_start + i)->linux_id, &cpu_set);
}
pthread_setaffinity_np(pthread_self(), sizeof(cpu_set_t), &cpu_set); 
 Use via pkg-config 
 If you would like to provide your project's build environment with the necessary compiler and linker flags in a portable manner, the library by default when built enables  CPUINFO_BUILD_PKG_CONFIG  and will generate a  pkg-config  manifest ( libcpuinfo.pc ). Here are several examples of how to use it: 
 Command Line 
 If you used your distro's package manager to install the library, you can verify that it is available to your build environment like so: 
 console
$ pkg-config --cflags --libs libcpuinfo
-I/usr/include/x86_64-linux-gnu/ -L/lib/x86_64-linux-gnu/ -lcpuinfo 
 If you have installed the library from source into a non-standard prefix, pkg-config may need help finding it: 
 console
$ PKG_CONFIG_PATH=""/home/me/projects/cpuinfo/prefix/lib/pkgconfig/:$PKG_CONFIG_PATH"" pkg-config --cflags --libs libcpuinfo
-I/home/me/projects/cpuinfo/prefix/include -L/home/me/projects/cpuinfo/prefix/lib -lcpuinfo 
 GNU Autotools 
 To  use  with the GNU Autotools include the following snippet in your project's  configure.ac : 
 ```makefile 
 CPU INFOrmation library... 
 PKG_CHECK_MODULES(
    [libcpuinfo], [libcpuinfo], [],
    [AC_MSG_ERROR([libcpuinfo missing...])])
YOURPROJECT_CXXFLAGS=""$YOURPROJECT_CXXFLAGS $libcpuinfo_CFLAGS""
YOURPROJECT_LIBS=""$YOURPROJECT_LIBS $libcpuinfo_LIBS""
``` 
 Meson 
 To use with Meson you just need to add  dependency('libcpuinfo')  as a dependency for your executable. 
 ```meson
project(
    'MyCpuInfoProject',
    'cpp',
    meson_version: '>=0.55.0'
) 
 executable(
    'MyCpuInfoExecutable',
    sources: 'main.cpp',
    dependencies: dependency('libcpuinfo')
)
``` 
 Bazel 
 This project can be built using  Bazel .  
 You can also use this library as a dependency to your Bazel project. Add to the  WORKSPACE  file: 
 ```python
load(""@bazel_tools//tools/build_defs/repo:git.bzl"", ""git_repository"") 
 git_repository(
    name = ""org_pytorch_cpuinfo"",
    branch = ""master"",
    remote = ""https://github.com/Vertexwahn/cpuinfo.git"",
)
``` 
 And to your  BUILD  file: 
 python
cc_binary(
    name = ""cpuinfo_test"",
    srcs = [
        # ...
    ],
    deps = [
        ""@org_pytorch_cpuinfo//:cpuinfo"",
    ],
) 
 CMake 
 To use with CMake use the  FindPkgConfig  module. Here is an example: 
 ```cmake
cmake_minimum_required(VERSION 3.6)
project(""MyCpuInfoProject"") 
 find_package(PkgConfig)
pkg_check_modules(CpuInfo REQUIRED IMPORTED_TARGET libcpuinfo) 
 add_executable(${PROJECT_NAME} main.cpp)
target_link_libraries(${PROJECT_NAME} PkgConfig::CpuInfo)
``` 
 Makefile 
 To use within a vanilla makefile, you can call pkg-config directly to supply compiler and linker flags using shell substitution. 
 makefile
CFLAGS=-g3 -Wall -Wextra -Werror ...
LDFLAGS=-lfoo ...
...
CFLAGS+= $(pkg-config --cflags libcpuinfo)
LDFLAGS+= $(pkg-config --libs libcpuinfo) 
 Exposed information 
 
 [x] Processor (SoC) name 
 [x] Microarchitecture 
 [x] Usable instruction sets 
 [ ] CPU frequency 
 [x] Cache 
 [x] Size 
 [x] Associativity 
 [x] Line size 
 [x] Number of partitions 
 [x] Flags (unified, inclusive, complex hash function) 
 [x] Topology (logical processors that share this cache level) 
 [ ] TLB 
 [ ] Number of entries 
 [ ] Associativity 
 [ ] Covered page types (instruction, data) 
 [ ] Covered page sizes 
 [x] Topology information 
 [x] Logical processors 
 [x] Cores 
 [x] Packages (sockets) 
 
 Supported environments: 
 
 [x] Android 
 [x] x86 ABI 
 [x] x86_64 ABI 
 [x] armeabi ABI 
 [x] armeabiv7-a ABI 
 [x] arm64-v8a ABI 
 [ ] ~~mips ABI~~ 
 [ ] ~~mips64 ABI~~ 
 [x] Linux 
 [x] x86 
 [x] x86-64 
 [x] 32-bit ARM (ARMv5T and later) 
 [x] ARM64 
 [ ] PowerPC64 
 [x] iOS 
 [x] x86 (iPhone simulator) 
 [x] x86-64 (iPhone simulator) 
 [x] ARMv7 
 [x] ARM64 
 [x] macOS 
 [x] x86 
 [x] x86-64 
 [x] ARM64 (Apple silicon) 
 [x] Windows 
 [x] x86 
 [x] x86-64 
 [x] arm64 
 
 Methods 
 
 Processor (SoC) name detection 
 [x] Using CPUID leaves 0x80000002–0x80000004 on x86/x86-64 
 [x] Using  /proc/cpuinfo  on ARM 
 [x] Using  ro.chipname ,  ro.board.platform ,  ro.product.board ,  ro.mediatek.platform ,  ro.arch  properties (Android) 
 [ ] Using kernel log ( dmesg ) on ARM Linux 
 [x] Using Windows registry on ARM64 Windows 
 Vendor and microarchitecture detection 
 [x] Intel-designed x86/x86-64 cores (up to Sunny Cove, Goldmont Plus, and Knights Mill) 
 [x] AMD-designed x86/x86-64 cores (up to Puma/Jaguar and Zen 2) 
 [ ] VIA-designed x86/x86-64 cores 
 [ ] Other x86 cores (DM&P, RDC, Transmeta, Cyrix, Rise) 
 [x] ARM-designed ARM cores (up to Cortex-A55, Cortex-A77, and Neoverse E1/N1/V1/N2) 
 [x] Qualcomm-designed ARM cores (Scorpion, Krait, and Kryo) 
 [x] Nvidia-designed ARM cores (Denver and Carmel) 
 [x] Samsung-designed ARM cores (Exynos) 
 [x] Intel-designed ARM cores (XScale up to 3rd-gen) 
 [x] Apple-designed ARM cores (up to Lightning and Thunder) 
 [x] Cavium-designed ARM cores (ThunderX) 
 [x] AppliedMicro-designed ARM cores (X-Gene) 
 Instruction set detection 
 [x] Using CPUID (x86/x86-64) 
 [x] Using  /proc/cpuinfo  on 32-bit ARM EABI (Linux) 
 [x] Using microarchitecture heuristics on (32-bit ARM) 
 [x] Using  FPSID  and  WCID  registers (32-bit ARM) 
 [x] Using  getauxval  (Linux/ARM) 
 [x] Using  /proc/self/auxv  (Android/ARM) 
 [ ] Using instruction probing on ARM (Linux) 
 [ ] Using CPUID registers on ARM64 (Linux) 
 [x] Using IsProcessorFeaturePresent on ARM64 Windows 
 Cache detection 
 [x] Using CPUID leaf 0x00000002 (x86/x86-64) 
 [x] Using CPUID leaf 0x00000004 (non-AMD x86/x86-64) 
 [ ] Using CPUID leaves 0x80000005-0x80000006 (AMD x86/x86-64) 
 [x] Using CPUID leaf 0x8000001D (AMD x86/x86-64) 
 [x] Using  /proc/cpuinfo  (Linux/pre-ARMv7) 
 [x] Using microarchitecture heuristics (ARM) 
 [x] Using chipset name (ARM) 
 [x] Using  sysctlbyname  (Mach) 
 [x] Using sysfs  typology  directories (ARM/Linux) 
 [ ] Using sysfs  cache  directories (Linux) 
 [x] Using  GetLogicalProcessorInformationEx  on ARM64 Windows 
 TLB detection 
 [x] Using CPUID leaf 0x00000002 (x86/x86-64) 
 [ ] Using CPUID leaves 0x80000005-0x80000006 and 0x80000019 (AMD x86/x86-64) 
 [x] Using microarchitecture heuristics (ARM) 
 Topology detection 
 [x] Using CPUID leaf 0x00000001 on x86/x86-64 (legacy APIC ID) 
 [x] Using CPUID leaf 0x0000000B on x86/x86-64 (Intel APIC ID) 
 [ ] Using CPUID leaf 0x8000001E on x86/x86-64 (AMD APIC ID) 
 [x] Using  /proc/cpuinfo  (Linux) 
 [x] Using  host_info  (Mach) 
 [x] Using  GetLogicalProcessorInformationEx  (Windows) 
 [x] Using sysfs (Linux) 
 [x] Using chipset name (ARM/Linux) 
 TensorFlow.js 
 TensorFlow.js is an open-source hardware-accelerated JavaScript library for
training and deploying machine learning models. 
 Develop ML in the Browser   
Use flexible and intuitive APIs to build models from scratch using the low-level
JavaScript linear algebra library or the high-level layers API. 
 Develop ML in Node.js   
Execute native TensorFlow with the same TensorFlow.js API under the Node.js
runtime. 
 Run Existing models   
Use TensorFlow.js model converters to run pre-existing TensorFlow models right
in the browser. 
 Retrain Existing models   
Retrain pre-existing ML models using sensor data connected to the browser or
other client-side data. 
 About this repo 
 This repository contains the logic and scripts that combine
several packages. 
 APIs:
-  TensorFlow.js Core ,
  a flexible low-level API for neural networks and numerical computation.
-  TensorFlow.js Layers ,
  a high-level API which implements functionality similar to
   Keras .
-  TensorFlow.js Data ,
  a simple API to load and prepare data analogous to
   tf.data .
-  TensorFlow.js Converter ,
  tools to import a TensorFlow SavedModel to TensorFlow.js
-  TensorFlow.js Vis ,
  in-browser visualization for TensorFlow.js models
-  TensorFlow.js AutoML ,
  Set of APIs to load and run models produced by
   AutoML Edge . 
 Backends/Platforms:
-  TensorFlow.js Node , Node backend via TensorFlow C++
-  TensorFlow.js WASM , WebAssembly backend
-  TensorFlow.js React Native , Support for React Native.
-  TensorFlow.js WebGPU , WebGPU backend. 
 If you care about bundle size, you can import those packages individually. 
 If you are looking for Node.js support, check out the  TensorFlow.js Node directory . 
 Examples 
 Check out our
 examples repository 
and our  tutorials . 
 Gallery 
 Be sure to check out  the gallery  of all projects related to TensorFlow.js. 
 Pre-trained models 
 Be sure to also check out our  models repository  where we host pre-trained models
on NPM. 
 Getting started 
 There are two main ways to get TensorFlow.js in your JavaScript project:
via  script tags   or  by installing it from  NPM 
and using a build tool like  Parcel ,
 WebPack , or  Rollup . 
 via Script Tag 
 Add the following code to an HTML file: 
 ```html

   
  Load TensorFlow.js  
   
 <!-- Place your code in the script tag below. You can also use an external .js file -->
<script>
  // Notice there is no 'import' statement. 'tf' is available on the index-page
  // because of the script tag above.

  // Define a model for linear regression.
  const model = tf.sequential();
  model.add(tf.layers.dense({units: 1, inputShape: [1]}));

  // Prepare the model for training: Specify the loss and the optimizer.
  model.compile({loss: 'meanSquaredError', optimizer: 'sgd'});

  // Generate some synthetic data for training.
  const xs = tf.tensor2d([1, 2, 3, 4], [4, 1]);
  const ys = tf.tensor2d([1, 3, 5, 7], [4, 1]);

  // Train the model using the data.
  model.fit(xs, ys).then(() => {
    // Use the model to do inference on a data point the model hasn't seen before:
    // Open the browser devtools to see the output
    model.predict(tf.tensor2d([5], [1, 1])).print();
  });
</script>
 
 
 
``` 
 Open up that HTML file in your browser, and the code should run! 
 via NPM 
 Add TensorFlow.js to your project using  yarn   or   npm .  Note:  Because
we use ES2017 syntax (such as  import ), this workflow assumes you are using a modern browser or a bundler/transpiler
to convert your code to something older browsers understand. See our
 examples 
to see how we use  Parcel  to build
our code. However, you are free to use any build tool that you prefer. 
 ```js
import * as tf from '@tensorflow/tfjs'; 
 // Define a model for linear regression.
const model = tf.sequential();
model.add(tf.layers.dense({units: 1, inputShape: [1]})); 
 // Prepare the model for training: Specify the loss and the optimizer.
model.compile({loss: 'meanSquaredError', optimizer: 'sgd'}); 
 // Generate some synthetic data for training.
const xs = tf.tensor2d([1, 2, 3, 4], [4, 1]);
const ys = tf.tensor2d([1, 3, 5, 7], [4, 1]); 
 // Train the model using the data.
model.fit(xs, ys).then(() => {
  // Use the model to do inference on a data point the model hasn't seen before:
  model.predict(tf.tensor2d([5], [1, 1])).print();
});
``` 
 See our  tutorials ,  examples 
and  documentation  for more details. 
 Importing pre-trained models 
 We support porting pre-trained models from:
-  TensorFlow SavedModel 
-  Keras 
 Find out more 
 TensorFlow.js  is a part of the
 TensorFlow  ecosystem. For more info:
- For help from the community, use  tensorflow.js  tag on Stack Overflow.
-  js.tensorflow.org 
-  Tutorials 
-  API reference 
-  Discussion mailing list 
 Thanks,  BrowserStack , for providing testing support. XNNPACK 
 XNNPACK is a highly optimized library of floating-point neural network inference operators for ARM, WebAssembly, and x86 platforms. XNNPACK is not intended for direct use by deep learning practitioners and researchers; instead it provides low-level performance primitives for accelerating high-level machine learning frameworks, such as  TensorFlow Lite ,  TensorFlow.js ,  PyTorch ,  ONNX Runtime , and  MediaPipe . 
 Supported Architectures 
 
 ARM64 on Android, Linux, macOS, and iOS (including WatchOS and tvOS) 
 ARMv7 (with NEON) on Android 
 ARMv6 (with VFPv2) on Linux 
 x86 and x86-64 (up to AVX512) on Windows, Linux, macOS, Android, and iOS simulator 
 WebAssembly MVP 
 WebAssembly SIMD 
 RISC-V (RV32GV and RV64GC) 
 
 Operator Coverage 
 XNNPACK implements the following neural network operators: 
 
 2D Convolution (including grouped and depthwise) 
 2D Deconvolution (AKA Transposed Convolution) 
 2D Average Pooling 
 2D Max Pooling 
 2D ArgMax Pooling (Max Pooling + indices) 
 2D Unpooling 
 2D Bilinear Resize 
 2D Depth-to-Space (AKA Pixel Shuffle) 
 Add (including broadcasting, two inputs only) 
 Subtract (including broadcasting) 
 Divide (including broadcasting) 
 Maximum (including broadcasting) 
 Minimum (including broadcasting) 
 Multiply (including broadcasting) 
 Squared Difference (including broadcasting) 
 Global Average Pooling 
 Channel Shuffle 
 Fully Connected 
 Abs (absolute value) 
 Bankers' Rounding (rounding to nearest, ties to even) 
 Ceiling (rounding to integer above) 
 Clamp (includes ReLU and ReLU6) 
 Convert (includes fixed-point and half-precision quantization and
  dequantization) 
 Copy 
 ELU 
 Floor (rounding to integer below) 
 HardSwish 
 Leaky ReLU 
 Negate 
 Sigmoid 
 Softmax 
 Square 
 Transpose 
 Truncation (rounding to integer towards zero) 
 PReLU 
 
 All operators in XNNPACK support NHWC layout, but additionally allow custom stride along the  C hannel dimension. Thus, operators can consume a subset of channels in the input tensor, and produce a subset of channels in the output tensor, providing a zero-cost Channel Split and Channel Concatenation operations. 
 Performance 
 Mobile phones 
 The table below presents  single-threaded  performance of XNNPACK library on three generations of MobileNet models and three generations of Pixel phones. 
 | Model                   | Pixel, ms | Pixel 2, ms | Pixel 3a, ms |
| ----------------------- | :-------: | :---------: | :----------: |
| FP32 MobileNet v1 1.0X  |    82     |      86     |      88      |
| FP32 MobileNet v2 1.0X  |    49     |      53     |      55      |
| FP32 MobileNet v3 Large |    39     |      42     |      44      |
| FP32 MobileNet v3 Small |    12     |      14     |      14      | 
 The following table presents  multi-threaded  (using as many threads as there are big cores) performance of XNNPACK library on three generations of MobileNet models and three generations of Pixel phones. 
 | Model                   | Pixel, ms | Pixel 2, ms | Pixel 3a, ms |
| ----------------------- | :-------: | :---------: | :----------: |
| FP32 MobileNet v1 1.0X  |    43     |      27     |      46      |
| FP32 MobileNet v2 1.0X  |    26     |      18     |      28      |
| FP32 MobileNet v3 Large |    22     |      16     |      24      |
| FP32 MobileNet v3 Small |     7     |       6     |       8      | 
 Benchmarked on March 27, 2020 with  end2end_bench --benchmark_min_time=5  on an Android/ARM64 build with Android NDK r21 ( bazel build -c opt --config android_arm64 :end2end_bench ) and neural network models with randomized weights and inputs. 
 Raspberry Pi 
 The table below presents  multi-threaded  performance of XNNPACK library on three generations of MobileNet models and three generations of Raspberry Pi boards. 
 | Model                   | RPi Zero W (BCM2835), ms | RPi 2 (BCM2836), ms | RPi 3+ (BCM2837B0), ms | RPi 4 (BCM2711), ms | RPi 4 (BCM2711, ARM64), ms |
| ----------------------- | :----------------------: | :-----------------: | :--------------------: | :-----------------: | :------------------------: |
| FP32 MobileNet v1 1.0X  |          3919            |         302         |          114           |          72         |             77             |
| FP32 MobileNet v2 1.0X  |          1987            |         191         |           79           |          41         |             46             |
| FP32 MobileNet v3 Large |          1658            |         161         |           67           |          38         |             40             |
| FP32 MobileNet v3 Small |           474            |          50         |           22           |          13         |             15             |
| INT8 MobileNet v1 1.0X  |          2589            |         128         |           46           |          29         |             24             |
| INT8 MobileNet v2 1.0X  |          1495            |          82         |           30           |          20         |             17             | 
 Benchmarked on Feb 8, 2022 with  end2end-bench --benchmark_min_time=5  on a Raspbian Buster build with CMake ( ./scripts/build-local.sh ) and neural network models with randomized weights and inputs. INT8 inference was evaluated on per-channel quantization schema. 
 Publications 
 
 Marat Dukhan ""The Indirect Convolution Algorithm"". Presented on  Efficient Deep Learning for Compute Vision (ECV) 2019  workshop ( slides ,  paper on ArXiv ). 
 Erich Elsen, Marat Dukhan, Trevor Gale, Karen Simonyan ""Fast Sparse ConvNets"".
   Paper on ArXiv ,  pre-trained sparse
  models . 
 Marat Dukhan, Artsiom Ablavatski ""The Two-Pass Softmax Algorithm"".
   Paper on ArXiv . 
 Yury Pisarchyk, Juhyun Lee ""Efficient Memory Management for Deep Neural Net Inference"".
   Paper on ArXiv . 
 
 Ecosystem 
 Machine Learning Frameworks 
 
 TensorFlow Lite . 
 TensorFlow.js WebAssembly backend . 
 PyTorch Mobile . 
 MediaPipe for the Web . 
 Alibaba HALO (Heterogeneity-Aware Lowering and Optimization) 
 Samsung ONE (On-device Neural Engine) 
 
 Acknowledgements 
 XNNPACK is a based on  QNNPACK  library. Over time its codebase diverged a lot, and XNNPACK API is no longer compatible with QNNPACK."
cvondrick,"Please Note 
 Intel has created an excellent annotation tool with the latest technologies. https://github.com/opencv/cvat  
 The project below is archived, and no further updates are expected. 
 2009 to 2020 
 VATIC - Video Annotation Tool from Irvine, California 
 VATIC is an online video annotation tool for computer vision research that
crowdsources work to Amazon's Mechanical Turk. Our tool makes it easy to build
massive, affordable video data sets.  
 
 INSTALLATION 
 Note: VATIC has only been tested on Ubuntu with Apache 2.2 HTTP server and a
MySQL server. This document will describe installation on this platform,
however it should work any operating system and with any server. 
 Download 
 You can download and extract VATIC from our website. Note: do NOT run the 
installer as root.  
 $ wget http://mit.edu/vondrick/vatic/vatic-install.sh
$ chmod +x vatic-install.sh
$ ./vatic-install.sh
$ cd vatic
 
 HTTP Server Configuration 
 Open the Apache configuration file. On Ubuntu, this file is located at: 
 /etc/apache2/sites-enabled/000-default
 
 If you do not use Apache on this computer for any other purpose, replace the
contents of the file with: 
 WSGIDaemonProcess www-data
WSGIProcessGroup www-data

<VirtualHost *:80>
    ServerName vatic.domain.edu
    DocumentRoot /path/to/vatic/public

    WSGIScriptAlias /server /path/to/vatic/server.py
    CustomLog /var/log/apache2/access.log combined
</VirtualHost>
 
 updating ServerName with your domain name, DocumentRoot with the path to
the public directory in VATIC, and WSGIScriptAlias to VATIC's server.py file. 
 If you do use Apache for other purposes, you will have to setup a new virtual
host with the correct document root and script alias, as shown above. 
 Make sure you have the mod_headers module enabled: 
 $ sudo cp /etc/apache2/mods-available/headers.load /etc/apache2/mods-enabled
 
 After making these changes, restart Apache: 
 $ sudo apache2ctl graceful
 
 SQL Server Configuration 
 We recommend creating a separate database specifically for VATIC: 
 $ mysql -u root
mysql> create database vatic;
 
 The next section will automatically create the necessary tables. 
 Setup 
 Inside the vatic directory, copy config.py-example to config.py: 
 $ cp config.py-example config.py
 
 Then open config.py and make changes to the following variables in order to
configure VATIC: 
 signature       Amazon Mechanical Turk AWS signature (secret access key)
accesskey       Amazon Mechanical Turk AWS access key (access key ID)
sandbox         If true, put into Mturk sandbox mode. For debugging.
localhost       The local HTTP address: http://vatic.domain.edu/ so it
                matches the ServerName in Apache.
database        Database connection string: for example,
                mysql://user:pass@localhost/vatic
geolocation     API key from ipinfodb.com for geolocation services
 
 If you do not plan on using VATIC on Mechcanical Turk (offlien mode only), you
can leave the signature and accesskey empty. 
 After saving results, you can then initialize the database: 
 $ turkic setup --database
 
 Note: if you want to reset the database, you can do this with: 
 $ turkic setup --database --reset
 
 which will require confirmation to reset in order to prevent data loss. 
 Finally, you must also allow VATIC to access turkic, a major dependency: 
 $ turkic setup --public-symlink
 
 ANNOTATION 
 Before you continue, you should verify that the installation was correct. You
can verify this with: 
 $ turkic status --verify
 
 If you receive any error messages, it means the installation was not complete
and you should review the previous section. Note: If you do not plan on
using Mechanical Turk, you can safely ignore any errors caused by Mechanical
Turk. 
 Frame Extraction 
 Our system requires that videos are extracted into JPEG frames. Our tool can 
do this automatically for you: 
 $ mkdir /path/to/output/directory
$ turkic extract /path/to/video.mp4 /path/to/output/directory
 
 By default, our tool will resize the frames to fit within a 720x480 rectangle.
We believe this resolution is ideal for online video viewing. You can change 
resolution with options: 
 $ turkic extract /path/to/video.mp4 /path/to/output/directory
  --width 1000 --height 1000
 
 or 
 $ turkic extract /path/to/video.mp4 /path/to/output/directory
  --no-resize
 
 The tool will maintain aspect ratio in all cases. 
 Alternatively, if you have already extracted frames, you can use the
formatframes command to format the video into a format that VATIC understands: 
 $ turkic formatframes /path/to/frames/ /path/to/output/directory
 
 The above command will read all the images in /path/to/frames and create
hard links (soft copy) in /path/to/output/directory. 
 Importing a Video 
 After extracting frames, the video can be imported into our tool for 
annotation. The general syntax for this operation is: 
 $ turkic load identifier /path/to/output/directory Label1 Label2 LabelN
 
 where identifier is a unique string that you will use to refer to this video,
/path/to/output/directory is the directory of frames, and LabelX are class
labels that you want annotated (e.g., Person, Car, Bicycle). You can have as
many class labels as you wish, but you must have at least one. 
 When a video is imported, it is broken into small segments typically of only a
few seconds. When all the segments are annotated, the annotations are merged
across segments because each segment overlaps another by a small margin. 
 The above command specifies all of the required options, but there are many
options available as well. We recommend using these options. 
 MTurk Options
    --title         The title that MTurk workers see
    --description   The description that MTurk workers see
    --duration      Time in seconds that a worker has to complete the task
    --lifetime      Time in seconds that the task is online
    --keywords      Keywords that MTurk workers can search on
    --offline       Disable MTurk and use for self annotation only

Compensation Options
    --cost                  The price advertised to MTurk workers
    --per-object-bonus      A bonus in dollars paid for each object
    --completion-bonus      A bonus in dollars paid for completing the task

Qualification Options
    --min-approved-percent  Minimum percent of tasks the worker must have
                            approved before they can work for you
    --min-approved-amount   Minimum number of tasks that the worker must 
                            have completed before they can work for you

Video Options
    --length        The length of each segment for this video in frames
    --overlap       The overlap between segments in frames
    --use-frames    When splitting into segments, only the frame intervals
                    specified in this file. Each line should contain a
                    start frame, followed by a space, then the stop frame.
                    Frames outside the intervals in this file will be
                    ignored.
    --skip          If specified, request annotations only every N frames.
    --blow-radius   When a user marks an annotation, blow away all other
                    annotations within this many frames. If you want to
                    allow the user to make fine-grained annotations, set
                    this number to a small integer, or 0 to disable. By
                    default, this is 5, which we recommend.
 
 You can also specify temporal attributes that each object label can take on.
For example, you may have a person object with attributes ""walking"", ""running"",
or ""sitting"". You can specify attributes the same way as labels, except you
prepend an ~ before the text, which bind the attribute to the previous label: 
 $ turkic load identifier /path/to/output/directory Label1 ~Attr1A ~Attr1B
  Label2 ~Attr2A ~Attr2B ~Attr2C Label3
 
 In the above example, Label1 will have attributes Attr1A and Attr1B, Label2
will have attributes Attr2B, Attr2B, and Attr2C and Label3 will have no 
attributes. Specifying attributes is optional. 
 Gold Standard Training 
 It turns out that video annotation is extremely challenging and most MTurk
workers lack the necessary patience. For this reason, we recommend requiring
workers to pass a ""gold standard"" video. When a new worker visits the task,
they will be redirected to a video for which the annotations are already known.
In order to move on to the true annotations, the worker must correctly annotate
the gold standard video first. We have found that this approach significantly
improves the quality of the annotations. 
 To use this feature, import a video to be used as the gold standard: 
 $ turkic load identifier-train /path/to/frames Label1 Label2 LabelN
  --for-training --for-training-start 0 --for-training-stop 500
  --for-training-overlap 0.5 --for-training-tolerance 0.1
  --for-training-mistakes 1
 
 You can also use any of the options described above. Explanations for the new
options are as follows: 
 --for-training              Specifies that this video is gold standard
--for-training-start        Specifies the first frame to use
--for-training-stop         Specifies the last frame to use
--for-training-overlap      Percent overlap that worker's boxes must match 
--for-training-tolerance    Percent that annotations must agree temporally
--for-training-mistakes     The number of completely wrong annotations 
                            allowed. We recommend setting this to a small,
                            nonzero integer.
 
 After running the above command, it will provide you with an URL for you to
input the ground truth annotation. You must make this ground truth annotation
as careful as possible, as it will be used to evaluate future workers. 
 You can now specify that a video should use a gold standard video: 
 $ turkic load identifier /path/to/output/directory Label1 Label2 LabelN
  --train-with identifier-train
 
 When a not-yet-seen worker visits this video, they will now be redirected to
to the training video and be required to pass the evaluation test first. 
 Publishing Tasks 
 When you are ready for the MTurk workers to annotate, you must publish the 
tasks, which will allow workers to start annotating: 
 $ turkic publish
 
 You can limit the number of tasks that are published: 
 $ turkic publish --limit 100
 
 Running above command repeatedly will launch tasks in batches of 100. You can
also disable all pending tasks: 
 $ turkic publish --disable
 
 which will ""unpublish"" tasks that have not yet been completed. 
 If you have videos that are offline only, you can see their access URLs with
the command: 
 $ turkic publish --offline
 
 Note: for the above command to work, you must have loaded the video with the
--offline parameter as well:  
 $ turkic load identifier /path/to/frames Person --offline
 
 Checking the Status 
 You can check the status of the video annotation server with the command: 
 $ turkic status
 
 This will list various statistics about the server, such as number of jobs
published and how many are completed. You can get even more statistics by
requesting additional information from Amazon: 
 $ turkic status --turk
 
 which will output how much money is left in your account, among other
statistics. 
 When all the videos are annotated, the last line will read: 
 Server is offline.
 
 Retrieving Annotations 
 You can get all the annotations for a video with the command: 
 $ turkic dump identifier -o output.txt
 
 which will write the file ""output.txt"" where each line contains one
annotation. Each line contains 10+ columns, separated by spaces. The
definition of these columns are: 
 1   Track ID. All rows with the same ID belong to the same path.
2   xmin. The top left x-coordinate of the bounding box.
3   ymin. The top left y-coordinate of the bounding box.
4   xmax. The bottom right x-coordinate of the bounding box.
5   ymax. The bottom right y-coordinate of the bounding box.
6   frame. The frame that this annotation represents.
7   lost. If 1, the annotation is outside of the view screen.
8   occluded. If 1, the annotation is occluded.
9   generated. If 1, the annotation was automatically interpolated.
10  label. The label for this annotation, enclosed in quotation marks.
11+ attributes. Each column after this is an attribute.
 
 By default, the above command will not attempt to merge annotations across
shot segments. You can request merging with the command: 
 $ turkic dump identifier -o output.txt --merge --merge-threshold 0.5
 
 The --merge-threshold option is optional, but it is a number between 0 and 1
that represents much the paths must agree in order to merge. 1 specifies a
perfect match and 0 specifies no match. In practice, 0.5 is sufficient. Merging
is done using the Hungarian algorithm. 
 You can also scale annotations by a factor, which is useful for when the
videos have been downsampled: 
 $ turkic dump identifier -o output.txt -s 2.8
 
 or force it to fit within a max dimension: 
 $ turkic dump identifier -o output.txt --dimensions 400x200
 
 or force it to fit within the dimensions of the original video: 
 $ turkic dump identifier -o output.txt --original-video /path/to/video.mp4
 
 The command can also output to many different formats. Available formats are: 
 --xml       Use XML
--json      Use JSON
--matlab    Use MATLAB
--pickle    Use Python's Pickle
--labelme   Use LabelMe video's XML format
--pascal    Use PASCAL VOC format, treating each frame as an image
 
 The specifications for these formats should be self explanatory. 
 Visualizing Videos 
 You can preview the annotations by visualizing the results: 
 $ turkic visualize identifier /tmp --merge
 
 which will output frames to /tmp with the bounding boxes with the file name
as the frame number. The visualization will contain some meta information
that can help you identify bad workers. You can remove this meta information
with the option: 
 $ turkic visualize identifer /tmp --merge --no-augment
 
 If you want to make a video of the visualization (e.g., with ffmpeg), it is
useful to renumber the frames so that they start counting at 0 and do not
have any gaps: 
 $ turkic visualize identifier /tmp --merge --renumber
 
 If you wish to display the class label and their attributes next to the box,
specify the --labels option: 
 $ turkic visualize identifier /tmp --labels
 
 Compensating Workers 
 When you are ready, you can compensate workers: 
 $ turkic compensate --default accept
 
 which will pay all workers for all outstanding tasks. We strongly recommend
paying all workers regardless of their quality. You should attempt to pay
workers at least once per day. 
 Finding Jobs 
 If you have found a small mistake in a video and want to make
the correction yourself, you can start an annotation session initialized with
the MTurk workers annotations: 
 $ turkic find --id identifier
$ turkic find --id identifier --frame frame
 
 where identifier is the identifier for the video and frame is the frame number
that the error occurs. In most cases, this command will return one URL for you
to make the corrections. If it outputs two URLs, it means the frame number
occurs in two overlapping segments, and so you may have to make changes to both
of the segments. You can also omit the frame argument, in which case it will
output all URLs for that video. 
 If you want to find the HIT id, assignment ID, or worker ID for a particular
video, specify the --ids parameter to the vet command: 
 $ turkic find --id identifer --ids
$ turkic find --id identifer --frame frame --ids
 
 will print a list of all the IDs for the video. If the corresponding segment
has been published and completed, it will list three strings: the HIT ID,
assignment ID, and the worker ID. If the job has been published but not
finished, it will just list the HIT ID. If the job has not yet been published,
it prints ""(not published)"". 
 Additionally, if you want to find the job that corresponds to a particular
HIT ID, you can use the find command: 
 $ turkic find --hitid HITID
 
 Quality Control 
 The gold standard does a ""pretty good"" job of weeding out bad workers.
Nonetheless, there will always be bad workers that we must identify and
invalidate. Our tool provides a method to sample the annotations provided by
workers, which you can then manually verify for correctness: 
 $ turkic sample /tmp
 
 which by default will pick 3 random videos that the worker has completed, and
pick 4 random frames from each of those videos, and write visualiations to a
file in /tmp. You can tweak the number of videos and the number of frames with
the options: 
 $ turkic sample /tmp --number 3 --frames 4
 
 Moreover, you can only look at work from a certain date: 
 $ turkic sample /tmp --since ""yesterday""
 
 The filename will follow the format of WORKERID-JOBID.jpg. Once you have
identified a mallicious worker, you can block them, invalidate ALL of their
work, and respawn their jobs with the command: 
 $ turkic invalidate workerid
 
 The options are also available: 
 --no-block      invalidate and respawn, but don't block
--no-publish    block and invalidate, but don't respawn
 
 You can also invalidate and respawn individual jobs with the command: 
 $ turkic invalidate --hit hitid
 
 Listing all Videos 
 You can retrieve a list of all videos in the system with: 
 $ turkic list
 
 If you want just the videos that have been published: 
 $ turkic list --published
 
 If you want just the videos that have been worked on: 
 $ turkic list --completed
 
 If you instead want the videos that are used for gold standard: 
 $ turkic list --training
 
 Finally, if you just want to count how many videos are in the system, use the
--count option, in combination with any of the above: 
 $ turkic list --count
$ turkic list --published --count
 
 If you want statistics about each video, then give the --stats option: 
 $ turkic list --stats
 
 Managing Workers 
 You can list all known workers with the command: 
 $ turkic workers
 
 which will dump every worker with the number of jobs they have completed. You
can also use this command to block and unblock workers: 
 $ turkic workers --block workerid
$ turkic workers --unblock workerid
 
 You can also search for workers by the first few letters of their ID: 
 $ turkic workers --search A3M
 
 Deleting a Video 
 You can delete a video at any time with: 
 $ turkic delete identifier
 
 If the video has already been annotated (even partially), this command will 
warn you and abort. You can force deletion with: 
 $ turkic delete identifier --force
 
 which will REMOVE ALL DATA AND CANNOT BE UNDONE. 
 REFERENCES 
 When using our system, please cite: 
 Carl Vondrick, Donald Patterson, Deva Ramanan. ""Efficiently Scaling Up
Crowdsourced Video Annotation"" International Journal of Computer Vision
(IJCV). June 2012.
 
 FEEDBACK AND BUGS 
 Please direct all comments and report all bugs to: 
 Carl Vondrick
vondrick@mit.edu
 
 Thanks for using our system! iHOG: Inverting Histograms of Oriented Gradients 
 This software package contains tools to invert and visualize HOG features.
It implements the Paired Dictionary Learning algorithm described in our
paper ""HOGgles: Visualizing Object Detection Features"" [1]. 
 
 Installation 
 Before you can use this tool, you must compile iHOG. Execute the 'compile'
script in MATLAB to compile the HOG feature extraction code and sparse coding
SPAMS toolbox: 
 $ cd /path/to/ihog
$ matlab
>> compile
 
 If you run into trouble compiling the SPAMS code, you might try opening 
the file  /path/to/ihog/spams/compile.m  and adjusting the settings for
your computer. 
 Remember to also adjust your path so MATLAB can find iHOG: 
 >> addpath(genpath('/path/to/ihog'))
 
 If you want to use iHOG in your own project, you can simply drop the iHOG
directory into the root of your project. 
 In order to use iHOG, you must have a learned paired dictionary. By default,
iHOG will attempt to download a pretrained one from MIT for you on the first
execution. If you wish to download it manually, simply do: 
 $ wget http://people.csail.mit.edu/vondrick/pd.mat
 
 Inverting HOG 
 To invert a HOG point, use the 'invertHOG()' function: 
 >> feat = features(im, 8);
>> ihog = invertHOG(feat);
>> imagesc(ihog); axis image;
 
 Computing the inverse should take no longer than a second for a typical sized
image on a modern computer. (It may slower the first time you invoke it as it
caches the paired dictionary from disk.) 
 Learning 
 We provide a prelearned dictionary in 'pd.mat', but you can learn your own if
you wish. Simply call the 'learnpairdict()' function and pass it a directory of
images: 
 >> pd = learnpairdict('/path/to/images/', 1000000, 1000, 5, 5);
 
 The above learns a 5x5 HOG patch paired dictionary with 1000 elements and a
training set size of one million window patches. Depending on the size of the
problem, it may take minutes or hours to complete. 
 Bundled Libraries 
 The iHOG package contains source code from the SPAMS sparse coding toolbox
(http://spams-devel.gforge.inria.fr/). We have modified their code to better
support 64 bit machines. 
 In addition, we have included a select few files from the discriminatively
trained deformable parts model (http://people.cs.uchicago.edu/~rbg/latent/).
We use their HOG computation code and glyph visualization code. 
 Questions and Comments 
 If you have any feedback, please write to Carl Vondrick at  vondrick@mit.edu . 
 References 
 If you use our software, please cite our conference paper: 
 [1] Carl Vondrick, Aditya Khosla, Tomasz Malisiewicz, Antonio Torralba.
""HOGgles: Visualizing Object Detection Features""  International Conference
on Computer Vision (ICCV), Sydney, Australia, December 2013. tmux-parallel 
 This is a simple recipe for launching jobs on a cluster. 
 It is mostly meant for clusters that lack proper queueing systems (such as the vision cluster at MIT).  
 Installation 
 
 Install tmux 
 Add this line to your  .tmux.conf : 
   bind-key y set-window-option synchronize-panes 
 Put  crunch.sh  into your path 
 Modify  crunch.sh  to change .csail.mit.edu domains to your domain 
 
 Usage 
 
 Start tmux 
 Run command:  crunch N machine1 machine2 machine3  where  N  is the number of replicas you want, and the rest are the hostnames of the machines you want to connect. This will cause tmux to open N*number_of_hosts panes, with SSH connections into each machine. 
 Press  Ctrl+b y  to synchronize all the panes. 
 Start typing, and your input will be broadcast to all machines. 
 
 Best practices 
 If you write your scripts in the right way, you can use the above workflow to process data in parallel. Below is a psuedo-code that shows the basic idea. It assumes you are using a networked file system (such as NFS). 
 workload = [""file1.json"", ""file2.json"", ...]

random.shuffle(workload) # in matlab, you must remember to set the random seed

for workitem in workload:
  in_path = ""input/"" + workitem
  out_path = ""output/"" + workitem
  lock_path = out_path + "".lock""

  if exist(out_path) or exist(lock_path):
    continue
  mkdir(lock_path) # create lock file

  # do heavy lifting here that eventually writes out_path

  rmdir(lock_path) # remove lock file
 
 CSAIL instructions 
 Remember, everytime you connect to a CSAIL machine, you must authenticate, otherwise your jobs will be killed eventually. To get around this, you can start your session with  longscreen , and start tmux inside the longscreen.  Torch Starter 
 This is a simple Torch7 starter package. It can be used  as a simplified kickoff point for a Torch project. 
 
 I pieced together this package largely from  Torch7 resources online . I mostly just copied the code, and stripped a lot of extra functionality out, to make it easier to hack on.  
 If something is not clear, or could be made more simple, please let me know. The goal is to be simple. 
 Installation 
 If you are at CSAIL, you can use my Torch installation:
 bash
. /data/vision/torralba/commonsense/torch/install/bin/torch-activate
export LD_LIBRARY_PATH=/data/vision/torralba/commonsense/cudnnv5/cuda/lib64:$LD_LIBRARY_PATH 
 Otherwise, installation is fairly simple. You need to install:
-  Torch7 
-  cunn  for training on GPU
-  cudnn  for faster training on GPU
-  tds  for some data structures
-  display  for graphs  
 You can install all of these with the commands:
```bash 
 install torch first 
 git clone https://github.com/torch/distro.git ~/torch --recursive
cd ~/torch; bash install-deps;
./install.sh 
 install libraries 
 luarocks install cunn
luarocks install cudnn
luarocks install tds
luarocks install https://raw.githubusercontent.com/szym/display/master/display-scm-0.rockspec
``` 
 Learning Resources 
 
 Torch Cheat Sheet 
 60 minute blitz 
 
 Model 
 I trained an AlexNet-esque network on  Places365 
with this code, which you can  download here . This model obtains
50% top-1 accuracy on the validation set. This is slightly worse than the published result because we didn't do averaging over 10 crops. 
 If you use this model, please cite the Places2 paper (of which I am not
affiliated).  Note this model is slightly different from the AlexNet in Caffe.
Notable differences: no groups in the convolutions, no
spatial normalization, batch normalizaiton, trained with Adam instead of SGD,
and sampling with replacement. It is unclear to me whether these changes have a
significant impact on performance.  
 Data Setup 
 By default, we assume you have a text file that lists your dataset. This text does not store your dataset; it just lists filepaths to it, and any meta data. Each line in this text file represents one training example, and its associated category ID. The syntax of the line should be: 
 <filename><tab><number> 
For example:
 bedroom/IMG_050283.jpg    5
bedroom/IMG_237761.jpg    5
office/IMG_838222.jpg     10 
The  <number>  should start counting at 1.  
 After you create this file, open  main.lua  and change  data_list  to point to this file. You can specify a  data_root  too, which will be prepended to each filename.  
 Training 
 Define your model in the  net  variable. By default, it is AlexNet. To learn more about the modules you can use, see  nn . You can also adjust your loss with the  criterion  variable.  
 Remember to also adjust any options in  opt , such as the learning rate and the number of classes. Setting these hyperparameters is a bit of an art, but generally it is recommended to use a learning rate of  0.001  and batch size of at least  64 , but  128  or  256  may be better if you have the memory. For a systematic study, see  this paper . 
 Finally, to start training, just do: 
 bash
$ CUDA_VISIBLE_DEVICES=0 th main.lua 
where you replace the number after  CUDA_VISIBLE_DEVICES  with the GPU you want to run on. 
You can find which GPU to use with  $ nvidia-smi  on our GPU cluster. Note: this number is 0-indexed, unlike the rest of Torch! 
 During training, it will dump snapshots to the  checkpoints/  directory every epoch. Each time you start a new experiment, you should change the  name  (in  opt ), to avoid overwriting previous experiments. The code will not warn you about this (to keep things simple). 
 Evaluation 
 To evaluate your model, you can use the  eval.lua  script. It mostly follows the same format as  main.lua . It reads your validation/testing dataset from a file similar to before, and sequentially runs through it, calculating both the top-1 and top-5 accuracy.  
 Graphics, Logs 
 If you want to see graphics and the loss over time, in a different shell on the same machine, run this command:
 bash
$ th -ldisplay.start 8000 0.0.0.0 
then navigate to  http://HOST.csail.mit.edu:8000  in your browser. Every 10th iteration it will
push graphs.  
 On the CSAIL vision cluster, you can run this code out-of-the-box, and it will start to train
AlexNet on the Places2 database.  torch-ffmpeg 
 This is a simple wrapper for FFmpeg in Torch7. There are a couple of other wrappers for FFmpeg already, but I found them difficult to install. 
 This wrapper:
- talks to FFmpeg via Unix pipes so it is easy to install
- it is a single Lua file (and only 100 lines), so easy to modify 
- it doesn't write to disk, so it is reasonably fast 
 Usage 
 The  demo.lua  code shows how to use it. It's pretty easy: 
 require 'torch-ffmpeg'
vid = FFmpeg('video.mp4')
frames = vid:read(10)
vid:close()
 
 frames  will be a T x 3 x W x H tensor, where T is number of frames read, and W and H is width and height. In the example above, T = 10. 
 Options 
 If you want to specify different options, such as a different starting point or change the frame rate, you can pass additional options to FFmpeg like so: 
 vid = FFmpeg('video.mp4', '-r 10') -- 10 fps
vid = FFmpeg('video.mp4', '-ss 00:00:07') -- seek to 7sec mark
vid = FFmpeg('video.mp4', '-s 100x100') -- downsample resolution to 100x100
vid = FFmpeg('video.mp4', '-r 10 -s 100x100') -- frame rate and downsample
 
 Note that seeking is approximate, but fast. Generating Videos with Scene Dynamics 
 This repository contains an implementation of  Generating Videos with Scene Dynamics  by Carl Vondrick, Hamed Pirsiavash, Antonio Torralba, to appear at NIPS 2016. The model learns to generate tiny videos using adversarial networks. 
 Example Generations 
 Below are some selected videos that are generated by our model. These videos are not real; they are hallucinated by a generative video model. While they are not photo-realistic, the motions are fairly reasonable for the scene category they are trained on. 
 
 Beach 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Golf 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Train Station 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Baby 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Training 
 The code requires a Torch7 installation.  
 To train a generator for video, see main.lua. This file will construct the networks, start many threads to load data, and train the networks. 
 For the conditional version, see main_conditional.lua. This is similar to main.lua, except the input to the model is a static image. 
 To generate videos, see generate.lua. This file will also output intermediate layers,
such as the mask and background image, which you can inspect manually. 
 Data 
 The data loading is designed assuming videos have been stabilized and flattened
into JPEG images. We do this for efficiency. Stabilization is computationally slow and
must be done offline, and reading one file per video is more efficient on NFS. 
 For our stabilization code, see the 'extra' directory.
Essentially, this will convert each video into an image of vertically
concatenated frames. After doing this, you create a text file listing
all the frames, which you pass into the data loader. 
 Models 
 You can download our pre-trained models  here  (1 GB ZIP file). 
 Notes 
 The code is based on  DCGAN  and our  starter code  in  Torch7 . 
 If you find this useful for your research, please consider citing our NIPS
paper. 
 License 
 MIT SoundNet 
 Code for paper "" SoundNet: Learning Sound Representations from Unlabeled Video "" by Yusuf Aytar, Carl Vondrick, Antonio Torralba. NIPS 2016 
 We learn rich natural sound representations by capitalizing on large amounts of unlabeled sound data collected in the wild. We leverage the natural synchronization between vision and sound to learn an acoustic representation using two-million unlabeled videos. We propose a student-teacher training procedure which transfers discriminative visual knowledge from well established visual models (e.g. ImageNet and PlacesCNN) into the sound modality using unlabeled video as a bridge. 
 
 Visualization of learned conv1 filters:
 
 Requirements 
 
 torch7 
 torch7 audio (and sox) 
 torch7 hdf5 (only for feature extraction) 
 probably a GPU 
 
 Pretrained Model 
 We provide pre-trained models that are trained over 2,000,000 unlabeled videos. You can download the 8 layer and 5 layer models  here . We recommend the 8 layer network. 
 Recognize Categories 
 You can use SoundNet to recognize sounds or as features (see next section). To recognize objects and scenes, you can use our provided script. First, create a text file where each line lists an audio file you wish to process. We use MP3 files, but most audio formats should be supported. Then, extract predictions into HDF5 files like so: 
 bash
$ list=data.txt th extract_predictions.lua 
 where  data.txt  is this text file. It will write HDF5 files to the location of the input files with the scores of each category. To map the dimension index back to the category name, use the files  categories/categories_places2.txt  for scenes and  categories/categories_imagenet.txt  for objects. 
 The script will also output the top scoring object and scene category. E 
 Feature Extraction 
 Using our network, you can extract good features for natural sounds. You can use our provided script to extract features. First, create a text file where each line lists an audio file you wish to process. We use MP3 files, but most audio formats should be supported. Then, extract features into HDF5 files like so: 
 bash
$ list=data.txt th extract_feat.lua 
 where  data.txt  is this text file. It will write HDF5 files to the location of the input files with the features. You can then do anything you wish with these features.  
 By default, it will extract the  conv7  layer. You can extract other layers like so: 
 ```bash
$ list=data.txt layer=24 th extract_feat.lua
```` 
 The model will predict a matrix of size TIME x DIM where DIM is the feature dimensionality and TIME is relative to the length of the sound file. If you wish a fixed length vector, you can average-pool or max-pool over time. 
 Advanced 
 If you want to write your own feature extraction code, it is very easy in Torch: 
 ```lua
sound = audio.load('file.mp3'):select(2,1):clone():mul(2^-23):view(1,1-1,1):cuda() 
 net = torch.load('soundnet8_final.t7')
net:forward(sound)
features = net.modules[24].output:float()
``` 
 Finetuning 
 If you want to fine-tune SoundNet on your own dataset, you can use  main_finetune.lua . Create a text file that lists your MP3 files and their corresponding categories as integers. Each line lists one file in the format: filename [space] integer. For example:
 /path/to/file1.mp3 1
/path/to/file2.mp3 5
/path/to/file3.mp3 2 
The integer at the end of the text file specifies the category. Note they should start counting at 1 (not zero). 
 Then, you can finetune SoundNet with the command:
 finetune=models/soundnet8_final.t7 data_list=dataset.txt data_root=/ nClasses=5 name=mynet1 th main_finetune.lua 
where the  data_list  variable points to your text file you created above, and  nClasses  specifies the number of categories you have. 
 Note you may have to modify  main_finetune.lua  depending on your needs. This is meant to just get you started. In particular, you may need to adjust the  fineSize  and  loadSize , which specify how many samples from the waveform to use. 
 Training 
 The code for training is in  main_train.lua , which will train the 8 layer SoundNet model. You can also use  main_train_small.lua  to train the 5 layer SoundNet model. To start training, just do: 
 bash
$ CUDA_VISIBLE_DEVICES=0 th main_train.lua 
 The code for loading the data is in  data/donkey_audio.lua . The training code will launch several threads. Each thread reads a different subset of the dataset. It will read MP3 files into a raw waveform. For the labels, it reads a large binary file that stores the class probabilities computed from ImageNet and Places networks. 
 Data 
 We plan to release 2 million MP3s and their corresponding class probabilities soon. Stay tuned. 
 References 
 If you use SoundNet in your research, please cite our paper: 
 SoundNet: Learning Sound Representations from Unlabeled Video 
Yusuf Aytar, Carl Vondrick, Antonio Torralba
NIPS 2016
"
MaximumEntropy,"SanDeepLearn 
 MNIST Multilayer Perceptron 
 ```
train_x, train_y, dev_x, dev_y, test_x, test_y = get_data(dataset='mnist') 
 x = T.fmatrix()
y = T.imatrix()
fc1 = FullyConnectedLayer(
    input_dim=train_x.shape[1],
    output_dim=500,
    activation='tanh'
)
fc2 = FullyConnectedLayer(
    input_dim=500,
    output_dim=500,
    activation='tanh'
)
fc3 = FullyConnectedLayer(
    input_dim=500,
    output_dim=10,
    activation='softmax'
) 
 params = fc1.params + fc2.params + fc3.params
act1 = fc1.fprop(x)
act2 = fc2.fprop(act1)
act3 = fc3.fprop(act2)
loss = T.nnet.categorical_crossentropy(
    act3,
    y
).mean() 
 print 'Compiling optimization method ...'
updates = Optimizer().sgd(
    loss,
    params,
    lr=0.01
) 
 print 'Compiling train function ...'
f_train = theano.function(
    inputs=[x, y],
    outputs=loss,
    updates=updates
) 
 print 'Compiling evaluation function ...'
f_eval = theano.function(
    inputs=[x],
    outputs=act3
) 
 print 'Training network ...'
for epoch in xrange(10):
    costs = []
    for batch in xrange(0, train_x.shape[0], 24):
        cost = f_train(
            train_x[batch:batch + 24],
            train_y[batch:batch + 24]
        )
        costs.append(cost)
    print 'Epoch %d Training Loss : %.3f ' % (epoch, np.mean(costs)) 
 dev_predictions = f_eval(dev_x)
test_predictions = f_eval(test_x)
print 'Accuracy on dev : %.3f%% ' % (
    100. * (np.argmax(dev_predictions, axis=1) == dev_y).mean()
)
print 'Accuracy on test : %.3f%% ' % (
    100. * (np.argmax(test_predictions, axis=1) == test_y).mean()
) 
 ``` 
 MNIST Le-Net 5 
 ```
train_x, train_y, dev_x, dev_y, test_x, test_y = get_data(dataset='mnist') 
 train_x = train_x.reshape(
    train_x.shape[0],
    1,
    int(np.sqrt(train_x.shape[1])),
    int(np.sqrt(train_x.shape[1]))
) 
 dev_x = dev_x.reshape(
    dev_x.shape[0],
    1,
    int(np.sqrt(dev_x.shape[1])),
    int(np.sqrt(dev_x.shape[1]))
)
test_x = test_x.reshape(
    test_x.shape[0],
    1,
    int(np.sqrt(test_x.shape[1])),
    int(np.sqrt(test_x.shape[1]))
) 
 x = T.tensor4()
y = T.imatrix() 
 convolution_layer0 = Convolution2DLayer(
    input_height=train_x.shape[2],
    input_width=train_x.shape[3],
    filter_width=5,
    filter_height=5,
    num_filters=20,
    num_feature_maps=1,
    flatten=False,
    wide=False
) 
 convolution_layer1 = Convolution2DLayer(
    input_height=convolution_layer0.output_height_shape,
    input_width=convolution_layer0.output_width_shape,
    filter_width=5,
    filter_height=5,
    num_filters=50,
    num_feature_maps=20,
    flatten=True,
    wide=False
) 
 fc1 = FullyConnectedLayer(800, 500, activation='tanh')
fc2 = FullyConnectedLayer(500, 10, activation='softmax') 
 params = convolution_layer0.params + convolution_layer1.params + \
    fc1.params + fc2.params
act1 = convolution_layer0.fprop(x)
act2 = convolution_layer1.fprop(act1)
act3 = fc1.fprop(act2)
act4 = fc2.fprop(act3)
loss = T.nnet.categorical_crossentropy(
    act4,
    y
).mean() 
 print 'Compiling optimization method ...'
updates = Optimizer().sgd(
    loss,
    params,
    lr=0.01
) 
 print 'Compiling train function ...'
f_train = theano.function(
    inputs=[x, y],
    outputs=loss,
    updates=updates
) 
 print 'Compiling evaluation function ...'
f_eval = theano.function(
    inputs=[x],
    outputs=act4
) 
 print 'Training network ...'
for epoch in xrange(10):
    costs = []
    for batch in xrange(0, train_x.shape[0], 24):
        cost = f_train(
            train_x[batch:batch + 24],
            train_y[batch:batch + 24]
        )
        costs.append(cost)
    print 'Epoch %d Training Loss : %.3f ' % (epoch, np.mean(costs)) 
 dev_predictions = f_eval(dev_x)
test_predictions = f_eval(test_x)
print 'Accuracy on dev : %.3f%% ' % (
    100. * (np.argmax(dev_predictions, axis=1) == dev_y).mean()
)
print 'Accuracy on test : %.3f%% ' % (
    100. * (np.argmax(test_predictions, axis=1) == test_y).mean()
) 
 ``` 
 Transmembrane helix prediction using Recurrent Neural Networks 
 ```
x = T.ivector()
y = T.ivector() 
 emb = EmbeddingLayer(20, 50, name='embedding')
if network == 'rnn':
    rnn = RNN(50, 50, name='rnn')
elif network == 'lstm':
    rnn = LSTM(50, 50, name='lstm')
fc1 = FullyConnectedLayer(50, 1, name='fc') 
 params = emb.params + rnn.params + \
    fc1.params
embs = emb.fprop(x)
act1 = rnn.fprop(embs)
act2 = fc1.fprop(act1)
loss = ((act2.transpose() - y) ** 2).mean() 
 print 'Compiling optimization method ...'
updates = Optimizer().sgd(
    loss,
    params,
    lr=0.01
) 
 print 'Compiling train function ...'
f_train = theano.function(
    inputs=[x, y],
    outputs=loss,
    updates=updates
) 
 print 'Compiling evaluation function ...'
f_eval = theano.function(
    inputs=[x],
    outputs=act2
) 
 print 'Training network ...'
for epoch in xrange(10):
    costs = []
    for data_point, labels in zip(train_x, train_y):
        cost = f_train(
            data_point,
            labels
        )
    costs.append(cost) 
 print 'Epoch %d Training Loss : %f ' % (epoch, np.mean(costs))
 
 accs = []
for data_point, labels in zip(test_x, test_y):
    preds = f_eval(data_point).squeeze()
    preds = [1 if pred > 0.5 else 0 for pred in preds]
    acc = sum([True if a == b else False for a, b in zip(preds, labels)]) \
        / float(len(preds))
    accs.append(acc)
print 'Testing Accuracy : %f%% ' % (np.mean(accs) * 100.) 
 ``` This is an experimental incremental seq-seq MT system wedding-template NeuralMT cudnn-rnn-benchmarks 
 All benchmarks are reported for a host with the following specifications : 
 * NVIDIA GeForce GTX TITAN X GPU

* Intel(R) Core(TM) i7-5930K CPU @ 3.50GHz

* CUDA 8.0, cudnnv5105
 
 These benchmarks are aimed at understanding the performance gains with using the cuDNN RNN implementation (https://devblogs.nvidia.com/parallelforall/optimizing-recurrent-neural-networks-cudnn-5/) in theano. 
 The benchmarks are evaluated similar to https://github.com/glample/rnn-benchmarks that compares RNN implementations in different deep learning frameworks. Results will be integrated into the above repository eventually. 
 Note: Results on regular RNNs cannot be compared as is between the two repositories as this benchmark uses the new theano GPU backend libgpuarray https://github.com/Theano/libgpuarray and different hardware specifications. 
 The Recurrent Networks take as input a 3D Tensor  batch_size x seq_length x hidden_size 
and output all hidden states, compute a MSE loss at each step and compute the gradients of error with respect to each parameter.
The  hidden_size  specifies the size of the output and input layer of the networks. 
 The code of the scripts we ran are available.
The code for the regular theano RNN implementations were borrowed from the rnn-benchmarks repository. 
 The reported  Train  time is the average time needed to run (forward, backward) for a single training example, the smaller the better. 
 A more exhaustive grid search will be done soon. 
 Note: The compile times, although not reported are much faster for the cuDNN implementation.  
 LSTM - cuDNN LSTM and GRU vs FastLSTM in rnn.py 
 This LSTM implementation used for these benchmarks does not use peephole connections between cell and gates. 
 Depth 1 
 Batch Size 32 x Seq Len 30 
 Hidden Size 128 
 | Version | Train (µs) | Forward only (µs) |
| ------------- | ------------- | ------------- |
| Theano LSTM | 204.5 | 57.1 |
| cuDNN Theano LSTM | 118.8 | 59.5 |
| cuDNN Theano GRU | 117.4 | 57.6 | 
 Hidden Size 512 
 | Version | Train (µs) | Forward only (µs) |
| ------------- | ------------- | ------------- |
| Theano LSTM | 530.9 | 148.1 |
| cuDNN Theano LSTM | 223.2 | 102.4 |
| cuDNN Theano GRU | 184.6 | 77.6 | 
 Hidden Size 1024 
 | Version | Train (µs) | Forward only (µs) |
| ------------- | ------------- | ------------- |
| Theano LSTM | 1102.0 | 294.0 |
| cuDNN Theano LSTM | 601.8 | 161.1 |
| cuDNN Theano GRU | 394.8 | 136.2 | 
 Batch Size 128 x Seq Len 30 
 Hidden Size 128 
 | Version | Train (µs) | Forward only (µs) |
| ------------- | ------------- | ------------- |
| Theano LSTM | 200.8 | 52.8 |
| cuDNN Theano LSTM | 33.4 | 15.0 |
| cuDNN Theano GRU | 32.2 | 14.4 | 
 Hidden Size 512 
 | Version | Train (µs) | Forward only (µs) |
| ------------- | ------------- | ------------- |
| Theano LSTM | 491.0 | 138.2 |
| cuDNN Theano LSTM | 100.8 | 31.7 |
| cuDNN Theano GRU | 83.3 | 26.5 | 
 Hidden Size 1024 
 | Version | Train (µs) | Forward only (µs) |
| ------------- | ------------- | ------------- |
| Theano LSTM | 1000.1 | 291.8 |
| cuDNN Theano LSTM | 221.2 | 69.0 |
| cuDNN Theano GRU | 181.3 | 59.1 | 
 Depth 3 
 Batch Size 128 x Seq Len 30 
 Hidden Size 512 
 | Version | Train (µs) | Forward only (µs) |
| ------------- | ------------- | ------------- |
| Theano LSTM | 778.3 | 418.3 |
| cuDNN Theano LSTM | 244.9 | 70.2 |
| cuDNN Theano GRU | 197.1 | 55.7 | 
 Hidden Size 1024 
 | Version | Train (µs) | Forward only (µs) |
| ------------- | ------------- | ------------- |
| Theano LSTM | 1592.8 | 882.7 |
| cuDNN Theano LSTM | 820.6 | 256.8 |
| cuDNN Theano GRU | 639.5 | 195.2 | 
 Batch Size 128 x Seq Len 200 
 Hidden Size 512 
 | Version | Train (µs) | Forward only (µs) |
| ------------- | ------------- | ------------- |
| Theano LSTM | 2196.6 | 1168.1 |
| cuDNN Theano LSTM | 1539.5 | 485.9 |
| cuDNN Theano GRU | 1253.8 | 386.4 | 
 Hidden Size 1024 
 | Version | Train (µs) | Forward only (µs) |
| ------------- | ------------- | ------------- |
| Theano LSTM | 5711.1 | 3427.9 |
| cuDNN Theano LSTM | 5342.5 | 1692.1 |
| cuDNN Theano GRU | 4163.4 | 1274.5 | personal_website Sequence to Sequence models with PyTorch 
 This repository contains implementations of Sequence to Sequence (Seq2Seq) models in PyTorch 
 At present it has implementations for :  
 * Vanilla Sequence to Sequence models

* Attention based Sequence to Sequence models from https://arxiv.org/abs/1409.0473 and https://arxiv.org/abs/1508.04025

* Faster attention mechanisms using dot products between the **final** encoder and decoder hidden states

* Sequence to Sequence autoencoders (experimental)
 
 Sequence to Sequence models 
 A vanilla sequence to sequence model presented in https://arxiv.org/abs/1409.3215, https://arxiv.org/abs/1406.1078 consits of using a recurrent neural network such as an LSTM (http://dl.acm.org/citation.cfm?id=1246450) or GRU (https://arxiv.org/abs/1412.3555) to encode a sequence of words or characters in a  source  language into a fixed length vector representation and then deocoding from that representation using another RNN in the  target  language. 
 
 An extension of sequence to sequence models that incorporate an attention mechanism was presented in https://arxiv.org/abs/1409.0473 that uses information from the RNN hidden states in the source language at each time step in the deocder RNN. This attention mechanism significantly improves performance on tasks like machine translation. A few variants of the attention model for the task of machine translation have been presented in https://arxiv.org/abs/1508.04025. 
 
 The repository also contains a simpler and faster variant of the attention mechanism that doesn't attend over the hidden states of the encoder at each time step in the deocder. Instead, it computes the a single batched dot product between all the hidden states of the decoder and encoder once after the decoder has processed all inputs in the target. This however comes at a minor cost in model performance. One advantage of this model is that it is possible to use the cuDNN LSTM in the attention based decoder as well since the attention is computed after running through all the inputs in the decoder. 
 Results on English - French WMT14 
 The following presents the model architecture and results obtained when training on the WMT14 English - French dataset. The training data is the english-french bitext from Europral-v7. The validation dataset is newstest2011 
 The model was trained with following configuration 
 * Source and target word embedding dimensions - 512

* Source and target LSTM hidden dimensions - 1024

* Encoder - 2 Layer Bidirectional LSTM

* Decoder - 1 Layer LSTM

* Optimization - ADAM with a learning rate of 0.0001 and batch size of 80

* Decoding - Greedy decoding (argmax)
 
 | Model | BLEU | Train Time Per Epoch |
| ------------- | ------------- | ------------- |
| Seq2Seq | 11.82 | 2h 50min |
| Seq2Seq FastAttention | 18.89 | 3h 45min |
| Seq2Seq Attention | 22.60 | 4h 47min | 
 Times reported are using a Pre 2016 Nvidia GeForce Titan X 
 Running 
 To run, edit the config file and execute python nmt.py --config  
 NOTE: This only runs on a GPU for now. IFT-6266 Class Project 
 Conditional Image Generation - Inpainting with MSCOCO 
 Introduction 
 I took a pretty standard approach to solving the problem of inpainting an image. Given a 64 x 64 image from MSCOCO, with it's center (32 x 32) masked out, I built a fully convolutional architecture that attempts to predict the center with an L2 reconstruction loss + a Wasserstein GAN (WGAN) objective. Some relevant literature that I used when building this model was 
 
 Convolutional autoencoders with an L2 reconstruction objective - https://pdfs.semanticscholar.org/1c6d/990c80e60aa0b0059415444cdf94b3574f0f.pdf 
 Mixing an L2 reconstruction objective with a GAN objective was presented in Context Encoders: Feature Learning by Inpainting - https://arxiv.org/abs/1604.07379.  
 Wasserstein GANs - https://arxiv.org/abs/1701.07875 
 Pix2Pix (Image-to-Image Translation with Conditional Adversarial Networks) - Fully convolutional model with an adverserial objective - https://arxiv.org/abs/1611.07004 
 DCGAN (Deep Convolutional Generative Adverserial Networks) - https://arxiv.org/abs/1511.06434 
 DenseNet (Denseley Connected Convolutional Networks) - https://arxiv.org/abs/1608.06993 
 UNet - https://arxiv.org/abs/1505.04597 
 Sentence Embeddings (Simple but tough to beat baseline for sentence embeddings) - https://openreview.net/pdf?id=SyK00v5xx 
 Adaptation of Word2Vec (Two/Too Simple Adaptations of Word2Vec for Syntax Problems) - http://www.cs.cmu.edu/~lingwang/papers/naacl2015.pdf 
 
 An example of masked image  and it's corresponding ground-truth is shown in the image below. 
 
 Architecture 
 I used a fully convolutional architecture for the generator, that is very similar to the DCGAN generator with strided convolution and transpose convolution layers, batch-normalization and ReLU activations. The difference between my generator and the DCGAN generator is that I don't have the initial linear transformation that they use to reshape the noise prior. The discriminator is also similar to the DCGAN discriminator with strided convolutions, batch normalization and ReLUs. 
 As in the pix2pix paper, I added skip connections between the feature maps produced by the regular convolutions and the transpose convoltion feature maps. The feature maps are concatenated along the channel axis. 
 Generator 
 | Layer | Filters | Input Shape | Output Shape | Kernel Size |
| ------------- | ------------- | ------------- | ------------ | ------------ |
| Conv1 | 32 | 3 x 64 x 64 | 32 x 32 x 32 | 4 x 4 |
| Conv2 | 64 | 32 x 32 x 32 | 64 x 16 x 16 | 4 x 4 |
| Conv3 | 96 | 64 x 16 x 16 | 96 x 8 x 8 | 4 x 4 |
| Conv4 | 128 | 96 x 8 x 8 | 128 x 4 x 4 | 4 x 4 |
| TConv1 | 96 | 128 x 4 x 4 | 96 x 8 x 8 | 4 x 4 |
| TConv2 | 64 | (96 + 96) x 8 x 8 | 64 x 16 x 16 | 4 x 4 |
| TConv3 | 1 | (64 + 64) x 16 x 16 | 1 x 32 x 32 | 4 x 4 | 
 The table above presents the generator's architecture where Conv  refers to a regular convolution with a stride of (2, 2) and TConv  refers to a transpose convolution, also with a stride of (2, 2). 
 Discriminator 
 | Layer | Filters | Input Shape | Output Shape | Kernel Size |
| ------------- | ------------- | ------------- | ------------ | ------------ |
| Conv1 | 32 | 3 x 32 x 32 | 32 x 16 x 16 | 4 x 4 |
| Conv2 | 64 | 32 x 16 x 16 | 64 x 8 x 8 | 4 x 4 |
| Conv3 | 96 | 64 x 8 x 8 | 96 x 4 x 4 | 4 x 4 |
| Conv4 | 128 | 96 x 4 x 4 | 128 x 2 x 2 | 4 x 4 |
| Conv5 | 256 | 128 x 2 x 2 | 256 x 1 x 1 | 2 x 2 | 
 Training 
 As evident from the Generator architecture, the model takes a 3 channel 64 x 64 image with the center masked out and tries to predict the center 32 x 32 square. The discriminator (when using the adverserial objective) takes ""fake"" 32 x 32 samples from our Generator and ""real"" 32 x 32 samples from our training data and is trained to distinguish between them. The generator is trained to fool the discriminator. This is formulated as a minimax game as described in Ian Goodfellow's original GAN paper. The L2 reconstruction objective tries to minimize the squared error between the predicted image center and the ground truth. The L2 + GAN objective simply adds these two losses together and backpropagates it through the network. In this work, I used a WGAN instead of a regular GAN which from an implementation perspective, simply removes  log  from the GAN objective and clamps the weights of the discriminator. 
 Adding Image Captions 
 The MSCOCO dataset contains several human written captions of each image. These written descriptions could improve the ability of our model to predict the masked out center. For example, in the image presented above, the sheep are masked out leaving only the green fields in the background visible. The model could therefore inpaint in many possible ways - with an empty field, a deer or a bear etc. However, if the model was present with a caption of the image saying ""Three sheep in a field of grass"", this could reduce it's search space. 
 In this project, I explored using a fixed representation of a caption using the ""Simple but tough to beat baseline for sentence embeddings"". In a nutshell, the paper uses a weighted average of the individual word embeddings for each word in the caption. The word embeddings for each word are trained on a large external corpus. In this paper, I used an adapted word2vec implementation (Two/Too Simple Adaptations of Word2Vec for Syntax Problems) trained on the English Gigaword 5 corpus. The weights of each word are inversely proportional to their word frequency in the text8 corpus. I then used the approach described in the paper to extract the first principal component on the sentence embeddings of the MSCOCO training captions and applied the described transformation on the training and dev set. 
 When using captions, I modified my generator to downsample all the way to 1 x 1 and then contatenated the embedding of the image's caption before upsampling with transpose convolutions. 
 Caption Generator 
 | Layer | Filters | Input Shape | Output Shape | Kernel Size |
| ------------- | ------------- | ------------- | ------------ | ------------ |
| Conv1 | 32 | 3 x 64 x 64 | 32 x 32 x 32 | 4 x 4 |
| Conv2 | 64 | 32 x 32 x 32 | 64 x 16 x 16 | 4 x 4 |
| Conv3 | 96 | 64 x 16 x 16 | 96 x 8 x 8 | 4 x 4 |
| Conv4 | 128 | 96 x 8 x 8 | 128 x 4 x 4 | 4 x 4 |
| Conv5 | 256 | 128 x 4 x 4 | 256 x 2 x 2 | 4 x 4 |
| Conv6 | 256 | 256 x 2 x 2 | 256 x 1 x 1 | 2 x 2 |
| TConv1 | 128 | (256 + 300emb) x 1 x 1 | 128 x 2 x 2 | 4 x 4 |
| TConv2 | 96 | (128 + 256) x 2 x 2 | 96 x 4 x 4 | 4 x 4 |
| TConv3 | 64 | (96 + 128) x 4 x 4 | 64 x 8 x 8 | 4 x 4 |
| TConv4 | 32 | (64 + 96) x 8 x 8 | 32 x 16 x 16 | 4 x 4 |
| TConv5 | 1 | (32 + 64) x 16 x 16 | 1 x 32 x 32 | 4 x 4 | 
 Hyperparameters 
 
 Optimizer - ADAM for both the generator and discriminator with a learning rate of 2e-3 
 Discriminator weight clamping -  (-0.03, 0.03) if discriminator is trained 5 times for every generator update else (-0.05, 0.05).  
 Batch size - 32 
 
 Results 
 In this section, I will present some of my cherry-picked inpainted images. All samples are inpaintings of the MSCOCO dev set. 
 L2 
 
 
 
 
 
 
 L2 + WGAN 
 
 
 
 
 
 
 L2 + WGAN + Caption 
 
 
 
 
 
 
 Conclusion & Discussion 
 I experimented with a simple method to solve the inpainting problem using a convolutional autoencoder with a WGAN objective. I also added information from image captions using a simple baseline for sentence embeddings. 
 The convolutional autoencoder with an L2 reconstruction objective yields reasonable results and appears to mostly predict the center as a smoothened interpolation of its surrounding. Adding the WGAN objective appeared to yield some structure to the predictions especially in the images that required inpainting the foreground and not just the background. Further, adding captions, seemed to help a little more in the quality of samples and also accelerated training (50-60 epochs for L2 + WGAN vs 30-40 epochs with captions). It also appeared to focus on drawing actual objects instead of just smoothing images with captions where a woman's face (first row 5th column) can be seen (if you squint really hard). welcome_tutorials 
 Various tutorials given for welcoming new students at MILA. [WIP] PyTorch Notebooks 
 A collection of Jupyter Notebooks to serve as an introduction to PyTorch. 
 This repository is a basically a clone of https://github.com/mila-udem/welcome_tutorials/tree/master/pytorch that I wrote a year ago, with modifications for PyTorch 1.0. 
 It contains 
 
 An Introduction to the Torch Tensor Library and examples of basic operations 
 Autograd 
 Utilities for implementing Neural Networks and Optimization Algorithms 
 Image classification example - MNIST & CIFAR-10 
 Neural Machine Translation with Sequence to Sequence models 
 Generative Adversarial Networks - MNIST 
"
jayanthkoushik,"cmu-ammml-project 
 Project for the Advanced Multimodal Machine Learning course at CMU. 
 Setup 
 Install required libraries 
 Bootstrapping is currently only supported for Ubuntu 14.04. You can use either Theano or TensorFlow as the backend. It's not possible to have both in a single setup since they require different CUDA/cuDNN versions. To bootstrap, simply specify the backend and run the bootstrap script.
 bash
$ BACKEND=theano bootstrap/bootstrap_ubuntu.sh
or
$ BACKEND=tensorflow bootstrap/bootstrap_ubuntu.sh 
If running without GPUs, set the  CPU_ONLY  flag:
 bash
$ BACKEND=theano CPU_ONLY=1 bootstrap/bootstrap_ubuntu.sh 
Reboot the system after this so drivers are properly loaded. 
 Install Python packages 
 Once the required libraries have been installed (either through the bootstrap script or manually), you can install the Python packages to a virtual environment using  install.sh :
 bash
$ ./install.sh 
By default, the virtualenv is created inside  ./env . You can change the location using the  VENV_DIR  variable:
 bash
$ VENV_DIR=venv ./install.sh 
You're ready to go now. You can access the virtualenv by sourcing its activate script:
 bash
$ source <VENV_DIR>/bin/activate 
To reinstall dependencies, or install new dependencies, simply run the install script again. neural-style 
 This repository contains Theano+Keras implementations of two algorithms for artistic style transfer. The algorithms can be used to mix the content of an image with the style of another image. For example, here is a photograph of the Gates Center at Carnegie Mellon Univeristy rendered in the style of a stained glass painting. Pre-trained models, and additional examples can be found on  my website . 
 
 
 
 
 
 There are two available implementations.  neural_style/slow_neural_style  contains the optimization based approach described in  A Neural Algorithm of Artistic Style  by Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. The algorithm is very general, in the sense that it can combine arbitrary content and style images, but the generation process is slow - it can take several minutes to generate a reasonably sized image. The implementation here mostly follows the original algorithm, except for a few details. There is an additional total-variation loss term, and the optimization is perfomed with Adam instead of L-BFGS. 
 neural_style/fast_neural_style  is based on the feed-forward approach described in  Perceptual Losses for Real-Time Style Transfer and Super-Resolution  by Justin Johnson, Alexandre Alahi, and Fei-Fei Li. It also incorporates the instance-normalization trick presented by Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky in  Instance Normalization: The Missing Ingredient for Fast Stylization . The algorithm generates styled images in real-time with a single pass through a convolutional neural network. However, this comes with some loss in flexibility. A separate model must be trained for each style image. Regardless, since (in my opinion), the final results are much more visually appealing, I use this method to generate all examples. Again, there are minor technical differences compared to the original models - refer to  neural_style/fast_neural_style/transformer_net.py  for the details. 
 Requirements 
 The program is written in Python, and uses  Theano ,  Keras ,  Scipy , and  tqdm . For Keras, you need to install a slightly modified version. You can do this using:  pip install --upgrade git+https://github.com/jayanthkoushik/keras.git . Instructions for installing the other packages can be found on their respecive websites. A GPU is not necessary, but can provide a significant speed up. There are  additional requirements  to use a GPU. 
 Usage 
 Optimization based approach 
 The general usage is
 bash
python neural_style/slow_neural_style/slow_neural_style.py --content-image </path/to/content/image> --style-image </path/to/style/image> --output-image </path/to/output/image> 
The resulting image will be written to the path specified by  --output-image . There are additional command line arguments to customize the behavior:
*  --content-size : resize the content image such that its longest side is this value before applying the style. The output image will have the same size. Setting a smaller size will speed up generation, and also consume less memory. The default ( None ) is to not resize.
*  --style-size : resize the style image as above before extracting its features. This option can be used to control the scale of transfered features. The default ( None ) is to not resize.
*  --model : use the specified model to extract features. Choices are  vgg16 ,  vgg19 , and  resnet50 . The default is  vgg16 .
*  --content-layer : use this layer to match the content. The possible values depend on the model, and the exact choices can be found by referring to the specific files on the  Keras repository . The default value is  block2_conv2 
*  --style-layers : use these layers to match the style. Specify a space separated list of values following the same conventions as for the content layer. The default value is  block1_conv2 block2_conv2 block3_conv3 block4_conv3 .
*  --content-weight : use this weight for content reconstruction. Default value is 4.
*  --style-weight : use this weight for style reconstruction. Default value is 5e-4.
*  --tv-weight : use this weight for total-variation regularization. Default value is 1e-4.
*  --iterations : run the optimization for this many iterations. Default value is 500.
*  --lr : use this learning rate for the Adam optimizer. Default value is 10.
*  --lr-decay : decay the learning rate linearly with this constant. If this value is  x , and the initial learning rate is  r , then the learning rate after  t  iterations is given by  r /(1 +  tx ). The default value is 5e-3.
*  --normalize-gradient : normalize the gradient to have unit L1 norm. The default is to not normalize the gradient. 
 Feed-forward approach 
 The feed-forward approach requires training a model for a style before it can be used to generate images. To train a model, you need a dataset of images. The pre-trained models were trained using  MSCOCO . The general usage is
 bash
python neural_style/fast_neural_style/fast_neural_style.py train --train-dir </path/to/training/images> --val-dir </path/to/validation/images> --style-image </path/to/style/image> --output-dir </path/to/output/directory> 
There are several command line arguments. Many arguments are the same as for the optimization approach, and you can refer to the script for default values. The new options are:
*  --train-iterations : train for this many iterations. The default is 40000 (about 2 epochs on MSCOCO with the default batch size).
*  --val-iterations : use this many iterations for every validation run. The default is 10.
*  --val-every : validate after this many iterations. The default is 1000.
*  --batch-size : use this many images for each batch of stochastic gradient descent. The default is 4.
*  --perceptual-model : extract perceptual features from this model. This option is the same as  --model  in the optimization approach. The default value is  vgg19 .
*  --test-image : during every validation run, evaluate on this image and save the result. The default ( None ) is to not use any test image.
*  --test-size : resize the test image as described for the optimization approach. The default ( None ) is to not resize.
*  --checkpoint : save the model after every validation. The default is to not checkpoint. 
 The training script will write files to the output directory. Results on the test image (if provided) will be available as  test_iter_<iterations>.jpg . If checkpointing was enabled, checkpoints will be saved as  model_checkpoint_<iterations>.jpg . After training is complete, training and validation losses will be written to  train_losses.pkl  and  val_losses.pkl  respectively, and the final model will be written to  model.h5 . You can also find pre-trained models  here . To evaluate a trained model on a new image, the usage is
 python neural_style/fast_neural_style/fast_neural_style.py eval --content-image </path/to/content/image> --output-image </path/to/output/image> --model </path/to/model/file> 
You can also resize the content image as before with the  --content-size  option. Although the actual image generation takes not more than a few hundred milliseconds, loading the model can take a few seconds. If you want to style many images, it will be faster to use the provided Jupyter notebook  playground.ipynb . This loads a specified model, and you can use it to style images on disk, or the web. SGD-Feedback 
 PLEASE SEE THE UPDATED CODEBASE AT https://github.com/rooa/eve. neural-style-models 
 Trained style transfer models for use with https://github.com/jayanthkoushik/neural-style. torch-gel 
 This package provides PyTorch implementations to solve the group elastic net
problem. Let  A j  ( j = 1 … p ) be feature matrices of sizes  m ×
n j  ( m  is the number of samples, and  n j  is the number
of features in the  j th  group), and let  y  be an  m × 1  vector of
the responses. Group elastic net finds coefficients  β j , and a bias
 β 0  that solve the optimization problem 
 
 min  β 0 , …, β p 
 ½ ║y - β 0  - ∑ A j  β j ║ 2 
    +  m ∑ √n j  (λ 1 ║β j ║ 
 + λ 2 ║β j ║ 2 ). 
 
 Here  λ 1  and  λ 2  are scalar coefficients that control
the amount of 2-norm and squared 2-norm regularization. This 2-norm
regularization encourages sparsity at the group level; entire  β j 
might become 0. The squared 2-norm regularization is in similar spirit to
elastic net, and addresses some of the issues of lasso. Note that group elastic
net includes as special cases group lasso ( λ 2  = 0 ), ridge
regression ( λ 1  = 0 ), elastic net (each  n j  = 1 ), and
lasso (each  n j  = 1  and  λ 2  = 0 ). The optimization
problem is convex, and can be solved efficiently. This package provides two
implementations; one based on proximal gradient descent, and one based on
coordinate descent. 
 Installation 
 Install with  pip 
 bash
pip install torchgel 
 tqdm  (for progress bars), and numpy are pulled in as dependencies. PyTorch
( v1.0+ ) is also needed, and needs to be installed manually. Refer to the
 PyTorch website  for instructions. 
 Usage 
 examples/main.ipynb  is a Jupyter notebook that walks
through using the package for a typical use-case. A more formal description of
the functions follows; and for details about the algorithms, refer to the
docstrings of files in the  gel  directory. 
 Solving Single Instances 
 The modules  gel.gelfista  and  gel.gelcd  provide implementations based on
proximal gradient descent and coordinate descent respectively. Both have similar
interfaces, and expose two main public functions:  make_A  and  gel_solve . The
feature matrices should be stored in a list (say  As ) as PyTorch tensor
matrices, and the responses should be stored in a PyTorch vector (say  y ).
Additionally, the sizes of the groups ( n j ) should be stored in a
vector (say  ns ). First use the  make_A  function to convert the feature
matrices into a suitable format: 
 python
A = make_A(As, ns) 
 Then pass  A ,  y  and other required arguments to  gel_solve . The general
interface is:: 
 python
b_0, B = gel_solve(A, y, l_1, l_2, ns, **kwargs) 
 l_1  and  l_2  are floats representing  λ 1  and  λ 2 
respectively. The method returns a float  b_0  representing the bias and a
PyTorch matrix  B  holding the other coefficients.  B  has size  p × 
max j   n j  with suitable zero padding. The following
sections cover additional details for the specific implementations. 
 Proximal Gradient Descent (FISTA) 
 The  gel.gelfista  module contains a proximal gradient descent implementation.
It's usage is just as described in the template above. Refer to the docstring
for  gel.gelfista.gel_solve  for details about the other arguments. 
 Coordinate Descent 
 The  gel.gelcd  module contains a coordinate descent implementation. Its usage
is a bit more involved than the FISTA implementation. Coordinate descent
iteratively solves single blocks (each corresponding to a single
 β j ). There are multiple solvers provided to solve the individual
blocks. These are the  gel.gelcd.block_solve_*  functions. Refer to their
docstrings for details about their arguments.  gel.gelcd.gel_solve  requires
passing a block solve function and its arguments (as a dictionary). Refer to
its docstring for further details. 
 Solution Paths 
 gel.gelpaths  provides a wrapper function  gel_paths  to solve the group
elastic net problem for multiple values of the regularization coefficients. It
implements a two-stage process. For a given  λ 1  and  λ 2 ,
first the group elastic net problem is solved and the feature blocks with
non-zero coefficients is extracted (the support). Then ridge regression models
are learned for each of several provided regularization values. The final model
is summarized using an arbitrary provided summary function, and the summary for
each combination of the regularization values is returned as a dictionary. The
docstring contains more details.  gel.ridgepaths  contains another useful function,
 ridge_paths  which can efficiently solve ridge regression for multiple
regularization values. 
 Citation 
 If you find this code useful in your research, please cite 
 @misc{koushik2017torchgel,
  author = {Koushik, Jayanth},
  title = {torch-gel},
  year = {2017},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/jayanthkoushik/torch-gel}},
} SGD-Feedback 
 PLEASE SEE THE UPDATED CODEBASE AT https://github.com/rooa/eve. shinyutils 
 Various utilities for common tasks. :sparkles: :sparkles: :sparkles: 
 Setup 
 Install with  pip  (Python 3.8 or higher is required). 
 bash
pip install shinyutils  # basic install
pip install ""shinyutils[colors]""  # install with color support
pip install ""shinyutils[matwrap]""  # install with matwrap module dependencies
pip install ""shinyutils[pt]""  # install with pt module dependencies
pip install ""shinyutils[all]""  # install with all optional dependencies 
 Usage 
 For documentation on usage, refer to docs/index.md. paje 
 paje  is a website creation and deployment system. It is composed of two major parts: 1) A  Jekyll  scaffolding with a clean minimalist theme, and support for math and bibliographies. 2) A  GitHub Actions  workflow to deploy the site using  GitHub Page . 
 Quickstart 
 
 
 Create a new GitHub repository for your site. If creating a personal page, which will be deployed to  <username>.github.io , the repository should be created with that name. 
 
 
 Create a new branch named  source , and switch to it ( git checkout -b source ). This is where the source for your site will live. The  master  branch will be used to deploy the site. 
 
 
 
 :warning:  The  master  branch will be overwritten when using the configuration provided in the quickstart . 
 
 
 Create a file named  index.md  at the root of your repository. This file will contain the root of your site. For now, just add a title: 
 
 ```markdown 
 title: Hello, World 
 ``` 
 
 Create a folder named  .github/workflows , and within it, create a  .yml  file, e.g.,  deploy.yml , with the following contents: 
 
 ```yaml
on:
  workflow_dispatch:
  push:
    branches:
    - source 
 jobs:
  main:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    - uses: jayanthkoushik/paje@v1
      with:
        setupscript: sh build.sh
        targetbranch: master
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
``` 
 
 Create a file named  build.sh . This will be run by  paje  prior to building your site. For now, add a line copying  index.md  to the  /www  folder ( paje  builds the website from this folder): 
 
 txt
cp index.md /www 
 
 
 Commit all three files, and push to GitHub. 
 
 
 In the  Actions  tab on your repository's GitHub page, you should see a new workflow run. This is  paje  building and deploying the site, and should take just a few minutes to run. 
 
 
 Once the workflow is complete (indicated by a check mark), the master branch should have two file,  index.html , and  404.html . The site has been built, and is ready to be deployed! 
 
 
 On the repository's GitHub page, go to the  Pages  section under  Settings . Here, set the source branch to  master , and save. 
 
 
 Your website should now be online! 
 
 
 Adding content 
 paje  uses  pandoc  to convert markdown files to html. Refer to the  docs  for pandoc's markdown syntax. The present guide will indicate elements which need to be written in a particular manner. Additionally, you can use the templating features of Jekyll to make your pages modular. Refer to the  Jekyll docs  for details. Note that any files added to the site should be copied to  /www  in  setup.sh . 
 Additional files 
 You can create additional markdown files to add pages to your site. For each new file, add a line to  setup.sh  copying that file to  /www , e.g.  cp newfile.md /www . This will create a  /newfile  page on your site. Files  must  start with  ---  for them to be recognized as pages. 
 Metadata 
 Metadata should be specified within  ---  at the top of the file, using yaml syntax. The default  paje  template handles the tags shown in the following example: 
 ```yaml 
 title: page title, displayed at the top of the page
subtitle: page sub-title, displayed below the title
description: page description meta data (not displayed)
nomath: true # will disable math support
extcss:
- local1.scss # custom css files for the page
- local2.scss
extjs:
- local1.js # custom javascript files for the page
- local2.js 
 
 ``` 
 Math 
 paje  supports typesetting math using  KaTeX . Inline and block expressions can be added as shown below: 
 ```txt
This is an inline expression: $f(x) = x^2 + 2x + 1$.
This is a block expression: 
 $$
f(x) = \int_{0^\infty} \exp(-x^2) \mathrm{d}x.
$$ 
 Note the empty lines surrounding the block expression.
These are necessary! You can also make equations: 
 $$
\begin{aligned}
f(x) &= sin(x).\
f'(x) &= cos(x).
\end{aligned}
$$ {#eq:ex} 
 And refer to them (@eq:ex) using tags.
``` 
 Bibliography 
 You can include a  bib  file of references, and add citations in your page. The page  must be named  references.bib , and you need to add the following line to  setup.sh :  cp references.bib /www/_includes . Refer to the pandoc guide for syntax used to make citations. 
 Figures 
 .png  images can be added as figures with captions and links: 
 ```txt
This is a figure: 
 {#fig:figid} 
 Note the surrounding empty lines! You can refer to
the figure (@fig:figid) like any other reference.
``` 
 Tables 
 Note that support for tables is finicky. They can be added as such: 
 ```txt
This is a table: 
 Header    col1   col2    col3 
 
 Row       1      2       3 
 : Table caption. {#tbl:tblid} 
 Note the empty lines surrounding the table (@tbl:tblid).
``` corgy 
 Elegant command line parsing for Python. 
 Corgy allows you to create a command line interface in Python, without worrying about boilerplate code. This results in cleaner, more modular code. 
 ```python
from typing import Annotated, Optional, Sequence
from corgy import Corgy
from corgy.types import KeyValuePairs 
 class ArgGroup(Corgy):
    arg1: Annotated[Optional[int], ""optional number""]
    arg2: Annotated[bool, ""a boolean""] 
 class MyArgs(Corgy):
    arg1: Annotated[int, ""a number""] = 1
    arg2: Annotated[Sequence[float], ""at least one float""]
    arg3: Annotated[KeyValuePairs[str, int], ""str to int map""]
    grp1: Annotated[ArgGroup, ""group 1""] 
 args = MyArgs.parse_from_cmdline()
``` 
 Compare this to the equivalent code which uses argparse: 
 ```python
from argparse import ArgumentParser, ArgumentTypeError, BooleanOptionalAction 
 def map_type(s):
    kvs = {}
    try:
        for kv in s.split("",""):
            k, v = kv.split(""="")
            kvs[k] = int(v)
    except Exception as e:
        raise ArgumentTypeError(e) from None
    return kvs 
 parser = ArgumentParser()
parser.add_argument(""--arg1"", type=int, help=""a number"", default=1)
parser.add_argument(""--arg2"", type=float, nargs=""+"", help=""at least one float"", required=True)
parser.add_argument(""--arg3"", type=map_type, help=""str to int map"", required=True) 
 grp_parser = parser.add_argument_group(""group 1"")
grp_parser.add_argument(""--grp1:arg1"", type=int, help=""optional number"")
grp_parser.add_argument(""--grp1:arg2"", help=""a boolean"", action=BooleanOptionalAction) 
 args = parser.parse_args()
``` 
 Corgy also provides support for more informative help messages from  argparse , and colorized output. Compare: 
 
 Install 
 corgy  is available on PyPI, and can be installed with pip: 
 bash
pip install corgy 
 Support for colorized output requires the  crayons  package, also available on PyPI. You can pull it as a dependency for  corgy  by installing with the  colors  extra: 
 bash
pip install corgy[colors] 
 Parsing  Corgy  objects from  toml  files requires the  tomli  package on Python versions below 3.11. This can be installed automatically with the  toml  extra: 
 bash
pip install corgy[toml] 
 Usage 
 For documentation on usage, refer to docs/index.md."
yangyi02,"fnn_paper 
 Paper writing for feedback neural networks awesome_ai 
 This aims to organize the latest AI research that I have learned. video_motion_mnist video_motion video_motion_synthetic Endpoint Error Results Summary 
 | Exp  | 01   | 02   | 03   | 04   | 05   | 06   | 07   | 08   | 09   | 10   | 11   | 12   | Comp | Eval | Note |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| 001  | 0.07 | 0.07 | 0.08 | 0.09 | 0.15 | 0.14 | 0.11 | 0.13 | 0.18 | 0.22 | 0.26 | 0.26 |      | Good | The simplest baseline |
| 002  | 0.01 | 0.01 | 0.03 | 0.06 | 0.08 | 0.04 | 0.04 | 0.03 | 0.12 | 0.19 | 0.18 | 0.12 | 001  | Good | Moving pixel occlude static pixel |
| 003  | 0.00 | 0.01 | 0.02 | 0.05 | 0.05 | 0.03 | 0.27 | 0.33 | 1.68 | 1.74 | 0.15 | 0.33 | 002  | Bad  | New appear pixel not in loss |
| 004  | 0.01 | 0.01 | 0.03 | 0.06 | 0.09 | 0.05 | 0.03 | 0.03 | 0.13 | 0.17 | 0.17 | 0.10 | 002  | Good | Decompose x and y |
| 005-1| 0.04 | 0.06 | 0.06 | 0.06 | 0.15 | 0.11 | 0.10 | 0.11 | 0.17 | 0.17 | ---- | ---- | 002  | Bad  | Neural net predict disappear |
| 005  | 0.19 | 0.17 | 0.26 | 0.20 | 0.12 | 0.20 | 0.43 | 0.69 | 1.87 | 2.25 | 0.81 | 0.56 | 003  | Bad  | Neural net predict disappear |
| 006  | 0.02 | 0.02 | 0.02 | 0.04 | 0.06 | 0.05 | 0.08 | 0.03 | 0.06 | 0.10 | 0.15 | 0.12 | 005-1| Good | Old pixel loss divided by total number of old pixels | 
| 006-1| 0.00 | 0.01 | 0.02 | 0.05 | 0.05 | 0.03 | 0.01 | 0.02 | 0.10 | 0.19 | 0.09 | 0.08 | 002  | Good | Old pixel loss divided by total number of old pixels |
| 007  | 0.02 | 0.02 | 0.04 | 0.05 | 0.07 | 0.05 | 0.08 | 0.09 | 0.13 | 0.15 | 0.19 | 0.18 | 006  | Bad  | Use avearge value at occlusion |
| 008  | 0.01 | 0.01 | 0.00 | 0.03 | 0.07 | 0.04 | 0.03 | 0.09 | 0.05 | 0.06 | 0.12 | 0.97 | 006  | Bad  | New appear and occlude location both not in loss |
| 008-1| 0.01 | 0.01 | 0.01 | 0.03 | 0.05 | 0.03 | 0.03 | 0.04 | 0.12 | 0.05 | 0.12 | 0.13 | 008  | Good | Extra loss for total number of new and conflicting pixels |
| 009  | 0.12 | 0.01 | 0.01 | 0.03 | 0.06 | 0.04 | 0.01 | 0.12 | 0.04 | 0.07 | 0.14 | 0.83 | 008  | Bad  | Decompose x and y |
| 010  | 0.08 | 0.01 | 0.00 | 0.02 | 0.05 | 0.04 | 0.05 | 0.22 | 0.28 | 0.05 | 0.13 | 0.94 | 008  | Bad  | Wider network, proves exp008 is bad |
| 011  | 0.01 | 0.01 | 0.02 | 0.04 | 0.06 | 0.04 | 0.04 | 0.03 | 0.07 | 0.12 | 0.12 | 0.11 | 006  | Good | Wider network |
| 012  | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 000  |      | |
| 013  | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 000  |      | |
| 014  | 0.04 | 0.05 | 0.03 | 0.06 | 0.13 | 0.09 | 0.09 | 0.09 | 0.14 | 0.14 | 0.27 | 0.20 | 001  | Good | Predict relative depth |
| 015  | 0.02 | 0.03 | 0.03 | 0.03 | 0.15 | 0.05 | 0.11 | 0.12 | 0.04 | 0.06 | 0.17 | 0.38 | 014  | Good | Old pixel loss divided by total number of old pixels |
| 016  | 0.02 | 0.04 | 0.04 | 0.06 | 0.08 | 0.07 | 0.07 | 0.11 | 0.11 | 0.22 | 0.19 | 0.19 | 014  | Bad  | Bidirectional model |
| 017  | 0.04 | 0.03 | 0.04 | 0.04 | 0.12 | 0.08 | 0.09 | 0.09 | 0.04 | 0.20 | 0.28 | 0.43 | 015  | Bad  | Add a few more layers at the bottom of neural net |
| 018  | 0.05 | 0.04 | 0.04 | 0.02 | 0.08 | 0.03 | 0.12 | 0.16 | 0.09 | 0.06 | 0.13 | 0.38 | 015  | Bad  | Predict depth using only one image |
| 019  | 0.00 | 0.05 | 0.00 | 0.06 | 0.07 | 0.07 | 0.01 | 0.07 | 0.02 | 0.06 | 0.22 | 0.21 | 018  | Good | Add segmentation temporal consistency loss |
| 020  | 0.00 | 0.04 | 0.01 | 0.03 | 0.09 | 0.08 | 0.03 | 0.07 | 0.02 | 0.06 | 0.20 | 0.18 | 019  | Good | Bidirectional model |
| 021  | 0.00 | 0.01 | 0.00 | 0.02 | 0.06 | 0.03 | 0.02 | 0.03 | 0.01 | 0.05 | 0.22 | 0.17 | 019  | Good | Add flow smoothness loss |
| 000  | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 000  |      | | 
 | Exp  | 01   | 02   | 03   | 04   | 05   | 06   | 07   | 08   | 09   | 10   | 11   | 12   | Comp | Eval | Note |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| 001  | 0.07 | 0.07 | 0.08 | 0.09 | 0.15 | 0.14 | 0.11 | 0.13 | 0.18 | 0.22 | 0.26 | 0.26 |      | Good | The simplest baseline |
| 002  | 0.01 | 0.01 | 0.03 | 0.06 | 0.08 | 0.04 | 0.04 | 0.03 | 0.12 | 0.19 | 0.18 | 0.12 | 001  | Good | Moving pixel occlude static pixel |
| 006-1| 0.00 | 0.01 | 0.02 | 0.05 | 0.05 | 0.03 | 0.01 | 0.02 | 0.10 | 0.19 | 0.09 | 0.08 | 002  | Good | Old pixel loss divided by total number of old pixels |
| 021  | 0.00 | 0.01 | 0.00 | 0.02 | 0.06 | 0.03 | 0.02 | 0.03 | 0.01 | 0.05 | 0.22 | 0.17 | 019  | Good | Add flow smoothness loss | 
 Take Home Message 
 
 We should use: new appear pixel not in loss, loss divided by total number of old pixels 
 Endpoint Error Results Summary 
 | Exp  | 01   | 02   | 03   | 04   | 05   | 06   | 07   | 08   | 09   | 10   | 11   | 12   | Comp | Eval | Note |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| 001  | 0.06 | 0.06 | 0.14 | 0.13 | 0.14 | 0.12 | 0.13 | 0.12 | 0.24 | 0.24 | 0.26 | 0.23 | ---- | ---- | The simplest baseline |
| 002  | 0.03 | 0.03 | 0.12 | 0.13 | 0.09 | 0.08 | 0.07 | 0.06 | 0.23 | 0.25 | 0.18 | 0.16 | 001  | Good | Moving pixel occlude static pixel |
| 003  | 0.04 | 0.03 | 0.11 | 0.12 | 0.08 | 0.07 | 0.29 | 0.25 | 0.32 | 0.37 | 0.11 | 0.23 | 002  | Bad  | New appear pixel not in loss |
| 004  | 0.03 | 0.03 | 0.11 | 0.11 | 0.08 | 0.06 | 0.04 | 0.04 | 0.18 | 0.23 | 0.10 | 0.11 | 002  | Good | Old pixel loss divided by total number of old pixels | 
| 005  | 0.04 | 0.03 | 0.14 | 0.14 | 0.10 | 0.12 | 0.06 | 0.06 | 0.24 | 0.22 | 0.16 | 0.22 | 004  | Bad  | New appear and occlude location both not in loss |
| 006  | 0.08 | 0.05 | 0.15 | 0.14 | 0.14 | 0.12 | 0.08 | 0.06 | 0.25 | 0.21 | 0.20 | 0.17 | 004  | Bad  | Neural net predict disappear |
| 007  | 0.03 | 0.03 | 0.11 | 0.12 | 0.08 | 0.07 | 0.06 | 0.06 | 0.19 | 0.23 | 0.14 | 0.15 | 004  | Bad  | Use avearge value at occlusion | 
 | 008  | 0.05 | 0.05 | 0.14 | 0.13 | 0.12 | 0.12 | 0.08 | 0.18 | 0.24 | 0.25 | 0.22 | 0.35 | 004  | Bad  | Predict relative depth |
| 009  |  |  |  |  |  |  |  |  |  |  |  |  |   |  | Predict relative depth using only one image |
| 010  |  |  |  |  |  |  |  |  |  |  |  |  |   |  | Add segmentation temporal consistency loss | 
 | 009  |  |  |  |  |  |  |  |  |  |  |  |  |   |  | Decompose x and y |
| 010  |  |  |  |  |  |  |  |  |  |  |  |  |   |  | Wider network, proves exp008 is bad |
| 011  |  |  |  |  |  |  |  |  |  |  |  |  |   |  | Wider network |
| 014  |  |  |  |  |  |  |  |  |  |  |  |  |   |  | Predict relative depth |
| 015  |  |  |  |  |  |  |  |  |  |  |  |  |   |  | Old pixel loss divided by total number of old pixels |
| 016  |  |  |  |  |  |  |  |  |  |  |  |  |   |  | Bidirectional model |
| 017  |  |  |  |  |  |  |  |  |  |  |  |  |   |  | Add a few more layers at the bottom of neural net |
| 018  |  |  |  |  |  |  |  |  |  |  |  |  |   |  | Predict depth using only one image |
| 019  |  |  |  |  |  |  |  |  |  |  |  |  |   |  | Add segmentation temporal consistency loss |
| 020  |  |  |  |  |  |  |  |  |  |  |  |  |   |  | Bidirectional model |
| 000  | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 000  |      | | 
 | Exp  | 01   | 02   | 03   | 04   | 05   | 06   | 07   | 08   | 09   | 10   | 11   | 12   | Comp | Eval | Note |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| |  |  |  |  |  |  |  |  |  |  |  |  | |  | The simplest baseline |
| |  |  |  |  |  |  |  |  |  |  |  |  | |  | Moving pixel occlude static pixel |
| |  |  |  |  |  |  |  |  |  |  |  |  | |  | Old pixel loss divided by total number of old pixels |
| |  |  |  |  |  |  |  |  |  |  |  |  | |  | Old pixel loss divided by total number of old pixels | 
| |  |  |  |  |  |  |  |  |  |  |  |  | |  | Wider network |
| |  |  |  |  |  |  |  |  |  |  |  |  | |  | Old pixel loss divided by total number of old pixels |
| |  |  |  |  |  |  |  |  |  |  |  |  | |  | Extra loss for total number of new and conflicting pixels | 
 Take Home Message 
 
 We should use: new appear pixel not in loss, loss divided by total number of old pixels 
 This is a continuation of video_motion2 that optimize the gpu memory usage to fit larger models and images. 
 
 No testing during training 
 Save intermediate models every hundred steps 
 Testing after the training finish 
 Adding bidirectional model 
 This is a continuation of video_motion3 that generate more realistic synthetic videos. Whether Unsupervised Depth and Optical Flow Help Few Shot Supervised Learning 
 Requirement 
 
 PyTorch >=0.4.0 
 Python 2.x 
 
 How to run the code 
 You only need to use 4 folders (mlt_attn, viper, kitti_det, cityscape) and they are all independent. 
 kitti_det: 
 
 Open  set_env.sh , change  DATA_PATH  and  CODE_PATH  to your own kitti data and code path. 
 Go to  scripts  folder, run  bash base.sh . 
 To run in cluster, go to  scripts_cluster  folder, run  bash base.sh . 
 
 cityscape: 
 
 Open  set_env.sh , change  DATA_PATH  and  CODE_PATH  to your own cityscape data and code path. 
 Go to  file_list  folder, open  create_cityscape_filelist.py  and change  cityscape_dir  to your own cityscape data path, and run  python create_cityscape_filelist.py . 
 Go to  scripts  folder, run  bash base.sh . 
 
 vdrift: 
 
 Go to  prepare  folder, open  create_seg_class.py  and  change  seg_path  and  seg_cls_path , then run it.  
 Open  set_env.sh , change  DATA_PATH  and  CODE_PATH  to your own vdrift data and code path. 
 Go to  scripts  folder, run  bash base.sh . 
 
 mlt_attn: 
 
 Open  mlt_data.py , change  self.img_dir ,  self.depth_dir ,  self.box_dir  to your own mlt data path. 
 To prepare the bounding boxes, you need to run  obtain_box.ipynb  first. 
 mkdir logs . 
 Go to  exps , run  bash base.sh . 
 
 Motivation 
 Although deep neural networks have achieved promising results on visual recognition, human uses much fewer supervised training labels to reach to the same level performance. In this work, we study how much depth, optical flow help object recognition, localization, detection and segmentation in cluttered scenes, particularly when there are not enough training labels.
- There is an intuition that depth can significantly help attention and detection for objects in clutted scenes. There is also another intuition that optical flow can help to attention to the moving objects.
- To obtain depth and flow, we use ground truth existed in the some datasets (i.e.  MLT ,  VDrift ). For other real world datasets (i.e.  KITTI ,  CityScape ), we use the unperfect unsupervisedly learned depth and optical flow estimation from  Joint Unsupervised Learning of Optical Flow and Depth by Watching Stereo Videos . 
 Problem Setting 
 Our ultimate goal is to study how much depth and optical flow can help high level semantic level tasks such as object recognition, localization, detection and segmentation, particularly when there are not enough training data. Hence our problem setting is below:
1. Given a direction keyword (i.e. ""top left"") and a cluttered RGB image (with or without depth & optical flow), how is the recognition accuracy improves?
2. Given an object keyword (i.e. ""television"") and a cluttered RGB image (with or without depth & optical flow), how is the localization accuracy improves?
3. Given a few supervised training labels, how does the unsupervised depth & optical flow help recognition / localization / detection /segmentation accuracy? 
 Dataset 
 We mainly use four datasets for experiments.
1.  MLT dataset : This is the main dataset we use to study depth effect because they contain ground truth object semantic segmentation, instance segmentation, depth, etc. MLT dataset contains 45K synthetic 3D indoor scenes with 400K fully annotated rendered images from different viewpoints.
2.  KITTI dataset : This is one standard dataset to use for detection evaluation. It also evaluates depth and flow.
3.  VDrift dataset : This is a synthetic dataset for semantic segmentation with ground truth depth and flow. 
4.  CityScape dataset : This is another standard dataset to use for segmentation evaluation. It does not contain depth and flow. 
 Some other dataset not used in the final experiments. 
1.  Mnist dataset : This is simply for debug model and code.
2.  VIPER dataset : This is the other main dataset we use to study optical flow effect because they contain ground truth object semantic segmentation, instance segmentation, optical flow, etc. VIPER dataset contains 250K fully annotated high resolution video frames. 
 | Datast | MLT | KITTI | VDrift | CityScape |
| ------ | ------ | ------ | ------ | ------ | 
| Task   | Recognition / Localization | Detection | Segmentation | Segmentation |
| Data   | RGB + D | RGB | RGB + D + F | RGB |
| Stereo | No | Yes | No | Yes |
| GT Depth | Yes | No | Yes | No |
| GT Flow | No | No | Yes | No |
| # Train |10742 | 5984 | 1264 | 2975 | 
| # Test | 4605 | 1497 | 317 | 500 | 
 Benchmark & Criteria 
 We use the standard benchmark accuracy as the quantitative measure:
1. The recognition benchmark is very simple, it is the classification accuracy.
2. The localization benchmark is same as recognition, using key words (i.e. ""top left"") to represent a location and then use classification accuracy.
3. We will further replace the localization problem to another word prediction problem (i.e. left, right, top, down, etc.), then we will merge the two tasks into one task which is word prediction (i.e. car, pedestrian or left, right). We will then use the word prediction accuracy as the final measure.
4. The detection benchmark is the mean average precision, where a bounding box is considered as correct if its IoU with the ground truth box is over 50%. 
5. The segmentation benchmark is the mean IoUs, which is the average intersection over union. 
 Model 
 We use different models on different tasks. In general, we use attention models for recognition and localization
1. We find it is straightforward to use the attention models for recognition and localization. There are two variations of attention models: 
(1) Hard attention models such as  Spatial Transformer Networks  and  Recurrent Model of Visual Attention .
(2) Soft attention models such as  Show, Attend and Tell . In this work, we also study the effect of these two different attention models on the recognition and localization tasks.
2. We use fully convolutional networks with skip connections upsampling for detection and segmentation. 
 Preliminary Results 
 You can see the full preliminary results in  results.pdf . 
 5/3/2018 
 
 Summarize the performance of using the bounding box to align v.s. no aligned and using RGB + depth v.s. RGB only, on  MLT dataset  with 7000 training images and 700 testing images for 6 most frequent classes.  
 Try to train a residual network to beat the previous VGG network, but residual network obtains worse performance on all different tasks. Need to figure out why. 
 
         
 5/10/2018 
 
 To make training/testing more stable, we now use 10742 training images and 4605 testing images for 6 most frequent classes on  MLT dataset . 
 Performance summarization: Random guess is 25%, brute-forcely train with only image 40%, brute-forcely train image with box 62%, suggesting a high location prior in the image, spatial transformer attention crop with gt box 69%.
 
 
 5/17/2018 
 
 The hard attention model (Spatial Transformer Networks) does not work on MLT dataset so far. I find the model struggling in finding the correct object location and is very sensitive to initialization. 
 Debug Spatial Transformer Networks on Mnist dataset and find it even fails on simple Mnist data. 
 
       
 5/24/2018 
 
 The Spatial Transformer Networks can work on Mnist dataset now, when Mnist digits are randomly uniformly located in the image. Both image-based attention and word-based attention can provide reasonable attention. Jointly training with them can achieve even better and faster convergence on learning classifiers and attention models. 
 
     
 5/31/2018 
 
 Conduct comparison between hard attention model and soft attention model on Mnist dataset.  
 Conclusion: using soft attention model performs much smoother and faster convergence than previous hard attention model (Spatial Transformer Networks). The oracle performance of soft attention model may be worse than the best hard attention model on recognition, however, in reality training converges much smoother. 
 Here I show the training convergence using the soft attention model and a baseline hard attention model (orange) on Mnist dataset. One can see the three (red, cyan, gray) soft-attention curves all converge much faster than the (orange) hard-attention (Spatial Transformer Networks) curve. For more hard attention model performances, please see the last week (5/24/2018) note. 
 
     
 6/7/2018 
 
 Now consistently use soft attention model and switch back to MLT dataset because Mnist dataset is too simple. 
 On MLT dataset, there are two main promising conclusions:  
 Adding depth significantly helps image recognition. For example, the testing recognition accuracy increases from 42% to 48% by adding depth. And after adding top-down direction signal, the testing accuracy further increases from 48% to 72% which is about 30% total absolute improvement to the baseline RGB only. 
 Adding top-down direction as keyword to obtain attention significantly helps image recognition. For example, the testing recognition accuracy increases from 48% to 61% by adding top-down direction on RGB image. 
 In total, the depth and direction keyword can significantly improve recognition accuracy from 42% to 72%, increasing 30%, and the training actually has not converged yet. And all curves haven't converged yet. 
 
     
 
 Input multiple scale images (256x256 + 128x128) 
 
 
 
 Attention maps (8x8 + 4x4) 
 
 yangyi02.github.io 
 Personal website Recognizing Proxemics in Personal Photos 
 Introduction 
 This is a Matlab implementation of proxemics recognition described in [1]. It includes a completely new dataset with training, testing, evaluation and visualization code. Much of the training and detection code is built on top of flexible mixtures-of-part model [2] and deformable part-based model [3]. The training code implements a quadratic program (QP) solver described in [4]. 
 To illustrate the use of the training code, this package uses positive images from the new PROXEMICS dataset, and negative images from the INRIA Person Background dataset [5]. We also include the new Percentage of Correctly Localized Keypoints (PCK) evaluation code from [6] for benchmark evaluation on pose estimation. 
 The code also makes use of the face detection results obtained from Microsoft Research.  
 Compatibility issues: The training code may require 4.5GB of memory. Modify line 32/33 in  learning/train.m  to use less memory at the cost of longer training times. 
 Acknowledgements: We graciously thank the authors of the previous code releases and image benchmarks for making them publically available. 
 Using the code 
 
 Download the  PROXMEMICS dataset (89MB)  and  INRIA Person Background dataset (59MB) , put them into  data/PROXEMICS  and  data/INRIA  respectively. Or you can simply call  bash download_data.sh  in Linux system. 
 Start Matlab (version >2013a). 
 Run  compile.m  to compile the helper functions. (You may also edit  compile.m  to use a different convolution routine depending on your system.) 
 Run  PROXSUB_demo.m  to see the training and detecting one particular proxemic submixture. 
 Or run  PROX_demo.m  to see the complete system for training and detecting one particular proxemic. 
 By default, the code is set to output the highest-scoring detection in an image given the two people's face bounding boxes detected from a face detector. 
 
 References 
 [1] Y. Yang, S. Baker, A. Kannan, D. Ramanan.  Recognizing Proxemics in Personal Photos . CVPR 2012. 
 [2] Y. Yang, D. Ramanan.  Articulated Pose Estimation using Flexible Mixtures of Parts . CVPR 2011. 
 [3] P. Felzenszwalb, R. Girshick, D. McAllester, D. Ramanan.  Discriminatively Trained Deformable Part Models . PAMI 2010. 
 [4] D. Ramanan.  Dual Coordinate Descent Solvers for Large Structured Prediction Problems . UCI Technical Report 2014. 
 [5] N. Dalal, B. Triggs.  Histograms of Oriented Gradients for Human Detection . CVPR 2005. 
 [6] Y. Yang, D. Ramanan.  Articulated Human Detection with Flexible Mixtures of Parts . PAMI 2013. Articulated Human Pose Estimation with Flexible Mixtures-of-Parts 
 Introduction 
 This is a Matlab implementation of the human pose estimation algorithm described in [1, 2]. It includes pre-trained full-body and upper-body models. Much of the detection code is built on top of deformable part-based model implementation [3]. The training code implements a quadratic program (QP) solver described in [4]. 
 The code is trained and tested using positive images from the PARSE dataset [5], the BUFFY dataset [6], and negative images from the INRIA Person Background dataset [7]. 
 Compatibility issues: The training code requires 7.5GB of memory. Modify line 32/33 in  code_full/learning/train.m  to use less memory at the cost of longer training times. 
 Acknowledgements: We graciously thank the authors of the previous code releases and image benchmarks for making them publically available. 
 Using the detection code 
 
 Move to the  code_basic  directory 
 Start Matlab (version > 2013a). 
 Run  compile.m  to compile the helper functions. (You may also edit  compile.m  to use a different convolution routine depending on your system.) 
 Run  demo.m  to see the detection code run on sample images. 
 By default, the code is set to output the highest-scoring detection in an image. 
 
 Using the learning code 
 
 Move to the  code_full  directory 
 Download the PARSE dataset (2.5MB), BUFFY dataset (21MB) and INRIA Person Background dataset (59MB), put them into  data/PARSE ,  data/BUFFY  and  data/INRIA  respectively. Or you can simply call  bash download_data.sh  in Linux system.  
 Start Matlab (version > 2013a). 
 Run  compile.m  to compile the helper functions. (You may also edit  compile.m  to use a different convolution routine depending on your system.) 
 Run  PARSE_demo.m  or  BUFFY_demo.m  to see the complete system including training and benchmark evaluation. 
 
 References 
 [1] Y. Yang, D. Ramanan.  Articulated Pose Estimation using Flexible Mixtures of Parts . CVPR 2011. 
 [2] Y. Yang, D. Ramanan.  Articulated Human Detection with Flexible Mixtures of Parts . PAMI 2013. 
 [3] P. Felzenszwalb, R. Girshick, D. McAllester, D. Ramanan.  Discriminatively Trained Deformable Part Models . PAMI 2010. 
 [4] D. Ramanan.  Dual Coordinate Descent Solvers for Large Structured Prediction Problems . UCI Technical Report 2014. 
 [5] D. Ramanan.  Learning to Parse Images of Articulated Bodies . NIPS 2006. 
 [6] V. Ferrari, M. Eichner, M. Marin-Jimenez, A. Zisserman.  Buffy Stickmen V3.01: Annotated data and evaluation routines for 2D human pose estimation . IJCV 2012.  
 [7] N. Dalal, B. Triggs.  Histograms of Oriented Gradients for Human Detection . CVPR 2005. 
 Version Update 
 pose-release-v1.4
1. Springs depend on both parent and child mixtures, not child mixtures any more.
2. Co-occurrence term learns hard mixtures co-occurrence (two mixtures never co-occur) during training.
3. New visualization codes for visualizing data, model, clustering results etc.
4. Remove latent mixture update during joint training 
5. New data structure for human detection and pose estimation.
6. Clustering features for every part depend on its parent and children. 
 | PARSE | Head | Shou | Elbo | Wris | Hip  | Knee | Ankl | Avg  |
| ----  | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| APK   | 88.3 | 83.9 | 54.3 | 34.9 | 76.2 | 65.5 | 57.5 | 65.8 |
| PCK   | 92.0 | 87.8 | 65.6 | 50.0 | 81.2 | 74.9 | 69.3 | 74.4 |  
 pose-release-v1.3
1. New convolution and other necessary files for windows machine to run the code.
2. New PCK and APK benchmarks, delete the old PCP criteria.
3. New functions for getting the highest score detection with overlap requirement.
4. First iteration joint training uses fixed mixture labels.
5. New visualization functions for showing the highest score detection and multiple detections.
6. New training code.
7. New non-maximum suppression after detection. 
 | PARSE | Head | Shou | Elbo | Wris | Hip  | Knee | Ankl | Avg  |
| ----  | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| APK   | 86.9 | 83.9 | 57.2 | 30.6 | 73.9 | 62.6 | 56.4 | 64.5 |
| PCK   | 89.5 | 86.8 | 67.6 | 48.0 | 80.2 | 74.6 | 69.5 | 73.8 |  
 pose-release-v1.2
First time release Layered Object Detection for Multi-Class Segmentation 
 Introduction 
 This is a Matlab implementation of the layered object segmentation algorithm described in [1, 2]. The algorithm utilizes the object detection results obtained from deformable part-based models [3] with superpixels obtained from [4], and builds a Bayesian inference framework towards the semantic and instance segmentation of multi-class objects. The code is trained and tested using the PASCAL VOC dataset [5].  
 Acknowledgements: We graciously thank the authors of the previous code releases and image benchmarks for making them publically available. 
 Using the code 
 The Matlab code is actually not runnable anymore. The repo is only used for a proof of concept.  
 The codes are mainly located in  initialization ,  bias field  and  segmentation , where  bias field  contains the bias field learning code and  segmentation  contains the layered segmentation inference code. 
 References 
 [1] Y. Yang, S. Hallman, D. Ramanan, C. Fowlkes.  Layered Object Detection for Multi-Class Segmentation . CVPR 2010. 
 [2] Y. Yang, S. Hallman, D. Ramanan, C. Fowlkes.  Layered Object Models for Image Segmentation . PAMI 2012. 
 [3] P. Felzenszwalb, R. Girshick, D. McAllester, D. Ramanan.  Discriminatively Trained Deformable Part Models . PAMI 2010. 
 [4] P. Arbelaez, M. Maire, C. Fowlkes, J. Malik.  Contour Detection and Hierarchical Image Segmentation . PAMI 2011. 
 [5] M. Everingham, L. Van Gool, J. Winn, A. Zisserman.  The PASCAL Visual Object Classes (VOC) Challenge . IJCV 2010. 3D Pose Annotation for Fine-Grained Object Categories 
 
 
 
 Contents of Directories 
 
 Anno3D: 3D pose annotations for StanfordCars/FGVC_Aircraft/CompCars 
 CAD: Sample CAD models from ShapeNet for visualization use; list of CAD models used in our datasets 
 Image: Sample images from the fine-grained datasets for visualization use 
 Scripts: Sample scripts to visualize the annotations 
 
 Format of Annotations 
 The annotations are packed in pickle (.pkl) files, each of which include a dictionary of annotations,  
 dict[key] = anno , 
 where  key  is the filename, and  anno  is a dictionary containing the following fields:
-  model_id : the corresponding fine-grained 3D CAD model ID from ShapeNet
-  azimuth, elevation, theta : the rotation angles introduced in Section 3.2 of the paper
-  distance : the parameter 'd' introduced in Section 3.2 of the paper
-  u, v : the principal point (u, v) introduced in Equation (3) 
-  f : the focal length f in Equation (3) 
 Usage 
 To use the 3D pose annotations, you need to download the 2D images from the fine-grained recognition datasets as well as the
3D models from the ShapeNet dataset. 
 1. Download 2D Images 
 (1) StanfordCars: 
 http://imagenet.stanford.edu/internal/car196/cars_train.tgz 
 http://imagenet.stanford.edu/internal/car196/cars_test.tgz 
 After downloading, put the extracted directory  cars_train  and  cars_test  under  Image/StanfordCars . 
 (2) FGVC_Aircraft: 
 http://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/archives/fgvc-aircraft-2013b.tar.gz 
 After downloading, put the extracted directory  fgvc-aircraft-2013b/data/images  under  Image/FGVC_Aircraft . 
 (3) CompCars: 
 https://www.dropbox.com/sh/46de2cre37fvzu6/AABXtX8QqA6sx37k1IyZmNQ2a?dl=0 
 Please contact the data provider of CompCars (yljatthu@gmail.com) for the extraction password.
After downloading, put the extracted directory  data/image  under  Image/CompCars . 
 2. Download 3D CAD Models 
 (1) Cars (For StanfordCars/CompCars Use) 
 http://shapenet.cs.stanford.edu/shapenet/obj-zip/ShapeNetCore.v1/02958343.zip 
 After downloading, put the extracted  02958343  under CAD/
Please see  CAD/StanfordCars3D.txt  and  CAD/CompCars3D.txt  for the list of CAD models used in our 3D pose datasets. 
 (2) Aircraft (For FGVC_Aircraft Use) 
 http://shapenet.cs.stanford.edu/shapenet/obj-zip/ShapeNetCore.v1/02691156.zip 
 After downloading, put the extracted  02691156  under CAD/
Please see  CAD/FGVC_Aircraft3D.txt  for the list of CAD models used in our 3D pose datasets. 
 Visualization and Scripts 
 To visualize the samples, run
 cd Scripts && bash ./sample_visualize.sh 
 The usage of our annotations can be seen from the following python scripts,
-  utils.py : containing the code for projection as well as converting the angles to rotation matrix and a simple .obj file
reader
-  sample_visualize.py : containing the code for visualization 
 Publications 
 
 3D Pose Estimation for Fine-Grained
Object Categories 
 Improving Annotation for 3D Pose Dataset of Fine-Grained Object Categories 
 
 Citation 
 Please cite the paper in your publications if it helps your research: 
 @inproceedings{wang20183d,
  title={3D Pose Estimation for Fine-Grained Object Categories},
  author={Wang, Yaming and Tan, Xiao and Yang, Yi and Liu, Xiao and Ding, Errui and Zhou, Feng and Davis, Larry S},
  booktitle={European Conference on Computer Vision Workshop},
  pages={619--632},
  year={2018},
  organization={Springer}
}

@article{wang2018improving,
  title={Improving Annotation for 3D Pose Dataset of Fine-Grained Object Categories},
  author={Wang, Yaming and Tan, Xiao and Yang, Yi and Li, Ziyu and Liu, Xiao and Zhou, Feng and Davis, Larry S},
  journal={arXiv preprint arXiv:1810.09263},
  year={2018}
}
 Sequential Convex Approximations to Joint Chance Constrained Programs: A Monte Carlo Approach 
 Introduction 
 This is a Matlab implementation of the sequential convex approximation algorithms for joint chance constrained problem. It includes a comparison between both conditional value-at-risk (CVaR) and sequential convex approximation for value-at-risk (Iterative dc). 
 Using the code 
 Use Matlab to run  example_run.m  directly. You may expect to see the result figure below: 
 
 Files explanation: 
 
 example_run.m : runing file, first open 
 main_function.m : including generating samples, apply cvar approximation, epsilon approximation and dc approximation, return results for a particular setting 
 gensample.m : generate normal distributions for all random variables 
 obj_fun.m : objective function 
 quantile.m : quantile for constraints 
 opt_cvar.m, opt_dc.m, opt_eps.m : optimization for cvar, one step dc approximation, epsilon approximation 
 con_fun_cvar.m, con_fun_dc.m, con_fun_eps.m : constraints for cvar, one step dc approximation, epsilon approximation 
 lincave.m : linear approximation for concave function 
 
 Citation 
 @article{hong2011sequential,
  title={Sequential convex approximations to joint chance constrained programs: A Monte Carlo approach},
  author={Hong, L Jeff and Yang, Yi and Zhang, Liwei},
  journal={Operations Research},
  volume={59},
  number={3},
  pages={617--630},
  year={2011},
  publisher={INFORMS}
}
 Demo on Vision Language Learning through PADDLE 
 
 
 
 Introduction 
 Human has the remarkable capability of grounding language with vision. 
In Artificial Intelligence, visual language grounding such as image captioning and image question answering are very important but challenging research topics. 
Recently, deep learning shows a great success in speech recognition and computer vision. 
We show that with deep neural networks such as Convolutional Neural Nets and Recurrent Neural Nets, we can further embrace computer vision and natural language. 
 At Baidu IDL, we started this research project in 2014, under the leadership of Baidu's Distinguished Scientist, Wei Xu, with a list of publications below: 
 
 Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN) 
 Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images 
 Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering 
 CNN-RNN: A Unified Framework for Multi-label Image Classification 
 ABC-CNN: An Attention Based Convolutional Neural Network for Visual Question Answering 
 Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks 
 
 The rest of contributors are Junhua Mao, Jiang Wang, Zhiheng Huang, Haoyuan Gao, Lei Wang, Kan Chen, Haonan Yu, Yi Yang, et al.   
 To use this code, you need to download and compile the latest version of  Paddle . 
 Directory Explanation 
 
 deploy_demo: demo shows caption generation given a pre-trained image captioning model 
 train_demo: demo shows training image captioning, image question answering, or multi-task models 
 full_code: the full code that trains and evaluates image captioning and image question answering models on large complicated dataset, such as Microsoft COCO. 
 Suppose the codes are located at deeplab/code 
 
 Create a folder for experiments at deeplab/exper 
 
 2.1 Create a folder for your specific experiment, let's take PASCAL VOC 2012 for example, at deeplab/exper/voc12 
 2.2 Create folders for voc12:
deeplab/exper/voc12/config      : where network config files are saved
deeplab/exper/voc12/features  : where the computed features will be saved (when train on train)
deeplab/exper/voc12/features2 : where the computed features will be saved (when train on trainval)
deeplab/exper/voc12/list : where you save the train, val, and test file lists
deeplab/exper/voc12/log : where the training/test logs will be saved
deeplab/exper/voc12/model : where the trained models will be saved
deeplab/exper/voc12/res : where the evaluation results will be saved 
 3.1 Test your own network: create a folder under config. For example, config/deeplab_largeFOV, where deeplab_largeFOV is the network you want to experiment with. Add your train.prototxt and test.prototxt in that folder (you can check some provided examples for reference) . 
 3.2 Set up your init.caffemodel at model/deeplab_largeFOV. You may want to soft link init.caffemodel to the modified VGG-16 net. 
 4.1 Then, modify the provided script for experiments: run_pascal.sh, where you should change the paths according to your setting. For example, you should specify where the caffe is by changing CAFFE_DIR. 
 4.2 You may need to modify sub.sed, if you want to replace some variables with your desired values in train.prototxt or test.prototxt. 
 
 The computed features are saved at folders features or features2, and you can run provided MATLAB scripts to evaluate the results (e.g., check the script at code/matlab/my_script/EvalSegResults). 
 DenseBox: A Fully Convolutional Neural Net Framework for Object Detection 
 Introduction 
 DenseBox is an unified framework for object detection with a single network. You can use the code to train/evaluate a network for object detection task. For more details, please refer to our  arXiv paper . 
 
 
 
 Directory Explanation 
 
 caffe_densebox: The caffe code for running densebox detection. This folder is similar to caffe, with only core cpp detection code added. 
 experiment: All the matlab/python/linux shell scripts for running experiments. If you want a start, please refer to experiment/kitti. 
 paper: latex files for writing the densebox paper. 
"
jcjohnson," A simple simulation of a swinging pendulum in Javascript, with a couple different integrators that I did for fun one weekend. 
 See a demo here: 
 http://cs.stanford.edu/people/jcjohns/pendulum/index.html ProjectEuler 
 My solutions to some of the Project Euler problems. 
 Solutions are mostly in C or C++, with a few in Java or Python. pycnn 
 Convolutional Neural Networks in Python. 
 For my own education, I wanted to implement a CNN framework from scratch in Python. Right now this is half-baked and not really useable for anything, but this served as a starting point for the CNN that we use in the assignments for  CS 231n . 
 When NVIDIA released  cuDNN  I was also curious to see if I could expose the CUDNN functionality to Python, with the goal of eventually implementing a GPU / CPU hybrid CNN implementation in Python. As a proof of concept, I implemented a Tensor4D object in C++ that manages CPU and GPU pointers to data, and several Layer objects that can operate on Tensor4D objects by calling into cuDNN functions. The CPU data of the Tensor4D objects are wrapped in numpy arrays, and exposed as Python classes using Boost-python. The implementation of the Tensor4D object can be found in the file  src/pycudnn.cpp  and example usage in Python can be found in the file  examples/pycudnn_example.py .  
 One day I want to revisit this and make it all actually work, but for now it will have to live a cursed, half-baked life. simple-amt 
 simple-amt is a microframework for working with  Amazon's Mechanical Turk  (AMT). It was designed with the following three principles in mind: 
 
 Abstract away the details of AMT to let you focus on your own task. 
 Place no restrictions on the structure of your AMT tasks. 
 Lightweight and easy to understand. 
 
 Quick start guide 
 Follow these steps to set up simple-amt and run a simple HIT on AMT. 
 Check out the codebase and set up a virtualenv 
 git clone https://github.com/jcjohnson/simple-amt.git
cd simple-amt
virtualenv .env
source .env/bin/activate
pip install -r requirements.txt 
 Configure your Amazon account 
 To use AMT, you'll need an Amazon AWS account. To interact with Amazon, simple-amt needs
an access key and corresponding secret key for your Amazon account. You can find these 
 here . Once you have these,
place then in a file called config.json for simple-amt:
```
cp config.json.example config.json 
 edit config.json; fill out the ""aws_access_key"" and ""aws_secret_key"" fields. 
 ```
 WARNING : Your AWS keys provide full access to your AWS account, so be careful about where you store your config.json file! 
 Launch some HITs 
 We've included a sample HIT that asks workers to write sentences to describe images. To launch a couple of these HITs on the AMT sandbox, run the following:
 python launch_hits.py \
  --html_template=hit_templates/image_sentence.html \
  --hit_properties_file=hit_properties/image_sentence.json \
  --input_json_file=examples/image_sentence/example_input.txt \
  --hit_ids_file=examples/image_sentence/hit_ids.txt 
This is the most complicated command that you will need to run; let's break it down:
- The file  image_sentence.html  is a simple  jinja2  template that defines the UI of the HIT;
you can find it in  hit_templates/image_sentence.html .
- The file  hit_properties/image_sentence.json  defines the properties of the HIT: title, keywords, price, etc.
- The file  examples/image_sentence/example_input.txt  contains inputs for the HITs that you want to create. The input to each HIT is a JSON object, and the input file contains one such JSON object per line. One HIT is created for each line of the input file.
- The IDs of the created HITs are written to the file  examples/image_sentence/hit_ids.txt . You will use this file as input to other commands in order to operate on the batch of HITs that you just created. 
 Note : You may be seeing an error message scrolling repeatedly if you're setting up AMT for the first time, asking you to ""Please log in to  https://requestersandbox.mturk.com/  and complete registration."" You have to register on that URL first and then run again. 
 Do your HITs 
 Your HITs should now be live on the  Mechanical Turk sandbox .
Open the sandbox and sort by ""HIT creation data (newest first)"".
You should see a HIT with the title "" Write sentences to describe images "" in the first page or two of results.
Complete one of the HITs. 
 The HIT can sometimes take several seconds to appear. You can double check that the HIT is up and available by going to the  requester sandbox  page and clicking  manage -> manage hits individually . 
 Also note that you may not satisfy the qualifications of your own HIT. In this case you can edit the file  hit_properties/image_sentence.json  and erase the lines corresponding to  HitsApproved  and  PercentApproved . Remember to bring down the HITs you've launched (see below), and then re-launch the HIT (see above). 
 Check HIT progress 
 You can check the status of your in-progress HITs by running the following command:
 python show_hit_progress.py --hit_ids_file=examples/image_sentence/hit_ids.txt 
 Get HIT results 
 You can fetch the results of your completed HITs by running the following command:
```
python get_results.py \
  --hit_ids_file=examples/image_sentence/hit_ids.txt \ 
 
 examples/image_sentence/results.txt
 ``
The results of all completed HITs are now stored as in the file examples/image_sentence/results.txt`.
Each line of the file contains a JSON blob with the results from a single assignment. 
 
 If you collect your results before all your hits have been completed and need to call get results again, you can optimize the function by passing in the results you have already collected using the following command:
```
python get_results.py \
  --hit_ids_file=examples/image_sentence/hit_ids.txt \
  --output_file=examples/image_sentence/results.txt \ 
 
 examples/image_sentence/results.txt
``` 
 
 Approve work 
 If you are satisfied with the results that you have gotten, you can approve all completed assignments from your HIT batch by running the following command:
 python approve_hits.py --hit_ids_file=examples/image_sentence/hit_ids.txt 
 Or if you want to approve individual assignments, you can save all the assignments id in a file  assignment_ids.txt  and then call the following command:
 python approve_assignments.py --assignment_ids_file=examples/image_sentence/assignment_ids.txt 
 Delete HITs 
 Once your HITs are completed and you have saved the results, you can delete the HITs from Amazon's database with the following command:
 python delete_hits.py --hit_ids_file=examples/image_sentence/hit_ids.txt 
 WARNING:  After running this command, your HITs will no longer be visible to workers, and you will no longer be able to retrieve HIT results from Amazon. Make sure that you have saved the HIT results before running this command. 
 Get All HITs 
 In the event that you want to get the results for all the hits that you have launched on mtc, regardless of what their hit ids are, you can call the following function. It will save a json array where every element is a hit result.
```
python get_all_hits.py \ 
 
 examples/image_sentence/all_results.txt
``` 
 
 Rejecting Work 
 If you are unhappy with the work done and want to reject the work, you can call the following command (please note that rejecting work harms worker's rating on the site and can influence their ability to find other work):
 python reject_hits.py --hit_ids_file=examples/image_sentence/hit_ids.txt 
 Or you can also reject individual assignments:
 python reject_assignments.py --assignment_ids_file=examples/image_sentence/assignment_ids.txt 
 You can also delete individual hit ids from the command line:
 python delete_hit.py --hit_id THE_HIT_ID_YOU_WANT_TO_DISABLE 
 Blocking Workers 
 In extreme cases, when you want to prevent a malicious worker from affecting your work, you can use the following commands to block or unblock them using their worker ids. Save the worker ids that you want to block in a file (e.g.  worker_ids.txt ) and call the following to block workers:
 python block_workers.py --worker_ids_file=examples/image_sentence/worker_ids.txt 
 or to unblock workers:
 python unblock_workers.py --worker_ids_file=examples/image_sentence/worker_ids.txt 
 Running on the production site 
 To run your HITs on the production AMT site, simply append a  --prod  flag to each of the above commands. 
 WARNING:  Running HITs on sandbox is free, but running HITs on the production site is not. In order to launch HITs your Mechanical Turk account must have sufficient funds to pay for all HITs; these funds will be held in escrow by Amazon once you
launch HITs, and will be paid to workers when you approve assignments.  
 Creating your own HITs 
 To create your own HITs, you'll need to do the following: 
 
 Create HTML template for HIT UI 
 Create HIT properties file 
 Prepare input file 
 
 We'll walk through each of these steps in more detail. 
 Build HIT UI 
 Building the UI is typically the most time-consuming step in creating a new type of HIT. You will have to do most of the work yourself, but simple-amt can still help. As a running example, we will use the UI defined in  hit_templates/simple.html . This is a very basic HIT that asks workers to write an example of a category, like a type of dog or a flavor of ice cream. 
 If you look at  hit_templates/simple.html , you'll notice that it looks like regular HTML except for the line
 {% include ""simpleamt.html"" %} 
This includes the file  hit_templates/simpleamt.html , which does two things: 
 
 Sets up DOM elements where HIT input and output will be stored; the only one of these that you need to know is the submit button, which has the ID  #submit-btn . 
 Sets up a global Javascript object called  simpleamt  that defines functions for working with Mechanical Turk on the frontend. 
 
 The Javascript  simpleamt  object provides the following functions: 
 
 simpleamt.getInput(default_input) : Attempts to get and parse the input JSON blob to this HIT. If this succeeds, the input JSON blob is returned as a Javascript object. If the input blob cannot be read (either during development when there is no input blob or if it cannot be parsed as valid JSON) then  default_input  is returned instead. If  default_input  is not passed to  getInput  then it defaults to  null . 
 simpleamt.setOutput(output) : Store the output JSON blob for this HIT.  output  should be a Javascript object that can be serialized to JSON. 
 simpleamt.isPreview() : Check to see if this HIT is in preview mode. Amazon uses a url parameter called  assignmentId  to indicate whether a HIT is being previewed. If the parameter does not exist (such as during development) then  simpleamt.isPreview  returns  false . 
 simpleamt.setupSubmit() : This performs a bit of bookkeeping to make it possible to submit results to Amazon. You  must  call this before the submit button is clicked; if you don't then Amazon will report an error when the user tries to submit the HIT. 
 
 To see a minimal example of these functions in action, look at the file  hit_templates/simple.html . 
 While developing a HIT template, you will need to render the template to produce a valid HTML page that you can view in a browser. You can do this using the  render_template.py  script. Use it like this: 
 python render_template.py --html_template=hit_templates/simple.html --rendered_html=rendered_templates/simple.html 
 The rendered template will be stored in a directory called  rendered_templates  (you can change this by passing in the complete destination path of where you want the html rendered file to be saved.). Whenever you change your HIT template you will need to rerender to see your changes. 
 To actually view the rendered template in a web browser, you will need to run a local HTTP server so that protocol-relative URLs resolve properly. Python makes this very easy; just run 
 python -m http.server 8080 
 then point your web browser at http://localhost:8080/. 
 Create HIT properties file 
 To launch HITs, you need both an HTML template defining the UI for the HIT and a JSON file storing properties of the HIT. An example JSON file is  hit_properties/simple.json . A HIT properties JSON file has the following fields (some are required and some are optional): 
 
 Title : Required. Must be a string. The title of your HIT. This will be the first part of your HIT that workers see, so it should be short and descriptive. 
 Description : Required. Must be a string. If a worker is intrigued by your HIT title, they can click on it to see the description. This should be a couple of sentences at most, giving a brief description of the HIT. 
 Keywords : Required. Must be a string of words separated by commas. These keywords are used by Mechanical Turk's search function. From my experience the Mechanical Turk search function isn't very smart, so it can help to explicitly conjugate verbs, include both singular and plural versions of nouns, and be creative to think of words that could be relevant. Picking good keywords for your HIT is a basically a small SEO problem. 
 Reward : Required. Must be a number. This is the amount of money (in US dollars) that will be paid to workers per assignment. Keep in mind that Amazon charges the greater of 20% or 0.1 cents as a commission fee, so your actual cost per HIT will be slightly higher than the reward. You should also always pay at least 5 cents per HIT to avoid paying unnecessary commission fees to Amazon. 
 AssignmentDurationInSeconds : Required. Must be an integer. This is the number of time (in seconds) that each worker has to complete the HIT before it expires. You should probably make this about 2 to 3 times the actual amount of time that you expect workers to spend on each assignment. 
 FrameHeight : Required. Must be an integer. When you HIT is displayed, Amazon renders the HIT content inside of an iframe. The height of the iframe is  FrameHeight  pixels. You should pick a number that is larger than your actual HIT content; if you don't then your HIT will be ugly and have nested scroll bars. 
 MaxAssignments : Required. Must be an integer. The number of assignments to create for each input. This means that  MaxAssignments  different workers will give you results for each HIT input. 
 Country : Optional. Must be a string. If you set this, then only workers from the specified country will be allowed to work on your HITs. This must be either a valid  ISO 3166 country code  or a valid  ISO 3166-2 subdivision code . I usually just use ""US"". 
 HitsApproved : Optional. Must be an integer. If you set this, then only workers who have had at least this many HITs approved on Mechanical Turk will be allowed to work on your assignments. 
 PercentApproved : Optional. Must be an integer. If you set this, then only workers who have had at least this percent of their submitted HITs approved will be allowed to work on your HITs. 
 QualificationId : Optional. If you have assigned qualifications to some workers, then this is only allow those workers with this qualification id to work on your hits. 
 QualificationComparator : Optional. Must be one of  < ,  = , or  > . This helps decide whether you want the workers to have a qualification that is greater, equal or less than the  QualificationInteger . 
 QualificationInteger : Optional. Must be an integer. The value used to threshold workers to be above, equal or below (determined by  QualificationComparator ). 
 open-sentences-hit 
 A UI for simple-amt. 
 Javascript main function in  js/open-sentences-backbone.js . 
 TODOs:
- Add a script to deploy static content (JS, CSS, images) and update the URLs in the HTML page
- simple-amt integration:
  - The  main  function in  js/open-sentences-backbone.js  is close, but not completely done
  - Get output graph by calling  app.toJSON()  in  main cnn-vis 
 Inspired by Google's recent  Inceptionism  blog post, cnn-vis is an open-source tool that lets you use convolutional neural networks to generate images. Here's an example: 
 
 You can find many more examples, along with scripts used to generate them, in the  example gallery . 
 Convolutional neural networks (CNNs) have become very popular in recent years for many tasks in computer vision, but most especially for image classification. A CNN takes an image (in the form of a pixel grid) as input, and transforms the image through several  layers  of nonlinear functions. In a classification setup, the final layer encodes the contents of the image in the form of a probability distribution over a set of classes. The lower layers tend to capture low-level image features such as oriented edges or corners, while the higher layers are thought to encode more semantically meaningful features such as object parts. 
 In order to use a CNN for a classification task, it needs to be  trained . We initialize the weights of the network randomly, then show it many examples of images whose labels are known. Based on the errors that the network makes in classifying these known images, we gradually adjust the weights of the network so that it correctly classifies these images. Two popular datasets for training CNNs are ImageNet [4] and MIT Places [10]. ImageNet contains 1000 categories of objects, such as dogs, birds, and other animals, while MIT Places contains 205 types of scenes such as bedrooms, kitchens, and forests. 
 Although CNNs perform well on a variety of tasks, it can be difficult to understand exactly what types of image features a CNN is using to work its magic. One trick for demystifying a CNN is to choose a neuron in a trained CNN, and attempt to generate an image that causes the neuron to activate strongly. We initialize the image with random noise, propagate the image forward through the network to compute the activation of the target neuron, then propagate the activation of the neuron backward through the network to compute an update direction for the image. We use this information to update the image, and repeat the process until convergence. This general strategy has been used to visualize the activations of individual neurons [8, 9], to generate images of particular object classes [5], to invert CNN features [1, 2], and to generate images to fool CNNs [3, 6]. 
 Inceptionism builds on this line of work, adding three unique twists: 
 
 Layer amplification : Instead of choosing a neuron and generating an image to maximize it, we instead choose a layer of the network and attempt to amplify the neurons in that layer that are already activated by the image. This leads to a feedback loop, causing the network to emphasize features of the image that are already present. Google's blog post doesn't specify many technical details around this point; in cnn-vis we achieve this by maximizing the sum of the absolute value and the square of each neuron in the chosen layer, but in principle any superlinear function of the chosen layer should work. 
 Multiscale, high-res images : Because they are so computationally expensive, CNNs tend to work on relatively low-resolution images. The state-of-the-art GoogLeNet network [7], for example, works on input images that are 224x224 pixels. To generate higher-resolution images, Inceptionism ""appl[ies] the algorithm iteratively on its own outputs and appl[ies] some zooming after each iteration"". In addition to giving high resolution images, this trick also causes the generated images to have structure at multiple scales, giving them a fractal-like appearance. In cnn-vis we implement this idea by tiling the image with overlapping 224x224 patches, and interleaving updates on each patch. After some number of iterations we upsample the image, retile it, and continue. 
 Non-random initialization : Instead of initializing our generated image with random noise or zero as is common in the literature, Inceptionism (and cnn-vis!) allow you to start from a user-specified image. I'm not sure how much scientific value this has, but it sure does look cool! 
 
 Setup 
 Caffe 
 cnn-vis is built on top of  Caffe , an excellent open-source CNN implementation from Berkeley. You'll need to do the following:
* Install Caffe; follow the official  installation instructions .
* Build the Python bindings for Caffe
* If you have an NVIDIA GPU, you can optionally install  cuDNN  to make Caffe even faster
* Set the environment variable  $CAFFE_ROOT  to point to the root directory of your Caffe installation
* Download the official Caffe pretrained GoogLeNet model; from  $CAFFE_ROOT  run the command
 ./scripts/download_model_binary.py models/bvlc_googlenet/ 
* Download a version of GoogLeNet pretrained on the MIT Places dataset  here ; place it in  $CAFFE_ROOT/models/googlenet_places . 
 cnn-vis 
 Clone the repo, create a virtual environment, install requirements, and add the Caffe Python library to the virtualenv:
 git clone https://github.com/jcjohnson/cnn-vis.git
cd cnn-vis
virtualenv .env
source .env/bin/activate
pip install -r requirements.txt
echo $CAFFE_ROOT/python > .env/lib/python2.7/site-packages/caffe.pth 
 Usage 
 cnn-vis is a standalone Python script; you can control its behavior by passing various command-line arguments. 
 There are quite a few knobs that can be tweaked that affect the final generated image. To help get you started, we've provided scripts that use cnn-vis to generate a bunch of example images in the  example gallery . For completeness we also document all options here. 
 CNN options 
 These options control the CNN that will be used to generate images.
*  --deploy_txt : Path to the Caffe .prototxt file that defines the CNN model to be used. cnn-vis expects that this model's input comes from a blob named  data . Default is the BVLC reference GoogLeNet.
*  --caffe_model : Path to the  .caffemodel  file giving the weights of the CNN model to be used. Default is the BVLC reference GoogLeNet.
*  --batch_size : The number of image patches to be computed in parallel. Larger values will take more GPU memory, but may be more efficient for larger images. Default is 1.
*  --mean_image : By convention, most Caffe pretrained models do not work on raw images, but instead work on the residual after subtracting some average image. This parameter gives the path to a  .npy  file giving the mean image; the default is the ImageNet mean image provided by Caffe.
*  --gpu : Which GPU to use for optimization. Setting this to a negative value will run the model in CPU mode. 
 Image options 
 These options define the objective that will be optimized to generate an image
*  --image_type : The type of image to generate. If this is  amplify_neuron  then we will attempt to maximize a single neuron in the network, similar to [5]. If this is  amplify_layer  then this will produce images in the style of Inceptionism.
*  --target_layer : The name of the layer to target in the network. Earlier layers tend to encode lower level features like edges or blobs, while later layers tend to encode higher-level features like object parts. For convenience, a complete list of layers in order for GoogLeNet is given in the file  googlenet_layers.txt .
*  --target_neuron : If  image_type  is  amplify_neuron , then  target_neuron  gives the index of the neuron to amplify. 
 Initialization options 
 Options for setting the initial image. You can either seed the initial image from an existing image, or use random noise. In the case of random noise, we generate Gaussian white noise, then smooth it using Gaussian blur to prevent TV regularization from dominating the first few steps of optimization.
*  --initial_image:  Path to an image file to use to start optimization. If this flag is not set, then the image will be initialized from smoothed Gaussian white noise instead.
*  --initialization_scale : If  initial_image  is not set, then this gives the standard deviation of the Gaussian white noise used to initialize the image. Default is 1.
*  --initialization_blur : If  initial_image  is not set, this gives the standard deviation for the Gaussian kernel used to smooth the white noise image. Default is 0, corresponding to no smoothing. 
 Resize options 
 Options for configuring multiscale zooming used to generate high-resolution images. To generate nice images, we want to start with a small initial size that is ideally not much bigger than the base resolution of the CNN, then gradually grow to larger images.  
 Sizes may be specified as multiples of a  base size ; for noise initializations the base size is the input size of the CNN, and for image initializations the base size is the original size of the initial image.
*  --initial_size : The initial size. Can be one of the following:
  * If not set, then the initial size is the base size.
  *  xF  where  F  is a floating point number, such as  x0.5 . The initial size will be a multiple of the base size.
  *  HxW  where  H  and  W  are integers, such as  600x800 . The initial image will have height and width  H  and  W  pixels respectively.
*  --final_size : The final size, in the same format as  --initial_size .
*  --num_sizes : The number of sizes to use. Default is 1.
*  --resize_type : How to space the intermediate sizes between the initial and final sizes. Choices are  geometric  or  linear ; default is  geometric . 
 Optimization options 
 We optimize using gradient descent, and use RMSProp to compute per-parameter adaptive learning rates.
*  --learning_rate : The learning rate to use. Default is 1.0.
*  --learning_rate_decay_iter : Every  learning_rate_decay_iter  iterations, reduce the learning rate. Default is 100.
*  --learning_rate_decay_factor : Every  learning_rate_decay_iter  iterations, reduce the learning rate by multiplying it by  learning_rate_decay_factor . Default is 1.0, corresponding to no learning rate decay.
*  --decay_rate : Decay rate for RMSProp. Default is 0.95. Usually when RMSProp is used for stochastic gradient descent, it is common to use values greater than 0.9 for the decay rate; however in this application our gradients are not stochastic, so lower decay rate values sometimes work well.
*  --num_steps : The number of optimization steps to take at each size.
*  --use_pixel_learning_rates : Because the image is tiled with overlapping windows of input size to the CNN, each pixel will be contained in either 1, 2, or 4 windows; this can cause ugly artifacts near the borders of window regions, especially for high learning rates. If this flag is passed, divide the learning rate for each pixel by the number of windows that the pixel is contained in; this can sometimes help alleviate this problem. 
 Layer amplification objective options 
 These options allow you to configure the objective that is used for layer amplification. During backpropagation, we set the gradient of the target layer to  -l1_weight * abs(a) - l2_weight * clip(a, -g, g) , where  a  are the activations of the target layer. This corresponds to maximizing the (weighted) sum of the absolute values and thresholded squares of the activations at the target layer. The generated image tends not to be very sensitive to the values of these parameters, so the defaults should work fine.
*  --l1_weight : The value of  l1_weight  in the equation above; default is 1.0.
*  --l2_weight : The value of  l2_weight  in the equation above; default is 1.0.
*  --grad_clip : The value of  g  in the equation above. Default is 5.0. 
 P-norm regularization options 
 P-norm regularization prevents individual pixels from getting too large. For noise initializations, p-norm regularization pulls each pixel toward zero (corresponding to the mean ImageNet color) and for image initializations, p-norm regularization will pull each pixel toward the value of that pixel in the initial image. For noise initializations, relatively weak p-norm regularization tends to work well; for image initializations, p-norm regularization is the only term enforcing visual consistency with the initial image, so p-norm regularization should be stronger.
*  --alpha : The exponent of the p-norm. Note that [5] uses L2 regularization, corresponding to  alpha=2.0  while [2] suggests using  alpha=6.0 . Default is 6.0.
*  --p_reg : Regularization constant for p-norm regularization. Larger values will cause the p-norm constraint to be enforced more strongly. Default is 1e-4.
*  --p_scale : Scaling constant; divide pixels by this value before computing the p-norm regularizer. Note that a non-unit value for  p_scale  can be absorbed into  p_reg , so this is technically redudent; however it can be useful for both numeric stability and to make it easier to compare values of  p_reg  across different values of  alpha . 
 Auxiliary p-norm regularization options 
 Parameters for a second p-norm regularizer; however the second p-norm regularizer always pulls towards zero, while the first p-norm regularizer pulls toward the initial image if it is given. If the initial image contains very saturated regions (either very white or very black) then even small deviations around the initial value can result in pixel values outside the [0, 255] range. A trick for getting around this problem is adding a second p-norm regularizer with a high exponent (maybe 11) and very low regularization constant (maybe 1e-11). This regularizer will have little effect on pixels near the center of the [0, 255] range, but will push pixels outside this range back toward zero.
*  --alpha_aux : Exponent for auxiliary p-norm regularization. Default is 6.
*  --p_reg_aux : Regularization strength for auxiliary p-norm regularization. Default is 0 (no auxiliary p-norm regularizer).
*  --p_scale_aux : Scaling constant for auxiliary p-norm regularizer, analogous to  p_scale . 
 Total Variation regularization options 
 Total Variation (TV) regularization encourages neighboring pixels to have similar values. For noise initializations this regularizer is critical; without it the generated image will exhibit large amounts of high-frequency noise. For image initializations it is less critical; strong p-regularization will keep the pixels close to the initial image, and this will be sufficient to prevent high-frequency noise. 
 As defined in [2], we compute the TV-norm of an image by approximating the magnitude of the image gradient using neighboring pixels, raising the image gradient to the power of beta, and summing over the image. 
 [2] suggests that starting with a low TV-norm regularization strength and increasing it over time gives good results. In cnn-vis we implement this idea by increasing the TV-norm regularization strength by a constant amount after a fixed number of iterations.
*  --beta : Exponent for TV-regularization. As discussed in [2], values less than 1 will give rise to patches of solid color; setting beta to 2 or 2.5 tends to give good results. Default is 2.0.
*  --tv_reg : Regularization strength for TV-regularization. Higher values will more strongly encourage the image to have a small TV-norm.
*  --tv_reg_scale : Similar to  p_scale , a scaling factor that the image is divided by prior to computing the TV-norm. As with  p_scale  this is technically redudent and can be absorbed into  tv_reg .
*  --tv_reg_step_iter : TV-norm regularization strength will be increased every  tv_reg_step_iter  steps. Default is 50.
*  --tv_reg_step : Every  tv_reg_step_iter  steps, TV-norm regularization strength will increase by this amount. Default is 0, corresponding to a fixed TV-norm regularization strength.
*  --tv_grad_operator : The operator used to compute image gradients for the TV-norm. Choices are  naive  (which uses neighboring pixels as in [2]),  sobel  (which uses the classic Sobel filter) or  sobel_squish  (which uses the kernels [[-1, 1, 0], [-2, 2, 0], [-1, 1, 0]] and [[-1, -2, -1], [1, 2, 1], [0, 0, 0]]). Using the Sobel operators tends to lead to high-frequency noise due to oscillatory behavior of neighboring pixels, so naive is probably the best choice for most applications.  
 Output options 
 Options for controlling the output.
 --output_file : Filename where the final image will be saved. Default is  out.png .
 --rescale_image : If this flag is given, then the image colors are rescaled to [0, 255] linearly; the minimum value of the image will be mapped to 0, and the maximum image value will map to 255. If this flag is not given, the image wil be clipped to the range [0, 255] for output. Rescaling the image values can reveal detail in highly saturated or desaturated image regions, but can lead to color distortion.
 --output_iter : After every  output_iter  steps of optimization, some outputs will be produced. Exactly what is produced is controlled by  iter_behavior .
 --iter_behavior : What should happen every  output_iter  steps. The allowed options are shown below. Options can be combined with  +  to have multiple types of behavior; for example  show+print+save  will do all three every  output_iter  steps.
  *  show : Show the current image using matplotlib
  *  save : Save the current image; the filename will append the size number and iteration number to  output_file .
  *  print : Print the current iteration number, along with some statistics about the image and the gradients from the different regularizers.
  *  plot_pix : Plot the values of a few image pixels over time using matplotlib; this can give you a rough sense for how the optimization process is proceeding. For example very oscillatory behavior indicates that something bad is happening, and the learning rate should be decreased.
*  --show_width ,  --show_height : Control the size of the shown image for the  show  behavior. Show the image with height and width of  show_height  and  show_width  inches, respectively. 
 References 
 [1] A. Dosovitskiy and T. Brox. ""Inverting Convolutional Networks with Convolutional Networks"", arXiv preprint arXiv:1506.02753 (2015). 
 [2] A. Mahendran and A. Vedaldi, ""Understanding Deep Image Representations by Inverting Them"", CVPR 2015 
 [3] A. Nguyen, J. Yosinski, J. Clune. ""Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images"", CVPR 2015 
 [4] O. Russakovsky, et al. ""Imagenet large scale visual recognition challenge"", IJCV 2014. 
 [5] K. Simonyan and A. Vedaldi and A. Zisserman, ""Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps"", ICLR 2014 
 [6] C. Szegedy, et al. ""Intriguing properties of neural networks."" arXiv preprint arXiv:1312.6199 (2013). 
 [7] C. Szegedy, et al. ""Going Deeper with Convolutions"", CVPR 2015. 
 [8] J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, H. Lipson H (2015) ""Understanding Neural Networks Through Deep Visualization"", ICML 2015 Deep Learning workshop. 
 [9] M. D. Zeiler and R. Fergus. ""Visualizing and understanding convolutional networks"", ECCV 2014. 
 [10] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva. ""Learning Deep Features for Scene Recognition using Places Database"", NIPS 2014. sorting.js 
 A visualization for a bunch of different sorting algorithms in Javascript. 
 See a demo here:
http://cs.stanford.edu/people/jcjohns/sorting.js/ 
 More specifically, we visualize in-place comparison-based sorting algorithms by highlighting each comparison in blue and each swap in red. Currently the following sorting algorithms are implemented: 
 
 Bubble sort 
 Selection sort 
 Insertion sort 
 Odd-even sort 
 Cocktail sort 
 Quicksort 
 Heapsort 
 Merge sort 
 Bitonic mergesort 
 Introsort 
 
 Quicksort and introsort recursively partition chunks of the array around a pivot value; the choice of pivot can have a big effect on the efficiency of the algorithm. We implement several pivoting choices discussed  here . 
 The current mergesort implementation isn't  really  in-place; during the merge step, we scan the sorted subarrays to build up a permutation that will merge them, then convert the permutation to a sequence of swaps. These intermediate data structures use a linear amount of extra memory. 
 For the bitonic mergesort I followed  this implementation . Because I am lazy, the bitonic mergesort implementation only supports arrays whose lengths are a power of two, and array lengths will be rounded up to the nearest power of two if bitonic mergesort is selected. texture-synthesis 
 This is a Torch implementation of a texture-synthesis algorithm very similar to [1]. 
 Given an input texture patch, the algorithm generates a larger version of the same texture. Here's an example: 
 Input 
 
 Output 
 
 You can see more examples  in the example gallery . 
 Usage 
 Texture synthesis is implemented in the script  synthesis.lua . The following command line options are available:
*  -source : Path to the source image.
*  -output_file : Path where the output should be written.
*  -height : Height of the output file, in pixels.
*  -width : Width of the output file, in pixels.
*  -k : Kernel size; must be an odd integer.
*  -gpu : Which GPU to use. Setting  gpu >= 0  will run in GPU mode, and setting  gpu < 0  will run in CPU-only mode. 
 You can see examples using different source images and kernel sizes  in the example gallery . 
 Works Cited: 
 [1] Efros, Alexei, and Thomas K. Leung. ""Texture synthesis by non-parametric sampling."" ICCV 1999. neural-style 
 This is a torch implementation of the paper  A Neural Algorithm of Artistic Style 
by Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. 
 The paper presents an algorithm for combining the content of one image with the style of another image using
convolutional neural networks. Here's an example that maps the artistic style of
 The Starry Night 
onto a night-time photograph of the Stanford campus: 
 
 
 
 
 
 Applying the style of different images to the same content image gives interesting results.
Here we reproduce Figure 2 from the paper, which renders a photograph of the Tubingen in Germany in a
variety of styles: 
 
 
 
 
 
 
 
 
 Here are the results of applying the style of various pieces of artwork to this photograph of the
golden gate bridge: 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Content / Style Tradeoff 
 The algorithm allows the user to trade-off the relative weight of the style and content reconstruction terms,
as shown in this example where we port the style of  Picasso's 1907 self-portrait  onto Brad Pitt: 
 
 
 
 
 
 
 
 
 
 
 Style Scale 
 By resizing the style image before extracting style features, we can control the types of artistic
features that are transfered from the style image; you can control this behavior with the  -style_scale  flag.
Below we see three examples of rendering the Golden Gate Bridge in the style of The Starry Night.
From left to right,  -style_scale  is 2.0, 1.0, and 0.5. 
 
 
 
 
 
 Multiple Style Images 
 You can use more than one style image to blend multiple artistic styles. 
 Clockwise from upper left: ""The Starry Night"" + ""The Scream"", ""The Scream"" + ""Composition VII"",
""Seated Nude"" + ""Composition VII"", and ""Seated Nude"" + ""The Starry Night"" 
 
 
 
 
 
 
 Style Interpolation 
 When using multiple style images, you can control the degree to which they are blended: 
 
 
 
 
 
 Transfer style but not color 
 If you add the flag  -original_colors 1  then the output image will retain the colors of the original image;
this is similar to  the recent blog post by deepart.io . 
 
 
 
 
 
 
 
 
 Setup: 
 Dependencies:
*  torch7 
*  loadcaffe 
 Optional dependencies:
* For CUDA backend:
  * CUDA 6.5+
  *  cunn 
* For cuDNN backend:
  *  cudnn.torch 
* For OpenCL backend:
  *  cltorch 
  *  clnn 
 After installing dependencies, you'll need to run the following script to download the VGG model:
 sh models/download_models.sh 
This will download the original  VGG-19 model .
Leon Gatys has graciously provided the modified version of the VGG-19 model that was used in their paper;
this will also be downloaded. By default the original VGG-19 model is used. 
 If you have a smaller memory GPU then using NIN Imagenet model will be better and gives slightly worse yet comparable results. You can get the details on the model from  BVLC Caffe ModelZoo  and can download the files from  NIN-Imagenet Download Link 
 You can find detailed installation instructions for Ubuntu in the  installation guide . 
 Usage 
 Basic usage:
 th neural_style.lua -style_image <image.jpg> -content_image <image.jpg> 
 OpenCL usage with NIN Model (This requires you download the NIN Imagenet model files as described above):
 th neural_style.lua -style_image examples/inputs/picasso_selfport1907.jpg -content_image examples/inputs/brad_pitt.jpg -output_image profile.png -model_file models/nin_imagenet_conv.caffemodel -proto_file models/train_val.prototxt -gpu 0 -backend clnn -num_iterations 1000 -seed 123 -content_layers relu0,relu3,relu7,relu12 -style_layers relu0,relu3,relu7,relu12 -content_weight 10 -style_weight 1000 -image_size 512 -optimizer adam 
 
 To use multiple style images, pass a comma-separated list like this: 
 -style_image starry_night.jpg,the_scream.jpg . 
 Note that paths to images should not contain the  ~  character to represent your home directory; you should instead use a relative
path or a full absolute path. 
 Options :
*  -image_size : Maximum side length (in pixels) of of the generated image. Default is 512.
*  -style_blend_weights : The weight for blending the style of multiple style images, as a
  comma-separated list, such as  -style_blend_weights 3,7 . By default all style images
  are equally weighted.
*  -gpu : Zero-indexed ID of the GPU to use; for CPU mode set  -gpu  to -1. 
 Optimization options :
*  -content_weight : How much to weight the content reconstruction term. Default is 5e0.
*  -style_weight : How much to weight the style reconstruction term. Default is 1e2.
*  -tv_weight : Weight of total-variation (TV) regularization; this helps to smooth the image.
  Default is 1e-3. Set to 0 to disable TV regularization.
*  -num_iterations : Default is 1000.
*  -init : Method for generating the generated image; one of  random  or  image .
  Default is  random  which uses a noise initialization as in the paper;  image 
  initializes with the content image.
*  -optimizer : The optimization algorithm to use; either  lbfgs  or  adam ; default is  lbfgs .
  L-BFGS tends to give better results, but uses more memory. Switching to ADAM will reduce memory usage;
  when using ADAM you will probably need to play with other parameters to get good results, especially
  the style weight, content weight, and learning rate; you may also want to normalize gradients when
  using ADAM.
*  -learning_rate : Learning rate to use with the ADAM optimizer. Default is 1e1.
*  -normalize_gradients : If this flag is present, style and content gradients from each layer will be
  L1 normalized. Idea from  andersbll/neural_artistic_style . 
 Output options :
*  -output_image : Name of the output image. Default is  out.png .
*  -print_iter : Print progress every  print_iter  iterations. Set to 0 to disable printing.
*  -save_iter : Save the image every  save_iter  iterations. Set to 0 to disable saving intermediate results. 
 Layer options :
*  -content_layers : Comma-separated list of layer names to use for content reconstruction.
  Default is  relu4_2 .
*  -style_layers : Comma-separated list of layer names to use for style reconstruction.
  Default is  relu1_1,relu2_1,relu3_1,relu4_1,relu5_1 . 
 Other options :
*  -style_scale : Scale at which to extract features from the style image. Default is 1.0.
*  -original_colors : If you set this to 1, then the output image will keep the colors of the content image.
*  -proto_file : Path to the  deploy.txt  file for the VGG Caffe model.
*  -model_file : Path to the  .caffemodel  file for the VGG Caffe model.
  Default is the original VGG-19 model; you can also try the normalized VGG-19 model used in the paper.
*  -pooling : The type of pooling layers to use; one of  max  or  avg . Default is  max .
  The VGG-19 models uses max pooling layers, but the paper mentions that replacing these layers with average
  pooling layers can improve the results. I haven't been able to get good results using average pooling, but
  the option is here.
*  -backend :  nn ,  cudnn , or  clnn . Default is  nn .  cudnn  requires
   cudnn.torch  and may reduce memory usage.
   clnn  requires  cltorch  and  clnn 
*  -cudnn_autotune : When using the cuDNN backend, pass this flag to use the built-in cuDNN autotuner to select
  the best convolution algorithms for your architecture. This will make the first iteration a bit slower and can
  take a bit more memory, but may significantly speed up the cuDNN backend. 
 Frequently Asked Questions 
 Problem:  Generated image has saturation artifacts: 
 
 Solution:  Update the  image  packge to the latest version:  luarocks install image 
 Problem:  Running without a GPU gives an error message complaining about  cutorch  not found 
 Solution: 
Pass the flag  -gpu -1  when running in CPU-only mode 
 Problem:  The program runs out of memory and dies 
 Solution:  Try reducing the image size:  -image_size 256  (or lower). Note that different image sizes will likely
require non-default values for  -style_weight  and  -content_weight  for optimal results.
If you are running on a GPU, you can also try running with  -backend cudnn  to reduce memory usage. 
 Problem:  Get the following error message: 
 models/VGG_ILSVRC_19_layers_deploy.prototxt.cpu.lua:7: attempt to call method 'ceil' (a nil value) 
 Solution:  Update  nn  package to the latest version:  luarocks install nn 
 Problem:  Get an error message complaining about  paths.extname 
 Solution:  Update  torch.paths  package to the latest version:  luarocks install paths 
 Problem:  NIN Imagenet model is not giving good results.  
 Solution:  Make sure the correct  -proto_file  is selected. Also make sure the correct parameters for  -content_layers  and  -style_layers  are set. (See OpenCL usage example above.) 
 Problem:   -backend cudnn  is slower than default NN backend 
 Solution:  Add the flag  -cudnn_autotune ; this will use the built-in cuDNN autotuner to select the best convolution algorithms. 
 Memory Usage 
 By default,  neural-style  uses the  nn  backend for convolutions and L-BFGS for optimization.
These give good results, but can both use a lot of memory. You can reduce memory usage with the following: 
 
 Use cuDNN : Add the flag  -backend cudnn  to use the cuDNN backend. This will only work in GPU mode. 
 Use ADAM : Add the flag  -optimizer adam  to use ADAM instead of L-BFGS. This should significantly
  reduce memory usage, but may require tuning of other parameters for good results; in particular you should
  play with the learning rate, content weight, style weight, and also consider using gradient normalization.
  This should work in both CPU and GPU modes. 
 Reduce image size : If the above tricks are not enough, you can reduce the size of the generated image;
  pass the flag  -image_size 256  to generate an image at half the default size. 
 
 With the default settings,  neural-style  uses about 3.5GB of GPU memory on my system;
switching to ADAM and cuDNN reduces the GPU memory footprint to about 1GB. 
 Speed 
 Speed can vary a lot depending on the backend and the optimizer.
Here are some times for running 500 iterations with  -image_size=512  on a Maxwell Titan X with different settings:
*  -backend nn -optimizer lbfgs : 62 seconds
*  -backend nn -optimizer adam : 49 seconds
*  -backend cudnn -optimizer lbfgs : 79 seconds
*  -backend cudnn -cudnn_autotune -optimizer lbfgs : 58 seconds
*  -backend cudnn -cudnn_autotune -optimizer adam : 44 seconds
*  -backend clnn -optimizer lbfgs : 169 seconds
*  -backend clnn -optimizer adam : 106 seconds  
 Here are the same benchmarks on a Pascal Titan X with cuDNN 5.0 on CUDA 8.0 RC:
*  -backend nn -optimizer lbfgs : 43 seconds
*  -backend nn -optimizer adam : 36 seconds
*  -backend cudnn -optimizer lbfgs : 45 seconds
*  -backend cudnn -cudnn_autotune -optimizer lbfgs : 30 seconds
*  -backend cudnn -cudnn_autotune -optimizer adam : 22 seconds 
 Multi-GPU scaling 
 You can use multiple GPUs to process images at higher resolutions; different layers of the network will be
computed on different GPUs. You can control which GPUs are used with the  -gpu  flag, and you can control
how to split layers across GPUs using the  -multigpu_strategy  flag. 
 For example in a server with four GPUs, you can give the flag  -gpu 0,1,2,3  to process on GPUs 0, 1, 2, and
3 in that order; by also giving the flag  -multigpu_strategy 3,6,12  you indicate that the first two layers
should be computed on GPU 0, layers 3 to 5 should be computed on GPU 1, layers 6 to 11 should be computed on
GPU 2, and the remaining layers should be computed on GPU 3. You will need to tune the  -multigpu_strategy 
for your setup in order to achieve maximal resolution. 
 We can achieve very high quality results at high resolution by combining multi-GPU processing with multiscale
generation as described in the paper
 Controlling Perceptual Factors in Neural Style Transfer  by Leon A. Gatys, 
Alexander S. Ecker, Matthias Bethge, Aaron Hertzmann and Eli Shechtman. 
 Here is a 3620 x 1905 image generated on a server with four Pascal Titan X GPUs: 
 
 The script used to generate this image  can be found here . 
 Implementation details 
 Images are initialized with white noise and optimized using L-BFGS. 
 We perform style reconstructions using the  conv1_1 ,  conv2_1 ,  conv3_1 ,  conv4_1 , and  conv5_1  layers
and content reconstructions using the  conv4_2  layer. As in the paper, the five style reconstruction losses have
equal weights. 
 Citation 
 If you find this code useful for your research, please cite: 
 @misc{Johnson2015,
  author = {Johnson, Justin},
  title = {neural-style},
  year = {2015},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/jcjohnson/neural-style}},
} bidi-muri 
 Dependencies:
- CUDA (not sure which version)
- cuDNN (not sure which version)
- Lua Torch
- Lua packages:
  - loadcaffe
  - torch-hdf5
  - torch cuDNN bindings
  - Some other Lua packages?
- python packages:
  - numpy
  - scipy
  - h5py 
 Pipeline:
- Preprocess data -- not sure how this worked? Maybe preprocess_v2.py?
- Start from pretrained model from caffe model zoo (need to find / download)
- Train the model with train_v2.lua; finetunes a pretrained model
- Dump preditions from the model to an HDF5 file with extract_scores.lua or test.lua, I'm not sure which
- Convert predictions from HDF5 to matlab format with probs_to_mat.py DenseCap 
 This is the code for the paper 
 DenseCap: Fully Convolutional Localization Networks for Dense Captioning ,
 
 Justin Johnson *,
 Andrej Karpathy *,
 Li Fei-Fei ,
 
(* equal contribution)
 
Presented at  CVPR 2016  (oral) 
 The paper addresses the problem of  dense captioning , where a computer detects objects in images and describes them in natural language. Here are a few example outputs: 
 
 The model is a deep convolutional neural network trained in an end-to-end fashion on the  Visual Genome  dataset. 
 We provide: 
 
 A  pretrained model 
 Code to  run the model on new images , on either CPU or GPU 
 Code to run a  live demo with a webcam 
 Evaluation code  for dense captioning 
 Instructions for  training the model 
 
 If you find this code useful in your research, please cite: 
 @inproceedings{densecap,
  title={DenseCap: Fully Convolutional Localization Networks for Dense Captioning},
  author={Johnson, Justin and Karpathy, Andrej and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and 
             Pattern Recognition},
  year={2016}
} 
 Installation 
 DenseCap is implemented in  Torch , and depends on the following packages:  torch/torch7 ,  torch/nn ,  torch/nngraph ,  torch/image ,  lua-cjson ,  qassemoquab/stnbhwd ,  jcjohnson/torch-rnn 
 After installing torch, you can install / update these dependencies by running the following: 
 bash
luarocks install torch
luarocks install nn
luarocks install image
luarocks install lua-cjson
luarocks install https://raw.githubusercontent.com/qassemoquab/stnbhwd/master/stnbhwd-scm-1.rockspec
luarocks install https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/torch-rnn-scm-1.rockspec 
 (Optional) GPU acceleration 
 If have an NVIDIA GPU and want to accelerate the model with CUDA, you'll also need to install
 torch/cutorch  and  torch/cunn ;
you can install / update these by running: 
 bash
luarocks install cutorch
luarocks install cunn
luarocks install cudnn 
 (Optional) cuDNN 
 If you want to use NVIDIA's cuDNN library, you'll need to register for the CUDA Developer Program (it's free)
and download the library from  NVIDIA's website ; you'll also need to install
the  cuDNN bindings for Torch  by running 
 bash
luarocks install cudnn 
 Pretrained model 
 You can download a pretrained DenseCap model by running the following script: 
 bash
 sh scripts/download_pretrained_model.sh 
 This will download a zipped version of the model (about 1.1 GB) to  data/models/densecap/densecap-pretrained-vgg16.t7.zip , unpack
 it to  data/models/densecap/densecap-pretrained-vgg16.t7  (about 1.2 GB) and then delete the zipped version. 
 This is not the exact model that was used in the paper, but is has comparable performance; using 1000 region proposals per image,
 it achieves a mAP of 5.70 on the test set which is slightly better than the mAP of 5.39 that we report in the paper. 
 Running on new images 
 To run the model on new images, use the script  run_model.lua . To run the pretrained model on the provided  elephant.jpg  image,
use the following command: 
 bash
th run_model.lua -input_image imgs/elephant.jpg 
 By default this will run in GPU mode; to run in CPU only mode, simply add the flag  -gpu -1 . 
 This command will write results into the folder  vis/data . We have provided a web-based visualizer to view these
results; to use it, change to the  vis  directory and start a local HTTP server: 
 bash
cd vis
python -m SimpleHTTPServer 8181 
 Then point your web browser to  http://localhost:8181/view_results.html . 
 If you have an entire directory of images on which you want to run the model, use the  -input_dir  flag instead: 
 bash
th run_model.lua -input_dir /path/to/my/image/folder 
 This run the model on all files in the folder  /path/to/my/image/folder/  whose filename does not start with  . . 
 The web-based visualizer is the prefered way to view results, but if you don't want to use it then you can instead
render an image with the detection boxes and captions ""baked in""; add the flag  -output_dir  to specify a directory
where output images should be written: 
 bash
th run_model.lua -input_dir /path/to/my/image/folder -output_dir /path/to/output/folder/ 
 The  run_model.lua  script has several other flags; you can  find details here . 
 Training 
 To train a new DenseCap model, you will following the following steps: 
 
 Download the raw images and region descriptions from  the Visual Genome website 
 Use the script  preprocess.py  to generate a single HDF5 file containing the entire dataset
    (details here) 
 Use the script  train.lua  to train the model  (details here) 
 Use the script  evaluate_model.lua  to evaluate a trained model on the validation or test data
    (details here) 
 
 For more instructions on training see  INSTALL.md  in  doc  folder. 
 Evaluation 
 In the paper we propose a metric for automatically evaluating dense captioning results.
Our metric depends on  METEOR , and
our evaluation code requires both Java and Python 2.7. The following script will download
and unpack the METEOR jarfile: 
 bash
sh scripts/setup_eval.sh 
 The evaluation code is  not required  to simply run a trained model on images; you can
 find more details about the evaluation code here . 
 Webcam demos 
 If you have a powerful GPU, then the DenseCap model is fast enough to run in real-time. We provide two
demos to allow you to run DenseCap on frames from a webcam. 
 Single-machine demo 
 If you have a single machine with both a webcam and a powerful GPU, then you can
use this demo to run DenseCap in real time at up to 10 frames per second. This demo depends on a few extra
Lua packages: 
 
 clementfarabet/lua---camera 
 torch/qtlua 
 
 You can install / update these dependencies by running the following: 
 bash
luarocks install camera
luarocks install qtlua 
 You can start the demo by running the following: 
 bash
qlua webcam/single_machine_demo.lua 
 Client / server demo 
 If you have a machine with a powerful GPU and another machine with a webcam, then
this demo allows you use the GPU machine as a server and the webcam machine as a client; frames will be
streamed from the client to to the server, the model will run on the server, and predictions will be shipped
back to the client for viewing. This allows you to run DenseCap on a laptop, but with network and filesystem
overhead you will typically only achieve 1 to 2 frames per second. 
 The server is written in Flask; on the server machine run the following to install dependencies: 
 bash
cd webcam
virtualenv .env
source .env/bin/activate
pip install -r requirements.txt
cd .. 
 For technical reasons, the server needs to serve content over SSL; it expects to find SSL key
files and certificate files in  webcam/ssl/server.key  and  webcam/ssl/server.crt  respectively.
You can generate a self-signed SSL certificate by running the following: 
 ```bash
mkdir webcam/ssl 
 Step 1: Generate a private key 
 openssl genrsa -des3 -out webcam/ssl/server.key 1024 
 Enter a password 
 Step 2: Generate a certificate signing request 
 openssl req -new -key webcam/ssl/server.key -out webcam/ssl/server.csr 
 Enter the password from above and leave all other fields blank 
 Step 3: Strip the password from the keyfile 
 cp webcam/ssl/server.key webcam/ssl/server.key.org
openssl rsa -in webcam/ssl/server.key.org -out webcam/ssl/server.key 
 Step 4: Generate self-signed certificate 
 openssl x509 -req -days 365 -in webcam/ssl/server.csr -signkey webcam/ssl/server.key -out webcam/ssl/server.crt 
 Enter the password from above 
 ``` 
 You can now run the following two commands to start the server; both will run forever: 
 bash
th webcam/daemon.lua
python webcam/server.py 
 On the client, point a web browser at the following page: 
 https://cs.stanford.edu/people/jcjohns/densecap/demo/web-client.html?server_url=SERVER_URL 
 but you should replace SERVER_URL with the actual URL of the server. 
 Note : If the server is using a self-signed SSL certificate, you may need to manually
tell your browser that the certificate is safe by pointing your client's web browser directly
at the server URL; you will get a message that the site is unsafe; for example on Chrome
you will see the following: 
 
 Afterward you should see a message telling you that the DenseCap server is running, and
the web client should work after refreshing. tiny-imagenet 
 Code to build the TinyImageNet dataset from the official ImageNet 2012 classification dataset. 
 Main file is  make_tiny_imagenet.py ; this expects ImageNet files to be unpacked into a directory named  imagenet . torch-rnn 
 torch-rnn provides high-performance, reusable RNN and LSTM modules for torch7, and uses these modules for character-level
language modeling similar to  char-rnn . 
 You can find documentation for the RNN and LSTM modules  here ; they have no dependencies other than  torch 
and  nn , so they should be easy to integrate into existing projects. 
 Compared to char-rnn, torch-rnn is up to  1.9x faster  and uses up to  7x less memory . For more details see 
the  Benchmark  section below. 
 Installation 
 Docker Images 
 Cristian Baldi has prepared Docker images for both CPU-only mode and GPU mode;
you can  find them here . 
 System setup 
 You'll need to install the header files for Python 2.7 and the HDF5 library. On Ubuntu you should be able to install
like this: 
 bash
sudo apt-get -y install python2.7-dev
sudo apt-get install libhdf5-dev 
 Python setup 
 The preprocessing script is written in Python 2.7; its dependencies are in the file  requirements.txt .
You can install these dependencies in a virtual environment like this: 
 ```bash
virtualenv .env                  # Create the virtual environment
source .env/bin/activate         # Activate the virtual environment
pip install -r requirements.txt  # Install Python dependencies 
 Work for a while ... 
 deactivate                       # Exit the virtual environment
``` 
 Lua setup 
 The main modeling code is written in Lua using  torch ; you can find installation instructions
 here . You'll need the following Lua packages: 
 
 torch/torch7 
 torch/nn 
 torch/optim 
 lua-cjson 
 torch-hdf5 
 
 After installing torch, you can install / update these packages by running the following: 
 ```bash 
 Install most things using luarocks 
 luarocks install torch
luarocks install nn
luarocks install optim
luarocks install lua-cjson 
 We need to install torch-hdf5 from GitHub 
 git clone https://github.com/deepmind/torch-hdf5
cd torch-hdf5
luarocks make hdf5-0-0.rockspec
``` 
 CUDA support (Optional) 
 To enable GPU acceleration with CUDA, you'll need to install CUDA 6.5 or higher and the following Lua packages:
-  torch/cutorch 
-  torch/cunn 
 You can install / update them by running: 
 bash
luarocks install cutorch
luarocks install cunn 
 OpenCL support (Optional) 
 To enable GPU acceleration with OpenCL, you'll need to install the following Lua packages:
-  cltorch 
-  clnn 
 You can install / update them by running: 
 bash
luarocks install cltorch
luarocks install clnn 
 OSX Installation 
 Jeff Thompson has written a very detailed installation guide for OSX that you  can find here . 
 Usage 
 To train a model and use it to generate new text, you'll need to follow three simple steps: 
 Step 1: Preprocess the data 
 You can use any text file for training models. Before training, you'll need to preprocess the data using the script
 scripts/preprocess.py ; this will generate an HDF5 file and JSON file containing a preprocessed version of the data. 
 If you have training data stored in  my_data.txt , you can run the script like this: 
 bash
python scripts/preprocess.py \
  --input_txt my_data.txt \
  --output_h5 my_data.h5 \
  --output_json my_data.json 
 This will produce files  my_data.h5  and  my_data.json  that will be passed to the training script. 
 There are a few more flags you can use to configure preprocessing;  read about them here 
 Step 2: Train the model 
 After preprocessing the data, you'll need to train the model using the  train.lua  script. This will be the slowest step.
You can run the training script like this: 
 bash
th train.lua -input_h5 my_data.h5 -input_json my_data.json 
 This will read the data stored in  my_data.h5  and  my_data.json , run for a while, and save checkpoints to files with 
names like  cv/checkpoint_1000.t7 . 
 You can change the RNN model type, hidden state size, and number of RNN layers like this: 
 bash
th train.lua -input_h5 my_data.h5 -input_json my_data.json -model_type rnn -num_layers 3 -rnn_size 256 
 By default this will run in GPU mode using CUDA; to run in CPU-only mode, add the flag  -gpu -1 . 
 To run with OpenCL, add the flag  -gpu_backend opencl . 
 There are many more flags you can use to configure training;  read about them here . 
 Step 3: Sample from the model 
 After training a model, you can generate new text by sampling from it using the script  sample.lua . Run it like this: 
 bash
th sample.lua -checkpoint cv/checkpoint_10000.t7 -length 2000 
 This will load the trained checkpoint  cv/checkpoint_10000.t7  from the previous step, sample 2000 characters from it,
and print the results to the console. 
 By default the sampling script will run in GPU mode using CUDA; to run in CPU-only mode add the flag  -gpu -1  and
to run in OpenCL mode add the flag  -gpu_backend opencl . 
 There are more flags you can use to configure sampling;  read about them here . 
 Benchmarks 
 To benchmark  torch-rnn  against  char-rnn , we use each to train LSTM language models for the tiny-shakespeare dataset
with 1, 2 or 3 layers and with an RNN size of 64, 128, 256, or 512. For each we use a minibatch size of 50, a sequence 
length of 50, and no dropout. For each model size and for both implementations, we record the forward/backward times and 
GPU memory usage over the first 100 training iterations, and use these measurements to compute the mean time and memory 
usage. 
 All benchmarks were run on a machine with an Intel i7-4790k CPU, 32 GB main memory, and a Titan X GPU. 
 Below we show the forward/backward times for both implementations, as well as the mean speedup of  torch-rnn  over 
 char-rnn . We see that  torch-rnn  is faster than  char-rnn  at all model sizes, with smaller models giving a larger
speedup; for a single-layer LSTM with 128 hidden units, we achieve a  1.9x speedup ; for larger models we achieve about
a 1.4x speedup. 
 
 Below we show the GPU memory usage for both implementations, as well as the mean memory saving of  torch-rnn  over
 char-rnn . Again  torch-rnn  outperforms  char-rnn  at all model sizes, but here the savings become more significant for
larger models: for models with 512 hidden units, we use  7x less memory  than  char-rnn . 
 
 TODOs 
 
 Get rid of Python / JSON / HDF5 dependencies? 
 cnn-benchmarks 
 Benchmarks for popular convolutional neural network models on CPU and different GPUs, with and without cuDNN. 
 Some general conclusions from this benchmarking: 
 
 Pascal Titan X > GTX 1080 : Across all models, the Pascal Titan X is  1.31x to 1.43x  faster than the GTX 1080 and  1.47x to 1.60x  faster than the Maxwell Titan X. This is without a doubt the best card you can get for deep learning right now. 
 GTX 1080 > Maxwell Titan X : Across all models, the GTX 1080 is  1.10x to 1.15x  faster than the Maxwell Titan X. 
 ResNet > VGG : ResNet-50 is faster than VGG-16 and more accurate than VGG-19 (7.02 vs 9.0); ResNet-101 is about the same speed as VGG-19 but much more accurate than VGG-16 (6.21 vs 9.0). 
 Always use cuDNN : On the Pascal Titan X, cuDNN is  2.2x to 3.0x  faster than nn; on the GTX 1080, cuDNN is  2.0x to 2.8x  faster than nn; on the Maxwell Titan X, cuDNN is  2.2x to 3.0x  faster than nn. 
 GPUs are critical : The Pascal Titan X with cuDNN is  49x to 74x  faster than dual Xeon E5-2630 v3 CPUs. 
 
 All benchmarks were run in Torch. 
The GTX 1080 and Maxwell Titan X benchmarks were run on a machine with dual
Intel Xeon E5-2630 v3 processors (8 cores each plus hyperthreading means 32
threads) and 64GB RAM running Ubuntu 14.04 with the CUDA 8.0 Release Candidate.
The Pascal Titan X benchmarks were run on a machine with an Intel Core i5-6500
CPU and 16GB RAM running Ubuntu 16.04 with the CUDA 8.0 Release Candidate.
The GTX 1080 Ti benchmarks were run on a machine with an Intel Core i7-7700 CPU
and 64GB RAM running Ubuntu 16.04 with the CUDA 8.0 release. 
 We benchmark all models with a minibatch size of 16 and an image size of 224 x 224;
this allows direct comparisons between models, and allows all but the ResNet-200 model
to run on the GTX 1080, which has only 8GB of memory. 
 The following models are benchmarked: 
 |Network|Layers|Top-1 error|Top-5 error|Speed (ms)|Citation|
|---|---:|---:|---:|---:|---|
| AlexNet |8|42.90|19.80|14.56| [1] |
| Inception-V1 |22|-|10.07|39.14| [2] |
| VGG-16 |16|27.00|8.80|128.62| [3] |
| VGG-19 |19|27.30|9.00|147.32| [3] |
| ResNet-18 |18|30.43|10.76|31.54| [4] |
| ResNet-34 |34|26.73|8.74|51.59| [4] |
| ResNet-50 |50|24.01|7.02|103.58| [4] |
| ResNet-101 |101|22.44|6.21|156.44| [4] |
| ResNet-152 |152|22.16|6.16|217.91| [4] |
| ResNet-200 |200|21.66|5.79|296.51| [5] | 
 Top-1 and Top-5 error are single-crop error rates on the ILSVRC 2012 Validation set,
except for VGG-16 and VGG-19 which instead use dense prediction on a 256x256 image.
This gives the VGG models a slight advantage, but I was unable to find single-crop error
rates for these models. All models perform better when using more than one crop at test-time. 
 Speed is the total time for a forward and backward pass on a Pascal Titan X with cuDNN 5.1. 
 You can download the model files used for benchmarking  here  (2.1 GB);
these were converted from Caffe or Torch checkpoints using the  convert_model.lua  script. 
 We use the following GPUs for benchmarking: 
 |GPU|Memory|Architecture|CUDA Cores|FP32 TFLOPS|Release Date|
|---|---|---|---:|---:|---|
| Pascal Titan X |12GB GDDRX5|Pascal|3584|10.16|August 2016|
| GTX 1080 |8GB GDDRX5|Pascal|2560|8.87|May 2016|
| GTX 1080 Ti |11GB GDDRX5|Pascal|3584|10.6|March 2017|
| Maxwell Titan X |12GB GDDR5|Maxwell|3072|6.14|March 2015| 
 AlexNet 
 (input 16 x 3 x 224 x 224) 
 We use the  BVLC AlexNet  from Caffe. 
 AlexNet uses grouped convolutions; this was a strategy to allow model parallelism over two GTX 580
GPUs, which had only 3GB of memory each. Grouped convolutions are no longer commonly used, and are
not even implemented by the  torch/nn  backend; therefore we can only
benchmark AlexNet using cuDNN. 
 |GPU|cuDNN|Forward (ms)|Backward (ms)|Total (ms)|
|---|---|---:|---:|---:|
|GTX 1080 Ti|5.1.10|4.31|9.58|13.89|
|Pascal Titan X|5.1.05|5.04|9.52|14.56|
|Pascal Titan X|5.0.05|5.32|10.90|16.23|
|GTX 1080|5.1.05|7.00|13.74|20.74|
|Maxwell Titan X|5.1.05|7.09|14.76|21.85|
|GTX 1080|5.0.05|7.35|15.73|23.08|
|Maxwell Titan X|5.0.05|7.55|17.78|25.33|
|Maxwell Titan X|4.0.07|8.03|17.91|25.94| 
 Inception-V1 
 (input 16 x 3 x 224 x 224) 
 We use the Torch implementation of Inception-V1 from
 soumith/inception.torch . 
 |GPU|cuDNN|Forward (ms)|Backward (ms)|Total (ms)|
|---|---|---:|---:|---:|
|GTX 1080 Ti|5.1.10|11.50|25.37|36.87|
|Pascal Titan X|5.1.05|12.06|27.08|39.14|
|Pascal Titan X|5.0.05|11.94|28.39|40.33|
|GTX 1080|5.0.05|16.08|40.08|56.16|
|Maxwell Titan X|5.1.05|19.29|42.69|61.98|
|Maxwell Titan X|5.0.05|19.27|46.41|65.68|
|Maxwell Titan X|4.0.07|21.04|49.41|70.45|
|GTX 1080 Ti|None|56.34|85.30|141.64|
|Pascal Titan X|None|57.46|85.90|143.36|
|GTX 1080|None|63.03|102.31|165.34|
|Maxwell Titan X|None|91.31|140.81|232.12| 
 VGG-16 
 (input 16 x 3 x 224 x 224) 
 This is Model D in  [3]  used in the ILSVRC-2014 competition,
 available here . 
 |GPU|cuDNN|Forward (ms)|Backward (ms)|Total (ms)|
|---|---|---:|---:|---:|
|GTX 1080 Ti|5.1.10|41.23|86.91|128.14|
|Pascal Titan X|5.1.05|41.59|87.03|128.62|
|Pascal Titan X|5.0.05|46.16|111.23|157.39|
|GTX 1080|5.1.05|59.37|123.42|182.79|
|Maxwell Titan X|5.1.05|62.30|130.48|192.78|
|GTX 1080|5.0.05|67.27|166.17|233.43|
|Maxwell Titan X|5.0.05|75.80|186.47|262.27|
|Maxwell Titan X|4.0.07|111.99|226.69|338.69|
|Pascal Titan X|None|98.15|260.38|358.53|
|GTX 1080|None|143.73|379.09|522.82|
|Maxwell Titan X|None|172.61|415.87|588.47|
|CPU: Dual Xeon E5-2630 v3|None|3101.76|5393.72|8495.48| 
 VGG-19 
 (input 16 x 3 x 224 x 224) 
 This is Model E in  [3]  used in the ILSVRC-2014 competition,
 available here . 
 |GPU|cuDNN|Forward (ms)|Backward (ms)|Total (ms)|
|---|---|---:|---:|---:|
|Pascal Titan X|5.1.05|48.09|99.23|147.32|
|GTX 1080 Ti|5.1.10|48.15|100.04|148.19|
|Pascal Titan X|5.0.05|55.75|134.98|190.73|
|GTX 1080|5.1.05|68.95|141.44|210.39|
|Maxwell Titan X|5.1.05|73.66|151.48|225.14|
|GTX 1080|5.0.05|79.79|202.02|281.81|
|Maxwell Titan X|5.0.05|93.47|229.34|322.81|
|Maxwell Titan X|4.0.07|139.01|279.21|418.22|
|Pascal Titan X|None|121.69|318.39|440.08|
|GTX 1080|None|176.36|453.22|629.57|
|Maxwell Titan X|None|215.92|491.21|707.13|
|CPU: Dual Xeon E5-2630 v3|None|3609.78|6239.45|9849.23| 
 ResNet-18 
 (input 16 x 3 x 224 x 224) 
 This is the 18-layer model described in  [4]  and implemented in 
 fb.resnet.torch . 
 |GPU|cuDNN|Forward (ms)|Backward (ms)|Total (ms)|
|---|---|---:|---:|---:|
|Pascal Titan X|5.1.05|10.14|21.40|31.54|
|GTX 1080 Ti|5.1.10|10.45|22.34|32.78|
|Pascal Titan X|5.0.05|10.06|23.08|33.13|
|GTX 1080|5.1.05|14.62|29.32|43.94|
|GTX 1080|5.0.05|14.84|32.68|47.52|
|Maxwell Titan X|5.1.05|16.87|34.55|51.42|
|Maxwell Titan X|5.0.05|17.08|37.79|54.87|
|Maxwell Titan X|4.0.07|21.54|42.26|63.80|
|Pascal Titan X|None|34.76|61.64|96.40|
|GTX 1080 Ti|None|50.04|65.99|116.03|
|GTX 1080|None|42.94|79.17|122.10|
|Maxwell Titan X|None|55.82|96.01|151.82|
|CPU: Dual Xeon E5-2630 v3|None|847.46|1348.33|2195.78| 
 ResNet-34 
 (input 16 x 3 x 224 x 224) 
 This is the 34-layer model described in  [4]  and implemented in 
 fb.resnet.torch . 
 |GPU|cuDNN|Forward (ms)|Backward (ms)|Total (ms)|
|---|---|---:|---:|---:|
|GTX 1080 Ti|5.1.10|16.71|34.60|51.31|
|Pascal Titan X|5.1.05|17.01|34.58|51.59|
|Pascal Titan X|5.0.05|16.91|38.67|55.58|
|GTX 1080|5.1.05|24.50|47.59|72.09|
|GTX 1080|5.0.05|24.76|55.00|79.76|
|Maxwell Titan X|5.1.05|27.33|52.90|80.23|
|Maxwell Titan X|5.0.05|28.79|63.19|91.98|
|Maxwell Titan X|4.0.07|40.12|76.00|116.11|
|Pascal Titan X|None|66.56|106.42|172.98|
|GTX 1080 Ti|None|86.30|109.43|195.73|
|GTX 1080|None|82.71|137.42|220.13|
|Maxwell Titan X|None|108.95|166.19|275.13|
|CPU: Dual Xeon E5-2630 v3|None|1530.01|2435.20|3965.21| 
 ResNet-50 
 (input 16 x 3 x 224 x 224) 
 This is the 50-layer model described in  [4]  and implemented in 
 fb.resnet.torch . 
 |GPU|cuDNN|Forward (ms)|Backward (ms)|Total (ms)|
|---|---|---:|---:|---:|
|GTX 1080 Ti|5.1.10|34.14|67.06|101.21|
|Pascal Titan X|5.1.05|35.03|68.54|103.58|
|Pascal Titan X|5.0.05|35.03|70.76|105.78|
|GTX 1080|5.1.05|50.64|99.18|149.82|
|GTX 1080|5.0.05|50.76|103.35|154.11|
|Maxwell Titan X|5.1.05|55.75|103.87|159.62|
|Maxwell Titan X|5.0.05|56.30|109.75|166.05|
|Maxwell Titan X|4.0.07|62.03|116.81|178.84|
|Pascal Titan X|None|87.62|158.96|246.58|
|GTX 1080 Ti|None|99.90|177.58|277.47|
|GTX 1080|None|109.79|201.40|311.18|
|Maxwell Titan X|None|137.14|247.65|384.79|
|CPU: Dual Xeon E5-2630 v3|None|2477.61|4149.64|6627.25| 
 ResNet-101 
 (input 16 x 3 x 224 x 224) 
 This is the 101-layer model described in  [4]  and implemented in 
 fb.resnet.torch . 
 |GPU|cuDNN|Forward (ms)|Backward (ms)|Total (ms)|
|---|---|---:|---:|---:|
|GTX 1080 Ti|5.1.10|52.18|102.08|154.26|
|Pascal Titan X|5.1.05|53.38|103.06|156.44|
|Pascal Titan X|5.0.05|53.28|108.20|161.48|
|GTX 1080|5.1.05|77.59|148.21|225.80|
|GTX 1080|5.0.05|77.39|158.19|235.58|
|Maxwell Titan X|5.1.05|87.76|159.73|247.49|
|Maxwell Titan X|5.0.05|88.45|172.12|260.57|
|Maxwell Titan X|4.0.07|108.96|189.93|298.90|
|Pascal Titan X|None|161.55|257.57|419.11|
|GTX 1080 Ti|None|162.03|266.77|428.81|
|GTX 1080|None|203.19|322.48|525.67|
|Maxwell Titan X|None|260.48|453.45|713.93|
|CPU: Dual Xeon E5-2630 v3|None|4414.91|6891.33|11306.24| 
 ResNet-152 
 (input 16 x 3 x 224 x 224) 
 This is the 152-layer model described in  [4]  and implemented in 
 fb.resnet.torch . 
 |GPU|cuDNN|Forward (ms)|Backward (ms)|Total (ms)|
|---|---|---:|---:|---:|
|GTX 1080 Ti|5.1.10|73.52|142.02|215.54|
|Pascal Titan X|5.1.05|75.45|142.47|217.91|
|Pascal Titan X|5.0.05|75.12|150.08|225.20|
|GTX 1080|5.1.05|109.32|204.98|314.30|
|GTX 1080|5.0.05|109.64|218.62|328.26|
|Maxwell Titan X|5.1.05|124.04|221.41|345.45|
|Maxwell Titan X|5.0.05|124.88|240.16|365.03|
|Maxwell Titan X|4.0.07|150.90|268.64|419.54|
|Pascal Titan X|None|238.04|371.40|609.43|
|GTX 1080 Ti|None|225.36|368.42|593.79|
|GTX 1080|None|299.05|461.67|760.72|
|Maxwell Titan X|None|382.39|583.83|966.22|
|CPU: Dual Xeon E5-2630 v3|None|6572.17|10300.61|16872.78| 
 ResNet-200 
 (input 16 x 3 x 224 x 224) 
 This is the 200-layer model described in  [5]  and implemented in 
 fb.resnet.torch . 
 Even with a batch size of 16, the 8GB GTX 1080 did not have enough memory to run
the model. 
 |GPU|cuDNN|Forward (ms)|Backward (ms)|Total (ms)|
|---|---|---:|---:|---:|
|Pascal Titan X|5.1.05|104.74|191.77|296.51|
|Pascal Titan X|5.0.05|104.36|201.92|306.27|
|Maxwell Titan X|5.0.05|170.03|320.80|490.83|
|Maxwell Titan X|5.1.05|169.62|383.80|553.42|
|Maxwell Titan X|4.0.07|203.52|356.35|559.87|
|Pascal Titan X|None|314.77|519.72|834.48|
|Maxwell Titan X|None|497.57|953.94|1451.51|
|CPU: Dual Xeon E5-2630 v3|None|8666.43|13758.73|22425.16| 
 Citations 
 
[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ""ImageNet Classification with Deep Convolutional Neural Networks."" NIPS 2012
 
 
[2] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,
Dragomir Anguelov, Dumitru Erhan, Andrew Rabinovich.
""Going Deeper with Convolutions."" CVPR 2015.
 
 
[3] Karen Simonyan and Andrew Zisserman. ""Very Deep Convolutional Networks for Large-Scale Image Recognition."" ICLR 2015
 
 
[4] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. ""Deep Residual Learning for Image Recognition."" CVPR 2016.
 
 
[5] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. ""Identity Mappings in Deep Residual Networks."" ECCV 2016. fast-neural-style 
 This is the code for the paper 
 Perceptual Losses for Real-Time Style Transfer and Super-Resolution 
 
 Justin Johnson ,
 Alexandre Alahi ,
 Li Fei-Fei 
 
Presented at  ECCV 2016 
 The paper builds on
 A Neural Algorithm of Artistic Style 
by Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge by training
feedforward neural networks that apply artistic styles to images.
After training, our feedforward networks can stylize images
 hundreds of times faster  than the optimization-based method presented
by Gatys et al. 
 This repository also includes an implementation of instance normalization as
described in the paper  Instance Normalization: The Missing Ingredient for Fast Stylization 
by Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. This simple trick
significantly improves the quality of feedforward style transfer models. 
 Stylizing this image of the Stanford campus at a resolution of 1200x630
takes  50 milliseconds  on a Pascal Titan X: 
 
 
 
 
 
 In this repository we provide:
- The style transfer models  used in the paper 
- Additional models  using instance normalization 
- Code for  running models on new images 
- A demo that runs models in  real-time off a webcam 
- Code for  training new feedforward style transfer models 
- An implementation of  optimization-based style transfer 
  method described by Gatys et al. 
 If you find this code useful for your research, please cite 
 @inproceedings{Johnson2016Perceptual,
  title={Perceptual losses for real-time style transfer and super-resolution},
  author={Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
  booktitle={European Conference on Computer Vision},
  year={2016}
} 
 Setup 
 All code is implemented in  Torch . 
 First  install Torch , then
update / install the following packages: 
 bash
luarocks install torch
luarocks install nn
luarocks install image
luarocks install lua-cjson 
 (Optional) GPU Acceleration 
 If you have an NVIDIA GPU, you can accelerate all operations with CUDA. 
 First  install CUDA , then
update / install the following packages: 
 bash
luarocks install cutorch
luarocks install cunn 
 (Optional) cuDNN 
 When using CUDA, you can use cuDNN to accelerate convolutions. 
 First  download cuDNN  and copy the
libraries to  /usr/local/cuda/lib64/ . Then install the Torch bindings for cuDNN: 
 bash
luarocks install cudnn 
 Pretrained Models 
 Download all pretrained style transfer models by running the script 
 bash
bash models/download_style_transfer_models.sh 
 This will download ten model files (~200MB) to the folder  models/ . 
 Models from the paper 
 The style transfer models we used in the paper will be located in the folder  models/eccv16 .
Here are some example results where we use these models to stylize this
image of the Chicago skyline with at an image size of 512: 
 
 
 
 
 
 
 
 
 
 
 
 
 Models with instance normalization 
 As discussed in the paper
 Instance Normalization: The Missing Ingredient for Fast Stylization 
by Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky, replacing batch
normalization with instance normalization significantly improves the quality
of feedforward style transfer models. 
 We have trained several models with instance normalization; after downloading
pretrained models they will be in the folder  models/instance_norm . 
 These models use the same architecture as those used in our paper, except with
half the number of filters per layer and with instance normalization instead of
batch normalization. Using narrower layers makes the models smaller and faster
without sacrificing model quality. 
 Here are some example outputs from these models, with an image size of 1024: 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Running on new images 
 The script  fast_neural_style.lua  lets you use a trained model to stylize new images: 
 bash
th fast_neural_style.lua \
  -model models/eccv16/starry_night.t7 \
  -input_image images/content/chicago.jpg \
  -output_image out.png 
 You can run the same model on an entire directory of images like this: 
 bash
th fast_neural_style.lua \
  -model models/eccv16/starry_night.t7 \
  -input_dir images/content/ \
  -output_dir out/ 
 You can control the size of the output images using the  -image_size  flag. 
 By default this script runs on CPU; to run on GPU, add the flag  -gpu 
specifying the GPU on which to run. 
 The full set of options for this script is  described here . 
 Webcam demo 
 You can use the script  webcam_demo.lua  to run one or more models in real-time
off a webcam stream. To run this demo you need to use  qlua  instead of  th : 
 bash
qlua webcam_demo.lua -models models/instance_norm/candy.t7 -gpu 0 
 You can run multiple models at the same time by passing a comma-separated list
to the  -models  flag: 
 bash
qlua webcam_demo.lua \
  -models models/instance_norm/candy.t7,models/instance_norm/udnie.t7 \
  -gpu 0 
 With a Pascal Titan X you can easily run four models in realtime at 640x480: 
 
 
 
 The webcam demo depends on a few extra Lua packages:
-  clementfarabet/lua---camera 
-  torch/qtlua 
 You can install / update these packages by running: 
 bash
luarocks install camera
luarocks install qtlua 
 The full set of options for this script is  described here . 
 Training new models 
 You can  find instructions for training new models here . 
 Optimization-based Style Transfer 
 The script  slow_neural_style.lua  is similar to the
 original neural-style , and uses
the optimization-based style-transfer method described by Gatys et al. 
 This script uses the same code for computing losses as the feedforward training
script, allowing for fair comparisons between feedforward style transfer networks
and optimization-based style transfer. 
 Compared to the original  neural-style ,
this script has the following improvements: 
 
 Remove dependency on protobuf and  loadcaffe 
 Support for many more CNN architectures, including ResNets 
 
 The full set of options for this script is  described here . 
 License 
 Free for personal or research use; for commercial use please contact me. 
 
 PyTorch is a Python package that provides two high-level features:
- Tensor computation (like NumPy) with strong GPU acceleration
- Deep neural networks built on a tape-based autograd system 
 You can reuse your favorite Python packages such as NumPy, SciPy and Cython to extend PyTorch when needed. 
 We are in an early-release beta. Expect some adventures and rough edges. 
 
 More about PyTorch 
 Installation 
 Binaries 
 From Source 
 Docker Image 
 Getting Started 
 Communication 
 Releases and Contributing 
 The Team 
 
 | System | 2.7 | 3.5 |
| --- | --- | --- |
| Linux CPU |   |   |
| Linux GPU |   |   |
| macOS CPU |   |   | 
 More about PyTorch 
 At a granular level, PyTorch is a library that consists of the following components: 
 
 
  torch  
  a Tensor library like NumPy, with strong GPU support  
 
 
  torch.autograd  
  a tape-based automatic differentiation library that supports all differentiable Tensor operations in torch  
 
 
  torch.nn  
  a neural networks library deeply integrated with autograd designed for maximum flexibility  
 
 
  torch.multiprocessing   
  Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training.  
 
 
  torch.utils  
  DataLoader, Trainer and other utility functions for convenience  
 
 
  torch.legacy(.nn/.optim)  
  legacy code that has been ported over from torch for backward compatibility reasons  
 
 
 Usually one uses PyTorch either as: 
 
 a replacement for NumPy to use the power of GPUs. 
 a deep learning research platform that provides maximum flexibility and speed 
 
 Elaborating further: 
 A GPU-Ready Tensor Library 
 If you use NumPy, then you have used Tensors (a.k.a ndarray). 
 
 PyTorch provides Tensors that can live either on the CPU or the GPU, and accelerate
compute by a huge amount. 
 We provide a wide variety of tensor routines to accelerate and fit your scientific computation needs
such as slicing, indexing, math operations, linear algebra, reductions.
And they are fast! 
 Dynamic Neural Networks: Tape-Based Autograd 
 PyTorch has a unique way of building neural networks: using and replaying a tape recorder. 
 Most frameworks such as TensorFlow, Theano, Caffe and CNTK have a static view of the world.
One has to build a neural network, and reuse the same structure again and again.
Changing the way the network behaves means that one has to start from scratch. 
 With PyTorch, we use a technique called reverse-mode auto-differentiation, which allows you to
change the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes
from several research papers on this topic, as well as current and past work such as
 autograd ,
 autograd ,
 Chainer , etc. 
 While this technique is not unique to PyTorch, it's one of the fastest implementations of it to date.
You get the best of speed and flexibility for your crazy research. 
 
 Python First 
 PyTorch is not a Python binding into a monolithic C++ framework.
It is built to be deeply integrated into Python.
You can use it naturally like you would use NumPy / SciPy / scikit-learn etc.
You can write your new neural network layers in Python itself, using your favorite libraries
and use packages such as Cython and Numba.
Our goal is to not reinvent the wheel where appropriate. 
 Imperative Experiences 
 PyTorch is designed to be intuitive, linear in thought and easy to use.
When you execute a line of code, it gets executed. There isn't an asynchronous view of the world.
When you drop into a debugger, or receive error messages and stack traces, understanding them is straightforward.
The stack trace points to exactly where your code was defined.
We hope you never spend hours debugging your code because of bad stack traces or asynchronous and opaque execution engines. 
 Fast and Lean 
 PyTorch has minimal framework overhead. We integrate acceleration libraries
such as Intel MKL and NVIDIA (cuDNN, NCCL) to maximize speed.
At the core, its CPU and GPU Tensor and neural network backends
(TH, THC, THNN, THCUNN) are written as independent libraries with a C99 API. 
They are mature and have been tested for years. 
 Hence, PyTorch is quite fast – whether you run small or large neural networks. 
 The memory usage in PyTorch is extremely efficient compared to Torch or some of the alternatives.
We've written custom memory allocators for the GPU to make sure that
your deep learning models are maximally memory efficient.
This enables you to train bigger deep learning models than before. 
 Extensions without Pain 
 Writing new neural network modules, or interfacing with PyTorch's Tensor API was designed to be straightforward
and with minimal abstractions. 
 You can write new neural network layers in Python using the torch API
 or your favorite NumPy-based libraries such as SciPy . 
 If you want to write your layers in C/C++, we provide an extension API based on
 cffi  that is efficient and with minimal boilerplate.
There is no wrapper code that needs to be written. You can see  a tutorial here  and  an example here . 
 Installation 
 Binaries 
 Commands to install from binaries via Conda or pip wheels are on our website: 
 http://pytorch.org 
 From Source 
 If you are installing from source, we highly recommend installing an  Anaconda  environment.
You will get a high-quality BLAS library (MKL) and you get a controlled compiler version regardless of your Linux distro. 
 Once you have  Anaconda  installed, here are the instructions. 
 If you want to compile with CUDA support, install
-  NVIDIA CUDA  7.5 or above
-  NVIDIA cuDNN  v5.x or above 
 If you want to disable CUDA support, export environment variable  NO_CUDA=1 . 
 Install optional dependencies 
 On Linux
```bash
export CMAKE_PREFIX_PATH=""$(dirname $(which conda))/../"" # [anaconda root directory] 
 Install basic dependencies 
 conda install numpy pyyaml mkl setuptools cmake gcc cffi 
 Add LAPACK support for the GPU 
 conda install -c soumith magma-cuda80 # or magma-cuda75 if CUDA 7.5
``` 
 On OSX
 bash
export CMAKE_PREFIX_PATH=[anaconda root directory]
conda install numpy pyyaml setuptools cmake cffi 
 Install PyTorch 
 On Linux
 bash
python setup.py install 
 On OSX
 bash
MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py install 
 Docker image 
 Dockerfile is supplied to build images with cuda support and cudnn v6. Build as usual
 docker build -t pytorch . 
Alternatively, if you want to use a runtime image, you can use the pre-built one from Docker Hub and run with nvidia-docker:
 nvidia-docker run --rm -ti --ipc=host pytorch/pytorch:latest 
Please note that PyTorch uses shared memory to share data between processes, so if torch multiprocessing is used (e.g.
for multithreaded data loaders) the default shared memory segment size that container runs with is not enough, and you
should increase shared memory size either with  --ipc=host  or  --shm-size  command line options to  nvidia-docker run . 
 Getting Started 
 Three pointers to get you started:
-  Tutorials: get you started with understanding and using PyTorch 
-  Examples: easy to understand pytorch code across all domains 
- The API Reference:  http://pytorch.org/docs/ 
 Communication 
 
 forums: discuss implementations, research, etc. http://discuss.pytorch.org 
 GitHub issues: bug reports, feature requests, install issues, RFCs, thoughts, etc. 
 Slack: general chat, online discussions, collaboration etc. https://pytorch.slack.com/ . Our slack channel is invite-only to promote a healthy balance between power-users and beginners. If you need a slack invite, ping us at soumith@pytorch.org 
 newsletter: no-noise, one-way email newsletter with important announcements about pytorch. You can sign-up here: http://eepurl.com/cbG0rv 
 
 Releases and Contributing 
 PyTorch has a 90 day release cycle (major releases).
It's current state is Beta, we expect no obvious bugs. Please let us know if you encounter a bug by  filing an issue . 
 We appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion. 
 If you plan to contribute new features, utility functions or extensions to the core, please first open an issue and discuss the feature with us.
Sending a PR without discussion might end up resulting in a rejected PR, because we might be taking the core in a different direction than you might be aware of. 
 The Team 
 PyTorch is a community driven project with several skillful engineers and researchers contributing to it. 
 PyTorch is currently maintained by  Adam Paszke ,  Sam Gross ,  Soumith Chintala  and  Gregory Chanan  with major contributions coming from 10s of talented individuals in various forms and means.
A non-exhaustive but growing list needs to mention: Trevor Killeen, Sasank Chilamkurthy, Sergey Zagoruyko, Adam Lerer, Francisco Massa, Alykhan Tejani, Luca Antiga, Alban Desmaison, Andreas Kopf, James Bradbury, Zeming Lin, Yuandong Tian, Guillaume Lample, Marat Dukhan, Natalia Gimelshein, Christian Sarofeen, Martin Raison, Edward Yang, Zachary Devito. 
 Note: this project is unrelated to  hughperkins/pytorch  with the same name. Hugh is a valuable contributor in the Torch community and has helped with many things Torch and PyTorch. pytorch-vgg 
 Some scripts to convert the  VGG-16 
and  VGG-19  models [1] from Caffe to PyTorch. 
 The converted models can be used with the PyTorch model zoo and are available here: 
 VGG-16: https://web.eecs.umich.edu/~justincj/models/vgg16-00b39a1b.pth 
 VGG-19: https://web.eecs.umich.edu/~justincj/models/vgg19-d01eb7cb.pth 
 These models expect different preprocessing than the other models in the PyTorch model zoo.
Images should be in BGR format in the range [0, 255], and the following BGR values should then be
subtracted from each pixel: [103.939, 116.779, 123.68] 
 [1] Karen Simonyan and Andrew Zisserman, ""Very Deep Convolutional Networks for Large-Scale Image Recognition"", ICLR 2015 This repository introduces the fundamental concepts of
 PyTorch 
through self-contained examples. 
 At its core, PyTorch provides two main features:
- An n-dimensional Tensor, similar to numpy but can run on GPUs
- Automatic differentiation for building and training neural networks 
 We will use a fully-connected ReLU network as our running example. The network
will have a single hidden layer, and will be trained with gradient descent to
fit random data by minimizing the Euclidean distance between the network output
and the true output. 
 NOTE:  These examples have been update for PyTorch 0.4, which made several
major changes to the core PyTorch API. Most notably, prior to 0.4 Tensors had
to be wrapped in Variable objects to use autograd; this functionality has now
been added directly to Tensors, and Variables are now deprecated. 
 Table of Contents 
 
 Warm-up: numpy 
 PyTorch: Tensors 
 PyTorch: Autograd 
 PyTorch: Defining new autograd functions 
 TensorFlow: Static Graphs 
 PyTorch: nn 
 PyTorch: optim 
 PyTorch: Custom nn Modules 
 PyTorch: Control Flow and Weight Sharing 
 
 Warm-up: numpy 
 Before introducing PyTorch, we will first implement the network using numpy. 
 Numpy provides an n-dimensional array object, and many functions for manipulating
these arrays. Numpy is a generic framework for scientific computing; it does not
know anything about computation graphs, or deep learning, or gradients. However
we can easily use numpy to fit a two-layer network to random data by manually
implementing the forward and backward passes through the network using numpy
operations: 
 ```python 
 Code in file tensor/two_layer_net_numpy.py 
 import numpy as np 
 N is batch size; D_in is input dimension; 
 H is hidden dimension; D_out is output dimension. 
 N, D_in, H, D_out = 64, 1000, 100, 10 
 Create random input and output data 
 x = np.random.randn(N, D_in)
y = np.random.randn(N, D_out) 
 Randomly initialize weights 
 w1 = np.random.randn(D_in, H)
w2 = np.random.randn(H, D_out) 
 learning_rate = 1e-6
for t in range(500):
  # Forward pass: compute predicted y
  h = x.dot(w1)
  h_relu = np.maximum(h, 0)
  y_pred = h_relu.dot(w2) 
 # Compute and print loss
  loss = np.square(y_pred - y).sum()
  print(t, loss) 
 # Backprop to compute gradients of w1 and w2 with respect to loss
  grad_y_pred = 2.0 * (y_pred - y)
  grad_w2 = h_relu.T.dot(grad_y_pred)
  grad_h_relu = grad_y_pred.dot(w2.T)
  grad_h = grad_h_relu.copy()
  grad_h[h < 0] = 0
  grad_w1 = x.T.dot(grad_h) 
 # Update weights
  w1 -= learning_rate * grad_w1
  w2 -= learning_rate * grad_w2
``` 
 PyTorch: Tensors 
 Numpy is a great framework, but it cannot utilize GPUs to accelerate its
numerical computations. For modern deep neural networks, GPUs often provide
speedups of  50x or greater , so
unfortunately numpy won't be enough for modern deep learning. 
 Here we introduce the most fundamental PyTorch concept: the  Tensor . A PyTorch
Tensor is conceptually identical to a numpy array: a Tensor is an n-dimensional
array, and PyTorch provides many functions for operating on these Tensors.
Any computation you might want to perform with numpy can also be accomplished
with PyTorch Tensors; you should think of them as a generic tool for scientific
computing. 
 However unlike numpy, PyTorch Tensors can utilize GPUs to accelerate their
numeric computations. To run a PyTorch Tensor on GPU, you use the  device 
argument when constructing a Tensor to place the Tensor on a GPU. 
 Here we use PyTorch Tensors to fit a two-layer network to random data. Like the
numpy example above we manually implement the forward and backward
passes through the network, using operations on PyTorch Tensors: 
 ```python 
 Code in file tensor/two_layer_net_tensor.py 
 import torch 
 device = torch.device('cpu') 
 device = torch.device('cuda') # Uncomment this to run on GPU 
 N is batch size; D_in is input dimension; 
 H is hidden dimension; D_out is output dimension. 
 N, D_in, H, D_out = 64, 1000, 100, 10 
 Create random input and output data 
 x = torch.randn(N, D_in, device=device)
y = torch.randn(N, D_out, device=device) 
 Randomly initialize weights 
 w1 = torch.randn(D_in, H, device=device)
w2 = torch.randn(H, D_out, device=device) 
 learning_rate = 1e-6
for t in range(500):
  # Forward pass: compute predicted y
  h = x.mm(w1)
  h_relu = h.clamp(min=0)
  y_pred = h_relu.mm(w2) 
 # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor
  # of shape (); we can get its value as a Python number with loss.item().
  loss = (y_pred - y).pow(2).sum()
  print(t, loss.item()) 
 # Backprop to compute gradients of w1 and w2 with respect to loss
  grad_y_pred = 2.0 * (y_pred - y)
  grad_w2 = h_relu.t().mm(grad_y_pred)
  grad_h_relu = grad_y_pred.mm(w2.t())
  grad_h = grad_h_relu.clone()
  grad_h[h < 0] = 0
  grad_w1 = x.t().mm(grad_h) 
 # Update weights using gradient descent
  w1 -= learning_rate * grad_w1
  w2 -= learning_rate * grad_w2
``` 
 PyTorch: Autograd 
 In the above examples, we had to manually implement both the forward and
backward passes of our neural network. Manually implementing the backward pass
is not a big deal for a small two-layer network, but can quickly get very hairy
for large complex networks. 
 Thankfully, we can use
 automatic differentiation 
to automate the computation of backward passes in neural networks. 
The  autograd  package in PyTorch provides exactly this functionality.
When using autograd, the forward pass of your network will define a
 computational graph ; nodes in the graph will be Tensors, and edges will be
functions that produce output Tensors from input Tensors. Backpropagating through
this graph then allows you to easily compute gradients. 
 This sounds complicated, it's pretty simple to use in practice. If we want to
compute gradients with respect to some Tensor, then we set  requires_grad=True 
when constructing that Tensor. Any PyTorch operations on that Tensor will cause
a computational graph to be constructed, allowing us to later perform backpropagation
through the graph. If  x  is a Tensor with  requires_grad=True , then after
backpropagation  x.grad  will be another Tensor holding the gradient of  x  with
respect to some scalar value. 
 Sometimes you may wish to prevent PyTorch from building computational graphs when
performing certain operations on Tensors with  requires_grad=True ; for example
we usually don't want to backpropagate through the weight update steps when
training a neural network. In such scenarios we can use the  torch.no_grad() 
context manager to prevent the construction of a computational graph. 
 Here we use PyTorch Tensors and autograd to implement our two-layer network;
now we no longer need to manually implement the backward pass through the
network: 
 ```python 
 Code in file autograd/two_layer_net_autograd.py 
 import torch 
 device = torch.device('cpu') 
 device = torch.device('cuda') # Uncomment this to run on GPU 
 N is batch size; D_in is input dimension; 
 H is hidden dimension; D_out is output dimension. 
 N, D_in, H, D_out = 64, 1000, 100, 10 
 Create random Tensors to hold input and outputs 
 x = torch.randn(N, D_in, device=device)
y = torch.randn(N, D_out, device=device) 
 Create random Tensors for weights; setting requires_grad=True means that we 
 want to compute gradients for these Tensors during the backward pass. 
 w1 = torch.randn(D_in, H, device=device, requires_grad=True)
w2 = torch.randn(H, D_out, device=device, requires_grad=True) 
 learning_rate = 1e-6
for t in range(500):
  # Forward pass: compute predicted y using operations on Tensors. Since w1 and
  # w2 have requires_grad=True, operations involving these Tensors will cause
  # PyTorch to build a computational graph, allowing automatic computation of
  # gradients. Since we are no longer implementing the backward pass by hand we
  # don't need to keep references to intermediate values.
  y_pred = x.mm(w1).clamp(min=0).mm(w2) 
 # Compute and print loss. Loss is a Tensor of shape (), and loss.item()
  # is a Python number giving its value.
  loss = (y_pred - y).pow(2).sum()
  print(t, loss.item()) 
 # Use autograd to compute the backward pass. This call will compute the
  # gradient of loss with respect to all Tensors with requires_grad=True.
  # After this call w1.grad and w2.grad will be Tensors holding the gradient
  # of the loss with respect to w1 and w2 respectively.
  loss.backward() 
 # Update weights using gradient descent. For this step we just want to mutate
  # the values of w1 and w2 in-place; we don't want to build up a computational
  # graph for the update steps, so we use the torch.no_grad() context manager
  # to prevent PyTorch from building a computational graph for the updates
  with torch.no_grad():
    w1 -= learning_rate * w1.grad
    w2 -= learning_rate * w2.grad 
 # Manually zero the gradients after running the backward pass
w1.grad.zero_()
w2.grad.zero_()
 
 ``` 
 PyTorch: Defining new autograd functions 
 Under the hood, each primitive autograd operator is really two functions that
operate on Tensors. The  forward  function computes output Tensors from input
Tensors. The  backward  function receives the gradient of the output Tensors
with respect to some scalar value, and computes the gradient of the input Tensors
with respect to that same scalar value. 
 In PyTorch we can easily define our own autograd operator by defining a subclass
of  torch.autograd.Function  and implementing the  forward  and  backward  functions.
We can then use our new autograd operator by constructing an instance and calling it
like a function, passing Tensors containing input data. 
 In this example we define our own custom autograd function for performing the ReLU
nonlinearity, and use it to implement our two-layer network: 
 ```python 
 Code in file autograd/two_layer_net_custom_function.py 
 import torch 
 class MyReLU(torch.autograd.Function):
  """"""
  We can implement our own custom autograd Functions by subclassing
  torch.autograd.Function and implementing the forward and backward passes
  which operate on Tensors.
  """"""
  @staticmethod
  def forward(ctx, x):
    """"""
    In the forward pass we receive a context object and a Tensor containing the
    input; we must return a Tensor containing the output, and we can use the
    context object to cache objects for use in the backward pass.
    """"""
    ctx.save_for_backward(x)
    return x.clamp(min=0) 
 @staticmethod
  def backward(ctx, grad_output):
    """"""
    In the backward pass we receive the context object and a Tensor containing
    the gradient of the loss with respect to the output produced during the
    forward pass. We can retrieve cached data from the context object, and must
    compute and return the gradient of the loss with respect to the input to the
    forward function.
    """"""
    x, = ctx.saved_tensors
    grad_x = grad_output.clone()
    grad_x[x < 0] = 0
    return grad_x 
 device = torch.device('cpu') 
 device = torch.device('cuda') # Uncomment this to run on GPU 
 N is batch size; D_in is input dimension; 
 H is hidden dimension; D_out is output dimension. 
 N, D_in, H, D_out = 64, 1000, 100, 10 
 Create random Tensors to hold input and output 
 x = torch.randn(N, D_in, device=device)
y = torch.randn(N, D_out, device=device) 
 Create random Tensors for weights. 
 w1 = torch.randn(D_in, H, device=device, requires_grad=True)
w2 = torch.randn(H, D_out, device=device, requires_grad=True) 
 learning_rate = 1e-6
for t in range(500):
  # Forward pass: compute predicted y using operations on Tensors; we call our
  # custom ReLU implementation using the MyReLU.apply function
  y_pred = MyReLU.apply(x.mm(w1)).mm(w2) 
 # Compute and print loss
  loss = (y_pred - y).pow(2).sum()
  print(t, loss.item()) 
 # Use autograd to compute the backward pass.
  loss.backward() 
 with torch.no_grad():
    # Update weights using gradient descent
    w1 -= learning_rate * w1.grad
    w2 -= learning_rate * w2.grad 
 # Manually zero the gradients after running the backward pass
w1.grad.zero_()
w2.grad.zero_()
 
 ``` 
 TensorFlow: Static Graphs 
 PyTorch autograd looks a lot like TensorFlow: in both frameworks we define
a computational graph, and use automatic differentiation to compute gradients.
The biggest difference between the two is that TensorFlow's computational graphs
are  static  and PyTorch uses  dynamic  computational graphs. 
 In TensorFlow, we define the computational graph once and then execute the same
graph over and over again, possibly feeding different input data to the graph.
In PyTorch, each forward pass defines a new computational graph. 
 Static graphs are nice because you can optimize the graph up front; for example
a framework might decide to fuse some graph operations for efficiency, or to
come up with a strategy for distributing the graph across many GPUs or many
machines. If you are reusing the same graph over and over, then this potentially
costly up-front optimization can be amortized as the same graph is rerun over
and over. 
 One aspect where static and dynamic graphs differ is control flow. For some models
we may wish to perform different computation for each data point; for example a
recurrent network might be unrolled for different numbers of time steps for each
data point; this unrolling can be implemented as a loop. With a static graph the
loop construct needs to be a part of the graph; for this reason TensorFlow
provides operators such as  tf.scan  for embedding loops into the graph. With
dynamic graphs the situation is simpler: since we build graphs on-the-fly for
each example, we can use normal imperative flow control to perform computation
that differs for each input. 
 To contrast with the PyTorch autograd example above, here we use TensorFlow to
fit a simple two-layer net: 
 ```python 
 Code in file autograd/tf_two_layer_net.py 
 import tensorflow as tf
import numpy as np 
 First we set up the computational graph: 
 N is batch size; D_in is input dimension; 
 H is hidden dimension; D_out is output dimension. 
 N, D_in, H, D_out = 64, 1000, 100, 10 
 Create placeholders for the input and target data; these will be filled 
 with real data when we execute the graph. 
 x = tf.placeholder(tf.float32, shape=(None, D_in))
y = tf.placeholder(tf.float32, shape=(None, D_out)) 
 Create Variables for the weights and initialize them with random data. 
 A TensorFlow Variable persists its value across executions of the graph. 
 w1 = tf.Variable(tf.random_normal((D_in, H)))
w2 = tf.Variable(tf.random_normal((H, D_out))) 
 Forward pass: Compute the predicted y using operations on TensorFlow Tensors. 
 Note that this code does not actually perform any numeric operations; it 
 merely sets up the computational graph that we will later execute. 
 h = tf.matmul(x, w1)
h_relu = tf.maximum(h, tf.zeros(1))
y_pred = tf.matmul(h_relu, w2) 
 Compute loss using operations on TensorFlow Tensors 
 loss = tf.reduce_sum((y - y_pred) ** 2.0) 
 Compute gradient of the loss with respect to w1 and w2. 
 grad_w1, grad_w2 = tf.gradients(loss, [w1, w2]) 
 Update the weights using gradient descent. To actually update the weights 
 we need to evaluate new_w1 and new_w2 when executing the graph. Note that 
 in TensorFlow the the act of updating the value of the weights is part of 
 the computational graph; in PyTorch this happens outside the computational 
 graph. 
 learning_rate = 1e-6
new_w1 = w1.assign(w1 - learning_rate * grad_w1)
new_w2 = w2.assign(w2 - learning_rate * grad_w2) 
 Now we have built our computational graph, so we enter a TensorFlow session to 
 actually execute the graph. 
 with tf.Session() as sess:
  # Run the graph once to initialize the Variables w1 and w2.
  sess.run(tf.global_variables_initializer()) 
 # Create numpy arrays holding the actual data for the inputs x and targets y
  x_value = np.random.randn(N, D_in)
  y_value = np.random.randn(N, D_out)
  for _ in range(500):
    # Execute the graph many times. Each time it executes we want to bind
    # x_value to x and y_value to y, specified with the feed_dict argument.
    # Each time we execute the graph we want to compute the values for loss,
    # new_w1, and new_w2; the values of these Tensors are returned as numpy
    # arrays.
    loss_value, _, _ = sess.run([loss, new_w1, new_w2],
                                feed_dict={x: x_value, y: y_value})
    print(loss_value)
``` 
 PyTorch: nn 
 Computational graphs and autograd are a very powerful paradigm for defining
complex operators and automatically taking derivatives; however for large
neural networks raw autograd can be a bit too low-level. 
 When building neural networks we frequently think of arranging the computation
into  layers , some of which have  learnable parameters  which will be
optimized during learning. 
 In TensorFlow, packages like  Keras ,
 TensorFlow-Slim ,
and  TFLearn  provide higher-level abstractions over
raw computational graphs that are useful for building neural networks. 
 In PyTorch, the  nn  package serves this same purpose. The  nn  package defines a set of
 Modules , which are roughly equivalent to neural network layers. A Module receives
input Tensors and computes output Tensors, but may also hold internal state such as
Tensors containing learnable parameters. The  nn  package also defines a set of useful
loss functions that are commonly used when training neural networks. 
 In this example we use the  nn  package to implement our two-layer network: 
 ```python 
 Code in file nn/two_layer_net_nn.py 
 import torch 
 device = torch.device('cpu') 
 device = torch.device('cuda') # Uncomment this to run on GPU 
 N is batch size; D_in is input dimension; 
 H is hidden dimension; D_out is output dimension. 
 N, D_in, H, D_out = 64, 1000, 100, 10 
 Create random Tensors to hold inputs and outputs 
 x = torch.randn(N, D_in, device=device)
y = torch.randn(N, D_out, device=device) 
 Use the nn package to define our model as a sequence of layers. nn.Sequential 
 is a Module which contains other Modules, and applies them in sequence to 
 produce its output. Each Linear Module computes output from input using a 
 linear function, and holds internal Tensors for its weight and bias. 
 After constructing the model we use the .to() method to move it to the 
 desired device. 
 model = torch.nn.Sequential(
          torch.nn.Linear(D_in, H),
          torch.nn.ReLU(),
          torch.nn.Linear(H, D_out),
        ).to(device) 
 The nn package also contains definitions of popular loss functions; in this 
 case we will use Mean Squared Error (MSE) as our loss function. Setting 
 reduction='sum' means that we are computing the  sum  of squared errors rather 
 than the mean; this is for consistency with the examples above where we 
 manually compute the loss, but in practice it is more common to use mean 
 squared error as a loss by setting reduction='elementwise_mean'. 
 loss_fn = torch.nn.MSELoss(reduction='sum') 
 learning_rate = 1e-4
for t in range(500):
  # Forward pass: compute predicted y by passing x to the model. Module objects
  # override the  call  operator so you can call them like functions. When
  # doing so you pass a Tensor of input data to the Module and it produces
  # a Tensor of output data.
  y_pred = model(x) 
 # Compute and print loss. We pass Tensors containing the predicted and true
  # values of y, and the loss function returns a Tensor containing the loss.
  loss = loss_fn(y_pred, y)
  print(t, loss.item()) 
 # Zero the gradients before running the backward pass.
  model.zero_grad() 
 # Backward pass: compute gradient of the loss with respect to all the learnable
  # parameters of the model. Internally, the parameters of each Module are stored
  # in Tensors with requires_grad=True, so this call will compute gradients for
  # all learnable parameters in the model.
  loss.backward() 
 # Update the weights using gradient descent. Each parameter is a Tensor, so
  # we can access its data and gradients like we did before.
  with torch.no_grad():
    for param in model.parameters():
      param.data -= learning_rate * param.grad
``` 
 PyTorch: optim 
 Up to this point we have updated the weights of our models by manually mutating
Tensors holding learnable parameters. This is not a huge burden
for simple optimization algorithms like stochastic gradient descent, but in practice
we often train neural networks using more sophisiticated optimizers like AdaGrad,
RMSProp, Adam, etc. 
 The  optim  package in PyTorch abstracts the idea of an optimization algorithm and
provides implementations of commonly used optimization algorithms. 
 In this example we will use the  nn  package to define our model as before, but we
will optimize the model using the Adam algorithm provided by the  optim  package: 
 ```python 
 Code in file nn/two_layer_net_optim.py 
 import torch 
 N is batch size; D_in is input dimension; 
 H is hidden dimension; D_out is output dimension. 
 N, D_in, H, D_out = 64, 1000, 100, 10 
 Create random Tensors to hold inputs and outputs. 
 x = torch.randn(N, D_in)
y = torch.randn(N, D_out) 
 Use the nn package to define our model and loss function. 
 model = torch.nn.Sequential(
          torch.nn.Linear(D_in, H),
          torch.nn.ReLU(),
          torch.nn.Linear(H, D_out),
        )
loss_fn = torch.nn.MSELoss(reduction='sum') 
 Use the optim package to define an Optimizer that will update the weights of 
 the model for us. Here we will use Adam; the optim package contains many other 
 optimization algorithms. The first argument to the Adam constructor tells the 
 optimizer which Tensors it should update. 
 learning_rate = 1e-4
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
for t in range(500):
  # Forward pass: compute predicted y by passing x to the model.
  y_pred = model(x) 
 # Compute and print loss.
  loss = loss_fn(y_pred, y)
  print(t, loss.item()) 
 # Before the backward pass, use the optimizer object to zero all of the
  # gradients for the Tensors it will update (which are the learnable weights
  # of the model)
  optimizer.zero_grad() 
 # Backward pass: compute gradient of the loss with respect to model parameters
  loss.backward() 
 # Calling the step function on an Optimizer makes an update to its parameters
  optimizer.step()
``` 
 PyTorch: Custom nn Modules 
 Sometimes you will want to specify models that are more complex than a sequence of
existing Modules; for these cases you can define your own Modules by subclassing
 nn.Module  and defining a  forward  which receives input Tensors and produces
output Tensors using other modules or other autograd operations on Tensors. 
 In this example we implement our two-layer network as a custom Module subclass: 
 ```python 
 Code in file nn/two_layer_net_module.py 
 import torch 
 class TwoLayerNet(torch.nn.Module):
  def  init (self, D_in, H, D_out):
    """"""
    In the constructor we instantiate two nn.Linear modules and assign them as
    member variables.
    """"""
    super(TwoLayerNet, self). init ()
    self.linear1 = torch.nn.Linear(D_in, H)
    self.linear2 = torch.nn.Linear(H, D_out) 
 def forward(self, x):
    """"""
    In the forward function we accept a Tensor of input data and we must return
    a Tensor of output data. We can use Modules defined in the constructor as
    well as arbitrary (differentiable) operations on Tensors.
    """"""
    h_relu = self.linear1(x).clamp(min=0)
    y_pred = self.linear2(h_relu)
    return y_pred 
 N is batch size; D_in is input dimension; 
 H is hidden dimension; D_out is output dimension. 
 N, D_in, H, D_out = 64, 1000, 100, 10 
 Create random Tensors to hold inputs and outputs 
 x = torch.randn(N, D_in)
y = torch.randn(N, D_out) 
 Construct our model by instantiating the class defined above. 
 model = TwoLayerNet(D_in, H, D_out) 
 Construct our loss function and an Optimizer. The call to model.parameters() 
 in the SGD constructor will contain the learnable parameters of the two 
 nn.Linear modules which are members of the model. 
 loss_fn = torch.nn.MSELoss(reduction='sum')
optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)
for t in range(500):
  # Forward pass: Compute predicted y by passing x to the model
  y_pred = model(x) 
 # Compute and print loss
  loss = loss_fn(y_pred, y)
  print(t, loss.item()) 
 # Zero gradients, perform a backward pass, and update the weights.
  optimizer.zero_grad()
  loss.backward()
  optimizer.step() 
 ``` 
 PyTorch: Control Flow + Weight Sharing 
 As an example of dynamic graphs and weight sharing, we implement a very strange
model: a fully-connected ReLU network that on each forward pass chooses a random
number between 1 and 4 and uses that many hidden layers, reusing the same weights
multiple times to compute the innermost hidden layers. 
 For this model can use normal Python flow control to implement the loop, and we
can implement weight sharing among the innermost layers by simply reusing the
same Module multiple times when defining the forward pass. 
 We can easily implement this model as a Module subclass: 
 ```python 
 Code in file nn/dynamic_net.py 
 import random
import torch 
 class DynamicNet(torch.nn.Module):
  def  init (self, D_in, H, D_out):
    """"""
    In the constructor we construct three nn.Linear instances that we will use
    in the forward pass.
    """"""
    super(DynamicNet, self). init ()
    self.input_linear = torch.nn.Linear(D_in, H)
    self.middle_linear = torch.nn.Linear(H, H)
    self.output_linear = torch.nn.Linear(H, D_out) 
 def forward(self, x):
    """"""
    For the forward pass of the model, we randomly choose either 0, 1, 2, or 3
    and reuse the middle_linear Module that many times to compute hidden layer
    representations. 
 Since each forward pass builds a dynamic computation graph, we can use normal
Python control-flow operators like loops or conditional statements when
defining the forward pass of the model.

Here we also see that it is perfectly safe to reuse the same Module many
times when defining a computational graph. This is a big improvement from Lua
Torch, where each Module could be used only once.
""""""
h_relu = self.input_linear(x).clamp(min=0)
for _ in range(random.randint(0, 3)):
  h_relu = self.middle_linear(h_relu).clamp(min=0)
y_pred = self.output_linear(h_relu)
return y_pred
 
 N is batch size; D_in is input dimension; 
 H is hidden dimension; D_out is output dimension. 
 N, D_in, H, D_out = 64, 1000, 100, 10 
 Create random Tensors to hold inputs and outputs. 
 x = torch.randn(N, D_in)
y = torch.randn(N, D_out) 
 Construct our model by instantiating the class defined above 
 model = DynamicNet(D_in, H, D_out) 
 Construct our loss function and an Optimizer. Training this strange model with 
 vanilla stochastic gradient descent is tough, so we use momentum 
 criterion = torch.nn.MSELoss(reduction='sum')
optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)
for t in range(500):
  # Forward pass: Compute predicted y by passing x to the model
  y_pred = model(x) 
 # Compute and print loss
  loss = criterion(y_pred, y)
  print(t, loss.item()) 
 # Zero gradients, perform a backward pass, and update the weights.
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()
``` Simple benchmarking script for torch.multinomial with varying batch size, number of categories, and number of samples. colab-testing 
 Playing around with Google Colab. This code is in no way polished or intended for public consumption at this time,
but making the repository public is the easiest way to clone it from a Colab instance."
eladhoffer,"eladtools torch package 
 some used tools for torch  
 You can install the package by opening a terminal, changing directory into the folder and typing: 
 luarocks make LMDB for Torch 
 Uses ffi to wrap LMDB (http://symas.com/mdb/) functions, and Torch to serialize/deserialize objects (can be replaced by changing lmdb.[de]serialize). 
 Available functions: 
 lmdb.env() 
 ```lua
open{
    Path = string                       -- Path of lmdb database
    [MapSize = number]                  -- Size of map  [default = 1099511627776]
    [NOSUBDIR = boolean]                --   [default = false]
    [NOMETASYNC = boolean]              --   [default = false]
    [RDONLY = boolean]                  --   [default = false]
    [WRITEMAP = boolean]                --   [default = false]
    [MAPASYNC = boolean]                --   [default = false]
    [NOSYNC = boolean]                  --   [default = false]
    [NOTLS = boolean]                   --   [default = false]
    [NOLOCK = boolean]                  --   [default = false]
    [Mode = number]                     --   [default = 664]
    [MaxDBs = number]                   --   [default = 1]
    [MaxReaders = number]               --   [default = 3]
    [Name = string]                     --   [default = Data]
} 
 txn(rdonly, parent_txn)
reader_check()
stat()
``` 
 lmdb.txn(env_obj, rdonly, parent_txn) 
 lua
dbi_open(name, flags)
commit()
abort()
reset()
renew()
put(key, data, flag)
get(key)
cursor() 
 lmdb.cursor() 
 lua
get(op)
put(key, data, flag)
del(flag)
close() 
 Usage Example 
 ```lua
require 'lmdb' 
 local db= lmdb.env{
    Path = './testDB',
    Name = 'testDB'
} 
 db:open()
print(db:stat()) -- Current status
local txn = db:txn() --Write transaction
local cursor = txn:cursor()
local x=torch.rand(10,3,256,256):byte() 
 -------Write-------
for i=1,10 do
    txn:put(i,x[i])
end
txn:commit() 
 local reader = db:txn(true) --Read-only transaction
local y = torch.Tensor(10,3,256,256) 
 -------Read-------
for i=1,10 do
    y[i] = reader:get(i)
end 
 reader:abort() 
 db:close()
``` Deep Learning on ImageNet using Torch 
 This is a complete training example for Deep Convolutional Networks on the ILSVRC classification task. 
 Data is preprocessed and cached as a LMDB data-base for fast reading. A separate thread buffers images from the LMDB record in the background. 
 Multiple GPUs are also supported by using nn.DataParallelTable (https://github.com/torch/cunn/blob/master/docs/cunnmodules.md). 
 This code allows training at 4ms/sample with the AlexNet model and 2ms for testing on a single GPU (using Titan Z with 1 active gpu) 
 Dependencies 
 
 Torch (http://torch.ch) 
 ""eladtools"" (https://github.com/eladhoffer/eladtools) for optimizer. 
 ""lmdb.torch"" (http://github.com/eladhoffer/lmdb.torch) for LMDB usage. 
 ""DataProvider.torch"" (https://github.com/eladhoffer/DataProvider.torch) for DataProvider class. 
 ""cudnn.torch"" (https://github.com/soumith/cudnn.torch) for faster training. Can be avoided by changing ""cudnn"" to ""nn"" in models. 
 
 To install all dependencies (assuming torch is installed) use:
 bash
luarocks install https://raw.githubusercontent.com/eladhoffer/eladtools/master/eladtools-scm-1.rockspec
luarocks install https://raw.githubusercontent.com/eladhoffer/lmdb.torch/master/lmdb.torch-scm-1.rockspec
luarocks install https://raw.githubusercontent.com/eladhoffer/DataProvider.torch/master/dataprovider-scm-1.rockspec 
 Data 
 
 To get the ILSVRC data, you should register on their site for access: http://www.image-net.org/ 
 Extract all archives and configure the data location and save dir in  Config.lua . You can also change the saved image size by editing the default value  ImageMinSide=256 . 
 LMDB records for fast read access are created by running  CreateLMDBs.lua .
It defaults to saving the compressed jpgs (about ~24GB for training data, ~1GB for validation data when smallest image dimension is 256). 
 To validate the LMDB configuration and test its loading speed, you can run  TestLMDBs.lua . 
 All data related functions used for training are available at  Data.lua . 
 
 Model configuration 
 Network model is defined by writing a  .lua file in  Models  folder, and selecting it using the  network  flag.
The model file must return a trainable network. It can also specify additional training options such optimization regime, input size modifications. 
 e.g for a model file:
 lua
local model = nn.Sequential():add(...)
return  --optional: you can also simply return model
{
  model = model,
  regime = {
    epoch        = {1,    19,   30,   44,   53  },
    learningRate = {1e-2, 5e-3, 1e-3, 5e-4, 1e-4},
    weightDecay  = {5e-4, 5e-4, 0,    0,    0   }
  }
} 
Currently available in  Models  folder are:  AlexNet ,  MattNet ,  OverFeat ,  GoogLeNet ,  CaffeRef ,  NiN . Some are available with a batch normalized version (denoted with  _BN ) 
 Training 
 You can start training using  Main.lua  by typing:
 lua
th Main.lua -network AlexNet -LR 0.01 
or if you have 2 gpus availiable,
 lua
th Main.lua -network AlexNet -LR 0.01 -nGPU 2 -batchSize 256 
A more elaborate example continuing a pretrained network and saving intermediate results
 lua
th Main.lua -network GoogLeNet_BN -batchSize 64 -nGPU 2 -save GoogLeNet_BN -bufferSize 9600 -LR 0.01 -checkpoint 320000 -weightDecay 1e-4 -load ./pretrainedNet.t7 
Buffer size should be adjusted to suit the used hardware and configuration. Default value is 5120 (40 batches of 128) which works well when using a non SSD drive and 16GB ram. Bigger buffer size allows better sample shuffling. 
 Output 
 Training output will be saved to folder defined with  save  flag. 
 The complete netowork will be saved on each epoch as  Net_<#epoch>.t7  along with
* A complete log  Log.txt 
* Error rate summary  ErrorRate.log  and accompanying                ErrorRate.log.eps  graph 
 Additional flags 
 |Flag             | Default Value        |Description
|:----------------|:--------------------:|:----------------------------------------------
|modelsFolder     |./Models/             | Models Folder
|network          |AlexNet               | Model file - must return valid network.
|LR               |0.01                  | learning rate
|LRDecay          |0                     | learning rate decay (in # samples)
|weightDecay      |5e-4                  | L2 penalty on the weights
|momentum         |0.9                   | momentum
|batchSize        |128,                  | batch size
|optimization     |'sgd'                 | optimization method
|seed             |123                   | torch manual random number generator seed
|epoch            |-1                    | number of epochs to train, -1 for unbounded
|testonly         |false                 | Just test loaded net on validation set
|threads          |8                     | number of threads
|type             |'cuda'                | float or cuda
|bufferSize       |5120                  | buffer size
|devid            |1                     | device ID (if using CUDA)
|nGPU             |1                     | num of gpu devices used
|constBatchSize   |false                 | do not allow varying batch sizes - e.g for ccn2 kernel
|load             |''                    | load existing net weights
|save             |time-identifier       | save directory
|optState         |false                 | Save optimization state every epoch
|checkpoint       |0                     | Save a weight check point every n samples. 0 for off
|augment          |1                     | data augmentation level - {1 - simple mirror and crops, 2 +scales, 3 +rotations}
|estMeanStd       |preDef                | estimate mean and std. Options: {preDef, simple, channel, image}
|shuffle          |true                  | shuffle training samples Deep Networks on classification tasks using Torch 
 This is a complete training example for {Cifar10/100, STL10, SVHN, MNIST} tasks 
 Data 
 You can get the needed data using @soumith's repo: https://github.com/soumith/cifar.torch.git 
 Dependencies 
 
 Torch (http://torch.ch) 
 ""DataProvider.torch"" (https://github.com/eladhoffer/DataProvider.torch) for DataProvider class. 
 ""cudnn.torch"" (https://github.com/soumith/cudnn.torch) for faster training. Can be avoided by changing ""cudnn"" to ""nn"" in models. 
 
 To install all dependencies (assuming torch is installed) use:
 bash
luarocks install https://raw.githubusercontent.com/eladhoffer/eladtools/master/eladtools-scm-1.rockspec
luarocks install https://raw.githubusercontent.com/eladhoffer/DataProvider.torch/master/dataprovider-scm-1.rockspec 
 Training 
 You can start training using:
 lua
th Main.lua -dataset Cifar10 -network Cifar10_Model -LR 0.1 
or,
 lua
th Main.lua -dataset Cifar100 -network Cifar100_Model -LR 0.1 
 Additional flags 
 |Flag             | Default Value        |Description
|:----------------|:--------------------:|:----------------------------------------------
|modelsFolder     |  ./Models/           | Models Folder
|network          |  Model.lua           | Model file - must return valid network.
|LR               |  0.1                 | learning rate
|LRDecay          |  0                   | learning rate decay (in # samples
|weightDecay      |  1e-4                | L2 penalty on the weights
|momentum         |  0.9                 | momentum
|batchSize        |  128                 | batch size
|optimization     |  sgd                 | optimization method
|epoch            |  -1                  | number of epochs to train (-1 for unbounded)
|threads          |  8                   | number of threads
|type             |  cuda                | float or cuda
|devid            |  1                   | device ID (if using CUDA)
|load             |  none                |  load existing net weights
|save             |  time-identifier     | save directory
|dataset          |  Cifar10             | Dataset - Cifar10, Cifar100, STL10, SVHN, MNIST
|whiten           |  false               | whiten data
|augment          |  false               | Augment training data
|preProcDir       |  ./PreProcData/      | Data for pre-processing (means,Pinv,P) Downloads STL10 dataset directly from http://cs.stanford.edu/~acoates/stl10/ and converts them to Torch tables.
Based on https://github.com/soumith/cifar.torch.git 
 STL-10 format 
 Writes 3 files: stl10-train.t7, stl10-test.t7, stl10-unlabeled.t7
Each of them is a table of the form:
 lua
th> stl10 = torch.load('stl10-train.t7')
th> print(stl10)
{
        data : ByteTensor - size: 5000x3x96x96
        label : ByteTensor - size: 5000
} 
The Unlabeled file has only the data field Deep Metric Learning Using Triplet Network 
 This code replicates the results from the paper “Deep metric learning using Triplet network” (http://arxiv.org/abs/1412.6622). 
 It can train a TripletNet on any of the {Cifar10/100, STL10, SVHN, MNIST} datasets. 
 Data 
 You can get the needed data using the following repos:
* CIFAR10/100: https://github.com/soumith/cifar.torch.git
* STL10: https://github.com/eladhoffer/stl10.torch
* SVHN: https://github.com/torch/tutorials/blob/master/A_datasets/svhn.lua
* MNIST: https://github.com/andresy/mnist 
 Dependencies 
 
 Torch (http://torch.ch) 
 ""eladtools"" (https://github.com/eladhoffer/eladtools) for optimizer. 
 ""nngraph"" (https://github.com/torch/nngraph) for TripletNet configuration. 
 ""cudnn.torch"" (https://github.com/soumith/cudnn.torch) for faster training. Can be avoided by changing ""cudnn"" to ""nn"" in models. 
 
 Models 
 Available models are at the “Models” directory. The basic Model.lua was used in the paper, while NiN based models achieve slightly better
results. 
 Training 
 You can start training using:
 lua
th Main.lua -dataset Cifar10 -LR 0.1 -save new_exp_dir 
 Additional flags 
 |Flag             | Default Value        |Description
|:----------------|:--------------------:|:----------------------------------------------
|modelsFolder     |  ./Models/           | Models Folder
|network          |  Model.lua           | Model file - must return valid network.
|LR               |  0.1                 | learning rate
|LRDecay          |  0                   | learning rate decay (in # samples
|weightDecay      |  1e-4                | L2 penalty on the weights
|momentum         |  0.9                 | momentum
|batchSize        |  128                 | batch size
|optimization     |  sgd                 | optimization method
|epoch            |  -1                  | number of epochs to train (-1 for unbounded)
|threads          |  8                   | number of threads
|type             |  cuda                | float or cuda
|devid            |  1                   | device ID (if using CUDA)
|load             |  none                | load existing net weights
|save             |  time-identifier     | save directory
|dataset          |  Cifar10             | Dataset - Cifar10, Cifar100, STL10, SVHN, MNIST
|normalize        |  1                   | 1 - normalize using only 1 mean and std values
|whiten           |  false               | whiten data
|augment          |  false               | Augment training data
|preProcDir       |  ./PreProcData/      | Data for pre-processing (means,Pinv,P) Trained GoogLeNet v2 model for  Torch  trained on the ILSVRC2012 classification task.
Accuracy is measured on validation set using a single center crop. 
 |Name             | Origin                          |Top1 Accuracy |Top5 Accuracy |Training Method|Additional Info
|:----------------|:--------------------------------|:-------------|:-------------|:--------------|:---------------
| GoogLeNet v2      | Batch Normalization Paper   | 0.69          |0.9 |        ImageNet-Training | Info 
 *Notice that numbers may be lower than published results. DeepLearningCourse 
 
Deep learning mini-course given in Technion Convolutional networks using Torch 
 This is a complete training example for Deep Convolutional Networks on various datasets (ImageNet, Cifar10, Cifar100, STL10, SVHN, MNIST). 
 It uses TorchNet ( https://github.com/torchnet/torchnet ) for fast data loading and measurements. 
 It aims to replace both  https://github.com/eladhoffer/ConvNet-torch  and  https://github.com/eladhoffer/ImageNet-Training 
 Multiple GPUs are also supported by using nn.DataParallelTable ( https://github.com/torch/cunn/blob/master/docs/cunnmodules.md ). 
 Dependencies 
 
 Torch ( http://torch.ch ) 
 torchnet ( https://github.com/torchnet/torchnet ) 
 cudnn.torch ( https://github.com/soumith/cudnn.torch ) for faster training (optional) 
 
 Data 
 
 To get Cifar data, use @soumith's repo: https://github.com/soumith/cifar.torch.git 
 To get the ILSVRC data, you should register on their site for access:  http://www.image-net.org/ 
 All data related functions used for training are available at  data.lua . 
 
 Model configuration 
 Network model is defined by writing a 
 .lua file in  models  folder, and selecting it using the  model  flag.
The model file must return a trainable network. It can also specify additional training options such optimization regime, input size modifications. 
 e.g for a model file: 
 lua
local model = nn.Sequential():add(...)
  model.inputSize = 224
  model.reshapeSize = 256
  model.regime = {
    epoch        = {1,    19,   30,   44,   53  },
    learningRate = {1e-2, 5e-3, 1e-3, 5e-4, 1e-4},
    weightDecay  = {5e-4, 5e-4, 0,    0,    0   }
  }
return model 
 Training 
 You can start training using  main.lua  by typing: 
 lua
th main.lua -model AlexNet -LR 0.01 
 or if you have 2 gpus availiable, 
 lua
th main.lua -model AlexNet -LR 0.01 -nGPU 2 -batchSize 256 
 A more elaborate example continuing a pretrained network and saving intermediate results 
 lua
th main.lua -model GoogLeNet_BN -batchSize 64 -nGPU 2 -save GoogLeNet_BN -bufferSize 9600 -LR 0.01 -checkpoint 320000 -weightDecay 1e-4 -load ./pretrainedNet.t7 
 Output 
 Training output will be saved to folder defined with  save  flag. 
 Additional flags 
 Flag           |  Default Value  | Description
:------------- | :-------------: | :-------------------------------------------------------------------------------
modelsFolder   |    ./models/    | Models Folder
network        |     AlexNet     | Model file - must return valid network.
LR             |      0.01       | learning rate
LRDecay        |        0        | learning rate decay (in # samples)
weightDecay    |      5e-4       | L2 penalty on the weights
momentum       |       0.9       | momentum
batchSize      |      128,       | batch size
optimization   |      'sgd'      | optimization method
seed           |       123       | torch manual random number generator seed
epoch          |       -1        | number of epochs to train, -1 for unbounded
threads        |        8        | number of threads
type           |     'cuda'      | float or cuda
devid          |        1        | device ID (if using CUDA)
nGPU           |        1        | num of gpu devices used
load           |       ''        | load existing net weights
save           | time-identifier | save directory
evalN'         |     100000      | evaluate every N samples
topK'          |        5        | measure top k error SemiSupContrast 
 Semi-supervised deep learning by metric embedding https://arxiv.org/abs/1611.01449 Convolutional networks using PyTorch 
 This is a complete training example for Deep Convolutional Networks on various datasets (ImageNet, Cifar10, Cifar100, MNIST). 
 Available models include:
 'alexnet', 'amoebanet', 'darts', 'densenet', 'googlenet', 'inception_resnet_v2', 'inception_v2', 'mnist', 'mobilenet', 'mobilenet_v2', 'nasnet', 'resnet', 'resnet_se', 'resnet_zi', 'resnet_zi_se', 'resnext', 'resnext_se' 
 It is based off  imagenet example in pytorch  with helpful additions such as:
  - Training on several datasets other than imagenet
  - Complete logging of trained experiment
  - Graph visualization of the training/validation loss and accuracy
  - Definition of preprocessing and optimization regime for each model
  - Distributed training 
 To clone:
  git clone --recursive https://github.com/eladhoffer/convNet.pytorch 
 example for efficient multi-gpu training of resnet50 (4 gpus, label-smoothing):
  python -m torch.distributed.launch --nproc_per_node=4  main.py --model resnet --model-config ""{'depth': 50}"" --eval-batch-size 512 --save resnet50_ls --label-smoothing 0.1 
 This code can be used to implement several recent papers:
  -  Hoffer et al. (2018): Fix your classifier: the marginal value of training the last weight layer 
  -  Hoffer et al. (2018): Norm matters: efficient and accurate normalization schemes in deep networks 
   For example, training ResNet18 with L1 norm (instead of batch-norm):
  ```
  python main.py --model resnet --model-config ""{'depth': 18, 'bn_norm': 'L1'}"" --save resnet18_l1 -b 128
  ```
 
 
 
 Banner et al. (2018): Scalable Methods for 8-bit Training of Neural Networks 
 For example, training ResNet18 with 8-bit quantization:
 python main.py --model resnet --model-config ""{'depth': 18, 'quantize':True}"" --save resnet18_8bit -b 64 
  -  Hoffer et al. (2020): Augment Your Batch: Improving Generalization Through Instance Repetition 
 For example, training the resnet44 + cutout example in paper:
 python main.py --dataset cifar10 --model resnet --model-config ""{'depth': 44}""  --duplicates 40 --cutout -b 64 --epochs 100 --save resnet44_cutout_m-40 
 
 
 Hoffer et al. (2019): Mix & Match: training convnets with mixed image sizes for improved accuracy, speed and scale resiliency 
 For example, training the resnet44 with mixed sizes example in paper:
 python main.py --model resnet --dataset cifar10 --save cifar10_mixsize_d -b 64 --model-config ""{'regime': 'sampled_D+'}"" --epochs 200 
Then, calibrate for specific size and evaluate using
 python evaluate.py ./results/cifar10_mixsize_d/checkpoint.pth.tar --dataset cifar10 -b 64 --input-size 32 --calibrate-bn 
Pretrained models (ResNet50, ImageNet) are also available  here 
 
 
 Dependencies 
 
 pytorch 
 torchvision  to load the datasets, perform image transforms 
 pandas  for logging to csv 
 bokeh  for training visualization 
 
 Data 
 
 Configure your dataset path with  datasets-dir  argument 
 To get the ILSVRC data, you should register on their site for access:  http://www.image-net.org/ 
 
 Model configuration 
 Network model is defined by writing a  .py file in  models  folder, and selecting it using the  model  flag. Model function must be registered in  models/__init__.py 
The model function must return a trainable network. It can also specify additional training options such optimization regime (either a dictionary or a function), and input transform modifications. 
 e.g for a model definition: 
 ```python
class Model(nn.Module): 
 def __init__(self, num_classes=1000):
    super(Model, self).__init__()
    self.model = nn.Sequential(...)

    self.regime = [
        {'epoch': 0, 'optimizer': 'SGD', 'lr': 1e-2,
            'weight_decay': 5e-4, 'momentum': 0.9},
        {'epoch': 15, 'lr': 1e-3, 'weight_decay': 0}
    ]

    self.data_regime = [
        {'epoch': 0, 'input_size': 128, 'batch_size': 256},
        {'epoch': 15, 'input_size': 224, 'batch_size': 64}
    ]
def forward(self, inputs):
    return self.model(inputs)
 
 def model(**kwargs):
        return Model()
``` 
 Citation 
 If you use the code in your paper, consider citing one of the implemented works.
 @inproceedings{hoffer2018fix,
  title={Fix your classifier: the marginal value of training the last weight layer},
  author={Elad Hoffer and Itay Hubara and Daniel Soudry},
  booktitle={International Conference on Learning Representations},
  year={2018},
  url={https://openreview.net/forum?id=S1Dh8Tg0-},
} 
 @inproceedings{hoffer2018norm,
  title={Norm matters: efficient and accurate normalization schemes in deep networks},
  author={Hoffer, Elad and Banner, Ron and Golan, Itay and Soudry, Daniel},
  booktitle={Advances in Neural Information Processing Systems},
  year={2018}
} 
 @inproceedings{banner2018scalable,
  title={Scalable Methods for 8-bit Training of Neural Networks},
  author={Banner, Ron and Hubara, Itay and Hoffer, Elad and Soudry, Daniel},
  booktitle={Advances in Neural Information Processing Systems},
  year={2018}
} 
 @inproceedings{Hoffer_2020_CVPR,
  author = {Hoffer, Elad and Ben-Nun, Tal and Hubara, Itay and Giladi, Niv and Hoefler, Torsten and Soudry, Daniel},
  title = {Augment Your Batch: Improving Generalization Through Instance Repetition},
  booktitle = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {June},
  year = {2020}
} 
 @article{hoffer2019mix,
  title={Mix \& Match: training convnets with mixed image sizes for improved accuracy, speed and scale resiliency},
  author={Hoffer, Elad and Weinstein, Berry and Hubara, Itay and Ben-Nun, Tal and Hoefler, Torsten and Soudry, Daniel},
  journal={arXiv preprint arXiv:1908.08986},
  year={2019}
} captionGen 
 Generate captions for an image using PyTorch Train longer, generalize better - Big batch training 
 This is a code repository used to generate the results appearing in  ""Train longer, generalize better: closing the generalization gap in large batch training of neural networks""  By Elad Hoffer, Itay Hubara and Daniel Soudry. 
 It is based off  convNet.pytorch  with some helpful options such as:
  - Training on several datasets
  - Complete logging of trained experiment
  - Graph visualization of the training/validation loss and accuracy
  - Definition of preprocessing and optimization regime for each model 
 Dependencies 
 
 pytorch 
 torchvision  to load the datasets, perform image transforms 
 pandas  for logging to csv 
 bokeh  for training visualization 
 
 Data 
 
 Configure your dataset path at  data.py . 
 To get the ILSVRC data, you should register on their site for access:  http://www.image-net.org/ 
 
 Experiment examples 
 bash
python main_normal.py --dataset cifar10 --model resnet --save cifar10_resnet44_bs2048_lr_fix --epochs 100 --b 2048 --lr_bb_fix;
python main_normal.py --dataset cifar10 --model resnet --save cifar10_resnet44_bs2048_regime_adaptation --epochs 100 --b 2048 --lr_bb_fix --regime_bb_fix;
python main_gbn.py --dataset cifar10 --model resnet --save cifar10_resnet44_bs2048_ghost_bn256 --epochs 100 --b 2048 --lr_bb_fix --mini-batch-size 256;
python main_normal.py --dataset cifar100 --model resnet --save cifar100_wresnet16_4_bs1024_regime_adaptation --epochs 100 --b 1024 --lr_bb_fix --regime_bb_fix;
python main_gbn.py --model mnist_f1 --dataset mnist --save mnist_baseline_bs4096_gbn --epochs 50 --b 4096 --lr_bb_fix --no-regime_bb_fix --mini-batch-size 128; 
- See  run_experiments.sh  for more examples 
 Model configuration 
 Network model is defined by writing a  .py file in  models  folder, and selecting it using the  model  flag. Model function must be registered in  models/__init__.py 
The model function must return a trainable network. It can also specify additional training options such optimization regime (either a dictionary or a function), and input transform modifications. 
 e.g for a model definition: 
 ```python
class Model(nn.Module): 
 def __init__(self, num_classes=1000):
    super(Model, self).__init__()
    self.model = nn.Sequential(...)

    self.regime = {
        0: {'optimizer': 'SGD', 'lr': 1e-2,
            'weight_decay': 5e-4, 'momentum': 0.9},
        15: {'lr': 1e-3, 'weight_decay': 0}
    }

    self.input_transform = {
        'train': transforms.Compose([...]),
        'eval': transforms.Compose([...])
    }
def forward(self, inputs):
    return self.model(inputs)
 
 def model(**kwargs):
        return Model()
``` Seq2Seq in PyTorch 
 This is a complete suite for training sequence-to-sequence models in  PyTorch . It consists of several models and code to both train and infer using them. 
 Using this code you can train:
* Neural-machine-translation (NMT) models
* Language models
* Image to caption generation
* Skip-thought sentence representations
* And more... 
 ## Installation
  git clone --recursive https://github.com/eladhoffer/seq2seq.pytorch
 cd seq2seq.pytorch; python setup.py develop 
 Models 
 Models currently available:
* Simple Seq2Seq recurrent model
* Recurrent Seq2Seq with attentional decoder
*  Google neural machine translation  (GNMT) recurrent model
* Transformer - attention-only model from  ""Attention Is All You Need"" 
 Datasets 
 Datasets currently available: 
 
 WMT16 
 WMT17 
 OpenSubtitles 2016 
 COCO image captions 
 Conceptual captions 
 
 All datasets can be tokenized using 3 available segmentation methods: 
 
 Character based segmentation 
 Word based segmentation 
 Byte-pair-encoding (BPE) as suggested by  bpe  with selectable number of tokens.   
 
 After choosing a tokenization method, a vocabulary will be generated and saved for future inference. 
 Training methods 
 The models can be trained using several methods: 
 
 Basic Seq2Seq - given encoded sequence, generate (decode) output sequence. Training is done with teacher-forcing. 
 Multi Seq2Seq - where several tasks (such as multiple languages) are trained simultaneously by using the data sequences as both input to the encoder and output for decoder. 
 Image2Seq - used to train image to caption generators. 
 
 Usage 
 Example training scripts are available in  scripts  folder. Inference examples are available in  examples  folder. 
 
 example for training a  transformer 
 on WMT16 according to original paper regime:
```
DATASET=${1:-""WMT16_de_en""}
DATASET_DIR=${2:-""./data/wmt16_de_en""}
OUTPUT_DIR=${3:-""./results""} 
 
 WARMUP=""4000""
LR0=""512**(-0.5)"" 
 python main.py \
  --save transformer \
  --dataset ${DATASET} \
  --dataset-dir ${DATASET_DIR} \
  --results-dir ${OUTPUT_DIR} \
  --model Transformer \
  --model-config ""{'num_layers': 6, 'hidden_size': 512, 'num_heads': 8, 'inner_linear': 2048}"" \
  --data-config ""{'moses_pretok': True, 'tokenization':'bpe', 'num_symbols':32000, 'shared_vocab':True}"" \
  --b 128 \
  --max-length 100 \
  --device-ids 0 \
  --label-smoothing 0.1 \
  --trainer Seq2SeqTrainer \
  --optimization-config ""[{'step_lambda':
                          \""lambda t: { \
                              'optimizer': 'Adam', \
                              'lr': ${LR0} * min(t   -0.5, t * ${WARMUP}   -1.5), \
                              'betas': (0.9, 0.98), 'eps':1e-9}\""
                          }]""
``` 
 
 example for training attentional LSTM based model with 3 layers in both encoder and decoder:
 python main.py \
  --save de_en_wmt17 \
  --dataset ${DATASET} \
  --dataset-dir ${DATASET_DIR} \
  --results-dir ${OUTPUT_DIR} \
  --model RecurrentAttentionSeq2Seq \
  --model-config ""{'hidden_size': 512, 'dropout': 0.2, \
                   'tie_embedding': True, 'transfer_hidden': False, \
                   'encoder': {'num_layers': 3, 'bidirectional': True, 'num_bidirectional': 1, 'context_transform': 512}, \
                   'decoder': {'num_layers': 3, 'concat_attention': True,\
                               'attention': {'mode': 'dot_prod', 'dropout': 0, 'output_transform': True, 'output_nonlinearity': 'relu'}}}"" \
  --data-config ""{'moses_pretok': True, 'tokenization':'bpe', 'num_symbols':32000, 'shared_vocab':True}"" \
  --b 128 \
  --max-length 80 \
  --device-ids 0 \
  --trainer Seq2SeqTrainer \
  --optimization-config ""[{'epoch': 0, 'optimizer': 'Adam', 'lr': 1e-3},
                          {'epoch': 6, 'lr': 5e-4},
                          {'epoch': 8, 'lr':1e-4},
                          {'epoch': 10, 'lr': 5e-5},
                          {'epoch': 12, 'lr': 1e-5}]"" \ 
 Norm Matters 
 See https://github.com/eladhoffer/convNet.pytorch for updated version of this code 
 This code was was used to implement  Norm matters: efficient and accurate normalization schemes in deep networks - Hoffer, Banner, Golan, Soudry (2018):  
 @inproceedings{hoffer2018norm,
  title={Norm matters: efficient and accurate normalization schemes in deep networks},
  author={Hoffer, Elad and Banner, Ron and Golan, Itay and Soudry, Daniel},
  booktitle={Advances in Neural Information Processing Systems},
  year={2018}
} 
 It is based off  imagenet example in pytorch  with some helpful additions such as:
  - Training on several datasets other than imagenet
  - Complete logging of trained experiment
  - Graph visualization of the training/validation loss and accuracy
  - Definition of preprocessing and optimization regime for each model 
 Dependencies 
 
 pytorch 
 torchvision  to load the datasets, perform image transforms 
 pandas  for logging to csv 
 bokeh  for training visualization 
 
 Data 
 
 Configure your dataset path at  data.py . 
 To get the ILSVRC data, you should register on their site for access:  http://www.image-net.org/ 
 
 Model configuration 
 Network model is defined by writing a  .py file in  models  folder, and selecting it using the  model  flag. Model function must be registered in  models/__init__.py 
The model function must return a trainable network. It can also specify additional training options such optimization regime (either a dictionary or a function), and input transform modifications. 
 e.g for a model definition: 
 ```python
class Model(nn.Module): 
 def __init__(self, num_classes=1000):
    super(Model, self).__init__()
    self.model = nn.Sequential(...)

    self.regime = [
        {'epoch': 0, 'optimizer': 'SGD', 'lr': 1e-2,
            'weight_decay': 5e-4, 'momentum': 0.9},
        {'epoch': 15, 'lr': 1e-3, 'weight_decay': 0}
    ]

    self.input_transform = {
        'train': transforms.Compose([...]),
        'eval': transforms.Compose([...])
    }
def forward(self, inputs):
    return self.model(inputs)
 
 def model(**kwargs):
        return Model()
``` Fix Your Classifier 
 PyTorch implementation of the fixed classifier described in  ""Fix your classifier: the marginal value of training the last weight layer""  (ICLR-2018) by Elad Hoffer, Itay Hubara and Daniel Soudry. 
 This repository holds both random orthogonal classifier, and Hadamard classifier. They can be used to replace any  nn.Linear  module normally used for classification. 
 @inproceedings{
hoffer2018fix,
title={Fix your classifier: the marginal value of training the last weight layer},
author={Elad Hoffer and Itay Hubara and Daniel Soudry},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=S1Dh8Tg0-},
} Quantized Convolutional networks using PyTorch 
 See https://github.com/eladhoffer/convNet.pytorch for updated version of this code 
 Code to replicate results in  Scalable Methods for 8-bit Training of Neural Networks 
 e.g: running an 8-bit quantized resnet18 from the paper on ImageNet 
 python main.py --model resnet_quantized --model_config ""{'depth': 18}"" --save quantized_resnet18 --dataset imagenet --b 128 
 Dependencies 
 
 pytorch 
 torchvision  to load the datasets, perform image transforms 
 pandas  for logging to csv 
 bokeh  for training visualization 
 
 Data 
 
 Configure your dataset path at  data.py . 
 To get the ILSVRC data, you should register on their site for access:  http://www.image-net.org/ 
 
 Model configuration 
 Network model is defined by writing a  .py file in  models  folder, and selecting it using the  model  flag. Model function must be registered in  models/__init__.py 
The model function must return a trainable network. It can also specify additional training options such optimization regime (either a dictionary or a function), and input transform modifications. 
 e.g for a model definition: 
 ```python
class Model(nn.Module): 
 def __init__(self, num_classes=1000):
    super(Model, self).__init__()
    self.model = nn.Sequential(...)

    self.regime = [
        {'epoch': 0, 'optimizer': 'SGD', 'lr': 1e-2,
            'weight_decay': 5e-4, 'momentum': 0.9},
        {'epoch': 15, 'lr': 1e-3, 'weight_decay': 0}
    ]

    self.input_transform = {
        'train': transforms.Compose([...]),
        'eval': transforms.Compose([...])
    }
def forward(self, inputs):
    return self.model(inputs)
 
 def model(**kwargs):
        return Model()
``` colab-notebooks 



# Self/Semi/Supervised Learning in Pytorch (Lightning)

 
 
 
 
  [![Paper](http://img.shields.io/badge/paper-arxiv.1001.2234-B31B1B.svg)](https://www.nature.com/articles/nature14539)
[![Conference](http://img.shields.io/badge/AnyConference-year-4b44ce.svg)](https://papers.nips.cc/paper/2020)  
 
 Description 
 Workbench to train self/semi and supervised deep learning models in Pytorch Lightning using Hydra configs.  
Based on  lightning-hydra-template . 
 How to run 
 Install dependencies
```yaml 
 clone project 
 git clone https://github.com/eladhoffer/3SL
cd 3SL 
 [OPTIONAL] create conda environment 
 conda env create -f conda_env_gpu.yaml -n myenv
conda activate myenv 
 install requirements 
 pip install -r requirements.txt
``` 
 Train model with default configuration (supervised resnet on cifar10)
```yaml 
 default 
 python run.py 
 train on CPU 
 python run.py trainer.gpus=0 
 train on GPU 
 python run.py trainer.gpus=1
``` 
 Train model with chosen experiment configuration from  configs/experiment/ 
 bash
python run.py experiment=experiment_name 
 You can override any parameter from command line like this
 bash
python run.py task=fixmatch_cifar data=fixmatch_cifar10 
"
ankurhanda,"VLP16DataReader 
 Data Grabber for VLP16 and displays the point cloud in OpenGL window 
 Needs PCL tinyobjloader{#mycopy} 
 Original source code here: http://syoyo.github.io/tinyobjloader/ 
 Tiny but poweful single file wavefront obj loader written in C++. No dependency except for C++ STL. It can parse 10M over polygons with moderate memory and time. 
 Good for embedding .obj loader to your (global illumination) renderer ;-) 
 Usage 
 std::string inputfile = ""cornell_box.obj"";
std::vector<tinyobj::shape_t> shapes;
std::vector<tinyobj::material_t> materials;

std::string err = tinyobj::LoadObj(shapes, materials, inputfile.c_str());

if (!err.empty()) {
  std::cerr << err << std::endl;
  exit(1);
}

std::cout << ""# of shapes    : "" << shapes.size() << std::endl;
std::cout << ""# of materials : "" << materials.size() << std::endl;

for (size_t i = 0; i < shapes.size(); i++) {
  printf(""shape[%ld].name = %s\n"", i, shapes[i].name.c_str());
  printf(""Size of shape[%ld].indices: %ld\n"", i, shapes[i].mesh.indices.size());
  printf(""Size of shape[%ld].material_ids: %ld\n"", i, shapes[i].mesh.material_ids.size());
  assert((shapes[i].mesh.indices.size() % 3) == 0);
  for (size_t f = 0; f < shapes[i].mesh.indices.size() / 3; f++) {
    printf(""  idx[%ld] = %d, %d, %d. mat_id = %d\n"", f, shapes[i].mesh.indices[3*f+0], shapes[i].mesh.indices[3*f+1], shapes[i].mesh.indices[3*f+2], shapes[i].mesh.material_ids[f]);
  }

  printf(""shape[%ld].vertices: %ld\n"", i, shapes[i].mesh.positions.size());
  assert((shapes[i].mesh.positions.size() % 3) == 0);
  for (size_t v = 0; v < shapes[i].mesh.positions.size() / 3; v++) {
    printf(""  v[%ld] = (%f, %f, %f)\n"", v,
      shapes[i].mesh.positions[3*v+0],
      shapes[i].mesh.positions[3*v+1],
      shapes[i].mesh.positions[3*v+2]);
  }
}

for (size_t i = 0; i < materials.size(); i++) {
  printf(""material[%ld].name = %s\n"", i, materials[i].name.c_str());
  printf(""  material.Ka = (%f, %f ,%f)\n"", materials[i].ambient[0], materials[i].ambient[1], materials[i].ambient[2]);
  printf(""  material.Kd = (%f, %f ,%f)\n"", materials[i].diffuse[0], materials[i].diffuse[1], materials[i].diffuse[2]);
  printf(""  material.Ks = (%f, %f ,%f)\n"", materials[i].specular[0], materials[i].specular[1], materials[i].specular[2]);
  printf(""  material.Tr = (%f, %f ,%f)\n"", materials[i].transmittance[0], materials[i].transmittance[1], materials[i].transmittance[2]);
  printf(""  material.Ke = (%f, %f ,%f)\n"", materials[i].emission[0], materials[i].emission[1], materials[i].emission[2]);
  printf(""  material.Ns = %f\n"", materials[i].shininess);
  printf(""  material.Ni = %f\n"", materials[i].ior);
  printf(""  material.dissolve = %f\n"", materials[i].dissolve);
  printf(""  material.illum = %d\n"", materials[i].illum);
  printf(""  material.map_Ka = %s\n"", materials[i].ambient_texname.c_str());
  printf(""  material.map_Kd = %s\n"", materials[i].diffuse_texname.c_str());
  printf(""  material.map_Ks = %s\n"", materials[i].specular_texname.c_str());
  printf(""  material.map_Ns = %s\n"", materials[i].normal_texname.c_str());
  std::map<std::string, std::string>::const_iterator it(materials[i].unknown_parameter.begin());
  std::map<std::string, std::string>::const_iterator itEnd(materials[i].unknown_parameter.end());
  for (; it != itEnd; it++) {
    printf(""  material.%s = %s\n"", it->first.c_str(), it->second.c_str());
  }
  printf(""\n"");
}
 alignYAxisWithGravity {#GPU Implementation} DeformationLoopClosure #Embedded Shape Deformation 
 Deformation Loop Closure sample code to enable non-rigid alignment of point clouds.  
 Dependencies 
 CVD
TooN
Eigen3
CSparse
PCL 
 Acknowledgements 
 If you feel this code was useful please cite the following papers  
 SynthCam3D: Semantic Understanding With Synthetic Indoor Scenes 
Ankur Handa, Viorica Patraucean, Vijay Badrinarayanan, Simon Stent, Roberto Cipolla, 
arXiv:1505.00171 
 This work builds upon the ideas from  
 Embedded Deformation for Shape Manipulation : 
Robert W. Sumner, Johannes Schmid and Mark Pauly, 
SIGGRAPH 2007 
 ElasticFusion: Dense SLAM Without A Pose Graph : 
Thomas Whelan, Stefan Leutenegger, Renato F. Salas-Moreno, Ben Glocker, and Andrew J. Davison,
RSS 2015 
 Various jacobians used in the code are derived in this document which might of some interest:  http://mi.eng.cam.ac.uk/~ah781/NonRigidDeformableEmbedding.pdf 
 Primary Contact:
handa.ankur@gmail.com Export Normals 
 The script blender_import_export_with_Normals_obj.py allows you to export an OBJ format model with normals-map. 
 Export UV maps 
 blender_import_export_with_UVs.py exports any OBJ model with smart UV mapping option in blender.  
 Smart UV projection and Export 
 If the mesh is further decomposed into multiple sub-meshes, blender_import_export_with_UVs_allobjects.py export the OBJ model with smart UV mapping for each sub-mesh. 
 How to import obj file in blender and rename materials with their object names 
 ~/workspace/code/blender-2.76b-linux-glibc211-x86_64/blender -b ../blender_scripts/dummy.blend --python ../blender_scripts/changematname_importOBJ.py bedroom1_layout.obj 
 Explanation 
 ```
import bpy, bmesh
import sys 
 print (""First argument: %s"" % str(sys.argv[5]))
full_path_to_file = sys.argv[5] # + '.obj' #""night_stand_0026.obj"" 
 print('sys.argv[0] =', sys.argv[0])
print('sys.argv[1] =', sys.argv[1])
print('sys.argv[2] =', sys.argv[2])
print('sys.argv[3] =', sys.argv[3])
print('sys.argv[4] =', sys.argv[4])
print('sys.argv[5] =', sys.argv[5]) 
 bpy.ops.import_scene.obj(filepath=full_path_to_file,
                         #To ensure that objects and groups are preserved while importing
                         #use the following flags
                         use_split_groups=True, 
                         use_split_objects=True, 
                         use_image_search=True, 
                         use_smooth_groups=False, 
                         use_groups_as_vgroups=False) 
 C = bpy.context
D = bpy.data 
 objects = [obj for obj in D.objects if obj.type == 'MESH'] 
 for obj in objects:
        objname=obj.name
            mat=D.materials.new(objname)
                obj.data.materials.clear()
                    obj.data.materials.append(mat) 
 obj_out = 'test.obj' 
 bpy.ops.export_scene.obj(filepath=obj_out, axis_forward='-Z', axis_up='Y'
                        use_normals=True, use_uvs=True, use_materials=True) 
 ``` ORB_SLAM 
 ORB-SLAM is a versatile and accurate Monocular SLAM solution able to compute in real-time the camera trajectory and a sparse 3D reconstruction of the scene in a wide variety of environments, ranging from small hand-held sequences to a car driven around several city blocks. It is able to close large loops and perform global relocalisation in real-time and from wide baselines. 
 See our project webpage: http://webdiis.unizar.es/~raulmur/orbslam/ 
 Related Publications: 
 [1] Raúl Mur-Artal, J. M. M. Montiel and Juan D. Tardós.  ORB-SLAM: A Versatile and Accurate Monocular SLAM System .  IEEE Transactions on Robotics (Accepted). 2015. arXiv preprint: http://arxiv.org/abs/1502.00956 
 1. License 
 ORB-SLAM is released under a GPLv3 license. Please note that we provide along ORB-SLAM a modified version of g2o and DBoW2 which are both BSD.  
 For a closed-source version of ORB-SLAM for commercial purposes, please contact the authors.  
 If you use ORB-SLAM in an academic work, please cite: 
 @article{murAcceptedTRO2015,
  title={{ORB-SLAM}: a Versatile and Accurate Monocular {SLAM} System},
  author={Mur-Artal, Ra\'ul, Montiel, J. M. M. and Tard\'os, Juan D.},
  journal={IEEE Transaction on Robotics (Accepted). arXiv preprint arXiv:1502.00956},
  year={2015}
 }
 
 2. Prerequisites (dependencies) 
 2.1 Boost 
 We use the Boost library to launch the different threads of our SLAM system. 
 sudo apt-get install libboost-all-dev
 
 2.2 ROS 
 We use ROS to receive images from the camera or from a recorded sequence (rosbag), and for visualization (rviz, image_view). 
 We have tested ORB-SLAM in Ubuntu 12.04 with ROS Fuerte, Groovy and Hydro; and in Ubuntu 14.04 with ROS Indigo . 
If you do not have already installed ROS in your computer, we recommend you to install the Full-Desktop version of ROS Fuerte (http://wiki.ros.org/fuerte/Installation/Ubuntu). 
 If you use ROS Indigo, remove the depency of opencv2 in the manifest.xml. 
 2.3 g2o (included) 
 We use g2o to perform several optimizations. We include a modified copy of the library including only the components we need 
and also some changes that are listed in  Thirdparty/g2o/Changes.txt . 
In order to compile g2o you will need to have installed CHOLMOD, BLAS, LAPACK and Eigen3. 
 sudo apt-get install libsuitesparse-dev
sudo apt-get install libblas-dev
sudo apt-get install liblapack-dev
sudo apt-get install libeigen3-dev
 
 2.4 DBoW2 (included) 
 We make use of some components of the DBoW2 library (https://github.com/dorian3d/DBoW2) for place recognition and feature matching. We include a modified copy of the library including only the components we need and also some modifications that are listed in  Thirdparty/DBoW2/LICENSE.txt . 
It only depends on OpenCV, but it should be included in the ROS distribution. 
 3. Installation 
 
 
 Make sure you have installed ROS and all library dependencies (boost, eigen3, cholmod, blas, lapack). 
 
 
 Clone the repository: 
 git clone https://github.com/raulmur/ORB_SLAM.git ORB_SLAM
 
 
 
 Add the path where you cloned ORB-SLAM to the  ROS_PACKAGE_PATH  environment variable (better if you add the export line to your .bashrc file) 
 
 
 Build g2o. Go into  Thirdparty/g2o/  and execute: 
 mkdir build
cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
make
 
 Tip: To achieve the best performance in your computer, set your favorite compilation flags in line 97 and 98 of   Thirdparty/g2o/CMakeLists.txt  
      (by default -03 -march=native) 
 
 
 Build DBoW2. Go into Thirdparty/DBoW2/ and execute: 
 mkdir build
cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
make
 
 Tip: Set your favorite compilation flags in line 4 and 5 of   Thirdparty/DBoW2/CMakeLists.txt  (by default -03 -march=native) 
 
 
 Build ORB_SLAM. In the ORB_SLAM root execute: 
 mkdir build
cd build
cmake .. -DROS_BUILD_TYPE=Release
make
 
 Tip: Set your favorite compilation flags in line 12 and 13 of   Thirdparty/DBoW2/CMakeLists.txt  (by default -03 -march=native) 
 
 
 4. Usage 
 See section 5 to run the Example Sequence . 
 
 Launch ORB-SLAM from the terminal ( roscore  should have been already executed): rosrun ORB_SLAM ORB_SLAM PATH_TO_VOCABULARY PATH_TO_SETTINGS_FILE
 
 
 
 You have to provide the path to the ORB vocabulary and to the settings file. The paths must be absolute or relative   to the ORB_SLAM directory. 
  We already provide the vocabulary file we use in  ORB_SLAM/Data/ORBvoc.yml . Uncompress the file, as it will be   loaded much faster. 
 
 
 The last processed frame is published to the topic  /ORB_SLAM/Frame . You can visualize it using  image_view : 
 rosrun image_view image_view image:=/ORB_SLAM/Frame _autosize:=true
 
 
 
 The map is published to the topic  /ORB_SLAM/Map , the current camera pose and global world coordinate origin are sent through  /tf  in frames  /ORB_SLAM/Camera  and  /ORB_SLAM/World  respectively.  Run  rviz  to visualize the map: 
 in ROS Fuerte : 
 rosrun rviz rviz -d Data/rviz.vcg
 
 in ROS Groovy or a newer version : 
 rosrun rviz rviz -d Data/rviz.rviz
 
 
 
 ORB_SLAM will receive the images from the topic  /camera/image_raw . You can now play your rosbag or start your camera node. 
If you have a sequence with individual image files, you will need to generate a bag from them. We provide a tool to do that: https://github.com/raulmur/BagFromImages. 
 
 
 Tip: Use a roslaunch to launch  ORB_SLAM ,  image_view  and  rviz  from just one instruction. We provide an example : 
 in ROS Fuerte : 
 roslaunch ExampleFuerte.launch
 
 in ROS Groovy or a newer version : 
 roslaunch ExampleGroovyOrNewer.launch
 
 5. Example Sequence 
 We provide the settings and the rosbag of an example sequence in our lab. In this sequence you will see a loop closure and two relocalisation from a big viewpoint change. 
 
 
 Download the rosbag file: 
    http://webdiis.unizar.es/~raulmur/orbslam/downloads/Example.bag.tar.gz.  
 Alternative link: https://drive.google.com/file/d/0B8Qa2__-sGYgRmozQ21oRHhUZWM/view?usp=sharing 
 Uncompress the file. 
 
 
 Launch ORB_SLAM with the settings for the example sequence. You should have already uncompressed the vocabulary file ( /Data/ORBvoc.yml.tar.gz ) 
 
 
 in ROS Fuerte : 
   roslaunch ExampleFuerte.launch

*in ROS Groovy or newer versions*:

  roslaunch ExampleGroovyHydro.launch
 
 
 Once the ORB vocabulary has been loaded, play the rosbag (press space to start): rosbag play --pause Example.bag
 
 
 
 6. The Settings File 
 ORB_SLAM reads the camera calibration and setting parameters from a YAML file. We provide an example in  Data/Settings.yaml , where you will find all parameters and their description. We use the camera calibration model of OpenCV. 
 Please make sure you write and call your own settings file for your camera (copy the example file and modify the calibration) 
 7. Failure Modes 
 You should expect to achieve good results in sequences similar to those in which we show results in our paper [1], in terms of camera movement and texture in the environment. In general our Monocular SLAM solution is expected to have a bad time in the following situations:
- Pure rotations in exploration
- Low texture environments
- Many (or big) moving objects, especially if they move slowly. 
 The system is able to initialize from planar and non-planar scenes. In the case of planar scenes, depending on the camera movement relative to the plane, it is possible that the system refuses to initialize, see the paper [1] for details.  
 8. Need Help? 
 If you have any trouble installing or running ORB-SLAM, contact the authors. Needs CVD, TooN and Pangolin Needs recent version of Pangolin parsing.dustbin 
 Junkyard for all parsing functions to solve all the problems in the world  Dependencies 
 Pangolin 
 CVars 
 CVD 
 Installation 
 mkdir build 
 cd build 
 cmake .. && make -j8  
 sudo make install 
 Rendering RGB-D Video 
 After you clone you should see a bedroom scene in the  data  folder under the name   bedroom1.tgz  and a sample trajectory in the  data/bedroom1_data  folder under the name  bedroom1_trajectory.txt . To render an RGB-D video follow the steps through 
 untar bedroom1.tgz in the  data  directory 
 tar -xvzf bedroom1.tgz 
 untar tex_lib.tgz in the  texture_library_samples  directory 
 tar -xvzf tex_lib.tgz 
 in your build run  
 ./Examples/TextureRender/TextureRender ../data/bedroom1.obj 
 The output files are written in the  ../data/bedroom1_data  folder Main Branch: https://github.com/ankurhanda/stanford-scene-database Caffe 
 This is a fork from https://github.com/HyeonwooNoh/DeconvNet but modified to have the following support. 
  0. CRFasRNN from the recent paper ""Conditional Random Fields as Recurrent Neural Networks"". 
  0. Uncertainty using drop-out ""Dropout as bayesian approximation: Representing model uncertainty in deep learning"".
  0. Floating point data type for input channels e.g. depth, height from ground plane and angle with gravity vector ""SynthCam3D: Semantic Understanding With Synthetic Indoor Scenes"". Extra modules for LSTMs  
 SpatialConvolutionNoBias 
 SpatialFullConvolutionNoBias SceneNet 
 All the necessary source code for SceneNet   SceneNet: Understanding Real World Indoor Scenes with Synthetic Data  will be available here soon. 
 Updates 
 This code enables depth and annotation rendering given a 3D model and trajectory. We provide a sample 3D model in the  data  folder and a trajectory in  data/room_89_simple_data  folder. More details will be updated soon. Things to do  
 
 [ ] Simulated Annealing 
 [ ]  RGB Rendering  to be merged with the code 
 [ ] Releasing Trajectories 
 [ ] Converting Depth to DHA format 
 [ ] Emphasise on Negative Focal Length 
 
 Dependencies 
 
 [Pangolin] (https://github.com/ankurhanda/Pangolin-local) Local copy of https://github.com/stevenlovegrove/Pangolin  
 CVD  (It will not be a dependency any more in future!) 
 TooN  (It will be replaced by Eigen3 in future!) 
 ImageUtilities 
 SceneGraphRendering  (will be merged within soon!) 
 OpenCV/OpenCV2 
 libnoise (from synaptic) 
 
 Build 
 mkdir build
cd build
cmake .. -DCUDA_PROPAGATE_HOST_FLAGS=0
make -j8 
 Demo 
 in your build, run 
 ./opengl_depth_rendering ../data/room_89_simple.obj 
You should have annotated images in the folder  data/room_89_simple_data  that should look like these 
 
 Adding noise to the depth maps 
 Video 
 SceneNet Basis Models (Work In Progress) 
 
 [ ]  Bedrooms 
 Bedroom Layouts 
 [ ]  Livingrooms 
 Livingroom Layouts 
 [ ]  Offices 
 Office Layouts 
 [ ]  Kitchens 
 Kitchen Layouts 
 [ ]  Bathrooms 
 Bathroom Layouts 
 
 Sample Texture Library 
 Download the sample texture library to texture the models from  here . Textured bedrooms are shown below.  
 
 Website 
 More information is available here at our website  robotvault.bitbucket.org 
 Labels 
 Label name and number mapping is from Eigen et al. arXiv 2015, ICCV 2015 
 | Label Number  | Label Name    |
|:-------------:|:-------------:|
| 1  | Bed         | 
| 2  | Books       | 
| 3  | Ceiling     | 
| 4  | Chair       | 
| 5  | Floor       | 
| 6  | Furniture   | 
| 7  | Objects     | 
| 8  | Picture     | 
| 9  | Sofa        | 
| 10 | Table       | 
| 11 | TV          | 
| 12 | Wall        | 
| 13 | Window      |   
 Conversion from SUN RGB-D/NYUv2 37/40 Labels 
 Get the 40 class mapping for NYUv2 from  http://www.cs.berkeley.edu/~sgupta/cvpr13/ . SUN RGB-D already provide the 37 class mapping in their meta-data files. 
 Label name and number are from SUN RGB-D/ NYUv2. Their corresponding SceneNet/Eigen et al. mapping is in the last column. 
 | SUN RGB-D/NYUv2Label Number  | Label Name    | Eigen et al./SceneNet Mapping | 
|:-------------:|:-------------:|:-------------:|
| 1  | Wall         | 12 | 
| 2  | Floor       | 5 |
| 3  | Cabinet     | 6 | 
| 4  | Bed       | 1 | 
| 5  | Chair   | 4 | 
| 6  | Sofa     | 9 | 
| 7  | Table     | 10 | 
| 8  | Door        | 12 | 
| 9 | Window       | 13 | 
| 10 | BookShelf         | 6 | 
| 11 | Picture        | 8 | 
| 12 | Counter      | 6 | 
| 13 | Blinds      | 13 | 
| 14 | Desks      | 10 
| 15 | Shelves      | 6 | 
| 16 | Curtain      | 13 | 
| 17 | Dresser      | 6 | 
| 18 | Pillow     | 7 | 
| 19 | Mirror      | 7 | 
| 20 | Floor-mat      | 5 | 
| 21 | Clothes      | 7 | 
| 22  | Ceiling         | 3 | 
| 23  | Books       | 2 | 
| 24  | Refrigerator     | 6 | 
| 25  | Television       | 11  | 
| 26  | Paper       | 7 | 
| 27  | Towel   | 7 | 
| 28  | Shower-curtain | 7   |
| 29  | Box     | 7 | 
| 30  | Whiteboard    | 7    |
| 31 | Person       | 7 |
| 32 | NightStand       | 6 |
| 33 | Toilet        | 7 | 
| 34 | Sink      | 7 | 
| 35 | Lamp      | 7 | 
| 36 | Bathtub      | 7 | 
| 37 | Bag      | 7 | 
| 38 | Other-structure | 7  |
| 39 | Other-furniture   | 6   |
| 40 | Other-prop      | 7 |  
 Accuracy Script 
 Compute the global/per-class accuracy with  getAccuracyNYU.m  script provided in the repository. 
 Latex Code 
 \usepackage[table]{xcolor}
\definecolor{bedColor}{rgb}{0, 0, 1}
\definecolor{booksColor}{rgb}{0.9137,0.3490,0.1882}
\definecolor{ceilColor}{rgb}{0, 0.8549, 0}
\definecolor{chairColor}{rgb}{0.5843,0,0.9412}
\definecolor{floorColor}{rgb}{0.8706,0.9451,0.0941}
\definecolor{furnColor}{rgb}{1.0000,0.8078,0.8078}
\definecolor{objsColor}{rgb}{0,0.8784,0.8980}
\definecolor{paintColor}{rgb}{0.4157,0.5333,0.8000}
\definecolor{sofaColor}{rgb}{0.4588,0.1137,0.1608}
\definecolor{tableColor}{rgb}{0.9412,0.1373,0.9216}
\definecolor{tvColor}{rgb}{0,0.6549,0.6118}
\definecolor{wallColor}{rgb}{0.9765,0.5451,0}
\definecolor{windColor}{rgb}{0.8824,0.8980,0.7608} 
 \begin{table*}
\begin{tabular}{l}
\textbf{13 class semantic segmentation: NYUv2} \\
\end{tabular}
\centering
\begin{tabular}{|l|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|p{0.5cm}|}
\hline
Training  & \cellcolor{bedColor}\rotatebox{90}{bed} & \cellcolor{booksColor}\rotatebox{90}{books}  & \cellcolor{ceilColor}\rotatebox{90}{ceil.} & \cellcolor{chairColor}\rotatebox{90}{chair}  & \cellcolor{floorColor}\rotatebox{90}{floor}  & \cellcolor{furnColor}\rotatebox{90}{furn}   & \cellcolor{objsColor}\rotatebox{90}{objs.} & \cellcolor{paintColor}\rotatebox{90}{paint.} & \cellcolor{sofaColor}\rotatebox{90}{sofa}   & \cellcolor{tableColor}\rotatebox{90}{table}  & \cellcolor{tvColor}\rotatebox{90}{tv}     & \cellcolor{wallColor}\rotatebox{90}{wall}   & \cellcolor{windColor}\rotatebox{90}{window} \\ \hline
NYU-DHA & 67.7 & 6.5 & 69.9 & 47.9 & \textbf{96.2} & 53.8 & 46.5 & 11.3 & 50.7 & 41.6 & 10.8 & 85.0 & 25.8 \\ \hline
SceneNet-DHA & 60.8 & 2.0 & 44.2 & 68.3 & 90.2 & 26.4 & 27.6 &  6.3 & 21.1 & 42.2 & 0 & \textbf{92.0} & 0.0 \\ \hline
SceneNet-FT-NYU-DHA & 70.8 & 5.3 & 75.0 & 58.9 & 95.9 & 63.3 & 48.4 & 15.2 & 58.0 & 43.6 & 22.3 &  85.1  & 29.9 \\ \hline
NYU-DO-DHA & 69.6 & 3.1 & 69.3 & 53.2 & 95.9 & 60.0 & 49.0 & 11.6 & 52.7 & 40.2 & 17.3 & 85.0 & 27.1 \\ \hline
SceneNet-DO-DHA & 67.9 & 4.7 & 41.2 & 67.7 & 87.9 & 38.4 & 25.6 &  6.3 & 16.3 & 43.8 & 0 & 88.6 & 1.0 \\ \hline
SceneNet-FT-NYU-DO-DHA & \textbf{70.8} & 5.5 & 76.2 & 59.6 & 95.9 & \textbf{62.3} & \textbf{50.0} & 18.0 & \textbf{61.3} & 42.2 & 22.2 & 86.1 & 32.1 \\ \hline
Eigen \textit{et al.} (rgbd+normals) \cite{Eigen:etal:ICCV2015} & 61.1 & \textbf{49.7} & 78.3 & \textbf{72.1} & 96.0 & 55.1 & 40.7 &\textbf{58.7} & 45.8 &\textbf{44.9}& \textbf{41.9} & 88.7 & \textbf{57.7}  \\ \hline
Hermans \textit{et al.}(rgbd+crf)\cite{Hermans:etal:ICRA2014} & 68.4 & N/A & \textbf{83.4} & 41.9 & 91.5 & 37.1 & 8.6 & N/A & 28.5 & 27.7 & 38.4 & 71.8 & 46.1 \\ \hline
\end{tabular}
\vspace{0.5mm} \vspace{0.5mm}
\caption{Results on NYUv2 test data for 13 semantic classes. We see a similar pattern here --- adding synthetic data helps immensely in improving the performance of nearly all functional categories of objects using DHA as input channels. As expected, accuracy on \textit{books}, \textit{painting}, \textit{tv}, and \textit{windows}, is compromised highlighting that the role of depth as a modality to segment these objects is limited. Note that we recomputed the accuracies of \cite{Eigen:etal:ICCV2015} using their publicly available annotations of 320$\times$240 and resizing them to 224$\times$224. Hermans \textit{et al.} \cite{Hermans:etal:ICRA2014} use ``\textit{Decoration}"" and ``\textit{Bookshelf}"" instead of \textit{painting} and \textit{books} as the other two classes. Therefore, they are not directly comparable. Also, their annotations are not publicly available but we have still added their results in the table. Note that they use 640$\times$480. Poor performance of SceneNet-DHA and SceneNet-DO-DHA on \textit{tv} and \textit{windows} is mainly due to limited training data for these classes in SceneNet.}
\label{table: CA breakdown for 13 classes}
\end{table*} 
 Relevant Documents 
 The whole process of data generation and experiments are in the following papers. If you find our work valuable and helpful, please consider citing the relevant document. Thank you.  
 SceneNet: an Annotated Model Generator for Indoor Scene Understanding ,  ICRA 2016 
 
Ankur Handa, Viorica Patraucean, Simon Stent, Roberto Cipolla 
 @inproceedings{Handa:etal:ICRA2016,
  author    = {Ankur Handa and 
               Viorica P{\u a}tr{\u a}ucean and
               Simon Stent and
               Roberto Cipolla},
  title     = {SceneNet: an Annotated Model Generator for Indoor Scene Understanding},
  booktitle = {ICRA},
  year      = {2016}
} 
 Understanding Real World Indoor Scenes With Synthetic Data ,  CVPR 2016 
 
Ankur Handa, Viorica Patraucean, Vijay Badrinarayanan, Simon Stent, Roberto Cipolla 
 @inproceedings{Handa:etal:CVPR2016,
  author    = {Ankur Handa and 
               Viorica P{\u a}tr{\u a}ucean and
               Vijay Badrinarayanan and 
               Simon Stent and
               Roberto Cipolla},
  title     = {Understanding Real World Indoor Scenes With Synthetic Data},
  booktitle = {CVPR},
  year      = {2016}
} 
 An up-to-date version is maintained on the arXiv  
 SceneNet: Understanding Real World Indoor Scenes with Synthetic Data  arXiv link 
 
Ankur Handa, Viorica Patraucean, Vijay Badrinarayanan, Simon Stent, Roberto Cipolla 
 @inproceedings{Handa:etal:arXiv2016,
  author    = {Ankur Handa and 
               Viorica P{\u a}tr{\u a}ucean and
               Vijay Badrinarayanan and 
               Simon Stent and
               Roberto Cipolla},
  title     = {SceneNet: Understanding Real World Indoor Scenes With Synthetic Data},
  booktitle = {arXiv},
  year      = {2015}
} 
 License 
 All the code and data are released under a creative commons license which is purely for research purposes only. Please view the summary here  http://creativecommons.org/licenses/by-nc/4.0/ ImageUtilities 
 mkdir build 
 cd build  
 cmake .. -DCUDA_PROPAGATE_HOST_FLAGS=0 
 make -j8 (it will throw lots of warnings but will be fixed soon!) 
 sudo make install CVD 
 ./configure 
 make -j8  
 sudo make install TooN 
 ./configure  
 sudo make install  What is Pangolin {#mainpage} 
 Pangolin is a lightweight portable rapid development library for managing OpenGL
display / interaction and abstracting video input. At its heart is a simple
OpenGl viewport manager which can help to modularise 3D visualisation without
adding to its complexity, and offers an advanced but intuitive 3D navigation
handler. Pangolin also provides a mechanism for manipulating program variables
through config files and ui integration, and has a flexible real-time plotter
for visualising graphical data. 
 The ethos of Pangolin is to reduce the boilerplate code that normally
gets written to visualise and interact with (typically image and 3D
based) systems, without compromising performance. It also enables write-once
code for a number of platforms, currently including Windows, Linux, OSX, Android
and IOS. 
 Code 
 Find the latest version on  Github : 
 git clone https://github.com/stevenlovegrove/Pangolin.git 
 Current build status on  Drone.io 
 
 Dependencies 
 Optional dependencies are enabled when found, otherwise they are silently disabled.
Check the CMake configure output for details. 
 Required Dependencies 
 
 
 OpenGL (Desktop / ES / ES2) 
 
 
 Glew 
 
 (win) built automatically 
 (deb) sudo apt-get install libglew-dev 
 
 (mac) sudo port install glew 
 
 
 CMake (for build environment) 
 
 (win) http://www.cmake.org/cmake/resources/software.html 
 (deb) sudo apt-get install cmake 
 (mac) sudo port install cmake 
 
 Recommended Dependencies 
 
 Boost (optional with C++11. Configure with 'cmake -DCPP11_NO_BOOST=1 ..' ) 
 (win) http://www.boost.org/users/download/ 
 (deb) sudo apt-get install libboost-dev libboost-thread-dev libboost-filesystem-dev 
 
 (mac) sudo port install boost 
 
 
 Python2 / Python3, for drop-down interactive console 
 
 (win) http://www.python.org/downloads/windows 
 (deb) sudo apt-get install libpython2.7-dev 
 (mac) preinstalled with osx 
 
 Optional Dependencies for video input 
 
 FFMPEG (For video decoding and image rescaling) 
 
 (deb) sudo apt-get install ffmpeg libavcodec-dev libavutil-dev libavformat-dev libswscale-dev 
 
 
 DC1394 (For firewire input) 
 
 
 (deb) sudo apt-get install libdc1394-22-dev libraw1394-dev 
 
 
 libuvc (For cross-platform webcam video input via libusb) 
 
 
 git://github.com/ktossell/libuvc.git 
 
 
 libjpeg, libpng, libtiff, libopenexr (For reading still-image sequences) 
 
 
 (deb) sudo apt-get install libjpeg-dev libpng12-dev libtiff5-dev libopenexr-dev 
 
 
 OpenNI / OpenNI2 (For Kinect / Xtrion / Primesense capture) 
 
 
 DepthSense SDK 
 
 
 Very Optional Dependencies 
 
 
 Eigen / TooN (These matrix types supported in the Pangolin API.) 
 
 
 CUDA Toolkit >= 3.2 (Some CUDA header-only interop utilities included) 
 
 
 http://developer.nvidia.com/cuda-downloads 
 
 
 Doxygen for generating html / pdf documentation. 
 
 
 Building 
 Pangolin uses the CMake portable pre-build tool. To checkout and build pangolin in the
directory 'build', enabling C++11 support instead of using Boost, execute the
following at a shell (or the equivelent using a GUI): 
 git clone https://github.com/stevenlovegrove/Pangolin.git
cd Pangolin
mkdir build
cd build
cmake -DCPP11_NO_BOOST=1 ..
make -j 
 If you would like to build the documentation and you have Doxygen installed, you
can execute: 
 make doc 
 Issues 
 Please visit  Github Issues  to view and report problems with Pangolin. Issues and pull requests should be raised against the devel branch which contains the current development version. 
 Please note; most Pangolin dependencies are optional - to disable a dependency which may be causing trouble on your machine, simply blank out it's include and library directories with a cmake configuration tool (e.g. ccmake or cmake-gui). This repository containts the code to render living room scene as well as the trajectories. 
 Relevant paper 
 @InProceedings{handa:etal:ICRA2014,
author    = {A. Handa and T. Whelan and J.B. McDonald and A.J. Davison},
title     = {A Benchmark for {RGB-D} Visual Odometry, {3D} Reconstruction and {SLAM}},
booktitle = {IEEE Intl. Conf. on Robotics and Automation, ICRA},
address   = {Hong Kong},
month     = {May},
year      = {2014}
} gvnn: Neural Network Library for Geometric Vision , ECCV Workshop on Deep Geometry, 2016  
 Ankur Handa, Michael Bloesch, Viorica Patraucean, Simon Stent, John McCormac, Andrew Davison 
 Link to the paper  gvnn 
 What is gvnn? 
 gvnn is primarily intended for self-supervised learning using low-level vision. It is inspired by the Spatial Transformer Networks (STN) paper that appeared in NIPS in 2015 and its open source code made available by  Maxime Oquab . The code is self contained  i.e.  the original implementation of STN by Maxime is also within the repository.  
 STs were mainly limited to applying only 2D transformations to the input. We added a new set of transformations often needed for manipulating data in 3D geometric computer vision. These include the 3D counterparts of what were used in original STN together with a lot more new transformations and different M-estimators. 
 
 SO3 layer   - Rotations are expressed in so3 vector (v1, v2, v3) 
 Euler layer - Rotations are also expressed in euler angles 
 SE3 and Sim3 layer  
 Camera Pin-hole projection layer 
 3D Grid Generator 
 
 Per-pixel 2D transformations 
 
 2D optical flow 
 6D Overparameterised optical flow 
 Per-pixel SE(2) 
 Slanted plane disparity 
 
 
 
 Per-pixel 3D transformations 
 
 6D SE3/Sim3 transformations 
 10D transformation 
 
 
 
 M-estimators 
 
 
 Below you will see some examples of how to use gvnn to set up architectures for self-supervised learning. We plan to make this a comprehensive and complete library to bridge the gap between geometry and deeplearning.  
 We are also performing large scale experiments on data collected both from real world and our previous work,  SceneNet  to test different geometric computer vision algorithms  e.g.  dense image registration, 3D reconstruction and place recognition for loop closure. 
 Recommendation 
 Please do a fresh pull in case you spot any errors since the repository is getting updated regularly. 
 Installation 
 luarocks make gvnn-scm-1.rockspec 
 How to run gvnn on just CPU 
 
 Comment out require 'libcugvnn' from init.lua. 
 Use the CMakeLists_CPU.txt  i.e.  copy CMakeLists_CPU.txt to CMakeLists.txt. 
 Do a fresh install of gvnn and if possible uninstall the previous gvnn version. 
 
 Unit tests - Forward/Backward pass checks 
 All the relevant unit tests are in test.lua. The gif image below shows how to run the this file and check for any forward/backward pass errors in the layer implementation. 
 
All the modules that are in the repository have been tested properly and pass the forward and backward pass checks as defined in the test.lua. In case of any errors or visible hot-spots you may find in the code, please create an issue. 
 SO3 Layer 
 Rotations are represented as so(3) 3-vector. This vector is turned into rotation matrix via the exponential map. For a more detailed view of the so(3) representation and exponential map read this tutorial from Ethan Eade:  Lie-Algebra Tutorial . This is what the exponential map is  Exponential Map . Also, Tom Drummond's notes on Lie-Algebra are a great source to learn about exponential maps  Tom Drummond's notes . The reason for choosing so3 representation is mainly due to its appealing properties when linearising rotations (via taylor series expansion) for iterative image alignment via classic linearise-solve-update rule. The figure below shows how linearisation for SO3 is fitting a local plane on the sphere  
 
 The backprop derivatives of this rotation parameterisation is all you need to make sure you can insert this layer within a network - the derivatives are a bit involved but they look like this  
 
 However, this derivative has singularity at (0,0,0) because of the division by the norm of the vector. Therefore, we have a threshold to check if the magnitude is small enough that we can use a first-order approximation of the exponential map. The derivatives of this linearised version are nothing but the Generators of the exponential map  Generators   
 To set up 3D rotation warping, you first need to homogenise the x,y positions to [x, y, 1]^T, apply the inverse camera calibration matrix to get the ray in 3D. This ray is rotated with the rotation and then backprojected into the 2D plane with PinHoleCameraProjection layer and interpolated with bilinear interpolation. 
 ``` lua 
require 'nn'
require 'gvnn' 
 concat = nn.ConcatTable() 
 height = 240
width  = 320
u0     = 160
v0     = 120 
 fx = 240
fy = 240 
 -- first branch is there to transpose inputs to BHWD, for the bilinear sampler
tranet=nn.Sequential()
tranet:add(nn.SelectTable(1))
tranet:add(nn.Identity())
tranet:add(nn.Transpose({2,3},{3,4})) 
 rotation_net = nn.Sequential()
rotation_net:add(nn.SelectTable(2))
rotation_net:add(nn.TransformationRotationSO3())
rotation_net:add(nn.Transform3DPoints_R(height, width, fx, fy, u0, v0))
rotation_net:add(nn.PinHoleCameraProjectionBHWD(height, width, fx, fy, u0, v0))
rotation_net:add(nn.ReverseXYOrder()) 
 concat:add(tranet)
concat:add(rotation_net) 
 warping_net = nn.Sequential()
warping_net:add(concat)
warping_net:add(nn.BilinearSamplerBHWD())
warping_net:add(nn.Transpose({3,4},{2,3})) 
 ``` 
 This is how to use the previous network to warp and plot the image 
 ``` lua
require 'image'
require 'nn'
require 'torch' 
 dofile('imagewarpingSO3.lua') 
 x = image.loadPNG('linen1.png')
input = torch.Tensor(1,1,240,320)
input[1] = x 
 r = torch.Tensor(1,3):zero()
r[1][1] = 0.2
--r[1][2] = 0.3
--r[1][3] = 0.4 
 t = {input, r} 
 out_w = warping_net:forward(t) 
 w = out_w[1] 
 image.display(x)
image.display(w) 
 image.save('warped.png', w)
``` 
 For running on cuda just do :cuda() wherever needed.  e.g.  warping_net = warping_net:cuda(), input = input:cuda() and r = r:cuda()  
 
 SE3 Layer 
 ``` lua
require 'nn'
require 'gvnn' 
 --dofile('ReverseXYOrder.lua') 
 concat = nn.ConcatTable()
concat_Rt_depth = nn.ConcatTable() 
 height = 480--240
width  = 640--320
u0     = 320--160
v0     = 240--120 
 fx =  480 --240
fy = -480 --240 
 -- first branch is there to transpose inputs to BHWD, for the bilinear sampler
tranet=nn.Sequential()
tranet:add(nn.SelectTable(1))
tranet:add(nn.Identity())
tranet:add(nn.Transpose({2,3},{3,4})) 
 -- converts the 6-vector (3-vector so3 for rotation and 3-vector for translation)
Rt_net = nn.Sequential()
Rt_net:add(nn.SelectTable(2))
Rt_net:add(nn.TransformationMatrix3x4SO3(true,false,true)) 
 depth = nn.Sequential()
depth:add(nn.SelectTable(3)) 
 concat_Rt_depth:add(Rt_net)
concat_Rt_depth:add(depth) 
 Transformation3x4net = nn.Sequential()
Transformation3x4net:add(concat_Rt_depth)
Transformation3x4net:add(nn.Transform3DPoints_Rt(height, width, fx, fy, u0, v0))
Transformation3x4net:add(nn.PinHoleCameraProjectionBHWD(height, width, fx, fy, u0, v0))
Transformation3x4net:add(nn.ReverseXYOrder()) 
 concat:add(tranet)
concat:add(Transformation3x4net) 
 warping_net = nn.Sequential()
warping_net:add(concat)
warping_net:add(nn.BilinearSamplerBHWD())
warping_net:add(nn.Transpose({3,4},{2,3}))
  lua
require 'gvnn'
require 'torch'
require 'image' 
 dofile('imagewarpingSE3.lua') 
 --local height=480
--local width =360 
 ref_rgb_image   = image.load('iclnuim/rgb/100.png') 
 ref_depth_image = image.load('iclnuim/depth/100.png')
ref_depth_image = (ref_depth_image*65535)/5000.0 
 print(ref_rgb_image:size())
print(ref_depth_image:size()) 
 --image.display(ref_rgb_image)
--image.display(ref_depth_image) 
 data_ref_rgb      = torch.Tensor(1,3,480,640)
data_ref_rgb[1]   = ref_rgb_image 
 data_ref_depth    = torch.Tensor(1,1,480,640)
data_ref_depth[1] = ref_depth_image 
 so3_t_vector      = torch.Tensor(1,6):uniform() 
 -- tx, ty, tz, rx, ry, rz
-- -0.00119339 -0.00449791 -0.00122229 0.00104319 -0.00694122 -0.00333668 
 --- so3 and translation vector 
 so3_t_vector[1][1] = 0--  0.00104319
so3_t_vector[1][2] = 0-- -0.00694122
so3_t_vector[1][3] = 0-- -0.00333668 
 so3_t_vector[1][4] = 0-- -0.00119339
so3_t_vector[1][5] = 0-- -0.00449791
so3_t_vector[1][6] = 0-- -0.00122229 
 inputTable = {data_ref_rgb:cuda(), so3_t_vector:cuda(), data_ref_depth:cuda()} 
 outImage = warping_net:cuda():forward(inputTable) 
 image.display(outImage[1])
```
expand... 
 Optical Flow 
 Optical flow is a 2D motion vector per-pixel. In many standard computer vision formulations, it is obtained via the solutions of a partial differential equations involving a data term which measures the pixel colour discrepency between the reference image at time  t  and a new image at time  t+1 , and a regulariser which helps smooth out the flow vectors at the neighbouring pixels. We provide two formulations of the optical flow vector  i.e.  the standard minimal parameterisation 2D vector and an over-parameterised 6DoF optical flow. Below, we show an example of how to use this layer to do self-supervised learning. The optical flow predicted by a convolutional LSTM is used to warp the frame at time  t  on frame at  t+1 . The relevant paper and code is available  here .  
 
 
 Spatio-temporal autoencoder with differential memory.  Viorica Patraucean, Ankur Handa, Roberto Cipolla , ICLRWorkshop Track 2016 
 Disparity 
 Again, standard low-level vision provides an intuitively appealing way to do self-supervised learning. Now let us imagine instead of two frames in a video what if we had a stereo pair? We can then warp the left frame on top of the right in a similar way where the network instead predicts the disparity.  
 
 
 Unsupervised CNN for Single View Depth Estimation: Geometry to the rescue.  Ravi Garg, Vijay Kumar BG, Gustavo Carneiro, Ian Reid , ECCV 2016. 
 Projection Layer 
 The projection layer allows to project 3D data onto a 2D image plane via the projection matrix (in our case we use pin-hole camera projection matrix). This is extremely useful for data involving any 3D point cloud, depth and/or mesh and their projections in the 2D plane. This is differentiable only upto a point  i.e.  the forward/backward pass checks fail if the z-coordinate is below a certain threshold. 
 
 
 Lens Distortion 
 
expand... 
 Nonrigid SO3 
 expand... 
 Nonrigid SE3 
 Tracking non-rigid deformable objects is possible via a full dense per-pixel SE3 motion field. We provide a non-rigid se3 layer which predicts per-pixel se3 vector that allows to warp one depth image onto another as a means to do self-supervised learning. 
 
 
 SE3-Nets: Learning Rigid Body Motion using Deep Neural Networks,  Arunkumar Byravan and Dieter Fox , arXiv, 2016. 
 M-estimators 
 M-estimators have a long history in traditional computer vision and statistics. Michael Black's early papers in the 90s provide a compendium of various m-estimators and how most of them are superior to the standard L2 loss function and their ability to cull the outliers from the estimation of model parameters. We provide 4 different m-estimators namely, L2, Huber, Cauchy and Tukey.
 
 Future Improvements 
 Bilinear interpolation can use the tex2D function within CUDA to speed up the interpolation. Also, need to add interpolation with taylor series expansion as done in classic PDE based variational optimisation methods. Warping should be done at a higher resolution and blurred and downsampled later on  i.e.   DBW model used in Unger's super-resolution method . 
 License 
 GPL. We would like to thank Dyson Technologies Limited for supporting this work. 
 Contact 
 Ankur Handa (handa(dot)ankur(at)gmail(dot)com) 
 Acknowledgements 
 If you find the code useful, please consider citing the following 
 @inproceedings{Handa:etal:ECCVW16,
  author    = {Ankur Handa and 
               Michael Bloesch and 
               Viorica P{\u a}tr{\u a}ucean and
               Simon Stent and
               John McCormac and
               Andrew Davison},
  title     = {gvnn: Neural Network Library for Geometric Computer Vision},
  booktitle = {ECCV Workshop on Geometry Meets Deep Learning},
  year      = {2016}
} 
 @Misc{STNImplementation,
    author = {Maxime Oquab},
    title={{Open Source Implementation of Spatial Transformer Networks}},
    howpublished={URL https://github.com/qassemoquab/stnbhwd},
    year={2015}
} What does this repository contain? 
 This repository contains 13 class labels for both train and test dataset in NYUv2. This is to avoid any hassle involved in parsing the data from the .mat files. If you are looking to train a network to do 13 class segmentation from RGB data, then this repository can provide you both the training/test dataset as well the corresponding ground truth labels. However, if your networks needs additionally depth data (either depth image or DHA features) then you will need to download the dataset from the  NYUv2 website  (~2.8GB) as well as the corresponding  toolbox . To summarise, this repository contains the following 
 
 
 The  train_labels_13  contains the ground truth annotation for 13 classes for NYUv2 training dataset while  test_labels_13  contains the ground truth for test dataset in NYUv2. 
 
 
 The training dataset (795 RGB images) can be obtained from  nyu_train_rgb  (277MB) while the test dataset (654 RGB images) can be obtained from  nyu_test_rgb  (227MB). 
 
 
 Important to remember that the label files are ordered but the rgb files are not. Though you can order the files using  gprename . 
 
 
 How do I obtain the DHA features? 
 Look for this in a corresponding  SUN RGB-D meta data repository . You will need rotation matrices for each training and test image. They are available here at  camera_rotations_NYU.txt . These matrices are used to align the floor normal vector to the canonical gravity vector. There are 1449 rotation matrices in total and the indices for these matrices corresponding to training and test data are in  splits.mat . Remember that labels are ordered  i.e.  training labels files are named with indices 1 to 795 and similarly for test dataset.  
 How do I benchmark? 
 getAccuracyNYU.m  available in the  SceneNetv1.0  repository allows you to obtain the avereage global and class accuracies.  
 What are the classes and where is the mapping form the class number to the class name? 
 The mapping is also available at  SceneNetv1.0  repository. What does this repository contain? 
 The  SUNRGBD2Dseg.mat  contained in the  SUNRGBDtoolbox/Metadata  directory needs a RAM of about  64GB  to load either in MATLAB or Octave. Therefore, for future use and to avoid any dependence on the .mat file, the data ( i.e.  semantic segmentation labels) is extracted and stored in this repository. We also provide links to the RGB data. If you are looking to do semantic segmentation on the RGB images, this repository is self contained for that and you should be able to do it without having to download the dataset from the original links provided in the SUN RGB-D paper. However, if you need additional depth data, you will need to download the  tgz file  from the dataset link. We also provide code to turn depth into DHA features used in the  SceneNet  paper, by using the rotation matrices provided in the SUN RGB-D dataset. To summarise, this repository contains the following 
 
 Train and test images path names in the SUN RGB-D provided in the  sunrgbd_training_images.txt  and  sunrgbd_testing_images.txt  respectively. 
 37 Class labels for images both in training and test dataset compressed in the  sunrgbd_test_train_labels.tar.gz  file. 
 The first 5050 images in the  sunrgbd_test_train_labels.tar.gz  contain labels for test dataset while training set labels begin from 5051 and end at 10335. 
 Training dataset (5285 jpg images) is available from  SUNRGBD-train_images.tgz 
 Test dataset (5050 jpg images) is available from  SUNRGBD-test_images.tgz 
 
 The mappings from 37 class labels to 13 class labels are provided in  SceneNetv1.0  repository. The 37 class names are stored in  seg37list.mat  that comes in the SUNRGBDtoolbox but it contains the following list. 
~~~ 
 
 
 seg = load('seg37list.mat');
seg.seg37list
ans = 
{
  [1,1] = wall
  [1,2] = floor
  [1,3] = cabinet
  [1,4] = bed
  [1,5] = chair
  [1,6] = sofa
  [1,7] = table
  [1,8] = door
  [1,9] = window
  [1,10] = bookshelf
  [1,11] = picture
  [1,12] = counter
  [1,13] = blinds
  [1,14] = desk
  [1,15] = shelves
  [1,16] = curtain
  [1,17] = dresser
  [1,18] = pillow
  [1,19] = mirror
  [1,20] = floor_mat
  [1,21] = clothes
  [1,22] = ceiling
  [1,23] = books
  [1,24] = fridge
  [1,25] = tv
  [1,26] = paper
  [1,27] = towel
  [1,28] = shower_curtain
  [1,29] = box
  [1,30] = whiteboard
  [1,31] = person
  [1,32] = night_stand
  [1,33] = toilet
  [1,34] = sink
  [1,35] = lamp
  [1,36] = bathtub
  [1,37] = bag
}
~~~ 
 
 
 
 
 13 class training/testing labels are provided in  train13labels.tgz  and  test13labels.tgz  respectively, in this directory. 
 
 
 This alleviates the burden of having to install MATLAB (that requires a license) on your computer and parsing the .mat files in the SUN RGB-D dataset. 
 Training on RGB data for 13 classes 
 
 Get the RGB training data from  SUNRGBD-training_images.tgz 
 Get the 13 class labels from  train13labels.tgz . 
 Correspondingly, get the test RGB images from  SUNRGBD-test_images.tgz  and 13 class labels from  test13labels.tgz  for any benchmarking. 
 
 Training on RGB data for 37 classes 
 
 Get the RGBs as above. 
 Dowload the  sunrgbd_train_test_labels.tar.gz  in this directory and untar it  tar -xvzf sunrgbd_train_test_labels.tar.gz . 
 Create two directories  mkdir -p labels/train labels/test 
 Move the first 5050 files in  test  directory 
 mv ../img-00[0-4]*.png test && mv ../img-0050[0-4]*.png test && mv ../img-005050.png test  
and remaining in  train  directory  mv ../img-*.png train . 
 If you need to create a  .txt  file with names of corresponding rgbs and labels, please follow this 
 paste sunrgbd_rgb_files.txt -d' ' sunrgbd_labels37_files.txt  where  sunrgbd_rgb_files.txt  contains the names of the rgb files and similarly for  sunrgbd_labels37_files.txt . You should see the following 
~~~
img-000001.jpg img-005051.png
img-000002.jpg img-005052.png
img-000003.jpg img-005053.png
img-000004.jpg img-005054.png
img-000005.jpg img-005055.png
img-000006.jpg img-005056.png
img-000007.jpg img-005057.png
img-000008.jpg img-005058.png
img-000009.jpg img-005059.png
img-000010.jpg img-005060.png
img-000011.jpg img-005061.png
....
~~~ 
 
 Training and test data for depth 
 We now also provide links to depth data which are  
 
 training depth  training_data_depth  (505MB) 
 test depth  test_data_depth  (550MB) 
 
 To obtain the depth in meters, divide the png values by 10,000. 
 How do I compute the DHA features? 
 
 Download the SUN RGB-D dataset and make sure your paths are set properly. The SUN RGB-D dataset can be obtained from the link,  SUN RGB-D . The SUN RGB-D toolbox needed to parse the files is available here at  toolbox . 
 Depth images are contained in the depth_bfx folder in the SUN RGB-D dataset. You should be able to see that in one of the folders after you compress the .zip file  SUNRGBD/kv1/NYUdata/NYU0034/depth_bfx . These depth images have been obtained by running an inpainting algorithm on the raw depth images (that contain holes and missing values) to obtain a complete depth image for which each pixel has a depth value. 
 Make sure you have octave installed on your machine. If not, please install via apt-get on your ubuntu machine  sudo apt-get install octave  or  pacman -Sy octave  on your arch linux machine. 
 Run the  computeDHA_SUNRGBD.m  file in Octave (type octave --no-gui in your terminal) and it should return you the DHA features saved .bin file (if you wish to save in another format you could easily modify the code).  
 Camera rotation matrices are provided in  sunrgbd_train_T_fileNames.txt , to allow for mapping the floor normal vector to gravity vector to obtain height from ground plane.  
 
 How do I benchmark? 
 getAccuracyNYU.m  available in the  SceneNetv1.0  repository allows you to obtain the avereage global and class accuracies.  
 What are the classes and where is the mapping form the class number to the class name? 
 The mapping is also available at  SceneNetv1.0  repository. Physics based scene generation 
 /mnt/disk/scenenet/chrono/src/demos/irrlicht  is the directory that contains the main file  demo_collision_trimesh.cpp  that generates the scenes, it contains  
 
 scaling and sampling the objects from ShapeNets 
 picking the room layout  
 running the physics  
 saving a .txt file with object wnids, object poses (transformations), layout fileName. 
 scale is the height in meters  i.e.  $$ scale \times \frac{y}{max_y - min_y}$$ 
 mapping from the object name to the corresponding texture from the texture_library was done by hand. 
 
 ```
layout_file: ./kitchen/kitchen8_layout.obj
object
03001627/5539a4a9f2f1b45f3eec26c23f5bc80b
wnid
04331277
scale
0.803991
transformation
0.93618 0.00954775 -0.351392 -2.20642
-0.0109176 0.999939 -0.00191719 -0.0741752
0.351352 0.0056312 0.936226 -1.12748 
 object
04379243/8f29431ef2b28d27bfb1fc5d146cf068
wnid
04379243
scale
0.315817
transformation
0.985232 -0.0818803 -0.150381 -1.6889
0.0808304 0.996642 -0.0130912 2.59301
0.150948 0.000742512 0.988541 1.38842 
 ```
- The details about mapping Scenenet object with SUN RGB-D is here in the /mnt/disk/scenenet/chrono/canon_textfiles 
 [ankur@bigboy canon_textfiles]$ ls
class_sampling_prob.py  objects_in_scene.txt  ShapeNetsSUNRGBDMapping.txt  uniq_object_wnids.txt      val_layouts.txt
get_uniq.py             rejected_models.txt   test_layouts.txt             unique_scenes.txt          wnid_sample_probabilities_english.txt
layouts.txt             scene_type.txt        train_layouts.txt            uniq_wnids_in_dataset.txt  wnid_sample_probabilities.txt
[ankur@bigboy canon_textfiles]$ 
 ScenenetLayouts directory 
 
 The following code in the file  changematname_importOBJ.py  ensures that the object name and the material name are the same.
```
import bpy, bmesh
import sys 
 
 print (""First argument: %s"" % str(sys.argv[5]))
full_path_to_file = sys.argv[5] # + '.obj' #""night_stand_0026.obj"" 
 print('sys.argv[0] =', sys.argv[0])
print('sys.argv[1] =', sys.argv[1])
print('sys.argv[2] =', sys.argv[2])
print('sys.argv[3] =', sys.argv[3])
print('sys.argv[4] =', sys.argv[4])
print('sys.argv[5] =', sys.argv[5]) 
 bpy.ops.import_scene.obj(filepath=full_path_to_file, use_split_groups=True, use_split_objects=True, use_image_search=True, use_smooth_groups=False, use_groups_as_vgroups=False) 
 C = bpy.context
D = bpy.data 
 objects = [obj for obj in D.objects if obj.type == 'MESH'] 
 for obj in objects:
    objname=obj.name
    mat=D.materials.new(objname)
    obj.data.materials.clear()
    obj.data.materials.append(mat) 
 argv = sys.argv 
 argv = argv[argv.index(""--"") + 1:] # get all args after ""--"" 
 obj_out = './' + full_path_to_file 
 bpy.ops.export_scene.obj(filepath=obj_out, axis_forward='-Z', axis_up='Y',
                        use_normals=True, use_uvs=True, use_materials=True)
 - The file **scenenet_mat_to_wnid.txt** contains the mapping from the scenenet object name to their corresponding wnid. It looks like this 
lightbulb 03665924
bulb 03665924
shield 04192858
chandelier 03005285
terrace 03899768
pane 03881893
railing 04047401
dish 03206908
carpet 04118021
....
``` 
 
 train_layouts.txt, test_layouts.txt and val_layouts.txt contain the corresponding train/test/val layouts used when creating scenes. 
 The file  blender_import_export_with_UVs_allobjects.py  contains the python code to do UVmap the objects using the cube-mapping option in the blender. 
 
 ShapeNetsObj3 directory 
 
 We ran deinterlace_png_list.sh to fix the interlacing issues CVD had while reading a png file.  
 filtered_model_info_and_texture.txt has information about various different attributes of the 3D models  e.g.  some models do not have any texture so we don't want to render them and this is the root of all the model information we need and has space separated columns with names  
 directory of the 3D model  
 wordnet Id 
 tags  
 up vector 
 front vector  
 textured (we ignore it!)  
 valid_texture (so is this!) 
 
 
 Just to double check - there are other .txt files like  ordered_filtered_model_info_and_texture.txt  which we don't use but it was just there as a copy of some form of sorting we did. 
 Additionally there are three more files with the following names  
 test_split_filtered_model_info_and_texture.txt 
 train_split_filtered_model_info_and_texture.txt 
 valid_split_filtered_model_info_and_texture.txt 
 This is just to ensure that the train/test/valid are fully separated such that no overlapping objects among these. 
 
 
 We manually removed certain models from the ShapeNets that were deemed too unrealistic as a result of their skewed aspect ratio. We also removed objects like guns, cars, planes etc.. The following codes filteres them out. 
 
 ```
with open('model_info_and_texture.txt') as f:
    model_lines = f.readlines() 
 with open('./rejected_models_for_aspect.txt') as f:
    rejected_models = f.readlines() 
 reject_dict = {}
for rejected_model in rejected_models:
    reject_dict[rejected_model.rstrip()] = True 
 print len(reject_dict)
print len(model_lines) - 1 
 f = open('filtered_model_info_and_texture.txt','w') 
 f.write(model_lines[0])
number_written = 0
for line in model_lines[1:]:
    model = line.split()[0]
    if model.rstrip() in reject_dict:
        continue
    f.write(line)
    number_written += 1 
 f.close()
```
- We have this  rejected_models_for_aspect.txt  file that stores the model paths of those 3D models which have skewed aspect ratio. So how did we figure out that the aspect ratio is skewed? We divides max_y with max_z and/or max_y with max_x  
 Trajectory Generation code 
 The directory name is  scenenetplayground  which contains the main file named  main_check_room_camera_intersection.cpp . What does it need?  
 
 it has  std::string layout_fileName = std::string(argv[1]);   where argv[1] is the txt output from the physics engine. 
 There is the .sh file  run_many.py  in the build of scenenetplayground. It reads the files from train_physics_layouts directory and then runs the ./room_camera_intersection and once it has done processing this file, it writes the file into processed directory and at the same time it writes the corresponding layout-object-trajectory file in  /mnt/disk/scenenet/ScenenetLayouts/train_text_layouts/ . When doing batch rendering, the OptiX code reads the files from this directory and once it has done rendering, the python script moves the file into the _processed version.  
 
 ```
from functools import partial
from multiprocessing.dummy import Pool
from subprocess import call
import os
import time
import shutil 
 def main(): 
 def get_command(file):
    filename = os.path.split(file)[1].split('.')[0]
    return './room_camera_intersection {0} {1} > /dev/null 2>&1'.format(file,filename)

input_dir = '/mnt/disk/scenenet/chrono/train_physics_layouts/'
output_dir = '/mnt/disk/scenenet/chrono/train_physics_layouts_processed/'

pool = Pool(10)
while True:
    all_files = []
    for root, dirs, files in os.walk(input_dir):
        for file in files:
            abs_path = os.path.join(root,file)
            all_files.append(abs_path)
    # This slight gap ensures that if a file was just being written to, it
    # will have completed by the time we process it.  Writing the file takes
    # less than 10ms normally.
    time.sleep(5)
    commands = [get_command(x) for x in all_files]
    try:
        for i, returncode in enumerate(pool.imap(partial(call, shell=True), commands)):
            if returncode == 0:
                print('Completed {0}'.format(all_files[i]))
                try:
                    shutil.move(all_files[i],output_dir)
                except:
                    pass
    except:
        pass
 
 if  name  == ' main ':
    main() 
 There is a corresponding **run_make.py** in the directory **/mnt/disk/scenenet/bin2/build** that renders scene and puts the processed files in the **/mnt/disk/scenenet/ScenenetLayouts/train_text_layouts_processed/** directory. 
import logging
import multiprocessing
import os
import shutil
import subprocess
import threading
import time 
 class ThreadWorker(threading.Thread):
    def  init (self, in_queue, queue_func, **kwargs):
        threading.Thread. init (self)
        self.in_queue = in_queue
        self.queue_func = queue_func
        for key, value in kwargs.items():
            setattr(self, key, value) 
 def run(self):
    while True:
        try:
            item = self.in_queue.get()
        except OSError as e:
            logging.info('Queue closed')
            break
        logging.info('Queue size:{0}'.format(self.in_queue.qsize()))
        self.queue_func(self,item)
        self.in_queue.task_done()
 
 def main():
    # Logging is thread safe
    logging.basicConfig(level=logging.INFO, format='%(asctime)s (%(threadName)-2s) %(message)s',) 
 gpu_list = [0,2,3]
input_dir = '/mnt/disk/scenenet/ScenenetLayouts/train_text_layouts/'
processed_dir = '/mnt/disk/scenenet/ScenenetLayouts/train_text_layouts_processed/'
output_dir = '/mnt/disk/scenenet/newdataset/'
time_limit = 3 * 60 * 60

# The main worker function
def my_queue_func(worker,abs_path):
    gpu_id = worker.gpu_id
    filename_without_suffix = os.path.split(abs_path)[1].split('.')[0]
    this_run_base_dir = os.path.join(output_dir,filename_without_suffix)
    logging.info('About to process:{0} on gpu:{1}'.format(abs_path,gpu_id))
    if not os.path.exists(this_run_base_dir):
        os.mkdir(this_run_base_dir)
        os.mkdir(os.path.join(this_run_base_dir,'instance'))
        os.mkdir(os.path.join(this_run_base_dir,'photo'))
        os.mkdir(os.path.join(this_run_base_dir,'depth'))
    command = 'export CUDA_VISIBLE_DEVICES={0} && ./Headless_SceneNet {1} {2} > {1}/output.log 2>&1'.format(gpu_id,this_run_base_dir,abs_path)
    try:
        result = subprocess.call(command,shell=True,timeout=time_limit)
        if result == 0:
            logging.info('Success: finished processing:{0} return code:{1}'.format(abs_path,result))
            shutil.move(abs_path,processed_dir)
        else:
            logging.info('Failure: finished processing:{0} return code:{1}'.format(abs_path,result))
    except subprocess.TimeoutExpired as e:
        logging.info('Timeout on processing:{0}'.format(abs_path))

while True:
    in_queue = multiprocessing.JoinableQueue()
    # Get the list of files
    for root, dirs, files in os.walk(input_dir):
        for file in files:
            abs_path = os.path.join(root,file)
            in_queue.put(abs_path)
    time.sleep(5)
    # Spawn a pool of threads, and pass them the queue instance 
    for gpu in gpu_list:
        worker = ThreadWorker(in_queue,my_queue_func,gpu_id=gpu)
        worker.setDaemon(True)
        worker.start()
    in_queue.join()
 
 if  name  == ' main ':
    main() 
 ``` 
 OptiX rendering 
 
 /mnt/disk/scenenet/bin2/src/headless_SceneNet.cpp is the main file we used to render the scenes. 
 run_main.py is the file we use to invoke the rendering with the OptiX render. 
 
 This is the main function in the headless_SceneNet.cpp 
```
int main(int argc, char* argv[])
{
    std::string base_obj_folder = ""/mnt/disk/scenenet/ShapeNetObj3/"";
    std::string base_layout_folder = ""/mnt/disk/scenenet/ScenenetLayouts/""; 
 //Set save base location
std::string save_base = std::string(argv[1]);
std::string layout_file = std::string(argv[2]);
std::cout<<""Layout text:""<<layout_file<<std::endl; 
 BaseScene scene; 
 scene.initScene(save_base,layout_file,base_obj_folder,base_layout_folder,0); 
 const int number_trajectory_steps = 100000; 
 for (int i = 0; i < number_trajectory_steps; ++i)
{
    if (!scene.trace(save_base+""/"",i))
    {
      std::cout<<""Breaking""<<std::endl;
      break;
    }
}
return 0;
}
```
- /mnt/disk/scenenet/bin2/src/Scene.h and /mnt/disk/scenenet/bin2/src/Scene.cpp are where the scene is defined. 
- /mnt/disk/scenenet/bin2/src/Geometry/TriangleMesh.cu - this is just ensure that duplicate faces are rendered properly. 
- Gets the normal that is pointing towards the ray i.e. opposite to ray direction. If it is not then flip it such that it is always pointing opposite to the ray direction. More importantly, we ignore the normals given by the .obj and use geometric normals instead. Why did we do that? Normals given by obj were not necessarily credible mostly because the models had bad normal meta data. 
- Calling rtPotentialIntersection returns true if the t value given could potentially be an intersection point.  If there is no texture we increase the t value by 0.001, this is designed to give priority to faces that have texture.  Because they will have a smaller t value meaning they intersected first and so will be the ones returned.
- We can change the settings for rendering in the OptixRenderer.cpp file in the /mnt/disk/scenenet/bin2/src/Renderer 
 
 
 ```
const unsigned int OptixRenderer::PHOTON_GRID_MAX_SIZE = 0;
const unsigned int OptixRenderer::MAX_PHOTON_COUNT = MAX_PHOTONS_DEPOSITS_PER_EMITTED;
const unsigned int OptixRenderer::PHOTON_LAUNCH_WIDTH = 512;
const unsigned int OptixRenderer::PHOTON_LAUNCH_HEIGHT = 1024;
// Ensure that NUM PHOTONS are a power of 2 for stochastic hash
const unsigned int OptixRenderer::EMITTED_PHOTONS_PER_ITERATION = OptixRenderer::PHOTON_LAUNCH_WIDTH*OptixRenderer::PHOTON_LAUNCH_HEIGHT; 
 const unsigned int OptixRenderer::NUM_PHOTON_ITERATIONS = 32;
const unsigned int OptixRenderer::NUM_PHOTONS = OptixRenderer::EMITTED_PHOTONS_PER_ITERATION OptixRenderer::NUM_PHOTON_ITERATIONS OptixRenderer::MAX_PHOTON_COUNT; 
 const unsigned int OptixRenderer::NUM_PHOTON_MAPS = 4;
const unsigned int OptixRenderer::RES_DOWNSAMPLE = 1;
const unsigned int OptixRenderer::NUM_ITERATIONS = 6;
```
- If there is some runtime error to do with optix rendering (or memory issues), remember to fix it by changing GPU settings in CMakeLists.txt file. 
 How to get the word-net ids. We created a file with 155 objects and their definitions. 
 ```
import math
import re
import os
import sys
import numpy as np
import PIL.Image
import numpy
from collections import namedtuple
from nltk.corpus import wordnet as wn 
 The additional categories lookup to a string only - the official wordnet objects return a 
 synset object with definitions, and an appropriate place in the graph 
 def get_id_to_wnsynset_dict():
    additional_cats = [('jetplane', '20000001'), ('straightwing', '20000000'), ('beanchair', '20000018'),
         ('tulipchair', '20000023'), ('clubchair', '20000027'), ('roundtable', '20000038'),
         ('headboardbeds', '20000004'), ('boxcabinet', '20000009'), ('headlessbeds', '20000005'),
         ('kingsizebeds', '20000006'), ('wassilychair', '20000024'),
         ('cantileverchair', '20000020'), ('shorttable', '20000039'), ('jetplane', '20000000'),
         ('transportairplane', '20000002'), ('miscbeds', '20000007'),
         ('doublecouch', '20000028'), ('barcelonachair', '20000016'), ('bedcabinet', '20000008'),
         ('chaise', '20000026'), ('zigzagchair', '20000026'), ('two-doorcabinet', '20000013'),
         ('sweptwing', '20000001'), ('deskcabinet', '20000010'), ('sidetable', '20000040'),
         ('NO.14chair', '20000021'), ('chaise', '20000022'), ('jet-propelledplane', '20000002'),
         ('rexchair', '20000022'), ('jetplane', '20000002'), ('leathercouch', '20000030'),
         ('rectangulartable', '20000037'), ('ballchair', '20000015'),
         ('garagecabinet', '20000011'), ('workshoptable', '20000041'),
         ('cabinettable', '20000036'), ('tallcabinet', '20000012'),
         ('L-shapedcouch', '20000029'), ('butterflychair', '20000019'), ('Xchair', '20000025')]
    syns = list(wn.all_synsets())
    wn_id_to_synset = {str(s.offset()).zfill(8): s for s in syns}
    for cat, wnid in additional_cats:
        wn_id_to_synset[wnid] = cat
    return wn_id_to_synset 
 def map_wnid_to_nyu13():
    nyu_13_classes = [(0,'Unknown'),
                      (1,'Bed'),
                      (2,'Books'),
                      (3,'Ceiling'),
                      (4,'Chair'),
                      (5,'Floor'),
                      (6,'Furniture'),
                      (7,'Objects'),
                      (8,'Picture'),
                      (9,'Sofa'),
                      (10,'Table'),
                      (11,'TV'),
                      (12,'Wall'),
                      (13,'Window')
    ]
    mapping = {
        '04593077':4, '03262932':4, '02933112':6, '03207941':7, '03063968':10, '04398044':7, '04515003':7,
        '00017222':7, '02964075':10, '03246933':10, '03904060':10, '03018349':6, '03786621':4, '04225987':7,
        '04284002':7, '03211117':11, '02920259':1, '03782190':11, '03761084':7, '03710193':7, '03367059':7,
        '02747177':7, '03063599':7, '04599124':7, '20000036':10, '03085219':7, '04255586':7, '03165096':1,
        '03938244':1, '14845743':7, '03609235':7, '03238586':10, '03797390':7, '04152829':11, '04553920':7,
        '04608329':10, '20000016':4, '02883344':7, '04590933':4, '04466871':7, '03168217':4, '03490884':7,
        '04569063':7, '03071021':7, '03221720':12, '03309808':7, '04380533':7, '02839910':7, '03179701':10,
        '02823510':7, '03376595':4, '03891251':4, '03438257':7, '02686379':7, '03488438':7, '04118021':5,
        '03513137':7, '04315948':7, '03092883':10, '15101854':6, '03982430':10, '02920083':1, '02990373':3,
        '03346455':12, '03452594':7, '03612814':7, '06415419':7, '03025755':7, '02777927':12, '04546855':12,
        '20000040':10, '20000041':10, '04533802':7, '04459362':7, '04177755':9, '03206908':7, '20000021':4,
        '03624134':7, '04186051':7, '04152593':11, '03643737':7, '02676566':7, '02789487':6, '03237340':6,
        '04502670':7, '04208936':7, '20000024':4, '04401088':7, '04372370':12, '20000025':4, '03956922':7,
        '04379243':10, '04447028':7, '03147509':7, '03640988':7, '03916031':7, '03906997':7, '04190052':6,
        '02828884':4, '03962852':1, '03665366':7, '02881193':7, '03920867':4, '03773035':12, '03046257':12,
        '04516116':7, '00266645':7, '03665924':7, '03261776':7, '03991062':7, '03908831':7, '03759954':7,
        '04164868':7, '04004475':7, '03642806':7, '04589593':13, '04522168':7, '04446276':7, '08647616':4,
        '02808440':7, '08266235':10, '03467517':7, '04256520':9, '04337974':7, '03990474':7, '03116530':6,
        '03649674':4, '04349401':7, '01091234':7, '15075141':7, '20000028':9, '02960903':7, '04254009':7,
        '20000018':4, '20000020':4, '03676759':11, '20000022':4, '20000023':4, '02946921':7, '03957315':7,
        '20000026':4, '20000027':4, '04381587':10, '04101232':7, '03691459':7, '03273913':7, '02843684':7,
        '04183516':7, '04587648':13, '02815950':3, '03653583':6, '03525454':7, '03405725':6, '03636248':7,
        '03211616':11, '04177820':4, '04099969':4, '03928116':7, '04586225':7, '02738535':4, '20000039':10,
        '20000038':10, '04476259':7, '04009801':11, '03909406':12, '03002711':7, '03085602':11, '03233905':6,
        '20000037':10, '02801938':7, '03899768':7, '04343346':7, '03603722':7, '03593526':7, '02954340':7,
        '02694662':7, '04209613':7, '02951358':7, '03115762':9, '04038727':6, '03005285':7, '04559451':7,
        '03775636':7, '03620967':10, '02773838':7, '20000008':6, '04526964':7, '06508816':7, '20000009':6,
        '03379051':7, '04062428':7, '04074963':7, '04047401':7, '03881893':13, '03959485':7, '03391301':7,
        '03151077':12, '04590263':13, '20000006':1, '03148324':6, '20000004':1, '04453156':7, '02840245':2,
        '04591713':7, '03050864':7, '03727837':5, '06277280':11, '03365592':5, '03876519':8, '03179910':7,
        '06709442':7, '03482252':7, '04223580':7, '02880940':7, '04554684':7, '20000030':9, '03085013':7,
        '03169390':7, '04192858':7, '20000029':9, '04331277':4, '03452741':7, '03485997':7, '20000007':1,
        '02942699':7, '03231368':10, '03337140':7, '03001627':4, '20000011':6, '20000010':6, '20000013':6,
        '04603729':10, '20000015':4, '04548280':12, '06410904':2, '04398951':10, '03693474':9, '04330267':7,
        '03015149':9, '04460038':7, '03128519':7, '04306847':7, '03677231':7, '02871439':6, '04550184':6,
        '14974264':7, '04344873':9, '03636649':7, '20000012':6, '02876657':7, '03325088':7, '04253437':7,
        '02992529':7, '03222722':12, '04373704':4, '02851099':13, '04061681':10, '04529681':7,
    }
    english_mapping = {}
    for key, value in mapping.iteritems():
        english_mapping[key] = nyu_13_classes[value]
    return english_mapping 
 def get_all_semantic_categories():
    load_wnids_for_physics_objects = '/home/bjm113/Research/chrono/canon_textfiles/ShapeNetsSUNRGBDMapping.txt'
    wnids = set()
    with open(load_wnids_for_physics_objects,'r') as f:
        lines = f.readlines()
        for line in lines:
            wnids.add(line.split()[0])
    load_wnids_for_layouts = '/home/bjm113/ScenenetLayouts/scenenet_mat_to_wnid.txt'
    with open(load_wnids_for_layouts,'r') as f:
        lines = f.readlines()
        for line in lines:
            wnids.add(line.split()[1])
    return wnids 
 semantic_categories = get_all_semantic_categories()
print('Semantic Categories')
print len(semantic_categories)
semantic_wnid_lookup = get_id_to_wnsynset_dict() 
 nyu13_mapping = map_wnid_to_nyu13()
for category in semantic_categories:
    if type(semantic_wnid_lookup[category]) == type('string'):
        print '{0:<30} -> {1:<15}'.format(semantic_wnid_lookup[category],nyu13_mapping[category][1])
    else:
        print '{0:<30} -> {1:<15}'.format(semantic_wnid_lookup[category].name(),nyu13_mapping[category][1])
```      tf-unet 
 tensorflow version of unet 
 u-net is defined in the  UNet.py   
 To make sure you don't consider the pixels with zero labels  i.e.  pixels with missing labels, do the following 
 Python
loss_map = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=pred, labels=gt)
loss_map = tf.multiply(loss_map,tf.to_float(tf.not_equal(gt,0))) 
to train 
 Python
python3 train_u-net.py   python custom-char-rnn.py 
It trains for num_epochs and then samples from the RNN updated website 
 This repo is built on a fork of  Jekyll Now  from  this repository .  Jekyll  is a static site generator that's perfect for GitHub hosted blogs ( Jekyll Repository ) 
 The website design is just a modification of  Jon Barron's website  and is converted for my own use, re-purposing my old markdown posts.  Feel free to use template for your own purposes , but please respect copyright for all the images/content in my  images ,  pdfs ,  _posts  folders.  
 issues 
 
 In general, jekyll will try to build a full page for every post. I skip that by forcing  permalink: / . This creates multiple entries in sitemap.xml for index.html but is otherwise fine.  
 If you want multiple paragraphs, consider using  excerpt_separator: <!--more-->  in  _config.yml , for my own use I didn't need this.  
 My own posts have lots of extra stuff left over from my old jekyll design (""author"", long descriptions, etc.), feel free to ignore them 
 I use thumbnails, so I can upload arbitrary sized images but then only display small ones. The  _make_thumbnails.sh  script generates them and the html template looks in  tn/  for all images.  
 I have three categories of post with slightly differerent formatting, so changing sizing requires edits in multiple paces.  
 If you use this, I'd appreciate a link back either to this repo or my personal website so others can find this too.  
 What does this repository contain 
 
 
 This repository contains the weights of UNet models trained on RGB as well as RGB-D data of  SceneNet RGB-D dataset . 
 
 
 It has code to reproduce the UNet used in the paper and also provides segmentation evaluation scripts. 
 
 
 The  test_models.py  contains the code to reproduce the numbers as obtained in the  ICCV 2017 paper . 
 
 
 Important things to keep in mind before using the code 
 
 
 Download the pytorch models from the  google drive link . It contains 10 models in  pth  format and overall 5.8 GBs in total in size.  
 
 
 This code was converted from the torch implementation used in the paper. The image scaling in torch is different from the OpenCV/PIL image scaling (see the  torch github thread ) and therefore we provide the rgb and depth files converted from torch in  npy  format. However, when using these mdoels to fine-tune we believe it should not be a problem using any different image scaling algorithm -- minor scaling discrepancies can be easily subsumed by the fine-tuning process. We only wanted to make sure here that the models produce exactly the numbers stated in the paper.   
 
 
 The depth scaling used for  NYUv2  was  1/1000  and  SUN RGB-D  was  1/10000 . This means that if you are using the  NYUv2  pretrained SceneNet RGB-D model that was fine-tuned on  NYUv2  dataset then you should scale down the depth values by a factor of  1000  before using it for any new future experiments. Similarly, you should scale down the depth values by  10000  if you are using  SUN RGB-D  pretrained on SceneNet RGB-D. 
 
 
 To obtain the numbers in the paper for 13 class segmentations do  python test_models.py 
 
 
 If you would like to get the filtered dataset with labels greater than 3 per image it is here at  google drive link . It contains the names of the files not the pngs and is 23MB in size. 
 
 
 Updates 
 
 Any future updates will be posted here. 
 Get Horovod 
 pip install horovod 
 How to run 
 Assuming you have 16 GPUs on your machine you can run the test code as  
 mpirun -np 16 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH=/usr/local/nvidia/lib64/ python hvd_allgather.py 
 This will gather tensors from all the GPUs via  hvd.allgather()  and all the gathered tensors then will be broadcast to all GPUs. Therefore, at the end of this every GPU will have the same gathered tensor. Passive Indoor Environments 
 F 
 
 Falling Things 
 
 I 
 
 InteriorNet 
 ICL-NUIM 
 
 P 
 
 PBRS 
 
 S 
 
 ScanNet 
 SceneNet 
 SceneNet RGB-D 
 Stanford 2D-3D-Semantics Dataset 
 SUN CG 
 
 V 
 
 VaFRIC 
 
 Indoor Environments that support navigation and interaction with objects 
 A 
 
 AI2Thor 
 
 C 
 
 CHALET 
 
 G 
 
 Gibson Env 
 
 H 
 
 Home Platform 
 House3d 
 
 M 
 
 Matterport3D   
 MINOS 
 
 R 
 
 RobotriX 
 
 U 
 
 UnrealCV 
 
 V 
 
 Virtual Home 
 
 Physics simulators 
 B 
 
 Bullet 
 
 D 
 
 DART 
 
 F 
 
 FleX 
 
 G 
 
 Gazebo 
 
 M 
 
 MuJoCo 
 
 O  
 
 ODE 
 
 Simulators for driving/outdoor navigation 
 A 
 
 AirSim 
 
 C 
 
 CARLA 
 RL Games: High performance RL library 
 Discord Channel Link 
 
 https://discord.gg/hnYRq7DsQh 
 
 Papers and related links 
 
 Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning: https://arxiv.org/abs/2108.10470 
 Transferring Dexterous Manipulation from GPU Simulation to a Remote Real-World TriFinger: https://s2r2-ig.github.io/ https://arxiv.org/abs/2108.09779 
 Is Independent Learning All You Need in the StarCraft Multi-Agent Challenge?  https://arxiv.org/abs/2011.09533 
 Superfast Adversarial Motion Priors (AMP) implementation: https://twitter.com/xbpeng4/status/1506317490766303235 https://github.com/NVIDIA-Omniverse/IsaacGymEnvs 
 OSCAR: Data-Driven Operational Space Control for Adaptive and Robust Robot Manipulation: https://cremebrule.github.io/oscar-web/ https://arxiv.org/abs/2110.00704 
 
 Some results on the different environments 
 
 NVIDIA Isaac Gym 
 
 
 
 
 
 
 Starcraft 2 Multi Agents   
 BRAX   
 Mujoco Envpool   
 Atari Envpool   
 Random Envs   
 
 Implemented in Pytorch: 
 
 PPO with the support of asymmetric actor-critic variant 
 Support of end-to-end GPU accelerated training pipeline with Isaac Gym and Brax 
 Masked actions support 
 Multi-agent training, decentralized and centralized critic variants 
 Self-play  
 
 Implemented in Tensorflow 1.x (was removed in this version): 
 
 Rainbow DQN 
 A2C 
 PPO 
 
 Quickstart: Colab in the Cloud 
 Explore RL Games quick and easily in colab notebooks: 
 
 Mujoco training  Mujoco envpool training example. 
 Brax training  Brax training example, with keeping all the observations and actions on GPU. 
 
 Installation 
 For maximum training performance a preliminary installation of Pytorch 1.9+ with CUDA 11.1+ is highly recommended: 
 conda install pytorch torchvision cudatoolkit=11.3 -c pytorch -c nvidia  or:
 pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html 
 Then: 
 pip install rl-games 
 To run CPU-based environments either Ray or envpool are required  pip install envpool  or  pip install ray 
To run Mujoco, Atari games or Box2d based environments training they need to be additionally installed with  pip install gym[mujoco] ,  pip install gym[atari]  or  pip install gym[box2d]  respectively. 
 To run Atari also  pip install opencv-python  is required. In addition installation of envpool for maximum simulation and training perfromance of Mujoco and Atari environments is highly recommended:  pip install envpool 
 Citing 
 If you use rl-games in your research please use the following citation: 
 bibtex
@misc{rl-games2022,
title = {rl-games: A High-performance Framework for Reinforcement Learning},
author = {Makoviichuk, Denys and Makoviychuk, Viktor},
month = {May},
year = {2022},
publisher = {GitHub},
journal = {GitHub repository},
howpublished = {\url{https://github.com/Denys88/rl_games}},
} 
 Development setup 
 ```bash
poetry install 
 install cuda related dependencies 
 poetry run pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html
``` 
 Training 
 NVIDIA Isaac Gym 
 Download and follow the installation instructions of Isaac Gym: https://developer.nvidia.com/isaac-gym 
And IsaacGymEnvs: https://github.com/NVIDIA-Omniverse/IsaacGymEnvs 
 Ant 
 python train.py task=Ant headless=True 
 python train.py task=Ant test=True checkpoint=nn/Ant.pth num_envs=100 
 Humanoid 
 python train.py task=Humanoid headless=True 
 python train.py task=Humanoid test=True checkpoint=nn/Humanoid.pth num_envs=100 
 Shadow Hand block orientation task 
 python train.py task=ShadowHand headless=True 
 python train.py task=ShadowHand test=True checkpoint=nn/ShadowHand.pth num_envs=100 
 Other 
 Atari Pong 
 bash
poetry install -E atari
poetry run python runner.py --train --file rl_games/configs/atari/ppo_pong.yaml
poetry run python runner.py --play --file rl_games/configs/atari/ppo_pong.yaml --checkpoint nn/PongNoFrameskip.pth 
 Brax Ant 
 bash
poetry install -E brax
poetry run pip install --upgrade ""jax[cuda]==0.3.13"" -f https://storage.googleapis.com/jax-releases/jax_releases.html
poetry run python runner.py --train --file rl_games/configs/brax/ppo_ant.yaml
poetry run python runner.py --play --file rl_games/configs/brax/ppo_ant.yaml --checkpoint runs/Ant_brax/nn/Ant_brax.pth 
 Experiment tracking 
 rl_games support experiment tracking with  Weights and Biases . 
 bash
poetry install -E atari
poetry run python runner.py --train --file rl_games/configs/atari/ppo_breakout_torch.yaml --track
WANDB_API_KEY=xxxx poetry run python runner.py --train --file rl_games/configs/atari/ppo_breakout_torch.yaml --track
poetry run python runner.py --train --file rl_games/configs/atari/ppo_breakout_torch.yaml --wandb-project-name rl-games-special-test --track
poetry run python runner.py --train --file rl_games/configs/atari/ppo_breakout_torch.yaml --wandb-project-name rl-games-special-test -wandb-entity openrlbenchmark --track 
 Multi GPU 
 We use  torchrun  to orchestrate any multi-gpu runs. 
 bash
torchrun --standalone --nnodes=1 --nproc_per_node=2 runner.py --train --file rl_games/configs/ppo_cartpole.yaml 
 Config Parameters 
 | Field                  | Example Value                             | Default  | Description                                                                                            |
|------------------------|-------------------------------------------|----------|--------------------------------------------------------------------------------------------------------|
| seed                   | 8                                         |  None    | Seed for pytorch, numpy etc.                                                            |
| algo                   |                                           |          | Algorithm block.                                             |
|   name                 | a2c_continuous                            |  None    | Algorithm name. Possible values are: sac, a2c_discrete, a2c_continuous                          |
| model                  |                                           |          | Model block.                                                                                        |
|   name                 | continuous_a2c_logstd                     |  None    | Possible values: continuous_a2c ( expects sigma to be (0, +inf), continuous_a2c_logstd  ( expects sigma to be (-inf, +inf), a2c_discrete, a2c_multi_discrete                      |
| network                |                                           |          | Network description.                                                                            |
|   name                 | actor_critic                              |          | Possible values: actor_critic or soft_actor_critic.                                                                           |
|   separate             | False                                     |          | Whether use or not separate network with same same architecture for critic. In almost all cases if you normalize value it is better to have it False                                                                                           |
|   space                |                                           |          | Network space                                                  |
|     continuous         |                                           |          | continuous or discrete                                |
|       mu_activation    | None                                      |          | Activation for mu. In almost all cases None works the best, but we may try tanh.                             |
|       sigma_activation | None                                      |          | Activation for sigma. Will be threated as log(sigma) or sigma depending on model.                                                                                    |
|       mu_init          |                                           |          | Initializer for mu.                                                   |
|         name           | default                                   |          |                                                                                     |
|       sigma_init       |                                           |          | Initializer for sigma. if you are using logstd model good value is 0.                          |
|         name           | const_initializer                         |          |                                                    |
|         val            | 0                                         |          |                  |
|       fixed_sigma      | True                                      |          | If true then sigma vector doesn't depend on input.                                                   |
|   cnn                  |                                           |          | Convolution block.                    |
|     type               | conv2d                                    |          | Type: right now two types supported: conv2d or conv1d                                               |
|     activation         | elu                                       |          | activation between conv layers.                                  |
|     initializer        |                                           |          | Initialier. I took some names from the tensorflow.                                                             |
|       name             | glorot_normal_initializer                 |          | Initializer name                                                                                         |
|       gain             | 1.4142                                    |          | Additional parameter.                                                                  |
|     convs              |                                           |          | Convolution layers. Same parameters as we have in torch.                                                                                        |
|         filters        | 32                                        |          | Number of filters.                                                                                                  |
|         kernel_size    | 8                                         |          | Kernel size.                                                                                                    |
|         strides        | 4                                         |          | Strides                                                                  |
|         padding        | 0                                         |          | Padding                                                                                          |
|         filters        | 64                                        |          | Next convolution layer info.                                                                  |
|         kernel_size    | 4                                         |          |                                                                                                          |
|         strides        | 2                                         |          |                                                                                                |
|         padding        | 0                                         |          |                                                              |
|         filters        | 64                                        |          |                                           |
|         kernel_size    | 3                                         |          |                                                                                                         |
|         strides        | 1                                         |          |                                                |
|         padding        | 0                                         |          |                      
|   mlp                  |                                           |          | MLP Block. Convolution is supported too. See other config examples.                                                                                           |
|     units              |                                           |          | Array of sizes of the MLP layers, for example: [512, 256, 128]                                              |
|     d2rl               | False                                     |          | Use d2rl architecture from https://arxiv.org/abs/2010.09163.                                                                                     |
|     activation         | elu                                       |          | Activations between dense layers.                                |
|     initializer        |                                           |          | Initializer.                                      |
|       name             | default                                   |          | Initializer name.                                |
|   rnn                  |                                           |          | RNN block.                                 |
|     name               | lstm                                      |          | RNN Layer name. lstm and gru are supported.                                                                                          |
|     units              | 256                                       |          | Number of units.                                             |
|     layers             | 1                                         |          | Number of layers                                                                                                  |
|     before_mlp         | False                                     | False    | Apply rnn before mlp block or not.                                                                                                  |
| config                 |                                           |          | RL Config block.                               |
|   reward_shaper        |                                           |          | Reward Shaper. Can apply simple transformations.                                              |
|     min_val            | -1                                        |          | You can apply min_val, max_val, scale and shift.                  |
|     scale_value        | 0.1                                       | 1        |  |
|   normalize_advantage  | True                                      | True     | Normalize Advantage.                                                              |
|   gamma                | 0.995                                     |          | Reward Discount                                                              |
|   tau                  | 0.95                                      |          | Lambda for GAE. Called tau by mistake long time ago because lambda is keyword in python :(         |
|   learning_rate        | 3e-4                                      |          | Learning rate.                                                   |
|   name                 | walker                                    |          | Name which will be used in tensorboard.                  |
|   save_best_after      | 10                                        |          | How many epochs to wait before start saving checkpoint with best score.                                                                                    |
|   score_to_win         | 300                                       |          | If score is >=value then this value training will stop.        |
|   grad_norm            | 1.5                                       |          | Grad norm. Applied if truncate_grads is True. Good value is in (1.0, 10.0)                                             |
|   entropy_coef         | 0                                         |          | Entropy coefficient. Good value for continuous space is 0. For discrete is 0.02                                              |
|   truncate_grads       | True                                      |          | Apply truncate grads or not. It stabilizes training.                                                  |
|   env_name             | BipedalWalker-v3                          |          | Envinronment name.            |
|   e_clip               | 0.2                                       |          | clip parameter for ppo loss.                                                                                 |
|   clip_value           | False                                     |          | Apply clip to the value loss. If you are using normalize_value you don't need it.                                                                                 |
|   num_actors           | 16                                        |          | Number of running actors/environments.                           |
|   horizon_length       | 4096                                      |          | Horizon length per each actor. Total number of steps will be num_actors*horizon_length * num_agents (if env is not MA num_agents==1).                          |
|   minibatch_size       | 8192                                      |          | Minibatch size. Total number number of steps must be divisible by minibatch size.                                                           |
|   minibatch_size_per_env | 8                                       |          | Minibatch size per env. If specified will overwrite total number number the default minibatch size with minibatch_size_per_env * nume_envs value.                                                           |
|   mini_epochs          | 4                                         |          | Number of miniepochs. Good value is in [1,10]                                                                            |
|   critic_coef          | 2                                         |          | Critic coef. by default critic_loss = critic_coef * 1/2 * MSE.                                                                                    |
|   lr_schedule          | adaptive                                  | None     | Scheduler type. Could be None, linear or adaptive. Adaptive is the best for continuous control tasks. Learning rate is changed changed every miniepoch  |
|   kl_threshold         | 0.008                                     |          | KL threshould for adaptive schedule. if KL < kl_threshold/2 lr = lr * 1.5 and opposite.                                            |
|   normalize_input      | True                                      |          | Apply running mean std for input.                                                                           |
|   bounds_loss_coef     | 0.0                                       |          | Coefficient to the auxiary loss for continuous space.    |
|   max_epochs           | 10000                                     |          | Maximum number of epochs to run.                     |
|   normalize_value      | True                                      |          | Use value running mean std normalization.                                                                                          |
|   use_diagnostics      | True                                      |          | Adds more information into the tensorboard.                                              |
|   value_bootstrap      | True                                      |          | Bootstraping value when episode is finished. Very useful for different locomotion envs.               |
|   bound_loss_type      | 'regularisation'                          | None     | Adds aux loss for continuous case. 'regularisation' is the sum of sqaured actions. 'bound' is the sam of actions higher than 1.1.                                              |
|   bounds_loss_coef     | 0.0005                                    | 0        | Regularisation coefficient               |
|   use_smooth_clamp     | False                                     |          | Use smooth clamp instead of regular for cliping               |
|   player               |                                           |          | Player configuration block.                                                                                |
|     render             | True                                      | False    | Render environment                                                                            |
|     determenistic      | True                                      | True     | Use deterministic policy ( argmax or mu) or stochastic.                                                                                |
|     games_num          | 200                                       |          | Number of games to run in the player mode.                                             |
|   env_config           |                                           |          | Env configuration block. It goes directly to the environment. This example was take for my atari wrapper.                                                                                |
|     skip               | 4                                         |          | Number of frames to skip                                                                           |
|     name               | 'BreakoutNoFrameskip-v4'                  |          | Name of exact atari env. Of course depending on your env this parameters may be different.                                                                                | 
 Custom network example: 
 simple test network 
This network takes dictionary observation.
To register it you can add code in your  init .py 
 from rl_games.envs.test_network import TestNetBuilder 
from rl_games.algos_torch import model_builder
model_builder.register_network('testnet', TestNetBuilder) 
 simple test environment 
 example environment   
 Additional environment supported properties and functions   
 | Field                       | Default Value   | Description                         |
|-----------------------------|-----------------|-------------------------------------|
| use_central_value           | False             | If true than returned obs is expected to be dict with 'obs' and 'state'                                    |
| value_size                  | 1               | Shape of the returned rewards. Network wil support multihead value automatically.                                    |
| concat_infos                | False           | Should default vecenv convert list of dicts to the dicts of lists. Very usefull if you want to use value_boostrapping. in this case you need to always return 'time_outs' : True or False, from the env.                                    |
| get_number_of_agents(self)  | 1               | Returns number of agents in the environment                                    |
| has_action_mask(self)       | False           | Returns True if environment has invalid actions mask.                                    |
| get_action_mask(self)       | None            | Returns action masks if  has_action_mask is true.  Good example is  SMAC Env                                  | 
 Release Notes 
 1.5.2 
 
 Added observation normalization to the SAC. 
 Returned back adaptive KL legacy mode. 
 
 1.5.1 
 
 Fixed build package issue. 
 
 1.5.0 
 
 Added wandb support. 
 Added poetry support. 
 Fixed various bugs. 
 Fixed cnn input was not divided by 255 in case of the dictionary obs. 
 Added more envpool mujoco and atari training examples. Some of the results: 15 min Mujoco humanoid training, 2 min atari pong. 
 Added Brax and Mujoco colab training examples. 
 Added 'seed' command line parameter. Will override seed in config in case it's > 0. 
 Deprecated  horovod  in favor of  torch.distributed  ( #171 ). 
 
 1.4.0 
 
 Added discord channel https://discord.gg/hnYRq7DsQh :) 
 Added envpool support with a few atari examples. Works 3-4x time faster than ray. 
 Added mujoco results. Much better than openai spinning up ppo results. 
 Added tcnn(https://github.com/NVlabs/tiny-cuda-nn) support. Reduces 5-10% of training time in the IsaacGym envs.  
 Various fixes and improvements. 
 
 1.3.2 
 
 Added 'sigma' command line parameter. Will override sigma for continuous space in case if fixed_sigma is True. 
 
 1.3.1 
 
 Fixed SAC not working 
 
 1.3.0 
 
 Simplified rnn implementation. Works a little bit slower but much more stable.  
 Now central value can be non-rnn if policy is rnn. 
 Removed load_checkpoint from the yaml file. now --checkpoint works for both train and play. 
 
 1.2.0 
 
 Added Swish (SILU) and GELU activations, it can improve Isaac Gym results for some of the envs. 
 Removed tensorflow and made initial cleanup of the old/unused code. 
 Simplified runner. 
 Now networks are created in the algos with load_network method. 
 
 1.1.4 
 
 Fixed crash in a play (test) mode in player, when simulation and rl_devices are not the same. 
 Fixed variuos multi gpu errors. 
 
 1.1.3 
 
 Fixed crash when running single Isaac Gym environment in a play (test) mode. 
 Added config parameter  clip_actions  for switching off internal action clipping and rescaling 
 
 1.1.0 
 
 Added to pypi:  pip install rl-games 
 Added reporting env (sim) step fps, without policy inference. Improved naming. 
 Renames in yaml config for better readability: steps_num to horizon_length amd lr_threshold to kl_threshold 
 
 Troubleshouting 
 
 Some of the supported envs are not installed with setup.py, you need to manually install them 
 Starting from rl-games 1.1.0 old yaml configs won't be compatible with the new version:  
 steps_num  should be changed to  horizon_length  amd  lr_threshold  to  kl_threshold 
 
 
 
 Known issues 
 
 Running a single environment with Isaac Gym can cause crash, if it happens switch to at least 2 environments simulated in parallel 
 lstm 
 mish mash of lstm models just for refernce  This repository contains the pdf of the paper on dexpilot. 
 Videos here: https://sites.google.com/view/dex-pilot HTC-Vive-Setup-Ubuntu 
 Step by step guide to setting up HTC Vive on Ubuntu 
 Ankur Handa and Clemens Eppner  
 Installing Steam 
 Get the latest versin of Steam from  Steam Store 
 Installing SteamVR 
 Make sure you have an up-to-date NVIDIA driver (on 20th Feb 2020, we had NVIDIA driver version 430.64 installed on our Ubuntu 16.04). We installed the driver via  NVIDIA-Linux-x86_64-430.64.run  file we obtained from the NVIDIA website. You will need to make sure it also installs 32-bit GL libraries, so click on Yes when it prompts during the installation process. 
 Troubleshooting 
 If you do not install 32-bit GL libraries, you may get the following error  
 You are missing the following 32-bit libraries, and Steam may not run:libGL.so.1 
 For further requirements please check  https://github.com/ValveSoftware/SteamVR-for-Linux 
 Search for SteamVR in the steam application, you should see the following image: 
   
 Click on Launch to install it. Make sure you have SteamVR beta (and not linux_temp) version installed otherwise the SteamVR apps and OpenVR will not work. Now, to ensure that you install SteamVR beta, right click on the SteamVR app and click on properties.  
   
 You will see another window pop up and now click on  BETAS  and you can select beta from the options presented in the menu  
   
 Installing OpenVR 
 Most of the process mentioned below is from  this blog 
 $ git clone https://github.com/ChristophHaag/openvr.git
$ cd openvr
$ mkdir build
$ cd !$
$ cmake -DCMAKE_BUILD_TYPE=Release ../
$ make 
 Now plug in the vive and set up the permissions as below:  
 $ sudo chmod a+rw /dev/hidraw* 
 Create a shell file to set up the variables necessary for running openvr via steamvr. On our computer the file, called  run_openvr.sh  looks like this: 
 ```
export openvr=~/workspace/codeopenvr
export steam=~/.steam
export steamvr=$steam/steam/steamapps/common/SteamVR 
 export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:\
/usr/lib/:\
/usr/lib32/:\
$openvr/lib/linux32/:\
$openvr/lib/linux64/:\
$steam/ubuntu12_32/steam-runtime/i386/lib/i386-linux-gnu/:\
$steam/ubuntu12_32/steam-runtime/amd64/lib/x86_64-linux-gnu/:\
$steamvr/bin/linux32/:\
$steamvr/bin/linux64/:\
$steamvr/drivers/lighthouse/bin/linux32/:\
$steamvr/drivers/lighthouse/bin/linux64/
``` 
 We also need to set up the following permissions  
 $ sudo vim /etc/udev/rules.d/83-hmd.rules
$ echo 'SUBSYSTEM==""usb"", ATTR{idVendor}==""0bb4"", MODE=""0666"", GROUP=""plugdev""' >> test.txt 
$ echo 'SUBSYSTEM==""usb"", ATTR{idVendor}==""28de"", MODE=""0666"", GROUP=""plugdev""' >> test.txt
$ sudo cp test.txt /etc/udev/rules.d/83-hmd.rules
$ sudo udevadm control --reload-rules 
The file  /etc/udev/rules.d/83-hmd.rules  may not exist, so it needs to be created first.  
 source the  run_openvr.sh  file  
 source run_openvr.sh 
 Running HelloVR example with OpenVR 
 Now run the Steam server via 
 $steamvr/bin/linux64/vrserver --keepalive 
In another window, run this  hellovr_opengl  example  
 ~/.steam/steam/ubuntu12_32/steam-runtime/run.sh ~/workspace/code/openvr/samples/bin/linux64/hellovr_opengl 
You should see the following images streamed in your Vive when you wear it  
   
   
 Running TrackingCamera example with OpenVR 
 To run this example, you have to enable the camera via  SteamVR  as shown in the gif below:  
   
 Once the camera is enabled, you should be able to play the tracking camera example via  
 ~/.steam/steam/ubuntu12_32/steam-runtime/run.sh ~/workspace/code/openvr/samples/bin/linux64/tracked_camera_openvr_sample 
 and see the following images  
   
   
 Installing pyopenvr 
 If you'd like to play these examples in python then we recommend to install this (unofficial) version of  pyopenvr . 
 This requires Python 3.5+ and many other dependencies including  pillow, pyopengl, glfw  etc.  
 In addition, it is important to change  c_void_p  in this  line  in the pyopenvr code to  c_uint32  otherwise any python example will crash with a message that might look like this: 
 self.texture.handle = self.resolve_texture_id
TypeError: cannot be converted to pointer 
 This is also mentioned in this particular  issue  in the pyopenvr repository.  
 Install  pyopenvr  via 
 python setup.py install 
Make sure that the  PYTHONPATH  variable is set up correctly: 
 export PYTHONPATH=~/workspace/code/pyopenvr/src:$PYTHONPATH 
Now you should be able run the samples in python via  
 ~/.steam/steam/ubuntu12_32/steam-runtime/run.sh python src/samples/glfw/hellovr_glfw.py 
and see the following image  
 nerf2D 
 nerf2D is a 2D toy illustration of the  Neural Radiance Fields . It shows how adding the gamma encoding (also referred to as positional encoding and Eq. 4 in the NeRF paper) improves results significantly.  
 The task is to reconstruct an image (pixel colour values) from its 2D coordinates. The dataset consists of tuples ((x, y), (r, g, b)) where the input is (x, y) and output is (r, g, b). We train a 2 layer MLP with relu activations to map (x, y) to (r, g, b). The input is normalised (as also mentioned in the paper) to range [-1, 1] and we also output in range [-1, 1]. The purpose of this 2D illustration is to show that lifting the input observation (x, y) to higher dimensions via these transformations (via gamma encoding) makes it easier for network to learn things. Training with raw (x, y) results in blurry reconstructions while adding gamma encoding shows dramatic improvements in the results  i.e.  it is able to preserve the sharp edges in the image.  
 
 The sin plots for various values of L are: 
 
 The corresponding cos plots are: 
 
 Below, we show results with and without positional encoding. We use 2 layer MLPs each with  128  features with  ReLU  activations. The left image is the dataset image, the middle is the reconstruction using positional encoding and the right is the reconstruction with just raw (x, y). The flickering in the images is due to renormalisation of (r, g, b) from [-1, 1] to [0, 255] at every epoch. Note that the network that uses (x, y) as input is hardly able to get any high frequency details in the results. 
 In the positional encoding we use  L=10  for most of the cases, but for higher frequency reconstructions this number could be increased. This largely varies from image to image so this should be treated as a hyper-parameter. This positional encoding bears a lot of resemeblance to the famous Random Fourier Features in the paper from  Rahimi & Recht . In this particular case of positional encoding used in the  NeRF  work that we implemented, we have features computed at different scales and a phase shift of pi/2. In our experiments, we found both scale and phase shift to be very important.  
 The repo also has code for experiments with  sawtooth and RBF features  with scale and phase shift. 
 Glasses Image 
 Image Credits: http://hof.povray.org/glasses.html 
 
 Cool Cows Image 
 Image Credits: http://hof.povray.org/vaches.html 
 
 House Image 
 Image Credits: http://hof.povray.org/dhouse39.html 
 
 Training with more layers 
 We also trained with 8 layers of MLPs each with 128 features and ReLU and BatchNorm.  
 
 Requirements 
 tensorflow 2.0
opencv-python
python 3.6 
 Contact 
 Ankur Handa (handa (dot) ankur (at) gmail (dot) com) This repository contains various robot arm and object URDFs. The URDF models have been obtained from various publicly available resources and we have made sure that the links to the appropriate repositories where we got the models from are also provided. The purpose of this repository to put together many well known and often used models in one place.  
 URDFs 
 Robots 
 | Robot Name      | Source                                                                   |
|-----------------|--------------------------------------------------------------------------|
| Franka Panda    | https://github.com/dic-iit/gym-ignition-models                           |
| UR5/UR10/Kinova | https://github.com/Gepetto/example-robot-data.git                        |
| KUKA iiwa       | https://github.com/kuka-isir/iiwa_description                            |
| Fetch Robot     | https://github.com/openai/roboschool/tree/master/roboschool/models_robot |
| YUMI            | https://github.com/OrebroUniversity/yumi                                 |
| Ginger          | https://github.com/Rayckey/GingerURDF                                    |
| Anymal          | https://github.com/ANYbotics/anymal_b_simple_description                 |
| Halodi          | https://github.com/Halodi/halodi-robot-models                            |
| R2 and Val      | https://github.com/gkjohnson/nasa-urdf-robots                            |
| Robotiq Gripper | https://github.com/a-price/robotiq_arg85_description                     |
| Barrett Hand    | https://github.com/jhu-lcsr-attic/bhand_model                            | 
 Object Assets 
 Kitchen asset is from https://github.com/code-iai/robot_scenarios_sim/tree/master/models_pkg/models/kitchen 
 Conversion from xacro to URDF 
 The repo also contains the code to convert xacro to urdf (obtained from https://github.com/doctorsrn/xacro2urdf).  
 Run  python xacro.py -o ./target.urdf urdf/origin.xacro  to start convertion. For example,  python xacro.py -o ./test_abb_4600.urdf urdf/irb4600_60_205.xacro  in  abb_irb4600_support  folder. If convert successfully, the  test_abb_4600.urdf  will be generated. Make sure to keep  xacro.py  to the same directory as  urdf  folder. SimKinect 
 Simulating Kinect Noise: adding noise to clean depth-maps rendered with a graphics engine. This is a part of the ICL-NUIM dataset. The depth noise is modelled as a combination of the noise models used in Barraon et al. 2013 and Bohg et al. 2014 as referenced below. 
 How to run 
 python add_noise.py   
 Sample Results 
 The left image is a sample depth image from a simulator and the right image is the depth image with noise added to it.  
 
 Citations 
 If you use this code, please consider citing the following papers 
 ```
@article{handa:etal:2014,
  title   = {A benchmark for RGB-D visual odometry, 3D reconstruction and SLAM},
  author  = {Handa, Ankur and Whelan, Thomas and McDonald, John and Davison, Andrew J},
  journal = {ICRA},
  year    = {2014},
} 
 @article{Barron:etal:2013A,
  author  = {Jonathan T. Barron and Jitendra Malik},
  title   = {Intrinsic Scene Properties from a Single RGB-D Image},
  journal = {CVPR},
  year    = {2013},
} 
 @article{Bohg:etal:2014,
  title   = {Robot arm pose estimation through pixel-wise part classification},
  author  = {Bohg, Jeannette and Romero, Javier and Herzog, Alexander and Schaal, Stefan},
  journal = {ICRA},
  year    = {2014},
} 
 ``` rand_conv 
 This repo shows how to create on-demand random colour augmentations by convolving an image with random conv2d filters. This has shown to improve the performance in RL as well as image classification by https://arxiv.org/abs/1910.05396 and https://openreview.net/pdf?id=BVSM0x3EDK6 
 The filter weights are initialised from a normal distribution with standard deviation of 1 / sqrt(C_in) * kernel_size. Since we are dealing with 3 channel input images C_in is 3 and we also want output to be 3 channel. 
 ``` 
 create the random_conv filter 
 m = nn.Conv2d(3, 3, kernel_size, stride=1, padding=kernel_size//2, bias=False).cuda() 
 std_normal = 1 / (np.sqrt(3) * kernel_size) 
 m.weight = torch.nn.Parameter(torch.normal(mean=torch.zeros_like(m.weight), 
                                            std=torch.ones_like(m.weight)*std_normal))
``` 
 However, as mentioned in the paper https://arxiv.org/abs/1910.05396, using only randomised inputs complicates training. Therefore, we do blending as suggested  
 
 As shown in the paper https://openreview.net/pdf?id=BVSM0x3EDK6, varying the parameters (kernel size and the blending), we get a wide variety of augmentations 
 
 Dependencies 
 Install the following via pip 
 fire
matplotlib
pprintpp
torch 
 How to run 
 The function call arguments are made via  fire . Therefore, to understand what arguments to be passed, we'd recommend to do this first 
 python rand_conv.py --help 
 It will show you the following help for commandline arguments 
 ```
NAME
    rand_conv.py - A function to apply random convolution filters to the image 
                   input (H x W x 3) ->rand_conv-> output (H x W x 3) 
 SYNOPSIS
    rand_conv.py  
 DESCRIPTION
    @param: kernel_size is the size of the convolution filter to be used 
    @param: weight_init is the initalisation scheme (whether it's normal or xavier etc.) 
    @param: alpha is the blending parameter i.e. alpha input_image + (1-alpha) convolved_image 
    @param: save_images is for saving the blended images 
 FLAGS
    --kernel_size=KERNEL_SIZE
    --weight_init=WEIGHT_INIT
    --alpha=ALPHA
    --save_images=SAVE_IMAGES
(END)
``` 
 Results 
 Running with --kernel_size=3 and --alpha=0.7 
 
 Running with --kernel_size=3 and --alpha=0.3 
 
 Running with --kernel_size=11 and --alpha=0.3 
 matplotlib_utils 
 Utility functions for quick matplotlib plotting.. Implicit Behaviour Cloning implementation https://arxiv.org/abs/2109.00137 
 The repo containes a 2D example of implicit behaviour cloning. The task is regress the 2D coordinates corresponding to a red pixel in the image. Below are the results with implicit models. The blue dots represent the pixels <=1 pixel error. The green triangles are training data points. 
 
 ## Requirements  
 
 pytorch-lightning 
 einops 
 hydra-core 
 fire  
"
rtqichen,"torch-convgru 
 Dumping code for a convolutional GRU network implementation using nngraph. Allows ""parameter sharing relaxation"". 
 Modification of torch-rnn's code base: https://github.com/jcjohnson/torch-rnn
Uses nngraph library to compute step-wise outputs and gradients for easy extendability. 
 Gated Recurrent Unit with Convolutional Mappings 
 This module takes as input a tensor of size (N,T,H,W,D)
and outputs a tensor of size (N,T,H,W,H)
Where N is the minibatch size. 
T is the length of the time domain.
H and W are height and width of an image, not constrained.
D is the number of input features.
F is the number of hidden features.
For now, only works if kernel width and height are odd,
and forces a stride of 1. 
 ""Parameter Sharing Relaxation"" ( http://arxiv.org/abs/1511.08228 ) 
 Instead of sharing weights at every time step, weights are shared at every  r -th time step. 
 ie. at time step  t , the module uses the  i -th set of weights where  i = t mod r. 
 Weights are penalized by  p*| Avg - Weight |  where  Avg  is the average of the weights and  p  is a penalty coefficient. 
 Usage 
 module = nn.SpatialConvGRU(input_dim, hidden_dim, kW, kH, [hidden_downscale], [relaxation], [relaxationPenality]) 
 
 hidden_downscale  (integer > 0) Lets the hidden state have a downsampled size wrt. the input. Defaults to  1 . 
 relaxation  (integer > 0) The number of relaxed time steps. Defaults to  1 . 
 Fast Patch-based Style Transfer of Arbitrary Style 
 Paper: https://arxiv.org/abs/1612.04337 
 Code is written in Torch. CUDA and CPU modes are available. 
 Examples 
 
 
 
 
 
 (3x3 Patches) Content - w/ Starry Night - w/ Small Worlds I 
 
 
 
 
 
 with AvgPooling - using Inverse Network - using Inverse Network 
 
 
 
 
 
 
 (w/ Composition X) Original - 5x5 Patch - 9x9 Patch - 15x15 Patch 
 
 
 
 
 
 
 (w/ La Muse) Original - 3x3 Patch - 5x5 Patch - 9x9 Patch 
 Download Pretrained VGG-19 
 git clone https://github.com/rtqichen/style-swap
cd style-swap/models
sh download_models.sh
cd .. 
 Usage 
 Stylizing a single image:
 th style-swap.lua --content images/content/bike.jpg --style images/style/starry_night.jpg 
 More options:
 th style-swap.lua --help 
 eg. increase  --patchSize  for more abstract stylization
 th style-swap.lua --content images/content/brad_pitt.jpg --style images/style/la_muse.jpg --patchSize 7 --patchStride 3 
 eg. use  --contentBatch  to stylize all images in a directory.
 th style-swap.lua --contentBatch images/content --style images/style/starry_night.jpg 
 Training an inverse network 
 Install nninit module:
 luarocks install nninit 
 Train:
 th train-vgg-decoder.lua --contentDir /path/to/dir --styleDir /path/to/dir 
 More options:
 th train-vgg-decoder.lua --help 
 For training the network in our paper, we used images from  MS COCO  and the  Painter by Numbers  competition hosted by Kaggle. A trained network can be downloaded  here . 
 Video 
 Frame-by-frame stylization can be done using the  -contentBatch  option. 
 An example script using  ffmpeg  to extract frames, stylize, and re-encode a video.
 mkdir video_frames
ffmpeg -i /path/to/video -qscale:v 2 video_frames/video_%04d.jpg
th style-swap --contentBatch video_frames --style /path/to/style/file --save stylized_frames
ffmpeg -i stylized_frames/video_%04d_stylized.jpg -c:v libx264 -pix_fmt yuv420p stylized_video.mp4 
 Examples of stylized videos are placed in the videos folder. (Original video by  TimeLapseHD .) 
 Reducing GPU Memory Usage 
 A few ways to reduce memory usage for  style-swap.lua :
- Decrease  --maxStyleSize  and  --maxContentSize . The latter changes the size of the resulting image.
- Increase  --patchStride . This extracts less patches to use for style swap. Best to use a larger  --patchSize  to ensure the patches still overlap.
- Last resort: use CPU-only mode by specifying  --cpu .  beta-TCVAE 
 This repository contains cleaned-up code for reproducing the quantitative experiments in Isolating Sources of Disentanglement in Variational Autoencoders [ arxiv ]. 
 Usage 
 To train a model: 
 python vae_quant.py --dataset [shapes/faces] --beta 6 --tcvae 
Specify  --conv  to use the convolutional VAE. We used a mlp for dSprites and conv for 3d faces. To see all options, use the  -h  flag. 
 The main computational difference between beta-VAE and beta-TCVAE is summarized in  these lines . 
 To evaluate the MIG of a model:
 python disentanglement_metrics.py --checkpt [checkpt] 
To see all options, use the  -h  flag. 
 Datasets 
 dSprites 
 Download the npz file from  here  and place it into  data/ . 
 3D faces 
 We cannot publicly distribute this due to the  license . Please contact me for the data. 
 Contact 
 Email rtqichen@cs.toronto.edu if you have questions about the code/data. 
 Bibtex 
 @inproceedings{chen2018isolating,
  title={Isolating Sources of Disentanglement in Variational Autoencoders},
  author={Chen, Ricky T. Q. and Li, Xuechen and Grosse, Roger and Duvenaud, David},
  booktitle = {Advances in Neural Information Processing Systems},
  year={2018}
} Free-form Jacobian of Reversible Dynamics (FFJORD) 
 Code for reproducing the experiments in the paper: 
 
 Will Grathwohl , Ricky T. Q. Chen , Jesse Bettencourt, Ilya Sutskever, David Duvenaud. ""FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models.""  International Conference on Learning Representations  (2019).
 [arxiv]   [bibtex] 
 
 Prerequisites 
 Install  torchdiffeq  from https://github.com/rtqichen/torchdiffeq. 
 Usage 
 Different scripts are provided for different datasets. To see all options, use the  -h  flag. 
 Toy 2d:
 python train_toy.py --data 8gaussians --dims 64-64-64 --layer_type concatsquash --save experiment1 
 Tabular datasets from  MAF :
 python train_tabular.py --data miniboone --nhidden 2 --hdim_factor 20 --num_blocks 1 --nonlinearity softplus --batch_size 1000 --lr 1e-3 
 MNIST/CIFAR10:
 python train_cnf.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True 
 VAE Experiments (based on  Sylvester VAE ):
 python train_vae_flow.py --dataset mnist --flow cnf_rank --rank 64 --dims 1024-1024 --num_blocks 2 
 Glow / Real NVP experiments are run using  train_discrete_toy.py  and  train_discrete_tabular.py . 
 Datasets 
 Tabular (UCI + BSDS300) 
 Follow instructions from https://github.com/gpapamak/maf and place them in  data/ . 
 VAE datasets 
 Follow instructions from https://github.com/riannevdberg/sylvester-flows and place them in  data/ . 
 Bespoke Flows 
 Here's a fun script that you can use to create your own 2D flow from an image!
 python train_img2d.py --img imgs/github.png --save github_flow 
 
 
 PyTorch Implementation of Differentiable ODE Solvers 
 This library provides ordinary differential equation (ODE) solvers implemented in PyTorch. Backpropagation through ODE solutions is supported using the adjoint method for constant memory cost. For usage of ODE solvers in deep learning applications, see reference [1]. 
 As the solvers are implemented in PyTorch, algorithms in this repository are fully supported to run on the GPU. 
 Installation 
 To install latest stable version:
 pip install torchdiffeq 
 To install latest on GitHub:
 pip install git+https://github.com/rtqichen/torchdiffeq 
 Examples 
 Examples are placed in the  examples  directory. 
 We encourage those who are interested in using this library to take a look at  examples/ode_demo.py  for understanding how to use  torchdiffeq  to fit a simple spiral ODE. 
 
 
 
 Basic usage 
 This library provides one main interface  odeint  which contains general-purpose algorithms for solving initial value problems (IVP), with gradients implemented for all main arguments. An initial value problem consists of an ODE and an initial value,
 dy/dt = f(t, y)    y(t_0) = y_0. 
The goal of an ODE solver is to find a continuous trajectory satisfying the ODE that passes through the initial condition. 
 To solve an IVP using the default solver:
```
from torchdiffeq import odeint 
 odeint(func, y0, t)
 ``
where func is any callable implementing the ordinary differential equation f(t, x) , y0 is an _any_-D Tensor representing the initial values, and t is a 1-D Tensor containing the evaluation points. The initial time is taken to be t[0]`. 
 Backpropagation through  odeint  goes through the internals of the solver. Note that this is not numerically stable for all solvers (but should probably be fine with the default  dopri5  method). Instead, we encourage the use of the adjoint method explained in [1], which will allow solving with as many steps as necessary due to O(1) memory usage. 
 To use the adjoint method:
```
from torchdiffeq import odeint_adjoint as odeint 
 odeint(func, y0, t)
 `` odeint_adjoint simply wraps around odeint`, but will use only O(1) memory in exchange for solving an adjoint ODE in the backward call. 
 The biggest  gotcha  is that  func  must be a  nn.Module  when using the adjoint method. This is used to collect parameters of the differential equation. 
 Differentiable event handling 
 We allow terminating an ODE solution based on an event function. Backpropagation through most solvers is supported. For usage of event handling in deep learning applications, see reference [2]. 
 This can be invoked with  odeint_event :
 from torchdiffeq import odeint_event
odeint_event(func, y0, t0, *, event_fn, reverse_time=False, odeint_interface=odeint, **kwargs) 
 -  func  and  y0  are the same as  odeint .
 -  t0  is a scalar representing the initial time value.
 -  event_fn(t, y)  returns a tensor, and is a required keyword argument.
 -  reverse_time  is a boolean specifying whether we should solve in reverse time. Default is  False .
 -  odeint_interface  is one of  odeint  or  odeint_adjoint , specifying whether adjoint mode should be used for differentiating through the ODE solution. Default is  odeint .
 -  **kwargs : any remaining keyword arguments are passed to  odeint_interface . 
 The solve is terminated at an event time  t  and state  y  when an element of  event_fn(t, y)  is equal to zero. Multiple outputs from  event_fn  can be used to specify multiple event functions, of which the first to trigger will terminate the solve. 
 Both the event time and final state are returned from  odeint_event , and can be differentiated. Gradients will be backpropagated through the event function. 
 The numerical precision for the event time is determined by the  atol  argument. 
 See example of simulating and differentiating through a bouncing ball in  examples/bouncing_ball.py . 
 
 
 
 Keyword arguments for odeint(_adjoint) 
 Keyword arguments: 
 
 rtol  Relative tolerance. 
 atol  Absolute tolerance. 
 method  One of the solvers listed below. 
 options  A dictionary of solver-specific options, see the  further documentation . 
 
 List of ODE Solvers: 
 Adaptive-step:
 -  dopri8  Runge-Kutta of order 8 of Dormand-Prince-Shampine.
 -  dopri5  Runge-Kutta of order 5 of Dormand-Prince-Shampine  [default] .
 -  bosh3  Runge-Kutta of order 3 of Bogacki-Shampine.
 -  fehlberg2  Runge-Kutta-Fehlberg of order 2.
 -  adaptive_heun  Runge-Kutta of order 2. 
 Fixed-step:
 -  euler  Euler method.
 -  midpoint  Midpoint method.
 -  rk4  Fourth-order Runge-Kutta with 3/8 rule.
 -  explicit_adams  Explicit Adams-Bashforth.
 -  implicit_adams  Implicit Adams-Bashforth-Moulton. 
 Additionally, all solvers available through SciPy are wrapped for use with  scipy_solver . 
 For most problems, good choices are the default  dopri5 , or to use  rk4  with  options=dict(step_size=...)  set appropriately small. Adjusting the tolerances (adaptive solvers) or step size (fixed solvers), will allow for trade-offs between speed and accuracy. 
 Frequently Asked Questions 
 Take a look at our  FAQ  for frequently asked questions. 
 Further documentation 
 For details of the adjoint-specific and solver-specific options, check out the  further documentation . 
 References 
 Applications of differentiable ODE solvers and event handling are discussed in these two papers: 
 Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, David Duvenaud. ""Neural Ordinary Differential Equations.""  Advances in Neural Information Processing Systems.  2018.  [arxiv] 
 @article{chen2018neuralode,
  title={Neural Ordinary Differential Equations},
  author={Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  journal={Advances in Neural Information Processing Systems},
  year={2018}
} 
 Ricky T. Q. Chen, Brandon Amos, Maximilian Nickel. ""Learning Neural Event Functions for Ordinary Differential Equations.""  International Conference on Learning Representations.  2021.  [arxiv] 
 @article{chen2021eventfn,
  title={Learning Neural Event Functions for Ordinary Differential Equations},
  author={Chen, Ricky T. Q. and Amos, Brandon and Nickel, Maximilian},
  journal={International Conference on Learning Representations},
  year={2021}
} 
 The seminorm option for computing adjoints is discussed in 
 Patrick Kidger, Ricky T. Q. Chen, Terry Lyons. ""'Hey, that’s not an ODE': Faster ODE Adjoints via Seminorms.""  International Conference on Machine
Learning.  2021.  [arxiv] 
 @article{kidger2021hey,
  title={""Hey, that's not an ODE"": Faster ODE Adjoints via Seminorms.},
  author={Kidger, Patrick and Chen, Ricky T. Q. and Lyons, Terry J.},
  journal={International Conference on Machine Learning},
  year={2021}
} 
 
 If you found this library useful in your research, please consider citing.
 @misc{torchdiffeq,
    author={Chen, Ricky T. Q.},
    title={torchdiffeq},
    year={2018},
    url={https://github.com/rtqichen/torchdiffeq},
} Residual Flows for Invertible Generative Modeling [ arxiv ] 
 
 
 
 Building on the use of  Invertible Residual Networks  in generative modeling, we propose:
+ Unbiased estimation of the log-density of samples.
+ Memory-efficient reformulation of the gradients.
+ LipSwish activation function. 
 As a result, Residual Flows scale to much larger networks and datasets. 
 
 
 
 Requirements 
 
 PyTorch 1.0+ 
 Python 3.6+ 
 
 Preprocessing 
 ImageNet:
1. Follow instructions in  preprocessing/create_imagenet_benchmark_datasets .
2. Convert .npy files to .pth using  preprocessing/convert_to_pth .
3. Place in  data/imagenet32  and  data/imagenet64 . 
 CelebAHQ 64x64 5bit: 
 
 Download from https://github.com/aravindsrinivas/flowpp/tree/master/flows_celeba. 
 Convert .npy files to .pth using  preprocessing/convert_to_pth . 
 Place in  data/celebahq64_5bit . 
 
 CelebAHQ 256x256:
``` 
 Download Glow's preprocessed dataset. 
 wget https://storage.googleapis.com/glow-demo/data/celeba-tfr.tar
tar -C data/celebahq -xvf celeb-tfr.tar
python extract_celeba_from_tfrecords
``` 
 Density Estimation Experiments 
 NOTE : By default, O(1)-memory gradients are enabled. However, the logged bits/dim during training will not be an actual estimate of bits/dim but whatever scalar was used to generate the unbiased gradients. If you want to check the actual bits/dim for training (and have sufficient GPU memory), set  --neumann-grad=False . Note however that the memory cost can stochastically vary during training if this flag is  False . 
 MNIST:
 python train_img.py --data mnist --imagesize 28 --actnorm True --wd 0 --save experiments/mnist 
 CIFAR10:
 python train_img.py --data cifar10 --actnorm True --save experiments/cifar10 
 ImageNet 32x32:
 python train_img.py --data imagenet32 --actnorm True --nblocks 32-32-32 --save experiments/imagenet32 
 ImageNet 64x64:
 python train_img.py --data imagenet64 --imagesize 64 --actnorm True --nblocks 32-32-32 --factor-out True --squeeze-first True --save experiments/imagenet64 
 CelebAHQ 256x256:
 python train_img.py --data celebahq --imagesize 256 --nbits 5 --actnorm True --act elu --batchsize 8 --update-freq 5 --n-exact-terms 8 --fc-end False --factor-out True --squeeze-first True --nblocks 16-16-16-16-16-16 --save experiments/celebahq256 
 Pretrained Models 
 Model checkpoints can be downloaded from  releases . 
 Use the argument  --resume [checkpt.pth]  to evaluate or sample from the model.  
 Each checkpoint contains two sets of parameters, one from training and one containing the exponential moving average (EMA) accumulated over the course of training. Scripts will automatically use the EMA parameters for evaluation and sampling. 
 BibTeX 
 @inproceedings{chen2019residualflows,
  title={Residual Flows for Invertible Generative Modeling},
  author={Chen, Ricky T. Q. and Behrmann, Jens and Duvenaud, David and Jacobsen, J{\""{o}}rn{-}Henrik},
  booktitle = {Advances in Neural Information Processing Systems},
  year={2019}
}"
bmccann,"party_predictor 
 A repo for a CS 229 final project that classifies American political speeches by party affiliation 
 test pytorch-xla-transformer-language-model 
 This repository is an open source test case for  pytorch/xla  that runs a minimal training loop for a  Transformer  language model on a single TPU device. 
 This code is intended to be used as reference for testing the compilation of the model by XLA, and is not intended to be used for training a reasonable language model. During initial runs, this code triggered recompilation far too often, but these issues have now been resolved.  
 Depends on Docker image  gcr.io/tpu-pytorch/xla:r0.1 . 
 bash
export TPU_IP=#YOU MUST SET YOUR TPU IP
export XRT_TPU_CONFIG=""tpu_worker;0;$TPU_IP""
export XLA_USE_32BIT_LONG=1
export XLA_IR_DEBUG=1
export XLA_HLO_DEBUG=1
python3 train.py 
Output is in  run.log ."
fmassa,"torch-nn 
 Additional functionnalities for torch7 neural networks 
 The following modules were merged to different repositories 
 
 SpatialAdaptiveMaxPooling ->  torch/nn 
 LocalResponseNormalization ->  inn  as LocalSameResponseNormalization 
 SpatialPyramidPooling ->  inn 
 Normalize with infinity norm ->  torch/nn 
 imagine-nn 
 Universite Paris-Est Marne-la-Vallee IMAGINE/LIGM torch neural network routines 
 Following modules are here for now: 
 lua
inn.SpatialMaxPooling(kW,kH,dW,dH)
inn.SpatialAveragePooling(kW,kH,dW,dH)
inn.SpatialCrossResponseNormalization(size, [alpha = 0.0001], [beta = 0.75], [k = 1])
inn.LocalResponseNormalization([size = 3], [alpha = 0.00005], [beta = 0.75])
inn.MeanSubtraction(mean)
inn.SpatialPyramidPooling({{w1,h1},{w2,h2},...,{wn,hn}}) 
 The difference with  inn.SpatialMax(Average)Pooling  and  nn.SpatialMax(Average)Pooling  is that output size computed with ceil instead of floor (as in Caffe and cuda-convnet2). Also SpatialAveragePooling does true average pooling, meaning that it divides outputs by kW*kH.
inn.SpatialMax(Average)Pooling(kW,kH,dW,dH) is equal to cudnn.SpatialMax(Average)Pooling(kW,kH,dW,dH):ceil(). 
 inn.SpatialCrossResponseNormalization  is local response normalization across maps in BDHW format (thanks to Caffe!). For details refer to https://code.google.com/p/cuda-convnet/wiki/LayerParams#Local_response_normalization_layer_(across_maps) 
 inn.LocalResponseNormalization  is a local response normalization in the same map in BDHW format. For details refer to https://code.google.com/p/cuda-convnet/wiki/LayerParams#Local_response_normalization_layer_(same_map) 
 inn.MeanSubtraction(mean)  is done to subtract the Imagenet mean directly on GPU. Mean tensor is expanded to BDHW batches without using additional memory. 
 inn.SpatialPyramidPooling({{w1,h1},{w2,h2},...,{wn,hn}})  is a pyramid of regions obtained by using Spatial Adaptive Max Pooling with parameters  (w1,h1),...,(wn,hn)  in the input. The result is a fixed-sized vector of size  w1*h1*...wn*hn  for any input dimension. For details see http://arxiv.org/abs/1406.4729 Object detection in torch 
 Implementation of some object detection frameworks in  torch . 
 Note on new code 
 You should probably check the  refactoring branch of this repository , which simplifies the code structure, and also contains an implementation of Fast-RCNN and threaded RCNN (making it much faster than standard RCNN). It will be merged to master soon. 
 Dependencies 
 It requires the following packages 
 
 xml 
 matio-ffi.torch 
 hdf5 
 inn 
 
 To install them all, do 
 ``` 
 xml 
 luarocks install xml 
 matio 
 OSX 
 brew install libmatio 
 Ubuntu 
 sudo apt-get install libmatio2 
 luarocks install matio
``` 
 To install  hdf5 , follow the instructions in  here 
 Running this code 
 First, clone this repo
 git clone https://github.com/fmassa/object-detection.torch.git 
 The zeiler pretrained model is available at  https://drive.google.com/open?id=0B-TTdm1WNtybdzdMUHhLc05PSE0&authuser=0 .
It is supposed to be at  data/models .
If you want to use your own model in SPP framework, make sure that it follows the pattern
 model = nn.Sequential()
model:add(features)
model:add(pooling_layer)
model:add(classifier) 
where  features  can be a  nn.Sequential  of several convolutions and  pooling_layer  is the last pooling with reshaping of the data to feed it to the classifer. See  models/zeiler.lua  for an example. 
 To finetune the network for detection, simply run
 th main.lua 
 To get an overview of the different parameters, do
 th main.lua -h 
 The default is to consider that the dataset is present in  datasets/VOCdevkit/VOC2007/ .
The default location of bounding boxes  .mat  files (in RCNN format) is supposed to be in  data/selective_search_data/ . OptNet - reducing memory usage in torch neural networks 
 Memory optimizations for torch neural networks. 
 Heavily inspired from the  Optimizer  from https://github.com/facebook/fb-caffe-exts 
 Installing 
 Simply do
 luarocks install optnet 
 How does it work ? 
 It goes over the network and verify which buffers can be reused.
It supports both inference (evaluation) mode and training mode. 
 Inference mode 
 Here is a list of currently tested modules. Numbers are for CPU version, with batch size of 1, for  double  type, in the format
 (total memory used, memory used for the outputs, memory used for the internal buffers, memory used for the parameters and grad parameters) : 
 | Network | before optimization | after optimization | Relative save |
| ------- | :--------: | :-------: | :------: |
|alexnet | (973MB, 6MB, 43MB, 924MB) | (472MB, 1.5MB, 9MB, 462MB) | (51%, 75%, 80%, 50%) |
|vgg-A | (2311MB, 69MB, 215MB, 2027MB) | (1106MB, 31MB, 61MB, 1014MB) | (52%, 55%, 72%, 50%) |
|googlenet | (505MB, 69MB, 145MB, 292MB) | (193MB, 31MB, 16MB, 146MB) | (62%, 54%, 89%, 50%) |
|resnet 110 (cifar)| (113MB, 16MB, 71MB, 26MB) | (15MB, 0.5MB, 1.3MB, 13MB) | (87%, 97%, 98%, 50%) | 
 Note that for most of the models, for a batch size of 1 most of the memory is spent with the  weights  and  gradWeights  of the network (and the latter can be safely freed during inference).
More interestingly, the the output size is  linearly  dependent on the batch size, which means that the total savings are much more significant for bigger batch sizes. 
 In a more realistic setup where we use  cudnn  and batch size of 128, the gains are
way more significant, specially for very deep networks like resnet. The memory usage is shown in the following table (for  float  type), following  (total memory used, memory used for the outputs, memory used for the parameters and grad parameters)  as  cudnn  almost don't use internal buffers: 
 | Network | before optimization | after optimization | Relative save |
| ------- | :--------: | :-------: | :------: |
|alexnet | (859MB, 397MB, 462MB) | (328MB, 97MB, 231MB) | (62%, 75%, 50%) |
|vgg-A | (5340MB, 4386MB, 1014MB) | (2467MB, 1960MB, 507MB) | (54%, 55%, 50%) |
|googlenet | (4536MB, 4390MB, 146MB) | (2066MB, 1993MB, 73MB) | (54%, 55%, 50%) |
|resnet 110 (cifar)| (1049MB, 1036MB, 13MB) | (39MB, 32MB, 7MB) | (96%, 97%, 50%) | 
 Training mode 
 We currently support a basic algorithm for training mode.
Using  cudnn  with batch size of 64, we currently obtain the following savings, in the format  (total memory used, memory used for the outputs, memory used for the gradInputs, memory used for the parameters and gradParameters) : 
 | Network | before optimization | after optimization | Relative save |
| ------- | :--------: | :-------: | :------: |
|alexnet | (963MB, 195MB, 303MB, 462MB) | (816MB, 195MB, 156MB, 462MB) | (15%, 0%, 48%, 0%) |
|vgg-A | (5433MB, 2191MB, 2228MB, 1014MB) | (4228MB, 2191MB, 1023MB, 1014MB) | (22%, 0%, 54%, 0%) |
|googlenet | (6092MB, 2195MB, 3346MB, 146MB) | (4844MB, 2195MB, 2098MB, 146MB) | (20%, 0%, 37%, 0%) |
|resnet 110 (cifar)| (664MB, 259MB, 392MB, 13MB) | (428MB, 259MB, 156MB, 13MB) | (36%, 0%, 60%, 0%) | 
 Note that the relative save of the  gradInput  stays constant for different batch sizes, meaning that the total relative savings will be more important for bigger batch sizes (as the parameters doesn't depend on the batch size). 
 We can setup the optimizations for training mode by using  mode='training'  as follows 
 ```lua
models = require 'optnet.models'
modelname = 'googlenet'
net, input = models modelname 
 opts = {inplace=true, mode='training'} 
 optnet = require 'optnet' 
 optnet.optimizeMemory(net, input, opts)
``` 
 Optional parameters 
 Here is a list of options that are currently supported, and should be passed in the  opts  table as a third argument:
*  inplace : uses in place modules when available (boolean)
*  mode : selects between  training  and  inference  optimization algorithm (string)
*  reuseBuffers : shares internal buffers between same modules (like unfolded images for convolution) (boolean)
*  removeGradParams : remove  gradWeight  and  gradBias  in the networks, saving their sharings so that they can be exactly reconstructed. Only applies for  inference  mode (boolean) 
 Visualizing the memory reuse 
 We can analyse the sharing of the internal buffers by looking at the computation
graph of the network before and after the sharing. 
 For that, we have the  graphgen(net, input, opts)  function, which creates the
graph corresponding to the network  net . The generated graph contains the storage
id of each  output , and same colors means same storage. 
 Note that  net  is a  nn  model, and  not  a  nngraph  network. This allows us
to use  optnet.graphgen  to generate graph visualizations of  nn  networks without
having to use  nngraph . 
 Let's have a look: 
 ```lua
-- some handy models are defined in optnet.models
-- like alexnet, googlenet, vgg and resnet
models = require 'optnet.models'
modelname = 'googlenet'
net, input = models modelname 
 generateGraph = require 'optnet.graphgen' 
 -- visual properties of the generated graph
-- follows graphviz attributes
graphOpts = {
displayProps =  {shape='ellipse',fontsize=14, style='solid'},
nodeData = function(oldData, tensor)
  return oldData .. '\n' .. 'Size: '.. tensor:numel()
end
} 
 g = generateGraph(net, input, graphOpts) 
 graph.dot(g,modelname,modelname)
``` 
 This generates the following graph: 
 
 Now what happens after we optimize the network ? Check the colors and the storage
ids. 
 ```lua
models = require 'optnet.models'
modelname = 'googlenet'
net, input = models modelname 
 opts = {inplace=true, reuseBuffers=true} 
 generateGraph = require 'optnet.graphgen' 
 optnet = require 'optnet' 
 optnet.optimizeMemory(net, input, opts) 
 graphOpts = {
displayProps =  {shape='ellipse',fontsize=14, style='solid'},
nodeData = function(oldData, tensor)
  return oldData .. '\n' .. 'Size: '.. tensor:numel()
end
} 
 g = generateGraph(net, input, graphOpts) 
 graph.dot(g,modelname..'_optimized',modelname..'_optimized')
```
 
 Counting the amount of saved memory 
 We can also provide a function to compute the amount of memory used by the network
in bytes, which allows us to check the amount of saved memory.
It decomposes the total amount of memory in four fields:
* total memory used by the outputs of each module
* total memory used by the gradInputs of each module
* total memory used by the internal buffers of each module
* total memory used by the weights and gradWeights of each module. 
 Here is an example 
 ```lua
optnet = require 'optnet' 
 models = require 'optnet.models'
modelname = 'googlenet'
net, input = models modelname 
 -- countUsedMemory needs the network to
-- be initialized with all its buffers
-- to output correct results
net:forward(input)
mem1 = optnet.countUsedMemory(net) 
 optnet.optimizeMemory(net, input) 
 net:forward(input)
mem2 = optnet.countUsedMemory(net) 
 optnet.removeOptimization(net) 
 net:forward(input)
mem3 = optnet.countUsedMemory(net) 
 print('Before optimization        : '.. mem1.total_size/1024/1024 .. ' MBytes')
print('After optimization         : '.. mem2.total_size/1024/1024 .. ' MBytes')
print('After removing optimization: '.. mem3.total_size/1024/1024 .. ' MBytes') 
 ``` torch-vision 
 This repository consists of: 
 
 vision.datasets  : Data loaders for popular vision datasets 
 vision.transforms  : Common image transformations such as random crop, rotations etc. 
 [WIP] vision.models  : Model definitions and Pre-trained models for popular models such as AlexNet, VGG, ResNet etc. 
 
 Installation 
 Binaries: 
 bash
conda install torchvision -c https://conda.anaconda.org/t/6N-MsQ4WZ7jo/soumith 
 From Source: 
 bash
pip install -r requirements.txt
pip install . 
 Datasets 
 The following dataset loaders are available: 
 
 COCO (Captioning and Detection) 
 LSUN Classification 
 ImageFolder 
 Imagenet-12 
 
 Datasets have the API:
-  __getitem__ 
-  __len__ 
They all subclass from  torch.utils.data.Dataset 
Hence, they can all be multi-threaded (python multiprocessing) using standard torch.utils.data.DataLoader. 
 For example: 
 torch.utils.data.DataLoader(coco_cap, batch_size=args.batchSize, shuffle=True, num_workers=args.nThreads) 
 In the constructor, each dataset has a slightly different API as needed, but they all take the keyword args: 
 
 transform  - a function that takes in an image and returns a transformed version 
 common stuff like  ToTensor ,  RandomCrop , etc. These can be composed together with  transforms.Compose  (see transforms section below) 
 target_transform  - a function that takes in the target and transforms it. For example, take in the caption string and return a tensor of word indices. 
 
 COCO 
 This requires the  COCO API to be installed 
 Captions: 
 dset.CocoCaptions(root=""dir where images are"", annFile=""json annotation file"", [transform, target_transform]) 
 Example: 
 ```python
import torchvision.datasets as dset
import torchvision.transforms as transforms
cap = dset.CocoCaptions(root = 'dir where images are', 
                        annFile = 'json annotation file', 
                        transform=transforms.ToTensor()) 
 print('Number of samples: ', len(cap))
img, target = cap[3] # load 4th sample 
 print(""Image Size: "", img.size())
print(target)
``` 
 Output: 
 Number of samples: 82783
Image Size: (3L, 427L, 640L)
[u'A plane emitting smoke stream flying over a mountain.', 
u'A plane darts across a bright blue sky behind a mountain covered in snow', 
u'A plane leaves a contrail above the snowy mountain top.', 
u'A mountain that has a plane flying overheard in the distance.', 
u'A mountain view with a plume of smoke in the background'] 
 Detection: 
 dset.CocoDetection(root=""dir where images are"", annFile=""json annotation file"", [transform, target_transform]) 
 LSUN 
 dset.LSUN(db_path, classes='train', [transform, target_transform]) 
 
 db_path = root directory for the database files 
 classes = 
 'train' - all categories, training set 
 'val' - all categories, validation set 
 'test' - all categories, test set 
 ['bedroom_train', 'church_train', ...] : a list of categories to load 
 
 ImageFolder 
 A generic data loader where the images are arranged in this way: 
 ```
root/dog/xxx.png
root/dog/xxy.png
root/dog/xxz.png 
 root/cat/123.png
root/cat/nsdf3.png
root/cat/asd932_.png
``` 
 dset.ImageFolder(root=""root folder path"", [transform, target_transform]) 
 It has the members: 
 
 self.classes  - The class names as a list 
 self.class_to_idx  - Corresponding class indices 
 self.imgs  - The list of (image path, class-index) tuples 
 
 Imagenet-12 
 This is simply implemented with an ImageFolder dataset. 
 The data is preprocessed  as described here 
 Here is an example . 
 Transforms 
 Transforms are common image transforms.
They can be chained together using  transforms.Compose 
 
 ToTensor()  - converts PIL Image to Tensor 
 Normalize(mean, std)  - normalizes the image given mean, std (for example: mean = [0.3, 1.2, 2.1]) 
 Scale(size, interpolation=Image.BILINEAR)  - Scales the smaller image edge to the given size. Interpolation modes are options from PIL 
 CenterCrop(size)  - center-crops the image to the given size 
 RandomCrop(size)  - Random crops the image to the given size. 
 RandomHorizontalFlip()  - hflip the image with probability 0.5 
 RandomSizedCrop(size, interpolation=Image.BILINEAR)  - Random crop with size 0.08-1 and aspect ratio 3/4 - 4/3 (Inception-style) 
 
 transforms.Compose 
 One can compose several transforms together.
For example. 
 python
transform = transforms.Compose([
    transforms.RandomSizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ],
                          std = [ 0.229, 0.224, 0.225 ]),
]) Python Library stats 
 pylibstats  is a small library that allows you to query some useful statistics for Python code-bases.
We currently report library imports, function calls and attributes. 
 Installation 
 You can install  pylibstats  with the following command 
 ```
python -m pip install 'git+https://github.com/fmassa/python-lib-stats.git' 
 (add --user if you don't have permission) 
 Or, to install it from a local clone: 
 git clone https://github.com/fmassa/python-lib-stats.git
python -m pip install -e python-lib-stats
``` 
 Usage 
 In order to find all uses (imports / function calls / attribute queries) from library  <mylib>  over codebase  <path/to/python/repo> , run the following command: 
 pylibstats --local_dir <path/to/python/repo> --library_name <mylib> --absolute_count 
 You can also use  pylibstats  as a library.
```python
import pylibstats 
 summary = pylibstats.process_local_repository(""path/to/repo"", library_name=""mylib"")
 ``
where summary is a Dict[Dict[str, int]] with the following entries: import_count , call_count and access_count , each field being a dictionary with the mapping of name: count`. 
 Example 
 Looking for all  torchvision  occurrences on the  DETR  codebase yields: 
 ``` 
                               Imports
 
 ===========================================================================
                                                                    | Count 
 
 torchvision                                                     : 4
torchvision.ops._new_empty_tensor                               : 1
torchvision.ops.misc._output_size                               : 1
torchvision.ops.boxes.box_area                                  : 1
torchvision.transforms                                          : 1
torchvision.transforms.functional                               : 1
torchvision.models._utils.IntermediateLayerGetter               : 1
 
 ===========================================================================
                                   Calls
===========================================================================
                                                                    | Count 
 
 torchvision.__version__.split                                   : 2
torchvision.ops.boxes.box_area                                  : 2
torchvision.transforms.RandomCrop.get_params                    : 2
torchvision._is_tracing                                         : 1
torchvision.ops.misc._output_size                               : 1
torchvision.ops._new_empty_tensor                               : 1
torchvision.ops.misc.interpolate                                : 1
torchvision.transforms.functional.crop                          : 1
torchvision.transforms.functional.hflip                         : 1
torchvision.transforms.functional.resize                        : 1
torchvision.transforms.functional.pad                           : 1
torchvision.transforms.functional.to_tensor                     : 1
torchvision.transforms.RandomErasing                            : 1
torchvision.transforms.functional.normalize                     : 1
torchvision.models._utils.IntermediateLayerGetter               : 1
torchvision.models.{?}                                          : 1
 
 ===========================================================================
                                   Attrs
===========================================================================
                                                                    | Count 
 
 torchvision.__version__.split                                   : 2
torchvision.ops.boxes.box_area                                  : 2
torchvision.datasets.CocoDetection                              : 2
torchvision.transforms.RandomCrop.get_params                    : 2
torchvision._is_tracing                                         : 1
torchvision.ops.misc._output_size                               : 1
torchvision.ops._new_empty_tensor                               : 1
torchvision.ops.misc.interpolate                                : 1
torchvision.transforms.functional.crop                          : 1
torchvision.transforms.functional.hflip                         : 1
torchvision.transforms.functional.resize                        : 1
torchvision.transforms.functional.pad                           : 1
torchvision.transforms.functional.to_tensor                     : 1
torchvision.transforms.RandomErasing                            : 1
torchvision.transforms.functional.normalize                     : 1
torchvision.models._utils.IntermediateLayerGetter               : 1
torchvision.models                                              : 1
 
 ```"
edouardelasalles,"Spatio-Temporal Neural Networks for Space-Time Series Forecasting and Relation Discovery 
 ICDM 2017 - IEEE International Conference on Data Mining series (ICDM) 
 Conference Paper 
 Journal Extension 
 Commands for reproducing synthetic experiments: 
 Heat Diffusion 
 STNN 
 python train_stnn.py --dataset heat --outputdir output_heat --manualSeed 2021 --xp stnn 
 STNN-R(efine) 
 python train_stnn.py --dataset heat --outputdir output_heat --manualSeed 5718 --xp stnn_r --mode refine --patience 800 --l1_rel 1e-8 
 STNN-D(iscovery) 
 python train_stnn.py --dataset heat --outputdir output_heat --manualSeed 9690 --xp stnn_d --mode discover --patience 1000 --l1_rel 3e-6 
 Modulated Heat Diffusion 
 STNN 
 python train_stnn.py --dataset heat_m --outputdir output_heat_m --manualSeed 679 --xp stnn 
 STNN-R(efine) 
 python train_stnn.py --dataset heat_m --outputdir output_heat_m --manualSeed 3488 --xp stnn_r --mode refine --l1_rel 1e-5 
 STNN-D(iscovery) 
 python train_stnn_.py --dataset heat_m --outputdir output_m --xp test --manualSeed 7664 --mode discover --patience 500 --l1_rel 3e-6 
 Data format 
 The file  heat.csv  contains the raw temperature data. The 200 rows correspond to the 200 timestep, and the 41 columns are the 41 space points.
The file  heat_relations.csv  contains the spatial relation between the 41 space points. It is a 41 by 41 adjacency matrix  A , where  A(i, j)  = 1 means that series  i  is a direct neighbor of series  j  in space, and is 0 otherwise. Dynamic Neural Language Models 
 Code and supplementary material for the paper ""Dynamic Neural Language Models"" Dynamic Author Representation 
 Official implementation of the paper  Learning Dynamic Author Representations with Temporal Language Models 
 Edouard Delasalles, Sylvain Lamprier, Ludovic Denoyer 
 IEEE International Conference on Data Mining, ICDM 2019 
 Model 
 High level view | Detailed View
 :---: | :---: 
  |  
 x  are textual documents, and  h  are dynamic vector representation of authors. The representation vectors  h  evolve through time with a residual transition function  f . The temporal author representation vectors are fed to an LSTM language model to predict next token probability. 
 Requirements 
 
 Python >= 3.6 
 PyTorch 1.1.0 
 pytorch-pretrained-bert  0.6.1 
 
 Data 
 Data used in the paper are provided as  .tar.gz  archives in the  data  directory.
-  s2  corpus was processed from the  2018-05-03  version of the  Semantic Scholar corpus  [1]
-  nyt  corpus was processed from data provided by Yao et al. for their paper  Dynamic Word Embeddings for Evolving Semantic Discovery  [2] 
 Reproducing results from the paper 
 To reproduce the experiments in the paper, you can run the commands below. You will need to add the option  --xp_dir model/will/be/saved/here  when launching an experiment to specify a location to save the model to. You can also run an experiment on a GPU by specifying  --device n  where  n  is a GPU device id. 
 Semantic Scholar 
 Modeling: 
 python train.py --corpus s2 --task modeling --wd 0.0003 
 Imputation: 
 python train.py --corpus s2 --task imputation 
 Prediction: 
 python train.py --corpus s2 --task prediction --l2_a 1 
 New York Time 
 Modeling: 
 python train.py --corpus nyt --task modeling --dropoutw 0.5 --dropouto 0.5 --lr_scheduling_burnin 25000 --lr_scheduling_niter 5000 --nlayers_dyn 2 
 Imputation: 
 python train.py --corpus nyt --task imputation --dropoutw 0.5 --dropouto 0.5 --lr_scheduling_burnin 25000 --lr_scheduling_niter 5000 --nlayers_dyn 2 
 Prediction: 
 python train.py --corpus nyt --task prediction --dropoutw 0.5 --dropouto 0.5 --lr_scheduling_burnin 25000 --lr_scheduling_niter 5000 --nlayers_dyn 2 --l2_a 1 
 References 
 [1] Ammar, Waleed, et al. ""Construction of the Literature Graph in Semantic Scholar."" Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers). 2018. 
 [2] Yao, Zijun, et al. ""Dynamic word embeddings for evolving semantic discovery."" Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining. ACM, 2018. Stochastic Latent Residual Video Prediction (SRVP) 
 Official implementation of the paper  Stochastic Latent Residual Video Prediction  (Jean-Yves Franceschi,  Edouard Delasalles,  Mickael Chen, Sylvain Lamprier, Patrick Gallinari), accepted and presented at ICML 2020. 
 Article 
 Presentation 
 Preprint 
 Project Website 
 Pretrained Models 
 Requirements 
 All models were trained with Python 3.7.6 and PyTorch 1.4.0 using CUDA 10.1. 
 A list required Python packages is available in the  requirements.txt  file. 
 To speed up training, we recommend to activate mixed-precision training in the options, whose performance gains were tested on the most recent Nvidia GPU architectures (starting from Volta).
We used Nvidia's  Apex  (v0.1) in mixed-precision mode ( O1 ) to produce results reported in the paper.
We also integrated PyTorch's more recent  mixed-precision training package  (made available in PyTorch 1.6.0), which should give similar results.
This is, however, an experimental feature and we cannot guarantee that it achieves the same results as Apex. 
 Datasets 
 Stochastic Moving MNIST 
 During training, this dataset is generated on the fly.
In order to generate a consistent testing set in an  .npz  file, the following commands should be executed:
 bash
python -m preprocessing.mmnist.make_test_set --data_dir $DIR --seq_len 25 
for the stochastic version of the dataset, or
 bash
python -m preprocessing.mmnist.make_test_set --data_dir $DIR --deterministic --seq_len 100 
for the deterministic version, where  $DIR  is the directory where the testing set should be saved. 
 KTH 
 To download the dataset at a given path  $DIR , execute the following command:
 bash
bash preprocessing/kth/download.sh $DIR 
(see also  https://github.com/edenton/svg/blob/master/data/download_kth.sh  from the official implementation of  SVG ). 
 In order to respectively train and test a model on this dataset, the following commands should be run:
 bash
python preprocessing/kth/convert.py --data_dir $DIR 
and
 bash
python preprocessing/kth/make_test_set.py --data_dir $DIR 
 Human3.6M 
 This dataset can be downloaded at  http://vision.imar.ro/human3.6m/description.php , after obtaining access from its owners.
Videos for every subject are included in  .tgz  archives. Each of these archives should be extracted in the same folder. 
 To preprocess the dataset in order to use it for training and testing, these videos should be processed using the following command:
 bash
python preprocessing/human/convert.py --data_dir $DIR 
where  $DIR  is the directory where Human3.6M videos are saved. 
 Finally, the testing set is created by choosing extracts from testing videos, with the following command:
 bash
python preprocessing/human/make_test_set.py --data_dir $DIR 
 All processed videos are saved in the same folder as the original dataset. 
 BAIR 
 To download the dataset at a given path  $DIR , execute the following command:
 bash
bash preprocessing/bair/download.sh $DIR 
(see also  https://github.com/edenton/svg/blob/master/data/download_bair.sh  from the official implementation of  SVG ). 
 In order to respectively train and test a model on this dataset, the following command should be run:
 bash
python preprocessing/bair/convert.py --data_dir $DIR 
 Training 
 In order to launch training on multiple GPUs, launch the following command:
 bash
OMP_NUM_THREADS=$NUMWORKERS python -m torch.distributed.launch --nproc_per_node=$NBDEVICES train.py --device $DEVICE1 $DEVICE2 --seed $SEED ... 
followed by the training options, where  $NBDEVICES  is the number of GPUs to be used,  $NUMWORKERS  is the number of processes per GPU to use for data loading (should be equal to the value given to the option  n_workers ),  $DEVICE1 $DEVICE2 ...  is a list of GPU indices whose length in equal to  $NBDEVICES , and  $SEED  is the chosen random seed.
Training can be accelerated using options  --apex_amp  or  --torch_amp  (see  requirements ). 
 Data directory ( $DATA_DIR ) and saving path ( $SAVE_DIR ) must be given using options  --data_dir $DATA_DIR --save_path $SAVE_DIR . 
 Training parameters are given by the following options:
- for Stochastic Moving MNIST:
 --ny 20 --nz 20 --beta_z 2 --nt_cond 5 --nt_inf 5 --dataset smmnist --nc 1 --seq_len 15 
- for Deterministic Moving MNIST:
 --ny 20 --nz 20 --beta_z 2 --nt_cond 5 --nt_inf 5 --dataset smmnist --deterministic --nc 1 --seq_len 15 --lr_scheduling_burnin 800000 --lr_scheduling_n_iter 100000 
- for KTH:
 --ny 50 --nz 50 --n_euler_steps 2 --res_gain 1.2 --archi vgg --skipco --nt_cond 10 --nt_inf 3 --obs_scale 0.2 --batch_size 100 --dataset kth --nc 1 --seq_len 20 --lr_scheduling_burnin 150000 --lr_scheduling_n_iter 50000 --val_interval 5000 --seq_len_test 30 
- for Human3.6M:
 --ny 50 --nz 50 --n_euler_steps 2 --res_gain 1.2 --archi vgg --skipco --nt_cond 8 --nt_inf 3 --obs_scale 0.2 --batch_size 100 --dataset human --nc 3 --seq_len 16 --lr_scheduling_burnin 325000 --lr_scheduling_n_iter 25000 --val_interval 20000 --batch_size_test 8 --seq_len_test 53 
- for BAIR:
 --ny 50 --nz 50 --n_euler_steps 2 --archi vgg --skipco --nt_cond 2 --nt_inf 2 --obs_scale 0.71 --batch_size 192 --dataset bair --nc 3 --seq_len 12 --lr_scheduling_burnin 1000000 --lr_scheduling_n_iter 500000 
 Please also refer to the help message of  train.py :
 bash
python train.py --help 
which lists all options and hyperparameters to train SRVP models. 
 Testing 
 To evaluate a trained model, the script  test.py  should be used as follows:
 bash
python test.py --data_dir $DATADIR --xp_dir $XPDIR --lpips_dir $LPIPSDIR 
where  $XPDIR  is a directory containing a checkpoint and the corresponding  json  configuration file (see the pretrained models for an example),  $DATADIR  is the directory containing the test set, and  $LPIPSDIR  is a directory where  v0.1 LPIPS weights  (from the official repository of  The Unreasonable Effectiveness of Deep Features as a Perceptual Metric ) are downloaded. 
 To run the evaluation on GPU, use the option  --device $DEVICE . 
 Model file name can be specified using the option  --model_name $MODEL_NAME  (for instance, to load best models selected on the evaluation sets of KTH and Human3.6M:  --model_name model_best.pt ). 
 PSNR, SSIM and LPIPS results reported in the paper were obtained with the following options:
- for stochastic Moving MNIST:
 bash
python test.py --data_dir $DATADIR --xp_dir $XPDIR --lpips_dir $LPIPSDIR --nt_gen 25 
- for deterministic Moving MNIST:
 bash
python test.py --data_dir $DATADIR --xp_dir $XPDIR --lpips_dir $LPIPSDIR --n_samples 1 --nt_gen 100 
- for KTH:
 bash
python test.py --data_dir $DATADIR --xp_dir $XPDIR --lpips_dir $LPIPSDIR --nt_gen 40 
- for Human3.6M:
 bash
python test.py --data_dir $DATADIR --xp_dir $XPDIR --lpips_dir $LPIPSDIR --nt_gen 53 
- for BAIR:
 bash
python test.py --data_dir $DATADIR --xp_dir $XPDIR --lpips_dir $LPIPSDIR --nt_gen 30 
Adding option  --fvd  additionally computes FVD. 
 Please also refer to the help message of  test.py :
 bash
python test.py --help 
 Troubleshooting 
 It has been reported  that using Apex mixed-precision training in specific configurations may lead to an excessive RAM usage due to  this memory leak issue in Apex .
We refer to the links hereinabove for solutions to this problem. 
 Please feel free to create an issue for any other problem that you might encounter using our code."
edouardoyallon,"ScatNetLight v0.1 - June 2015 
 ScatNetLight is a MATLAB implementation of the Scattering Networks optimized for image classification on complex datasets. The software 'scatnet_light' and a classification pipeline are included in this version. 
 See homepage of the Scattering Networks:
http://www.di.ens.fr/data/scattering 
 Software used in the paper:
Deep Roto-Translation Scattering for Object Classification, Edouard Oyallon, Stéphane Mallat, accepted to CVPR 2015, http://arxiv.org/abs/1412.8659 
 Author contact: Edouard Oyallon, edouard.oyallon@ens.fr
Contributor: Matthew Hirn, matthew.hirn@ens.fr 
 Install ScatNetLight + Classification pipeline 
 
 launch 'load_path_software' from matlab shell 
 
 [optional] : add the following two lines to your startup.m file
so that matlab does the addpath automatically when it starts:  
 addpath /path/to/scatnet;
load_path_software;
 
 Quickstarts 
 See 'quick_tutorial.m' in the main folder or the 'tutorial' folder in scatnet_light. 
 You will have to modify the function 'create_config_caltech_101', 'create_config_caltech_256', 'create_config_cifar_10' and 'create_config_cifar_100'. The dataset will also have to be downloaded and put in separate folders. The software is outputting intermediary features that can take a consequent size on a harddriver. Also, a server with at least 256go of memory is recommended for most of the experiments, if you want to use the dimensionality reduction algorithm.(it can be removed by setting option.Classification.nAverageKernel and option.Classification.D equal to -1) 
 Copyright 
 This code is mainly based on the implementation of ScatNet v0.2 done by Laurent Sifre and Joakim Anden 
 The code for the Selesnick wavelet filters was written by Shihua Cai, Keyong Li, and Ivan Selesnick. It has been included in ScatNet with their permission. 
 The supervised algorithm of feedforward selection based on OLS has been written by Xiu Cheng for the class specificic part, Matthew Hirn for the selection of variable algorithm. 
 The SVM's code is an extension of libsvm written by Joakim Anden. 
 The code to compute kernel has been written by Jordi and Gustavo Camps and is included with their permission. THIS VERSION OF THE SCATTERING IS OBSOLETE 
 Please check out our new version (pytorch): https://github.com/edouardoyallon/pyscatwave - any version of the Scattering Transform in lua can be considered as obsolete. 
 
 ScatWave 
 ScatWave is a Torch implementation of 2D scattering using CUDA libraries, designed for images. 
 Disclaimer 
 This software belongs to the team DATA @ ENS, its main author is Edouard Oyallon. 
 How to install 
 Assuming Torch is already installed on your computer, simply cd in scatwave_pkg, then 'luarocks make'
Make sure you have FFTW and cuFFT installed and that the libraries are linked to the software. 
 Few results... 
 ScatWave + 3FC = 83.0 on CIFAR 10 
ScatWave + 3FC = 56.7 on CIFAR 100 
ScatWave + Deepnet = 91.4% on CIFAR10 
ScatWave + Deepnet = 69.5% on CIFAR100 
 Usage 
 scatwave = require 'scatwave' 
x=torch.FloatTensor(128,3,32,32) 
scat = scatwave.network.new(3,x:size()) 
scat_coeff = scat(x) -- or scat(x,1) 
 
You can go to cuda via: 
scat=scat:cuda() 
 Reproducing the paper 
 
 
 Data can be downloaded from this page: https://github.com/szagoruyko/wide-residual-networks/blob/master/README.md. 
The whitened versions work quite better and are used in this work. 
 
 
 training the network on cifar10: 
th train_cifar10.lua 
 
 
 training the network on cifar100: 
th train_cifar100.lua 
 
 
 transfering to matlab F1: 
th get_F1.lua 
 
 
 analysing the operator: 
matlab sparsify_F1.m 
 
 
 retraining the deepnet with a new F1: 
th retrain_with_fix_F1_pretrained_end_cifar10.lua 
 
 
 replace the scattering by a deepnet with a pretrained and fixed model: 
th replace_scattering_fix_end_cifar10.lua 
 
 
 Contributors 
 Edouard Oyallon. Contacts: firstname.lastname@ens.fr 
 Team DATA - Ecole Normale Supérieure 
 Acknowledgements 
 The author is thankful to Sergey Zagoruyko for helpfull discussions, codes and enlightments. Many parts of this work are based on codes that he shared, and this had a major impact on this work. I would like to thank also Mathieu Andreux, 
Eugene Belilovsky, Carmine Cella, Michael Eickenberg for helpful discussions. Building a Regular Classification Boundary with Deep Networks 
 This is the code for the CVPR17 paper  ""Building a Regular Classification Boundary with Deep Networks"" by Edouard Oyallon. A large part of the code is inspired from https://github.com/bgshih/tf_resnet_cifar yet it has been modified a lot. 
 To run all the experiments and obtain the figure of the paper, you can simply do: 
 bash script_nonlinearity_alpha.bash
python build_figure_paper.py 
 The best accuracy on CIFAR10 should be 95.4, and on CIFAR100 it should be 79.6, with n_channel equal to 512, alpha=1.0. 
 Acknowledgement 
 Code modified by Edouard Oyallon - Team DATA ENS, 2016 Announcement 
 11/18 
 This package is no longer supported. We have now released kymatio: http://www.kymat.io/ , https://github.com/kymatio/kymatio which includes 1D-2D-3D fast, optimized, differentiable Scattering Transform and subsumes all the behavior of pyscatwave. Among other things you can now more easily use differentiable 2d scattering and use the CPU if desired. kymatio will be well supported with a substantially larger development team than pyscatwave. 
 07/18 
 We just released a differentiable 2D Scattering example in the master. It is not memory efficient yet, neither fast. 
 PyScatWave 
 CuPy/PyTorch Scattering implementation 
 A scattering network is a Convolutional Network with filters predefined to be wavelets that are not learned and it can be used in vision task such as classification of images. The scattering transform can drastically reduce the spatial resolution of the input (e.g. 224x224->14x14) with demonstrably neglible loss in dicriminative power.    
 The software uses PyTorch + NumPy FFT on CPU, and PyTorch + CuPy + CuFFT on GPU. 
 Previous (lua-based) versions of the code can be found at  https://github.com/edouardoyallon/scatwave 
 If using this code for your research please cite our paper: 
 E. Oyallon, E. Belilovsky, S. Zagoruyko  Scaling the Scattering Transform: Deep Hybrid Networks 
 You can find experiments from the paper in the following repository:
https://github.com/edouardoyallon/scalingscattering/ 
 We used PyTorch for running experiments in  https://arxiv.org/abs/1703.08961 ,
but it is possible to use scattering with other frameworks (e.g. Chainer, Theano or Tensorflow) if one copies Scattering outputs to CPU (or run on CPU and convert to  numpy.ndarray  via  .numpy() ). 
 Benchmarks 
 We do some simple timings and comparisons to the previous (multi-core CPU) implementation of scattering (ScatnetLight). We benchmark the software using a 1080 GPU. Below we show input sizes (WxHx3xBatchSize) and speed: 
 32 × 32 × 3 × 128 (J=2)- 0.03s (speed of 8x vs ScatNetLight) 
 256 × 256 × 3 × 128 (J=2) - 0.71 s (speed up of 225x vs ScatNetLight) 
 Installation 
 The software was tested on Linux with anaconda Python 2.7 and
various GPUs, including Titan X, 1080s, 980s, K20s, and Titan X Pascal. 
 The first step is to install pytorch following instructions from
 http://pytorch.org , then you can run  pip : 
 pip install -r requirements.txt
python setup.py install 
 Usage 
 Example: 
 ```python
import torch
from scatwave.scattering import Scattering 
 scat = Scattering(M=32, N=32, J=2).cuda()
x = torch.randn(1, 3, 32, 32).cuda() 
 print scat(x).size()
``` 
 Contribution 
 All contributions are welcome. 
 Authors 
 Edouard Oyallon, Eugene Belilovsky, Sergey Zagoruyko Scaling The Scattering Transform : Deep Hybrid Networks 
 This repository contains the experiments found in the paper: https://arxiv.org/abs/1703.08961
 
 Requirements 
 In order to run our experiments you will need at minimum the following python packages: pytorch,opencv,pyscatwave package.
The simplest way to install pytorch and opencv is through anaconda. We recommend python 2.7 + anaconda.
The pyscatwave package can be found here https://github.com/edouardoyallon/pyscatwave 
 Imagenet 
 We provide a pre-trained model similar to the one described in the paper.  
 To run the trained model of scattering+resnet on imagenet ILSVRC validation set: 
 1) Make sure you have downloaded at least the validation set of ILSVRC2012 and have it organized by class categories
 Note : due to problems with pytorch dataset constructors make sure your imagenet directory has no hidden files, or extra directories besides the 1000 ILSVRC categories.. otherwise all the images will be mislabeled
2) Download the model file from  http://www.di.ens.fr/~oyallon/scatter_resnet_10_model.pt7
3) Add this to the imagenet/ directory
4) Run the script main_test.py to evaluate on the ILSVRC validation set specifying --imagenetpath to point to your imagenet directory 
 Training scripts for imagenet and SLE feature extractor will be added soon 
 STL-10 
 Simply run python main_STL.py script in the STL directory 
 CIFAR-10 
 To run the small sample experiments
Example: 
 bash
python main_small_sample_class_normalized.py --model resnet12_8_scat --save ""test""  --seed 1 --sampleSize 500 --mul 20 pyscatlight 
 This code is for the paper
http://researchers.lille.inria.fr/~valko/hp/publications/oyallon2018compressing.pdf 
 Please cite the following if you use this code in your paper
@InProceedings{Oyallon_2018_ECCV,
author = {Oyallon, Edouard and Belilovsky, Eugene and Zagoruyko, Sergey and Valko, Michal},
title = {Compressing the Input for CNNs with the First-Order Scattering Transform},
booktitle = {The European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
} 
 Installing pyscatlight: 
 pip install -r requirements.txt
python setup.py install 
 A typical call for classification looks like: 
 python main_scatMultiGPU.py PATHIMAGENET --arch scat50 
It is also possible to employ main_scatDistributed.py which requires NCCL. 
 For detection, a pretrained model can be found here: https://s3.amazonaws.com/modelzoo-networks/scatresnet50.tar.gz 
 A typical call for detection looks like: 
 python trainval_net_scat.py  --dataset pascal_voc --net scatnet  --cuda --save_dir FOLDER --nw 4 --lr 1e-2 --lr_decay_step 6 --bs  3 --epochs 10 --s 3 --mGPUs 
 Our detection pipeline relies on https://github.com/jwyang/faster-rcnn.pytorch . It was successfully re-tested for pascal
and coco using pytorch 0.4.0, with python 3.5 . For
the sake of reproducibility, the files that were modified or created at time of submission were: 
 detection/lib/roi_data_layer/roibatchLoader.py 
 detection/trainval_net_scat.py 
 detection/test_net_scat.py 
 detection/lib/model/utils/blob.py - has to be modified for COCO 
 detection/cfgs/scatnet.yml 
 detection/cfgs/scatnet_ls.yml 
 detection/lib/model/faster_rcnn/scatnet.py Kymatio: wavelet scattering in PyTorch 
 Kymatio is a Python package for wavelet scattering transforms, built on top of PyTorch. 
 
 
 
 Use Kymatio if you need a library that:
* integrates wavelet scattering in a deep learning architecture,
* supports 1-D, 2-D, and 3-D wavelets, and
* runs seamlessly on CPU and GPU hardware. 
 Website:  https://kymatio.github.io 
 Installation 
 Dependencies 
 Kymatio requires: 
 
 Python (>= 3.6) 
 PyTorch (>= 0.4) 
 SciPy (>= 0.13) 
 
 We also strongly recommend running Kymatio in a Conda environment since this
simplifies installation of PyTorch. 
 Linux 
 conda install pytorch torchvision -c pytorch
pip install -i https://test.pypi.org/simple/ kymatio==0.0.1 
 macOS 
 conda install pytorch torchvision -c pytorch
pip install -i https://test.pypi.org/simple/ kymatio==0.0.1 
 The software was tested on Linux with Anaconda Python 3 and
various GPUs, including Titan X, 1080s, 980s, K20s, and Titan X Pascal. 
 The software uses PyTorch + NumPy FFT on CPU, and PyTorch + CuPy + CuFFT on GPU. 
 If you use this code in your work please cite our paper: 
 The scattering authors,  Kymatio: Fast Scattering in 1-D,2-D,3-D 
 This code unifies multiple previous efforts:
    - PyScatWave/ScatWave,
    - ScatNetLight,
    - ScatNet, and others 
 Optimized package 
 If you have a CUDA-enabled GPU, you may run 
 pip install -r requirements_optional_cuda.txt 
 after installation to install the optimized  skcuda  backend. To enable it, set
the  KYMATIO_BACKEND  environment variable to  skcuda . For more information,
see the documentation. 
 Documentation 
 To build the documentation, please run 
 pip install -r requirements_optional.txt
cd doc; make clean; make html 
 You may then read the documentation in  doc/build/html/index.html . lazy-training-code 
 This code was based on https://github.com/kuangliu/pytorch-cifar .  
 Reproducing CNNs experiments 
 If you want to obtain CNN experiments accuracies and loss from the paper, simply run: 
 cd cnn
sh script.sh 
 The  double  precision experiments require a Tesla or Volta GPUs for handling this numerical precision at a reasonable speed... 
 Reproducing shallow experiments 
 All the codes necessary to reproduce the results from the paper as located in  shallow-nn 
 Contributions 
 All contributions are welcome. Interferometric Graph Transform 
 This repository contains the code corresponding to the ICML-2020 publication: https://arxiv.org/abs/2006.05722 
 If you have some troubles to reproduce some results, some questions regarding the code or even find a bug, please open directly an issue 
in this repository. If your questions concern some technical elements of the paper, please send directly an email to the author, firstname.name[AT]lip6.fr 
 If you wish to cite this paper, please use: 
 @inproceedings{oyallon2020interferometric,
  title={Interferometric Graph Transform: a Deep Unsupervised Graph Representation},
  author={Oyallon, Edouard},
  booktitle={37th International Conference on Machine Learning (ICML 2020)},
  year={2020}
} 
 Reproducing the numerical results 
 In order to reproduce the experiments of the paper  Interferometric Graph Transform: a Deep Unsupervised Graph Representation , 
make sure you have  pytorch  and  python 3  installed, as well as  MATLAB  (only for the Haar Scattering experiments) and please
 follow the next instructions. 
 Community detection experiments 
 Please replace (or add) the files of https://github.com/alelab-upenn/graph-scattering-transforms with the ones in the folder ""community-detection"". Then run the code as described in this GitHub repository, by adding the corresponding dataset, as described. 
 Images experiments 
 Please run the following instructions on a GPU: 
 CIFAR10/MNIST, graph 
 $DATASET=cifar/mnist ,  $PATH_TO_MODEL  refers to the path in which the first script stores the weight of the model.  
 python build_representation.py --K 40 --J 3 --lr_schedule '{0:1e0,500:1e-1,1000:1e-2,1500:1e-3}' --epochs 5  --dataset $DATASET
python classif.py --J 3 --feature interferometric --C 1e-3 --mode 1 --classifier 'svm'  --dataset $DATASET --path $PATH_TO_MODEL 
 CIFAR10, scattering 
 python classif.py --J 3 --feature scattering --C 1e-3 --mode 1 --classifier 'svm'  --dataset cifar 
 Graph experiments 
 NTU/SBU 
 Move the dataset (available  https://github.com/mazariahmed06/graph_skeletons ) in the same folder as the main file, then  $DATASET=NTU_xview_values/NTU_xsub_values/SBU ,
 $PATH_TO_MODEL  refers to the path in which the first script stores the weight of the model,  $ORDER  is the required order and 
 $K  the respective number of filters, which should end by 0 (e.g.,  $ORDER=1  leads to  $K=30,0  for instance). 
 Finally,  $LR_SCHEDULE  must be the learning rate schedule reported in the paper (as above). Add  --no_averaging  to remove the averaging from 
the experiments, yet this will substantially increase the computation time because the representation is larger. 
 python build_representation_graph.py --K $K --order $ORDER --lr_schedule $LR_SCHEDULE --epochs 5 --data_name $DATASET
python classif_graph.py --mode 'svm' --order 1 --data_name $DATASET --file $PATH_TO_MODEL  --K 10,5,0 --C 1e-1 
 Haar scattering 
 First download  HaarScat  from  https://www.di.ens.fr/data/software/ and then switch and run the scripts in the folder demo that we provided."
SeanNaren,"deepspeech.torch 
 
 
 Implementation of  Baidu Warp-CTC  using torch7.
Creates a network based on the  DeepSpeech2  architecture using the Torch7 library, trained with the CTC activation function. 
 Features 
 
 Train large models with large datasets via online loading using  LMDB  and multi-GPU support. 
 Supports variable length batches via padding. 
 Implements the  AN4 Audio database  (50 mins of data).
Has also been extended to train using the  LibriSpeech  dataset (1000 hours of data). Custom dataset preparation is explained in documentation. 
 
 Branches 
 There are currently two branches, Master and Phoneme:
* Master: This branch trains DeepSpeech2. Also included is an evaluation script which calculates the WER/CER, as well as a prediction script.
This branch is useful for understanding how the DeepSpeech and CTC works and is easy to run after installation. Highly recommended to checkout this branch.
* Phonemes: This branch is experimental and uses phonemes rather than character based predictions. This is fully credited and extended by  CCorfield  and his awesome work in porting to use phonemes. In addition to this
I'd like to also thank  Shane Walker  for his awesome recent conversion to use phonemes as well. 
 Installation/Data Preparation/Documentation 
 Follow Instructions/Data Preparation/Documentation found in the wiki  here  to set up and run the code. 
 Technical documentation can be found  here . 
 Pre-trained Networks 
 Pre-trained networks are available for AN4 as well as LibriSpeech for CUDA only (since they use cudnn RNNs). Download Links and accuracies are below. DeepSpeech-light is a smaller model which is less intensive to train (based on LSTMs rather than RNNs). 
 AN4 
 an4Test 
 |Network                | WER       | CER       |Link       |
|-----------------|:--------:|:--------:|:--------:|
|DeepSpeech-light| N/A     | N/A | N/A |
|DeepSpeech | 12    | 3.07 |  Download  | 
 LibriSpeech 
 Librispeech-test-clean 
 |Network                | WER       | CER       |Link       |
|-----------------|:--------:|:--------:|:--------:|
|DeepSpeech-light| 15     | 1.34 |  Download  |
|DeepSpeech | 12    | 1.55 |  Download  | 
 Librispeech-test-other 
 |Network                | WER       | CER       |Link       |
|-----------------|:--------:|:--------:|:--------:|
|DeepSpeech-light| 36    | 3.80 | (Download Above) |
|DeepSpeech | 33    | 3.24 | (Download Above) | 
 Once you're set up, you can start training from these nets by using the below parameters (you might need to change the other parameters described in the wiki) after setting the project up: 
 lua
th Train.lua -loadModel -loadPath /path/to/model.t7 
 Acknowledgements 
 Lots of people helped/contributed to this project that deserve recognition:
* Soumith Chintala for his support on Torch7 and the vast open source projects he has contributed that made this project possible!
* Charles Corfield for his work on the Phoneme Dataset and his overall contribution and aid throughout.
* Will Frey for his thorough communication and aid in the development process.
* Ding Ling, Yuan Yang and Yan Xia for their significant contribution to online training, multi-gpu support and many other important features.
* Erich Elsen and the team from Baidu for their contribution of Warp-CTC that made this possible, and the encouraging words and support given throughout the project.
* Maciej Korzepa for his huge help in training a model on Librispeech! QlearningExample.torch 
 
 Torch plays catch! Based on Eder Santanas'  implementation  in keras. Highly recommend reading his informative and easy to follow blog post  here . 
 Agent has to catch the fruit before it falls to the ground. Agent wins if he succeeds to catch the fruit, loses if he fails. 
 Dependencies 
 To install torch7 follow the guide  here . 
 Other dependencies can be installed via luarocks: 
 luarocks install optim
luarocks install image 
 How to run 
 To train a model, run the  Train.lua  script. You can configure parameters such as below: 
 th Train.lua -epoch 1000 #Configures the number of epochs. More parameters are available, check scrip.t 
 Visualization 
 To visualise the agent playing the game after training a model, use the  TorchPlaysCatch.lua  script using qlua rather than th as below: 
 qlua TorchPlaysCatch.lua 
 Much like the train script, there are configurable options. Check the script for more details! 
 Acknowledgements 
 Eder Santana, Keras plays catch, (2016), GitHub repository, https://gist.github.com/EderSantana/c7222daa328f0e885093 deepspeech.pytorch 
 
 Implementation of DeepSpeech2 for PyTorch using  PyTorch Lightning . The repo supports training/testing and inference using the  DeepSpeech2  model. Optionally a  kenlm  language model can be used at inference time. 
 Install 
 Several libraries are needed to be installed for training to work. I will assume that everything is being installed in
an Anaconda installation on Ubuntu, with PyTorch installed. 
 Install  PyTorch  if you haven't already. 
 If you want decoding to support beam search with an optional language model, install ctcdecode:
 git clone --recursive https://github.com/parlance/ctcdecode.git
cd ctcdecode && pip install . 
 Finally clone this repo and run this within the repo:
 pip install -r requirements.txt
pip install -e . # Dev install 
 If you plan to use Multi-node training, you'll need etcd. Below is the command to install on Ubuntu.
 sudo apt-get install etcd 
 Docker 
 To use the image with a GPU you'll need to have  nvidia-docker  installed. 
 bash
sudo docker run -ti --gpus all -v `pwd`/data:/workspace/data --tmpfs /tmp -p 8888:8888 --net=host --ipc=host seannaren/deepspeech.pytorch:latest # Opens a Jupyter notebook, mounting the /data drive in the container 
 Optionally you can use the command line by changing the entrypoint: 
 bash
sudo docker run -ti --gpus all -v `pwd`/data:/workspace/data --tmpfs /tmp --entrypoint=/bin/bash --net=host --ipc=host seannaren/deepspeech.pytorch:latest 
 Training 
 Datasets 
 Currently supports  AN4 ,  TEDLIUM ,  Voxforge ,  Common Voice  and  LibriSpeech . Scripts will setup the dataset and create manifest files used in data-loading. The scripts can be found in the data/ folder. Many of the scripts allow you to download the raw datasets separately if you choose so. 
 Training Commands 
 AN4 
 ```bash
cd data/ && python an4.py && cd .. 
 python train.py +configs=an4
``` 
 LibriSpeech 
 ```bash
cd data/ && python librispeech.py && cd .. 
 python train.py +configs=librispeech
``` 
 Common Voice 
 ```bash
cd data/ && python common_voice.py && cd .. 
 python train.py +configs=commonvoice
``` 
 TEDlium 
 ```bash
cd data/ && python ted.py && cd .. 
 python train.py +configs=tedlium
``` 
 Custom Dataset 
 To create a custom dataset you must create a JSON file containing the locations of the training/testing data. This has to be in the format of:
 json
{
  ""root_path"":""path/to"",
  ""samples"":[
    {""wav_path"":""audio.wav"",""transcript_path"":""text.txt""},
    {""wav_path"":""audio2.wav"",""transcript_path"":""text2.txt""},
    ...
  ]
} 
Where the  root_path  is the root directory,  wav_path  is to the audio file, and the  transcript_path  is to a text file containing the transcript on one line. This can then be used as stated below. 
 Note on CSV files ... 
 Up until release  V2.1 , deepspeech.pytorch used CSV manifest files instead of JSON.
These manifest files are formatted similarly as a 2 column table:
 /path/to/audio.wav,/path/to/text.txt
/path/to/audio2.wav,/path/to/text2.txt
... 
Note that this format is incompatible  V3.0  onwards. 
 Merging multiple manifest files 
 To create bigger manifest files (to train/test on multiple datasets at once) we can merge manifest files together like below. 
 cd data/
python merge_manifests.py manifest_1.json manifest_2.json --out new_manifest_dir 
 Modifying Training Configs 
 Configuration is done via  Hydra . 
 Defaults can be seen in  config.py . Below is how you can override values set already: 
 python train.py data.train_path=data/train_manifest.json data.val_path=data/val_manifest.json 
 Use  python train.py --help  for all parameters and options. 
 You can also specify a config file to keep parameters stored in a yaml file like so: 
 Create folder  experiment/  and file  experiment/an4.yaml :
 yaml
data:
  train_path: data/an4_train_manifest.json
  val_path: data/an4_val_manifest.json 
 python train.py +experiment=an4 
 To see options available, check  here . 
 Multi-GPU Training 
 We support single-machine multi-GPU training via  PyTorch Lightning . 
 Below is an example command when training on a machine with 4 local GPUs: 
 python train.py +configs=an4 trainer.gpus=4 
 Multi-Node Training 
 Also supported is multi-machine capabilities using TorchElastic. This requires a node to exist as an explicit etcd host (which could be one of the GPU nodes but isn't recommended), a shared mount across your cluster to load/save checkpoints and communication between the nodes. 
 Below is an example where we've set one of our GPU nodes as our etcd host however if you're scaling up, it would be suggested to have a separate instance as your etcd instance to your GPU nodes as this will be a single point of failure. 
 Assumed below is a shared drive called /share where we save our checkpoints and data to access. 
 Run on the etcd host:
 PUBLIC_HOST_NAME=127.0.0.1 # Change to public host name for all nodes to connect
etcd --enable-v2 \
     --listen-client-urls http://$PUBLIC_HOST_NAME:4377 \
     --advertise-client-urls http://$PUBLIC_HOST_NAME:4377 \
     --listen-peer-urls http://$PUBLIC_HOST_NAME:4379 
 Run on each GPU node:
 python -m torchelastic.distributed.launch \
        --nnodes=2 \
        --nproc_per_node=4 \
        --rdzv_id=123 \
        --rdzv_backend=etcd \
        --rdzv_endpoint=$PUBLIC_HOST_NAME:4377 \
        train.py data.train_path=/share/data/an4_train_manifest.json \
                 data.val_path=/share/data/an4_val_manifest.json model.precision=half \
                 data.num_workers=8 checkpoint.save_folder=/share/checkpoints/ \
                 checkpoint.checkpoint=true checkpoint.load_auto_checkpoint=true checkpointing.save_n_recent_models=3 \
                 data.batch_size=8 trainer.max_epochs=70 \
                 trainer.accelerator=ddp trainer.gpus=4 trainer.num_nodes=2 
 Using the  load_auto_checkpoint=true  flag we can re-continue training from the latest saved checkpoint. 
 Currently it is expected that there is an NFS drive/shared mount across all nodes within the cluster to load the latest checkpoint from. 
 Augmentation 
 There is support for three different types of augmentations: SpecAugment, noise injection and random tempo/gain perturbations. 
 SpecAugment 
 Applies simple Spectral Augmentation techniques directly on Mel spectogram features to make the model more robust to variations in input data. To enable SpecAugment, use the  --spec-augment  flag when training. 
 SpecAugment implementation was adapted from  this  project. 
 Noise Injection 
 Dynamically adds noise into the training data to increase robustness. To use, first fill a directory up with all the noise files you want to sample from.
The dataloader will randomly pick samples from this directory. 
 To enable noise injection, use the  --noise-dir /path/to/noise/dir/  to specify where your noise files are. There are a few noise parameters to tweak, such as
 --noise_prob  to determine the probability that noise is added, and the  --noise-min ,  --noise-max  parameters to determine the minimum and maximum noise to add in training. 
 Included is a script to inject noise into an audio file to hear what different noise levels/files would sound like. Useful for curating the noise dataset. 
 python noise_inject.py --input-path /path/to/input.wav --noise-path /path/to/noise.wav --output-path /path/to/input_injected.wav --noise-level 0.5 # higher levels means more noise 
 Tempo/Gain Perturbation 
 Applies small changes to the tempo and gain when loading audio to increase robustness. To use, use the  --speed-volume-perturb  flag when training. 
 Checkpoints 
 Typically checkpoints are stored in  lightning_logs/  in the current working directory of the script. 
 This can be adjusted: 
 python train.py checkpoint.file_path=save_dir/ 
 To load a previously saved checkpoint: 
 python train.py trainer.resume_from_checkpoint=lightning_logs/deepspeech_checkpoint_epoch_N_iter_N.ckpt 
 This continues from the same training state. 
 Testing/Inference 
 To evaluate a trained model on a test set (has to be in the same format as the training set): 
 python test.py model.model_path=models/deepspeech.pth test_path=/path/to/test_manifest.json 
 An example script to output a transcription has been provided: 
 python transcribe.py \
       model.model_path=models/deepspeech.pth \
       model.cuda=True \
       chunk_size_seconds=-1 \
       audio_path=audio_path=/path/to/audio.wav 
 If you used mixed-precision or half precision when training the model, you can use the  model.precision=half  for a speed/memory benefit. If you want to transcribe a long audio file that does not fit in the GPU, change the value of  chunk_size_seconds  to a positive number which represents the chunk size in seconds that will be used to segment the long audio file based on it. 
 Inference Server 
 Included is a basic server script that will allow post request to be sent to the server to transcribe files. 
 ```
python server.py --host 0.0.0.0 --port 8000 # Run on one window 
 curl -X POST http://0.0.0.0:8000/transcribe -H ""Content-type: multipart/form-data"" -F ""file=@/path/to/input.wav""
``` 
 Using an ARPA LM 
 We support using kenlm based LMs. Below are instructions on how to take the LibriSpeech LMs found  here  and tune the model to give you the best parameters when decoding, based on LibriSpeech. 
 Tuning the LibriSpeech LMs 
 First ensure you've set up the librispeech datasets from the data/ folder.
In addition download the latest pre-trained librispeech model from the releases page, as well as the ARPA model you want to tune from  here . For the below we use the 3-gram ARPA model (3e-7 prune). 
 First we need to generate the acoustic output to be used to evaluate the model on LibriSpeech val.
 python test.py data.test_path=data/librispeech_val_manifest.json model.model_path=librispeech_pretrained_v2.pth save_output=librispeech_val_output.npy 
 We use a beam width of 128 which gives reasonable results. We suggest using a CPU intensive node to carry out the grid search. 
 python search_lm_params.py --num-workers 16 --saved-output librispeech_val_output.npy --output-path libri_tune_output.json --lm-alpha-from 0 --lm-alpha-to 5 --lm-beta-from 0 --lm-beta-to 3 --lm-path 3-gram.pruned.3e-7.arpa  --model-path librispeech_pretrained_v2.pth --beam-width 128 --lm-workers 16 
 This will run a grid search across the alpha/beta parameters using a beam width of 128. Use the below script to find the best alpha/beta params: 
 python select_lm_params.py --input-path libri_tune_output.json 
 Use the alpha/beta parameters when using the beam decoder. 
 Building your own LM 
 To build your own LM you need to use the KenLM repo found  here . Have a read of the documentation to get a sense of how to train your own LM. The above steps once trained can be used to find the appropriate parameters. 
 Alternate Decoders 
 By default,  test.py  and  transcribe.py  use a  GreedyDecoder  which picks the highest-likelihood output label at each timestep. Repeated and blank symbols are then filtered to give the final output. 
 A beam search decoder can optionally be used with the installation of the  ctcdecode  library as described in the Installation section. The  test  and  transcribe  scripts have a  lm  config. To use the beam decoder, add  lm.decoder_type=beam . The beam decoder enables additional decoding parameters:
-  lm.beam_width  how many beams to consider at each timestep
-  lm.lm_path  optional binary KenLM language model to use for decoding
-  lm.alpha  weight for language model
-  lm.beta  bonus weight for words 
 Time offsets 
 Use the  offsets=true  flag to get positional information of each character in the transcription when using  transcribe.py  script. The offsets are based on the size
of the output tensor, which you need to convert into a format required.
For example, based on default parameters you could multiply the offsets by a scalar (duration of file in seconds / size of output) to get the offsets in seconds. 
 Pre-trained models 
 Pre-trained models can be found under releases  here . 
 Acknowledgements 
 Thanks to  Egor  and  Ryan  for their contributions! 
 In Chinese 中文版 
 warp-ctc 
 A fast parallel implementation of CTC, on both CPU and GPU. 
 Introduction 
 Connectionist Temporal Classification 
is a loss function useful for performing supervised learning on sequence data,
without needing an alignment between input data and labels.  For example, CTC
can be used to train
 end-to-end 
 systems  for
 speech recognition ,
which is how we have been using it at Baidu's Silicon Valley AI Lab. 
 
 The illustration above shows CTC computing the probability of an output
sequence ""THE CAT "", as a sum over all possible alignments of input sequences
that could map to ""THE CAT "", taking into account that labels may be duplicated
because they may stretch over several time steps of the input data (represented by
the spectrogram at the bottom of the image).
Computing the sum of all such probabilities explicitly would be prohibitively costly due to the
combinatorics involved, but CTC uses dynamic programming to dramatically
reduce the complexity of the computation. Because CTC is a differentiable function,
it can be used during standard SGD training of deep neural networks. 
 In our lab, we focus on scaling up recurrent neural networks, and CTC loss is an
important component. To make our system efficient, we parallelized the CTC
algorithm, as described in  this paper .
This project contains our high performance CPU and CUDA versions of the CTC loss,
along with bindings for  Torch .
The library provides a simple C interface, so that it is easy to
integrate into deep learning frameworks. 
 This implementation has improved training scalability beyond the
performance improvement from a faster parallel CTC implementation. For
GPU-focused training pipelines, the ability to keep all data local to
GPU memory allows us to spend interconnect bandwidth on increased data
parallelism. 
 Performance 
 Our CTC implementation is efficient compared with many of the other publicly available implementations.  It is
also written to be as numerically stable as possible.  The algorithm is numerically sensitive and we have observed
catastrophic underflow even in double precision with the standard calculation - the result of division of 
two numbers on the order of 1e-324 which should have been approximately one, instead become infinity 
when the denominator underflowed to 0.  Instead, by performing the calculation in log space, it is numerically
stable even in single precision floating point at the cost of significantly more expensive operations.  Instead of
one machine instruction, addition requires the evaluation of multiple transcendental functions.  Because of this,
the speed of CTC implementations can only be fairly compared if they are both performing the calculation the same
way. 
 We compare our performance with  Eesen , 
a CTC implementation built on 
 Theano ,
and a Cython CPU only implementation  Stanford-CTC .
We benchmark the Theano implementation operating on 32-bit floating-point numbers and doing the calculation in log-space,
in order to match the other implementations we compare against.  Stanford-CTC was modified to perform the calculation
in log-space as it did not support it natively.  It also does not support minibatches larger than 1, so would require
an awkward memory layout to use in a real training pipeline, we assume linear increase in cost with minibatch size. 
 We show results on two problem sizes relevant to our English and Mandarin end-to-end models, respectively, where  T  represents the number of timesteps in the input to CTC,  L  represents the length of the labels for each example, and  A  represents the alphabet size. 
 On the GPU, our performance at a minibatch of 64 examples ranges from 7x faster to 155x faster than Eesen, and 46x to 68x faster than the Theano implementation. 
 GPU Performance 
 Benchmarked on a single NVIDIA Titan X GPU. 
 |  T =150,  L =40,  A =28           | warp-ctc  | Eesen   | Theano  |
|-----------------------------------|-------|---------|---------|
|  N =1                             | 3.1 ms| .5 ms   | 67 ms |
|  N =16                            | 3.2 ms| 6  ms   | 94 ms |
|  N =32                            | 3.2 ms| 12 ms   | 119 ms |
|  N =64                            | 3.3 ms| 24 ms   | 153 ms |
|  N =128                           | 3.5 ms| 49 ms   | 231 ms | 
 |  T =150,  L =20,  A =5000         | warp-ctc  | Eesen   | Theano  |
|-----------------------------------|-------|---------|---------|
|  N =1                             | 7 ms  | 40   ms | 120 ms |
|  N =16                            | 9 ms  | 619  ms | 385 ms |
|  N =32                            | 11 ms | 1238 ms | 665 ms |
|  N =64                            | 16 ms | 2475 ms | 1100 ms |
|  N =128                           | 23 ms | 4950 ms | 2100 ms | 
 CPU Performance 
 Benchmarked on a dual-socket machine with two Intel E5-2660 v3
processors - warp-ctc used 40 threads to maximally take advantage of the CPU resources.
Eesen doesn't provide a CPU implementation. We noticed that the Theano implementation was not
parallelizing computation across multiple threads.  Stanford-CTC provides no mechanism
for parallelization across threads. 
 |  T =150,  L =40,  A =28           | warp-ctc  | Stanford-CTC   | Theano  |
|-----------------------------------|-------|---------|---------|
|  N =1                             | 2.6 ms|  13 ms  | 15 ms |
|  N =16                            | 3.4 ms|  208 ms | 180 ms |
|  N =32                            | 3.9 ms|  416 ms | 375 ms |
|  N =64                            | 6.6 ms|  832 ms | 700 ms |
|  N =128                           |12.2 ms| 1684 ms | 1340 ms | 
 |  T =150,  L =20,  A =5000         | warp-ctc  | Stanford-CTC   | Theano  |
|-----------------------------------|-------|---------|---------|
|  N =1                             | 21 ms |  31 ms  | 850 ms  |
|  N =16                            | 37 ms |  496 ms | 10800 ms|
|  N =32                            | 54 ms |  992 ms | 22000 ms|
|  N =64                            | 101 ms| 1984 ms | 42000 ms|
|  N =128                           | 184 ms| 3968 ms | 86000 ms| 
 Interface 
 The interface is in  include/ctc.h .
It supports CPU or GPU execution, and you can specify OpenMP parallelism
if running on the CPU, or the CUDA stream if running on the GPU. We
took care to ensure that the library does not perform memory
allocation internally, in order to avoid synchronizations and
overheads caused by memory allocation. 
 Compilation 
 warp-ctc has been tested on Ubuntu 14.04 and OSX 10.10.  Windows is not supported
at this time. 
 First get the code: 
 git clone https://github.com/baidu-research/warp-ctc.git
cd warp-ctc 
 create a build directory: 
 mkdir build
cd build 
 if you have a non standard CUDA install  export CUDA_BIN_PATH=/path_to_cuda  so that CMake detects CUDA and
to ensure Torch is detected, make sure  th  is in  $PATH 
 run cmake and build: 
 cmake ../
make 
 The C library and torch shared libraries should now be built along with test
executables.  If CUDA was detected, then  test_gpu  will be built;  test_cpu 
will always be built. 
 Tests 
 To run the tests, make sure the CUDA libraries are in  LD_LIBRARY_PATH  ( DYLD_LIBRARY_PATH  for OSX). 
 The Torch tests must be run from the  torch_binding/tests/  directory. 
 Torch Installation 
 luarocks make torch_binding/rocks/warp-ctc-scm-1.rockspec 
 You can also install without cloning the repository using 
 luarocks install http://raw.githubusercontent.com/baidu-research/warp-ctc/master/torch_binding/rocks/warp-ctc-scm-1.rockspec 
 There is a Torch CTC  tutorial . 
 Contributing 
 We welcome improvements from the community, please feel free to submit pull
requests. 
 Known Issues  / Limitations 
 The CUDA implementation requires a device of at least compute capability 3.0. 
 The CUDA implementation supports a maximum label length of 639 (timesteps are
unlimited). CORD-19-ANN 
 
     GitHub Pages 
 This repo contains the scripts and models to search  CORD-19  using  S-BERT  embeddings via  nmslib  or  faiss . 
 Sentence embeddings are not perfect for searching (see  this issue ) however can provide insight into the data that basic search functionality cannot. There is still room to improve the retrieval of relevant documents. 
 We're not versed in the medical field, so any feedback or improvements we deeply encourage in the form of issues/PRs! 
 We've included pre-trained models and the FAISS index to start your own server with instructions below. 
 Finally we provide a front-end that can be used to search through the dataset and extract information via a UI. Instructions and installation for the front-end can be found  here . 
 We currently are hosting the server on a gcp instance, if anyone can contribute for a more permanent hosting solution it would be appreciated. 
 Installation 
 Source 
 We assume you have installed PyTorch and the necessary CUDA packages from  here . We suggest using Conda to make installation easier.
``` 
 Install FAISS 
 conda install faiss-cpu -c pytorch # Other instructions can be found at https://github.com/facebookresearch/faiss/blob/master/INSTALL.md 
 git clone https://github.com/SeanNaren/CORD-19-ANN.git --recursive
cd CORD-19-ANN/
pip install -r requirements.txt
pip install .
``` 
 Docker 
 We also provide a docker container: 
 docker pull seannaren/cord-19-ann
sudo docker run -it --net=host --ipc=host --entrypoint=/bin/bash --rm seannaren/cord-19-ann 
 Download Models 
 We currently offer sentence models trained on  BlueBERT  (base uncased model) and  BioBERT  (base cased model) with the appropriate metadata/index. We currently serve S-BlueBERT however it is interchangeable. 
 Download S-BERT Models and Search Index 
 Download the corresponding Model and Index file. We suggest using S-BioBERT and assume you have done so for the subsequent commands. They are interchangeable however. 
 | Model                       | Index                          | Test MedNLI Accuracy | Test STS Benchmark Cosine Pearson |
|-----------------------------|--------------------------------|-----------------|------------------------------|
|  S-BioBERT Base Cased     |  BioBERT_faiss_PCAR128_SQ8   | 0.7482          | 0.7122                       |
|  S-BlueBERT Base Uncased  |  BlueBERT_faiss_PCAR128_SQ8  | 0.7525          | 0.6923                       |
| S-Bert Base Cased             |                                | 0.5689          | 0.7265                       | 
 Download Metadata 
 wget https://github.com/SeanNaren/CORD-19-ANN/releases/download/V1.0/cord_19_dataset_formatted_2020_03_27.tar.gz
tar -xzvf cord_19_dataset_formatted_2020_03_27.tar.gz cord_19_dataset_formatted/ 
 Searching the Index 
 We assume you've chosen the s-biobert model, it should be straightforward to swap in any other pre-trained models offered in this repo by modifying the paths below. 
 We recommend using the server but we do offer a simple script to search given a text file of sentences: 
 echo ""These RNA transcripts may be spliced to give rise to mRNAs encoding the envelope (Env) glycoproteins (Fig. 1a)"" > sentences.txt
python search_index.py --index_path biobert_mli_faiss_PCAR128_SQ8 --index_type faiss --model_name_or_path s-biobert_base_cased_mli/ --dataset_path cord_19_dataset_formatted/ --input_path sentences.txt --output_path output.json 
 Using the server 
 To start the server:
 YOUR_IP=0.0.0.0
YOUR_PORT=1337
python index_server.py --index_path biobert_mli_faiss_PCAR128_SQ8 --index_type faiss --model_name_or_path s-biobert_base_cased_mli/ --dataset_path cord_19_dataset_formatted/ --address $YOUR_IP --port $YOUR_PORT --silent 
 To test the server:
 curl --header ""Content-Type: application/json"" \
  --request POST \
  --data '[""These RNA transcripts may be spliced to give rise to mRNAs encoding the envelope (Env) glycoproteins (Fig. 1a)""]' \
  http://$YOUR_IP:$YOUR_PORT/query 
 Output Format 
 The output from the index is a JSON object containing the top K hits from the index, an example of the API is given below: 
 [
  {
    ""query"": ""These RNA transcripts may be spliced to give rise to mRNAs encoding the envelope (Env) glycoproteins (Fig. 1a)"",
    ""hits"": [
      {
        ""title"": ""Title"",
        ""authors"": [
          ""...""
        ],
        ""abstract"": [
          ""...""
        ],
        ""paragraph"": ""Paragraph that included the hit"",
        ""sentence"": ""The semantically similar sentence"",
        ""distance"": 42,
      }
    ]
  }
] 
 Creating the Index from scratch 
 The process requires a GPU enabled node such as a GCP n8 node with a nvidia-tesla-v100 to generate the embeddings, with at-least 20GB RAM. 
 Preparing the dataset 
 Currently we tokenize at the sentence level using SciSpacy, however future work may look into using paragraph level tokenization. 
 mkdir datasets/
python download_data.py
python extract_sentences.py --num_workers 16 
 Generating embeddings 
 Using fine-tuned BioBERT/BlueBERT 
 Using sentence-transformers we can fine-tune either model. BlueBERT offers only uncased models whereas BioBERT offer a cased model. We've converted them into PyTorch format and included them in releases, to download: 
 wget https://github.com/SeanNaren/CORD-19-ANN/releases/download/V1.0/s-biobert_base_cased_mli.tar.gz
wget https://github.com/SeanNaren/CORD-19-ANN/releases/download/V1.0/s-bluebert_base_uncased_mli.tar.gz
tar -xzvf s-biobert_base_cased_mli.tar.gz
tar -xzvf s-bluebert_base_uncased_mli.tar.gz 
 Using Pre-trained BioBERT/BlueBERT 
 python generate_embeddings.py --model_name_or_path s-biobert_base_cased_mli/ --embedding_path biobert_embeddings.npy --device cuda --batch_size 256 # If you want to use biobert
python generate_embeddings.py --model_name_or_path s-bluebert_base_uncased_mli/ --embedding_path bluebert_embeddings.npy --device cuda --batch_size 256 # If you want to use bluebert 
 Using pre-trained S-BERT models 
 You can also use the standard pre-trained model from the S-BERT repo like below, however we suggest using the fine-tuned models offered in this repo. 
 python generate_embeddings.py --model_name_or_path bert-base-nli-mean-tokens --embedding_path pretrained_embeddings.npy --device cuda --batch_size 256 
 Training the model from scratch 
 This takes a few hours on a V100 GPU. 
 If you'd like to include the MedNLI dataset during training, you'll need to download the dataset from  here . Getting access requires credentialed access which requires some efforts and a waiting period of up to two weeks. 
 Once trained the model is saved to the  output/  folder by default. Inside there you'll find checkpoints such as  output/training_nli/biobert-2020-03-30_10-51-49/  after training has finished. Use this as the model path when generating your embeddings. 
 ```
wget https://github.com/SeanNaren/CORD-19-ANN/releases/download/V1.0/biobert_cased_v1.1.tar.gz
wget https://github.com/SeanNaren/CORD-19-ANN/releases/download/V1.0/bluebert_base_uncased.tar.gz
tar -xzvf biobert_cased_v1.1.tar.gz
tar -xzvf bluebert_base_uncased.tar.gz 
 mkdkir datasets/
python sentence-transformers/examples/datasets/get_data.py --output_path datasets/
python sentence-transformers/examples/training_nli_transformers.py --model_name_or_path biobert_cased_v1.1/
python sentence-transformers/examples/training_nli_transformers.py --model_name_or_path bluebert_base_uncased/ --do_lower_case 
 Training with medNLI 
 python sentence-transformers/examples/training_nli_transformers.py --model_name_or_path biobert_cased_v1.1/ --mli_dataset_path path/to/mednli/
python sentence-transformers/examples/training_nli_transformers.py --model_name_or_path bluebert_base_uncased/ --mli_dataset_path path/to/mednli/ --do_lower_case
``` 
 To exclude the MedNLI but still evaluate on the data (still requires the MedNLI dataset), use the  --exclude_mli . 
 Create the Index 
 We have the ability to use faiss or nmslib given the parameter below. We've exposed the FAISS config string for modifying the index. More details about selecting the index can be seen  here . 
 python create_index.py --output_path index --embedding_path pretrained_embeddings.npy --index_type faiss # Swap to scibert_embeddings.npy if using fine-tuned SciBERT embeddings 
 Clustering 
 We also took the example clustering script out of sentence-transformers and added it to this repository for using the pre-trained models. An example below: 
 python cluster_sentences.py --input_path sentences.txt --model_name_or_path biobert_cased_v1.1/ --device cpu 
 There is also a more interactive version available using the Google Colab demo:    
 Acknowledgements 
 Thanks to the authors of the various libraries that made this possible! 
 
 sentence-transformers 
 cord-19 
 scibert 
 nmslib 
 FAISS 
 Lightning Barlow Twins 
 
 
 
 This is a  PyTorch Lightning  port of the  Barlow Twins implementation  release by Facebook Research. 
 Hyper-parameters have been set based on commands in the PyTorch Barlow Twins implementation README. 
 Usage 
 pip install -r requirements.txt 
 Training 
 Train your own backbone on the ImageNet dataset (look inside  train.py  to use CIFAR10 dataset instead). Requires you to have ImageNet downloaded, instructions  here . 
 Have a look at the  PyTorch Lightning Trainer  documentation for more flags to enable. 
 Note that the Facebook released ResNet pre-trained weights is trained with a total effective batch size of  2048 . Modify the batch size and the number of GPUs based on compute, with a helpful table  here  which can be used to estimate training times.   
 python train.py --gpus 8 --batch_size 256 
 Linear Evaluation 
 Run linear evaluation on the  pre-trained ResNet weights  provided by Facebook Research. Requires you to have ImageNet downloaded, instructions  here . Look inside  evaluate.py  to swap to CIFAR10. 
 python evaluate.py --gpus 8 --batch_size 256 
 Run linear evaluation on a trained BarlowTwins  LightningModule :
 python evaluate.py --gpus 8 --batch_size 256 --model_path /path/to/lightning_logs/model.ckpt 
 To customize parameters, look at  train.py  and  evaluate.py  respectively. 
 License 
 Unfortunately I've had to duplicate the licence from the original Barlow Twins Pytorch Implementation, which is a restrictive non-commercial licence :( 
 This project is under the CC-BY-NC 4.0 license. See  LICENSE  for details. 
 Citations 
 Thanks to  Phoeby  for the illustration :) 
 @article{zbontar2021barlow,
  title={Barlow Twins: Self-Supervised Learning via Redundancy Reduction},
  author={Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, St{\'e}phane},
  journal={arXiv preprint arXiv:2103.03230},
  year={2021}
} min-LLM 
 Minimal code to train a relatively large language model (1-10B parameters). 
 
 Minimal codebase to learn and adapt for your own use cases 
 Concise demonstration of tricks to optimally train a larger language model 
 Allows exploration of compute optimal models at smaller sizes based on realistic scaling laws 
 
 The project was inspired by  megatron  and all sub-variants. This repo can be seen as a condensed variant, where some of the very large scaling tricks are stripped out for the sake of readability/simplicity. 
 For example, the library does not include Tensor Parallelism/Pipeline Parallelism. If you need to reach those 100B+ parameter models, I suggest looking at  megatron . 
 Setup 
 Make sure you're installing/running on a CUDA supported machine. 
 To improve performance, we use a few fused kernel layers from Apex (if you're unsure what fused kernels are for, I highly suggest  this  blogpost). 
 git clone https://github.com/NVIDIA/apex
cd apex
pip install -v --disable-pip-version-check --no-cache-dir --global-option=""--cpp_ext"" --global-option=""--cuda_ext"" ./ 
 Install the rest of the requirements: 
 pip install -r requirements.txt 
 Train 
 To train a 1.5B parameter model based on the Megatron architecture sizes using 8 GPUs (model will not fit on 1 GPU with optimal throughput, we scale to multiple). 
 deepspeed --num_gpus 8 train.py --batch_size_per_gpu 16   
 References 
 Code:  
 
 minGPT  - A lot of the base code was borrowed and extended from this awesome library 
 microGPT  - A helpful example with xFormers 
 Megatron-DeepSpeed  - Learning the use of Deepspeed with the Megatron architecture/3d parallelism. 
 
 Papers: 
 
 Efficient Large-Scale Language Model Training on GPU Clusters
Using Megatron-LM 
 Training Compute-Optimal Large Language Models 
 What Language Model to Train if You Have One Million GPU Hours? 
"
alykhantejani,"Fraction 
 An Fraction representation for Java to avoid floating point errors over multiple operations by representing numbers as fractions. 
 Example: 
 Normally you would do this:
    double a = 1.2;
    double b = 2.4;
    double c = a + b;
    System.out.println(c);
and you would get this output:
    3.5999999999999996 
 Using fractions you can do this:
    Fraction a = new Fraction(6, 5);
    Fraction b = new Fraction(12, 5);
    Fraction c = Fraction.add(a, b);
    System.out.println(c.toDouble());
and you will get:
    3.6 
 Build Instructions: 
 Fraction is built with Gradle, if you havent got this installed you can grab the latest copy from: http://www.gradle.org/downloads.html 
 To build simply type: gradle build
Or if you use eclipse, to set up an eclipse project type: gradle eclipse SemanticTextonForest 
 Semantic Texton Forest Implementation (Shotton et al.) Minimal Mistakes 
 Minimal Mistakes  is a two column responsive Jekyll theme perfect for powering your GitHub hosted blog.  
 Minimal Mistakes is all about: 
 
 Responsive templates. Looking good on mobile, tablet, and desktop. 
 Gracefully degrading in older browsers. Compatible with Internet Explorer 8+ and all modern browsers.  
 Minimal embellishments -- content first. 
 Optional large feature images for posts and pages. 
 Simple and clear permalink structure. 
 Custom 404 page  to get you started. 
 Support for Disqus Comments 
 
 
 See a  live version of Minimal Mistakes  hosted on GitHub. 
 Getting Started 
 Minimal Mistakes takes advantage of Sass and data files to make customizing easier. These features require Jekyll 2.x and will not work with older versions of Jekyll. 
 To learn how to install and use this theme check out the  Setup Guide  for more information. siamese_network 
 A Siamese network implementation in torch (simple example on MNIST to embed to 2D space) tweetbot 
 A bot to generate 140 character tweets (and possibly tweet them) 
 This will use a Recurrent Neural Network (RNN) based of  Andreij Karpathy's awesome char-rnn . 
 More details to follow... generate_function_samples 
 A python script to generate samples from a given function. For example, for the function (The Maclaurin expansion  sin(x) ), 
 y = (2.5*x - (2.5*x^3)/6 + (2.5*x^5)/120)  
We can plot 15 samples using this equation, with added jitter of  +/- 0.5 . We can also plot the function across the whole range without jitter on the same plot with the command: 
 ```
ython generate_function_samples.py --function '(2.5 x - (2.5 x 3)/6 + (2.5*x 5)/120)' --jitter 0.5 --num_samples 36 --plot_out_file example_output.png --variable_ranges '{""x"" : [-2.5, 2.5] }' --draw_true_function 
 ```
which will produce the following plot:
 
 We can also plot 3D samples. For example for the function 
 y = sin((x^2))/2 - (x1^2)/4 + 3)cos(2x + 1 - e^x1) 
we can plot 36 samples using the command below:
 python generate_function_samples.py --function 'sin(0.5*x**2 - 0.25*x1**2 + 3) * cos(2*x + 1 - exp(x1))' --jitter 0.1 --num_samples 36 --draw_true_function --plot_out_file example_output_3d.png --variable_ranges '{""x"": [-1.5, 1.5], ""x1"": [-1.5, 1.5]}' --display 
which will produce the following plot:
 gradient_descent_blog 
 Code for the blog post http://alykhantejani.github.io//backpropgartion-part-1 
 To produce the gradient descent graphs for example 1 and 2 please run the following code: 
 python example1.py && python example2.py nninit 
 Weight initialization schemes for PyTorch nn.Modules. This is a port of the popular  nninit  for  Torch7  by  @kaixhin . 
 Update 
 This repo has been merged into  PyTorch's nn module , I recommend you use that version going forward. 
 PyTorch Example 
 ```python
import nninit
from torch import nn
import torch.nn.init as init
import numpy as np 
 class Net(nn.Module):
  def  init (self):
     super(Net, self). init ()
     self.conv1 = nn.Conv2d(5, 10, (3, 3))
     init.xavier_uniform(self.conv1.weight, gain=np.sqrt(2))
     init.constant(self.conv1.bias, 0.1) 
 network = Net()
``` 
 Installation 
 Clone the repo and run  python setup install 
 Usage 
 ```python
import nninit
from torch import nn
import numpy as np 
 class Net(nn.Module):
  def  init (self):
     super(Net, self). init ()
     self.conv1 = nn.Conv2d(5, 10, (3, 3))
     nninit.xavier_uniform(self.conv1.weight, gain=np.sqrt(2))
     nninit.constant(self.conv1.bias, 0.1) 
 network = Net()
``` 
 Supported Schemes 
 
 nninit.uniform(tensor, a=0, b=1)  - Fills  tensor  with values from a uniform, U(a,b) 
 nninit.normal(tensor, mean=0, std=1)  - Fills  tensor  with values drawn from a normal distribution with the given mean and std 
 nninit.constant(tensor, val)  - Fills  tensor  with the constant  val 
 nninit.xavier_uniform(tensor, gain=1)  - Fills  tensor  with values according to the method described in  ""Understanding the difficulty of training deep feedforward neural networks"" - Glorot, X. and Bengio, Y. , using a uniform distribution. 
 nninit.xavier_normal(tensor, gain=1)  - Fills  tensor  with values according to the method described in  ""Understanding the difficulty of training deep feedforward neural networks"" - Glorot, X. and Bengio, Y. , using a normal distribution. 
 nninit.kaiming_uniform(tensor, gain=1)  - Fills  tensor  with values according to the method described in  ""Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification"" - He, K. et al.  using a uniform distribution. 
 nninit.kaiming_normal(tensor, gain=1)  - Fills  tensor  with values according to the method described in  ""Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification"" - He, K. et al.  using a normal distribution. 
 nninit.orthogonal(tensor, gain=1)  - Fills the  tensor  with a (semi) orthogonal matrix. Reference:  ""Exact solutions to the nonlinear dynamics of learning in deep linear neural networks"" - Saxe, A. et al. 
 nninit.sparse(tensor, sparsity, std=0.01)  - Fills the 2D  tensor  as a sparse matrix, where the non-zero elements will be drawn from a normal distribution with mean=0 and std= std . 
"
StephanZheng,"Detecting Adversarial Examples via Neural Fingerprinting 
 
 This is code that implements  Neural Fingerprinting , a technique to detect adversarial examples. 
 This accompanies the paper  Detecting Adversarial Examples via Neural Fingerprinting ,  Sumanth Dathathri(*), Stephan Zheng(*), Richard Murray and Yisong Yue, 2018  (* = equal contribution), which can be found here: 
 https://arxiv.org/abs/1803.03870 
 If you use this code or work, please cite: 
 bibtex
@inproceedings{dathathri_zheng_2018_neural_fingerprinting,
  title  = {Detecting Adversarial Examples via Neural Fingerprinting},
  author={Dathathri, Sumanth and Zheng, Stephan and Murray, Richard and Yue, Yisong},
  year   = {2018}
  eprint = {1803.03870}
  ee     = {https://arxiv.org/abs/1803.03870}
} 
 To clone the repository, run: 
 git clone https://github.com/StephanZheng/neural-fingerprinting
cd neural-fingerprinting 
 Results 
 Neural Fingerprinting achieves near-perfect detection rates on MNIST, CIFAR and MiniImageNet-20. 
 
 
ROC curves for detection of different attacks on CIFAR. 
 Requirements and Installation 
 We have tested this codebase with the following dependencies (we cannot guarantee compatibility with other versions). 
 
 PyTorch >= 0.2
(torch (0.2.0.post3)
torchvision (0.1.9)) 
 Tensorflow >=1.4.1
(tensorflow (1.4.1)) 
 Keras 2.0.8 
 https://github.com/gzuidhof/nn-transfer : to transfer models from Tensorflow to PyTorch. 
 scikit-learn 
 
 To install these dependencies, run: 
 ``` 
 PyTorch: find detailed instructions on  http://pytorch.org/ 
 pip install torch
pip install torchvision 
 TF: find detailed instructions on  http://tensorflow.org/ 
 pip install keras
pip install tensorflow-gpu 
 nn_transfer 
 git clone https://github.com/gzuidhof/nn-transfer
cd nn-transfer
pip install . 
 pip install sklearn
``` 
 This codebase relies on third-party implementations for adversarial attacks and code to transfer generated attacks from Tensorflow to PyTorch. 
 
 Local Intrinsic Dimensionality for Adversarial Subspace Detection  https://github.com/xingjunm/lid_adversarial_subspace_detection : a library to generate all adversarial attacks. 
 Cleverhans:  https://github.com/tensorflow/cleverhans : a library to generate gradient-based attacks, called by the LID code. This codebase has been included in the  third_party  folder. 
 https://github.com/rwightman/pytorch-nips2017-attack-example : code to generate iterative fast-gradient attacks on ImageNet examples. 
 
 Quick-start 
 To train and evaluate models with fingerprints, use the launcher script  run.sh , which contains example calls to run the code. 
 The flags that can be set for the launcher are: 
 ./run.sh dataset train attack eval grid num_dx eps epoch_for_eval 
 where 
 
 dataset: 'mnist', 'cifar' or 'miniimagenet' 
 train: 'train' or 'notrain' -- do training or not 
 attack: 'train' or 'notrain' -- create adversarial examples or not 
 eval: 'eval' or 'noeval' -- do evaluation or not 
 grid: 'grid' or 'nogrid' -- enables a grid search for hyperparameter tuning. 
 num_dx: number of fingerprint directions 
 eps: standard deviation of randomly sampled fingerprint directions 
 epoch_for_eval: which model epoch to use for evaluation 
 
 For instance, the following command trains a convolutional neural network for MNIST with 10 fingerprints with epsilon = 0.1, and evaluates the model after 10 epochs of training: 
 ./run.sh mnist train attack eval nogrid 10 0.1 10 
 Running training, attacks and evaluation 
 
 To train a model with fingerprints: 
 
 ```bash
NAME=mnist 
 LOGDIR=/tmp/nfp/$NAME/log
DATADIR=/tmp/nfp/$NAME/data
mkdir -p $LOGDIR
mkdir -p $DATADIR 
 NUMDX=10
EPS=0.1
NUM_EPOCHS=10 
 python $NAME/train_fingerprint.py \
--batch-size 128 \
--test-batch-size 128 \
--epochs $NUM_EPOCHS \
--lr 0.01 \
--momentum 0.9 \
--seed 0 \
--log-interval 10 \
--log-dir $LOGDIR \
--data-dir $DATADIR \
--eps=$EPS \
--num-dx=$NUMDX \
--num-class=10 \
--name=$NAME
``` 
 
 Creating adversarial attacks for the model after 10 epochs of training: 
 
 ADV_EX_DIR=/tmp/nfp/$NAME/attacks
EPOCH=10
python $NAME/gen_whitebox_adv.py \
--attack ""all"" \
--ckpt $LOGDIR/ckpt/state_dict-ep_$EPOCH.pth \
--log-dir $ADV_EX_DIR \
--batch-size 128 
 
 Evaluating model 
 
 ```
EVAL_LOGDIR=$LOGDIR/eval/epoch_$EPOCH
mkdir -p $EVAL_LOGDIR 
 python $NAME/eval_fingerprint.py \
--batch-size 128 \
--epochs 100 \
--lr 0.001 \
--momentum 0.9 \
--seed 0 \
--log-interval 10 \
--ckpt $LOGDIR/ckpt/state_dict-ep_$EPOCH.pth \
--log-dir $EVAL_LOGDIR \
--fingerprint-dir $LOGDIR \
--adv-ex-dir $ADV_EX_DIR \
--data-dir $DATADIR \
--eps=$eps \
--num-dx=$numdx \
--num-class=10 \
--name=$NAME
```"
satwikkottur,"f14-cv-project 
 Fall 2014 Course project for Computer Vision course MovieRecommend 
 Course Project for CMU 10-701/15-781 - Machine Learning 
 A movie recommender system based on Collaborative Filtering and Topic Modeling (LDA) 
 External Dependencies 
 
 Apache Commons Math 3.3 
 la4j library for linear algebra 
 nltk for python 2.7 
 FluidSim 
 Fluid simulation - Water and Fire interaction StochasticMCMC 
 MCMC for posterior distribution sampling 
 Satwik Kottur  and  Krishna Pillutla , Carnegie Mellon University 
 This project is a part of  10-708: Probabilistic Graphical Models , Fall 2015, in
requirement for the course completion. 
 The idea is to handle non-smooth energy functions in the setting of Hamiltonian dynamics for Monte Carlo Markov chain (MCMCM) sampling. Hamiltonian Monte Carlo (HMC) methods evolve a set of differential equations and non-smooth energies do not fit in, as in. The  report  provides further details on the various strategies adopted to solve the problem. 
 Parts of the code are adapted from two sources:
 1.  Hamiltonian Monte Carlo MATLAB Implementation 
 2.  Tianqi Chen's Implementation for Stochastic Gradient-HMC Visual Word2Vec (vis-w2v) 
 Learning visually grounded word embeddings from abstract image   
 
 Paper 
 Satwik Kottur*, Ramakrishna Vedantam*, José Moura, Devi Parikh 
 Visual Word2Vec (vis-w2v): Learning Visually grounded embeddings from abstract images 
[ ArXiv ] [ Project Page ] 
* = equal contribution 
 Pre-trained Visual Word2vec Embeddings can be found  here ,
trained on wikipedia and MSCOCO captions. 
 
 Code Structure 
 The code is organized as follows: 
 
 visword2vec.c  : Main code for training vis-w2v 
 refineFunctions.c  : Contains functions related to refining embeddings 
 helperFunctions.c  : Mostly contain assisting functions like io, tokenization, etc. 
 visualFunction.c  : Contains code to refine based on tuples and also perform common sense (cs) task on test and validation sets 
 vpFunctions.c  : Contains code to refine based on sentences and also perform visual paraphrasing (vp) task 
 structs.h  : Contains structures defined for the code 
 filepaths.h  : Contains the paths to the files needed for the above two tasks-cs,vp 
 Makefile  : Helps to setup, compile and run programs 
 
 Other files can be ignored for now.   
 Program accepts following as inline arguments:   
 
 embed-path  : Initialization for the embeddings (pre-trained using word2vec usually) 
  Format: Header should have  vocabsize   dimensions 
  Each following row should first have the word, and embeddings delimited by space   
 output  : Path to where to store the output embeddings 
 size  : Size of the hidden layer (should match with the pre-loaded embeddings) 
 threads  : Number of threads to use for refining, loading and other operations 
 
 Currently only saving one embedding is supported. For multi embeddings simply turn
the  trainMulti  flag (top of  visword2vec.c  file) to 1. However, saving can be done by
uncommenting code in  trainModel() . 
 Steps for usage 
 
 
 Makefile 
 
 Liblinear-2.1 must be compiled and the path must be correctly set 
 yael must be setup (for k means) and corresponding paths setup
Link here:  yael 
 cs and vp options should have correctly  -embed-path  options 
 
 
 
 filepaths.h : 
 
 Make sure all the paths are accessible and correctly set 
 Any change to this file, should be followed by re-compiling the code 
 
 
 
 liblinearWrapper.h : 
 
 Additionally, you also need to link the correct path to liblibear 
 
 
 
 Other dependencies: 
 
 NTLK is used for tokenization and lemmatization (VP task) 
 
 
 
 To run either cs or vp, comment or uncomment corresponding wrapper calls in 
 trainModel()  function of visword2vec. And then  make cs  or  make vp  for the
two tasks correspondingly to compile and run.  make  simply compiles while 
 make clean  cleans up all the binaries.   
 NOTE : All the binaries are stored in  bin/  folder (might have to create one if 
doesnt exist beforehand).   
 
 Tasks 
 In this paper, we deal with three tasks: Common Sense Assertion Classification, Visual Paraphrasing and Text-based Image Retrieval. 
 A. Common Sense Assertion Classification  ( Project page ) 
Download the dataset from their project page  here .
Code to process this dataset further is given in  utils/cs/ .
The following are the pre-processing steps: 
 
 Extract the training (P, R, S) tuples 
 Extract the visual features for clustering 
 Extract the test and val (P, R, S) tuples 
 Extract the  word2vec  embeddings to initialize from (you can alternatively use any other embeddings to begin with, we recommend you use pre-trained embeddings to reproduce results from the paper). 
 
 All the above four steps can be done by simply running:
 cd utils/cs/
python extractData.py <path to downloaded data> <path to save the data>(optional) 
By default it created a folder  data/cs  and saves the files in this folder. This will produce files  word2vec_cs.bin ,  PRS_train.txt ,  PRS_test.txt ,  PRS_val.txt  and  visual_train.txt  at destination folder corresponding to above files. Once these files are produced, open  filepath.h  and make sure the macros point to right file paths.
``` 
 define ROOT_CS ""data/cs/"" 
 define CS_VISUAL_FEATURE_FILE ROOT_CS  ""visual_train.txt"" 
 define CS_PRS_TRAIN_FILE ROOT_CS ""PRS_train.txt"" 
 define CS_PRS_TEST_FILE ROOT_CS ""PRS_test.txt"" 
 define CS_PRS_VAL_FILE ROOT_CS ""PRS_val.txt"" 
 Now, to run, simply: 
make
./visword2vec -cs 1 -embed-path data/cs/word2vec_cs.bin -output cs_refined.bin -size 200 -clusters 25
```
You can also give in other parameters to suit your needs. 
 
 B. Visual Paraphrasing  ( Project page ) 
Download the VP dataset from their project page  here .
Also download the clipart scenes and descriptions (ASD) used to train  vis-w2v  from the  clipart  project page  here . 
 All the scripts needed for pre-processing are available in  utils/vp  folder.  
 Follow the steps below:
 Training data 
 Step 1:  Run the  fetchVPTrainData.m  function to extract relevant data for training  vis-w2v .
```
cd utils/vp 
 
 
 fetchVPTrainData( ,  ,  ); 
 
 
 For example: 
 
 
 fetchVPTrainData('data/vp/AbstractScenes_v1.1', 'data/vp/imagine_v1/', 'data/vp/');
``` 
 
 
 It does the following (not important. If you just want desired data, run the above command):
  * Extracting visual features  abstract_features.txt  from Abstract Scene Dataset (ASD) using MATLAB script.
  ``` 
 
 
 cd utils/vp
extractAbstractFeatures( ,  ) 
 
 
 For example:  
 
 
 extractAbstractFeatures('data/vp/AbstractScenes_v1.1', 'data/vp/')
   * The alignment between ASD and VP datasets is given in two files `SceneMap.txt` and `SceneMapV1_10020.txt` present in `utils/vp/`. We will use them along with train/test split of VP and select features from training sentences only, again using MATLAB. 
  cd utils/vp
alignAbstractFeatures( ,  ,  ) 
 
 
 For example: 
 
 
 alignAbstractFeatures('data/vp/imagine_v1/', 'data/vp/', 'data/vp/')
   * Get the training sentences from VP dataset (for learning `vis-w2v`). This would produce `vp_train_sentences_raw.txt`. 
  cd utils/vp
saveVPTrainSentences( ,  ) 
 
 
 For example: 
 
 
 saveVPTrainSentences('data/vp/imagine_v1/', 'data/vp')
  ``` 
 
 
 Task data 
 Step 2:  One should use our new embeddings  vis-w2v  in place of  word2vec  in visual paraphrasing task ( imagine_v1/code/feature/compute_features_vp.m  at line 32). Alternatively, we can save their other text features (co-occurance and total frequency) and use it in our code for speed and smoother interface between learning embeddings and performing the task. 
This can be achieved by adding the following lines to  imagine_v1/code/feature/compute_features_vp.m  before line 30, and running  imagine_v1/code/script_vp.m . 
 save('vp_txt_features.mat', 'feat_vp_text_tf_1', 'feat_vp_text_tf_2', 'feat_vp_text_coc_1', 'feat_vp_text_coc_2');
% Escape from running remainder code
error('Saved features, getting out!') 
 Step 3 : Next, we obtain all the relevant information to perform the visual paraphrasing task (using MATLAB)
* Sentence pairs:  vp_sentences_1.txt  and  vp_sentences_2.txt 
* Other textual features:  vp_features_coc_l.txt ,  vp_features_coc_2.txt ,  vp_features_tf_l.txt ,  vp_features_tf_2.txt 
* Ground truth:  vp_ground_truth.txt 
* Train / test split:  vp_split.txt 
* Train / val split:  vp_val_inds_1k.txt 
 ```
cd /utils/vp 
 
 
 fetchVPTaskData( ,  ,  ) 
 
 
 For example: 
 
 
 fetchVPTaskData('data/vp/imagine_v1/', 'data/vp/imagine_v1/code/', 'data/vp')
``` 
 
 
 Step 4 : Finally, we tokenize and lemmatize all the sentences, i.e,  vp_sentences_1.txt ,  vp_sentences_2.txt  and  vp_train_sentences_raw.txt . 
 ```
cd /utils/vp
python lemmatizeVPTrain.py    
 For example:
python lemmatizeVPTrain.py data/vp/ data/vp/
 ``
Phew! That's a lot of pre-processing. Now we are all set to learn vis-w2v embeddings while performing the visual paraphrasing task.
Like before, check all the filepaths in filepaths.h` before proceeding. 
 Now, to run, simply:
 make
./visword2vec -vp 1 -embed-path data/vp/word2vec_vp.bin -output vp_refined.bin -size 200 -clusters 100 
You can also give in other parameters to suit your needs. 
For VP, these are  mode  that indicates the training context and  window-size   that indicates the size in  WINDOW  mode.
  *  DESCRIPTIONS : Use all the three sentences for training
  *  SENTENCES : Use sentences one after the other
  *  WINDOW : Use a context window of size  window-size  (default 5)
  *  WORDS : Use each word separately 
 The program prints 100 runs with both validation and test performance. We choose the run with best validation performance and report the corresponding test result. 
 
 C. Text-based Image Retrieval 
This task involves retrieving the appropriate image based on the associated tuple. We collect the data and make it available at  utils/text-ret/text_ret_tuples.pickle  along with ground truths for each image as  utils/text-ret/text_ret_gt.txt . The goal is to retrieve correct image from the list of ground truth tuples using each of collected queries in pickle file as query. 
 The code for this task is provided in Python in  utils/text_ret/  along with the data. To run, we need to point it to the data directory along with embedding paths. There are two modes for this task: (A) SINGLE - this uses single embedding for P, R, S. (B) MULTI - this uses three different embeddings for each of P, R, S. The inputs are given accordingly. 
 ```
cd utils/text-ret/
python performRetrieval.py    
  (or)
python performRetrieval.py        
 For example:
python performRetrieval.py ./ cs_refined.bin
``` 
 For any further information/questions, feel free to email the authors at  skottur@andrew.cmu.edu  or  vrama91@vt.edu Minimal Mistakes 
 Minimal Mistakes  is a two column responsive Jekyll theme perfect for powering your GitHub hosted blog. 
 Jekyll 3 Update:  A version of Minimal Mistakes compatible with Jekyll 3 can be found in the  jekyll3  branch . GitHub Pages is  locked at version 2.4  so some keep that in mind if you're trying to use 3.0 features that aren't supported there yet. 
 Minimal Mistakes is all about: 
 
 Responsive templates. Looking good on mobile, tablet, and desktop. 
 Gracefully degrading in older browsers. Compatible with Internet Explorer 8+ and all modern browsers. 
 Minimal embellishments -- content first. 
 Optional large feature images for posts and pages. 
 Simple and clear permalink structure. 
 Custom 404 page  to get you started. 
 Support for Disqus Comments 
 
 
 See a  live version of Minimal Mistakes  hosted on GitHub. 
 Getting Started 
 Minimal Mistakes takes advantage of Sass and data files to make customizing easier. These features require Jekyll 2.x and will not work with older versions of Jekyll. 
 To learn how to install and use this theme check out the  Setup Guide  for more information. CLEVR-Dialog 
 This repository contains code for the paper: 
 CLEVR-Dialog: A Diagnostic Dataset for Multi-Round Reasoning in Visual Dialog 
 Satwik Kottur ,  José M. F. Moura ,  Devi Parikh ,  Dhruv Batra ,  Marcus Rohrbach 
[[PDF][7]] [[ArXiv][1]] [ Code ] 
 Oral Presentation   
 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2019 
 If you find this code useful, consider citing our work: 
 @inproceedings{Kottur2019CLEVRDialog,
    title  = {CLEVR-Dialog: A Diagnostic Dataset for Multi-Round Reasoning in Visual Dialog},  
    author = {Kottur, Satwik and Moura, Jos\'e M. F. and Parikh, Devi and   
              Batra, Dhruv and Rohrbach, Marcus},  
    journal = {arXiv preprint arXiv:1903.03166},
    year   = {2019}  
} 
 Abstract 
 Visual Dialog is a multimodal task of answering a sequence of questions 
grounded in an image, using the conversation history as context.
It entails challenges in vision, language, reasoning, and grounding.
However, studying these subtasks in isolation on large, real datasets is 
infeasible as it requires prohibitively-expensive complete annotation of the 
'state' of all images and dialogs.  
 We develop CLEVR-Dialog, a large diagnostic dataset for studying multi-round 
reasoning in visual dialog.
Specifically, we construct a dialog grammar that is grounded in the scene 
graphs of the images from the CLEVR dataset.
This combination results in a dataset where all aspects of the visual dialog 
are fully annotated.
In total, CLEVR-Dialog contains 5 instances of 10-round dialogs for about 85k 
CLEVR images, totaling to 4.25M question-answer pairs.  
 We use CLEVR-Dialog to benchmark performance of standard visual dialog models;
in particular, on visual coreference resolution (as a function of the 
coreference distance).
This is the first analysis of its kind for visual dialog models that was not 
possible without this dataset.
We hope the findings from CLEVR-Dialog will help inform the development of 
future models for visual dialog. 
 
This repository generates a version of our diagnostic dataset  CLEVR-Dialog 
(figure above). 
 Setup 
 The code is in Python3 with following python package dependencies: 
 bash
pip install absl-py
pip install json
pip install tqdm
pip install numpy 
 Directory Structure 
 The repository contains the following files: 
 
 generate_dataset.py : Main script to generate the dataset 
 constraints.py : List of constraints for caption and question generation 
 clevr_utils.py : Utility functions to dialog generation 
 global_vars.py : List of global variables along with initialization 
 
 In addition, the dataset generation code requires following files: 
 
 templates/synonyms.json : Compilation of words and their synonyms 
 templates/metainfo.json : Contains information about attributes and their values for CLEVR objects 
 templates/captions  and  templates/questions : Caption and question templates respectively. 
 
 CLEVR Images 
 Our dataset is built on  CLEVR  images, which can be downloaded from  here .
Extract the images and scene JSON files in  data/  folder.
We will only use CLEVR  train  and  val  splits as scene JSON files are unavailable for  test  split. 
 Generating CLEVR-Dialog Dataset 
 To generate the dataset, please check  run_me.sh .
Additional details about the supported flags can be found in  generate_dataset.py .
An example command is shown below: 
 bash
DATA_ROOT='data/CLEVR_v1.0/'
python -u generate_dataset.py \
    --scene_path=${DATA_ROOT}""scenes/CLEVR_train_scenes.json"" \
    --num_beams=100 \
    --num_workers=1 \
    --save_path=${DATA_ROOT}""clevr_dialog_train_raw.json"" \
    --num_images=10 
 CLEVR-Dialog Annotations 
 The generated JSON contains a list of dialogs on CLEVR images with following fields:   
 
 split : Specifies if the CLEVR split is train/val.  
 image_index : CLEVR image index.  
 image_filename : CLEVR image filename.  
 dialogs : List of dialog instances for a given image, each with following fields: 
        |-- caption : Caption for the dialog instance 
        |-- template_info : Template information for the dialog (caption + 10 questions) 
        |-- dialog : Text for the ten rounds of dialog, each with following fields: 
             
     |-- question : Question text for the current round 
             
     |-- answer : Answer text for the current round 
             
     |-- template : Question template for the current round 
        |-- graph : Scene graph information for the dialog, with following fields: 
             
     |-- objects : Objects with attributes discussed in the dialog 
             
     |-- counts : Specific object counts discussed in the dialog 
             
     |-- relationships : Object relationships discussed in the dialog 
             
     |-- exists : Object existences discussed in the dialog 
             
     |-- history : List of incremental scene graph information conveyed in each round    
 
 The dataset used in the paper can be downloaded here:  train  and  val  splits. 
 Contributors 
 
 Satwik Kottur 
 
 For any questions, please feel free to contact the above contributor(s). 
 License 
 This project is licensed under the license found in the LICENSE file in the
root directory of this source tree ( here )."
JimmyWhitaker,"NerualNet 
 Personal Experiment of writing a neural network package (inefficient) from scratch in Java. It was used to understand some of the concepts behind Neural Nets like backprop and drop-connect. It is focused mainly on the XOR and MNIST toy problems. LeNet5-MNIST-Example 
 This contains an iPython Notebook that trains and tests a LeNet5 Convolutional Neural Network on the MNIST dataset.  
 Dependencies 
 Python 2.7 ,
 numpy/scipy ,
 Theano 
 The code has been adapted from its original form to allow parameter tuning. The model can take some time to train depending on the parameters given, and what version of Theano is installed.  Pach Zero Shot 
 Scaling Zero Shot learning from Hugging Face to production with Pachyderm.  
 Hugging Face came out with  this , and I wanted to scale it with  Pachyderm . 
 Running with Docker 
 docker run -v `pwd`/data/:/data/ --entrypoint=python3 jimmywhitaker/zero-shot:v0.1 zs_predict.py --sequences /data/input/test_input.txt --labels /data/labels/test_labels.txt --output /data/output/ 
 Running with Pachyderm 
 Start a Pachyderm cluster with  Pachyderm Hub . 
 Run: 
 make zero-shot-base 
 The code 
 ```
$ python zs_predict.py --help
usage: zs_predict.py [-h] [--sequences DIR] [--labels DIR] [--output DIR] 
 Zero Shot Predictor 
 optional arguments:
  -h, --help       show this help message and exit
  --sequences DIR  input sequences to be classified
  --labels DIR     labels to be applied to sequences
  --output DIR     output directory for predictions
``` Label Studio with Pachyderm 
   Moving : Project in the process of moving to  Pachyderm examples  to allow for better support.  
 
 
 
 Label Studio  supports many different types of data labeling tasks, while  Pachyderm  allows you to incorporate data versioning and data-driven pipelines. Integrating both open source components is a useful way to manage the labeling component of the  data loop . This integration connects a Pachyderm versioned data backend with Label Studio to support versioning datasets and tracking the data lineage of pipelines built off the versioned datasets. 
 How it works 
 Label Studio can utilize an s3 backend, reading data from an S3 bucket and writing labels to an output S3 location. Pachyderm has an S3 compliant gateway that allows reading data from its file system and writing data to its filesystem (organizing it with commits that can start pipelines). 
 We'll create a text labeling example by: 
 
 Start a Label Studio instance that uses Pachyderm as its backend 
 Push data to Pachyderm that automatically populates Label Studio 
 Label the data in Label Studio 
 Version our dataset in Pachyderm 
 
 Note: Label studio currently doesn't support arbitrary s3 storage (only AWS s3 and GCS), so I modified Label Studio's s3 storage backend to support generic object storage endpoints, which allows us to connect to the Pachyderm s3 gateway running locally. you can see the code  here 
 Getting Started 
 This example uses a Pachyderm deployment for scaling and management. We can deploy a cluster on  Pacyderm Hub  for free or deploy locally as described here:  Pachyderm Getting Started 
 Once everything is up, we can check the setup by running: 
1.  kubectl get all  to ensure all the pods are up and ready. 
2.  pachctl version  which will show both the  pachctl  and  pachd  versions. 
 Configuring .env file 
 The  .env  file needs to be configured for your Pachyderm s3 gateway. Pachyderm's s3 gateway is accessed through an  http  endpoint that is available on port  30600  on the Pachyderm cluster. This address is used to as the  ENDPOINT_URL  for the Label Studio backend in the  .env  file.  
 Pachyderm Hub 
 If you are running your cluster on Pachyderm Hub, you can find out your  ENDPOINT_URL  by clicking the  Connect  button. You should see an address that looks something like:  
 grpcs://hub-xx-xxxYYxxYY.clusters.pachyderm.io:31400 
 Just change the protocol to  http  and port to  30600 . This will now point at the S3 gateway.  
 https://hub-xx-xxxYYxxYY.clusters.pachyderm.io:30600 
 The  AWS_ACCESS_KEY_ID  and  AWS_SECRET_ACCESS_KEY  in your  .env  file should be set to your Pachyderm  session_token  located in your Pachyderm config (typically in  ~/.pachyderm/config.json ). More info on Pachyderm's  S3 gateway .  
 If you get the following error,  
 botocore.exceptions.ClientError: An error occurred (403) when calling the HeadBucket operation: Forbidden 
 this is typically due to an expired session token. Reconnect to the cluster and update your  .env  with the new token.  
 Minikube configuration 
 If you are running Pachyderm locally on minikube, you can get the  ENDPOINT_URL  for the Pachyderm s3 gateway by running the command: 
 $ minikube ip
192.168.64.8 
 If you are running Pachyderm with authentication, then you can follow the same steps as the Hub setup. If not running with authentication, you can pass any non-empty string to  AWS_ACCESS_KEY_ID  and  AWS_SECRET_ACCESS_KEY  in your  .env  file. 
  ## Creating a new project
A new project requires creating a new configuration (see some of the [examples](examples/)). Creating a new project with Label Studio can be done by from the command line. We'll use the Docker image that we created to do this, adding the `--init` flag which will create the project. 

```shell
docker run --env-file .env -v $(pwd)/examples/my_new_project:/my_new_project -p 8080:8080 --entrypoint=label-studio jimmywhitaker/label-studio:latest start /my_new_project/ --source s3 --source-path master.raw_data --target s3-completions --target-path master.labeled_data --input-format=image --template image_bbox --source-params ""{\""use_blob_urls\"": false, \""regex\"": \"".*\""}""

```  
 Running the Text Labeling Example 
 ``` bash 
 Pachyderm Setup 
 pachctl create repo raw_data
pachctl create repo labeled_data
pachctl create branch labeled_data@master
pachctl create branch raw_data@master 
 Start a local instance of Label Studio (needs the .env for the Pach s3 gateway) 
 docker run --env-file .env -v $(pwd)/examples/my_text_project:/my_text_project -p 8080:8080 jimmywhitaker/label-studio:latest 
 Navigate to http://localhost:8080/tasks 
 Upload data 
 pachctl put file raw_data@master:/test-example.json -f raw_data/test-example.json --split json --target-file-datums 1 
 Modify the data before it's labeled 
 pachctl put file raw_data@master:/test-example.json -f raw_data/test-example2.json --split json --target-file-datums 1 --overwrite 
 Label data (2 examples) in the UI 
 Version your dataset (v1) 
 pachctl list branch labeled_data
pachctl create branch labeled_data@v1 --head master
pachctl list branch labeled_data 
 Label more data in the UI 
 Version your dataset (v2) 
 pachctl list branch labeled_data
pachctl create branch labeled_data@v2 --head master 
 Download dataset for v1 locally 
 pachctl get file -r labeled_data@v1:/ -o labeled_data/ 
 ``` 
 Next Steps 
 
 The output does have a reference for what the input file location was (could potentially be used to track consistency between raw and labeled if raw changes). 
 Make deployment super easy 
 Build a helm chart to deploy label studio to Kubernetes with necessary env  
 Standardize label studio project creation - different examples of configs 
 Ability to update  input raw data  - currently if it's labeled, then it's captured in the source and target json files.  
 Rectify the source and target files to have provenance for the labeling 
 
 Known Issues and Gotchas 
 
 One example per source file  
 Must be json files or figure out how to get s3 signed urls to frontend.  
 When file is updated after labeled, it's not re-loaded (not sure what should happen here - should it be removed from the labeled data repo when the raw data is removed?) 
 When raw data is changed after that example is labeled, the task doesn't update. It does update when  
 It seems as though the target and the source states are tied somehow, so it won't automatically update 
 If a raw file is removed or changed, then labels associated with that file should be removed. Since it's a single file per example, a changed file should be the deleting of one and addition of another. For now, this would need to be an external process that  
 Label Studio automatically tries to start an image labeling config and if there is labeled data, this will throw errors until you load a compatible config for what's already labeled (i.e. you should not use the  --init  and  --force  flags after you've created the project). 
 Market Sentiment Example 
 
 
 
 The purpose of this example is to illustrate how the two, independent yet symbiotic loops in the  machine learning loop  can work together practically. The code loop is managed by git+unittest+GitHub Actions, while the data loop and the data+code interactiona are managed by Pachyderm.  
 The original technique is a  sentiment analysis classifier  that uses the  Financial Phrase Bank Dataset . In this example, we use a reduced version of this blog post and dataset for simplicity and transparency to show the interactions more than the techniques themselves.  
 Running the example 
 The easiest way to run this example is to use the  Makefile  once the cluster is configured (steps below). 
 Setup Pachyderm, S3 gateway, and Label Studio 
 
 Start a pachyderm cluster - get the endpoint address (if using hub, just look at the address where the dash is being served or if minikube, run  minikube ip ) 
 Create a pachyderm token (details)
 bash
pachctl auth get-auth-token --ttl ""624h"" | grep Token | awk '{print $2}' 
 Create a  .env  file inserting the endpoint address and token. 
 ENDPOINT_URL=https://<pachyderm_endpoint_address>:30600
AWS_REGION=us-east-1
AWS_ACCESS_KEY_ID=<pachyderm_token>
AWS_SECRET_ACCESS_KEY=<pachyderm_token> 
 Run label studio with the following commands (you won't see any tasks until you add data):
```bash
pachctl create repo raw_data
pachctl create repo labeled_data
pachctl create branch labeled_data@master
pachctl create branch raw_data@master 
 
 Start a local instance of Label Studio (needs the configured .env for the Pach s3 gateway) 
 docker run -it --env-file .env -v $(pwd)/label-studio-project:/my_text_project -p 8080:8080 jimmywhitaker/label-studio:pach-ls0.9 
 Navigate to http://localhost:8080/tasks 
 ``` 
 Setup GitHub Action 
 Details on the  Pachyderm GitHub Actions 
1. Get the Pachyderm cluster URL (the same address as the s3 gateway, but with port 31400)
2. Create another Pachyderm token like in the previous step.
 bash
pachctl auth get-auth-token --ttl ""624h"" | grep Token | awk '{print $2}' 
3. Create Pachyderm token,  DockerHub username, and DockerHub token  secrets in GitHub (see  Managing Access Tokens ). See our  GitHub Actions Example  for details. 
4. Once these tokens are in place, the pipelines will be pushed each time code is merged to the master branch. 
 Repository Structure 
 Data-tests 
 TODO 
 market_sentiment 
 The main code of the project. This includes the python files needed to load data, etc.  
 Pachyderm 
 This directory holds all the pachyderm pipelines. These pipelines define the code that will be run on our data in our Pachyderm cluster. Once deployed, they will automatically process any data changes, such as, when new data is labeled, it will automatically create a new dataset and train a model when that dataset is ready.  
 pachyderm-github-action 
 The Pachyderm GitHub Action is used to deploy our pipelines when code is pushed to our repository. It handles the building of the Docker container, pushing it to our Docker registry, and updates our pipelines with the new version of this container.  
 tests 
 Unit tests for our code that will be run before building our Docker container.  
 label-studio-project 
 Project configuration for a sentiment analysis in Label Studio using Pachyderm's s3 gateway as the backend to add versioning to the labeling environment. Data is read from Pachyderm and written back to Pachyderm, which adds versioning automatically.  
 TODO and Known Issues 
 
 There's currently some lag in the s3 gateway communications (likely because test cluster is very small). More investigation needed on this.  
 Bug where changing label modifies the wrong file in s3 
 Makefile for integration and prod testing 
 Create staging branch for deployment that can be migrated into production.  
 Add more unit tests 
 Add data tests 
 Pachyderm -> ClearML 
 Pachyderm logging to ClearML. ClearML is a great visualization and reporting framework for machine learning models, but one of the difficulties is keeping up with changing datasets. By using Pachyderm as our execution platform, we can version our executions, code, data, and models while still tracking everything in ClearML.  
 
 
 
 This is a simple integration example where we use ClearML for monitoring our jobs and experiments, while using Pachyderm to manage our data and automatically run pipelines when our code or data changes. 
 TLDR; How it works 
 
 Spin up Pachyderm  Use minikube or cloud deployment and connect to it. 
 Spin up ClearML  (Using ClearML Hosted Community Edition) 
 Create a ClearML config . 
 Copy the access credentials'  CLEARML_API_ACCESS_KEY  and  CLEARML_API_SECRET_KEY  into the  secrets.json  file. We'll use this file to make a  Pachyderm secret . This keeps our access keys from being built into our container or put in plaintext somewhere. 
 Create the secret with  pachctl create secret -f secrets.json 
 Run  make all  to create a data repository and the pipeline.  
 
 Note: Downloading the data locally and then pushing it to Pachyderm may seem counterintuitive at first. Downloading the data locally and then pushing it to a remote cluster seems like an extra step, especially when dealing with a standard dataset like MNIST. However, if we think about a real world use case where multiple teams may be manipulating the data (removing examples, adding classes, etc.) then having a history for each of these models can be very useful. In most production settings with supervised learning, the  labeling environment can be directly connected to the data repository , automating this step. 
 MNIST example 
 
 Creates a project in ClearML with the name of the Pachyderm pipeline.  
 Trains an MNIST classifier in a Pachyderm Job 
 Logs training info from training to ClearML for monitoring and comparison. 
 If the Data or Pachyderm Pipeline changes, it kicks off a new training process. 
 
 Future Goals 
 
 Create a more robust example than MNIST. 
 Multi-GPU - ClearML has some really cool features here, but right now Pachyderm is executing the job in a single Pod.  
 Consistent naming of runs between Pachyderm and ClearML - we would ideally like to have the jobs use the same hash ID.  
 Add commit or branch information to the model referenced by ClearML 
 Pachyderm Tutorial 
 
 This is a repo for a Pachyderm tutorial 
 
 python
%load_ext autoreload
%autoreload 2 
 This file will become your README and also the index of your documentation. 
 Install 
 
 
 Create a free Pachyderm cluster on  Pachyderm Hub . 
 
 
 Connect to JupyterHub in the cluster 
 
 
 Clone this repo  
 
 
 pip install pachyderm_tutorial 
 Pachyderm File System Basics 
 Create a Pachyderm data repository called  data 
 python
!pachctl version 
 COMPONENT           VERSION             
pachctl             1.13.0              
pachd               1.12.5
 
 python
!pachctl create repo data 
 python
!pachctl list repo 
 NAME CREATED      SIZE (MASTER) ACCESS LEVEL 
data 1 second ago 0B            OWNER
 
 When we list our repos, we can see that we have an empty data repository, with no data in it. So let's add some data. 
 python
%%writefile iris.csv
5.1,3.5,1.4,0.2,Iris-setosa
4.9,3.0,1.4,0.2,Iris-setosa
4.7,3.2,1.3,0.2,Iris-setosa
4.6,3.1,1.5,0.2,Iris-setosa
7.0,3.2,4.7,1.4,Iris-versicolor
6.4,3.2,4.5,1.5,Iris-versicolor
6.9,3.1,4.9,1.5,Iris-versicolor
5.5,2.3,4.0,1.3,Iris-versicolor
6.3,3.3,6.0,2.5,Iris-virginica
5.8,2.7,5.1,1.9,Iris-virginica
7.1,3.0,5.9,2.1,Iris-virginica
6.3,2.9,5.6,1.8,Iris-virginica 
 Writing iris.csv
 
 Data repositories in Pachyderm automatically track versions of the data placed in  them. Similar to Git, we organize our data via branches, so we will push our data to the  master  branch of our  data  repository. 
 python
!pachctl put file data@master -f iris.csv 
 iris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s
[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s
[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s
[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s
[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s
[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s
[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s
[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s
[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s
[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s
[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s
[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s
[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s
[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s
 
 We can look at the data that's been uploaded to our  data  repository, by listing the files on the master branch. 
 python
!pachctl list file data@master 
 NAME      TYPE SIZE 
/iris.csv file 364B
 
 Similarly, if we want to delete our file we can do that as well.  
 python
!pachctl delete file data@master:/iris.csv 
 python
!pachctl list file data@master 
 NAME TYPE SIZE
 
 Now let's add it back again.  
 python
!pachctl put file data@master -f iris.csv 
 iris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s
[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s
[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s
[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s
[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s
[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s
[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s
[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s
 
 Now we can list all of the commits that have been made to our repository to see the history of the  master  branch. 
 python
!pachctl list commit data 
 REPO BRANCH COMMIT                           FINISHED       SIZE PROGRESS DESCRIPTION
data master be040bd068fb4e67ae59cd2f40bb2a0c 16 minutes ago 364B -         
data master 44e71eeb040e4453a13014c22c9a80ed 17 minutes ago 0B   -         
data master 4670ab8ce78e4306a5789144f30b4afd 20 minutes ago 364B -
 
 Pachyderm keeps a record of all the changes that happen to the  data  repository. This way if we ever want to revert to a previous version of our dataset, we can do it.  
 For example, if we wanted to change move the head of our  master  branch to the first commit, we could run the following command (note the commit hashes will be different if you are running this yourself):  
 python
!pachctl create branch data@master --head 4670ab8ce78e4306a5789144f30b4afd 
 python
!pachctl list branch data 
 BRANCH HEAD                             TRIGGER 
master 4670ab8ce78e4306a5789144f30b4afd -
 
 We can also use  Ancestry Syntax  to traverse commits.  ^  for the parent of the commit or we can reference the commits in numerical order using  .n , where  n  is the commit number.  
 ```python 
 Reset the head of our master branch 
 !pachctl create branch data@master --head be040bd068fb4e67ae59cd2f40bb2a0c
``` 
 ```python 
 Previous commit to the head of the master branch 
 !pachctl list commit data@master^
``` 
 REPO BRANCH COMMIT                           FINISHED       SIZE PROGRESS DESCRIPTION
data master 44e71eeb040e4453a13014c22c9a80ed 17 minutes ago 0B   -         
data master 4670ab8ce78e4306a5789144f30b4afd 21 minutes ago 364B -
 
 ```python 
 First commit to the master branch 
 !pachctl list commit data@master.1
``` 
 REPO BRANCH COMMIT                           FINISHED       SIZE PROGRESS DESCRIPTION
data master 4670ab8ce78e4306a5789144f30b4afd 21 minutes ago 364B -
 
 python
!pachctl list commit data@master.-1 
 REPO BRANCH COMMIT                           FINISHED       SIZE PROGRESS DESCRIPTION
data master 44e71eeb040e4453a13014c22c9a80ed 17 minutes ago 0B   -         
data master 4670ab8ce78e4306a5789144f30b4afd 21 minutes ago 364B -
 
 python
!pachctl list branch data 
 BRANCH HEAD                             TRIGGER 
master be040bd068fb4e67ae59cd2f40bb2a0c -
 
 Pachyderm Pipeline System Basics 
 Pachyderm also lets you create code pipelines that can be triggered by your data. These pipelines connect to your data repositories, ensuring that they run anytime your data is updated. 
 Here is an example of a Pachyderm pipeline.  
 python
%%writefile count.yaml
pipeline:
  name: count
description: Count the number of lines in a csv file
input:
  pfs:
    glob: /
    repo: data
transform:
  cmd: ['/bin/sh']
  stdin: ['wc -l /pfs/data/iris.csv > /pfs/out/line_count.txt']
  image: alpine:3.14.0 
 Writing count.yaml
 
 When our pipeline runs, it will map the data from our  data  repository into the container that's running our pipeline. It will also automatically create a new data repository to hold and version our output(s).  
 We can submit our pipeline to Pachyderm by using the  create pipeline  command. 
 python
!pachctl create pipeline -f count.yaml 
 If we list our pipelines, we can see the  status of them. 
 python
!pachctl list pipeline 
 NAME  VERSION INPUT  CREATED        STATE / LAST JOB  DESCRIPTION                             
count 1       data:/ 15 seconds ago [32mrunning[0m / [32msuccess[0m Count the number of lines in a csv file
 
 It looks like our pipeline failed, so let's inspect it and see why. 
 python
!pachctl list job --pipeline=count 
 ID                               PIPELINE STARTED       DURATION  RESTART PROGRESS  DL   UL  STATE   
9bd27e46d1b64bd0bde16acefa5dff15 count    4 seconds ago 2 seconds 0       1 + 0 / 1 364B 22B [32msuccess[0m
 
 We can inspect the logs for the pipeline to see what went wrong. (Note there are 3  tries  here.) 
 python
!pachctl list file count@master 
 NAME            TYPE SIZE 
/line_count.txt file 22B
 
 python
!pachctl get file count@master:/line_count.txt -o ./line_count.txt 
 ./line_count.txt 0.00b / 22.00 b [---------------------------------] 0s 0.00 b/s
[1A[J./line_count.txt 22.00b / 22.00 b [================================] 0s 0.00 b/s
[1A[J./line_count.txt 22.00b / 22.00 b [================================] 0s 0.00 b/s
[1A[J./line_count.txt 22.00b / 22.00 b [================================] 0s 0.00 b/s
 
 python
cat line_count.txt 
 12 /pfs/data/iris.csv
 
 Clear all our data out for the next section 
 ```python 
 Uncomment and run if continuing on 
 !pachctl delete pipeline --all 
 !pachctl delete repo --all 
 ``` 
 Python-Pachyderm 
 Create a client to connect to the Pachyderm clutser. 
 python
print(python_pachyderm.__version__) 
 6.2.0
 
 python
client = python_pachyderm.Client.new_from_config() 
 python
client.create_repo('data') 
 python
with client.commit(""data"", ""master"") as commit:
    client.put_file_bytes(commit, ""data.txt"", b""DATA"") 
 python
client.list_repo() 
 [repo {
  name: ""data""
}
created {
  seconds: 1624489726
  nanos: 662871220
}
size_bytes: 4
auth_info {
  access_level: OWNER
}
branches {
  repo {
    name: ""data""
  }
  name: ""master""
}
]
 
 ```python
file_list = client.list_file(('data','master'), '/', include_contents=True) 
 for f in file_list: 
    print(f)
``` 
 file {
  commit {
    repo {
      name: ""data""
    }
    id: ""3309835f727c4e4aa760ebf26421cdb8""
  }
  path: ""/data.txt""
}
file_type: FILE
size_bytes: 4
hash: ""\330Ukb\227\364\273^7gc\022\025\254\201\004\000\026\331\314G\013\003^)\006\030\304]\004\305\025""
objects {
  hash: ""4ba7d4149c32f5ccc6e54190beef0f503d1e637249baa9e4b123f5aa5c89506f299c10a7e32ab1e4bae30ed32df848f87d9b03a640320b0ca758c5ee56cb2db4""
}
committed {
  seconds: 1624489733
  nanos: 99330690
}
 Superb.ai + Pachyderm Integration 
 
 This example shows how you can create a  Pachyderm  pipeline to automatically version and save data you've labeled in  Superb.ai  to use in downstream machine learning workflows.  
 The integration connects to your SuperbAI project, ingests the data into Pachyderm on a  cron schedule .  
 Once your data is ingested into Pachyderm, you can perform data tests, train a model, or any other type of data automation you may want to do, all while having full end-to-end reproducibility.  
 Requirements 
 You will need an account for each of the tools. Free accounts can now be used to run this example! 
*  Superb.AI account 
* Setup a  Pachyderm Hub  Cluster 
 Run this example 
 
 Generate an Access API Key in SuperbAI. 
 
 Put the key and your user name in the  secrets.json  file.  
 Create the Pachyderm secret 
 
 bash
pachctl create secret -f secrets.json 
 
 Create the cron pipeline to synchronize your  Sample project  from SuperbAI to Pachyderm. This pipeline will run every minute to check for new data (you can configure it to run more or less often in the cron spec in  sample_project.yml ). 
 
 bash
pachctl create pipeline -f sample_project.yml 
 
 Pachyderm will automatically kick off the pipeline and import the data from your sample project. 
 
 PachLite 
 The PachLite is a hack project to simplify a couple of convenience functions.  
 Install 
 The project uses  poetry : 
 bash
poetry shell
poetry install 
 Using the CLI 
 ```
$ pachlite
Usage: pachlite [OPTIONS] COMMAND [ARGS]... 
 PyPach Utilities for developing Pachyderm pipelines locally. 
 Options:
  --help  Show this message and exit. 
 Commands:
  build  Build pachyderm pipeline
  run    Run python file locally as if it were a pipeline
``` 
 ```
$ pachlite build --help
Usage: pachlite build [OPTIONS] [ENTRYPOINT_ARGS]... 
 Build pachyderm pipeline 
 Options:
  -n, --name TEXT         Name of pipeline
  -d, --description TEXT  Description of pipeline
  --image TEXT            Name of docker image to be used for the entrypoint
  -i, --input_repo TEXT   Input repo(s) - format repo@branch
  --entrypoint PATH
  --help                  Show this message and exit.
``` 
 ```
pachlite run --help
Usage: pachlite run [OPTIONS] ENTRYPOINT [ENTRYPOINT_ARGS]... 
 Run python file locally as if it were a pipeline 
 Options:
  -i, --input TEXT  Input repo(s) - format repo@branch
  --help            Show this message and exit.
```"
bamos,"Installation 
 Clone this repo with git's  --recursive  flag to
obtain all the submodules, then run
 bootstrap.sh 
to create symlinks in the home directory and
bootstrap other programs. 
 About 
 This is the source code for my personal website.
Unless stated otherwise, all content is MIT-licensed,
and some of the CV portions are created with the code
in the  bamos/cv  repo. 
 w3c compliance continuous integration 
 Travis CI builds the static website with Jekyll and uses
 validate.rb  to check content for w3c compliance.
Simon Sigurdhsson wrote the
 original validate.rb script ,
available in the public domain by the CC0 license,
and the modifications here are also available in the public domain
by the CC0 license. This is a collection of short shell scripts I have added to my
 PATH  variable to run from anywhere. 
 To add these to your  PATH , clone the repo and add the following
to your  bashrc  or  zshrc , replacing  <python-scripts> 
with the location of the cloned repository.
Furthermore, see my  dotfiles  repo for my
complete Mac and Linux system configurations. 
 ```Bash 
 Add additional directories to the path. 
 pathadd() {
  [ -d ""$1"" ] && [[ "":$PATH:"" !=  "":$1:""  ]] && PATH=""${PATH:+""$PATH:""}$1""
} 
 pathadd  /python2.7
pathadd  /python3
``` 
 2x2-slides.sh 
 Takes a PDF of slides as input and outputs them tiled
in a 2x2 landscape PDF. 
 alarm.sh 
 Linux computers can be used as an alarm clock with a program called
 rtcwake , which will sleep the computer until a specifed time.
 alarm.sh  is a simple wrapper around  rtcwake  to infer the
full data given a time.
For example, the following command will sleep the computer until the
next occurring 7PM. 
 Bash
alarm.sh 7:00PM 
 analyze-pcap.sh 
 Use tcpflow and foremost to analyze TCP streams in a pcap file.
For example, the following command automatically analyzes a pcap file. 
 Bash
analyze-pcap.sh traffic.pcap 
 compare-dirs.sh 
 Compares the files in 2 directories and
detects duplicates based on MD5 checksums.
For example, the following command compares the
files in  test_dir1  and  test_dir2 . 
 ```
./compare-dirs.sh test_dir1 test_dir2
Creating checksums for files in dir1.
Checking files in dir2.
md5:  60b725f10c9c85c70d97880dfe8191b3
dir1: test_dir1/a
dir2: test_dir2/a_renamed 
 md5:  3b5d5c3712955042212316173ccf37be
dir1: test_dir1/b
dir2: test_dir2/b
``` 
 createpdf.sh 
 Create a pdf document from a plaintext document,
with optional source code highlighting. 
 ./createpdf.sh main.cpp c++ 
 notify-postponed.sh 
 Send a notification when there are postponed
messages in  mutt . 
 timesheets.sh 
 Plaintext timesheet management. PARSEC Benchmark Suite 3.0 
 Patch Info 
 This patch provides the following enhancements: 
 
 Ability to build on x86_64 Arch Linux to overcome minor bugs. 
 Updated openssl, see Abhishek Sagar's  blog post 
 Corrected uctcpip, see Yungang Bao's  response 
 Generalized builds. Instead of calling  gcc  tools and overwriting
   CFLAGS ,  CC ,  CFLAGS , and related environment variables set
  in a global bldconf are preserved while building packages. 
 
 Overview 
 The Princeton Application Repository for Shared-Memory Computers (PARSEC) is a
collection of parallel programs which can be used for performance studies of
multiprocessor machines. 
 The PARSEC distribution is composed of packages and the PARSEC framework.
Packages correspond to benchmark programs, libraries and other essential
components. Each package can be compiled in a number of ways as determined by
a build configuration. Build configurations contain information such as which
features of the package should be enabled, which compilers to use and how the
package should be optimized. PARSEC ships with predefined inputs that can be
used to run the benchmarks. The inputs for each program exhibit different
characteristics such as execution time and working set size. 
 Licenses 
 Before you start using, modifying or distributing PARSEC, its programs or the
supplied inputs in any way, make sure you understand all licenses involved.
The PARSEC framework itself is available under a liberal open source license,
as explained in the file LICENSE which is in the same directory as this README
file. Each program uses its own license, which is different in some cases.
Some of the inputs have their own license, too. Licenses for source code can
be found in the 'src/' directory of each package. Licenses for inputs can
be found in the 'inputs/' directory of the package. 
 We distribute the programs and their workloads bundled with PARSEC merely to
allow PARSEC users a convenient access to them and because the license terms
allow us do so in each case. You have to take appropriate steps yourself to
make sure you don't violate any license terms. 
 Requirements 
 PARSEC requires at least 8 GB, but we recommend 12 GB or more. The disk usage
can be broken down as follows: 
 PARSEC occupies about 7 GB with a raw installation. Additional 500 MB are
needed for each set of binaries. To build packages and run them extra space is
required for temporary files, up to several GB if the whole suite is to be
built and executed in one run without cleaning up intermittently. 
 The PARSEC benchmarks have been parallelized with pthreads, OpenMP, TBB and
atomic instructions. Many workloads support more than one parallelization. Each
parallelization has its own requirements that have to be fulfilled in order to
be able to build and run workloads that use it. By default only support for
pthreads and OpenMP are needed. Gcc supports OpenMP since version 4.2.0. 
 PARSEC has been successfully tested on the following systems: 
 
 Linux/i386 
 Linux/x86_64 
 Linux/Itanium 
 Solaris/Sparc 
 
 Limited support exists for the following platforms, but not all benchmark
programs might be available: 
 
 Darwin/PowerPC 
 
 Usage 
 PARSEC ships with several tools which are installed in the 'bin/' directory.
You can use them to customize and manage your installation of the benchmark
suite. 'parsecmgmt' is the main tool. Its purpose is to build and run
packages as well as perform other management operations. 'bldconfadd' and
'bldconfdel' can be used to create and delete your own build configurations. 
 When you build and run PARSEC with 'parsecmgmt', it will create a log file
which will contain all output in the 'log/' directory. You can get a help
summary for each tool by invoking it with option '-h'. 
 A full set of man pages documenting PARSEC, its tools and the most important
parts of the source code is given in the  man/' directory. If you add the bin/' directory to the PATH environment variable and the  man/' directory
to the MANPATH variable then all tools and man pages are accessible at the
command line without having to specify the full path every time. A bash shell
script env.sh' is provided in the root directory of the PARSEC distribution
which modifies the environment in that way. If you work with the bash shell you
can make use of it before you start working with PARSEC by executing
 source env.sh'. You can then start browsing the documentation by running man parsec'. Support for other shells is currently not available. 
 The following examples assume that the 'bin/' directory of PARSEC is in your
path. 
 How to Build PARSEC 
 Before compiling the PARSEC benchmarks, please change some variables in the
file ""config/gcc.bldconf"" such that the PARSEC command  can locate the 
compiler path correctly. 
 To compile all programs of the benchmark suite with the default configuration,
simply run: 
 parsecmgmt -a build
 
 Building the whole benchmark suite takes a lot of time, usually 30-60 min
depending on your system It is possible to selectively build packages
(option '-p') and to choose different build configurations (option '-c'), read
Section 4 for a more comprehensive explanation of build configurations and
Section 6 for more complex usage examples. 
 How to Run PARSEC 
 After you have built the suite, you can use the following command to run all
benchmarks with the minimal test input: 
 parsecmgmt -a run
 
 The test should finish within 5 seconds. Its purpose is to quickly verify that
all benchmarks have been built successfully and are executable. Do not use it
for performance measurements. You can also choose a different input (option
'-i') and alter the number of threads (option '-n'). Section 5 explains the
different inputs and Section 6 gives more detailed examples. 
 How to Get More Information 
 To query PARSEC about all available packages and features, you can use the
following command: 
 parsecmgmt -a info
 
 If you would like to see information about dynamically created files such as
the available builds, you can run: 
 parsecmgmt -a status
 
 Build Configurations 
 Besides the regular configuration which can be altered to customize the
benchmark suite, PARSEC also uses build configurations. A build configuration
is a specific way to compile a program. It determines, for example, what
compiler to use and which features of the package to enable. Build
configurations should be your first approach to alter how a benchmark program
is created. The only limitation is that the interface and output of a benchmark
must remain the same, otherwise the program will become incompatible with the
PARSEC framework. However, you can adapt the run configuration and reference
outputs to eliminate this problem. 
 PARSEC ships with the following preinstalled build configurations: 
 
 'gcc'       Build parallel version of suite with gcc 
 'gcc-serial'    Build serial version of suite with gcc 
 'gcc-hooks'     Build parallel version of suite with PARSEC hooks
              enabled with gcc 
 'icc'       Build parallel version of suite with Intel compiler 
 'gcc-pthreads'  Build with pthreads parallelization (if supported) 
 'gcc-openmp'    Build with OpenMP parallelization (if supported) 
 'gcc-tbb'       Build with TBB parallelization (if supported) 
 
 For example, to build PARSEC with enabled hooks, you can use: 
 parsecmgmt -a build -c gcc-hooks
 
 The three build configurations 'gcc-pthreads', 'gcc-openmp' and 'gcc-tbb' can
be used to compile a workload with one of these three parallelization models if
it is supported by the program. The build configuration 'gcc' implicitly
defines a standard parallelization for each workload by falling back to one of
these three configurations for each benchmark program. 
 We also defined an alias for each parallelization model that will be resolved
resolved to a complete list of all workloads that support this parallelization
model. They have the same name as the parallelization. For example, to build
all workloads which support OpenMP with exactly that parallelization, you
can use: 
 parsecmgmt -a build -p openmp -c gcc-openmp
 
 Additional build configurations can be created with the tool 'bldconfadd'. For
example, to add a new configuration 'myconfig' that is based on gcc-serial,
you can use the following command: 
 bldconfadd -n myconfig -s gcc-serial
 
 To remove this configuration, you can use 'bldconfdel' as follows: 
 bldconfdel -n myconfig
 
 Performance Measurement & Research 
 For each benchmark, we define a Region-Of-Interest (ROI) which includes the
computationally intensive, parallelized phase of the benchmark, but not the
initialization or shutdown phase. Instead of measuring the total program
runtime, you can use and report the execution time of the ROI for any kind of
analysis and comparisons. We provide six inputs for each benchmark program: 
 
 'test'    Minimal input to verify that programs are executable. 
 'simdev'  Very small input which causes code execution comparable
            to a typical input for this program. Intended for
            microarchitectural simulator development. 
 'simsmall'    Small input for performance measurements with
            microarchitectural simulators 
 'simmedium'   Medium-sized input for performance measurements with
            microarchitectural simulators 
 'simlarge'    Large-sized input for performance measurements with
            microarchitectural simulators 
 'native'  Very large input intended for large-scale experiments
            on real machines 
 
 All inputs except 'test' and 'simdev' can be used for performance analysis. As
a rough guideline, on a Pentium 4 processor with 3.0 GHz you can expect
approximately the following execution times: 
 
 'test'    almost instantaneous 
 'simdev'  almost instantaneous 
 'simsmall'    ~1s 
 'simsmall'    ~3-5s 
 'simlarge'    ~12-20s 
 'native'  ~10-30min 
 
 The exact runtime depends on the program and its inputs and deviates in some
cases from the described rule of thumb. Different build configurations and
system parameters may also result in a deviation. 
 PARSEC Hooks 
 All benchmark programs of the benchmark suite support PARSEC hooks, a library
which allows rapid instrumentation of all benchmark. If the benchmarks are
compiled with hooks enabled, at various locations in the code the programs will
call the corresponding hook function. Among other things, the ROI has been
instrumented in such a way, and by default the PARSEC hooks library measures the
time spent in the ROI. 
 Documentation and code of the PARSEC hooks is available in the 'hooks' package
in group 'libs'. PARSEC hooks are enabled in the build configuration 'gcc-hooks'
and disabled in all other ones. 
 More Examples 
 Compatible to PARSEC 2.1 
 Build 'x264' and 'blackscholes' benchmarks with Intel compilers: 
 parsecmgmt -a build -p x264 blackscholes -c icc
 
 Build serial version of all kernels: 
 parsecmgmt -a build -p kernels -c gcc-serial
 
 Run a self-test with all applications: 
 parsecmgmt -a run -p apps -i test
 
 Use version of 'vips' and all kernels with enabled hooks to measure the
performance on a real machine with 32 threads: 
 parsecmgmt -a run -p vips kernels -c gcc-hooks -n 32
 
 Use SimpleScalar simulator to run a simulation with the 'gcc-hooks'
build configuration of the 'freqmine' application with small simulator inputs
and 4 threads: 
 parsecmgmt -a run -c gcc-hooks -s sim-outorder -p freqmine \
+ i simsmall -n 4
 
 Use an executable 'qsub' to submit runs with all kernels with 16 threads and
large simulator inputs to a batch system (this requires that a program 'qsub'
is installed and in the path which handles the job submission): 
 parsecmgmt -a run -s qsub -p kernels -i simlarge -n 16
 
 Clean up after a build or benchmarking run: 
 parsecmgmt -a fullclean -p all
 
 Uninstall 'gcc-serial' build of package 'gsl' and build a new version: 
 parsecmgmt -a uninstall -p gsl -c gcc-serial
parsecmgmt -a build -p gsl -c gcc-serial
 
 Network benchmarks 
 Check the status of all components involved in network benchmarks: 
     parsecmgmt -a status -p netapps
 
 Build network benchmark 'netstreamcluster': 
     parsecmgmt -a build -p raytrace
 
 Build all network benchmark: 
     parsecmgmt -a build -p netapps
 
 Run network benchmark 'netdedup' w/ input 'native' and 2 server threads: 
     parsecmgmt -a run -p netdedup -i native -n 2
 
 Run network benchmark 'netferret' w/ input 'simlarge', 4 server threads 
and 2 client connections: 
     parsecmgmt -a run -p netferret -i simlarge -n 4 -t 2
 
 For simulation, run 'netdedup' server on a simulator w/ 4 threads and 
run client on a real machine: 
     parsecmgmt -a run -p netdedup -i simlarge -n 4 -m server
    parsecmgmt -a run -p netdedup -i simlarge -m client
 
 Do a full cleanup for network benchmarks: 
     parsecmgmt -a fullclean -p netapps
 
 SPLASH-2 Suite and SPLASH-2x Suite 
 Check the status of SPLASH-2 suite and SPLASH-2x suite: 
     parsecmgmt -a status -p splash2
    parsecmgmt -a status -p splash2x
 
 Build benchmark 'raytrace' from SPLASH-2x suite other than PARSEC:
        parsecmgmt -a build -p splash2x.raytrace
        parsecmgmt -a build -p raytrace   ## defaultly from PARSEC (for comparison) 
 Build benchmark 'fft' from SPLASH-2 suite with 'gcc-serial' build configuration: 
     parsecmgmt -a build -c gcc-serial -p splash2.fft
 
 Build all benchmarks from SPLASH-2 suite and SPLASH-2x suite: 
     parsecmgmt -a build -p splash2
    parsecmgmt -a build -p splash2x
 
 Run benchmark 'fft' from SPLASH-2x w/ input 'simsmall' and 4 threads: 
     parsecmgmt -a run -p splash2x.fft -i simsmall -n 4
 
 Do a full cleanup for SPLASH-2 suite 
     parsecmgmt -a fullclean -p splash2
 
 Structure 
 The PARSEC suite is composed of the software packages, which are the benchmark
programs and their required libraries and tools, and the framework, which is
everything else. Software packages are located in the 'pkgs/' directory. Each
software package is part of exactly one package group, which has its own
subdirectory. For example, a package named 'foo' which belongs to group 'bar' 
would be located in the directory 'pkgs/bar/foo/'. 
 PARSEC has the following structure: 
 bin/        directory with PARSEC tools
config/     global configuration
log/        log files of builds and runs (dynamically created)
man/        man pages of the PARSEC distribution
pkgs/       package groups which contain the packages
version     file with version number of PARSEC distribution
 
 A package has the following directory structure: 
 inputs/     directory with input archives (optional)
inst/       installation directory (dynamically created)
obj/        build directory (dynamically created)
outputs/    directory with reference outputs (optional)
parsec/     PARSEC configuration files
run/        directory for program execution (dynamically created)
src/        source code of the package
version     file with package version
 
 Some of these directories will be auto-generated by 'parsecmgmt' on the fly. 
 Each package can have multiple builds and installations, and 'parsecmgmt' will
use separate subdirectories for them. PARSEC uses the name of the build
configuration and the platform for which the program is compiled to form a key
that is used to distinguish different builds and installations of a package. 
 Configuration Files 
 PARSEC distinguishes between global and local configuration. Global
configuration files are located in the 'config/' directory in the root of the
benchmark suite. Local configuration files are only valid for a single software
package and are stored in the 'parsec/' directory of the package they belong to. 
 The following types of configuration files exist: 'parsec.conf' is a global
file which defines the general structure of the benchmark suite, such as
aliases and which software packages exist. Files named ' .sysconf' define
the basic programs which 'parsecmgmt' is to use on each operating system
platform. To port 'parsecmgmt' to a new operating system, a corresponding file
has to be created. ' .runconf' are configuration files which determine how
benchmark programs are to be executed. Finally, files named '*.bldconf'
describe the build configuration of a package. Run and build configurations
are composed of both global and local configuration files. 
 Manual Usage 
 It is possible to build and run PARSEC benchmark programs manually. To do so,
the correct build configuration respectively run configuration should be used
as defined in the PARSEC configuration files. 
 The following steps are executed by 'parsecmgmt' to build a package: 
 
 Set variable PARSECDIR to the installation root of PARSEC 
 Set variable PARSECPLAT to the build key used to identify the platform 
 Source system configuration 
 Source global build configuration 
 Source local build configuration 
 Create build directory and change to it 
 If it exists, modify build environment and call configure script 
 Modify build environment and execute 'make' 
 Modify build environment and execute 'make install' 
 
 The following steps are executed by 'parsecmgmt' to run a benchmark: 
 
 Set variable PARSECDIR to the installation root of PARSEC 
 Set variable PARSECPLAT to the build key used to identify the platform 
 Source system configuration 
 Source global run configuration
        - Source local run configuration 
 Create run directory and cd to it 
 Unpack desired input 
 Execute benchmark with parameters from run configuration 
 
 Contact 
 You can reach the PARSEC team as follows: 
 <http://parsec.cs.princeton.edu/>
parsec@lists.cs.princeton.edu
 

This README is auto-generated with generate-readme.sh
Please add changes there.

 Reading List 
 
 This repository contains my open source reading list.
I keep track of books by editing the files here and
the results are automatically published as a website at
 http://bamos.github.io/reading-list . 
 This repository just contains stubbed examples of how to use
this project. Ping me if you're interested in my
personal reading list! 
 Goals 
 
 Plaintext and friendly data format. 
 Minimal hosting and deployment overhead. 
 Offline editing support. 
 
 Technologies Used 
 
 Linux and OSX. Windows should also work with Cygwin, but
  I haven't tried. Please file any issues related to this. 
 YAML  data. 
 GitHub Pages  hosts and automatically
  deploys a 100% client-side website that can also be edited offline.
   Bower  manages 3rd party library dependencies
  used on the site, stored in  bower.json . 
 
 Creating Your Reading List: Quick Start 
 
 Fork or copy the contents of this repository into a new GitHub repository.
  Make sure the default branch is set to  gh-pages  for deployment.
  At this point, you should be able to see my site hosted at.
   http://<your-github-name>.github.io/reading-list 
 Update the  data 
  and personalize  index.html .
  Push your changes to GitHub to see them immediately on the new site. 
 Replace links to http://bamos.github.io/reading-list with your URL. 
 
 Local Deployment 
 Most browsers will not be able to open  index.html  directly
from the filesystem because the js loads YAML resources.
One workaround is to use start a simple Python static
web server with  python2 -m SimpleHTTPServer 
and access the website with  localhost:8000 . 
 Updating Bower Dependencies 
 Run  bower update  to obtain the dependencies in  bower_components .
Run  ./update-vendor-deps.sh 
to copy the necessary portions into  vendor . 
 Scripts 
 The  scripts  directory contains Haskell and
Ruby scripts to select random books and quotes from
 data/finished.yaml . 
 Import from Goodreads 
 The  Goodreads Ruby script  by
 @seanosaur 
uses  Goodreads' API 
to import books into data files.
Please follow their ToS and add appropriate references
to Goodreads if this is used. 
 Warning : This script only pulls the first 200 books.
Improvements to this are being tracked in
 this issue . 
 Inspiration 
 The following projects inspired me to create
a GitHub-hosted reading list. 
 Name | Stargazers | Description
----|----|----
 cmonty/reading-list  | 8 | Track books I've read and any thoughts I've had. Also uses Wiki to track knowledge.
 coryschires/reading-list  | 18 | List of books and screencasts related to development, user experience design, and entrepreneurship. 
 DavidRagone/reading_list  | 2 | List of books I have read related to development, user experience design, and entrepreneurship
 eightbitraptor/reading_list  | 19 | 
 engeld/reading-list  | 0 | A collection of my reading list and notes.
 gbtekkie/ReadingList  | 2 | handy collection of tekkie readings
 jaredcacurak/reading-list  | 3 | My reading list. 
 People using this repo for their reading list 
 Ping me if you'd like to be added or removed. 
 Name | Stargazers | Description
----|----|----
 aerovolts/reading-list  | 0 | My personal reading list.
 ammadafsar/reading-list  | 0 | My reading list. Made so that I add things I will later read and commit to read them all in order.  
 connors511/reading-list  | 0 | My reading list. Made so that I add things I will later read and commit to read them all in order.
 jakehschwartz/reading-list  | 0 | My reading list.
 markroxor/reading-list  | 0 | My reading list.
 rwfeather/reading-list  | 0 | My reading list.
 samtron1412/reading-list  | 0 | My reading list.
 seanosaur/reading_list  | 2 | 
 wrideout/reading-list  | 2 | My reading list. 
 Credits and Licensing 
 All portions are
 MIT licensed 
by Brandon Amos unless otherwise noted. 
 This project uses and modifies the following open source projects
and resources.
Modifications remain under the original license. 
 | Project | Modified | License |
|---|---|---|
|  Twitter bootstrap  | No | MIT |
|  handlebars.js  | No | MIT License
|  IronSummitMedia/startbootstrap-grayscale  | Yes | Apache 2 |
|  makeusebrew/bootbox  | No | MIT |
|  MathJax  | No | Apache |
|  Flickr Photo  | Yes |  cc by-nc-sa 2.0  |
|  TimelineJS  | No | Mozilla Public License 
 LaTeX  is a typesetting program
for producing high quality technical documents.
Formatting LaTeX documents is difficult and modifying pre-built
templates often require extensive knowledge of the template.
This repository contains simple LaTeX templates for common documents.
Screenshots of each template are available on  this webpage ,
which is automatically created from  generate.py .
See my other LaTeX projects at  bamos/cv  and
 bamos/beamer-snippets . 
 
 Writing Check 
 btford/write-good  is a naive linter for English prose
and works well on LaTeX documents.
If  write-good  is installed, the Makefile's in this project will output
a list of warnings and tips for improving writing after
building the LaTeX documents. 
 Contributing 
 Contributions are highly welcomed!
If you want to add a similar template, please add to
the  latex-templates  directory and I'm happy to merge pull requests.
If you want to use the static webpage generation framework to present
a different set of templates with other motivations,
I'm happy to link to your project here. 
 Webpage Generation Process 
 The Python 3 script  generate.py  produces a static website in  dist .
 generate.py  loops through the collection of snippets and uses
 Jinja  templates to output LaTeX documents. 
 This project uses  Grunt  to deploy  dist  to  Github pages 
in the  gh-pages  branch with the  grunt-build-control  plugin.
 package.json  manages the  npm  dependencies.
Running  npm install  installs the dependencies. 
 
 grunt generate  produces the static site in  dist , and 
 grunt deploy  pushes the  dist  directory to the  gh-pages  branch. 
 
 Similar Projects 
 There are many approaches to sharing LaTeX templates online,
and this project uniquely adds a static webpage generation process
to generate previews of templates managed in Git.
The following list shows a short sampling of projects,
and I'm happy to merge pull requests of other projects. 
 Git Repositories 
 
To generate the following list, install https://github.com/jacquev6/PyGithub
and download the `github-repo-summary.py` script from
https://github.com/bamos/python-scripts/blob/master/python3/github-repo-summary.py.
Please add projects to the list in the comment and in the table below.

github-repo-summary.py \
  cmichi/latex-template-collection \
  deedydas/Latex-Templates \
  MartinThoma/LaTeX-examples \
  RichardLitt/latex-templates \
  stevegeek/latex-templates
 
 Generated on 2015-10-03, see the Markdown source of this file for more details. 
 Name | Stargazers | Description
----|----|----
 cmichi/latex-template-collection  | 255 | A collection of different LaTeX templates (cv, invoices, timesheets, letters, etc.).
 deedydas/Latex-Templates  | 58 | A concise set of Latex templates that serves a small set of needs - CV, Essays, Articles and Problem Sets
 MartinThoma/LaTeX-examples  | 176 | Examples for the usage of LaTeX
 RichardLitt/latex-templates  | 1 | My Personal LaTeX Templates
 stevegeek/latex-templates  | 20 | A collection of my LaTeX templates: CV (resume), letter head and PhD Thesis 
 Websites 
 
 http://www.latextemplates.com/ 
 https://www.sharelatex.com/templates 
 https://www.writelatex.com/templates 
 
 Licensing 
 All modified referenced code has license of the original source.
All other portions are under the MIT license. 
 
 This is a collection of short Python scripts I use in Linux.
Portions of this README are automatically generated with
 generate-readme.py 
to insert the script-level comments as descriptions below. 
 Adding to your PATH 
 I have these added to my  PATH 
 variable 
to run from anywhere.
Clone the repo and add the following
to your  bashrc  or  zshrc , replacing  <python-scripts> 
with the location of the cloned repository.
See my  dotfiles 
repo for my complete Mac and Linux system configurations. 
 ```Bash 
 Add additional directories to the path. 
 pathadd() {
  [ -d ""$1"" ] && [[ "":$PATH:"" !=  "":$1:""  ]] && PATH=""${PATH:+""$PATH:""}$1""
} 
 pathadd  /python2.7
pathadd  /python3
``` 
 Dependencies 
 These scripts are written in Python 3 except when external
libraries don't support Python 3.
Dependencies for Python 2 and 3 for all scripts are
included in  requirements-{2,3}.txt  and can be installed
using  pip  with  pip3 install -r requirements-3.txt . 
 Travis CI 
 Continuous integration is provided by Travis CI
 here .
 .travis.yml 
calls
 .travis-script-2.sh 
and
 .travis-script-3.sh 
to ensure  requirements-{2,3}.txt  have all of the Python 2 and Python 3 scripts
and that there are no careless errors that cause the scripts to
not execute the help message.
 pep8  will fail the build
if pep8 styling conventions aren't met. 
 Scripts 
 The subheadings link to the script contents on GitHub. 
 generate-readme.py 
 
 Authors:  Brandon Amos 
 Created: 2015.03.27 
 
 Generates the README for
 bamos/python-scripts 
so that the README and scripts contain synchronized documentation.
Script descriptions are obtained by parsing the docstrings
and inserted directly into the README as markdown. 
 python2.7/music-organizer.py 
 
 Authors:  Brandon Amos 
 Created: 2014.04.19 
 
 This script (music-organizer.py) organizes my music collection for
iTunes and  mpv  using tag information.
The directory structure is  <artist>/<track> , where  <artist>  and  <track> 
are lower case strings separated by dashes. 
 See my blog post
 Using Python to organize a music directory 
for a more detailed overview of this script. 
 python2.7/mt.py 
 
 Authors:  Brandon Amos 
 Created: 2014.11.30 
 
 This script implements the simple
 multitail 
example to tail multiple files and append the filename to the beginning
of the output. 
 python2.7/caffe-compute-image-mean.py 
 
 Authors:  Brandon Amos 
 Created: 2015.08.10 
 
 This script computes the mean of a directory of images for Caffe. 
 python2.7/fix-music-tags.py 
 
 Authors:  Brandon Amos 
 Created: 2015.12.30 
 
 This script (fix-music-tags.py) mass-removes unwanted music tags. 
 python3/github-repo-summary.py 
 
 Authors:  Brandon Amos 
 Created: 2014.11.02 
 
 Produces a Markdown table concisely summarizing a list of GitHub repositories. 
 python3/link-checker.py 
 
 Authors:  Brandon Amos 
 Created: 2014.02.06 
 
 Script to be run by crontab to report broken links. 
 Builds upon linkchecker (Ubuntu: sudo apt-get install linkchecker)
to hide warnings and to send a concise email if bad links are found. 
 
 python3/phonetic.py 
 
 Authors:  Brandon Amos 
 Created: 2014.02.14 
 
 Obtain the NATO phonetic alphabet representation from short phrases. 
 $ phonetic.py github
g - golf
i - india
t - tango
h - hotel
u - uniform
b - bravo 
 python3/rank-writing.py 
 
 Authors:  Brandon Amos 
 Created: 2014.02.14 
 
 rank-writing.py  ranks the writing quality of my
blog's Markdown posts and my project's Markdown README files. 
 The following programs should be on your  PATH :
+  aspell 
+  write-good 
+  diction 
 ```
$ rank-writing.py *.md 
 === 2013-05-03-scraping-tables-python.md ===
Total: 53
├── aspell: 34
├── diction: 0
└── write-good: 19 
 ... 
 === 2013-04-16-pdf-from-plaintext.md ===
Total: 0
├── aspell: 0
├── diction: 0
└── write-good: 0
``` 
 python3/get-osx-wallpaper.py 
 
 Authors:  Brandon Amos 
 Created: 2015.03.25 
 
 This is a Python script that outputs the path of the current
OSX wallpaper.
This is helpful when the desktop wallpaper is randomized
across a large collection of pictures and you want to
delete the current wallpaper. 
 Warning 
 
 This approach doesn't work with multiple monitors or virtual desktops. 
 
 Tested On 
 
 OSX Yosemite 10.10.2 with a single desktop on a MBP. 
 
 Usage 
 Ensure  db_path  and  wallpaper_dir  are correctly set below. 
 Assuming  get-osx-wallpaper.py  is on your path,
check the output with the following 
 $ get-osx-wallpaper.py
/Users/bamos/Pictures/wallpaper/nature/496.jpg 
 Please ensure this is correct before trying to remove it! 
 This can be paired with other commands such as  open  or  rm .
Run  killall Dock  to refresh the changes after removing the file.
Note that the dock will be restarted and all windows will be
unminimized. 
 $ open $(get-osx-wallpaper.py)
$ rm $(get-osx-wallpaper.py) && killall Dock 
 Example alias definitions for bash and zsh are available in
https://github.com/bamos/dotfiles/blob/master/.funcs: 
 alias open-wallpaper='open $(get-osx-wallpaper.py)'
alias rm-wallpaper='rm $(get-osx-wallpaper.py) && killall Dock' 
 python3/merge-pdfs-printable.py 
 
 Authors:  Brandon Amos 
 Created: 2014.10.17 
 
 The printers in my office print a cover page before every job, and
I don't like printing many cover pages if I want to submit multiple
papers separately so that the papers don't overlap. This script will
merge PDF documents and insert blank pages so that the printed pages
won't overlap documents. The modulo option is helpful to print 2 PDF
pages per physical page side. 
 The script uses PyPDF2 to merge the documents and to extract the
number of pages in the input documents and ghostscript to create a
blank PDF page. 
 $ merge-pdfs-printable.py a.pdf b.pdf c.pdf --modulo 4
a.pdf
  + Pages: 6
  + Added 2 blank pages.
b.pdf
  + Pages: 13
  + Added 3 blank pages.
c.pdf
  + Pages: 13
  + Added 3 blank pages.
Merged output is in '/tmp/tmpm2n5g0mh-merge.pdf'. 
 Note: Some of my decrypted PDF documents have resulted in
PyPDF2.utils.PdfReadError: file has not been decrypted. My current
workaround solution is to run pdf2ps on the PDF and then ps2pdf on the
PS file. 
 python3/remove-duplicates.py 
 
 Authors:  Brandon Amos 
 Created: 2015.06.06 
 
 Detect and remove duplicate images using average hashing. 
 http://www.hackerfactor.com/blog/index.php?/archives/432-Looks-Like-It.html 
 python3/word-counter.py 
 
 Authors:  Brandon Amos 
 Created: 2014.11.7 
 
 Count work frequencies within a file. 
 ```
$ word-counter.py shakespeare.md --numWords 4 --maxTuples 3 
 === Sliding Window: 1 ===
    3473: 'shall'
    2238: 'would'
    2153: 'which'
    2074: 'their' 
 === Sliding Window: 2 ===
    248: 'exeunt scene'
    117: 'second lord.'
    105: 'first lord.'
    102: 'queen elizabeth.' 
 === Sliding Window: 3 ===
    36: 'william shakespeare dramatis'
    34: 'shakespeare dramatis personae'
    18: 'comes here? enter'
    14: 'duke's palace enter'
``` 
 python3/eval-expr.py 
 
 Authors: J. Sebastian,  Brandon Amos 
 Created: 2013.08.01 
 
 A module to evaluate a mathematical expression using Python's AST. 
 
 Original by: J. Sebastian at http://stackoverflow.com/questions/2371436. 
 Modifications by:  Brandon Amos . 
 
 If you want a command-line expression evaluator, use
 Russell91/pythonpy . 
 $ eval-expr.py '(((4+6)*10)<<2)'
(((4+6)*10)<<2) = 400 
 python3/merge-mutt-contacts.py 
 
 Authors:  Brandon Amos 
 Created: 2014.01.08 
 
 Merges two mutt contact files. 
 Similar Projects 
 There are many potpourri Python script repositories on GitHub.
The following list shows a short sampling of projects,
and I'm happy to merge pull requests of other projects. 
 Name | Stargazers | Description
----|----|----
 averagesecurityguy/Python-Examples  | 26 | Example scripts for common python tasks
 ClarkGoble/Scripts  | 29 | My scripts - primarily using python and appscript
 computermacgyver/twitter-python  | 66 | Simple example scripts for Twitter data collection with Tweepy in Python
 gpambrozio/PythonScripts  | 38 | A bunch of Python scripts I made and that might interest somebody else
 realpython/python-scripts  | 568 | because i'm tired of gists This repo contains the source for my CV: 
 
 generate.py  creates a  website 
  and  PDF 
  from a shared  YAML source 
  by using Jinja templates. 
 The publications are rendered from a single
   BibTeX  file.
  The abstracts are displayed in the website output
  and the selected publications here are highlighted. 
 The  YAML source  links to all author websites,
  which will automatically be added to the
  publication lists in the website and PDF. 
 GitHub stars are automatically scraped and cached on disk. 
 
 Building and running 
 Dependencies are included in  requirements.txt  and can be installed
using  pip  with  pip3 install -r requirements.txt .
 make  will call  generate.py  and
build the LaTeX documents with  latexmk  and  biber .
The Makefile can also: 
 
 Stage to my website with  make stage , 
 Start a local jekyll server of my website with updated
  documents with  make jekyll , and 
 Push updated documents to my website with  make push . 
 
 What to modify 
 Change the content in  cv.yaml .
You should also look through the template files to make sure there isn't any
special-case code that needs to be modified.
The  Makefile  can also start a Jekyll server and push the
new documents to another repository with  make jekyll  and  make push . 
 Warnings 
 
 Strings in  cv.yaml  should be LaTeX (though, the actual LaTeX formatting
   should be in the left in the templates as much as possible). 
 If you do include any new LaTeX commands, make sure that one of the
    REPLACEMENTS  in  generate.py  converts them properly. 
 The LaTeX templates use modified Jinja delimiters to avoid overlaps with
   normal LaTeX. See  generate.py  for details. 
 
 Other people using this code 
 You are welcome to use this code with or without attribution in the
documents you produce, and add a link back here if you want! 
 
 
 Alessandro Checco 
 Alex Sludds 
 Amara Dinesh Kumar 
 Boyo Chen 
 Chaitanya Ahuja  ( code ) 
 Chaitanya Bapat 
 Chieh Hubert Lin (林杰) 
 Colin Clement  ( code ) 
 Daniel Schaefer 
 David B. Lindell  ( code ) 
 Emir Ceyani  ( code ) 
 Franziska Meier 
 Guojin Chen 
 Jean Nassar 
 Joan Cano 
 Juan Martín Loyola  ( code ) 
 Jun Xiong 
 Jérémie Lumbroso 
 Krishnaditya Kancharla 
 Lamin Juwara  ( code ) 
 Lizeth Joseline Fuentes Pérez  ( code ) 
 Marco Piccirilli 
 Matthew L. Bendall 
 Murali Koppula 
 Nathan P. Lawrence  ( code ) 
 Nazim Coskun 
 Nicholas J. Loman 
 Nikos Doulaveras 
 Norman Kabir 
 Nurpeiis Baimukan 
 Olalekan Ogunmolu 
 Pieter Vanderpol 
 Prachi Sudrik 
 Pınar Demetçi 
 Qian Ge 
 Renan Souza  ( code ) 
 Stefan Doerr 
 Steve T.K. Jan 
 Swaminathan Gurumurthy 
 Vinayakumar Ravi 
 Wen-Yen Chang 
 Wilka Carvalho 
 Yann-Aël Le Borgne 
 You-Feng Wu 
 About. 
 Build LLVM, Polly, and PoCL in an Ubuntu 12.04 Docker image.
This provides 2 docker images for a stable LLVM 3.3 build
and the latest LLVM 3.4 builds.
See the  llvm-3.3  and  llvm-3.4 . Github Wiki Link Validator 
 Ensure all internal links in a Github Wiki are valid. 
 Github wiki's present a unique problem for link validation because
 traditional validation 
is usually done by checking page response (usually for 404).
However, the default behavior of a Wiki for an invalid link
is to direct to a page creating the page, and there is currently
no option to change this.
Creating new pages like this is easy, but in large Wiki's
links become difficult to manage and can lead to difficulties
validating links. 
 This is a Python 3.3 script to crawl a published Github wiki and
detect internal links pointing to invalid locations. 
 Dependencies. 
 
 Python 3.3 
 BeautifulSoup 4 
 
 Example. 
 $ ./link-validator.py https://github.com/bamos/github-wiki-link-validator/wiki 
 
 https://github.com/bamos/github-wiki-link-validator/wiki 
 https://github.com/bamos/github-wiki-link-validator/wiki/links 
 https://github.com/bamos/github-wiki-link-validator/wiki/invalid 
 https://github.com/bamos/github-wiki-link-validator/wiki/B 
 https://github.com/bamos/github-wiki-link-validator/wiki/C 
 https://github.com/bamos/github-wiki-link-validator/wiki/Valid 
 https://github.com/bamos/github-wiki-link-validator/wiki/C 
 https://github.com/bamos/github-wiki-link-validator/wiki/D 
 https://github.com/bamos/github-wiki-link-validator/wiki/E 
 Beamer Snippets 
 The  Beamer  package enables  LaTeX  to produce high quality
technical presentations and can be used in combination with the
 TikZ  package for including publication-quality diagrams in presentations.
However, formatting LaTeX presentations using Beamer and TikZ is
difficult and time consuming, so I often reuse code snippets from
previous presentations.
This repository contains a collection of snippets I'm gathering from
my papers and presentations, which are best viewed on
 this webpage .
See my other LaTeX projects at  bamos/cv  and
 bamos/latex-templates . 
 
 Contributing. 
 Contributions are highly welcomed!
Please add to the  snippets  directory, modify the
 preamble , and send a pull request.
If you substantially modify the preamble, I recommend keeping
your repo as a fork, and I'll keep a list of forks in this README. 
 Webpage Generation Process. 
 The Python 3 script  generate.py  produces a static website in  dist .
 generate.py  loops through the collection of snippets and uses
 Jinja  templates to output LaTeX documents.
 Makefile.slides  is then used to generate PNG images from
each slide, and Jinja HTML templates are used to create  index.html . 
 Grunt 
 Grunt  is used to deploy  dist  to  Github pages 
in the  gh-pages  branch with the  grunt-build-control  plugin.
The  npm  dependencies are managed in  package.json 
and can be installed with  npm install . 
 
 grunt generate  produces the static site in  dist , and 
 grunt deploy  pushes the  dist  directory to the  gh-pages  branch. 
 
 Licensing. 
 All LaTeX portions are released into the public domain,
and other portions are under the MIT license. Zsh History Analysis 
 zsh  logs commands and timestamps to  ~/.zsh_history  for
shell features such as reverse history search.
This repository is a fun project that provides shell, Python, and R
scripts to parse, analyze, visualize  .zsh_history  files.
These scripts can be extended to support Bash's  .bash_history . 
 Getting Started 
 You can run this on your  .zsh_history  files by cloning this repository
with  git clone https://github.com/bamos/zsh-history-analysis.git 
and installing the following prerequisites.
Ensure you have increased the history file size so commands aren't removed.
Then, follow the steps in  Control Flow  to generate the plots. 
 Prerequisites 
 
 PATH  contains  python3  and  Rscript , which can be installed from
  your package manager.
  In Arch Linux, the required packages are
   python 
  and  r . 
 R :  ggplot2  and
   reshape  are installed
  from an R shell with  install.packages(""ggplot2"") 
  and  install.packages(""reshape"") . 
 
 Increasing the History File Size 
 Unfortunately, zsh's default history file size is limited to
10000 lines by default and will truncate the history to this
length by deduplicating entries and removing old data. 
 Adding the following lines to  .zshrc  will remove the limits and
deduplication of the history file. 
 Bash
export HISTSIZE=1000000000
export SAVEHIST=$HISTSIZE
setopt EXTENDED_HISTORY 
 Control Flow 
 The following is the control flow for generating plots. 
 
 Archive all  .zsh_history  files in  data/<server>.zsh_history .
 ./pull-history-data.sh  is a script to partially help archiving the data
that will pull files from a list of servers separated by newlines in a
file named  servers . 
 Run  ./analyze.py  to analyze the raw data files.
 ./analyze.py --help  will provide a help menu with the supported options. 
 Run  ./plot.r  to generate plots from the analyzed data. 
 
 Sample Results 
 Command Frequencies 
 At a given hour or weekday, how frequently do I run commands?
The following shows the average number of commands executed
for each hour and weekday.
I average 10 commands per hour overnight and
a little more during the day, and Wednesdays seem to be
my least productive days. 
 
 
 Many hours have 0 commands executed since I'm not typing commands every hour of
every day, so these points have a high standard deviation.
 Empirical Cumulative Distribution Functions (ECDF's) 
provide a deeper visualization of the distributions. 
 
 
 Average command length 
 
 What command was over 100 characters!?
 analyze.py  will output the top five commands, and these
long commands are from using the full path to an executable,
such as the Android ARM cross compiler, as shown in the following output. 
 Bash
$ ./analyze.py commandLengths
  105: /opt/android-ndk-r9/toolchains/arm-linux-androideabi-4.8/prebuilt/linux-x86/bin/arm-linux-androideabi-gcc 
 Scoping into the majority of the data shows that almost 50% of my
commands are one or two characters. 
 
 Top Commands 
 Since almost 50% of my commands are one or two characters,
what are the top commands?
The following plot shows the top commands are Linux utilities
and  oh-my-zsh  aliases. 
 
 My Undergrad vs Grad Student Frequencies 
 
 GitHub README Link Checker 
 
  ![](https://raw.githubusercontent.com/bamos/girl/master/screenshot.png)  
 girl  is a  Gi thub  R eadme  L ink Checker
served over HTTP with  Scala 
and  Spray .
A public version is hosted at  http://bamos.github.io/girl . 
 Whitelist 
 To prevent misuse, girl restricts usage to
GitHub users with
over 50 followers or users and organizations on the
 whitelist .
Please add your accounts
and submit a pull request to gain access.
Thanks! 
 Building 
 Before running locally,
set your  GitHub API token 
in the environment variable  GITHUB_TOKEN ,
or modify the GitHub API connection in
 Girl.scala 
to another option from
 kohsuke.github.GitHub .
Also in  Girl.scala , if desired, set the minimum number of
required followers to zero:  val reqFollowers = 0 . 
 girl  is built with  sbt .
Executing  sbt run  from the  girl  directory will download
the dependencies, compile the source code, and start
an HTTP server on  0.0.0.0:8585 .
 Main.scala 
configures the interface and port. 
 sbt-revolver  is helpful for development.
Start an  sbt  shell and execute  ~re-start ,
which re-compiles and restarts the server upon source code changes. 
 Deployment with Docker 
 Girl can be deployed as a container with  Docker .
After replacing the string  <token>  in the
 Dockerfile 
with your GitHub API token, the following command
will build and start the girl as an HTTP server on port 8585
of the container. 
 docker build -t girl . 
 Running as a System Service 
 girl.service 
is an example  systemd 
service that calls
 start-service.sh 
to automatically start girl with the system. 
 Modify the paths to this repo on your system in both of the scripts
and copy  girl.service  to  /etc/systemd/system/girl.service .
A symlink will not work, see
 this bug report 
for more details. 
 Basic controls are: 
 sudo systemctl start girl
sudo systemctl stop girl
sudo systemctl restart girl 
 And run on startup with: 
 sudo systemctl enable girl 
 Licensing 
 All portions are  MIT-licensed . 
 
 SnowGlobe provides minimal-configuration web analytics
for small-scale and personal websites. 
 SnowGlobe integrates components from the
 Snowplow analytics framework  to achieve this.
The  JavaScript tracker  pipes into the
 Scala collector  and  Scala enricher 
and outputs  Snowplow enriched events 
to a tab separated file. 
 SnowGlobe uniquely provides Haskell-driven analytics on the data. 
 Crafted by  Brandon Amos . 
 Motivation: Why create SnowGlobe? 
 I've used  Google Analytics ,
 Piwik , and  Clicky 
for my personal website analytics over the last 3 years.
These are great systems and I still use some of them for
the web interfaces and other analytics.
However, I enjoy hacking on the systems I use and
felt limited. 
 My ongoing goals in building SnowGlobe are to: 
 
 Pipe a short daily summary of my real-time analytics to my LCD display. 
 Email me daily, weekly, monthly, and yearly reports with
   the information I want. 
 Create interesting real-time visualizations (not started) 
 
 Why not use SQL to query the data? 
 You should.
I'm using this project to learn Haskell. 
 Progress 
 Real-time summary on an LCD display 
 Somebody in my research group gave me a small LCD display
a few weeks ago (in May 2015) and said that he couldn't
find any useful information to put on it,
This, along with my other goals, inspired me to finally
create SnowGlobe. 
 Piping a daily summary to my LCD display with SnowGlobe works well.
I've replaced my actual stats with the string  NNNN | MMM , which
represents that I've had  NNNN  total page views by  MMM  visitors.
If you're curious, the other information on the device are the
number of tasks I have (for today, tomorrow, and the next day),
and the temperature and amount of GPU memory I'm currently
using for experiments. 
 
 Recurring Reports 
 Daily recurring reports are also working well.
I plan to expand the information in the weekly reports
and will add monthly and yearly reports in the future.
The following screenshot shows an example report
from my email client,  mutt .
More information on setting these reports up below. 
 
 Prerequisites, installing, and configuration 
 
 Install wget, Python, sbt, scala, and the JRE (≥ 1.7). 
 If the default Java distribution is not version 1.7 or above,
   set  JAVA_HOME  in  scripts/env.sh . 
 Run  ./scripts/bootstrap.sh  to download Snowplow and GeoLite binaries. 
 Update the  uri  in  ip_lookups.json 
   to use the absolute path to your repository. 
 
 Collecting and storing events to TSV files 
 On the server, start the collector and enricher with the following
instructions.
For persistent collection, use an init daemon (preferred),
or a detached screen or tmux session. 
 
 conf/collector.conf  and  conf/enrich.conf  contain sensible default
   configurations for Snowplow's collector and enricher. 
 scripts/start-collect-enrich.sh  will start Snowplow's collector on port
  8081 to pipe Base 64 serialized Thrift raw event output to Snowplow's
  Scala enricher.
  The enricher will output  Snowplow enriched events 
  as rows in  data/events.tsv , which will use ~720 bytes per event.
  This script uses  stdbuf  from GNU coreutils to disable stdout
  buffering. 
 snowglobe.service  is a
   systemctl unit 
  to run  start-collect-enrich.sh .
  Copy the config with  sudo cp sample/snowglobe.service /etc/systemd/system/ 
  and reload the units with  sudo systemctl daemon-reload .
  Do not use a symlink because of
   this  systemd behavior/bug.
  Start the service with  sudo systemctl start  and
  run on boot with  sudo systemctl enable . 
 
 Adding JavaScript tags to your webpages 
 Next, ensure that the collector and enricher are properly configured
and started by opening  sample/index.html  on the server.
 data/events.tsv  should now contain the tab separated event. 
 Next, copy and paste the following code to your webpage's
templates to send events on the pages you wish to track,
and add Snowplow's JavaScript library  sp.js  to your website's
resource directory.
Make sure to change  localhost:8081  to your server and port,
and ensure the port is open. 
 ```JavaScript 
 
 
try {
  // Use localhost as the server for testing on the same computer,
  // but change to your deployed server IP address or hostname
  // for production.
  var snowplowTracker = Snowplow.getTrackerUrl('localhost:8081');
  snowplowTracker.enableLinkTracking();
  snowplowTracker.trackPageView();
} catch (err) {}
 
 ``` 
 Analytics 
 Main.hs 
is the entry point for analytics.
This is a single-threaded application and not ready for large deployments,
but easy to make small changes to.
 optparse-applicative 
parses command-line arguments
and
 cassava 
parses the raw events file into an
 EnrichedEvent , defined in
 EnrichedEvent.hs ,
and contains features of SnowPlow's
 EnrichedEvent.scala . 
 EnrichedEvent.hs ,
further enriches events by providing
 getLocation  and  getOrganization  functions,
which use
 hs-GeoIP 
and
 whois-hs .
SnowPlow's enricher should add this information to events,
but there's a subtle bug in my configuration
or the software that I'll further track
down in issue
 #4 
sometime. 
 Queries.hs 
groups, filters, and extracts information from the events. 
 Building 
 To build the analytics binary, first install  GHC , the
Glasgow Haskell Compiler, and  cabal , a build system
and library database:
  + OSX:  brew install ghc cabal-install 
  + Ubuntu:  apt-get install ghc cabal-install 
  + Arch:  pacman -S ghc cabal-install 
 Next,  cd  into the  snowglobe  directory and
install the Haskell dependencies and build
the code with:  cabal install .
The binary  snowglobe-analytics  is now
in  ~/.cabal/bin/snowglobe-analytics .
You can add  cabal s bin directory to your  PATH  or use
the full path. 
 Command Line Interface 
 ```
$ snowglobe-analytics
SnowGlobe Analytics 
 Usage: snowglobe-analysis --events FILE COMMAND 
 Available options:
-h,--help                Show this help text
--events FILE            Location of events.tsv 
 Available commands:
DaySummary               Print a concise summary for the current day.
DayReport                Print a report for the current day.
WeekReport               Print a report for the past week.
``` 
 DaySummary: Status on LCD 
 Using  snowglobe-analytics  in  DaySummary  mode on the command
line outputs the following, showing  NNNN  total page views by  MMM  visitors. 
 $ snowglobe-analysis --events data/events.tsv DaySummary
NNNN | MMM 
 I pipe this output to my LCD display with
 LCD4Linux 
by defining the following widget in  /etc/lcd4linux.conf .
LCD4Linux's
 exec documentation 
says to use  exec  as a last resort,
but the following is working well for me so far. 
 Widget Hits {
    class 'Text'
    expression exec('su bamos -c ""/home/bamos/.cabal/bin/snowglobe-analysis --events /home/bamos/repos/snowglobe/data/events.tsv DaySummary""',10000)
    width 20
    align 'R'
    prefix 'web'
    update 1000
} 
 In the layout, define a row to use the output from this widget. 
 Layout stats {
  ...
  Row2.Col1 'Hits'
  ...
} 
 Recurring Email Reports 
 The  DayReport  mode outputs the report in plaintext to stdout.
Pipe this output to your favorite command-line mailer
and add it to your favorite job scheduler.
I use  mutt  and vanilla  cron , respectively. 
 The following  cron  entry calls
 analytics-daily-report.sh  and
 analytics-weekly-report.sh . 
 59 23 * * * /home/bamos/repos/snowglobe/scripts/analytics-daily-report.sh
59 23 * * 0 /home/bamos/repos/snowglobe/scripts/analytics-weekly-report.sh 
 Licensing 
 SnowGlobe portions copyright 2014-2015 Brandon Amos under the
Apache License. 
 Snowplow portions retain their licenses from Snowplow Analytics Ltd.
and remain unmodified. 
 The  original SnowGlobe graphic 
is open-sourced under
 CC-SA 2.0 ,
and my modifications are under the same license. Subscribe for Weekly Emails 
 Crafted by  Brandon Amos . 
 
 
 This repository contains my minimal-maintenance conference tracker
and an opinionated list of CS conferences.
I find this slightly easier than keeping the dates on Google Calendar
because I can glance at the entire list, keep track of URLs,
and add reminders when I need to add next year's conference. 
 
 Project Structure 
 The Python source in  report.py  generates a plaintext
report from the  YAML data .
I have a  Cron  job send
weekly emails with  email-report.sh 
and  mutt . 
 Adding and Updating Data 
 As conferences pass and the data becomes outdated,
I'm happy to accept pull requests with updated data.
If you want to track yours by adding/removing conferences,
please fork this repo. 
 Conference Lists 
 
 Wikipedia 
 RichardLitt/awesome-conferences 
 WikiCFP: CS 
 ecos.torch •            
 Unofficial ECOS bindings to solve linear programs (LPs) and
second-order cone programs (SOCPs) in Torch. 
 
 You may also be interested in:
+ Torch  Gurobi  bindings at
   bamos/gurobi.torch .
+ A Torch library for convex optimization that implements
  spectral projected gradient and projected gradient descent at
   bamos/cvx-optim.torch . 
 Introduction to ECOS 
 Visit http://www.embotech.com/ECOS for detailed information on ECOS. 
 ECOS is a numerical software for solving convex second-order cone programs (SOCPs) of type 
 min  c'*x
s.t. A*x = b
G*x <=_K h 
 where the last inequality is generalized, i.e.  h - G*x  belongs to the cone  K .
ECOS supports the positive orthant  R_+ , second-order cones  Q_n  defined as
 Q_n = { (t,x) | t >= || x ||_2 } 
with t a scalar and  x  in  R_{n-1} ,
and the exponential cone  K_e  defined as 
 K_e = closure{(x,y,z) | exp(x/z) <= y/z, z>0} 
 where  (x,y,z)  is in  R_3 .
The cone  K  is therefore a direct product of the positive orthant,
second-order, and exponential cones: 
 K = R_+ x Q_n1 x ... x Q_nN x K_e x ... x K_e 
 This Library 
 This repository provides unofficial  Torch  bindings to
the  ECOS C API . 
 Setup 
 After  setting up Torch ,
this library can be installed with: 
 bash
luarocks install https://github.com/bamos/ecos.torch/raw/master/ecos-scm-1.rockspec 
 or equivalently: 
 bash
git clone https://github.com/bamos/ecos.torch.git --recursive
cd ecos.torch
luarocks make 
 Usage 
 Linear Program 
 ```lua
local ecos = require 'ecos' 
 local G = torch.Tensor{{-1, 1}, {-1, -1}, {0, -1}, {1, -2}}
local h = torch.Tensor{1.0, -2.0, 0.0, 4.0}
local c = torch.Tensor{2.0, 1.0} 
 local status, x = ecos.solve{c=c, G=G, h=h}
print(x) -- Optimal x is [0.5, 1.5]
``` 
 Tests 
 After installing the library with  luarocks , our tests in
 test.lua 
can be run with  th test.lua . 
 Issues and Roadmap 
 
 While this is working well for linear programs, I am not using
   and have not tested the second-order or exponential cones.
   Please comment in
    Issue 1 
   if you are interested in using these. 
 ECOS uses a sparse matrix format and the code current converts
   a dense Torch tensor to a sparse array in C in every call.
   This could potentially be optimized by using sparse Torch
   tensors and doing the conversion in Torch.
   Then the user could then optionally maintain sparse
   Tensors in Torch.
   Please comment in
    Issue 2 
   if you are interested in this. 
 
 License 
 
 ECOS is under the GPL and remains unmodified. 
 The original code in this repository (the ECOS bindings) is
   Apache-licensed . 
 gurobi.torch •            
 Unofficial Gurobi Torch bindings. 
 
 You may also be interested in:
+ Torch  ECOS  bindings at
   bamos/ecos.torch .
+ A Torch library for convex optimization that implements
  spectral projected gradient and projected gradient descent at
   bamos/cvx-optim.torch . 
 Installation 
 
 Set  GUROBI_HOME  in your shell's current session (with  export GUROBI_HOME=.. )
   and initialization file ( .bashrc  or  .zshrc ). 
 luarocks make 
 
 Usage 
 The following solves the linear program 
 min  c'*x s.t. G*x <= h 
 Linear Program 
 ```lua
local gurobi = require 'gurobi' 
 local G = torch.Tensor{{-1, 1}, {-1, -1}, {0, -1}, {1, -2}}
local h = torch.Tensor{1.0, -2.0, 0.0, 4.0}
local c = torch.Tensor{2.0, 1.0} 
 local env = gurobi.loadenv("""")
local model = gurobi.newmodel(env, """", c)
gurobi.addconstrs(model, G, 'LE', h)
local status, x = gurobi.solve(model)
print(x) -- Optimal x is [0.5, 1.5] 
 gurobi.free(env, model)
``` 
 Linear Program with Incrementally Added Constraints 
 ```lua
local gurobi = require 'gurobi' 
 -- minimize y
-- subject to y >= x
--            y >= -x
--            y >= x + 1
local c = torch.Tensor{0.0, 1.0}
local G = torch.Tensor{{1, -1}, {-1, -1}, {1, -1}}
local h = torch.Tensor{0.0, 0.0, -1.0} 
 local env = gurobi.loadenv("""")
local model = gurobi.newmodel(env, """", c) 
 local I = {{1,2}}
gurobi.addconstrs(model, G[I], 'LE', h[I]) 
 local status, x = gurobi.solve(model)
print(x) -- Optimal at this point is [0, 0] 
 gurobi.addconstr(model, G[3], 'LE', h[3])
status, x = gurobi.solve(model)
print(x) -- Optimal at this point is [-0.5, 0.5] 
 gurobi.free(env, model)
``` 
 OpenMP Parallel Solves 
 ```lua
local env = gurobi.loadenv("""")
local model1 = gurobi.newmodel(env, """", c)
gurobi.addconstrs(model1, G, 'LE', h) 
 local model2 = gurobi.newmodel(env, """", c)
gurobi.addconstrs(model2, G, 'LE', h) 
 local status, xs = gurobi.solvePar({model1, model2})
``` 
 Tests 
 After installing the library with  luarocks , our tests in
 test.lua 
can be run with  th test.lua . 
 Licensing 
 
 Gurobi is proprietary software. 
 The original code in this repository (the Gurobi bindings) is
   Apache-licensed . 
 Image Completion with Deep Learning in TensorFlow 
 
 
 See my blog post for more details and usage instructions . 
 This repository implements Raymond Yeh and Chen Chen et al.'s paper
   Semantic Image Inpainting with Perceptual and Contextual Losses . 
 Most of the code in this repository was written by modifying a
  duplicate of  Taehoon Kim's 
 DCGAN-tensorflow  project,
  which is MIT-licensed.
  My modifications are also  MIT-licensed . 
 The  ./checkpoint  directory contains a pre-trained
  model for faces, trained on the CelebA dataset for 20 epochs. 
 
 Citations 
 Please consider citing this project in your
publications if it helps your research.
The following is a  BibTeX 
and plaintext reference.
The BibTeX entry requires the  url  LaTeX package. 
 ```
@misc{amos2016image,
    title        = {{Image Completion with Deep Learning in TensorFlow}},
    author       = {Amos, Brandon},
    howpublished = {\url{http://bamos.github.io/2016/08/09/deep-completion}},
    note         = {Accessed: [Insert date here]}
} 
 Brandon Amos. Image Completion with Deep Learning in TensorFlow.
http://bamos.github.io/2016/08/09/deep-completion.
Accessed: [Insert date here]
``` Block •    
 An intelligent block matrix library for numpy, PyTorch, and beyond.
Crafted by  Brandon Amos  with significant
contributions by  Eric Wong . 
 
 Why do we need an intelligent block matrix library? 
 Let's try to construct the KKT matrix from Mattingley and Boyd's
 CVXGEN 
paper in numpy and PyTorch: 
 
 Without  block , there is no way to infer the appropriate sizes of
the zero and identity matrix blocks.
It is an inconvenience to think about what size these
matrices should be. 
 
 What does  block  do? 
 Block acts a lot like  np.bmat  and replaces: 
 
 Any constant with an appropriately shaped block matrix
  filled with that constant. 
 The string  'I'  with an appropriately shaped identity matrix. 
 The string  '-I'  with an appropriately shaped negated identity matrix. 
 [Request more features.] 
 
 Isn't constructing large block matrices with a lot of zeros inefficient? 
 Yes,  block  is meant to be a quick prototyping tool and
there's probably a more efficient way to solve your system
if it has a lot of zeros or identity elements. 
 How does  block  handle numpy and PyTorch with the same interface? 
 I wrote the logic to handle matrix sizing to be agnostic
of the matrix library being used.
numpy and PyTorch are just backends.
More backends can easily be added for your favorite
Python matrix library. 
 ```Python
class Backend(metaclass=ABCMeta): 
 @abstractmethod
def extract_shape(self, x): pass

@abstractmethod
def build_eye(self, n): pass

@abstractmethod
def build_full(self, shape, fill_val): pass

@abstractmethod
def build(self, rows): pass

@abstractmethod
def is_complete(self, rows): pass
 
 ``` 
 Getting Started 
 
 Install:  pip install block 
 Usage:  from block import block 
 Run tests in  test.py :  nosetests test.py 
 
 Issues and Contributions 
 I'd be happy to hear from you about any issues or features you
add, please  file an issue 
or  send in a PR . 
 Licensing 
 This repository is
 Apache-licensed . A PyTorch Implementation of DenseNet 
 This is a  PyTorch  implementation of the
DenseNet-BC architecture as described in the
paper  Densely Connected Convolutional Networks 
by G. Huang, Z. Liu, K. Weinberger, and L. van der Maaten.
This implementation gets a CIFAR-10+ error rate of
4.77 with a 100-layer DenseNet-BC with a growth rate of 12.
Their official implementation and links to many other
third-party implementations are available in the
 liuzhuang13/DenseNet 
repo on GitHub. 
 
 Why DenseNet? 
 As this table from the DenseNet paper shows, it provides
competitive state of the art results on CIFAR-10,
CIFAR-100, and SVHN. 
 
 Why yet another DenseNet implementation? 
 PyTorch is a great new framework and it's nice to have these
kinds of re-implementations around so that they can be integrated
with other PyTorch projects. 
 How do you know this implementation is correct? 
 Interestingly while implementing this, I had a lot of
trouble getting it to converge and looked at every part
of the code closer than I usually would.
I compared all of the model's hidden states and gradients
with the official implementation to make sure my code was correct
and even trained a VGG-style network on CIFAR-10 with the
training code here.
It turns out that I uncovered a new critical PyTorch
bug (now fixed) that was causing this. 
 I have left around my original message about how this
isn't working and the things that I have checked
 in this document .
I think this should be interesting for other people to
see my development and debugging strategies when
having issues implementing a model that's known
to converge.
I also started
 this PyTorch forum thread ,
which has a few other discussion points.
You may also be interested in
 my script that
compares PyTorch gradients to Torch gradients 
and
 my script that numerically checks PyTorch gradients . 
 My convergence issues were due to a critical PyTorch bug
related to using  torch.cat  with convolutions with cuDNN
enabled (which it is by default when CUDA is used).
This bug caused incorrect gradients and the fix to
this bug is to disable cuDNN (which doesn't have
to be done anymore because it's fixed).
The oversight in my debugging strategies that caused me to
not find this error is that I did not think to disable cuDNN.
Until now, I have assumed that the cuDNN option in frameworks
are bug-free, but have learned that this is not always the case.
I may have also found something if I would have numerically
debugged  torch.cat  layers with convolutions instead of
fully connected layers. 
 Adam fixed the PyTorch bug that caused this in
 this PR 
and has been merged into Torch's master branch.
 If you are interested in using the DenseNet code in
this repository, make sure your PyTorch version
contains  this PR 
and was downloaded after 2017-02-10. 
 What does the PyTorch compute graph of the model look like? 
 You can see the compute graph  here ,
which I created with  make_graph.py ,
which I copied from
 Adam Paszke's gist .
Adam says PyTorch will soon have a better way to create
compute graphs. 
 How does this implementation perform? 
 By default, this repo trains a 100-layer DenseNet-BC with
an growth rate of 12 on the CIFAR-10 dataset with
data augmentations.
Due to GPU memory sizes, this is the largest model I am able to run.
The paper reports a final test error of 4.51 with this
architecture and we obtain a final test error of 4.77. 
 
 Why don't people use ADAM instead of SGD for training ResNet-style models? 
 I also tried training a net with ADAM and found that it didn't
converge as well with the default hyper-parameters compared
to SGD with a reasonable learning rate schedule. 
 
 What about the non-BC version? 
 I haven't tested this as thoroughly, you should make sure
it's working as expected if you plan to use and modify it.
Let me know if you find anything wrong with it. 
 A paradigm for ML code 
 I like to include a few features in my projects
that I don't see in some other re-implementations
that are present in this repo.
The training code in  train.py  uses  argparse  so the batch size
and some other hyper-params can easily be changed
and as the model is training, progress is written
out to csv files in a work directory also defined
by the arguments.
Then a separate script  plot.py  plots the
progress written out by the training script.
The training script calls  plot.py  after every epoch,
but it can importantly be run on its own so figures
can be tweaked without re-running the entire experiment. 
 Help wanted: Improving memory utilization and multi-GPU support 
 I think there are ways to improve the memory utilization
in this code as in the
 the official space-efficient Torch implementation .
I also would be interested in multi-GPU support. 
 Running the code and viewing convergence 
 First install PyTorch (ideally in an anaconda3 distribution).
 ./train.py  will create a model, start training it,
and save progress to  args.save , which is
 work/cifar10.base  by default.
The training script will call  plot.py  after
every epoch to create plots from the saved progress. 
 Citations 
 The following is a  BibTeX 
entry for the DenseNet paper that you should cite
if you use this model. 
 @article{Huang2016Densely,
  author = {Huang, Gao and Liu, Zhuang and Weinberger, Kilian Q.},
  title = {Densely Connected Convolutional Networks},
  journal = {arXiv preprint arXiv:1608.06993},
  year = {2016}
} 
 If you use this implementation, please also consider citing this implementation and
code repository with the following BibTeX or plaintext entry.
The BibTeX entry requires the  url  LaTeX package. 
 ```
@misc{amos2017densenet,
  title = {{A PyTorch Implementation of DenseNet}},
  author = {Amos, Brandon and Kolter, J. Zico},
  howpublished = {\url{https://github.com/bamos/densenet.pytorch}},
  note = {Accessed: [Insert date here]}
} 
 Brandon Amos, J. Zico Kolter
A PyTorch Implementation of DenseNet
https://github.com/bamos/densenet.pytorch.
Accessed: [Insert date here]
``` 
 Licensing 
 This repository is
 Apache-licensed . setGPU 
 A small Python library that automatically sets  CUDA_VISIBLE_DEVICES 
to the least-loaded GPU on multi-GPU systems and can be used by: 
 
 Putting  import setGPU  before any import
   that will use a GPU like Torch, TensorFlow, or JAX. 
 Defining an alias such as
    alias setGPU='eval $(python3 -m setGPU)' 
   and calling that to set the GPU in the shell before running
   another program that uses the GPU. 
 
 Installation 
 pip install git+https://github.com/bamos/setGPU.git 
 Dependencies 
 
 Jongwook Choi's   gpustat  library ( pip install gpustat ) 
 
 Licensing 
 This code is in the public domain. 
 
 diffcp 
 diffcp  is a Python package for computing the derivative of a convex cone program, with respect to its problem data. The derivative is implemented as an abstract linear map, with methods for its forward application and its adjoint.  
 The implementation is based on the calculations in our paper  Differentiating through a cone program . 
 Installation 
 diffcp  is available on PyPI, as a source distribution. Install it with 
 bash
pip install diffcp 
 You will need a C++11-capable compiler to build  diffcp . 
 diffcp  requires:
*  NumPy  >= 1.15
*  SciPy  >= 1.10
*  SCS  >= 2.0.2
*  pybind11  >= 2.4
*  threadpoolctl  >= 1.1
* Python 3.x 
 diffcp  uses Eigen; Eigen operations can be automatically vectorized by compilers. To enable vectorization, install with 
 bash
MARCH_NATIVE=1 pip install diffcp 
 OpenMP can be enabled by passing extra arguments to your compiler. For example, on linux, you can tell gcc to activate the OpenMP extension by specifying the flag ""-fopenmp"": 
 bash
OPENMP_FLAG=""-fopenmp"" pip install diffcp 
 To enable both vectorization and OpenMP (on linux), use 
 bash
MARCH_NATIVE=1 OPENMP_FLAG=""-fopenmp"" pip install diffcp 
 Cone programs 
 diffcp  differentiates through a primal-dual cone program pair. The primal problem must be expressed as  
 minimize        c'x
subject to      Ax + s = b
                s in K 
where   x  and  s  are variables,  A ,  b  and  c  are the user-supplied problem data, and  K  is a user-defined convex cone. The corresponding dual problem is 
 minimize        b'y
subject to      A'y + c == 0
                y in K^* 
 with dual variable  y . 
 Usage 
 diffcp  exposes the function 
 python
solve_and_derivative(A, b, c, cone_dict, warm_start=None, **kwargs). 
 This function returns a primal-dual solution  x ,  y , and  s , along with
functions for evaluating the derivative and its adjoint (transpose).
These functions respectively compute right and left multiplication of the derivative
of the solution map at  A ,  b , and  c  by a vector.
In the case that the problem is not solved, i.e. SCS returns something
other than ""Solved"" or ""Solved/Innacurate"" for status, we raise
a  SolverError  Exception. 
 Arguments 
 The arguments  A ,  b , and  c  correspond to the problem data of a cone program.
*  A  must be a  SciPy sparse CSC matrix .
*  b  and  c  must be NumPy arrays.
*  cone_dict  is a dictionary that defines the convex cone  K .
*  warm_start  is an optional tuple  (x, y, s)  at which to warm-start SCS.
*  **kwargs  are keyword arguments to forward to SCS (e.g.,  verbose=True ). 
 These inputs must conform to the  SCS convention  for problem data. The keys in  cone_dict  correspond to the cones, with
*  diffcp.ZERO  for the zero cone,
*  diffcp.POS  for the positive orthant,
*  diffcp.SOC  for a product of SOC cones,
*  diffcp.PSD  for a product of PSD cones, and
*  diffcp.EXP  for a product of exponential cones. 
 The values in  cone_dict  denote the sizes of each cone; the values of  diffcp.SOC ,  diffcp.PSD , and  diffcp.EXP  should be lists. The order of the rows of  A  must match the ordering of the cones given above. For more details, consult the  SCS documentation . 
 Return value 
 The function  solve_and_derivative  returns a tuple 
 python
(x, y, s, derivative, adjoint_derivative) 
 
 
 x ,  y , and  s  are a primal-dual solution. 
 
 
 derivative  is a function that applies the derivative at  (A, b, c)  to perturbations  dA ,  db ,  dc . It has the signature 
 derivative(dA, db, dc) -> dx, dy, ds , where  dA  is a SciPy sparse CSC matrix with the same sparsity pattern as  A , and  db  and  dc  are NumPy arrays.  dx ,  dy , and  ds  are NumPy arrays, approximating the change in the primal-dual solution due to the perturbation. 
 
 
 adjoint_derivative  is a function that applies the adjoint of the derivative to perturbations  dx ,  dy ,  ds . It has the signature 
 adjoint_derivative(dx, dy, ds) -> dA, db, dc , where  dx ,  dy , and  ds  are NumPy arrays. 
 
 
 Example 
 ```python
import numpy as np
from scipy import sparse 
 import diffcp 
 cone_dict = {
    diffcp.ZERO: 3,
    diffcp.POS: 3,
    diffcp.SOC: [5]
} 
 m = 3 + 3 + 5
n = 5 
 A, b, c = diffcp.utils.random_cone_prog(m, n, cone_dict)
x, y, s, D, DT = diffcp.solve_and_derivative(A, b, c, cone_dict) 
 evaluate the derivative 
 nonzeros = A.nonzero()
data = 1e-4 * np.random.randn(A.size)
dA = sparse.csc_matrix((data, nonzeros), shape=A.shape)
db = 1e-4 * np.random.randn(m)
dc = 1e-4 * np.random.randn(n)
dx, dy, ds = D(dA, db, dc) 
 evaluate the adjoint of the derivative 
 dx = c
dy = np.zeros(m)
ds = np.zeros(m)
dA, db, dc = DT(dx, dy, ds)
``` 
 For more examples, including the SDP example described in the paper, see the  examples  directory. 
 Citing 
 If you wish to cite  diffcp , please use the following BibTex: 
 ```
@article{diffcp2019,
    author       = {Agrawal, A. and Barratt, S. and Boyd, S. and Busseti, E. and Moursi, W.},
    title        = {Differentiating through a Cone Program},
    journal      = {Journal of Applied and Numerical Optimization},
    year         = {2019},
    volume       = {1},
    number       = {2},
    pages        = {107--115},
} 
 @misc{diffcp,
    author       = {Agrawal, A. and Barratt, S. and Boyd, S. and Busseti, E. and Moursi, W.},
    title        = {{diffcp}: differentiating through a cone program, version 1.0},
    howpublished = {\url{https://github.com/cvxgrp/diffcp}},
    year         = 2019
}
``` 
 The following thesis concurrently derived the mathematics behind differentiating cone programs.
 @phdthesis{amos2019differentiable,
  author       = {Brandon Amos},
  title        = {{Differentiable Optimization-Based Modeling for Machine Learning}},
  school       = {Carnegie Mellon University},
  year         = 2019,
  month        = May,
} Differentiable Optimization-Based Modeling for Machine Learning 
 
 This repository is by  Brandon Amos 
  and contains the full source code and data to produce
   my thesis document . 
 The slides are available in
   pdf 
  and
   pptx 
  format. 
 
 Unpublished work in this thesis 
 
 Chapter 2 
  provides some preliminaries and background information on differentiable convex
  optimization layers, including derivations for the optimization (or variational)
  viewpoints of the ReLU, sigmoid, and softmax. 
 Chapter 7 
  presents an early version of differentiable CVXPY layers,
  which is now available  here .
  As a bibliographic note, the cone program differentiation derivation
  in section 7.3 here remains unpublished in this thesis and was done
  concurrent to and independent of
   Differentiating Through a Cone Program . 
 
 Publications behind this thesis 
 Some of the content here is behind these publications: 
 
 
 
 Differentiable Convex Optimization Layers 
A. Agrawal*,  B. Amos* , S. Barratt*, S. Boyd*, S. Diamond*, and J. Kolter* 
NeurIPS 2019 
[1] [ pdf ]  [ code ]  
 
 
 
 
 Differentiable MPC for End-to-end Planning and Control 
 B. Amos , I. Rodriguez, J. Sacks, B. Boots, and J. Kolter 
NeurIPS 2018 
[2] [ pdf ]  [ code ]  
 
 
 
 
 Depth-Limited Solving for Imperfect-Information Games 
N. Brown, T. Sandholm, and  B. Amos 
NeurIPS 2018 
[3] [ pdf ]  
 
 
 
 
 Learning Awareness Models 
 B. Amos , L. Dinh, S. Cabi, T. Rothörl, S. Colmenarejo, A. Muldal, T. Erez, Y. Tassa, N. de Freitas, and M. Denil 
ICLR 2018 
[4] [ pdf ]  
 
 
 
 
 Task-based End-to-end Model Learning 
P. Donti,  B. Amos , and J. Kolter 
NeurIPS 2017 
[5] [ pdf ]  [ code ]  
 
 
 
 
 OptNet: Differentiable Optimization as a Layer in Neural Networks 
 B. Amos  and J. Kolter 
ICML 2017 
[6] [ pdf ]  [ code ]  
 
 
 
 
 Input Convex Neural Networks 
 B. Amos , L. Xu, and J. Kolter 
ICML 2017 
[7] [ pdf ]  [ code ]  
 
 
 
 
 Collapsed Variational Inference for Sum-Product Networks 
H. Zhao, T. Adel, G. Gordon, and  B. Amos 
ICML 2016 
[8] [ pdf ]  
 
 
 
 
 OpenFace: A general-purpose face recognition library with mobile applications 
 B. Amos , B. Ludwiczuk, and M. Satyanarayanan 
CMU 2016 
[9] [ pdf ]  [ code ]  
 
 
 
 
 The experimental source code and libraries produced for this
thesis are freely available as open source software and
are available in the following repositories. 
 
 [ cvxgrp/cvxpylayers ]
  Differentiable convex optimization layers in CVXPY. 
 [ locuslab/mpc.pytorch ]
  A stand-alone PyTorch library for the differentiable
  model predictive control approach. 
 [ locuslab/differentiable-mpc ]
  PyTorch experiments for the differentiable MPC work. 
 [ locuslab/qpth ]:
  A stand-alone PyTorch library for the OptNet QP layers. 
 [ locuslab/optnet ]
  PyTorch experiments for OptNet. 
 [ locuslab/icnn ]
  TensorFlow experiments for input-convex neural networks. 
 [ cmusatyalab/openface ]
  Face recognition with deep neural networks. 
 [ bamos/block ]
  An intelligent block matrix library for numpy, PyTorch, and beyond. 
 [ bamos/dcgan-completion.tensorflow ]
  Image Completion with Deep Learning in TensorFlow. 
 [ bamos/densenet.pytorch ]
  A PyTorch implementation of DenseNet. 
 
 
 
 This repository started from
   Cyrus Omar's thesis code ,
  which is based on a CMU thesis template
  by  David Koes 
  and others before. 
 Of standalone interest,
   refs.sort.sh 
  uses biber to alphabetize and standardize my bibliography in
   refs.bib 
  so it doesn't get too messy.
  This uses the configuration in
   refs.conf . 
 I use  update-pdf.sh 
  to keep the latest PDF only in HEAD, although Git LFS or a related
  project may be a better solution. 
 
 
 
 
 The BibTeX for this document is: 
 @phdthesis{amos2019differentiable,
  author       = {Brandon Amos},
  title        = {{Differentiable Optimization-Based Modeling for Machine Learning}},
  school       = {Carnegie Mellon University},
  year         = 2019,
  month        = May,
} 
 
 
 cvxpylayers 
 cvxpylayers is a Python library for constructing differentiable convex
optimization layers in PyTorch, JAX, and TensorFlow using CVXPY.
A convex optimization layer solves a parametrized convex optimization problem
in the forward pass to produce a solution.
It computes the derivative of the solution with respect to
the parameters in the backward pass. 
 This library accompanies our  NeurIPS 2019 paper 
on differentiable convex optimization layers.
For an informal introduction to convex optimization layers, see our
 blog post . 
 Our package uses  CVXPY  for specifying
parametrized convex optimization problems. 
 
 Installation 
 Usage 
 Examples 
 Contributing 
 Projects using cvxpylayers 
 License 
 Citing 
 
 Installation 
 Use the package manager  pip  to install
cvxpylayers. 
 bash
pip install cvxpylayers 
 Our package includes convex optimization layers for
PyTorch, JAX, and TensorFlow 2.0;
the layers are functionally equivalent. You will need to install
 PyTorch ,
 JAX , or
 TensorFlow 
separately, which can be done by following the instructions on their websites. 
 cvxpylayers has the following dependencies:
* Python 3
*  NumPy 
*  CVXPY  >= 1.1.a4
*  PyTorch  >= 1.0,  JAX  >= 0.2.12, or  TensorFlow  >= 2.0
*  diffcp  >= 1.0.13 
 Usage 
 Below are usage examples of our PyTorch, JAX, and TensorFlow layers.
Note that the parametrized convex optimization problems must be constructed
in CVXPY, using
 DPP . 
 PyTorch 
 ```python
import cvxpy as cp
import torch
from cvxpylayers.torch import CvxpyLayer 
 n, m = 2, 3
x = cp.Variable(n)
A = cp.Parameter((m, n))
b = cp.Parameter(m)
constraints = [x >= 0]
objective = cp.Minimize(0.5 * cp.pnorm(A @ x - b, p=1))
problem = cp.Problem(objective, constraints)
assert problem.is_dpp() 
 cvxpylayer = CvxpyLayer(problem, parameters=[A, b], variables=[x])
A_tch = torch.randn(m, n, requires_grad=True)
b_tch = torch.randn(m, requires_grad=True) 
 solve the problem 
 solution, = cvxpylayer(A_tch, b_tch) 
 compute the gradient of the sum of the solution with respect to A, b 
 solution.sum().backward()
``` 
 Note:  CvxpyLayer  cannot be traced with  torch.jit . 
 JAX 
 ```python
import cvxpy as cp
import jax
from cvxpylayers.jax import CvxpyLayer 
 n, m = 2, 3
x = cp.Variable(n)
A = cp.Parameter((m, n))
b = cp.Parameter(m)
constraints = [x >= 0]
objective = cp.Minimize(0.5 * cp.pnorm(A @ x - b, p=1))
problem = cp.Problem(objective, constraints)
assert problem.is_dpp() 
 cvxpylayer = CvxpyLayer(problem, parameters=[A, b], variables=[x])
key = jax.random.PRNGKey(0)
key, k1, k2 = jax.random.split(key, 3)
A_jax = jax.random.normal(k1, shape=(m, n))
b_jax = jax.random.normal(k2, shape=(m,)) 
 solution, = cvxpylayer(A_jax, b_jax) 
 compute the gradient of the summed solution with respect to A, b 
 dcvxpylayer = jax.grad(lambda A, b: sum(cvxpylayer(A, b)[0]), argnums=[0, 1])
gradA, gradb = dcvxpylayer(A_jax, b_jax)
``` 
 Note:  CvxpyLayer  cannot be traced with the JAX  jit  or  vmap  operations. 
 TensorFlow 2 
 ```python
import cvxpy as cp
import tensorflow as tf
from cvxpylayers.tensorflow import CvxpyLayer 
 n, m = 2, 3
x = cp.Variable(n)
A = cp.Parameter((m, n))
b = cp.Parameter(m)
constraints = [x >= 0]
objective = cp.Minimize(0.5 * cp.pnorm(A @ x - b, p=1))
problem = cp.Problem(objective, constraints)
assert problem.is_dpp() 
 cvxpylayer = CvxpyLayer(problem, parameters=[A, b], variables=[x])
A_tf = tf.Variable(tf.random.normal((m, n)))
b_tf = tf.Variable(tf.random.normal((m,))) 
 with tf.GradientTape() as tape:
  # solve the problem, setting the values of A, b to A_tf, b_tf
  solution, = cvxpylayer(A_tf, b_tf)
  summed_solution = tf.math.reduce_sum(solution) 
 compute the gradient of the summed solution with respect to A, b 
 gradA, gradb = tape.gradient(summed_solution, [A_tf, b_tf])
``` 
 Note:  CvxpyLayer  cannot be traced with  tf.function . 
 Log-log convex programs 
 Starting with version 0.1.3, cvxpylayers can also differentiate through log-log convex programs (LLCPs), which generalize geometric programs. Use the keyword argument  gp=True  when constructing a  CvxpyLayer  for an LLCP. Below is a simple usage example 
 ```python
import cvxpy as cp
import torch
from cvxpylayers.torch import CvxpyLayer 
 x = cp.Variable(pos=True)
y = cp.Variable(pos=True)
z = cp.Variable(pos=True) 
 a = cp.Parameter(pos=True, value=2.)
b = cp.Parameter(pos=True, value=1.)
c = cp.Parameter(value=0.5) 
 objective_fn = 1/(x y z)
objective = cp.Minimize(objective_fn)
constraints = [a (x y + x z + y z) <= b, x >= y**c]
problem = cp.Problem(objective, constraints)
assert problem.is_dgp(dpp=True) 
 layer = CvxpyLayer(problem, parameters=[a, b, c],
                   variables=[x, y, z], gp=True)
a_tch = torch.tensor(a.value, requires_grad=True)
b_tch = torch.tensor(b.value, requires_grad=True)
c_tch = torch.tensor(c.value, requires_grad=True) 
 x_star, y_star, z_star = layer(a_tch, b_tch, c_tch)
sum_of_solution = x_star + y_star + z_star
sum_of_solution.backward()
``` 
 Examples 
 Our  examples  subdirectory contains simple applications of convex optimization
layers in IPython notebooks. 
 Contributing 
 Pull requests are welcome. For major changes, please open an issue first to
discuss what you would like to change. 
 Please make sure to update tests as appropriate. 
 Please lint the code with  flake8 .
 bash
pip install flake8  # if not already installed
flake8 
 Running tests 
 cvxpylayers uses the  pytest  framework for running tests.
To install  pytest , run:
 bash
pip install pytest 
 Execute the tests from the main directory of this repository with:
 bash
pytest cvxpylayers/{torch,jax,tensorflow} 
 Projects using cvxpylayers 
 Below is  a list of projects using cvxpylayers. If you have used cvxpylayers in a project, you're welcome to make a PR to add it to this list.
*  Learning Convex Optimization Control Policies 
*  Learning Convex Optimization Models 
 License 
 cvxpylayers carries an Apache 2.0 license. 
 Citing 
 If you use cvxpylayers for research, please cite our accompanying  NeurIPS paper : 
 @inproceedings{cvxpylayers2019,
  author={Agrawal, A. and Amos, B. and Barratt, S. and Boyd, S. and Diamond, S. and Kolter, Z.},
  title={Differentiable Convex Optimization Layers},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019},
} 
 If you use cvxpylayers to differentiate through a log-log convex program, please cite the accompanying  paper : 
 @article{agrawal2020differentiating,
  title={Differentiating through log-log convex programs},
  author={Agrawal, Akshay and Boyd, Stephen},
  journal={arXiv},
  archivePrefix={arXiv},
  eprint={2004.12553},
  primaryClass={math.OC},
  year={2020},
} QNSTOP: quasi-Newton stochastic optimization algorithm. 
 The package  QNSTOP  is a suite of serial and parallel Fortran 95/2003 codes
for deterministic global optimization and stochastic optimization, with
the serial driver subroutine  QNSTOPS  and the parallel driver subroutine
QNSTOPP. The organization of this repository is: 
 
 src  contains the ACT TOMS code. See the  README  here for further details. 
 docs  contains website docs. 
 This repository contains the slides behind  my  major
presentations with a  CC-BY  license. 
 
 [2022] Learning with differentiable and amortized optimization 
 Powerpoint  |
 PDF 
 Optimization has been a transformative modeling and decision-making paradigm over the past century that computationally encodes non-trivial reasoning operations.  Developments in optimization foundations alongside domain experts have resulted in breakthroughs for 1) controlling robotic, autonomous, mechanical, and multi-agent systems, 2) making operational decisions based on future predictions, 3) efficiently transporting or matching resources, information, and measures, 4) allocating budgets and portfolios, 5) designing materials, molecules, and other structures, 6) solving inverse problems to infer underlying hidden costs, incentives, geometries, terrains, and other structures, and 7) learning and meta-learning the parameters of predictive and statistical models. These settings often analytically specify the relevant models of the world along with an explicit objective to optimize for. Once these are specified, computational optimization solvers are able to search over the space of possible solutions or configurations and return the best one. 
 The magic of optimization stops when 1) the relevant models of the world are too difficult or impossible to specify, leading to inaccurate or incomplete representations of the true setting, and 2) solving the optimization problem is computationally challenging and takes too long to return a solution on today's hardware. Machine learning methods help overcome both of these by providing fast predictive models and powerful latent abstractions of the world. In this talk, I will cover two ways of tightly integrating optimization and machine learning methods:] 
 
 
 Differentiable optimization  characterizes how the solution to an optimization problem changes as the inputs change. In machine learning settings, differentiable optimization provides an implicit layer that integrates optimization-based domain knowledge into the model and enables unknown parts of the optimization problem to be learned. I will cover the foundations of learning these layers with implicit differentiation and highlight applications in robotics and control settings. 
 
 
 Amortized optimization  rapidly predicts approximate solutions to optimization problems and is useful when repeatedly solving optimization problems. Traditional optimization methods typically solve every new problem instance from scratch, ignoring shared structures and information when solving a new instance. In contrast, a solver augmented with amortized optimization learns the shared structure present in the solution mappings and better-searches the domain. I will cover the foundations of amortized optimization and highlight new applications in control and optimal transport. 
 
 
 [2022] Amortized optimization for computing optimal transport maps 
 Powerpoint  |
 PDF 
 [2022] Differentiable optimization 
 Powerpoint  |  PDF 
 [2022] Differentiable control 
 Powerpoint  |  PDF 
 [2022] Amortized optimization 
 Powerpoint  |  PDF 
 [2021] On the model-based stochastic value gradient for continuous RL 
 Powerpoint  |  PDF 
 [2021] Riemannian Convex Potential Maps 
 Keynote  |  PDF 
 [2020] Differentiable cross-entropy method 
 Powerpoint  |  PDF 
 [2019] Ph.D. Thesis: Differentiable optimization-based modeling for machine learning 
 Powerpoint  |  PDF 
 [2018] PyTorch libraries for linear algebra, optimization, and control 
 Powerpoint  |  PDF 
 [2018] OptNet, end-to-end task-based learning, and control 
 Powerpoint  |  PDF 
 [2018] Differentiable MPC 
 Powerpoint  |  PDF ] |  Poster Powerpoint  |  Poster PDF 
 [2017] OptNet 
 Powerpoint  |  PDF 
 [2017] ICNN 
 Powerpoint  |  PDF"
