bmccann,['party_predictor \n A repo for a CS 229 final project that classifies American political speeches by party affiliation \n test', 'pytorch-xla-transformer-language-model \n This repository is an open source test case for  pytorch/xla  that runs a minimal training loop for a  Transformer  language model on a single TPU device. \n This code is intended to be used as reference for testing the compilation of the model by XLA, and is not intended to be used for training a reasonable language model. During initial runs, this code triggered recompilation far too often, but these issues have now been resolved.  \n Depends on Docker image  gcr.io/tpu-pytorch/xla:r0.1 . \n bash\nexport TPU_IP=#YOU MUST SET YOUR TPU IP\nexport XRT_TPU_CONFIG="tpu_worker;0;$TPU_IP"\nexport XLA_USE_32BIT_LONG=1\nexport XLA_IR_DEBUG=1\nexport XLA_HLO_DEBUG=1\npython3 train.py \nOutput is in  run.log .']
fmassa,['torch-nn \n Additional functionnalities for torch7 neural networks \n The following modules were merged to different repositories \n \n SpatialAdaptiveMaxPooling ->  torch/nn \n LocalResponseNormalization ->  inn  as LocalSameResponseNormalization \n SpatialPyramidPooling ->  inn \n Normalize with infinity norm ->  torch/nn \n', 'imagine-nn \n Universite Paris-Est Marne-la-Vallee IMAGINE/LIGM torch neural network routines \n Following modules are here for now: \n lua\ninn.SpatialMaxPooling(kW,kH,dW,dH)\ninn.SpatialAveragePooling(kW,kH,dW,dH)\ninn.SpatialCrossResponseNormalization(size, [alpha = 0.0001], [beta = 0.75], [k = 1])\ninn.LocalResponseNormalization([size = 3], [alpha = 0.00005], [beta = 0.75])\ninn.MeanSubtraction(mean)\ninn.SpatialPyramidPooling({{w1,h1},{w2,h2},...,{wn,hn}}) \n The difference with  inn.SpatialMax(Average)Pooling  and  nn.SpatialMax(Average)Pooling  is that output size computed with ceil instead of floor (as in Caffe and cuda-convnet2). Also SpatialAveragePooling does true average pooling, meaning that it divides outputs by kW*kH.\ninn.SpatialMax(Average)Pooling(kW,kH,dW,dH) is equal to cudnn.SpatialMax(Average)Pooling(kW,kH,dW,dH):ceil(). \n inn.SpatialCrossResponseNormalization  is local response normalization across maps in BDHW format (thanks to Caffe!). For details refer to https://code.google.com/p/cuda-convnet/wiki/LayerParams#Local_response_normalization_layer_(across_maps) \n inn.LocalResponseNormalization  is a local response normalization in the same map in BDHW format. For details refer to https://code.google.com/p/cuda-convnet/wiki/LayerParams#Local_response_normalization_layer_(same_map) \n inn.MeanSubtraction(mean)  is done to subtract the Imagenet mean directly on GPU. Mean tensor is expanded to BDHW batches without using additional memory. \n inn.SpatialPyramidPooling({{w1,h1},{w2,h2},...,{wn,hn}})  is a pyramid of regions obtained by using Spatial Adaptive Max Pooling with parameters  (w1,h1),...,(wn,hn)  in the input. The result is a fixed-sized vector of size  w1*h1*...wn*hn  for any input dimension. For details see http://arxiv.org/abs/1406.4729', 'Object detection in torch \n Implementation of some object detection frameworks in  torch . \n Note on new code \n You should probably check the  refactoring branch of this repository , which simplifies the code structure, and also contains an implementation of Fast-RCNN and threaded RCNN (making it much faster than standard RCNN). It will be merged to master soon. \n Dependencies \n It requires the following packages \n \n xml \n matio-ffi.torch \n hdf5 \n inn \n \n To install them all, do \n ``` \n xml \n luarocks install xml \n matio \n OSX \n brew install libmatio \n Ubuntu \n sudo apt-get install libmatio2 \n luarocks install matio\n``` \n To install  hdf5 , follow the instructions in  here \n Running this code \n First, clone this repo\n git clone https://github.com/fmassa/object-detection.torch.git \n The zeiler pretrained model is available at  https://drive.google.com/open?id=0B-TTdm1WNtybdzdMUHhLc05PSE0&authuser=0 .\nIt is supposed to be at  data/models .\nIf you want to use your own model in SPP framework, make sure that it follows the pattern\n model = nn.Sequential()\nmodel:add(features)\nmodel:add(pooling_layer)\nmodel:add(classifier) \nwhere  features  can be a  nn.Sequential  of several convolutions and  pooling_layer  is the last pooling with reshaping of the data to feed it to the classifer. See  models/zeiler.lua  for an example. \n To finetune the network for detection, simply run\n th main.lua \n To get an overview of the different parameters, do\n th main.lua -h \n The default is to consider that the dataset is present in  datasets/VOCdevkit/VOC2007/ .\nThe default location of bounding boxes  .mat  files (in RCNN format) is supposed to be in  data/selective_search_data/ .', "OptNet - reducing memory usage in torch neural networks \n Memory optimizations for torch neural networks. \n Heavily inspired from the  Optimizer  from https://github.com/facebook/fb-caffe-exts \n Installing \n Simply do\n luarocks install optnet \n How does it work ? \n It goes over the network and verify which buffers can be reused.\nIt supports both inference (evaluation) mode and training mode. \n Inference mode \n Here is a list of currently tested modules. Numbers are for CPU version, with batch size of 1, for  double  type, in the format\n (total memory used, memory used for the outputs, memory used for the internal buffers, memory used for the parameters and grad parameters) : \n | Network | before optimization | after optimization | Relative save |\n| ------- | :--------: | :-------: | :------: |\n|alexnet | (973MB, 6MB, 43MB, 924MB) | (472MB, 1.5MB, 9MB, 462MB) | (51%, 75%, 80%, 50%) |\n|vgg-A | (2311MB, 69MB, 215MB, 2027MB) | (1106MB, 31MB, 61MB, 1014MB) | (52%, 55%, 72%, 50%) |\n|googlenet | (505MB, 69MB, 145MB, 292MB) | (193MB, 31MB, 16MB, 146MB) | (62%, 54%, 89%, 50%) |\n|resnet 110 (cifar)| (113MB, 16MB, 71MB, 26MB) | (15MB, 0.5MB, 1.3MB, 13MB) | (87%, 97%, 98%, 50%) | \n Note that for most of the models, for a batch size of 1 most of the memory is spent with the  weights  and  gradWeights  of the network (and the latter can be safely freed during inference).\nMore interestingly, the the output size is  linearly  dependent on the batch size, which means that the total savings are much more significant for bigger batch sizes. \n In a more realistic setup where we use  cudnn  and batch size of 128, the gains are\nway more significant, specially for very deep networks like resnet. The memory usage is shown in the following table (for  float  type), following  (total memory used, memory used for the outputs, memory used for the parameters and grad parameters)  as  cudnn  almost don't use internal buffers: \n | Network | before optimization | after optimization | Relative save |\n| ------- | :--------: | :-------: | :------: |\n|alexnet | (859MB, 397MB, 462MB) | (328MB, 97MB, 231MB) | (62%, 75%, 50%) |\n|vgg-A | (5340MB, 4386MB, 1014MB) | (2467MB, 1960MB, 507MB) | (54%, 55%, 50%) |\n|googlenet | (4536MB, 4390MB, 146MB) | (2066MB, 1993MB, 73MB) | (54%, 55%, 50%) |\n|resnet 110 (cifar)| (1049MB, 1036MB, 13MB) | (39MB, 32MB, 7MB) | (96%, 97%, 50%) | \n Training mode \n We currently support a basic algorithm for training mode.\nUsing  cudnn  with batch size of 64, we currently obtain the following savings, in the format  (total memory used, memory used for the outputs, memory used for the gradInputs, memory used for the parameters and gradParameters) : \n | Network | before optimization | after optimization | Relative save |\n| ------- | :--------: | :-------: | :------: |\n|alexnet | (963MB, 195MB, 303MB, 462MB) | (816MB, 195MB, 156MB, 462MB) | (15%, 0%, 48%, 0%) |\n|vgg-A | (5433MB, 2191MB, 2228MB, 1014MB) | (4228MB, 2191MB, 1023MB, 1014MB) | (22%, 0%, 54%, 0%) |\n|googlenet | (6092MB, 2195MB, 3346MB, 146MB) | (4844MB, 2195MB, 2098MB, 146MB) | (20%, 0%, 37%, 0%) |\n|resnet 110 (cifar)| (664MB, 259MB, 392MB, 13MB) | (428MB, 259MB, 156MB, 13MB) | (36%, 0%, 60%, 0%) | \n Note that the relative save of the  gradInput  stays constant for different batch sizes, meaning that the total relative savings will be more important for bigger batch sizes (as the parameters doesn't depend on the batch size). \n We can setup the optimizations for training mode by using  mode='training'  as follows \n ```lua\nmodels = require 'optnet.models'\nmodelname = 'googlenet'\nnet, input = models modelname \n opts = {inplace=true, mode='training'} \n optnet = require 'optnet' \n optnet.optimizeMemory(net, input, opts)\n``` \n Optional parameters \n Here is a list of options that are currently supported, and should be passed in the  opts  table as a third argument:\n*  inplace : uses in place modules when available (boolean)\n*  mode : selects between  training  and  inference  optimization algorithm (string)\n*  reuseBuffers : shares internal buffers between same modules (like unfolded images for convolution) (boolean)\n*  removeGradParams : remove  gradWeight  and  gradBias  in the networks, saving their sharings so that they can be exactly reconstructed. Only applies for  inference  mode (boolean) \n Visualizing the memory reuse \n We can analyse the sharing of the internal buffers by looking at the computation\ngraph of the network before and after the sharing. \n For that, we have the  graphgen(net, input, opts)  function, which creates the\ngraph corresponding to the network  net . The generated graph contains the storage\nid of each  output , and same colors means same storage. \n Note that  net  is a  nn  model, and  not  a  nngraph  network. This allows us\nto use  optnet.graphgen  to generate graph visualizations of  nn  networks without\nhaving to use  nngraph . \n Let's have a look: \n ```lua\n-- some handy models are defined in optnet.models\n-- like alexnet, googlenet, vgg and resnet\nmodels = require 'optnet.models'\nmodelname = 'googlenet'\nnet, input = models modelname \n generateGraph = require 'optnet.graphgen' \n -- visual properties of the generated graph\n-- follows graphviz attributes\ngraphOpts = {\ndisplayProps =  {shape='ellipse',fontsize=14, style='solid'},\nnodeData = function(oldData, tensor)\n  return oldData .. '\\n' .. 'Size: '.. tensor:numel()\nend\n} \n g = generateGraph(net, input, graphOpts) \n graph.dot(g,modelname,modelname)\n``` \n This generates the following graph: \n \n Now what happens after we optimize the network ? Check the colors and the storage\nids. \n ```lua\nmodels = require 'optnet.models'\nmodelname = 'googlenet'\nnet, input = models modelname \n opts = {inplace=true, reuseBuffers=true} \n generateGraph = require 'optnet.graphgen' \n optnet = require 'optnet' \n optnet.optimizeMemory(net, input, opts) \n graphOpts = {\ndisplayProps =  {shape='ellipse',fontsize=14, style='solid'},\nnodeData = function(oldData, tensor)\n  return oldData .. '\\n' .. 'Size: '.. tensor:numel()\nend\n} \n g = generateGraph(net, input, graphOpts) \n graph.dot(g,modelname..'_optimized',modelname..'_optimized')\n```\n \n Counting the amount of saved memory \n We can also provide a function to compute the amount of memory used by the network\nin bytes, which allows us to check the amount of saved memory.\nIt decomposes the total amount of memory in four fields:\n* total memory used by the outputs of each module\n* total memory used by the gradInputs of each module\n* total memory used by the internal buffers of each module\n* total memory used by the weights and gradWeights of each module. \n Here is an example \n ```lua\noptnet = require 'optnet' \n models = require 'optnet.models'\nmodelname = 'googlenet'\nnet, input = models modelname \n -- countUsedMemory needs the network to\n-- be initialized with all its buffers\n-- to output correct results\nnet:forward(input)\nmem1 = optnet.countUsedMemory(net) \n optnet.optimizeMemory(net, input) \n net:forward(input)\nmem2 = optnet.countUsedMemory(net) \n optnet.removeOptimization(net) \n net:forward(input)\nmem3 = optnet.countUsedMemory(net) \n print('Before optimization        : '.. mem1.total_size/1024/1024 .. ' MBytes')\nprint('After optimization         : '.. mem2.total_size/1024/1024 .. ' MBytes')\nprint('After removing optimization: '.. mem3.total_size/1024/1024 .. ' MBytes') \n ```", 'torch-vision \n This repository consists of: \n \n vision.datasets  : Data loaders for popular vision datasets \n vision.transforms  : Common image transformations such as random crop, rotations etc. \n [WIP] vision.models  : Model definitions and Pre-trained models for popular models such as AlexNet, VGG, ResNet etc. \n \n Installation \n Binaries: \n bash\nconda install torchvision -c https://conda.anaconda.org/t/6N-MsQ4WZ7jo/soumith \n From Source: \n bash\npip install -r requirements.txt\npip install . \n Datasets \n The following dataset loaders are available: \n \n COCO (Captioning and Detection) \n LSUN Classification \n ImageFolder \n Imagenet-12 \n \n Datasets have the API:\n-  __getitem__ \n-  __len__ \nThey all subclass from  torch.utils.data.Dataset \nHence, they can all be multi-threaded (python multiprocessing) using standard torch.utils.data.DataLoader. \n For example: \n torch.utils.data.DataLoader(coco_cap, batch_size=args.batchSize, shuffle=True, num_workers=args.nThreads) \n In the constructor, each dataset has a slightly different API as needed, but they all take the keyword args: \n \n transform  - a function that takes in an image and returns a transformed version \n common stuff like  ToTensor ,  RandomCrop , etc. These can be composed together with  transforms.Compose  (see transforms section below) \n target_transform  - a function that takes in the target and transforms it. For example, take in the caption string and return a tensor of word indices. \n \n COCO \n This requires the  COCO API to be installed \n Captions: \n dset.CocoCaptions(root="dir where images are", annFile="json annotation file", [transform, target_transform]) \n Example: \n ```python\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\ncap = dset.CocoCaptions(root = \'dir where images are\', \n                        annFile = \'json annotation file\', \n                        transform=transforms.ToTensor()) \n print(\'Number of samples: \', len(cap))\nimg, target = cap[3] # load 4th sample \n print("Image Size: ", img.size())\nprint(target)\n``` \n Output: \n Number of samples: 82783\nImage Size: (3L, 427L, 640L)\n[u\'A plane emitting smoke stream flying over a mountain.\', \nu\'A plane darts across a bright blue sky behind a mountain covered in snow\', \nu\'A plane leaves a contrail above the snowy mountain top.\', \nu\'A mountain that has a plane flying overheard in the distance.\', \nu\'A mountain view with a plume of smoke in the background\'] \n Detection: \n dset.CocoDetection(root="dir where images are", annFile="json annotation file", [transform, target_transform]) \n LSUN \n dset.LSUN(db_path, classes=\'train\', [transform, target_transform]) \n \n db_path = root directory for the database files \n classes = \n \'train\' - all categories, training set \n \'val\' - all categories, validation set \n \'test\' - all categories, test set \n [\'bedroom_train\', \'church_train\', ...] : a list of categories to load \n \n ImageFolder \n A generic data loader where the images are arranged in this way: \n ```\nroot/dog/xxx.png\nroot/dog/xxy.png\nroot/dog/xxz.png \n root/cat/123.png\nroot/cat/nsdf3.png\nroot/cat/asd932_.png\n``` \n dset.ImageFolder(root="root folder path", [transform, target_transform]) \n It has the members: \n \n self.classes  - The class names as a list \n self.class_to_idx  - Corresponding class indices \n self.imgs  - The list of (image path, class-index) tuples \n \n Imagenet-12 \n This is simply implemented with an ImageFolder dataset. \n The data is preprocessed  as described here \n Here is an example . \n Transforms \n Transforms are common image transforms.\nThey can be chained together using  transforms.Compose \n \n ToTensor()  - converts PIL Image to Tensor \n Normalize(mean, std)  - normalizes the image given mean, std (for example: mean = [0.3, 1.2, 2.1]) \n Scale(size, interpolation=Image.BILINEAR)  - Scales the smaller image edge to the given size. Interpolation modes are options from PIL \n CenterCrop(size)  - center-crops the image to the given size \n RandomCrop(size)  - Random crops the image to the given size. \n RandomHorizontalFlip()  - hflip the image with probability 0.5 \n RandomSizedCrop(size, interpolation=Image.BILINEAR)  - Random crop with size 0.08-1 and aspect ratio 3/4 - 4/3 (Inception-style) \n \n transforms.Compose \n One can compose several transforms together.\nFor example. \n python\ntransform = transforms.Compose([\n    transforms.RandomSizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ],\n                          std = [ 0.229, 0.224, 0.225 ]),\n])', 'Python Library stats \n pylibstats  is a small library that allows you to query some useful statistics for Python code-bases.\nWe currently report library imports, function calls and attributes. \n Installation \n You can install  pylibstats  with the following command \n ```\npython -m pip install \'git+https://github.com/fmassa/python-lib-stats.git\' \n (add --user if you don\'t have permission) \n Or, to install it from a local clone: \n git clone https://github.com/fmassa/python-lib-stats.git\npython -m pip install -e python-lib-stats\n``` \n Usage \n In order to find all uses (imports / function calls / attribute queries) from library  <mylib>  over codebase  <path/to/python/repo> , run the following command: \n pylibstats --local_dir <path/to/python/repo> --library_name <mylib> --absolute_count \n You can also use  pylibstats  as a library.\n```python\nimport pylibstats \n summary = pylibstats.process_local_repository("path/to/repo", library_name="mylib")\n ``\nwhere summary is a Dict[Dict[str, int]] with the following entries: import_count , call_count and access_count , each field being a dictionary with the mapping of name: count`. \n Example \n Looking for all  torchvision  occurrences on the  DETR  codebase yields: \n ``` \n                               Imports\n \n ===========================================================================\n                                                                    | Count \n \n torchvision                                                     : 4\ntorchvision.ops._new_empty_tensor                               : 1\ntorchvision.ops.misc._output_size                               : 1\ntorchvision.ops.boxes.box_area                                  : 1\ntorchvision.transforms                                          : 1\ntorchvision.transforms.functional                               : 1\ntorchvision.models._utils.IntermediateLayerGetter               : 1\n \n ===========================================================================\n                                   Calls\n===========================================================================\n                                                                    | Count \n \n torchvision.__version__.split                                   : 2\ntorchvision.ops.boxes.box_area                                  : 2\ntorchvision.transforms.RandomCrop.get_params                    : 2\ntorchvision._is_tracing                                         : 1\ntorchvision.ops.misc._output_size                               : 1\ntorchvision.ops._new_empty_tensor                               : 1\ntorchvision.ops.misc.interpolate                                : 1\ntorchvision.transforms.functional.crop                          : 1\ntorchvision.transforms.functional.hflip                         : 1\ntorchvision.transforms.functional.resize                        : 1\ntorchvision.transforms.functional.pad                           : 1\ntorchvision.transforms.functional.to_tensor                     : 1\ntorchvision.transforms.RandomErasing                            : 1\ntorchvision.transforms.functional.normalize                     : 1\ntorchvision.models._utils.IntermediateLayerGetter               : 1\ntorchvision.models.{?}                                          : 1\n \n ===========================================================================\n                                   Attrs\n===========================================================================\n                                                                    | Count \n \n torchvision.__version__.split                                   : 2\ntorchvision.ops.boxes.box_area                                  : 2\ntorchvision.datasets.CocoDetection                              : 2\ntorchvision.transforms.RandomCrop.get_params                    : 2\ntorchvision._is_tracing                                         : 1\ntorchvision.ops.misc._output_size                               : 1\ntorchvision.ops._new_empty_tensor                               : 1\ntorchvision.ops.misc.interpolate                                : 1\ntorchvision.transforms.functional.crop                          : 1\ntorchvision.transforms.functional.hflip                         : 1\ntorchvision.transforms.functional.resize                        : 1\ntorchvision.transforms.functional.pad                           : 1\ntorchvision.transforms.functional.to_tensor                     : 1\ntorchvision.transforms.RandomErasing                            : 1\ntorchvision.transforms.functional.normalize                     : 1\ntorchvision.models._utils.IntermediateLayerGetter               : 1\ntorchvision.models                                              : 1\n \n ```']
edouardelasalles,['Spatio-Temporal Neural Networks for Space-Time Series Forecasting and Relation Discovery \n ICDM 2017 - IEEE International Conference on Data Mining series (ICDM) \n Conference Paper \n Journal Extension \n Commands for reproducing synthetic experiments: \n Heat Diffusion \n STNN \n python train_stnn.py --dataset heat --outputdir output_heat --manualSeed 2021 --xp stnn \n STNN-R(efine) \n python train_stnn.py --dataset heat --outputdir output_heat --manualSeed 5718 --xp stnn_r --mode refine --patience 800 --l1_rel 1e-8 \n STNN-D(iscovery) \n python train_stnn.py --dataset heat --outputdir output_heat --manualSeed 9690 --xp stnn_d --mode discover --patience 1000 --l1_rel 3e-6 \n Modulated Heat Diffusion \n STNN \n python train_stnn.py --dataset heat_m --outputdir output_heat_m --manualSeed 679 --xp stnn \n STNN-R(efine) \n python train_stnn.py --dataset heat_m --outputdir output_heat_m --manualSeed 3488 --xp stnn_r --mode refine --l1_rel 1e-5 \n STNN-D(iscovery) \n python train_stnn_.py --dataset heat_m --outputdir output_m --xp test --manualSeed 7664 --mode discover --patience 500 --l1_rel 3e-6 \n Data format \n The file  heat.csv  contains the raw temperature data. The 200 rows correspond to the 200 timestep, and the 41 columns are the 41 space points.\nThe file  heat_relations.csv  contains the spatial relation between the 41 space points. It is a 41 by 41 adjacency matrix  A , where  A(i, j)  = 1 means that series  i  is a direct neighbor of series  j  in space, and is 0 otherwise.', 'Dynamic Neural Language Models \n Code and supplementary material for the paper "Dynamic Neural Language Models"', 'Dynamic Author Representation \n Official implementation of the paper  Learning Dynamic Author Representations with Temporal Language Models \n Edouard Delasalles, Sylvain Lamprier, Ludovic Denoyer \n IEEE International Conference on Data Mining, ICDM 2019 \n Model \n High level view | Detailed View\n :---: | :---: \n  |  \n x  are textual documents, and  h  are dynamic vector representation of authors. The representation vectors  h  evolve through time with a residual transition function  f . The temporal author representation vectors are fed to an LSTM language model to predict next token probability. \n Requirements \n \n Python >= 3.6 \n PyTorch 1.1.0 \n pytorch-pretrained-bert  0.6.1 \n \n Data \n Data used in the paper are provided as  .tar.gz  archives in the  data  directory.\n-  s2  corpus was processed from the  2018-05-03  version of the  Semantic Scholar corpus  [1]\n-  nyt  corpus was processed from data provided by Yao et al. for their paper  Dynamic Word Embeddings for Evolving Semantic Discovery  [2] \n Reproducing results from the paper \n To reproduce the experiments in the paper, you can run the commands below. You will need to add the option  --xp_dir model/will/be/saved/here  when launching an experiment to specify a location to save the model to. You can also run an experiment on a GPU by specifying  --device n  where  n  is a GPU device id. \n Semantic Scholar \n Modeling: \n python train.py --corpus s2 --task modeling --wd 0.0003 \n Imputation: \n python train.py --corpus s2 --task imputation \n Prediction: \n python train.py --corpus s2 --task prediction --l2_a 1 \n New York Time \n Modeling: \n python train.py --corpus nyt --task modeling --dropoutw 0.5 --dropouto 0.5 --lr_scheduling_burnin 25000 --lr_scheduling_niter 5000 --nlayers_dyn 2 \n Imputation: \n python train.py --corpus nyt --task imputation --dropoutw 0.5 --dropouto 0.5 --lr_scheduling_burnin 25000 --lr_scheduling_niter 5000 --nlayers_dyn 2 \n Prediction: \n python train.py --corpus nyt --task prediction --dropoutw 0.5 --dropouto 0.5 --lr_scheduling_burnin 25000 --lr_scheduling_niter 5000 --nlayers_dyn 2 --l2_a 1 \n References \n [1] Ammar, Waleed, et al. "Construction of the Literature Graph in Semantic Scholar." Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers). 2018. \n [2] Yao, Zijun, et al. "Dynamic word embeddings for evolving semantic discovery." Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining. ACM, 2018.', "Stochastic Latent Residual Video Prediction (SRVP) \n Official implementation of the paper  Stochastic Latent Residual Video Prediction  (Jean-Yves Franceschi,  Edouard Delasalles,  Mickael Chen, Sylvain Lamprier, Patrick Gallinari), accepted and presented at ICML 2020. \n Article \n Presentation \n Preprint \n Project Website \n Pretrained Models \n Requirements \n All models were trained with Python 3.7.6 and PyTorch 1.4.0 using CUDA 10.1. \n A list required Python packages is available in the  requirements.txt  file. \n To speed up training, we recommend to activate mixed-precision training in the options, whose performance gains were tested on the most recent Nvidia GPU architectures (starting from Volta).\nWe used Nvidia's  Apex  (v0.1) in mixed-precision mode ( O1 ) to produce results reported in the paper.\nWe also integrated PyTorch's more recent  mixed-precision training package  (made available in PyTorch 1.6.0), which should give similar results.\nThis is, however, an experimental feature and we cannot guarantee that it achieves the same results as Apex. \n Datasets \n Stochastic Moving MNIST \n During training, this dataset is generated on the fly.\nIn order to generate a consistent testing set in an  .npz  file, the following commands should be executed:\n bash\npython -m preprocessing.mmnist.make_test_set --data_dir $DIR --seq_len 25 \nfor the stochastic version of the dataset, or\n bash\npython -m preprocessing.mmnist.make_test_set --data_dir $DIR --deterministic --seq_len 100 \nfor the deterministic version, where  $DIR  is the directory where the testing set should be saved. \n KTH \n To download the dataset at a given path  $DIR , execute the following command:\n bash\nbash preprocessing/kth/download.sh $DIR \n(see also  https://github.com/edenton/svg/blob/master/data/download_kth.sh  from the official implementation of  SVG ). \n In order to respectively train and test a model on this dataset, the following commands should be run:\n bash\npython preprocessing/kth/convert.py --data_dir $DIR \nand\n bash\npython preprocessing/kth/make_test_set.py --data_dir $DIR \n Human3.6M \n This dataset can be downloaded at  http://vision.imar.ro/human3.6m/description.php , after obtaining access from its owners.\nVideos for every subject are included in  .tgz  archives. Each of these archives should be extracted in the same folder. \n To preprocess the dataset in order to use it for training and testing, these videos should be processed using the following command:\n bash\npython preprocessing/human/convert.py --data_dir $DIR \nwhere  $DIR  is the directory where Human3.6M videos are saved. \n Finally, the testing set is created by choosing extracts from testing videos, with the following command:\n bash\npython preprocessing/human/make_test_set.py --data_dir $DIR \n All processed videos are saved in the same folder as the original dataset. \n BAIR \n To download the dataset at a given path  $DIR , execute the following command:\n bash\nbash preprocessing/bair/download.sh $DIR \n(see also  https://github.com/edenton/svg/blob/master/data/download_bair.sh  from the official implementation of  SVG ). \n In order to respectively train and test a model on this dataset, the following command should be run:\n bash\npython preprocessing/bair/convert.py --data_dir $DIR \n Training \n In order to launch training on multiple GPUs, launch the following command:\n bash\nOMP_NUM_THREADS=$NUMWORKERS python -m torch.distributed.launch --nproc_per_node=$NBDEVICES train.py --device $DEVICE1 $DEVICE2 --seed $SEED ... \nfollowed by the training options, where  $NBDEVICES  is the number of GPUs to be used,  $NUMWORKERS  is the number of processes per GPU to use for data loading (should be equal to the value given to the option  n_workers ),  $DEVICE1 $DEVICE2 ...  is a list of GPU indices whose length in equal to  $NBDEVICES , and  $SEED  is the chosen random seed.\nTraining can be accelerated using options  --apex_amp  or  --torch_amp  (see  requirements ). \n Data directory ( $DATA_DIR ) and saving path ( $SAVE_DIR ) must be given using options  --data_dir $DATA_DIR --save_path $SAVE_DIR . \n Training parameters are given by the following options:\n- for Stochastic Moving MNIST:\n --ny 20 --nz 20 --beta_z 2 --nt_cond 5 --nt_inf 5 --dataset smmnist --nc 1 --seq_len 15 \n- for Deterministic Moving MNIST:\n --ny 20 --nz 20 --beta_z 2 --nt_cond 5 --nt_inf 5 --dataset smmnist --deterministic --nc 1 --seq_len 15 --lr_scheduling_burnin 800000 --lr_scheduling_n_iter 100000 \n- for KTH:\n --ny 50 --nz 50 --n_euler_steps 2 --res_gain 1.2 --archi vgg --skipco --nt_cond 10 --nt_inf 3 --obs_scale 0.2 --batch_size 100 --dataset kth --nc 1 --seq_len 20 --lr_scheduling_burnin 150000 --lr_scheduling_n_iter 50000 --val_interval 5000 --seq_len_test 30 \n- for Human3.6M:\n --ny 50 --nz 50 --n_euler_steps 2 --res_gain 1.2 --archi vgg --skipco --nt_cond 8 --nt_inf 3 --obs_scale 0.2 --batch_size 100 --dataset human --nc 3 --seq_len 16 --lr_scheduling_burnin 325000 --lr_scheduling_n_iter 25000 --val_interval 20000 --batch_size_test 8 --seq_len_test 53 \n- for BAIR:\n --ny 50 --nz 50 --n_euler_steps 2 --archi vgg --skipco --nt_cond 2 --nt_inf 2 --obs_scale 0.71 --batch_size 192 --dataset bair --nc 3 --seq_len 12 --lr_scheduling_burnin 1000000 --lr_scheduling_n_iter 500000 \n Please also refer to the help message of  train.py :\n bash\npython train.py --help \nwhich lists all options and hyperparameters to train SRVP models. \n Testing \n To evaluate a trained model, the script  test.py  should be used as follows:\n bash\npython test.py --data_dir $DATADIR --xp_dir $XPDIR --lpips_dir $LPIPSDIR \nwhere  $XPDIR  is a directory containing a checkpoint and the corresponding  json  configuration file (see the pretrained models for an example),  $DATADIR  is the directory containing the test set, and  $LPIPSDIR  is a directory where  v0.1 LPIPS weights  (from the official repository of  The Unreasonable Effectiveness of Deep Features as a Perceptual Metric ) are downloaded. \n To run the evaluation on GPU, use the option  --device $DEVICE . \n Model file name can be specified using the option  --model_name $MODEL_NAME  (for instance, to load best models selected on the evaluation sets of KTH and Human3.6M:  --model_name model_best.pt ). \n PSNR, SSIM and LPIPS results reported in the paper were obtained with the following options:\n- for stochastic Moving MNIST:\n bash\npython test.py --data_dir $DATADIR --xp_dir $XPDIR --lpips_dir $LPIPSDIR --nt_gen 25 \n- for deterministic Moving MNIST:\n bash\npython test.py --data_dir $DATADIR --xp_dir $XPDIR --lpips_dir $LPIPSDIR --n_samples 1 --nt_gen 100 \n- for KTH:\n bash\npython test.py --data_dir $DATADIR --xp_dir $XPDIR --lpips_dir $LPIPSDIR --nt_gen 40 \n- for Human3.6M:\n bash\npython test.py --data_dir $DATADIR --xp_dir $XPDIR --lpips_dir $LPIPSDIR --nt_gen 53 \n- for BAIR:\n bash\npython test.py --data_dir $DATADIR --xp_dir $XPDIR --lpips_dir $LPIPSDIR --nt_gen 30 \nAdding option  --fvd  additionally computes FVD. \n Please also refer to the help message of  test.py :\n bash\npython test.py --help \n Troubleshooting \n It has been reported  that using Apex mixed-precision training in specific configurations may lead to an excessive RAM usage due to  this memory leak issue in Apex .\nWe refer to the links hereinabove for solutions to this problem. \n Please feel free to create an issue for any other problem that you might encounter using our code."]
edouardoyallon,["ScatNetLight v0.1 - June 2015 \n ScatNetLight is a MATLAB implementation of the Scattering Networks optimized for image classification on complex datasets. The software 'scatnet_light' and a classification pipeline are included in this version. \n See homepage of the Scattering Networks:\nhttp://www.di.ens.fr/data/scattering \n Software used in the paper:\nDeep Roto-Translation Scattering for Object Classification, Edouard Oyallon, Stéphane Mallat, accepted to CVPR 2015, http://arxiv.org/abs/1412.8659 \n Author contact: Edouard Oyallon, edouard.oyallon@ens.fr\nContributor: Matthew Hirn, matthew.hirn@ens.fr \n Install ScatNetLight + Classification pipeline \n \n launch 'load_path_software' from matlab shell \n \n [optional] : add the following two lines to your startup.m file\nso that matlab does the addpath automatically when it starts:  \n addpath /path/to/scatnet;\nload_path_software;\n \n Quickstarts \n See 'quick_tutorial.m' in the main folder or the 'tutorial' folder in scatnet_light. \n You will have to modify the function 'create_config_caltech_101', 'create_config_caltech_256', 'create_config_cifar_10' and 'create_config_cifar_100'. The dataset will also have to be downloaded and put in separate folders. The software is outputting intermediary features that can take a consequent size on a harddriver. Also, a server with at least 256go of memory is recommended for most of the experiments, if you want to use the dimensionality reduction algorithm.(it can be removed by setting option.Classification.nAverageKernel and option.Classification.D equal to -1) \n Copyright \n This code is mainly based on the implementation of ScatNet v0.2 done by Laurent Sifre and Joakim Anden \n The code for the Selesnick wavelet filters was written by Shihua Cai, Keyong Li, and Ivan Selesnick. It has been included in ScatNet with their permission. \n The supervised algorithm of feedforward selection based on OLS has been written by Xiu Cheng for the class specificic part, Matthew Hirn for the selection of variable algorithm. \n The SVM's code is an extension of libsvm written by Joakim Anden. \n The code to compute kernel has been written by Jordi and Gustavo Camps and is included with their permission.", "THIS VERSION OF THE SCATTERING IS OBSOLETE \n Please check out our new version (pytorch): https://github.com/edouardoyallon/pyscatwave - any version of the Scattering Transform in lua can be considered as obsolete. \n \n ScatWave \n ScatWave is a Torch implementation of 2D scattering using CUDA libraries, designed for images. \n Disclaimer \n This software belongs to the team DATA @ ENS, its main author is Edouard Oyallon. \n How to install \n Assuming Torch is already installed on your computer, simply cd in scatwave_pkg, then 'luarocks make'\nMake sure you have FFTW and cuFFT installed and that the libraries are linked to the software. \n Few results... \n ScatWave + 3FC = 83.0 on CIFAR 10 \nScatWave + 3FC = 56.7 on CIFAR 100 \nScatWave + Deepnet = 91.4% on CIFAR10 \nScatWave + Deepnet = 69.5% on CIFAR100 \n Usage \n scatwave = require 'scatwave' \nx=torch.FloatTensor(128,3,32,32) \nscat = scatwave.network.new(3,x:size()) \nscat_coeff = scat(x) -- or scat(x,1) \n \nYou can go to cuda via: \nscat=scat:cuda() \n Reproducing the paper \n \n \n Data can be downloaded from this page: https://github.com/szagoruyko/wide-residual-networks/blob/master/README.md. \nThe whitened versions work quite better and are used in this work. \n \n \n training the network on cifar10: \nth train_cifar10.lua \n \n \n training the network on cifar100: \nth train_cifar100.lua \n \n \n transfering to matlab F1: \nth get_F1.lua \n \n \n analysing the operator: \nmatlab sparsify_F1.m \n \n \n retraining the deepnet with a new F1: \nth retrain_with_fix_F1_pretrained_end_cifar10.lua \n \n \n replace the scattering by a deepnet with a pretrained and fixed model: \nth replace_scattering_fix_end_cifar10.lua \n \n \n Contributors \n Edouard Oyallon. Contacts: firstname.lastname@ens.fr \n Team DATA - Ecole Normale Supérieure \n Acknowledgements \n The author is thankful to Sergey Zagoruyko for helpfull discussions, codes and enlightments. Many parts of this work are based on codes that he shared, and this had a major impact on this work. I would like to thank also Mathieu Andreux, \nEugene Belilovsky, Carmine Cella, Michael Eickenberg for helpful discussions.", 'Building a Regular Classification Boundary with Deep Networks \n This is the code for the CVPR17 paper  "Building a Regular Classification Boundary with Deep Networks" by Edouard Oyallon. A large part of the code is inspired from https://github.com/bgshih/tf_resnet_cifar yet it has been modified a lot. \n To run all the experiments and obtain the figure of the paper, you can simply do: \n bash script_nonlinearity_alpha.bash\npython build_figure_paper.py \n The best accuracy on CIFAR10 should be 95.4, and on CIFAR100 it should be 79.6, with n_channel equal to 512, alpha=1.0. \n Acknowledgement \n Code modified by Edouard Oyallon - Team DATA ENS, 2016', 'Announcement \n 11/18 \n This package is no longer supported. We have now released kymatio: http://www.kymat.io/ , https://github.com/kymatio/kymatio which includes 1D-2D-3D fast, optimized, differentiable Scattering Transform and subsumes all the behavior of pyscatwave. Among other things you can now more easily use differentiable 2d scattering and use the CPU if desired. kymatio will be well supported with a substantially larger development team than pyscatwave. \n 07/18 \n We just released a differentiable 2D Scattering example in the master. It is not memory efficient yet, neither fast. \n PyScatWave \n CuPy/PyTorch Scattering implementation \n A scattering network is a Convolutional Network with filters predefined to be wavelets that are not learned and it can be used in vision task such as classification of images. The scattering transform can drastically reduce the spatial resolution of the input (e.g. 224x224->14x14) with demonstrably neglible loss in dicriminative power.    \n The software uses PyTorch + NumPy FFT on CPU, and PyTorch + CuPy + CuFFT on GPU. \n Previous (lua-based) versions of the code can be found at  https://github.com/edouardoyallon/scatwave \n If using this code for your research please cite our paper: \n E. Oyallon, E. Belilovsky, S. Zagoruyko  Scaling the Scattering Transform: Deep Hybrid Networks \n You can find experiments from the paper in the following repository:\nhttps://github.com/edouardoyallon/scalingscattering/ \n We used PyTorch for running experiments in  https://arxiv.org/abs/1703.08961 ,\nbut it is possible to use scattering with other frameworks (e.g. Chainer, Theano or Tensorflow) if one copies Scattering outputs to CPU (or run on CPU and convert to  numpy.ndarray  via  .numpy() ). \n Benchmarks \n We do some simple timings and comparisons to the previous (multi-core CPU) implementation of scattering (ScatnetLight). We benchmark the software using a 1080 GPU. Below we show input sizes (WxHx3xBatchSize) and speed: \n 32 × 32 × 3 × 128 (J=2)- 0.03s (speed of 8x vs ScatNetLight) \n 256 × 256 × 3 × 128 (J=2) - 0.71 s (speed up of 225x vs ScatNetLight) \n Installation \n The software was tested on Linux with anaconda Python 2.7 and\nvarious GPUs, including Titan X, 1080s, 980s, K20s, and Titan X Pascal. \n The first step is to install pytorch following instructions from\n http://pytorch.org , then you can run  pip : \n pip install -r requirements.txt\npython setup.py install \n Usage \n Example: \n ```python\nimport torch\nfrom scatwave.scattering import Scattering \n scat = Scattering(M=32, N=32, J=2).cuda()\nx = torch.randn(1, 3, 32, 32).cuda() \n print scat(x).size()\n``` \n Contribution \n All contributions are welcome. \n Authors \n Edouard Oyallon, Eugene Belilovsky, Sergey Zagoruyko', 'Scaling The Scattering Transform : Deep Hybrid Networks \n This repository contains the experiments found in the paper: https://arxiv.org/abs/1703.08961\n \n Requirements \n In order to run our experiments you will need at minimum the following python packages: pytorch,opencv,pyscatwave package.\nThe simplest way to install pytorch and opencv is through anaconda. We recommend python 2.7 + anaconda.\nThe pyscatwave package can be found here https://github.com/edouardoyallon/pyscatwave \n Imagenet \n We provide a pre-trained model similar to the one described in the paper.  \n To run the trained model of scattering+resnet on imagenet ILSVRC validation set: \n 1) Make sure you have downloaded at least the validation set of ILSVRC2012 and have it organized by class categories\n Note : due to problems with pytorch dataset constructors make sure your imagenet directory has no hidden files, or extra directories besides the 1000 ILSVRC categories.. otherwise all the images will be mislabeled\n2) Download the model file from  http://www.di.ens.fr/~oyallon/scatter_resnet_10_model.pt7\n3) Add this to the imagenet/ directory\n4) Run the script main_test.py to evaluate on the ILSVRC validation set specifying --imagenetpath to point to your imagenet directory \n Training scripts for imagenet and SLE feature extractor will be added soon \n STL-10 \n Simply run python main_STL.py script in the STL directory \n CIFAR-10 \n To run the small sample experiments\nExample: \n bash\npython main_small_sample_class_normalized.py --model resnet12_8_scat --save "test"  --seed 1 --sampleSize 500 --mul 20', 'pyscatlight \n This code is for the paper\nhttp://researchers.lille.inria.fr/~valko/hp/publications/oyallon2018compressing.pdf \n Please cite the following if you use this code in your paper\n@InProceedings{Oyallon_2018_ECCV,\nauthor = {Oyallon, Edouard and Belilovsky, Eugene and Zagoruyko, Sergey and Valko, Michal},\ntitle = {Compressing the Input for CNNs with the First-Order Scattering Transform},\nbooktitle = {The European Conference on Computer Vision (ECCV)},\nmonth = {September},\nyear = {2018}\n} \n Installing pyscatlight: \n pip install -r requirements.txt\npython setup.py install \n A typical call for classification looks like: \n python main_scatMultiGPU.py PATHIMAGENET --arch scat50 \nIt is also possible to employ main_scatDistributed.py which requires NCCL. \n For detection, a pretrained model can be found here: https://s3.amazonaws.com/modelzoo-networks/scatresnet50.tar.gz \n A typical call for detection looks like: \n python trainval_net_scat.py  --dataset pascal_voc --net scatnet  --cuda --save_dir FOLDER --nw 4 --lr 1e-2 --lr_decay_step 6 --bs  3 --epochs 10 --s 3 --mGPUs \n Our detection pipeline relies on https://github.com/jwyang/faster-rcnn.pytorch . It was successfully re-tested for pascal\nand coco using pytorch 0.4.0, with python 3.5 . For\nthe sake of reproducibility, the files that were modified or created at time of submission were: \n detection/lib/roi_data_layer/roibatchLoader.py \n detection/trainval_net_scat.py \n detection/test_net_scat.py \n detection/lib/model/utils/blob.py - has to be modified for COCO \n detection/cfgs/scatnet.yml \n detection/cfgs/scatnet_ls.yml \n detection/lib/model/faster_rcnn/scatnet.py', 'Kymatio: wavelet scattering in PyTorch \n Kymatio is a Python package for wavelet scattering transforms, built on top of PyTorch. \n \n \n \n Use Kymatio if you need a library that:\n* integrates wavelet scattering in a deep learning architecture,\n* supports 1-D, 2-D, and 3-D wavelets, and\n* runs seamlessly on CPU and GPU hardware. \n Website:  https://kymatio.github.io \n Installation \n Dependencies \n Kymatio requires: \n \n Python (>= 3.6) \n PyTorch (>= 0.4) \n SciPy (>= 0.13) \n \n We also strongly recommend running Kymatio in a Conda environment since this\nsimplifies installation of PyTorch. \n Linux \n conda install pytorch torchvision -c pytorch\npip install -i https://test.pypi.org/simple/ kymatio==0.0.1 \n macOS \n conda install pytorch torchvision -c pytorch\npip install -i https://test.pypi.org/simple/ kymatio==0.0.1 \n The software was tested on Linux with Anaconda Python 3 and\nvarious GPUs, including Titan X, 1080s, 980s, K20s, and Titan X Pascal. \n The software uses PyTorch + NumPy FFT on CPU, and PyTorch + CuPy + CuFFT on GPU. \n If you use this code in your work please cite our paper: \n The scattering authors,  Kymatio: Fast Scattering in 1-D,2-D,3-D \n This code unifies multiple previous efforts:\n    - PyScatWave/ScatWave,\n    - ScatNetLight,\n    - ScatNet, and others \n Optimized package \n If you have a CUDA-enabled GPU, you may run \n pip install -r requirements_optional_cuda.txt \n after installation to install the optimized  skcuda  backend. To enable it, set\nthe  KYMATIO_BACKEND  environment variable to  skcuda . For more information,\nsee the documentation. \n Documentation \n To build the documentation, please run \n pip install -r requirements_optional.txt\ncd doc; make clean; make html \n You may then read the documentation in  doc/build/html/index.html .', 'lazy-training-code \n This code was based on https://github.com/kuangliu/pytorch-cifar .  \n Reproducing CNNs experiments \n If you want to obtain CNN experiments accuracies and loss from the paper, simply run: \n cd cnn\nsh script.sh \n The  double  precision experiments require a Tesla or Volta GPUs for handling this numerical precision at a reasonable speed... \n Reproducing shallow experiments \n All the codes necessary to reproduce the results from the paper as located in  shallow-nn \n Contributions \n All contributions are welcome.', 'Interferometric Graph Transform \n This repository contains the code corresponding to the ICML-2020 publication: https://arxiv.org/abs/2006.05722 \n If you have some troubles to reproduce some results, some questions regarding the code or even find a bug, please open directly an issue \nin this repository. If your questions concern some technical elements of the paper, please send directly an email to the author, firstname.name[AT]lip6.fr \n If you wish to cite this paper, please use: \n @inproceedings{oyallon2020interferometric,\n  title={Interferometric Graph Transform: a Deep Unsupervised Graph Representation},\n  author={Oyallon, Edouard},\n  booktitle={37th International Conference on Machine Learning (ICML 2020)},\n  year={2020}\n} \n Reproducing the numerical results \n In order to reproduce the experiments of the paper  Interferometric Graph Transform: a Deep Unsupervised Graph Representation , \nmake sure you have  pytorch  and  python 3  installed, as well as  MATLAB  (only for the Haar Scattering experiments) and please\n follow the next instructions. \n Community detection experiments \n Please replace (or add) the files of https://github.com/alelab-upenn/graph-scattering-transforms with the ones in the folder "community-detection". Then run the code as described in this GitHub repository, by adding the corresponding dataset, as described. \n Images experiments \n Please run the following instructions on a GPU: \n CIFAR10/MNIST, graph \n $DATASET=cifar/mnist ,  $PATH_TO_MODEL  refers to the path in which the first script stores the weight of the model.  \n python build_representation.py --K 40 --J 3 --lr_schedule \'{0:1e0,500:1e-1,1000:1e-2,1500:1e-3}\' --epochs 5  --dataset $DATASET\npython classif.py --J 3 --feature interferometric --C 1e-3 --mode 1 --classifier \'svm\'  --dataset $DATASET --path $PATH_TO_MODEL \n CIFAR10, scattering \n python classif.py --J 3 --feature scattering --C 1e-3 --mode 1 --classifier \'svm\'  --dataset cifar \n Graph experiments \n NTU/SBU \n Move the dataset (available  https://github.com/mazariahmed06/graph_skeletons ) in the same folder as the main file, then  $DATASET=NTU_xview_values/NTU_xsub_values/SBU ,\n $PATH_TO_MODEL  refers to the path in which the first script stores the weight of the model,  $ORDER  is the required order and \n $K  the respective number of filters, which should end by 0 (e.g.,  $ORDER=1  leads to  $K=30,0  for instance). \n Finally,  $LR_SCHEDULE  must be the learning rate schedule reported in the paper (as above). Add  --no_averaging  to remove the averaging from \nthe experiments, yet this will substantially increase the computation time because the representation is larger. \n python build_representation_graph.py --K $K --order $ORDER --lr_schedule $LR_SCHEDULE --epochs 5 --data_name $DATASET\npython classif_graph.py --mode \'svm\' --order 1 --data_name $DATASET --file $PATH_TO_MODEL  --K 10,5,0 --C 1e-1 \n Haar scattering \n First download  HaarScat  from  https://www.di.ens.fr/data/software/ and then switch and run the scripts in the folder demo that we provided.']
SeanNaren,["deepspeech.torch \n \n \n Implementation of  Baidu Warp-CTC  using torch7.\nCreates a network based on the  DeepSpeech2  architecture using the Torch7 library, trained with the CTC activation function. \n Features \n \n Train large models with large datasets via online loading using  LMDB  and multi-GPU support. \n Supports variable length batches via padding. \n Implements the  AN4 Audio database  (50 mins of data).\nHas also been extended to train using the  LibriSpeech  dataset (1000 hours of data). Custom dataset preparation is explained in documentation. \n \n Branches \n There are currently two branches, Master and Phoneme:\n* Master: This branch trains DeepSpeech2. Also included is an evaluation script which calculates the WER/CER, as well as a prediction script.\nThis branch is useful for understanding how the DeepSpeech and CTC works and is easy to run after installation. Highly recommended to checkout this branch.\n* Phonemes: This branch is experimental and uses phonemes rather than character based predictions. This is fully credited and extended by  CCorfield  and his awesome work in porting to use phonemes. In addition to this\nI'd like to also thank  Shane Walker  for his awesome recent conversion to use phonemes as well. \n Installation/Data Preparation/Documentation \n Follow Instructions/Data Preparation/Documentation found in the wiki  here  to set up and run the code. \n Technical documentation can be found  here . \n Pre-trained Networks \n Pre-trained networks are available for AN4 as well as LibriSpeech for CUDA only (since they use cudnn RNNs). Download Links and accuracies are below. DeepSpeech-light is a smaller model which is less intensive to train (based on LSTMs rather than RNNs). \n AN4 \n an4Test \n |Network                | WER       | CER       |Link       |\n|-----------------|:--------:|:--------:|:--------:|\n|DeepSpeech-light| N/A     | N/A | N/A |\n|DeepSpeech | 12    | 3.07 |  Download  | \n LibriSpeech \n Librispeech-test-clean \n |Network                | WER       | CER       |Link       |\n|-----------------|:--------:|:--------:|:--------:|\n|DeepSpeech-light| 15     | 1.34 |  Download  |\n|DeepSpeech | 12    | 1.55 |  Download  | \n Librispeech-test-other \n |Network                | WER       | CER       |Link       |\n|-----------------|:--------:|:--------:|:--------:|\n|DeepSpeech-light| 36    | 3.80 | (Download Above) |\n|DeepSpeech | 33    | 3.24 | (Download Above) | \n Once you're set up, you can start training from these nets by using the below parameters (you might need to change the other parameters described in the wiki) after setting the project up: \n lua\nth Train.lua -loadModel -loadPath /path/to/model.t7 \n Acknowledgements \n Lots of people helped/contributed to this project that deserve recognition:\n* Soumith Chintala for his support on Torch7 and the vast open source projects he has contributed that made this project possible!\n* Charles Corfield for his work on the Phoneme Dataset and his overall contribution and aid throughout.\n* Will Frey for his thorough communication and aid in the development process.\n* Ding Ling, Yuan Yang and Yan Xia for their significant contribution to online training, multi-gpu support and many other important features.\n* Erich Elsen and the team from Baidu for their contribution of Warp-CTC that made this possible, and the encouraging words and support given throughout the project.\n* Maciej Korzepa for his huge help in training a model on Librispeech!", "QlearningExample.torch \n \n Torch plays catch! Based on Eder Santanas'  implementation  in keras. Highly recommend reading his informative and easy to follow blog post  here . \n Agent has to catch the fruit before it falls to the ground. Agent wins if he succeeds to catch the fruit, loses if he fails. \n Dependencies \n To install torch7 follow the guide  here . \n Other dependencies can be installed via luarocks: \n luarocks install optim\nluarocks install image \n How to run \n To train a model, run the  Train.lua  script. You can configure parameters such as below: \n th Train.lua -epoch 1000 #Configures the number of epochs. More parameters are available, check scrip.t \n Visualization \n To visualise the agent playing the game after training a model, use the  TorchPlaysCatch.lua  script using qlua rather than th as below: \n qlua TorchPlaysCatch.lua \n Much like the train script, there are configurable options. Check the script for more details! \n Acknowledgements \n Eder Santana, Keras plays catch, (2016), GitHub repository, https://gist.github.com/EderSantana/c7222daa328f0e885093", 'deepspeech.pytorch \n \n Implementation of DeepSpeech2 for PyTorch using  PyTorch Lightning . The repo supports training/testing and inference using the  DeepSpeech2  model. Optionally a  kenlm  language model can be used at inference time. \n Install \n Several libraries are needed to be installed for training to work. I will assume that everything is being installed in\nan Anaconda installation on Ubuntu, with PyTorch installed. \n Install  PyTorch  if you haven\'t already. \n If you want decoding to support beam search with an optional language model, install ctcdecode:\n git clone --recursive https://github.com/parlance/ctcdecode.git\ncd ctcdecode && pip install . \n Finally clone this repo and run this within the repo:\n pip install -r requirements.txt\npip install -e . # Dev install \n If you plan to use Multi-node training, you\'ll need etcd. Below is the command to install on Ubuntu.\n sudo apt-get install etcd \n Docker \n To use the image with a GPU you\'ll need to have  nvidia-docker  installed. \n bash\nsudo docker run -ti --gpus all -v `pwd`/data:/workspace/data --tmpfs /tmp -p 8888:8888 --net=host --ipc=host seannaren/deepspeech.pytorch:latest # Opens a Jupyter notebook, mounting the /data drive in the container \n Optionally you can use the command line by changing the entrypoint: \n bash\nsudo docker run -ti --gpus all -v `pwd`/data:/workspace/data --tmpfs /tmp --entrypoint=/bin/bash --net=host --ipc=host seannaren/deepspeech.pytorch:latest \n Training \n Datasets \n Currently supports  AN4 ,  TEDLIUM ,  Voxforge ,  Common Voice  and  LibriSpeech . Scripts will setup the dataset and create manifest files used in data-loading. The scripts can be found in the data/ folder. Many of the scripts allow you to download the raw datasets separately if you choose so. \n Training Commands \n AN4 \n ```bash\ncd data/ && python an4.py && cd .. \n python train.py +configs=an4\n``` \n LibriSpeech \n ```bash\ncd data/ && python librispeech.py && cd .. \n python train.py +configs=librispeech\n``` \n Common Voice \n ```bash\ncd data/ && python common_voice.py && cd .. \n python train.py +configs=commonvoice\n``` \n TEDlium \n ```bash\ncd data/ && python ted.py && cd .. \n python train.py +configs=tedlium\n``` \n Custom Dataset \n To create a custom dataset you must create a JSON file containing the locations of the training/testing data. This has to be in the format of:\n json\n{\n  "root_path":"path/to",\n  "samples":[\n    {"wav_path":"audio.wav","transcript_path":"text.txt"},\n    {"wav_path":"audio2.wav","transcript_path":"text2.txt"},\n    ...\n  ]\n} \nWhere the  root_path  is the root directory,  wav_path  is to the audio file, and the  transcript_path  is to a text file containing the transcript on one line. This can then be used as stated below. \n Note on CSV files ... \n Up until release  V2.1 , deepspeech.pytorch used CSV manifest files instead of JSON.\nThese manifest files are formatted similarly as a 2 column table:\n /path/to/audio.wav,/path/to/text.txt\n/path/to/audio2.wav,/path/to/text2.txt\n... \nNote that this format is incompatible  V3.0  onwards. \n Merging multiple manifest files \n To create bigger manifest files (to train/test on multiple datasets at once) we can merge manifest files together like below. \n cd data/\npython merge_manifests.py manifest_1.json manifest_2.json --out new_manifest_dir \n Modifying Training Configs \n Configuration is done via  Hydra . \n Defaults can be seen in  config.py . Below is how you can override values set already: \n python train.py data.train_path=data/train_manifest.json data.val_path=data/val_manifest.json \n Use  python train.py --help  for all parameters and options. \n You can also specify a config file to keep parameters stored in a yaml file like so: \n Create folder  experiment/  and file  experiment/an4.yaml :\n yaml\ndata:\n  train_path: data/an4_train_manifest.json\n  val_path: data/an4_val_manifest.json \n python train.py +experiment=an4 \n To see options available, check  here . \n Multi-GPU Training \n We support single-machine multi-GPU training via  PyTorch Lightning . \n Below is an example command when training on a machine with 4 local GPUs: \n python train.py +configs=an4 trainer.gpus=4 \n Multi-Node Training \n Also supported is multi-machine capabilities using TorchElastic. This requires a node to exist as an explicit etcd host (which could be one of the GPU nodes but isn\'t recommended), a shared mount across your cluster to load/save checkpoints and communication between the nodes. \n Below is an example where we\'ve set one of our GPU nodes as our etcd host however if you\'re scaling up, it would be suggested to have a separate instance as your etcd instance to your GPU nodes as this will be a single point of failure. \n Assumed below is a shared drive called /share where we save our checkpoints and data to access. \n Run on the etcd host:\n PUBLIC_HOST_NAME=127.0.0.1 # Change to public host name for all nodes to connect\netcd --enable-v2 \\\n     --listen-client-urls http://$PUBLIC_HOST_NAME:4377 \\\n     --advertise-client-urls http://$PUBLIC_HOST_NAME:4377 \\\n     --listen-peer-urls http://$PUBLIC_HOST_NAME:4379 \n Run on each GPU node:\n python -m torchelastic.distributed.launch \\\n        --nnodes=2 \\\n        --nproc_per_node=4 \\\n        --rdzv_id=123 \\\n        --rdzv_backend=etcd \\\n        --rdzv_endpoint=$PUBLIC_HOST_NAME:4377 \\\n        train.py data.train_path=/share/data/an4_train_manifest.json \\\n                 data.val_path=/share/data/an4_val_manifest.json model.precision=half \\\n                 data.num_workers=8 checkpoint.save_folder=/share/checkpoints/ \\\n                 checkpoint.checkpoint=true checkpoint.load_auto_checkpoint=true checkpointing.save_n_recent_models=3 \\\n                 data.batch_size=8 trainer.max_epochs=70 \\\n                 trainer.accelerator=ddp trainer.gpus=4 trainer.num_nodes=2 \n Using the  load_auto_checkpoint=true  flag we can re-continue training from the latest saved checkpoint. \n Currently it is expected that there is an NFS drive/shared mount across all nodes within the cluster to load the latest checkpoint from. \n Augmentation \n There is support for three different types of augmentations: SpecAugment, noise injection and random tempo/gain perturbations. \n SpecAugment \n Applies simple Spectral Augmentation techniques directly on Mel spectogram features to make the model more robust to variations in input data. To enable SpecAugment, use the  --spec-augment  flag when training. \n SpecAugment implementation was adapted from  this  project. \n Noise Injection \n Dynamically adds noise into the training data to increase robustness. To use, first fill a directory up with all the noise files you want to sample from.\nThe dataloader will randomly pick samples from this directory. \n To enable noise injection, use the  --noise-dir /path/to/noise/dir/  to specify where your noise files are. There are a few noise parameters to tweak, such as\n --noise_prob  to determine the probability that noise is added, and the  --noise-min ,  --noise-max  parameters to determine the minimum and maximum noise to add in training. \n Included is a script to inject noise into an audio file to hear what different noise levels/files would sound like. Useful for curating the noise dataset. \n python noise_inject.py --input-path /path/to/input.wav --noise-path /path/to/noise.wav --output-path /path/to/input_injected.wav --noise-level 0.5 # higher levels means more noise \n Tempo/Gain Perturbation \n Applies small changes to the tempo and gain when loading audio to increase robustness. To use, use the  --speed-volume-perturb  flag when training. \n Checkpoints \n Typically checkpoints are stored in  lightning_logs/  in the current working directory of the script. \n This can be adjusted: \n python train.py checkpoint.file_path=save_dir/ \n To load a previously saved checkpoint: \n python train.py trainer.resume_from_checkpoint=lightning_logs/deepspeech_checkpoint_epoch_N_iter_N.ckpt \n This continues from the same training state. \n Testing/Inference \n To evaluate a trained model on a test set (has to be in the same format as the training set): \n python test.py model.model_path=models/deepspeech.pth test_path=/path/to/test_manifest.json \n An example script to output a transcription has been provided: \n python transcribe.py \\\n       model.model_path=models/deepspeech.pth \\\n       model.cuda=True \\\n       chunk_size_seconds=-1 \\\n       audio_path=audio_path=/path/to/audio.wav \n If you used mixed-precision or half precision when training the model, you can use the  model.precision=half  for a speed/memory benefit. If you want to transcribe a long audio file that does not fit in the GPU, change the value of  chunk_size_seconds  to a positive number which represents the chunk size in seconds that will be used to segment the long audio file based on it. \n Inference Server \n Included is a basic server script that will allow post request to be sent to the server to transcribe files. \n ```\npython server.py --host 0.0.0.0 --port 8000 # Run on one window \n curl -X POST http://0.0.0.0:8000/transcribe -H "Content-type: multipart/form-data" -F "file=@/path/to/input.wav"\n``` \n Using an ARPA LM \n We support using kenlm based LMs. Below are instructions on how to take the LibriSpeech LMs found  here  and tune the model to give you the best parameters when decoding, based on LibriSpeech. \n Tuning the LibriSpeech LMs \n First ensure you\'ve set up the librispeech datasets from the data/ folder.\nIn addition download the latest pre-trained librispeech model from the releases page, as well as the ARPA model you want to tune from  here . For the below we use the 3-gram ARPA model (3e-7 prune). \n First we need to generate the acoustic output to be used to evaluate the model on LibriSpeech val.\n python test.py data.test_path=data/librispeech_val_manifest.json model.model_path=librispeech_pretrained_v2.pth save_output=librispeech_val_output.npy \n We use a beam width of 128 which gives reasonable results. We suggest using a CPU intensive node to carry out the grid search. \n python search_lm_params.py --num-workers 16 --saved-output librispeech_val_output.npy --output-path libri_tune_output.json --lm-alpha-from 0 --lm-alpha-to 5 --lm-beta-from 0 --lm-beta-to 3 --lm-path 3-gram.pruned.3e-7.arpa  --model-path librispeech_pretrained_v2.pth --beam-width 128 --lm-workers 16 \n This will run a grid search across the alpha/beta parameters using a beam width of 128. Use the below script to find the best alpha/beta params: \n python select_lm_params.py --input-path libri_tune_output.json \n Use the alpha/beta parameters when using the beam decoder. \n Building your own LM \n To build your own LM you need to use the KenLM repo found  here . Have a read of the documentation to get a sense of how to train your own LM. The above steps once trained can be used to find the appropriate parameters. \n Alternate Decoders \n By default,  test.py  and  transcribe.py  use a  GreedyDecoder  which picks the highest-likelihood output label at each timestep. Repeated and blank symbols are then filtered to give the final output. \n A beam search decoder can optionally be used with the installation of the  ctcdecode  library as described in the Installation section. The  test  and  transcribe  scripts have a  lm  config. To use the beam decoder, add  lm.decoder_type=beam . The beam decoder enables additional decoding parameters:\n-  lm.beam_width  how many beams to consider at each timestep\n-  lm.lm_path  optional binary KenLM language model to use for decoding\n-  lm.alpha  weight for language model\n-  lm.beta  bonus weight for words \n Time offsets \n Use the  offsets=true  flag to get positional information of each character in the transcription when using  transcribe.py  script. The offsets are based on the size\nof the output tensor, which you need to convert into a format required.\nFor example, based on default parameters you could multiply the offsets by a scalar (duration of file in seconds / size of output) to get the offsets in seconds. \n Pre-trained models \n Pre-trained models can be found under releases  here . \n Acknowledgements \n Thanks to  Egor  and  Ryan  for their contributions!', '\n In Chinese 中文版 \n warp-ctc \n A fast parallel implementation of CTC, on both CPU and GPU. \n Introduction \n Connectionist Temporal Classification \nis a loss function useful for performing supervised learning on sequence data,\nwithout needing an alignment between input data and labels.  For example, CTC\ncan be used to train\n end-to-end \n systems  for\n speech recognition ,\nwhich is how we have been using it at Baidu\'s Silicon Valley AI Lab. \n \n The illustration above shows CTC computing the probability of an output\nsequence "THE CAT ", as a sum over all possible alignments of input sequences\nthat could map to "THE CAT ", taking into account that labels may be duplicated\nbecause they may stretch over several time steps of the input data (represented by\nthe spectrogram at the bottom of the image).\nComputing the sum of all such probabilities explicitly would be prohibitively costly due to the\ncombinatorics involved, but CTC uses dynamic programming to dramatically\nreduce the complexity of the computation. Because CTC is a differentiable function,\nit can be used during standard SGD training of deep neural networks. \n In our lab, we focus on scaling up recurrent neural networks, and CTC loss is an\nimportant component. To make our system efficient, we parallelized the CTC\nalgorithm, as described in  this paper .\nThis project contains our high performance CPU and CUDA versions of the CTC loss,\nalong with bindings for  Torch .\nThe library provides a simple C interface, so that it is easy to\nintegrate into deep learning frameworks. \n This implementation has improved training scalability beyond the\nperformance improvement from a faster parallel CTC implementation. For\nGPU-focused training pipelines, the ability to keep all data local to\nGPU memory allows us to spend interconnect bandwidth on increased data\nparallelism. \n Performance \n Our CTC implementation is efficient compared with many of the other publicly available implementations.  It is\nalso written to be as numerically stable as possible.  The algorithm is numerically sensitive and we have observed\ncatastrophic underflow even in double precision with the standard calculation - the result of division of \ntwo numbers on the order of 1e-324 which should have been approximately one, instead become infinity \nwhen the denominator underflowed to 0.  Instead, by performing the calculation in log space, it is numerically\nstable even in single precision floating point at the cost of significantly more expensive operations.  Instead of\none machine instruction, addition requires the evaluation of multiple transcendental functions.  Because of this,\nthe speed of CTC implementations can only be fairly compared if they are both performing the calculation the same\nway. \n We compare our performance with  Eesen , \na CTC implementation built on \n Theano ,\nand a Cython CPU only implementation  Stanford-CTC .\nWe benchmark the Theano implementation operating on 32-bit floating-point numbers and doing the calculation in log-space,\nin order to match the other implementations we compare against.  Stanford-CTC was modified to perform the calculation\nin log-space as it did not support it natively.  It also does not support minibatches larger than 1, so would require\nan awkward memory layout to use in a real training pipeline, we assume linear increase in cost with minibatch size. \n We show results on two problem sizes relevant to our English and Mandarin end-to-end models, respectively, where  T  represents the number of timesteps in the input to CTC,  L  represents the length of the labels for each example, and  A  represents the alphabet size. \n On the GPU, our performance at a minibatch of 64 examples ranges from 7x faster to 155x faster than Eesen, and 46x to 68x faster than the Theano implementation. \n GPU Performance \n Benchmarked on a single NVIDIA Titan X GPU. \n |  T =150,  L =40,  A =28           | warp-ctc  | Eesen   | Theano  |\n|-----------------------------------|-------|---------|---------|\n|  N =1                             | 3.1 ms| .5 ms   | 67 ms |\n|  N =16                            | 3.2 ms| 6  ms   | 94 ms |\n|  N =32                            | 3.2 ms| 12 ms   | 119 ms |\n|  N =64                            | 3.3 ms| 24 ms   | 153 ms |\n|  N =128                           | 3.5 ms| 49 ms   | 231 ms | \n |  T =150,  L =20,  A =5000         | warp-ctc  | Eesen   | Theano  |\n|-----------------------------------|-------|---------|---------|\n|  N =1                             | 7 ms  | 40   ms | 120 ms |\n|  N =16                            | 9 ms  | 619  ms | 385 ms |\n|  N =32                            | 11 ms | 1238 ms | 665 ms |\n|  N =64                            | 16 ms | 2475 ms | 1100 ms |\n|  N =128                           | 23 ms | 4950 ms | 2100 ms | \n CPU Performance \n Benchmarked on a dual-socket machine with two Intel E5-2660 v3\nprocessors - warp-ctc used 40 threads to maximally take advantage of the CPU resources.\nEesen doesn\'t provide a CPU implementation. We noticed that the Theano implementation was not\nparallelizing computation across multiple threads.  Stanford-CTC provides no mechanism\nfor parallelization across threads. \n |  T =150,  L =40,  A =28           | warp-ctc  | Stanford-CTC   | Theano  |\n|-----------------------------------|-------|---------|---------|\n|  N =1                             | 2.6 ms|  13 ms  | 15 ms |\n|  N =16                            | 3.4 ms|  208 ms | 180 ms |\n|  N =32                            | 3.9 ms|  416 ms | 375 ms |\n|  N =64                            | 6.6 ms|  832 ms | 700 ms |\n|  N =128                           |12.2 ms| 1684 ms | 1340 ms | \n |  T =150,  L =20,  A =5000         | warp-ctc  | Stanford-CTC   | Theano  |\n|-----------------------------------|-------|---------|---------|\n|  N =1                             | 21 ms |  31 ms  | 850 ms  |\n|  N =16                            | 37 ms |  496 ms | 10800 ms|\n|  N =32                            | 54 ms |  992 ms | 22000 ms|\n|  N =64                            | 101 ms| 1984 ms | 42000 ms|\n|  N =128                           | 184 ms| 3968 ms | 86000 ms| \n Interface \n The interface is in  include/ctc.h .\nIt supports CPU or GPU execution, and you can specify OpenMP parallelism\nif running on the CPU, or the CUDA stream if running on the GPU. We\ntook care to ensure that the library does not perform memory\nallocation internally, in order to avoid synchronizations and\noverheads caused by memory allocation. \n Compilation \n warp-ctc has been tested on Ubuntu 14.04 and OSX 10.10.  Windows is not supported\nat this time. \n First get the code: \n git clone https://github.com/baidu-research/warp-ctc.git\ncd warp-ctc \n create a build directory: \n mkdir build\ncd build \n if you have a non standard CUDA install  export CUDA_BIN_PATH=/path_to_cuda  so that CMake detects CUDA and\nto ensure Torch is detected, make sure  th  is in  $PATH \n run cmake and build: \n cmake ../\nmake \n The C library and torch shared libraries should now be built along with test\nexecutables.  If CUDA was detected, then  test_gpu  will be built;  test_cpu \nwill always be built. \n Tests \n To run the tests, make sure the CUDA libraries are in  LD_LIBRARY_PATH  ( DYLD_LIBRARY_PATH  for OSX). \n The Torch tests must be run from the  torch_binding/tests/  directory. \n Torch Installation \n luarocks make torch_binding/rocks/warp-ctc-scm-1.rockspec \n You can also install without cloning the repository using \n luarocks install http://raw.githubusercontent.com/baidu-research/warp-ctc/master/torch_binding/rocks/warp-ctc-scm-1.rockspec \n There is a Torch CTC  tutorial . \n Contributing \n We welcome improvements from the community, please feel free to submit pull\nrequests. \n Known Issues  / Limitations \n The CUDA implementation requires a device of at least compute capability 3.0. \n The CUDA implementation supports a maximum label length of 639 (timesteps are\nunlimited).', 'CORD-19-ANN \n \n     GitHub Pages \n This repo contains the scripts and models to search  CORD-19  using  S-BERT  embeddings via  nmslib  or  faiss . \n Sentence embeddings are not perfect for searching (see  this issue ) however can provide insight into the data that basic search functionality cannot. There is still room to improve the retrieval of relevant documents. \n We\'re not versed in the medical field, so any feedback or improvements we deeply encourage in the form of issues/PRs! \n We\'ve included pre-trained models and the FAISS index to start your own server with instructions below. \n Finally we provide a front-end that can be used to search through the dataset and extract information via a UI. Instructions and installation for the front-end can be found  here . \n We currently are hosting the server on a gcp instance, if anyone can contribute for a more permanent hosting solution it would be appreciated. \n Installation \n Source \n We assume you have installed PyTorch and the necessary CUDA packages from  here . We suggest using Conda to make installation easier.\n``` \n Install FAISS \n conda install faiss-cpu -c pytorch # Other instructions can be found at https://github.com/facebookresearch/faiss/blob/master/INSTALL.md \n git clone https://github.com/SeanNaren/CORD-19-ANN.git --recursive\ncd CORD-19-ANN/\npip install -r requirements.txt\npip install .\n``` \n Docker \n We also provide a docker container: \n docker pull seannaren/cord-19-ann\nsudo docker run -it --net=host --ipc=host --entrypoint=/bin/bash --rm seannaren/cord-19-ann \n Download Models \n We currently offer sentence models trained on  BlueBERT  (base uncased model) and  BioBERT  (base cased model) with the appropriate metadata/index. We currently serve S-BlueBERT however it is interchangeable. \n Download S-BERT Models and Search Index \n Download the corresponding Model and Index file. We suggest using S-BioBERT and assume you have done so for the subsequent commands. They are interchangeable however. \n | Model                       | Index                          | Test MedNLI Accuracy | Test STS Benchmark Cosine Pearson |\n|-----------------------------|--------------------------------|-----------------|------------------------------|\n|  S-BioBERT Base Cased     |  BioBERT_faiss_PCAR128_SQ8   | 0.7482          | 0.7122                       |\n|  S-BlueBERT Base Uncased  |  BlueBERT_faiss_PCAR128_SQ8  | 0.7525          | 0.6923                       |\n| S-Bert Base Cased             |                                | 0.5689          | 0.7265                       | \n Download Metadata \n wget https://github.com/SeanNaren/CORD-19-ANN/releases/download/V1.0/cord_19_dataset_formatted_2020_03_27.tar.gz\ntar -xzvf cord_19_dataset_formatted_2020_03_27.tar.gz cord_19_dataset_formatted/ \n Searching the Index \n We assume you\'ve chosen the s-biobert model, it should be straightforward to swap in any other pre-trained models offered in this repo by modifying the paths below. \n We recommend using the server but we do offer a simple script to search given a text file of sentences: \n echo "These RNA transcripts may be spliced to give rise to mRNAs encoding the envelope (Env) glycoproteins (Fig. 1a)" > sentences.txt\npython search_index.py --index_path biobert_mli_faiss_PCAR128_SQ8 --index_type faiss --model_name_or_path s-biobert_base_cased_mli/ --dataset_path cord_19_dataset_formatted/ --input_path sentences.txt --output_path output.json \n Using the server \n To start the server:\n YOUR_IP=0.0.0.0\nYOUR_PORT=1337\npython index_server.py --index_path biobert_mli_faiss_PCAR128_SQ8 --index_type faiss --model_name_or_path s-biobert_base_cased_mli/ --dataset_path cord_19_dataset_formatted/ --address $YOUR_IP --port $YOUR_PORT --silent \n To test the server:\n curl --header "Content-Type: application/json" \\\n  --request POST \\\n  --data \'["These RNA transcripts may be spliced to give rise to mRNAs encoding the envelope (Env) glycoproteins (Fig. 1a)"]\' \\\n  http://$YOUR_IP:$YOUR_PORT/query \n Output Format \n The output from the index is a JSON object containing the top K hits from the index, an example of the API is given below: \n [\n  {\n    "query": "These RNA transcripts may be spliced to give rise to mRNAs encoding the envelope (Env) glycoproteins (Fig. 1a)",\n    "hits": [\n      {\n        "title": "Title",\n        "authors": [\n          "..."\n        ],\n        "abstract": [\n          "..."\n        ],\n        "paragraph": "Paragraph that included the hit",\n        "sentence": "The semantically similar sentence",\n        "distance": 42,\n      }\n    ]\n  }\n] \n Creating the Index from scratch \n The process requires a GPU enabled node such as a GCP n8 node with a nvidia-tesla-v100 to generate the embeddings, with at-least 20GB RAM. \n Preparing the dataset \n Currently we tokenize at the sentence level using SciSpacy, however future work may look into using paragraph level tokenization. \n mkdir datasets/\npython download_data.py\npython extract_sentences.py --num_workers 16 \n Generating embeddings \n Using fine-tuned BioBERT/BlueBERT \n Using sentence-transformers we can fine-tune either model. BlueBERT offers only uncased models whereas BioBERT offer a cased model. We\'ve converted them into PyTorch format and included them in releases, to download: \n wget https://github.com/SeanNaren/CORD-19-ANN/releases/download/V1.0/s-biobert_base_cased_mli.tar.gz\nwget https://github.com/SeanNaren/CORD-19-ANN/releases/download/V1.0/s-bluebert_base_uncased_mli.tar.gz\ntar -xzvf s-biobert_base_cased_mli.tar.gz\ntar -xzvf s-bluebert_base_uncased_mli.tar.gz \n Using Pre-trained BioBERT/BlueBERT \n python generate_embeddings.py --model_name_or_path s-biobert_base_cased_mli/ --embedding_path biobert_embeddings.npy --device cuda --batch_size 256 # If you want to use biobert\npython generate_embeddings.py --model_name_or_path s-bluebert_base_uncased_mli/ --embedding_path bluebert_embeddings.npy --device cuda --batch_size 256 # If you want to use bluebert \n Using pre-trained S-BERT models \n You can also use the standard pre-trained model from the S-BERT repo like below, however we suggest using the fine-tuned models offered in this repo. \n python generate_embeddings.py --model_name_or_path bert-base-nli-mean-tokens --embedding_path pretrained_embeddings.npy --device cuda --batch_size 256 \n Training the model from scratch \n This takes a few hours on a V100 GPU. \n If you\'d like to include the MedNLI dataset during training, you\'ll need to download the dataset from  here . Getting access requires credentialed access which requires some efforts and a waiting period of up to two weeks. \n Once trained the model is saved to the  output/  folder by default. Inside there you\'ll find checkpoints such as  output/training_nli/biobert-2020-03-30_10-51-49/  after training has finished. Use this as the model path when generating your embeddings. \n ```\nwget https://github.com/SeanNaren/CORD-19-ANN/releases/download/V1.0/biobert_cased_v1.1.tar.gz\nwget https://github.com/SeanNaren/CORD-19-ANN/releases/download/V1.0/bluebert_base_uncased.tar.gz\ntar -xzvf biobert_cased_v1.1.tar.gz\ntar -xzvf bluebert_base_uncased.tar.gz \n mkdkir datasets/\npython sentence-transformers/examples/datasets/get_data.py --output_path datasets/\npython sentence-transformers/examples/training_nli_transformers.py --model_name_or_path biobert_cased_v1.1/\npython sentence-transformers/examples/training_nli_transformers.py --model_name_or_path bluebert_base_uncased/ --do_lower_case \n Training with medNLI \n python sentence-transformers/examples/training_nli_transformers.py --model_name_or_path biobert_cased_v1.1/ --mli_dataset_path path/to/mednli/\npython sentence-transformers/examples/training_nli_transformers.py --model_name_or_path bluebert_base_uncased/ --mli_dataset_path path/to/mednli/ --do_lower_case\n``` \n To exclude the MedNLI but still evaluate on the data (still requires the MedNLI dataset), use the  --exclude_mli . \n Create the Index \n We have the ability to use faiss or nmslib given the parameter below. We\'ve exposed the FAISS config string for modifying the index. More details about selecting the index can be seen  here . \n python create_index.py --output_path index --embedding_path pretrained_embeddings.npy --index_type faiss # Swap to scibert_embeddings.npy if using fine-tuned SciBERT embeddings \n Clustering \n We also took the example clustering script out of sentence-transformers and added it to this repository for using the pre-trained models. An example below: \n python cluster_sentences.py --input_path sentences.txt --model_name_or_path biobert_cased_v1.1/ --device cpu \n There is also a more interactive version available using the Google Colab demo:    \n Acknowledgements \n Thanks to the authors of the various libraries that made this possible! \n \n sentence-transformers \n cord-19 \n scibert \n nmslib \n FAISS \n', "Lightning Barlow Twins \n \n \n \n This is a  PyTorch Lightning  port of the  Barlow Twins implementation  release by Facebook Research. \n Hyper-parameters have been set based on commands in the PyTorch Barlow Twins implementation README. \n Usage \n pip install -r requirements.txt \n Training \n Train your own backbone on the ImageNet dataset (look inside  train.py  to use CIFAR10 dataset instead). Requires you to have ImageNet downloaded, instructions  here . \n Have a look at the  PyTorch Lightning Trainer  documentation for more flags to enable. \n Note that the Facebook released ResNet pre-trained weights is trained with a total effective batch size of  2048 . Modify the batch size and the number of GPUs based on compute, with a helpful table  here  which can be used to estimate training times.   \n python train.py --gpus 8 --batch_size 256 \n Linear Evaluation \n Run linear evaluation on the  pre-trained ResNet weights  provided by Facebook Research. Requires you to have ImageNet downloaded, instructions  here . Look inside  evaluate.py  to swap to CIFAR10. \n python evaluate.py --gpus 8 --batch_size 256 \n Run linear evaluation on a trained BarlowTwins  LightningModule :\n python evaluate.py --gpus 8 --batch_size 256 --model_path /path/to/lightning_logs/model.ckpt \n To customize parameters, look at  train.py  and  evaluate.py  respectively. \n License \n Unfortunately I've had to duplicate the licence from the original Barlow Twins Pytorch Implementation, which is a restrictive non-commercial licence :( \n This project is under the CC-BY-NC 4.0 license. See  LICENSE  for details. \n Citations \n Thanks to  Phoeby  for the illustration :) \n @article{zbontar2021barlow,\n  title={Barlow Twins: Self-Supervised Learning via Redundancy Reduction},\n  author={Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, St{\\'e}phane},\n  journal={arXiv preprint arXiv:2103.03230},\n  year={2021}\n}", 'min-LLM \n Minimal code to train a relatively large language model (1-10B parameters). \n \n Minimal codebase to learn and adapt for your own use cases \n Concise demonstration of tricks to optimally train a larger language model \n Allows exploration of compute optimal models at smaller sizes based on realistic scaling laws \n \n The project was inspired by  megatron  and all sub-variants. This repo can be seen as a condensed variant, where some of the very large scaling tricks are stripped out for the sake of readability/simplicity. \n For example, the library does not include Tensor Parallelism/Pipeline Parallelism. If you need to reach those 100B+ parameter models, I suggest looking at  megatron . \n Setup \n Make sure you\'re installing/running on a CUDA supported machine. \n To improve performance, we use a few fused kernel layers from Apex (if you\'re unsure what fused kernels are for, I highly suggest  this  blogpost). \n git clone https://github.com/NVIDIA/apex\ncd apex\npip install -v --disable-pip-version-check --no-cache-dir --global-option="--cpp_ext" --global-option="--cuda_ext" ./ \n Install the rest of the requirements: \n pip install -r requirements.txt \n Train \n To train a 1.5B parameter model based on the Megatron architecture sizes using 8 GPUs (model will not fit on 1 GPU with optimal throughput, we scale to multiple). \n deepspeed --num_gpus 8 train.py --batch_size_per_gpu 16   \n References \n Code:  \n \n minGPT  - A lot of the base code was borrowed and extended from this awesome library \n microGPT  - A helpful example with xFormers \n Megatron-DeepSpeed  - Learning the use of Deepspeed with the Megatron architecture/3d parallelism. \n \n Papers: \n \n Efficient Large-Scale Language Model Training on GPU Clusters\nUsing Megatron-LM \n Training Compute-Optimal Large Language Models \n What Language Model to Train if You Have One Million GPU Hours? \n']
alykhantejani,['Fraction \n An Fraction representation for Java to avoid floating point errors over multiple operations by representing numbers as fractions. \n Example: \n Normally you would do this:\n    double a = 1.2;\n    double b = 2.4;\n    double c = a + b;\n    System.out.println(c);\nand you would get this output:\n    3.5999999999999996 \n Using fractions you can do this:\n    Fraction a = new Fraction(6, 5);\n    Fraction b = new Fraction(12, 5);\n    Fraction c = Fraction.add(a, b);\n    System.out.println(c.toDouble());\nand you will get:\n    3.6 \n Build Instructions: \n Fraction is built with Gradle, if you havent got this installed you can grab the latest copy from: http://www.gradle.org/downloads.html \n To build simply type: gradle build\nOr if you use eclipse, to set up an eclipse project type: gradle eclipse', 'SemanticTextonForest \n Semantic Texton Forest Implementation (Shotton et al.)', 'Minimal Mistakes \n Minimal Mistakes  is a two column responsive Jekyll theme perfect for powering your GitHub hosted blog.  \n Minimal Mistakes is all about: \n \n Responsive templates. Looking good on mobile, tablet, and desktop. \n Gracefully degrading in older browsers. Compatible with Internet Explorer 8+ and all modern browsers.  \n Minimal embellishments -- content first. \n Optional large feature images for posts and pages. \n Simple and clear permalink structure. \n Custom 404 page  to get you started. \n Support for Disqus Comments \n \n \n See a  live version of Minimal Mistakes  hosted on GitHub. \n Getting Started \n Minimal Mistakes takes advantage of Sass and data files to make customizing easier. These features require Jekyll 2.x and will not work with older versions of Jekyll. \n To learn how to install and use this theme check out the  Setup Guide  for more information.', 'siamese_network \n A Siamese network implementation in torch (simple example on MNIST to embed to 2D space)', "tweetbot \n A bot to generate 140 character tweets (and possibly tweet them) \n This will use a Recurrent Neural Network (RNN) based of  Andreij Karpathy's awesome char-rnn . \n More details to follow...", 'generate_function_samples \n A python script to generate samples from a given function. For example, for the function (The Maclaurin expansion  sin(x) ), \n y = (2.5*x - (2.5*x^3)/6 + (2.5*x^5)/120)  \nWe can plot 15 samples using this equation, with added jitter of  +/- 0.5 . We can also plot the function across the whole range without jitter on the same plot with the command: \n ```\nython generate_function_samples.py --function \'(2.5 x - (2.5 x 3)/6 + (2.5*x 5)/120)\' --jitter 0.5 --num_samples 36 --plot_out_file example_output.png --variable_ranges \'{"x" : [-2.5, 2.5] }\' --draw_true_function \n ```\nwhich will produce the following plot:\n \n We can also plot 3D samples. For example for the function \n y = sin((x^2))/2 - (x1^2)/4 + 3)cos(2x + 1 - e^x1) \nwe can plot 36 samples using the command below:\n python generate_function_samples.py --function \'sin(0.5*x**2 - 0.25*x1**2 + 3) * cos(2*x + 1 - exp(x1))\' --jitter 0.1 --num_samples 36 --draw_true_function --plot_out_file example_output_3d.png --variable_ranges \'{"x": [-1.5, 1.5], "x1": [-1.5, 1.5]}\' --display \nwhich will produce the following plot:\n', 'gradient_descent_blog \n Code for the blog post http://alykhantejani.github.io//backpropgartion-part-1 \n To produce the gradient descent graphs for example 1 and 2 please run the following code: \n python example1.py && python example2.py', 'nninit \n Weight initialization schemes for PyTorch nn.Modules. This is a port of the popular  nninit  for  Torch7  by  @kaixhin . \n Update \n This repo has been merged into  PyTorch\'s nn module , I recommend you use that version going forward. \n PyTorch Example \n ```python\nimport nninit\nfrom torch import nn\nimport torch.nn.init as init\nimport numpy as np \n class Net(nn.Module):\n  def  init (self):\n     super(Net, self). init ()\n     self.conv1 = nn.Conv2d(5, 10, (3, 3))\n     init.xavier_uniform(self.conv1.weight, gain=np.sqrt(2))\n     init.constant(self.conv1.bias, 0.1) \n network = Net()\n``` \n Installation \n Clone the repo and run  python setup install \n Usage \n ```python\nimport nninit\nfrom torch import nn\nimport numpy as np \n class Net(nn.Module):\n  def  init (self):\n     super(Net, self). init ()\n     self.conv1 = nn.Conv2d(5, 10, (3, 3))\n     nninit.xavier_uniform(self.conv1.weight, gain=np.sqrt(2))\n     nninit.constant(self.conv1.bias, 0.1) \n network = Net()\n``` \n Supported Schemes \n \n nninit.uniform(tensor, a=0, b=1)  - Fills  tensor  with values from a uniform, U(a,b) \n nninit.normal(tensor, mean=0, std=1)  - Fills  tensor  with values drawn from a normal distribution with the given mean and std \n nninit.constant(tensor, val)  - Fills  tensor  with the constant  val \n nninit.xavier_uniform(tensor, gain=1)  - Fills  tensor  with values according to the method described in  "Understanding the difficulty of training deep feedforward neural networks" - Glorot, X. and Bengio, Y. , using a uniform distribution. \n nninit.xavier_normal(tensor, gain=1)  - Fills  tensor  with values according to the method described in  "Understanding the difficulty of training deep feedforward neural networks" - Glorot, X. and Bengio, Y. , using a normal distribution. \n nninit.kaiming_uniform(tensor, gain=1)  - Fills  tensor  with values according to the method described in  "Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification" - He, K. et al.  using a uniform distribution. \n nninit.kaiming_normal(tensor, gain=1)  - Fills  tensor  with values according to the method described in  "Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification" - He, K. et al.  using a normal distribution. \n nninit.orthogonal(tensor, gain=1)  - Fills the  tensor  with a (semi) orthogonal matrix. Reference:  "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks" - Saxe, A. et al. \n nninit.sparse(tensor, sparsity, std=0.01)  - Fills the 2D  tensor  as a sparse matrix, where the non-zero elements will be drawn from a normal distribution with mean=0 and std= std . \n']
StephanZheng,['Detecting Adversarial Examples via Neural Fingerprinting \n \n This is code that implements  Neural Fingerprinting , a technique to detect adversarial examples. \n This accompanies the paper  Detecting Adversarial Examples via Neural Fingerprinting ,  Sumanth Dathathri(*), Stephan Zheng(*), Richard Murray and Yisong Yue, 2018  (* = equal contribution), which can be found here: \n https://arxiv.org/abs/1803.03870 \n If you use this code or work, please cite: \n bibtex\n@inproceedings{dathathri_zheng_2018_neural_fingerprinting,\n  title  = {Detecting Adversarial Examples via Neural Fingerprinting},\n  author={Dathathri, Sumanth and Zheng, Stephan and Murray, Richard and Yue, Yisong},\n  year   = {2018}\n  eprint = {1803.03870}\n  ee     = {https://arxiv.org/abs/1803.03870}\n} \n To clone the repository, run: \n git clone https://github.com/StephanZheng/neural-fingerprinting\ncd neural-fingerprinting \n Results \n Neural Fingerprinting achieves near-perfect detection rates on MNIST, CIFAR and MiniImageNet-20. \n \n \nROC curves for detection of different attacks on CIFAR. \n Requirements and Installation \n We have tested this codebase with the following dependencies (we cannot guarantee compatibility with other versions). \n \n PyTorch >= 0.2\n(torch (0.2.0.post3)\ntorchvision (0.1.9)) \n Tensorflow >=1.4.1\n(tensorflow (1.4.1)) \n Keras 2.0.8 \n https://github.com/gzuidhof/nn-transfer : to transfer models from Tensorflow to PyTorch. \n scikit-learn \n \n To install these dependencies, run: \n ``` \n PyTorch: find detailed instructions on  http://pytorch.org/ \n pip install torch\npip install torchvision \n TF: find detailed instructions on  http://tensorflow.org/ \n pip install keras\npip install tensorflow-gpu \n nn_transfer \n git clone https://github.com/gzuidhof/nn-transfer\ncd nn-transfer\npip install . \n pip install sklearn\n``` \n This codebase relies on third-party implementations for adversarial attacks and code to transfer generated attacks from Tensorflow to PyTorch. \n \n Local Intrinsic Dimensionality for Adversarial Subspace Detection  https://github.com/xingjunm/lid_adversarial_subspace_detection : a library to generate all adversarial attacks. \n Cleverhans:  https://github.com/tensorflow/cleverhans : a library to generate gradient-based attacks, called by the LID code. This codebase has been included in the  third_party  folder. \n https://github.com/rwightman/pytorch-nips2017-attack-example : code to generate iterative fast-gradient attacks on ImageNet examples. \n \n Quick-start \n To train and evaluate models with fingerprints, use the launcher script  run.sh , which contains example calls to run the code. \n The flags that can be set for the launcher are: \n ./run.sh dataset train attack eval grid num_dx eps epoch_for_eval \n where \n \n dataset: \'mnist\', \'cifar\' or \'miniimagenet\' \n train: \'train\' or \'notrain\' -- do training or not \n attack: \'train\' or \'notrain\' -- create adversarial examples or not \n eval: \'eval\' or \'noeval\' -- do evaluation or not \n grid: \'grid\' or \'nogrid\' -- enables a grid search for hyperparameter tuning. \n num_dx: number of fingerprint directions \n eps: standard deviation of randomly sampled fingerprint directions \n epoch_for_eval: which model epoch to use for evaluation \n \n For instance, the following command trains a convolutional neural network for MNIST with 10 fingerprints with epsilon = 0.1, and evaluates the model after 10 epochs of training: \n ./run.sh mnist train attack eval nogrid 10 0.1 10 \n Running training, attacks and evaluation \n \n To train a model with fingerprints: \n \n ```bash\nNAME=mnist \n LOGDIR=/tmp/nfp/$NAME/log\nDATADIR=/tmp/nfp/$NAME/data\nmkdir -p $LOGDIR\nmkdir -p $DATADIR \n NUMDX=10\nEPS=0.1\nNUM_EPOCHS=10 \n python $NAME/train_fingerprint.py \\\n--batch-size 128 \\\n--test-batch-size 128 \\\n--epochs $NUM_EPOCHS \\\n--lr 0.01 \\\n--momentum 0.9 \\\n--seed 0 \\\n--log-interval 10 \\\n--log-dir $LOGDIR \\\n--data-dir $DATADIR \\\n--eps=$EPS \\\n--num-dx=$NUMDX \\\n--num-class=10 \\\n--name=$NAME\n``` \n \n Creating adversarial attacks for the model after 10 epochs of training: \n \n ADV_EX_DIR=/tmp/nfp/$NAME/attacks\nEPOCH=10\npython $NAME/gen_whitebox_adv.py \\\n--attack "all" \\\n--ckpt $LOGDIR/ckpt/state_dict-ep_$EPOCH.pth \\\n--log-dir $ADV_EX_DIR \\\n--batch-size 128 \n \n Evaluating model \n \n ```\nEVAL_LOGDIR=$LOGDIR/eval/epoch_$EPOCH\nmkdir -p $EVAL_LOGDIR \n python $NAME/eval_fingerprint.py \\\n--batch-size 128 \\\n--epochs 100 \\\n--lr 0.001 \\\n--momentum 0.9 \\\n--seed 0 \\\n--log-interval 10 \\\n--ckpt $LOGDIR/ckpt/state_dict-ep_$EPOCH.pth \\\n--log-dir $EVAL_LOGDIR \\\n--fingerprint-dir $LOGDIR \\\n--adv-ex-dir $ADV_EX_DIR \\\n--data-dir $DATADIR \\\n--eps=$eps \\\n--num-dx=$numdx \\\n--num-class=10 \\\n--name=$NAME\n```']
satwikkottur,['f14-cv-project \n Fall 2014 Course project for Computer Vision course', 'MovieRecommend \n Course Project for CMU 10-701/15-781 - Machine Learning \n A movie recommender system based on Collaborative Filtering and Topic Modeling (LDA) \n External Dependencies \n \n Apache Commons Math 3.3 \n la4j library for linear algebra \n nltk for python 2.7 \n', 'FluidSim \n Fluid simulation - Water and Fire interaction', "StochasticMCMC \n MCMC for posterior distribution sampling \n Satwik Kottur  and  Krishna Pillutla , Carnegie Mellon University \n This project is a part of  10-708: Probabilistic Graphical Models , Fall 2015, in\nrequirement for the course completion. \n The idea is to handle non-smooth energy functions in the setting of Hamiltonian dynamics for Monte Carlo Markov chain (MCMCM) sampling. Hamiltonian Monte Carlo (HMC) methods evolve a set of differential equations and non-smooth energies do not fit in, as in. The  report  provides further details on the various strategies adopted to solve the problem. \n Parts of the code are adapted from two sources:\n 1.  Hamiltonian Monte Carlo MATLAB Implementation \n 2.  Tianqi Chen's Implementation for Stochastic Gradient-HMC", 'Visual Word2Vec (vis-w2v) \n Learning visually grounded word embeddings from abstract image   \n \n Paper \n Satwik Kottur*, Ramakrishna Vedantam*, José Moura, Devi Parikh \n Visual Word2Vec (vis-w2v): Learning Visually grounded embeddings from abstract images \n[ ArXiv ] [ Project Page ] \n* = equal contribution \n Pre-trained Visual Word2vec Embeddings can be found  here ,\ntrained on wikipedia and MSCOCO captions. \n \n Code Structure \n The code is organized as follows: \n \n visword2vec.c  : Main code for training vis-w2v \n refineFunctions.c  : Contains functions related to refining embeddings \n helperFunctions.c  : Mostly contain assisting functions like io, tokenization, etc. \n visualFunction.c  : Contains code to refine based on tuples and also perform common sense (cs) task on test and validation sets \n vpFunctions.c  : Contains code to refine based on sentences and also perform visual paraphrasing (vp) task \n structs.h  : Contains structures defined for the code \n filepaths.h  : Contains the paths to the files needed for the above two tasks-cs,vp \n Makefile  : Helps to setup, compile and run programs \n \n Other files can be ignored for now.   \n Program accepts following as inline arguments:   \n \n embed-path  : Initialization for the embeddings (pre-trained using word2vec usually) \n  Format: Header should have  vocabsize   dimensions \n  Each following row should first have the word, and embeddings delimited by space   \n output  : Path to where to store the output embeddings \n size  : Size of the hidden layer (should match with the pre-loaded embeddings) \n threads  : Number of threads to use for refining, loading and other operations \n \n Currently only saving one embedding is supported. For multi embeddings simply turn\nthe  trainMulti  flag (top of  visword2vec.c  file) to 1. However, saving can be done by\nuncommenting code in  trainModel() . \n Steps for usage \n \n \n Makefile \n \n Liblinear-2.1 must be compiled and the path must be correctly set \n yael must be setup (for k means) and corresponding paths setup\nLink here:  yael \n cs and vp options should have correctly  -embed-path  options \n \n \n \n filepaths.h : \n \n Make sure all the paths are accessible and correctly set \n Any change to this file, should be followed by re-compiling the code \n \n \n \n liblinearWrapper.h : \n \n Additionally, you also need to link the correct path to liblibear \n \n \n \n Other dependencies: \n \n NTLK is used for tokenization and lemmatization (VP task) \n \n \n \n To run either cs or vp, comment or uncomment corresponding wrapper calls in \n trainModel()  function of visword2vec. And then  make cs  or  make vp  for the\ntwo tasks correspondingly to compile and run.  make  simply compiles while \n make clean  cleans up all the binaries.   \n NOTE : All the binaries are stored in  bin/  folder (might have to create one if \ndoesnt exist beforehand).   \n \n Tasks \n In this paper, we deal with three tasks: Common Sense Assertion Classification, Visual Paraphrasing and Text-based Image Retrieval. \n A. Common Sense Assertion Classification  ( Project page ) \nDownload the dataset from their project page  here .\nCode to process this dataset further is given in  utils/cs/ .\nThe following are the pre-processing steps: \n \n Extract the training (P, R, S) tuples \n Extract the visual features for clustering \n Extract the test and val (P, R, S) tuples \n Extract the  word2vec  embeddings to initialize from (you can alternatively use any other embeddings to begin with, we recommend you use pre-trained embeddings to reproduce results from the paper). \n \n All the above four steps can be done by simply running:\n cd utils/cs/\npython extractData.py <path to downloaded data> <path to save the data>(optional) \nBy default it created a folder  data/cs  and saves the files in this folder. This will produce files  word2vec_cs.bin ,  PRS_train.txt ,  PRS_test.txt ,  PRS_val.txt  and  visual_train.txt  at destination folder corresponding to above files. Once these files are produced, open  filepath.h  and make sure the macros point to right file paths.\n``` \n define ROOT_CS "data/cs/" \n define CS_VISUAL_FEATURE_FILE ROOT_CS  "visual_train.txt" \n define CS_PRS_TRAIN_FILE ROOT_CS "PRS_train.txt" \n define CS_PRS_TEST_FILE ROOT_CS "PRS_test.txt" \n define CS_PRS_VAL_FILE ROOT_CS "PRS_val.txt" \n Now, to run, simply: \nmake\n./visword2vec -cs 1 -embed-path data/cs/word2vec_cs.bin -output cs_refined.bin -size 200 -clusters 25\n```\nYou can also give in other parameters to suit your needs. \n \n B. Visual Paraphrasing  ( Project page ) \nDownload the VP dataset from their project page  here .\nAlso download the clipart scenes and descriptions (ASD) used to train  vis-w2v  from the  clipart  project page  here . \n All the scripts needed for pre-processing are available in  utils/vp  folder.  \n Follow the steps below:\n Training data \n Step 1:  Run the  fetchVPTrainData.m  function to extract relevant data for training  vis-w2v .\n```\ncd utils/vp \n \n \n fetchVPTrainData( ,  ,  ); \n \n \n For example: \n \n \n fetchVPTrainData(\'data/vp/AbstractScenes_v1.1\', \'data/vp/imagine_v1/\', \'data/vp/\');\n``` \n \n \n It does the following (not important. If you just want desired data, run the above command):\n  * Extracting visual features  abstract_features.txt  from Abstract Scene Dataset (ASD) using MATLAB script.\n  ``` \n \n \n cd utils/vp\nextractAbstractFeatures( ,  ) \n \n \n For example:  \n \n \n extractAbstractFeatures(\'data/vp/AbstractScenes_v1.1\', \'data/vp/\')\n   * The alignment between ASD and VP datasets is given in two files `SceneMap.txt` and `SceneMapV1_10020.txt` present in `utils/vp/`. We will use them along with train/test split of VP and select features from training sentences only, again using MATLAB. \n  cd utils/vp\nalignAbstractFeatures( ,  ,  ) \n \n \n For example: \n \n \n alignAbstractFeatures(\'data/vp/imagine_v1/\', \'data/vp/\', \'data/vp/\')\n   * Get the training sentences from VP dataset (for learning `vis-w2v`). This would produce `vp_train_sentences_raw.txt`. \n  cd utils/vp\nsaveVPTrainSentences( ,  ) \n \n \n For example: \n \n \n saveVPTrainSentences(\'data/vp/imagine_v1/\', \'data/vp\')\n  ``` \n \n \n Task data \n Step 2:  One should use our new embeddings  vis-w2v  in place of  word2vec  in visual paraphrasing task ( imagine_v1/code/feature/compute_features_vp.m  at line 32). Alternatively, we can save their other text features (co-occurance and total frequency) and use it in our code for speed and smoother interface between learning embeddings and performing the task. \nThis can be achieved by adding the following lines to  imagine_v1/code/feature/compute_features_vp.m  before line 30, and running  imagine_v1/code/script_vp.m . \n save(\'vp_txt_features.mat\', \'feat_vp_text_tf_1\', \'feat_vp_text_tf_2\', \'feat_vp_text_coc_1\', \'feat_vp_text_coc_2\');\n% Escape from running remainder code\nerror(\'Saved features, getting out!\') \n Step 3 : Next, we obtain all the relevant information to perform the visual paraphrasing task (using MATLAB)\n* Sentence pairs:  vp_sentences_1.txt  and  vp_sentences_2.txt \n* Other textual features:  vp_features_coc_l.txt ,  vp_features_coc_2.txt ,  vp_features_tf_l.txt ,  vp_features_tf_2.txt \n* Ground truth:  vp_ground_truth.txt \n* Train / test split:  vp_split.txt \n* Train / val split:  vp_val_inds_1k.txt \n ```\ncd /utils/vp \n \n \n fetchVPTaskData( ,  ,  ) \n \n \n For example: \n \n \n fetchVPTaskData(\'data/vp/imagine_v1/\', \'data/vp/imagine_v1/code/\', \'data/vp\')\n``` \n \n \n Step 4 : Finally, we tokenize and lemmatize all the sentences, i.e,  vp_sentences_1.txt ,  vp_sentences_2.txt  and  vp_train_sentences_raw.txt . \n ```\ncd /utils/vp\npython lemmatizeVPTrain.py    \n For example:\npython lemmatizeVPTrain.py data/vp/ data/vp/\n ``\nPhew! That\'s a lot of pre-processing. Now we are all set to learn vis-w2v embeddings while performing the visual paraphrasing task.\nLike before, check all the filepaths in filepaths.h` before proceeding. \n Now, to run, simply:\n make\n./visword2vec -vp 1 -embed-path data/vp/word2vec_vp.bin -output vp_refined.bin -size 200 -clusters 100 \nYou can also give in other parameters to suit your needs. \nFor VP, these are  mode  that indicates the training context and  window-size   that indicates the size in  WINDOW  mode.\n  *  DESCRIPTIONS : Use all the three sentences for training\n  *  SENTENCES : Use sentences one after the other\n  *  WINDOW : Use a context window of size  window-size  (default 5)\n  *  WORDS : Use each word separately \n The program prints 100 runs with both validation and test performance. We choose the run with best validation performance and report the corresponding test result. \n \n C. Text-based Image Retrieval \nThis task involves retrieving the appropriate image based on the associated tuple. We collect the data and make it available at  utils/text-ret/text_ret_tuples.pickle  along with ground truths for each image as  utils/text-ret/text_ret_gt.txt . The goal is to retrieve correct image from the list of ground truth tuples using each of collected queries in pickle file as query. \n The code for this task is provided in Python in  utils/text_ret/  along with the data. To run, we need to point it to the data directory along with embedding paths. There are two modes for this task: (A) SINGLE - this uses single embedding for P, R, S. (B) MULTI - this uses three different embeddings for each of P, R, S. The inputs are given accordingly. \n ```\ncd utils/text-ret/\npython performRetrieval.py    \n  (or)\npython performRetrieval.py        \n For example:\npython performRetrieval.py ./ cs_refined.bin\n``` \n For any further information/questions, feel free to email the authors at  skottur@andrew.cmu.edu  or  vrama91@vt.edu', "Minimal Mistakes \n Minimal Mistakes  is a two column responsive Jekyll theme perfect for powering your GitHub hosted blog. \n Jekyll 3 Update:  A version of Minimal Mistakes compatible with Jekyll 3 can be found in the  jekyll3  branch . GitHub Pages is  locked at version 2.4  so some keep that in mind if you're trying to use 3.0 features that aren't supported there yet. \n Minimal Mistakes is all about: \n \n Responsive templates. Looking good on mobile, tablet, and desktop. \n Gracefully degrading in older browsers. Compatible with Internet Explorer 8+ and all modern browsers. \n Minimal embellishments -- content first. \n Optional large feature images for posts and pages. \n Simple and clear permalink structure. \n Custom 404 page  to get you started. \n Support for Disqus Comments \n \n \n See a  live version of Minimal Mistakes  hosted on GitHub. \n Getting Started \n Minimal Mistakes takes advantage of Sass and data files to make customizing easier. These features require Jekyll 2.x and will not work with older versions of Jekyll. \n To learn how to install and use this theme check out the  Setup Guide  for more information.", 'CLEVR-Dialog \n This repository contains code for the paper: \n CLEVR-Dialog: A Diagnostic Dataset for Multi-Round Reasoning in Visual Dialog \n Satwik Kottur ,  José M. F. Moura ,  Devi Parikh ,  Dhruv Batra ,  Marcus Rohrbach \n[[PDF][7]] [[ArXiv][1]] [ Code ] \n Oral Presentation   \n Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2019 \n If you find this code useful, consider citing our work: \n @inproceedings{Kottur2019CLEVRDialog,\n    title  = {CLEVR-Dialog: A Diagnostic Dataset for Multi-Round Reasoning in Visual Dialog},  \n    author = {Kottur, Satwik and Moura, Jos\\\'e M. F. and Parikh, Devi and   \n              Batra, Dhruv and Rohrbach, Marcus},  \n    journal = {arXiv preprint arXiv:1903.03166},\n    year   = {2019}  \n} \n Abstract \n Visual Dialog is a multimodal task of answering a sequence of questions \ngrounded in an image, using the conversation history as context.\nIt entails challenges in vision, language, reasoning, and grounding.\nHowever, studying these subtasks in isolation on large, real datasets is \ninfeasible as it requires prohibitively-expensive complete annotation of the \n\'state\' of all images and dialogs.  \n We develop CLEVR-Dialog, a large diagnostic dataset for studying multi-round \nreasoning in visual dialog.\nSpecifically, we construct a dialog grammar that is grounded in the scene \ngraphs of the images from the CLEVR dataset.\nThis combination results in a dataset where all aspects of the visual dialog \nare fully annotated.\nIn total, CLEVR-Dialog contains 5 instances of 10-round dialogs for about 85k \nCLEVR images, totaling to 4.25M question-answer pairs.  \n We use CLEVR-Dialog to benchmark performance of standard visual dialog models;\nin particular, on visual coreference resolution (as a function of the \ncoreference distance).\nThis is the first analysis of its kind for visual dialog models that was not \npossible without this dataset.\nWe hope the findings from CLEVR-Dialog will help inform the development of \nfuture models for visual dialog. \n \nThis repository generates a version of our diagnostic dataset  CLEVR-Dialog \n(figure above). \n Setup \n The code is in Python3 with following python package dependencies: \n bash\npip install absl-py\npip install json\npip install tqdm\npip install numpy \n Directory Structure \n The repository contains the following files: \n \n generate_dataset.py : Main script to generate the dataset \n constraints.py : List of constraints for caption and question generation \n clevr_utils.py : Utility functions to dialog generation \n global_vars.py : List of global variables along with initialization \n \n In addition, the dataset generation code requires following files: \n \n templates/synonyms.json : Compilation of words and their synonyms \n templates/metainfo.json : Contains information about attributes and their values for CLEVR objects \n templates/captions  and  templates/questions : Caption and question templates respectively. \n \n CLEVR Images \n Our dataset is built on  CLEVR  images, which can be downloaded from  here .\nExtract the images and scene JSON files in  data/  folder.\nWe will only use CLEVR  train  and  val  splits as scene JSON files are unavailable for  test  split. \n Generating CLEVR-Dialog Dataset \n To generate the dataset, please check  run_me.sh .\nAdditional details about the supported flags can be found in  generate_dataset.py .\nAn example command is shown below: \n bash\nDATA_ROOT=\'data/CLEVR_v1.0/\'\npython -u generate_dataset.py \\\n    --scene_path=${DATA_ROOT}"scenes/CLEVR_train_scenes.json" \\\n    --num_beams=100 \\\n    --num_workers=1 \\\n    --save_path=${DATA_ROOT}"clevr_dialog_train_raw.json" \\\n    --num_images=10 \n CLEVR-Dialog Annotations \n The generated JSON contains a list of dialogs on CLEVR images with following fields:   \n \n split : Specifies if the CLEVR split is train/val.  \n image_index : CLEVR image index.  \n image_filename : CLEVR image filename.  \n dialogs : List of dialog instances for a given image, each with following fields: \n     \xa0\xa0 |-- caption : Caption for the dialog instance \n     \xa0\xa0 |-- template_info : Template information for the dialog (caption + 10 questions) \n     \xa0\xa0 |-- dialog : Text for the ten rounds of dialog, each with following fields: \n     \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n     |-- question : Question text for the current round \n     \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n     |-- answer : Answer text for the current round \n     \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n     |-- template : Question template for the current round \n     \xa0\xa0 |-- graph : Scene graph information for the dialog, with following fields: \n     \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n     |-- objects : Objects with attributes discussed in the dialog \n     \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n     |-- counts : Specific object counts discussed in the dialog \n     \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n     |-- relationships : Object relationships discussed in the dialog \n     \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n     |-- exists : Object existences discussed in the dialog \n     \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n     |-- history : List of incremental scene graph information conveyed in each round    \n \n The dataset used in the paper can be downloaded here:  train  and  val  splits. \n Contributors \n \n Satwik Kottur \n \n For any questions, please feel free to contact the above contributor(s). \n License \n This project is licensed under the license found in the LICENSE file in the\nroot directory of this source tree ( here ).']
JimmyWhitaker,['NerualNet \n Personal Experiment of writing a neural network package (inefficient) from scratch in Java. It was used to understand some of the concepts behind Neural Nets like backprop and drop-connect. It is focused mainly on the XOR and MNIST toy problems.', 'LeNet5-MNIST-Example \n This contains an iPython Notebook that trains and tests a LeNet5 Convolutional Neural Network on the MNIST dataset.  \n Dependencies \n Python 2.7 ,\n numpy/scipy ,\n Theano \n The code has been adapted from its original form to allow parameter tuning. The model can take some time to train depending on the parameters given, and what version of Theano is installed. ', 'Pach Zero Shot \n Scaling Zero Shot learning from Hugging Face to production with Pachyderm.  \n Hugging Face came out with  this , and I wanted to scale it with  Pachyderm . \n Running with Docker \n docker run -v `pwd`/data/:/data/ --entrypoint=python3 jimmywhitaker/zero-shot:v0.1 zs_predict.py --sequences /data/input/test_input.txt --labels /data/labels/test_labels.txt --output /data/output/ \n Running with Pachyderm \n Start a Pachyderm cluster with  Pachyderm Hub . \n Run: \n make zero-shot-base \n The code \n ```\n$ python zs_predict.py --help\nusage: zs_predict.py [-h] [--sequences DIR] [--labels DIR] [--output DIR] \n Zero Shot Predictor \n optional arguments:\n  -h, --help       show this help message and exit\n  --sequences DIR  input sequences to be classified\n  --labels DIR     labels to be applied to sequences\n  --output DIR     output directory for predictions\n```', 'Label Studio with Pachyderm \n   Moving : Project in the process of moving to  Pachyderm examples  to allow for better support.  \n \n \n \n Label Studio  supports many different types of data labeling tasks, while  Pachyderm  allows you to incorporate data versioning and data-driven pipelines. Integrating both open source components is a useful way to manage the labeling component of the  data loop . This integration connects a Pachyderm versioned data backend with Label Studio to support versioning datasets and tracking the data lineage of pipelines built off the versioned datasets. \n How it works \n Label Studio can utilize an s3 backend, reading data from an S3 bucket and writing labels to an output S3 location. Pachyderm has an S3 compliant gateway that allows reading data from its file system and writing data to its filesystem (organizing it with commits that can start pipelines). \n We\'ll create a text labeling example by: \n \n Start a Label Studio instance that uses Pachyderm as its backend \n Push data to Pachyderm that automatically populates Label Studio \n Label the data in Label Studio \n Version our dataset in Pachyderm \n \n Note: Label studio currently doesn\'t support arbitrary s3 storage (only AWS s3 and GCS), so I modified Label Studio\'s s3 storage backend to support generic object storage endpoints, which allows us to connect to the Pachyderm s3 gateway running locally. you can see the code  here \n Getting Started \n This example uses a Pachyderm deployment for scaling and management. We can deploy a cluster on  Pacyderm Hub  for free or deploy locally as described here:  Pachyderm Getting Started \n Once everything is up, we can check the setup by running: \n1.  kubectl get all  to ensure all the pods are up and ready. \n2.  pachctl version  which will show both the  pachctl  and  pachd  versions. \n Configuring .env file \n The  .env  file needs to be configured for your Pachyderm s3 gateway. Pachyderm\'s s3 gateway is accessed through an  http  endpoint that is available on port  30600  on the Pachyderm cluster. This address is used to as the  ENDPOINT_URL  for the Label Studio backend in the  .env  file.  \n Pachyderm Hub \n If you are running your cluster on Pachyderm Hub, you can find out your  ENDPOINT_URL  by clicking the  Connect  button. You should see an address that looks something like:  \n grpcs://hub-xx-xxxYYxxYY.clusters.pachyderm.io:31400 \n Just change the protocol to  http  and port to  30600 . This will now point at the S3 gateway.  \n https://hub-xx-xxxYYxxYY.clusters.pachyderm.io:30600 \n The  AWS_ACCESS_KEY_ID  and  AWS_SECRET_ACCESS_KEY  in your  .env  file should be set to your Pachyderm  session_token  located in your Pachyderm config (typically in  ~/.pachyderm/config.json ). More info on Pachyderm\'s  S3 gateway .  \n If you get the following error,  \n botocore.exceptions.ClientError: An error occurred (403) when calling the HeadBucket operation: Forbidden \n this is typically due to an expired session token. Reconnect to the cluster and update your  .env  with the new token.  \n Minikube configuration \n If you are running Pachyderm locally on minikube, you can get the  ENDPOINT_URL  for the Pachyderm s3 gateway by running the command: \n $ minikube ip\n192.168.64.8 \n If you are running Pachyderm with authentication, then you can follow the same steps as the Hub setup. If not running with authentication, you can pass any non-empty string to  AWS_ACCESS_KEY_ID  and  AWS_SECRET_ACCESS_KEY  in your  .env  file. \n  ## Creating a new project\nA new project requires creating a new configuration (see some of the [examples](examples/)). Creating a new project with Label Studio can be done by from the command line. We\'ll use the Docker image that we created to do this, adding the `--init` flag which will create the project. \n\n```shell\ndocker run --env-file .env -v $(pwd)/examples/my_new_project:/my_new_project -p 8080:8080 --entrypoint=label-studio jimmywhitaker/label-studio:latest start /my_new_project/ --source s3 --source-path master.raw_data --target s3-completions --target-path master.labeled_data --input-format=image --template image_bbox --source-params "{\\"use_blob_urls\\": false, \\"regex\\": \\".*\\"}"\n\n```  \n Running the Text Labeling Example \n ``` bash \n Pachyderm Setup \n pachctl create repo raw_data\npachctl create repo labeled_data\npachctl create branch labeled_data@master\npachctl create branch raw_data@master \n Start a local instance of Label Studio (needs the .env for the Pach s3 gateway) \n docker run --env-file .env -v $(pwd)/examples/my_text_project:/my_text_project -p 8080:8080 jimmywhitaker/label-studio:latest \n Navigate to http://localhost:8080/tasks \n Upload data \n pachctl put file raw_data@master:/test-example.json -f raw_data/test-example.json --split json --target-file-datums 1 \n Modify the data before it\'s labeled \n pachctl put file raw_data@master:/test-example.json -f raw_data/test-example2.json --split json --target-file-datums 1 --overwrite \n Label data (2 examples) in the UI \n Version your dataset (v1) \n pachctl list branch labeled_data\npachctl create branch labeled_data@v1 --head master\npachctl list branch labeled_data \n Label more data in the UI \n Version your dataset (v2) \n pachctl list branch labeled_data\npachctl create branch labeled_data@v2 --head master \n Download dataset for v1 locally \n pachctl get file -r labeled_data@v1:/ -o labeled_data/ \n ``` \n Next Steps \n \n The output does have a reference for what the input file location was (could potentially be used to track consistency between raw and labeled if raw changes). \n Make deployment super easy \n Build a helm chart to deploy label studio to Kubernetes with necessary env  \n Standardize label studio project creation - different examples of configs \n Ability to update  input raw data  - currently if it\'s labeled, then it\'s captured in the source and target json files.  \n Rectify the source and target files to have provenance for the labeling \n \n Known Issues and Gotchas \n \n One example per source file  \n Must be json files or figure out how to get s3 signed urls to frontend.  \n When file is updated after labeled, it\'s not re-loaded (not sure what should happen here - should it be removed from the labeled data repo when the raw data is removed?) \n When raw data is changed after that example is labeled, the task doesn\'t update. It does update when  \n It seems as though the target and the source states are tied somehow, so it won\'t automatically update \n If a raw file is removed or changed, then labels associated with that file should be removed. Since it\'s a single file per example, a changed file should be the deleting of one and addition of another. For now, this would need to be an external process that  \n Label Studio automatically tries to start an image labeling config and if there is labeled data, this will throw errors until you load a compatible config for what\'s already labeled (i.e. you should not use the  --init  and  --force  flags after you\'ve created the project). \n', 'Market Sentiment Example \n \n \n \n The purpose of this example is to illustrate how the two, independent yet symbiotic loops in the  machine learning loop  can work together practically. The code loop is managed by git+unittest+GitHub Actions, while the data loop and the data+code interactiona are managed by Pachyderm.  \n The original technique is a  sentiment analysis classifier  that uses the  Financial Phrase Bank Dataset . In this example, we use a reduced version of this blog post and dataset for simplicity and transparency to show the interactions more than the techniques themselves.  \n Running the example \n The easiest way to run this example is to use the  Makefile  once the cluster is configured (steps below). \n Setup Pachyderm, S3 gateway, and Label Studio \n \n Start a pachyderm cluster - get the endpoint address (if using hub, just look at the address where the dash is being served or if minikube, run  minikube ip ) \n Create a pachyderm token (details)\n bash\npachctl auth get-auth-token --ttl "624h" | grep Token | awk \'{print $2}\' \n Create a  .env  file inserting the endpoint address and token. \n ENDPOINT_URL=https://<pachyderm_endpoint_address>:30600\nAWS_REGION=us-east-1\nAWS_ACCESS_KEY_ID=<pachyderm_token>\nAWS_SECRET_ACCESS_KEY=<pachyderm_token> \n Run label studio with the following commands (you won\'t see any tasks until you add data):\n```bash\npachctl create repo raw_data\npachctl create repo labeled_data\npachctl create branch labeled_data@master\npachctl create branch raw_data@master \n \n Start a local instance of Label Studio (needs the configured .env for the Pach s3 gateway) \n docker run -it --env-file .env -v $(pwd)/label-studio-project:/my_text_project -p 8080:8080 jimmywhitaker/label-studio:pach-ls0.9 \n Navigate to http://localhost:8080/tasks \n ``` \n Setup GitHub Action \n Details on the  Pachyderm GitHub Actions \n1. Get the Pachyderm cluster URL (the same address as the s3 gateway, but with port 31400)\n2. Create another Pachyderm token like in the previous step.\n bash\npachctl auth get-auth-token --ttl "624h" | grep Token | awk \'{print $2}\' \n3. Create Pachyderm token,  DockerHub username, and DockerHub token  secrets in GitHub (see  Managing Access Tokens ). See our  GitHub Actions Example  for details. \n4. Once these tokens are in place, the pipelines will be pushed each time code is merged to the master branch. \n Repository Structure \n Data-tests \n TODO \n market_sentiment \n The main code of the project. This includes the python files needed to load data, etc.  \n Pachyderm \n This directory holds all the pachyderm pipelines. These pipelines define the code that will be run on our data in our Pachyderm cluster. Once deployed, they will automatically process any data changes, such as, when new data is labeled, it will automatically create a new dataset and train a model when that dataset is ready.  \n pachyderm-github-action \n The Pachyderm GitHub Action is used to deploy our pipelines when code is pushed to our repository. It handles the building of the Docker container, pushing it to our Docker registry, and updates our pipelines with the new version of this container.  \n tests \n Unit tests for our code that will be run before building our Docker container.  \n label-studio-project \n Project configuration for a sentiment analysis in Label Studio using Pachyderm\'s s3 gateway as the backend to add versioning to the labeling environment. Data is read from Pachyderm and written back to Pachyderm, which adds versioning automatically.  \n TODO and Known Issues \n \n There\'s currently some lag in the s3 gateway communications (likely because test cluster is very small). More investigation needed on this.  \n Bug where changing label modifies the wrong file in s3 \n Makefile for integration and prod testing \n Create staging branch for deployment that can be migrated into production.  \n Add more unit tests \n Add data tests \n', "Pachyderm -> ClearML \n Pachyderm logging to ClearML. ClearML is a great visualization and reporting framework for machine learning models, but one of the difficulties is keeping up with changing datasets. By using Pachyderm as our execution platform, we can version our executions, code, data, and models while still tracking everything in ClearML.  \n \n \n \n This is a simple integration example where we use ClearML for monitoring our jobs and experiments, while using Pachyderm to manage our data and automatically run pipelines when our code or data changes. \n TLDR; How it works \n \n Spin up Pachyderm  Use minikube or cloud deployment and connect to it. \n Spin up ClearML  (Using ClearML Hosted Community Edition) \n Create a ClearML config . \n Copy the access credentials'  CLEARML_API_ACCESS_KEY  and  CLEARML_API_SECRET_KEY  into the  secrets.json  file. We'll use this file to make a  Pachyderm secret . This keeps our access keys from being built into our container or put in plaintext somewhere. \n Create the secret with  pachctl create secret -f secrets.json \n Run  make all  to create a data repository and the pipeline.  \n \n Note: Downloading the data locally and then pushing it to Pachyderm may seem counterintuitive at first. Downloading the data locally and then pushing it to a remote cluster seems like an extra step, especially when dealing with a standard dataset like MNIST. However, if we think about a real world use case where multiple teams may be manipulating the data (removing examples, adding classes, etc.) then having a history for each of these models can be very useful. In most production settings with supervised learning, the  labeling environment can be directly connected to the data repository , automating this step. \n MNIST example \n \n Creates a project in ClearML with the name of the Pachyderm pipeline.  \n Trains an MNIST classifier in a Pachyderm Job \n Logs training info from training to ClearML for monitoring and comparison. \n If the Data or Pachyderm Pipeline changes, it kicks off a new training process. \n \n Future Goals \n \n Create a more robust example than MNIST. \n Multi-GPU - ClearML has some really cool features here, but right now Pachyderm is executing the job in a single Pod.  \n Consistent naming of runs between Pachyderm and ClearML - we would ideally like to have the jobs use the same hash ID.  \n Add commit or branch information to the model referenced by ClearML \n", 'Pachyderm Tutorial \n \n This is a repo for a Pachyderm tutorial \n \n python\n%load_ext autoreload\n%autoreload 2 \n This file will become your README and also the index of your documentation. \n Install \n \n \n Create a free Pachyderm cluster on  Pachyderm Hub . \n \n \n Connect to JupyterHub in the cluster \n \n \n Clone this repo  \n \n \n pip install pachyderm_tutorial \n Pachyderm File System Basics \n Create a Pachyderm data repository called  data \n python\n!pachctl version \n COMPONENT           VERSION             \npachctl             1.13.0              \npachd               1.12.5\n \n python\n!pachctl create repo data \n python\n!pachctl list repo \n NAME CREATED      SIZE (MASTER) ACCESS LEVEL \ndata 1 second ago 0B            OWNER\n \n When we list our repos, we can see that we have an empty data repository, with no data in it. So let\'s add some data. \n python\n%%writefile iris.csv\n5.1,3.5,1.4,0.2,Iris-setosa\n4.9,3.0,1.4,0.2,Iris-setosa\n4.7,3.2,1.3,0.2,Iris-setosa\n4.6,3.1,1.5,0.2,Iris-setosa\n7.0,3.2,4.7,1.4,Iris-versicolor\n6.4,3.2,4.5,1.5,Iris-versicolor\n6.9,3.1,4.9,1.5,Iris-versicolor\n5.5,2.3,4.0,1.3,Iris-versicolor\n6.3,3.3,6.0,2.5,Iris-virginica\n5.8,2.7,5.1,1.9,Iris-virginica\n7.1,3.0,5.9,2.1,Iris-virginica\n6.3,2.9,5.6,1.8,Iris-virginica \n Writing iris.csv\n \n Data repositories in Pachyderm automatically track versions of the data placed in  them. Similar to Git, we organize our data via branches, so we will push our data to the  master  branch of our  data  repository. \n python\n!pachctl put file data@master -f iris.csv \n iris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s\n[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s\n[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s\n[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s\n[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s\n[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s\n[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s\n[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s\n[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s\n[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s\n[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s\n[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s\n[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s\n[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s\n \n We can look at the data that\'s been uploaded to our  data  repository, by listing the files on the master branch. \n python\n!pachctl list file data@master \n NAME      TYPE SIZE \n/iris.csv file 364B\n \n Similarly, if we want to delete our file we can do that as well.  \n python\n!pachctl delete file data@master:/iris.csv \n python\n!pachctl list file data@master \n NAME TYPE SIZE\n \n Now let\'s add it back again.  \n python\n!pachctl put file data@master -f iris.csv \n iris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s\n[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s\n[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s\n[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s\n[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s\n[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s\n[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s\n[1A[Jiris.csv 364.00b / 364.00 b [======================================] 0s 0.00 b/s\n \n Now we can list all of the commits that have been made to our repository to see the history of the  master  branch. \n python\n!pachctl list commit data \n REPO BRANCH COMMIT                           FINISHED       SIZE PROGRESS DESCRIPTION\ndata master be040bd068fb4e67ae59cd2f40bb2a0c 16 minutes ago 364B -         \ndata master 44e71eeb040e4453a13014c22c9a80ed 17 minutes ago 0B   -         \ndata master 4670ab8ce78e4306a5789144f30b4afd 20 minutes ago 364B -\n \n Pachyderm keeps a record of all the changes that happen to the  data  repository. This way if we ever want to revert to a previous version of our dataset, we can do it.  \n For example, if we wanted to change move the head of our  master  branch to the first commit, we could run the following command (note the commit hashes will be different if you are running this yourself):  \n python\n!pachctl create branch data@master --head 4670ab8ce78e4306a5789144f30b4afd \n python\n!pachctl list branch data \n BRANCH HEAD                             TRIGGER \nmaster 4670ab8ce78e4306a5789144f30b4afd -\n \n We can also use  Ancestry Syntax  to traverse commits.  ^  for the parent of the commit or we can reference the commits in numerical order using  .n , where  n  is the commit number.  \n ```python \n Reset the head of our master branch \n !pachctl create branch data@master --head be040bd068fb4e67ae59cd2f40bb2a0c\n``` \n ```python \n Previous commit to the head of the master branch \n !pachctl list commit data@master^\n``` \n REPO BRANCH COMMIT                           FINISHED       SIZE PROGRESS DESCRIPTION\ndata master 44e71eeb040e4453a13014c22c9a80ed 17 minutes ago 0B   -         \ndata master 4670ab8ce78e4306a5789144f30b4afd 21 minutes ago 364B -\n \n ```python \n First commit to the master branch \n !pachctl list commit data@master.1\n``` \n REPO BRANCH COMMIT                           FINISHED       SIZE PROGRESS DESCRIPTION\ndata master 4670ab8ce78e4306a5789144f30b4afd 21 minutes ago 364B -\n \n python\n!pachctl list commit data@master.-1 \n REPO BRANCH COMMIT                           FINISHED       SIZE PROGRESS DESCRIPTION\ndata master 44e71eeb040e4453a13014c22c9a80ed 17 minutes ago 0B   -         \ndata master 4670ab8ce78e4306a5789144f30b4afd 21 minutes ago 364B -\n \n python\n!pachctl list branch data \n BRANCH HEAD                             TRIGGER \nmaster be040bd068fb4e67ae59cd2f40bb2a0c -\n \n Pachyderm Pipeline System Basics \n Pachyderm also lets you create code pipelines that can be triggered by your data. These pipelines connect to your data repositories, ensuring that they run anytime your data is updated. \n Here is an example of a Pachyderm pipeline.  \n python\n%%writefile count.yaml\npipeline:\n  name: count\ndescription: Count the number of lines in a csv file\ninput:\n  pfs:\n    glob: /\n    repo: data\ntransform:\n  cmd: [\'/bin/sh\']\n  stdin: [\'wc -l /pfs/data/iris.csv > /pfs/out/line_count.txt\']\n  image: alpine:3.14.0 \n Writing count.yaml\n \n When our pipeline runs, it will map the data from our  data  repository into the container that\'s running our pipeline. It will also automatically create a new data repository to hold and version our output(s).  \n We can submit our pipeline to Pachyderm by using the  create pipeline  command. \n python\n!pachctl create pipeline -f count.yaml \n If we list our pipelines, we can see the  status of them. \n python\n!pachctl list pipeline \n NAME  VERSION INPUT  CREATED        STATE / LAST JOB  DESCRIPTION                             \ncount 1       data:/ 15 seconds ago [32mrunning[0m / [32msuccess[0m Count the number of lines in a csv file\n \n It looks like our pipeline failed, so let\'s inspect it and see why. \n python\n!pachctl list job --pipeline=count \n ID                               PIPELINE STARTED       DURATION  RESTART PROGRESS  DL   UL  STATE   \n9bd27e46d1b64bd0bde16acefa5dff15 count    4 seconds ago 2 seconds 0       1 + 0 / 1 364B 22B [32msuccess[0m\n \n We can inspect the logs for the pipeline to see what went wrong. (Note there are 3  tries  here.) \n python\n!pachctl list file count@master \n NAME            TYPE SIZE \n/line_count.txt file 22B\n \n python\n!pachctl get file count@master:/line_count.txt -o ./line_count.txt \n ./line_count.txt 0.00b / 22.00 b [---------------------------------] 0s 0.00 b/s\n[1A[J./line_count.txt 22.00b / 22.00 b [================================] 0s 0.00 b/s\n[1A[J./line_count.txt 22.00b / 22.00 b [================================] 0s 0.00 b/s\n[1A[J./line_count.txt 22.00b / 22.00 b [================================] 0s 0.00 b/s\n \n python\ncat line_count.txt \n 12 /pfs/data/iris.csv\n \n Clear all our data out for the next section \n ```python \n Uncomment and run if continuing on \n !pachctl delete pipeline --all \n !pachctl delete repo --all \n ``` \n Python-Pachyderm \n Create a client to connect to the Pachyderm clutser. \n python\nprint(python_pachyderm.__version__) \n 6.2.0\n \n python\nclient = python_pachyderm.Client.new_from_config() \n python\nclient.create_repo(\'data\') \n python\nwith client.commit("data", "master") as commit:\n    client.put_file_bytes(commit, "data.txt", b"DATA") \n python\nclient.list_repo() \n [repo {\n  name: "data"\n}\ncreated {\n  seconds: 1624489726\n  nanos: 662871220\n}\nsize_bytes: 4\nauth_info {\n  access_level: OWNER\n}\nbranches {\n  repo {\n    name: "data"\n  }\n  name: "master"\n}\n]\n \n ```python\nfile_list = client.list_file((\'data\',\'master\'), \'/\', include_contents=True) \n for f in file_list: \n    print(f)\n``` \n file {\n  commit {\n    repo {\n      name: "data"\n    }\n    id: "3309835f727c4e4aa760ebf26421cdb8"\n  }\n  path: "/data.txt"\n}\nfile_type: FILE\nsize_bytes: 4\nhash: "\\330Ukb\\227\\364\\273^7gc\\022\\025\\254\\201\\004\\000\\026\\331\\314G\\013\\003^)\\006\\030\\304]\\004\\305\\025"\nobjects {\n  hash: "4ba7d4149c32f5ccc6e54190beef0f503d1e637249baa9e4b123f5aa5c89506f299c10a7e32ab1e4bae30ed32df848f87d9b03a640320b0ca758c5ee56cb2db4"\n}\ncommitted {\n  seconds: 1624489733\n  nanos: 99330690\n}\n', "Superb.ai + Pachyderm Integration \n \n This example shows how you can create a  Pachyderm  pipeline to automatically version and save data you've labeled in  Superb.ai  to use in downstream machine learning workflows.  \n The integration connects to your SuperbAI project, ingests the data into Pachyderm on a  cron schedule .  \n Once your data is ingested into Pachyderm, you can perform data tests, train a model, or any other type of data automation you may want to do, all while having full end-to-end reproducibility.  \n Requirements \n You will need an account for each of the tools. Free accounts can now be used to run this example! \n*  Superb.AI account \n* Setup a  Pachyderm Hub  Cluster \n Run this example \n \n Generate an Access API Key in SuperbAI. \n \n Put the key and your user name in the  secrets.json  file.  \n Create the Pachyderm secret \n \n bash\npachctl create secret -f secrets.json \n \n Create the cron pipeline to synchronize your  Sample project  from SuperbAI to Pachyderm. This pipeline will run every minute to check for new data (you can configure it to run more or less often in the cron spec in  sample_project.yml ). \n \n bash\npachctl create pipeline -f sample_project.yml \n \n Pachyderm will automatically kick off the pipeline and import the data from your sample project. \n \n", 'PachLite \n The PachLite is a hack project to simplify a couple of convenience functions.  \n Install \n The project uses  poetry : \n bash\npoetry shell\npoetry install \n Using the CLI \n ```\n$ pachlite\nUsage: pachlite [OPTIONS] COMMAND [ARGS]... \n PyPach Utilities for developing Pachyderm pipelines locally. \n Options:\n  --help  Show this message and exit. \n Commands:\n  build  Build pachyderm pipeline\n  run    Run python file locally as if it were a pipeline\n``` \n ```\n$ pachlite build --help\nUsage: pachlite build [OPTIONS] [ENTRYPOINT_ARGS]... \n Build pachyderm pipeline \n Options:\n  -n, --name TEXT         Name of pipeline\n  -d, --description TEXT  Description of pipeline\n  --image TEXT            Name of docker image to be used for the entrypoint\n  -i, --input_repo TEXT   Input repo(s) - format repo@branch\n  --entrypoint PATH\n  --help                  Show this message and exit.\n``` \n ```\npachlite run --help\nUsage: pachlite run [OPTIONS] ENTRYPOINT [ENTRYPOINT_ARGS]... \n Run python file locally as if it were a pipeline \n Options:\n  -i, --input TEXT  Input repo(s) - format repo@branch\n  --help            Show this message and exit.\n```']
bamos,["Installation \n Clone this repo with git's  --recursive  flag to\nobtain all the submodules, then run\n bootstrap.sh \nto create symlinks in the home directory and\nbootstrap other programs.", '\n About \n This is the source code for my personal website.\nUnless stated otherwise, all content is MIT-licensed,\nand some of the CV portions are created with the code\nin the  bamos/cv  repo. \n w3c compliance continuous integration \n Travis CI builds the static website with Jekyll and uses\n validate.rb  to check content for w3c compliance.\nSimon Sigurdhsson wrote the\n original validate.rb script ,\navailable in the public domain by the CC0 license,\nand the modifications here are also available in the public domain\nby the CC0 license.', 'This is a collection of short shell scripts I have added to my\n PATH  variable to run from anywhere. \n To add these to your  PATH , clone the repo and add the following\nto your  bashrc  or  zshrc , replacing  <python-scripts> \nwith the location of the cloned repository.\nFurthermore, see my  dotfiles  repo for my\ncomplete Mac and Linux system configurations. \n ```Bash \n Add additional directories to the path. \n pathadd() {\n  [ -d "$1" ] && [[ ":$PATH:" !=  ":$1:"  ]] && PATH="${PATH:+"$PATH:"}$1"\n} \n pathadd  /python2.7\npathadd  /python3\n``` \n 2x2-slides.sh \n Takes a PDF of slides as input and outputs them tiled\nin a 2x2 landscape PDF. \n alarm.sh \n Linux computers can be used as an alarm clock with a program called\n rtcwake , which will sleep the computer until a specifed time.\n alarm.sh  is a simple wrapper around  rtcwake  to infer the\nfull data given a time.\nFor example, the following command will sleep the computer until the\nnext occurring 7PM. \n Bash\nalarm.sh 7:00PM \n analyze-pcap.sh \n Use tcpflow and foremost to analyze TCP streams in a pcap file.\nFor example, the following command automatically analyzes a pcap file. \n Bash\nanalyze-pcap.sh traffic.pcap \n compare-dirs.sh \n Compares the files in 2 directories and\ndetects duplicates based on MD5 checksums.\nFor example, the following command compares the\nfiles in  test_dir1  and  test_dir2 . \n ```\n./compare-dirs.sh test_dir1 test_dir2\nCreating checksums for files in dir1.\nChecking files in dir2.\nmd5:  60b725f10c9c85c70d97880dfe8191b3\ndir1: test_dir1/a\ndir2: test_dir2/a_renamed \n md5:  3b5d5c3712955042212316173ccf37be\ndir1: test_dir1/b\ndir2: test_dir2/b\n``` \n createpdf.sh \n Create a pdf document from a plaintext document,\nwith optional source code highlighting. \n ./createpdf.sh main.cpp c++ \n notify-postponed.sh \n Send a notification when there are postponed\nmessages in  mutt . \n timesheets.sh \n Plaintext timesheet management.', 'PARSEC Benchmark Suite 3.0 \n Patch Info \n This patch provides the following enhancements: \n \n Ability to build on x86_64 Arch Linux to overcome minor bugs. \n Updated openssl, see Abhishek Sagar\'s  blog post \n Corrected uctcpip, see Yungang Bao\'s  response \n Generalized builds. Instead of calling  gcc  tools and overwriting\n   CFLAGS ,  CC ,  CFLAGS , and related environment variables set\n  in a global bldconf are preserved while building packages. \n \n Overview \n The Princeton Application Repository for Shared-Memory Computers (PARSEC) is a\ncollection of parallel programs which can be used for performance studies of\nmultiprocessor machines. \n The PARSEC distribution is composed of packages and the PARSEC framework.\nPackages correspond to benchmark programs, libraries and other essential\ncomponents. Each package can be compiled in a number of ways as determined by\na build configuration. Build configurations contain information such as which\nfeatures of the package should be enabled, which compilers to use and how the\npackage should be optimized. PARSEC ships with predefined inputs that can be\nused to run the benchmarks. The inputs for each program exhibit different\ncharacteristics such as execution time and working set size. \n Licenses \n Before you start using, modifying or distributing PARSEC, its programs or the\nsupplied inputs in any way, make sure you understand all licenses involved.\nThe PARSEC framework itself is available under a liberal open source license,\nas explained in the file LICENSE which is in the same directory as this README\nfile. Each program uses its own license, which is different in some cases.\nSome of the inputs have their own license, too. Licenses for source code can\nbe found in the \'src/\' directory of each package. Licenses for inputs can\nbe found in the \'inputs/\' directory of the package. \n We distribute the programs and their workloads bundled with PARSEC merely to\nallow PARSEC users a convenient access to them and because the license terms\nallow us do so in each case. You have to take appropriate steps yourself to\nmake sure you don\'t violate any license terms. \n Requirements \n PARSEC requires at least 8 GB, but we recommend 12 GB or more. The disk usage\ncan be broken down as follows: \n PARSEC occupies about 7 GB with a raw installation. Additional 500 MB are\nneeded for each set of binaries. To build packages and run them extra space is\nrequired for temporary files, up to several GB if the whole suite is to be\nbuilt and executed in one run without cleaning up intermittently. \n The PARSEC benchmarks have been parallelized with pthreads, OpenMP, TBB and\natomic instructions. Many workloads support more than one parallelization. Each\nparallelization has its own requirements that have to be fulfilled in order to\nbe able to build and run workloads that use it. By default only support for\npthreads and OpenMP are needed. Gcc supports OpenMP since version 4.2.0. \n PARSEC has been successfully tested on the following systems: \n \n Linux/i386 \n Linux/x86_64 \n Linux/Itanium \n Solaris/Sparc \n \n Limited support exists for the following platforms, but not all benchmark\nprograms might be available: \n \n Darwin/PowerPC \n \n Usage \n PARSEC ships with several tools which are installed in the \'bin/\' directory.\nYou can use them to customize and manage your installation of the benchmark\nsuite. \'parsecmgmt\' is the main tool. Its purpose is to build and run\npackages as well as perform other management operations. \'bldconfadd\' and\n\'bldconfdel\' can be used to create and delete your own build configurations. \n When you build and run PARSEC with \'parsecmgmt\', it will create a log file\nwhich will contain all output in the \'log/\' directory. You can get a help\nsummary for each tool by invoking it with option \'-h\'. \n A full set of man pages documenting PARSEC, its tools and the most important\nparts of the source code is given in the  man/\' directory. If you add the bin/\' directory to the PATH environment variable and the  man/\' directory\nto the MANPATH variable then all tools and man pages are accessible at the\ncommand line without having to specify the full path every time. A bash shell\nscript env.sh\' is provided in the root directory of the PARSEC distribution\nwhich modifies the environment in that way. If you work with the bash shell you\ncan make use of it before you start working with PARSEC by executing\n source env.sh\'. You can then start browsing the documentation by running man parsec\'. Support for other shells is currently not available. \n The following examples assume that the \'bin/\' directory of PARSEC is in your\npath. \n How to Build PARSEC \n Before compiling the PARSEC benchmarks, please change some variables in the\nfile "config/gcc.bldconf" such that the PARSEC command  can locate the \ncompiler path correctly. \n To compile all programs of the benchmark suite with the default configuration,\nsimply run: \n parsecmgmt -a build\n \n Building the whole benchmark suite takes a lot of time, usually 30-60 min\ndepending on your system It is possible to selectively build packages\n(option \'-p\') and to choose different build configurations (option \'-c\'), read\nSection 4 for a more comprehensive explanation of build configurations and\nSection 6 for more complex usage examples. \n How to Run PARSEC \n After you have built the suite, you can use the following command to run all\nbenchmarks with the minimal test input: \n parsecmgmt -a run\n \n The test should finish within 5 seconds. Its purpose is to quickly verify that\nall benchmarks have been built successfully and are executable. Do not use it\nfor performance measurements. You can also choose a different input (option\n\'-i\') and alter the number of threads (option \'-n\'). Section 5 explains the\ndifferent inputs and Section 6 gives more detailed examples. \n How to Get More Information \n To query PARSEC about all available packages and features, you can use the\nfollowing command: \n parsecmgmt -a info\n \n If you would like to see information about dynamically created files such as\nthe available builds, you can run: \n parsecmgmt -a status\n \n Build Configurations \n Besides the regular configuration which can be altered to customize the\nbenchmark suite, PARSEC also uses build configurations. A build configuration\nis a specific way to compile a program. It determines, for example, what\ncompiler to use and which features of the package to enable. Build\nconfigurations should be your first approach to alter how a benchmark program\nis created. The only limitation is that the interface and output of a benchmark\nmust remain the same, otherwise the program will become incompatible with the\nPARSEC framework. However, you can adapt the run configuration and reference\noutputs to eliminate this problem. \n PARSEC ships with the following preinstalled build configurations: \n \n \'gcc\'       Build parallel version of suite with gcc \n \'gcc-serial\'    Build serial version of suite with gcc \n \'gcc-hooks\'     Build parallel version of suite with PARSEC hooks\n              enabled with gcc \n \'icc\'       Build parallel version of suite with Intel compiler \n \'gcc-pthreads\'  Build with pthreads parallelization (if supported) \n \'gcc-openmp\'    Build with OpenMP parallelization (if supported) \n \'gcc-tbb\'       Build with TBB parallelization (if supported) \n \n For example, to build PARSEC with enabled hooks, you can use: \n parsecmgmt -a build -c gcc-hooks\n \n The three build configurations \'gcc-pthreads\', \'gcc-openmp\' and \'gcc-tbb\' can\nbe used to compile a workload with one of these three parallelization models if\nit is supported by the program. The build configuration \'gcc\' implicitly\ndefines a standard parallelization for each workload by falling back to one of\nthese three configurations for each benchmark program. \n We also defined an alias for each parallelization model that will be resolved\nresolved to a complete list of all workloads that support this parallelization\nmodel. They have the same name as the parallelization. For example, to build\nall workloads which support OpenMP with exactly that parallelization, you\ncan use: \n parsecmgmt -a build -p openmp -c gcc-openmp\n \n Additional build configurations can be created with the tool \'bldconfadd\'. For\nexample, to add a new configuration \'myconfig\' that is based on gcc-serial,\nyou can use the following command: \n bldconfadd -n myconfig -s gcc-serial\n \n To remove this configuration, you can use \'bldconfdel\' as follows: \n bldconfdel -n myconfig\n \n Performance Measurement & Research \n For each benchmark, we define a Region-Of-Interest (ROI) which includes the\ncomputationally intensive, parallelized phase of the benchmark, but not the\ninitialization or shutdown phase. Instead of measuring the total program\nruntime, you can use and report the execution time of the ROI for any kind of\nanalysis and comparisons. We provide six inputs for each benchmark program: \n \n \'test\'    Minimal input to verify that programs are executable. \n \'simdev\'  Very small input which causes code execution comparable\n            to a typical input for this program. Intended for\n            microarchitectural simulator development. \n \'simsmall\'    Small input for performance measurements with\n            microarchitectural simulators \n \'simmedium\'   Medium-sized input for performance measurements with\n            microarchitectural simulators \n \'simlarge\'    Large-sized input for performance measurements with\n            microarchitectural simulators \n \'native\'  Very large input intended for large-scale experiments\n            on real machines \n \n All inputs except \'test\' and \'simdev\' can be used for performance analysis. As\na rough guideline, on a Pentium 4 processor with 3.0 GHz you can expect\napproximately the following execution times: \n \n \'test\'    almost instantaneous \n \'simdev\'  almost instantaneous \n \'simsmall\'    ~1s \n \'simsmall\'    ~3-5s \n \'simlarge\'    ~12-20s \n \'native\'  ~10-30min \n \n The exact runtime depends on the program and its inputs and deviates in some\ncases from the described rule of thumb. Different build configurations and\nsystem parameters may also result in a deviation. \n PARSEC Hooks \n All benchmark programs of the benchmark suite support PARSEC hooks, a library\nwhich allows rapid instrumentation of all benchmark. If the benchmarks are\ncompiled with hooks enabled, at various locations in the code the programs will\ncall the corresponding hook function. Among other things, the ROI has been\ninstrumented in such a way, and by default the PARSEC hooks library measures the\ntime spent in the ROI. \n Documentation and code of the PARSEC hooks is available in the \'hooks\' package\nin group \'libs\'. PARSEC hooks are enabled in the build configuration \'gcc-hooks\'\nand disabled in all other ones. \n More Examples \n Compatible to PARSEC 2.1 \n Build \'x264\' and \'blackscholes\' benchmarks with Intel compilers: \n parsecmgmt -a build -p x264 blackscholes -c icc\n \n Build serial version of all kernels: \n parsecmgmt -a build -p kernels -c gcc-serial\n \n Run a self-test with all applications: \n parsecmgmt -a run -p apps -i test\n \n Use version of \'vips\' and all kernels with enabled hooks to measure the\nperformance on a real machine with 32 threads: \n parsecmgmt -a run -p vips kernels -c gcc-hooks -n 32\n \n Use SimpleScalar simulator to run a simulation with the \'gcc-hooks\'\nbuild configuration of the \'freqmine\' application with small simulator inputs\nand 4 threads: \n parsecmgmt -a run -c gcc-hooks -s sim-outorder -p freqmine \\\n+ i simsmall -n 4\n \n Use an executable \'qsub\' to submit runs with all kernels with 16 threads and\nlarge simulator inputs to a batch system (this requires that a program \'qsub\'\nis installed and in the path which handles the job submission): \n parsecmgmt -a run -s qsub -p kernels -i simlarge -n 16\n \n Clean up after a build or benchmarking run: \n parsecmgmt -a fullclean -p all\n \n Uninstall \'gcc-serial\' build of package \'gsl\' and build a new version: \n parsecmgmt -a uninstall -p gsl -c gcc-serial\nparsecmgmt -a build -p gsl -c gcc-serial\n \n Network benchmarks \n Check the status of all components involved in network benchmarks: \n     parsecmgmt -a status -p netapps\n \n Build network benchmark \'netstreamcluster\': \n     parsecmgmt -a build -p raytrace\n \n Build all network benchmark: \n     parsecmgmt -a build -p netapps\n \n Run network benchmark \'netdedup\' w/ input \'native\' and 2 server threads: \n     parsecmgmt -a run -p netdedup -i native -n 2\n \n Run network benchmark \'netferret\' w/ input \'simlarge\', 4 server threads \nand 2 client connections: \n     parsecmgmt -a run -p netferret -i simlarge -n 4 -t 2\n \n For simulation, run \'netdedup\' server on a simulator w/ 4 threads and \nrun client on a real machine: \n     parsecmgmt -a run -p netdedup -i simlarge -n 4 -m server\n    parsecmgmt -a run -p netdedup -i simlarge -m client\n \n Do a full cleanup for network benchmarks: \n     parsecmgmt -a fullclean -p netapps\n \n SPLASH-2 Suite and SPLASH-2x Suite \n Check the status of SPLASH-2 suite and SPLASH-2x suite: \n     parsecmgmt -a status -p splash2\n    parsecmgmt -a status -p splash2x\n \n Build benchmark \'raytrace\' from SPLASH-2x suite other than PARSEC:\n        parsecmgmt -a build -p splash2x.raytrace\n        parsecmgmt -a build -p raytrace   ## defaultly from PARSEC (for comparison) \n Build benchmark \'fft\' from SPLASH-2 suite with \'gcc-serial\' build configuration: \n     parsecmgmt -a build -c gcc-serial -p splash2.fft\n \n Build all benchmarks from SPLASH-2 suite and SPLASH-2x suite: \n     parsecmgmt -a build -p splash2\n    parsecmgmt -a build -p splash2x\n \n Run benchmark \'fft\' from SPLASH-2x w/ input \'simsmall\' and 4 threads: \n     parsecmgmt -a run -p splash2x.fft -i simsmall -n 4\n \n Do a full cleanup for SPLASH-2 suite \n     parsecmgmt -a fullclean -p splash2\n \n Structure \n The PARSEC suite is composed of the software packages, which are the benchmark\nprograms and their required libraries and tools, and the framework, which is\neverything else. Software packages are located in the \'pkgs/\' directory. Each\nsoftware package is part of exactly one package group, which has its own\nsubdirectory. For example, a package named \'foo\' which belongs to group \'bar\' \nwould be located in the directory \'pkgs/bar/foo/\'. \n PARSEC has the following structure: \n bin/        directory with PARSEC tools\nconfig/     global configuration\nlog/        log files of builds and runs (dynamically created)\nman/        man pages of the PARSEC distribution\npkgs/       package groups which contain the packages\nversion     file with version number of PARSEC distribution\n \n A package has the following directory structure: \n inputs/     directory with input archives (optional)\ninst/       installation directory (dynamically created)\nobj/        build directory (dynamically created)\noutputs/    directory with reference outputs (optional)\nparsec/     PARSEC configuration files\nrun/        directory for program execution (dynamically created)\nsrc/        source code of the package\nversion     file with package version\n \n Some of these directories will be auto-generated by \'parsecmgmt\' on the fly. \n Each package can have multiple builds and installations, and \'parsecmgmt\' will\nuse separate subdirectories for them. PARSEC uses the name of the build\nconfiguration and the platform for which the program is compiled to form a key\nthat is used to distinguish different builds and installations of a package. \n Configuration Files \n PARSEC distinguishes between global and local configuration. Global\nconfiguration files are located in the \'config/\' directory in the root of the\nbenchmark suite. Local configuration files are only valid for a single software\npackage and are stored in the \'parsec/\' directory of the package they belong to. \n The following types of configuration files exist: \'parsec.conf\' is a global\nfile which defines the general structure of the benchmark suite, such as\naliases and which software packages exist. Files named \' .sysconf\' define\nthe basic programs which \'parsecmgmt\' is to use on each operating system\nplatform. To port \'parsecmgmt\' to a new operating system, a corresponding file\nhas to be created. \' .runconf\' are configuration files which determine how\nbenchmark programs are to be executed. Finally, files named \'*.bldconf\'\ndescribe the build configuration of a package. Run and build configurations\nare composed of both global and local configuration files. \n Manual Usage \n It is possible to build and run PARSEC benchmark programs manually. To do so,\nthe correct build configuration respectively run configuration should be used\nas defined in the PARSEC configuration files. \n The following steps are executed by \'parsecmgmt\' to build a package: \n \n Set variable PARSECDIR to the installation root of PARSEC \n Set variable PARSECPLAT to the build key used to identify the platform \n Source system configuration \n Source global build configuration \n Source local build configuration \n Create build directory and change to it \n If it exists, modify build environment and call configure script \n Modify build environment and execute \'make\' \n Modify build environment and execute \'make install\' \n \n The following steps are executed by \'parsecmgmt\' to run a benchmark: \n \n Set variable PARSECDIR to the installation root of PARSEC \n Set variable PARSECPLAT to the build key used to identify the platform \n Source system configuration \n Source global run configuration\n        - Source local run configuration \n Create run directory and cd to it \n Unpack desired input \n Execute benchmark with parameters from run configuration \n \n Contact \n You can reach the PARSEC team as follows: \n <http://parsec.cs.princeton.edu/>\nparsec@lists.cs.princeton.edu\n', "\n\nThis README is auto-generated with generate-readme.sh\nPlease add changes there.\n\n Reading List \n \n This repository contains my open source reading list.\nI keep track of books by editing the files here and\nthe results are automatically published as a website at\n http://bamos.github.io/reading-list . \n This repository just contains stubbed examples of how to use\nthis project. Ping me if you're interested in my\npersonal reading list! \n Goals \n \n Plaintext and friendly data format. \n Minimal hosting and deployment overhead. \n Offline editing support. \n \n Technologies Used \n \n Linux and OSX. Windows should also work with Cygwin, but\n  I haven't tried. Please file any issues related to this. \n YAML  data. \n GitHub Pages  hosts and automatically\n  deploys a 100% client-side website that can also be edited offline.\n   Bower  manages 3rd party library dependencies\n  used on the site, stored in  bower.json . \n \n Creating Your Reading List: Quick Start \n \n Fork or copy the contents of this repository into a new GitHub repository.\n  Make sure the default branch is set to  gh-pages  for deployment.\n  At this point, you should be able to see my site hosted at.\n   http://<your-github-name>.github.io/reading-list \n Update the  data \n  and personalize  index.html .\n  Push your changes to GitHub to see them immediately on the new site. \n Replace links to http://bamos.github.io/reading-list with your URL. \n \n Local Deployment \n Most browsers will not be able to open  index.html  directly\nfrom the filesystem because the js loads YAML resources.\nOne workaround is to use start a simple Python static\nweb server with  python2 -m SimpleHTTPServer \nand access the website with  localhost:8000 . \n Updating Bower Dependencies \n Run  bower update  to obtain the dependencies in  bower_components .\nRun  ./update-vendor-deps.sh \nto copy the necessary portions into  vendor . \n Scripts \n The  scripts  directory contains Haskell and\nRuby scripts to select random books and quotes from\n data/finished.yaml . \n Import from Goodreads \n The  Goodreads Ruby script  by\n @seanosaur \nuses  Goodreads' API \nto import books into data files.\nPlease follow their ToS and add appropriate references\nto Goodreads if this is used. \n Warning : This script only pulls the first 200 books.\nImprovements to this are being tracked in\n this issue . \n Inspiration \n The following projects inspired me to create\na GitHub-hosted reading list. \n Name | Stargazers | Description\n----|----|----\n cmonty/reading-list  | 8 | Track books I've read and any thoughts I've had. Also uses Wiki to track knowledge.\n coryschires/reading-list  | 18 | List of books and screencasts related to development, user experience design, and entrepreneurship. \n DavidRagone/reading_list  | 2 | List of books I have read related to development, user experience design, and entrepreneurship\n eightbitraptor/reading_list  | 19 | \n engeld/reading-list  | 0 | A collection of my reading list and notes.\n gbtekkie/ReadingList  | 2 | handy collection of tekkie readings\n jaredcacurak/reading-list  | 3 | My reading list. \n People using this repo for their reading list \n Ping me if you'd like to be added or removed. \n Name | Stargazers | Description\n----|----|----\n aerovolts/reading-list  | 0 | My personal reading list.\n ammadafsar/reading-list  | 0 | My reading list. Made so that I add things I will later read and commit to read them all in order.  \n connors511/reading-list  | 0 | My reading list. Made so that I add things I will later read and commit to read them all in order.\n jakehschwartz/reading-list  | 0 | My reading list.\n markroxor/reading-list  | 0 | My reading list.\n rwfeather/reading-list  | 0 | My reading list.\n samtron1412/reading-list  | 0 | My reading list.\n seanosaur/reading_list  | 2 | \n wrideout/reading-list  | 2 | My reading list. \n Credits and Licensing \n All portions are\n MIT licensed \nby Brandon Amos unless otherwise noted. \n This project uses and modifies the following open source projects\nand resources.\nModifications remain under the original license. \n | Project | Modified | License |\n|---|---|---|\n|  Twitter bootstrap  | No | MIT |\n|  handlebars.js  | No | MIT License\n|  IronSummitMedia/startbootstrap-grayscale  | Yes | Apache 2 |\n|  makeusebrew/bootbox  | No | MIT |\n|  MathJax  | No | Apache |\n|  Flickr Photo  | Yes |  cc by-nc-sa 2.0  |\n|  TimelineJS  | No | Mozilla Public License", "\n LaTeX  is a typesetting program\nfor producing high quality technical documents.\nFormatting LaTeX documents is difficult and modifying pre-built\ntemplates often require extensive knowledge of the template.\nThis repository contains simple LaTeX templates for common documents.\nScreenshots of each template are available on  this webpage ,\nwhich is automatically created from  generate.py .\nSee my other LaTeX projects at  bamos/cv  and\n bamos/beamer-snippets . \n \n Writing Check \n btford/write-good  is a naive linter for English prose\nand works well on LaTeX documents.\nIf  write-good  is installed, the Makefile's in this project will output\na list of warnings and tips for improving writing after\nbuilding the LaTeX documents. \n Contributing \n Contributions are highly welcomed!\nIf you want to add a similar template, please add to\nthe  latex-templates  directory and I'm happy to merge pull requests.\nIf you want to use the static webpage generation framework to present\na different set of templates with other motivations,\nI'm happy to link to your project here. \n Webpage Generation Process \n The Python 3 script  generate.py  produces a static website in  dist .\n generate.py  loops through the collection of snippets and uses\n Jinja  templates to output LaTeX documents. \n This project uses  Grunt  to deploy  dist  to  Github pages \nin the  gh-pages  branch with the  grunt-build-control  plugin.\n package.json  manages the  npm  dependencies.\nRunning  npm install  installs the dependencies. \n \n grunt generate  produces the static site in  dist , and \n grunt deploy  pushes the  dist  directory to the  gh-pages  branch. \n \n Similar Projects \n There are many approaches to sharing LaTeX templates online,\nand this project uniquely adds a static webpage generation process\nto generate previews of templates managed in Git.\nThe following list shows a short sampling of projects,\nand I'm happy to merge pull requests of other projects. \n Git Repositories \n \nTo generate the following list, install https://github.com/jacquev6/PyGithub\nand download the `github-repo-summary.py` script from\nhttps://github.com/bamos/python-scripts/blob/master/python3/github-repo-summary.py.\nPlease add projects to the list in the comment and in the table below.\n\ngithub-repo-summary.py \\\n  cmichi/latex-template-collection \\\n  deedydas/Latex-Templates \\\n  MartinThoma/LaTeX-examples \\\n  RichardLitt/latex-templates \\\n  stevegeek/latex-templates\n \n Generated on 2015-10-03, see the Markdown source of this file for more details. \n Name | Stargazers | Description\n----|----|----\n cmichi/latex-template-collection  | 255 | A collection of different LaTeX templates (cv, invoices, timesheets, letters, etc.).\n deedydas/Latex-Templates  | 58 | A concise set of Latex templates that serves a small set of needs - CV, Essays, Articles and Problem Sets\n MartinThoma/LaTeX-examples  | 176 | Examples for the usage of LaTeX\n RichardLitt/latex-templates  | 1 | My Personal LaTeX Templates\n stevegeek/latex-templates  | 20 | A collection of my LaTeX templates: CV (resume), letter head and PhD Thesis \n Websites \n \n http://www.latextemplates.com/ \n https://www.sharelatex.com/templates \n https://www.writelatex.com/templates \n \n Licensing \n All modified referenced code has license of the original source.\nAll other portions are under the MIT license.", '\n \n This is a collection of short Python scripts I use in Linux.\nPortions of this README are automatically generated with\n generate-readme.py \nto insert the script-level comments as descriptions below. \n Adding to your PATH \n I have these added to my  PATH \n variable \nto run from anywhere.\nClone the repo and add the following\nto your  bashrc  or  zshrc , replacing  <python-scripts> \nwith the location of the cloned repository.\nSee my  dotfiles \nrepo for my complete Mac and Linux system configurations. \n ```Bash \n Add additional directories to the path. \n pathadd() {\n  [ -d "$1" ] && [[ ":$PATH:" !=  ":$1:"  ]] && PATH="${PATH:+"$PATH:"}$1"\n} \n pathadd  /python2.7\npathadd  /python3\n``` \n Dependencies \n These scripts are written in Python 3 except when external\nlibraries don\'t support Python 3.\nDependencies for Python 2 and 3 for all scripts are\nincluded in  requirements-{2,3}.txt  and can be installed\nusing  pip  with  pip3 install -r requirements-3.txt . \n Travis CI \n Continuous integration is provided by Travis CI\n here .\n .travis.yml \ncalls\n .travis-script-2.sh \nand\n .travis-script-3.sh \nto ensure  requirements-{2,3}.txt  have all of the Python 2 and Python 3 scripts\nand that there are no careless errors that cause the scripts to\nnot execute the help message.\n pep8  will fail the build\nif pep8 styling conventions aren\'t met. \n Scripts \n The subheadings link to the script contents on GitHub. \n generate-readme.py \n \n Authors:  Brandon Amos \n Created: 2015.03.27 \n \n Generates the README for\n bamos/python-scripts \nso that the README and scripts contain synchronized documentation.\nScript descriptions are obtained by parsing the docstrings\nand inserted directly into the README as markdown. \n python2.7/music-organizer.py \n \n Authors:  Brandon Amos \n Created: 2014.04.19 \n \n This script (music-organizer.py) organizes my music collection for\niTunes and  mpv  using tag information.\nThe directory structure is  <artist>/<track> , where  <artist>  and  <track> \nare lower case strings separated by dashes. \n See my blog post\n Using Python to organize a music directory \nfor a more detailed overview of this script. \n python2.7/mt.py \n \n Authors:  Brandon Amos \n Created: 2014.11.30 \n \n This script implements the simple\n multitail \nexample to tail multiple files and append the filename to the beginning\nof the output. \n python2.7/caffe-compute-image-mean.py \n \n Authors:  Brandon Amos \n Created: 2015.08.10 \n \n This script computes the mean of a directory of images for Caffe. \n python2.7/fix-music-tags.py \n \n Authors:  Brandon Amos \n Created: 2015.12.30 \n \n This script (fix-music-tags.py) mass-removes unwanted music tags. \n python3/github-repo-summary.py \n \n Authors:  Brandon Amos \n Created: 2014.11.02 \n \n Produces a Markdown table concisely summarizing a list of GitHub repositories. \n python3/link-checker.py \n \n Authors:  Brandon Amos \n Created: 2014.02.06 \n \n Script to be run by crontab to report broken links. \n Builds upon linkchecker (Ubuntu: sudo apt-get install linkchecker)\nto hide warnings and to send a concise email if bad links are found. \n \n python3/phonetic.py \n \n Authors:  Brandon Amos \n Created: 2014.02.14 \n \n Obtain the NATO phonetic alphabet representation from short phrases. \n $ phonetic.py github\ng - golf\ni - india\nt - tango\nh - hotel\nu - uniform\nb - bravo \n python3/rank-writing.py \n \n Authors:  Brandon Amos \n Created: 2014.02.14 \n \n rank-writing.py  ranks the writing quality of my\nblog\'s Markdown posts and my project\'s Markdown README files. \n The following programs should be on your  PATH :\n+  aspell \n+  write-good \n+  diction \n ```\n$ rank-writing.py *.md \n === 2013-05-03-scraping-tables-python.md ===\nTotal: 53\n├── aspell: 34\n├── diction: 0\n└── write-good: 19 \n ... \n === 2013-04-16-pdf-from-plaintext.md ===\nTotal: 0\n├── aspell: 0\n├── diction: 0\n└── write-good: 0\n``` \n python3/get-osx-wallpaper.py \n \n Authors:  Brandon Amos \n Created: 2015.03.25 \n \n This is a Python script that outputs the path of the current\nOSX wallpaper.\nThis is helpful when the desktop wallpaper is randomized\nacross a large collection of pictures and you want to\ndelete the current wallpaper. \n Warning \n \n This approach doesn\'t work with multiple monitors or virtual desktops. \n \n Tested On \n \n OSX Yosemite 10.10.2 with a single desktop on a MBP. \n \n Usage \n Ensure  db_path  and  wallpaper_dir  are correctly set below. \n Assuming  get-osx-wallpaper.py  is on your path,\ncheck the output with the following \n $ get-osx-wallpaper.py\n/Users/bamos/Pictures/wallpaper/nature/496.jpg \n Please ensure this is correct before trying to remove it! \n This can be paired with other commands such as  open  or  rm .\nRun  killall Dock  to refresh the changes after removing the file.\nNote that the dock will be restarted and all windows will be\nunminimized. \n $ open $(get-osx-wallpaper.py)\n$ rm $(get-osx-wallpaper.py) && killall Dock \n Example alias definitions for bash and zsh are available in\nhttps://github.com/bamos/dotfiles/blob/master/.funcs: \n alias open-wallpaper=\'open $(get-osx-wallpaper.py)\'\nalias rm-wallpaper=\'rm $(get-osx-wallpaper.py) && killall Dock\' \n python3/merge-pdfs-printable.py \n \n Authors:  Brandon Amos \n Created: 2014.10.17 \n \n The printers in my office print a cover page before every job, and\nI don\'t like printing many cover pages if I want to submit multiple\npapers separately so that the papers don\'t overlap. This script will\nmerge PDF documents and insert blank pages so that the printed pages\nwon\'t overlap documents. The modulo option is helpful to print 2 PDF\npages per physical page side. \n The script uses PyPDF2 to merge the documents and to extract the\nnumber of pages in the input documents and ghostscript to create a\nblank PDF page. \n $ merge-pdfs-printable.py a.pdf b.pdf c.pdf --modulo 4\na.pdf\n  + Pages: 6\n  + Added 2 blank pages.\nb.pdf\n  + Pages: 13\n  + Added 3 blank pages.\nc.pdf\n  + Pages: 13\n  + Added 3 blank pages.\nMerged output is in \'/tmp/tmpm2n5g0mh-merge.pdf\'. \n Note: Some of my decrypted PDF documents have resulted in\nPyPDF2.utils.PdfReadError: file has not been decrypted. My current\nworkaround solution is to run pdf2ps on the PDF and then ps2pdf on the\nPS file. \n python3/remove-duplicates.py \n \n Authors:  Brandon Amos \n Created: 2015.06.06 \n \n Detect and remove duplicate images using average hashing. \n http://www.hackerfactor.com/blog/index.php?/archives/432-Looks-Like-It.html \n python3/word-counter.py \n \n Authors:  Brandon Amos \n Created: 2014.11.7 \n \n Count work frequencies within a file. \n ```\n$ word-counter.py shakespeare.md --numWords 4 --maxTuples 3 \n === Sliding Window: 1 ===\n    3473: \'shall\'\n    2238: \'would\'\n    2153: \'which\'\n    2074: \'their\' \n === Sliding Window: 2 ===\n    248: \'exeunt scene\'\n    117: \'second lord.\'\n    105: \'first lord.\'\n    102: \'queen elizabeth.\' \n === Sliding Window: 3 ===\n    36: \'william shakespeare dramatis\'\n    34: \'shakespeare dramatis personae\'\n    18: \'comes here? enter\'\n    14: \'duke\'s palace enter\'\n``` \n python3/eval-expr.py \n \n Authors: J. Sebastian,  Brandon Amos \n Created: 2013.08.01 \n \n A module to evaluate a mathematical expression using Python\'s AST. \n \n Original by: J. Sebastian at http://stackoverflow.com/questions/2371436. \n Modifications by:  Brandon Amos . \n \n If you want a command-line expression evaluator, use\n Russell91/pythonpy . \n $ eval-expr.py \'(((4+6)*10)<<2)\'\n(((4+6)*10)<<2) = 400 \n python3/merge-mutt-contacts.py \n \n Authors:  Brandon Amos \n Created: 2014.01.08 \n \n Merges two mutt contact files. \n Similar Projects \n There are many potpourri Python script repositories on GitHub.\nThe following list shows a short sampling of projects,\nand I\'m happy to merge pull requests of other projects. \n Name | Stargazers | Description\n----|----|----\n averagesecurityguy/Python-Examples  | 26 | Example scripts for common python tasks\n ClarkGoble/Scripts  | 29 | My scripts - primarily using python and appscript\n computermacgyver/twitter-python  | 66 | Simple example scripts for Twitter data collection with Tweepy in Python\n gpambrozio/PythonScripts  | 38 | A bunch of Python scripts I made and that might interest somebody else\n realpython/python-scripts  | 568 | because i\'m tired of gists', "This repo contains the source for my CV: \n \n generate.py  creates a  website \n  and  PDF \n  from a shared  YAML source \n  by using Jinja templates. \n The publications are rendered from a single\n   BibTeX  file.\n  The abstracts are displayed in the website output\n  and the selected publications here are highlighted. \n The  YAML source  links to all author websites,\n  which will automatically be added to the\n  publication lists in the website and PDF. \n GitHub stars are automatically scraped and cached on disk. \n \n Building and running \n Dependencies are included in  requirements.txt  and can be installed\nusing  pip  with  pip3 install -r requirements.txt .\n make  will call  generate.py  and\nbuild the LaTeX documents with  latexmk  and  biber .\nThe Makefile can also: \n \n Stage to my website with  make stage , \n Start a local jekyll server of my website with updated\n  documents with  make jekyll , and \n Push updated documents to my website with  make push . \n \n What to modify \n Change the content in  cv.yaml .\nYou should also look through the template files to make sure there isn't any\nspecial-case code that needs to be modified.\nThe  Makefile  can also start a Jekyll server and push the\nnew documents to another repository with  make jekyll  and  make push . \n Warnings \n \n Strings in  cv.yaml  should be LaTeX (though, the actual LaTeX formatting\n   should be in the left in the templates as much as possible). \n If you do include any new LaTeX commands, make sure that one of the\n    REPLACEMENTS  in  generate.py  converts them properly. \n The LaTeX templates use modified Jinja delimiters to avoid overlaps with\n   normal LaTeX. See  generate.py  for details. \n \n Other people using this code \n You are welcome to use this code with or without attribution in the\ndocuments you produce, and add a link back here if you want! \n \n \n Alessandro Checco \n Alex Sludds \n Amara Dinesh Kumar \n Boyo Chen \n Chaitanya Ahuja  ( code ) \n Chaitanya Bapat \n Chieh Hubert Lin (林杰) \n Colin Clement  ( code ) \n Daniel Schaefer \n David B. Lindell  ( code ) \n Emir Ceyani  ( code ) \n Franziska Meier \n Guojin Chen \n Jean Nassar \n Joan Cano \n Juan Martín Loyola  ( code ) \n Jun Xiong \n Jérémie Lumbroso \n Krishnaditya Kancharla \n Lamin Juwara  ( code ) \n Lizeth Joseline Fuentes Pérez  ( code ) \n Marco Piccirilli \n Matthew L. Bendall \n Murali Koppula \n Nathan P. Lawrence  ( code ) \n Nazim Coskun \n Nicholas J. Loman \n Nikos Doulaveras \n Norman Kabir \n Nurpeiis Baimukan \n Olalekan Ogunmolu \n Pieter Vanderpol \n Prachi Sudrik \n Pınar Demetçi \n Qian Ge \n Renan Souza  ( code ) \n Stefan Doerr \n Steve T.K. Jan \n Swaminathan Gurumurthy \n Vinayakumar Ravi \n Wen-Yen Chang \n Wilka Carvalho \n Yann-Aël Le Borgne \n You-Feng Wu \n", 'About. \n Build LLVM, Polly, and PoCL in an Ubuntu 12.04 Docker image.\nThis provides 2 docker images for a stable LLVM 3.3 build\nand the latest LLVM 3.4 builds.\nSee the  llvm-3.3  and  llvm-3.4 .', "Github Wiki Link Validator \n Ensure all internal links in a Github Wiki are valid. \n Github wiki's present a unique problem for link validation because\n traditional validation \nis usually done by checking page response (usually for 404).\nHowever, the default behavior of a Wiki for an invalid link\nis to direct to a page creating the page, and there is currently\nno option to change this.\nCreating new pages like this is easy, but in large Wiki's\nlinks become difficult to manage and can lead to difficulties\nvalidating links. \n This is a Python 3.3 script to crawl a published Github wiki and\ndetect internal links pointing to invalid locations. \n Dependencies. \n \n Python 3.3 \n BeautifulSoup 4 \n \n Example. \n $ ./link-validator.py https://github.com/bamos/github-wiki-link-validator/wiki \n \n https://github.com/bamos/github-wiki-link-validator/wiki \n https://github.com/bamos/github-wiki-link-validator/wiki/links \n https://github.com/bamos/github-wiki-link-validator/wiki/invalid \n https://github.com/bamos/github-wiki-link-validator/wiki/B \n https://github.com/bamos/github-wiki-link-validator/wiki/C \n https://github.com/bamos/github-wiki-link-validator/wiki/Valid \n https://github.com/bamos/github-wiki-link-validator/wiki/C \n https://github.com/bamos/github-wiki-link-validator/wiki/D \n https://github.com/bamos/github-wiki-link-validator/wiki/E \n", "Beamer Snippets \n The  Beamer  package enables  LaTeX  to produce high quality\ntechnical presentations and can be used in combination with the\n TikZ  package for including publication-quality diagrams in presentations.\nHowever, formatting LaTeX presentations using Beamer and TikZ is\ndifficult and time consuming, so I often reuse code snippets from\nprevious presentations.\nThis repository contains a collection of snippets I'm gathering from\nmy papers and presentations, which are best viewed on\n this webpage .\nSee my other LaTeX projects at  bamos/cv  and\n bamos/latex-templates . \n \n Contributing. \n Contributions are highly welcomed!\nPlease add to the  snippets  directory, modify the\n preamble , and send a pull request.\nIf you substantially modify the preamble, I recommend keeping\nyour repo as a fork, and I'll keep a list of forks in this README. \n Webpage Generation Process. \n The Python 3 script  generate.py  produces a static website in  dist .\n generate.py  loops through the collection of snippets and uses\n Jinja  templates to output LaTeX documents.\n Makefile.slides  is then used to generate PNG images from\neach slide, and Jinja HTML templates are used to create  index.html . \n Grunt \n Grunt  is used to deploy  dist  to  Github pages \nin the  gh-pages  branch with the  grunt-build-control  plugin.\nThe  npm  dependencies are managed in  package.json \nand can be installed with  npm install . \n \n grunt generate  produces the static site in  dist , and \n grunt deploy  pushes the  dist  directory to the  gh-pages  branch. \n \n Licensing. \n All LaTeX portions are released into the public domain,\nand other portions are under the MIT license.", 'Zsh History Analysis \n zsh  logs commands and timestamps to  ~/.zsh_history  for\nshell features such as reverse history search.\nThis repository is a fun project that provides shell, Python, and R\nscripts to parse, analyze, visualize  .zsh_history  files.\nThese scripts can be extended to support Bash\'s  .bash_history . \n Getting Started \n You can run this on your  .zsh_history  files by cloning this repository\nwith  git clone https://github.com/bamos/zsh-history-analysis.git \nand installing the following prerequisites.\nEnsure you have increased the history file size so commands aren\'t removed.\nThen, follow the steps in  Control Flow  to generate the plots. \n Prerequisites \n \n PATH  contains  python3  and  Rscript , which can be installed from\n  your package manager.\n  In Arch Linux, the required packages are\n   python \n  and  r . \n R :  ggplot2  and\n   reshape  are installed\n  from an R shell with  install.packages("ggplot2") \n  and  install.packages("reshape") . \n \n Increasing the History File Size \n Unfortunately, zsh\'s default history file size is limited to\n10000 lines by default and will truncate the history to this\nlength by deduplicating entries and removing old data. \n Adding the following lines to  .zshrc  will remove the limits and\ndeduplication of the history file. \n Bash\nexport HISTSIZE=1000000000\nexport SAVEHIST=$HISTSIZE\nsetopt EXTENDED_HISTORY \n Control Flow \n The following is the control flow for generating plots. \n \n Archive all  .zsh_history  files in  data/<server>.zsh_history .\n ./pull-history-data.sh  is a script to partially help archiving the data\nthat will pull files from a list of servers separated by newlines in a\nfile named  servers . \n Run  ./analyze.py  to analyze the raw data files.\n ./analyze.py --help  will provide a help menu with the supported options. \n Run  ./plot.r  to generate plots from the analyzed data. \n \n Sample Results \n Command Frequencies \n At a given hour or weekday, how frequently do I run commands?\nThe following shows the average number of commands executed\nfor each hour and weekday.\nI average 10 commands per hour overnight and\na little more during the day, and Wednesdays seem to be\nmy least productive days. \n \n \n Many hours have 0 commands executed since I\'m not typing commands every hour of\nevery day, so these points have a high standard deviation.\n Empirical Cumulative Distribution Functions (ECDF\'s) \nprovide a deeper visualization of the distributions. \n \n \n Average command length \n \n What command was over 100 characters!?\n analyze.py  will output the top five commands, and these\nlong commands are from using the full path to an executable,\nsuch as the Android ARM cross compiler, as shown in the following output. \n Bash\n$ ./analyze.py commandLengths\n  105: /opt/android-ndk-r9/toolchains/arm-linux-androideabi-4.8/prebuilt/linux-x86/bin/arm-linux-androideabi-gcc \n Scoping into the majority of the data shows that almost 50% of my\ncommands are one or two characters. \n \n Top Commands \n Since almost 50% of my commands are one or two characters,\nwhat are the top commands?\nThe following plot shows the top commands are Linux utilities\nand  oh-my-zsh  aliases. \n \n My Undergrad vs Grad Student Frequencies \n \n', 'GitHub README Link Checker \n \n  ![](https://raw.githubusercontent.com/bamos/girl/master/screenshot.png)  \n girl  is a  Gi thub  R eadme  L ink Checker\nserved over HTTP with  Scala \nand  Spray .\nA public version is hosted at  http://bamos.github.io/girl . \n Whitelist \n To prevent misuse, girl restricts usage to\nGitHub users with\nover 50 followers or users and organizations on the\n whitelist .\nPlease add your accounts\nand submit a pull request to gain access.\nThanks! \n Building \n Before running locally,\nset your  GitHub API token \nin the environment variable  GITHUB_TOKEN ,\nor modify the GitHub API connection in\n Girl.scala \nto another option from\n kohsuke.github.GitHub .\nAlso in  Girl.scala , if desired, set the minimum number of\nrequired followers to zero:  val reqFollowers = 0 . \n girl  is built with  sbt .\nExecuting  sbt run  from the  girl  directory will download\nthe dependencies, compile the source code, and start\nan HTTP server on  0.0.0.0:8585 .\n Main.scala \nconfigures the interface and port. \n sbt-revolver  is helpful for development.\nStart an  sbt  shell and execute  ~re-start ,\nwhich re-compiles and restarts the server upon source code changes. \n Deployment with Docker \n Girl can be deployed as a container with  Docker .\nAfter replacing the string  <token>  in the\n Dockerfile \nwith your GitHub API token, the following command\nwill build and start the girl as an HTTP server on port 8585\nof the container. \n docker build -t girl . \n Running as a System Service \n girl.service \nis an example  systemd \nservice that calls\n start-service.sh \nto automatically start girl with the system. \n Modify the paths to this repo on your system in both of the scripts\nand copy  girl.service  to  /etc/systemd/system/girl.service .\nA symlink will not work, see\n this bug report \nfor more details. \n Basic controls are: \n sudo systemctl start girl\nsudo systemctl stop girl\nsudo systemctl restart girl \n And run on startup with: \n sudo systemctl enable girl \n Licensing \n All portions are  MIT-licensed .', '\n \n SnowGlobe provides minimal-configuration web analytics\nfor small-scale and personal websites. \n SnowGlobe integrates components from the\n Snowplow analytics framework  to achieve this.\nThe  JavaScript tracker  pipes into the\n Scala collector  and  Scala enricher \nand outputs  Snowplow enriched events \nto a tab separated file. \n SnowGlobe uniquely provides Haskell-driven analytics on the data. \n Crafted by  Brandon Amos . \n Motivation: Why create SnowGlobe? \n I\'ve used  Google Analytics ,\n Piwik , and  Clicky \nfor my personal website analytics over the last 3 years.\nThese are great systems and I still use some of them for\nthe web interfaces and other analytics.\nHowever, I enjoy hacking on the systems I use and\nfelt limited. \n My ongoing goals in building SnowGlobe are to: \n \n Pipe a short daily summary of my real-time analytics to my LCD display. \n Email me daily, weekly, monthly, and yearly reports with\n   the information I want. \n Create interesting real-time visualizations (not started) \n \n Why not use SQL to query the data? \n You should.\nI\'m using this project to learn Haskell. \n Progress \n Real-time summary on an LCD display \n Somebody in my research group gave me a small LCD display\na few weeks ago (in May 2015) and said that he couldn\'t\nfind any useful information to put on it,\nThis, along with my other goals, inspired me to finally\ncreate SnowGlobe. \n Piping a daily summary to my LCD display with SnowGlobe works well.\nI\'ve replaced my actual stats with the string  NNNN | MMM , which\nrepresents that I\'ve had  NNNN  total page views by  MMM  visitors.\nIf you\'re curious, the other information on the device are the\nnumber of tasks I have (for today, tomorrow, and the next day),\nand the temperature and amount of GPU memory I\'m currently\nusing for experiments. \n \n Recurring Reports \n Daily recurring reports are also working well.\nI plan to expand the information in the weekly reports\nand will add monthly and yearly reports in the future.\nThe following screenshot shows an example report\nfrom my email client,  mutt .\nMore information on setting these reports up below. \n \n Prerequisites, installing, and configuration \n \n Install wget, Python, sbt, scala, and the JRE (≥ 1.7). \n If the default Java distribution is not version 1.7 or above,\n   set  JAVA_HOME  in  scripts/env.sh . \n Run  ./scripts/bootstrap.sh  to download Snowplow and GeoLite binaries. \n Update the  uri  in  ip_lookups.json \n   to use the absolute path to your repository. \n \n Collecting and storing events to TSV files \n On the server, start the collector and enricher with the following\ninstructions.\nFor persistent collection, use an init daemon (preferred),\nor a detached screen or tmux session. \n \n conf/collector.conf  and  conf/enrich.conf  contain sensible default\n   configurations for Snowplow\'s collector and enricher. \n scripts/start-collect-enrich.sh  will start Snowplow\'s collector on port\n  8081 to pipe Base 64 serialized Thrift raw event output to Snowplow\'s\n  Scala enricher.\n  The enricher will output  Snowplow enriched events \n  as rows in  data/events.tsv , which will use ~720 bytes per event.\n  This script uses  stdbuf  from GNU coreutils to disable stdout\n  buffering. \n snowglobe.service  is a\n   systemctl unit \n  to run  start-collect-enrich.sh .\n  Copy the config with  sudo cp sample/snowglobe.service /etc/systemd/system/ \n  and reload the units with  sudo systemctl daemon-reload .\n  Do not use a symlink because of\n   this  systemd behavior/bug.\n  Start the service with  sudo systemctl start  and\n  run on boot with  sudo systemctl enable . \n \n Adding JavaScript tags to your webpages \n Next, ensure that the collector and enricher are properly configured\nand started by opening  sample/index.html  on the server.\n data/events.tsv  should now contain the tab separated event. \n Next, copy and paste the following code to your webpage\'s\ntemplates to send events on the pages you wish to track,\nand add Snowplow\'s JavaScript library  sp.js  to your website\'s\nresource directory.\nMake sure to change  localhost:8081  to your server and port,\nand ensure the port is open. \n ```JavaScript \n \n \ntry {\n  // Use localhost as the server for testing on the same computer,\n  // but change to your deployed server IP address or hostname\n  // for production.\n  var snowplowTracker = Snowplow.getTrackerUrl(\'localhost:8081\');\n  snowplowTracker.enableLinkTracking();\n  snowplowTracker.trackPageView();\n} catch (err) {}\n \n ``` \n Analytics \n Main.hs \nis the entry point for analytics.\nThis is a single-threaded application and not ready for large deployments,\nbut easy to make small changes to.\n optparse-applicative \nparses command-line arguments\nand\n cassava \nparses the raw events file into an\n EnrichedEvent , defined in\n EnrichedEvent.hs ,\nand contains features of SnowPlow\'s\n EnrichedEvent.scala . \n EnrichedEvent.hs ,\nfurther enriches events by providing\n getLocation  and  getOrganization  functions,\nwhich use\n hs-GeoIP \nand\n whois-hs .\nSnowPlow\'s enricher should add this information to events,\nbut there\'s a subtle bug in my configuration\nor the software that I\'ll further track\ndown in issue\n #4 \nsometime. \n Queries.hs \ngroups, filters, and extracts information from the events. \n Building \n To build the analytics binary, first install  GHC , the\nGlasgow Haskell Compiler, and  cabal , a build system\nand library database:\n  + OSX:  brew install ghc cabal-install \n  + Ubuntu:  apt-get install ghc cabal-install \n  + Arch:  pacman -S ghc cabal-install \n Next,  cd  into the  snowglobe  directory and\ninstall the Haskell dependencies and build\nthe code with:  cabal install .\nThe binary  snowglobe-analytics  is now\nin  ~/.cabal/bin/snowglobe-analytics .\nYou can add  cabal s bin directory to your  PATH  or use\nthe full path. \n Command Line Interface \n ```\n$ snowglobe-analytics\nSnowGlobe Analytics \n Usage: snowglobe-analysis --events FILE COMMAND \n Available options:\n-h,--help                Show this help text\n--events FILE            Location of events.tsv \n Available commands:\nDaySummary               Print a concise summary for the current day.\nDayReport                Print a report for the current day.\nWeekReport               Print a report for the past week.\n``` \n DaySummary: Status on LCD \n Using  snowglobe-analytics  in  DaySummary  mode on the command\nline outputs the following, showing  NNNN  total page views by  MMM  visitors. \n $ snowglobe-analysis --events data/events.tsv DaySummary\nNNNN | MMM \n I pipe this output to my LCD display with\n LCD4Linux \nby defining the following widget in  /etc/lcd4linux.conf .\nLCD4Linux\'s\n exec documentation \nsays to use  exec  as a last resort,\nbut the following is working well for me so far. \n Widget Hits {\n    class \'Text\'\n    expression exec(\'su bamos -c "/home/bamos/.cabal/bin/snowglobe-analysis --events /home/bamos/repos/snowglobe/data/events.tsv DaySummary"\',10000)\n    width 20\n    align \'R\'\n    prefix \'web\'\n    update 1000\n} \n In the layout, define a row to use the output from this widget. \n Layout stats {\n  ...\n  Row2.Col1 \'Hits\'\n  ...\n} \n Recurring Email Reports \n The  DayReport  mode outputs the report in plaintext to stdout.\nPipe this output to your favorite command-line mailer\nand add it to your favorite job scheduler.\nI use  mutt  and vanilla  cron , respectively. \n The following  cron  entry calls\n analytics-daily-report.sh  and\n analytics-weekly-report.sh . \n 59 23 * * * /home/bamos/repos/snowglobe/scripts/analytics-daily-report.sh\n59 23 * * 0 /home/bamos/repos/snowglobe/scripts/analytics-weekly-report.sh \n Licensing \n SnowGlobe portions copyright 2014-2015 Brandon Amos under the\nApache License. \n Snowplow portions retain their licenses from Snowplow Analytics Ltd.\nand remain unmodified. \n The  original SnowGlobe graphic \nis open-sourced under\n CC-SA 2.0 ,\nand my modifications are under the same license.', "Subscribe for Weekly Emails \n Crafted by  Brandon Amos . \n \n \n This repository contains my minimal-maintenance conference tracker\nand an opinionated list of CS conferences.\nI find this slightly easier than keeping the dates on Google Calendar\nbecause I can glance at the entire list, keep track of URLs,\nand add reminders when I need to add next year's conference. \n \n Project Structure \n The Python source in  report.py  generates a plaintext\nreport from the  YAML data .\nI have a  Cron  job send\nweekly emails with  email-report.sh \nand  mutt . \n Adding and Updating Data \n As conferences pass and the data becomes outdated,\nI'm happy to accept pull requests with updated data.\nIf you want to track yours by adding/removing conferences,\nplease fork this repo. \n Conference Lists \n \n Wikipedia \n RichardLitt/awesome-conferences \n WikiCFP: CS \n", "ecos.torch •            \n Unofficial ECOS bindings to solve linear programs (LPs) and\nsecond-order cone programs (SOCPs) in Torch. \n \n You may also be interested in:\n+ Torch  Gurobi  bindings at\n   bamos/gurobi.torch .\n+ A Torch library for convex optimization that implements\n  spectral projected gradient and projected gradient descent at\n   bamos/cvx-optim.torch . \n Introduction to ECOS \n Visit http://www.embotech.com/ECOS for detailed information on ECOS. \n ECOS is a numerical software for solving convex second-order cone programs (SOCPs) of type \n min  c'*x\ns.t. A*x = b\nG*x <=_K h \n where the last inequality is generalized, i.e.  h - G*x  belongs to the cone  K .\nECOS supports the positive orthant  R_+ , second-order cones  Q_n  defined as\n Q_n = { (t,x) | t >= || x ||_2 } \nwith t a scalar and  x  in  R_{n-1} ,\nand the exponential cone  K_e  defined as \n K_e = closure{(x,y,z) | exp(x/z) <= y/z, z>0} \n where  (x,y,z)  is in  R_3 .\nThe cone  K  is therefore a direct product of the positive orthant,\nsecond-order, and exponential cones: \n K = R_+ x Q_n1 x ... x Q_nN x K_e x ... x K_e \n This Library \n This repository provides unofficial  Torch  bindings to\nthe  ECOS C API . \n Setup \n After  setting up Torch ,\nthis library can be installed with: \n bash\nluarocks install https://github.com/bamos/ecos.torch/raw/master/ecos-scm-1.rockspec \n or equivalently: \n bash\ngit clone https://github.com/bamos/ecos.torch.git --recursive\ncd ecos.torch\nluarocks make \n Usage \n Linear Program \n ```lua\nlocal ecos = require 'ecos' \n local G = torch.Tensor{{-1, 1}, {-1, -1}, {0, -1}, {1, -2}}\nlocal h = torch.Tensor{1.0, -2.0, 0.0, 4.0}\nlocal c = torch.Tensor{2.0, 1.0} \n local status, x = ecos.solve{c=c, G=G, h=h}\nprint(x) -- Optimal x is [0.5, 1.5]\n``` \n Tests \n After installing the library with  luarocks , our tests in\n test.lua \ncan be run with  th test.lua . \n Issues and Roadmap \n \n While this is working well for linear programs, I am not using\n   and have not tested the second-order or exponential cones.\n   Please comment in\n    Issue 1 \n   if you are interested in using these. \n ECOS uses a sparse matrix format and the code current converts\n   a dense Torch tensor to a sparse array in C in every call.\n   This could potentially be optimized by using sparse Torch\n   tensors and doing the conversion in Torch.\n   Then the user could then optionally maintain sparse\n   Tensors in Torch.\n   Please comment in\n    Issue 2 \n   if you are interested in this. \n \n License \n \n ECOS is under the GPL and remains unmodified. \n The original code in this repository (the ECOS bindings) is\n   Apache-licensed . \n", 'gurobi.torch •            \n Unofficial Gurobi Torch bindings. \n \n You may also be interested in:\n+ Torch  ECOS  bindings at\n   bamos/ecos.torch .\n+ A Torch library for convex optimization that implements\n  spectral projected gradient and projected gradient descent at\n   bamos/cvx-optim.torch . \n Installation \n \n Set  GUROBI_HOME  in your shell\'s current session (with  export GUROBI_HOME=.. )\n   and initialization file ( .bashrc  or  .zshrc ). \n luarocks make \n \n Usage \n The following solves the linear program \n min  c\'*x s.t. G*x <= h \n Linear Program \n ```lua\nlocal gurobi = require \'gurobi\' \n local G = torch.Tensor{{-1, 1}, {-1, -1}, {0, -1}, {1, -2}}\nlocal h = torch.Tensor{1.0, -2.0, 0.0, 4.0}\nlocal c = torch.Tensor{2.0, 1.0} \n local env = gurobi.loadenv("")\nlocal model = gurobi.newmodel(env, "", c)\ngurobi.addconstrs(model, G, \'LE\', h)\nlocal status, x = gurobi.solve(model)\nprint(x) -- Optimal x is [0.5, 1.5] \n gurobi.free(env, model)\n``` \n Linear Program with Incrementally Added Constraints \n ```lua\nlocal gurobi = require \'gurobi\' \n -- minimize y\n-- subject to y >= x\n--            y >= -x\n--            y >= x + 1\nlocal c = torch.Tensor{0.0, 1.0}\nlocal G = torch.Tensor{{1, -1}, {-1, -1}, {1, -1}}\nlocal h = torch.Tensor{0.0, 0.0, -1.0} \n local env = gurobi.loadenv("")\nlocal model = gurobi.newmodel(env, "", c) \n local I = {{1,2}}\ngurobi.addconstrs(model, G[I], \'LE\', h[I]) \n local status, x = gurobi.solve(model)\nprint(x) -- Optimal at this point is [0, 0] \n gurobi.addconstr(model, G[3], \'LE\', h[3])\nstatus, x = gurobi.solve(model)\nprint(x) -- Optimal at this point is [-0.5, 0.5] \n gurobi.free(env, model)\n``` \n OpenMP Parallel Solves \n ```lua\nlocal env = gurobi.loadenv("")\nlocal model1 = gurobi.newmodel(env, "", c)\ngurobi.addconstrs(model1, G, \'LE\', h) \n local model2 = gurobi.newmodel(env, "", c)\ngurobi.addconstrs(model2, G, \'LE\', h) \n local status, xs = gurobi.solvePar({model1, model2})\n``` \n Tests \n After installing the library with  luarocks , our tests in\n test.lua \ncan be run with  th test.lua . \n Licensing \n \n Gurobi is proprietary software. \n The original code in this repository (the Gurobi bindings) is\n   Apache-licensed . \n', "Image Completion with Deep Learning in TensorFlow \n \n \n See my blog post for more details and usage instructions . \n This repository implements Raymond Yeh and Chen Chen et al.'s paper\n   Semantic Image Inpainting with Perceptual and Contextual Losses . \n Most of the code in this repository was written by modifying a\n  duplicate of  Taehoon Kim's \n DCGAN-tensorflow  project,\n  which is MIT-licensed.\n  My modifications are also  MIT-licensed . \n The  ./checkpoint  directory contains a pre-trained\n  model for faces, trained on the CelebA dataset for 20 epochs. \n \n Citations \n Please consider citing this project in your\npublications if it helps your research.\nThe following is a  BibTeX \nand plaintext reference.\nThe BibTeX entry requires the  url  LaTeX package. \n ```\n@misc{amos2016image,\n    title        = {{Image Completion with Deep Learning in TensorFlow}},\n    author       = {Amos, Brandon},\n    howpublished = {\\url{http://bamos.github.io/2016/08/09/deep-completion}},\n    note         = {Accessed: [Insert date here]}\n} \n Brandon Amos. Image Completion with Deep Learning in TensorFlow.\nhttp://bamos.github.io/2016/08/09/deep-completion.\nAccessed: [Insert date here]\n```", "Block •    \n An intelligent block matrix library for numpy, PyTorch, and beyond.\nCrafted by  Brandon Amos  with significant\ncontributions by  Eric Wong . \n \n Why do we need an intelligent block matrix library? \n Let's try to construct the KKT matrix from Mattingley and Boyd's\n CVXGEN \npaper in numpy and PyTorch: \n \n Without  block , there is no way to infer the appropriate sizes of\nthe zero and identity matrix blocks.\nIt is an inconvenience to think about what size these\nmatrices should be. \n \n What does  block  do? \n Block acts a lot like  np.bmat  and replaces: \n \n Any constant with an appropriately shaped block matrix\n  filled with that constant. \n The string  'I'  with an appropriately shaped identity matrix. \n The string  '-I'  with an appropriately shaped negated identity matrix. \n [Request more features.] \n \n Isn't constructing large block matrices with a lot of zeros inefficient? \n Yes,  block  is meant to be a quick prototyping tool and\nthere's probably a more efficient way to solve your system\nif it has a lot of zeros or identity elements. \n How does  block  handle numpy and PyTorch with the same interface? \n I wrote the logic to handle matrix sizing to be agnostic\nof the matrix library being used.\nnumpy and PyTorch are just backends.\nMore backends can easily be added for your favorite\nPython matrix library. \n ```Python\nclass Backend(metaclass=ABCMeta): \n @abstractmethod\ndef extract_shape(self, x): pass\n\n@abstractmethod\ndef build_eye(self, n): pass\n\n@abstractmethod\ndef build_full(self, shape, fill_val): pass\n\n@abstractmethod\ndef build(self, rows): pass\n\n@abstractmethod\ndef is_complete(self, rows): pass\n \n ``` \n Getting Started \n \n Install:  pip install block \n Usage:  from block import block \n Run tests in  test.py :  nosetests test.py \n \n Issues and Contributions \n I'd be happy to hear from you about any issues or features you\nadd, please  file an issue \nor  send in a PR . \n Licensing \n This repository is\n Apache-licensed .", "A PyTorch Implementation of DenseNet \n This is a  PyTorch  implementation of the\nDenseNet-BC architecture as described in the\npaper  Densely Connected Convolutional Networks \nby G. Huang, Z. Liu, K. Weinberger, and L. van der Maaten.\nThis implementation gets a CIFAR-10+ error rate of\n4.77 with a 100-layer DenseNet-BC with a growth rate of 12.\nTheir official implementation and links to many other\nthird-party implementations are available in the\n liuzhuang13/DenseNet \nrepo on GitHub. \n \n Why DenseNet? \n As this table from the DenseNet paper shows, it provides\ncompetitive state of the art results on CIFAR-10,\nCIFAR-100, and SVHN. \n \n Why yet another DenseNet implementation? \n PyTorch is a great new framework and it's nice to have these\nkinds of re-implementations around so that they can be integrated\nwith other PyTorch projects. \n How do you know this implementation is correct? \n Interestingly while implementing this, I had a lot of\ntrouble getting it to converge and looked at every part\nof the code closer than I usually would.\nI compared all of the model's hidden states and gradients\nwith the official implementation to make sure my code was correct\nand even trained a VGG-style network on CIFAR-10 with the\ntraining code here.\nIt turns out that I uncovered a new critical PyTorch\nbug (now fixed) that was causing this. \n I have left around my original message about how this\nisn't working and the things that I have checked\n in this document .\nI think this should be interesting for other people to\nsee my development and debugging strategies when\nhaving issues implementing a model that's known\nto converge.\nI also started\n this PyTorch forum thread ,\nwhich has a few other discussion points.\nYou may also be interested in\n my script that\ncompares PyTorch gradients to Torch gradients \nand\n my script that numerically checks PyTorch gradients . \n My convergence issues were due to a critical PyTorch bug\nrelated to using  torch.cat  with convolutions with cuDNN\nenabled (which it is by default when CUDA is used).\nThis bug caused incorrect gradients and the fix to\nthis bug is to disable cuDNN (which doesn't have\nto be done anymore because it's fixed).\nThe oversight in my debugging strategies that caused me to\nnot find this error is that I did not think to disable cuDNN.\nUntil now, I have assumed that the cuDNN option in frameworks\nare bug-free, but have learned that this is not always the case.\nI may have also found something if I would have numerically\ndebugged  torch.cat  layers with convolutions instead of\nfully connected layers. \n Adam fixed the PyTorch bug that caused this in\n this PR \nand has been merged into Torch's master branch.\n If you are interested in using the DenseNet code in\nthis repository, make sure your PyTorch version\ncontains  this PR \nand was downloaded after 2017-02-10. \n What does the PyTorch compute graph of the model look like? \n You can see the compute graph  here ,\nwhich I created with  make_graph.py ,\nwhich I copied from\n Adam Paszke's gist .\nAdam says PyTorch will soon have a better way to create\ncompute graphs. \n How does this implementation perform? \n By default, this repo trains a 100-layer DenseNet-BC with\nan growth rate of 12 on the CIFAR-10 dataset with\ndata augmentations.\nDue to GPU memory sizes, this is the largest model I am able to run.\nThe paper reports a final test error of 4.51 with this\narchitecture and we obtain a final test error of 4.77. \n \n Why don't people use ADAM instead of SGD for training ResNet-style models? \n I also tried training a net with ADAM and found that it didn't\nconverge as well with the default hyper-parameters compared\nto SGD with a reasonable learning rate schedule. \n \n What about the non-BC version? \n I haven't tested this as thoroughly, you should make sure\nit's working as expected if you plan to use and modify it.\nLet me know if you find anything wrong with it. \n A paradigm for ML code \n I like to include a few features in my projects\nthat I don't see in some other re-implementations\nthat are present in this repo.\nThe training code in  train.py  uses  argparse  so the batch size\nand some other hyper-params can easily be changed\nand as the model is training, progress is written\nout to csv files in a work directory also defined\nby the arguments.\nThen a separate script  plot.py  plots the\nprogress written out by the training script.\nThe training script calls  plot.py  after every epoch,\nbut it can importantly be run on its own so figures\ncan be tweaked without re-running the entire experiment. \n Help wanted: Improving memory utilization and multi-GPU support \n I think there are ways to improve the memory utilization\nin this code as in the\n the official space-efficient Torch implementation .\nI also would be interested in multi-GPU support. \n Running the code and viewing convergence \n First install PyTorch (ideally in an anaconda3 distribution).\n ./train.py  will create a model, start training it,\nand save progress to  args.save , which is\n work/cifar10.base  by default.\nThe training script will call  plot.py  after\nevery epoch to create plots from the saved progress. \n Citations \n The following is a  BibTeX \nentry for the DenseNet paper that you should cite\nif you use this model. \n @article{Huang2016Densely,\n  author = {Huang, Gao and Liu, Zhuang and Weinberger, Kilian Q.},\n  title = {Densely Connected Convolutional Networks},\n  journal = {arXiv preprint arXiv:1608.06993},\n  year = {2016}\n} \n If you use this implementation, please also consider citing this implementation and\ncode repository with the following BibTeX or plaintext entry.\nThe BibTeX entry requires the  url  LaTeX package. \n ```\n@misc{amos2017densenet,\n  title = {{A PyTorch Implementation of DenseNet}},\n  author = {Amos, Brandon and Kolter, J. Zico},\n  howpublished = {\\url{https://github.com/bamos/densenet.pytorch}},\n  note = {Accessed: [Insert date here]}\n} \n Brandon Amos, J. Zico Kolter\nA PyTorch Implementation of DenseNet\nhttps://github.com/bamos/densenet.pytorch.\nAccessed: [Insert date here]\n``` \n Licensing \n This repository is\n Apache-licensed .", "setGPU \n A small Python library that automatically sets  CUDA_VISIBLE_DEVICES \nto the least-loaded GPU on multi-GPU systems and can be used by: \n \n Putting  import setGPU  before any import\n   that will use a GPU like Torch, TensorFlow, or JAX. \n Defining an alias such as\n    alias setGPU='eval $(python3 -m setGPU)' \n   and calling that to set the GPU in the shell before running\n   another program that uses the GPU. \n \n Installation \n pip install git+https://github.com/bamos/setGPU.git \n Dependencies \n \n Jongwook Choi's   gpustat  library ( pip install gpustat ) \n \n Licensing \n This code is in the public domain.", '\n \n diffcp \n diffcp  is a Python package for computing the derivative of a convex cone program, with respect to its problem data. The derivative is implemented as an abstract linear map, with methods for its forward application and its adjoint.  \n The implementation is based on the calculations in our paper  Differentiating through a cone program . \n Installation \n diffcp  is available on PyPI, as a source distribution. Install it with \n bash\npip install diffcp \n You will need a C++11-capable compiler to build  diffcp . \n diffcp  requires:\n*  NumPy  >= 1.15\n*  SciPy  >= 1.10\n*  SCS  >= 2.0.2\n*  pybind11  >= 2.4\n*  threadpoolctl  >= 1.1\n* Python 3.x \n diffcp  uses Eigen; Eigen operations can be automatically vectorized by compilers. To enable vectorization, install with \n bash\nMARCH_NATIVE=1 pip install diffcp \n OpenMP can be enabled by passing extra arguments to your compiler. For example, on linux, you can tell gcc to activate the OpenMP extension by specifying the flag "-fopenmp": \n bash\nOPENMP_FLAG="-fopenmp" pip install diffcp \n To enable both vectorization and OpenMP (on linux), use \n bash\nMARCH_NATIVE=1 OPENMP_FLAG="-fopenmp" pip install diffcp \n Cone programs \n diffcp  differentiates through a primal-dual cone program pair. The primal problem must be expressed as  \n minimize        c\'x\nsubject to      Ax + s = b\n                s in K \nwhere   x  and  s  are variables,  A ,  b  and  c  are the user-supplied problem data, and  K  is a user-defined convex cone. The corresponding dual problem is \n minimize        b\'y\nsubject to      A\'y + c == 0\n                y in K^* \n with dual variable  y . \n Usage \n diffcp  exposes the function \n python\nsolve_and_derivative(A, b, c, cone_dict, warm_start=None, **kwargs). \n This function returns a primal-dual solution  x ,  y , and  s , along with\nfunctions for evaluating the derivative and its adjoint (transpose).\nThese functions respectively compute right and left multiplication of the derivative\nof the solution map at  A ,  b , and  c  by a vector.\nIn the case that the problem is not solved, i.e. SCS returns something\nother than "Solved" or "Solved/Innacurate" for status, we raise\na  SolverError  Exception. \n Arguments \n The arguments  A ,  b , and  c  correspond to the problem data of a cone program.\n*  A  must be a  SciPy sparse CSC matrix .\n*  b  and  c  must be NumPy arrays.\n*  cone_dict  is a dictionary that defines the convex cone  K .\n*  warm_start  is an optional tuple  (x, y, s)  at which to warm-start SCS.\n*  **kwargs  are keyword arguments to forward to SCS (e.g.,  verbose=True ). \n These inputs must conform to the  SCS convention  for problem data. The keys in  cone_dict  correspond to the cones, with\n*  diffcp.ZERO  for the zero cone,\n*  diffcp.POS  for the positive orthant,\n*  diffcp.SOC  for a product of SOC cones,\n*  diffcp.PSD  for a product of PSD cones, and\n*  diffcp.EXP  for a product of exponential cones. \n The values in  cone_dict  denote the sizes of each cone; the values of  diffcp.SOC ,  diffcp.PSD , and  diffcp.EXP  should be lists. The order of the rows of  A  must match the ordering of the cones given above. For more details, consult the  SCS documentation . \n Return value \n The function  solve_and_derivative  returns a tuple \n python\n(x, y, s, derivative, adjoint_derivative) \n \n \n x ,  y , and  s  are a primal-dual solution. \n \n \n derivative  is a function that applies the derivative at  (A, b, c)  to perturbations  dA ,  db ,  dc . It has the signature \n derivative(dA, db, dc) -> dx, dy, ds , where  dA  is a SciPy sparse CSC matrix with the same sparsity pattern as  A , and  db  and  dc  are NumPy arrays.  dx ,  dy , and  ds  are NumPy arrays, approximating the change in the primal-dual solution due to the perturbation. \n \n \n adjoint_derivative  is a function that applies the adjoint of the derivative to perturbations  dx ,  dy ,  ds . It has the signature \n adjoint_derivative(dx, dy, ds) -> dA, db, dc , where  dx ,  dy , and  ds  are NumPy arrays. \n \n \n Example \n ```python\nimport numpy as np\nfrom scipy import sparse \n import diffcp \n cone_dict = {\n    diffcp.ZERO: 3,\n    diffcp.POS: 3,\n    diffcp.SOC: [5]\n} \n m = 3 + 3 + 5\nn = 5 \n A, b, c = diffcp.utils.random_cone_prog(m, n, cone_dict)\nx, y, s, D, DT = diffcp.solve_and_derivative(A, b, c, cone_dict) \n evaluate the derivative \n nonzeros = A.nonzero()\ndata = 1e-4 * np.random.randn(A.size)\ndA = sparse.csc_matrix((data, nonzeros), shape=A.shape)\ndb = 1e-4 * np.random.randn(m)\ndc = 1e-4 * np.random.randn(n)\ndx, dy, ds = D(dA, db, dc) \n evaluate the adjoint of the derivative \n dx = c\ndy = np.zeros(m)\nds = np.zeros(m)\ndA, db, dc = DT(dx, dy, ds)\n``` \n For more examples, including the SDP example described in the paper, see the  examples  directory. \n Citing \n If you wish to cite  diffcp , please use the following BibTex: \n ```\n@article{diffcp2019,\n    author       = {Agrawal, A. and Barratt, S. and Boyd, S. and Busseti, E. and Moursi, W.},\n    title        = {Differentiating through a Cone Program},\n    journal      = {Journal of Applied and Numerical Optimization},\n    year         = {2019},\n    volume       = {1},\n    number       = {2},\n    pages        = {107--115},\n} \n @misc{diffcp,\n    author       = {Agrawal, A. and Barratt, S. and Boyd, S. and Busseti, E. and Moursi, W.},\n    title        = {{diffcp}: differentiating through a cone program, version 1.0},\n    howpublished = {\\url{https://github.com/cvxgrp/diffcp}},\n    year         = 2019\n}\n``` \n The following thesis concurrently derived the mathematics behind differentiating cone programs.\n @phdthesis{amos2019differentiable,\n  author       = {Brandon Amos},\n  title        = {{Differentiable Optimization-Based Modeling for Machine Learning}},\n  school       = {Carnegie Mellon University},\n  year         = 2019,\n  month        = May,\n}', "Differentiable Optimization-Based Modeling for Machine Learning \n \n This repository is by  Brandon Amos \n  and contains the full source code and data to produce\n   my thesis document . \n The slides are available in\n   pdf \n  and\n   pptx \n  format. \n \n Unpublished work in this thesis \n \n Chapter 2 \n  provides some preliminaries and background information on differentiable convex\n  optimization layers, including derivations for the optimization (or variational)\n  viewpoints of the ReLU, sigmoid, and softmax. \n Chapter 7 \n  presents an early version of differentiable CVXPY layers,\n  which is now available  here .\n  As a bibliographic note, the cone program differentiation derivation\n  in section 7.3 here remains unpublished in this thesis and was done\n  concurrent to and independent of\n   Differentiating Through a Cone Program . \n \n Publications behind this thesis \n Some of the content here is behind these publications: \n \n \n \n Differentiable Convex Optimization Layers \nA. Agrawal*,  B. Amos* , S. Barratt*, S. Boyd*, S. Diamond*, and J. Kolter* \nNeurIPS 2019 \n[1] [ pdf ]  [ code ]  \n \n \n \n \n Differentiable MPC for End-to-end Planning and Control \n B. Amos , I. Rodriguez, J. Sacks, B. Boots, and J. Kolter \nNeurIPS 2018 \n[2] [ pdf ]  [ code ]  \n \n \n \n \n Depth-Limited Solving for Imperfect-Information Games \nN. Brown, T. Sandholm, and  B. Amos \nNeurIPS 2018 \n[3] [ pdf ]  \n \n \n \n \n Learning Awareness Models \n B. Amos , L. Dinh, S. Cabi, T. Rothörl, S. Colmenarejo, A. Muldal, T. Erez, Y. Tassa, N. de Freitas, and M. Denil \nICLR 2018 \n[4] [ pdf ]  \n \n \n \n \n Task-based End-to-end Model Learning \nP. Donti,  B. Amos , and J. Kolter \nNeurIPS 2017 \n[5] [ pdf ]  [ code ]  \n \n \n \n \n OptNet: Differentiable Optimization as a Layer in Neural Networks \n B. Amos  and J. Kolter \nICML 2017 \n[6] [ pdf ]  [ code ]  \n \n \n \n \n Input Convex Neural Networks \n B. Amos , L. Xu, and J. Kolter \nICML 2017 \n[7] [ pdf ]  [ code ]  \n \n \n \n \n Collapsed Variational Inference for Sum-Product Networks \nH. Zhao, T. Adel, G. Gordon, and  B. Amos \nICML 2016 \n[8] [ pdf ]  \n \n \n \n \n OpenFace: A general-purpose face recognition library with mobile applications \n B. Amos , B. Ludwiczuk, and M. Satyanarayanan \nCMU 2016 \n[9] [ pdf ]  [ code ]  \n \n \n \n \n The experimental source code and libraries produced for this\nthesis are freely available as open source software and\nare available in the following repositories. \n \n [ cvxgrp/cvxpylayers ]\n  Differentiable convex optimization layers in CVXPY. \n [ locuslab/mpc.pytorch ]\n  A stand-alone PyTorch library for the differentiable\n  model predictive control approach. \n [ locuslab/differentiable-mpc ]\n  PyTorch experiments for the differentiable MPC work. \n [ locuslab/qpth ]:\n  A stand-alone PyTorch library for the OptNet QP layers. \n [ locuslab/optnet ]\n  PyTorch experiments for OptNet. \n [ locuslab/icnn ]\n  TensorFlow experiments for input-convex neural networks. \n [ cmusatyalab/openface ]\n  Face recognition with deep neural networks. \n [ bamos/block ]\n  An intelligent block matrix library for numpy, PyTorch, and beyond. \n [ bamos/dcgan-completion.tensorflow ]\n  Image Completion with Deep Learning in TensorFlow. \n [ bamos/densenet.pytorch ]\n  A PyTorch implementation of DenseNet. \n \n \n \n This repository started from\n   Cyrus Omar's thesis code ,\n  which is based on a CMU thesis template\n  by  David Koes \n  and others before. \n Of standalone interest,\n   refs.sort.sh \n  uses biber to alphabetize and standardize my bibliography in\n   refs.bib \n  so it doesn't get too messy.\n  This uses the configuration in\n   refs.conf . \n I use  update-pdf.sh \n  to keep the latest PDF only in HEAD, although Git LFS or a related\n  project may be a better solution. \n \n \n \n \n The BibTeX for this document is: \n @phdthesis{amos2019differentiable,\n  author       = {Brandon Amos},\n  title        = {{Differentiable Optimization-Based Modeling for Machine Learning}},\n  school       = {Carnegie Mellon University},\n  year         = 2019,\n  month        = May,\n}", "\n \n \n cvxpylayers \n cvxpylayers is a Python library for constructing differentiable convex\noptimization layers in PyTorch, JAX, and TensorFlow using CVXPY.\nA convex optimization layer solves a parametrized convex optimization problem\nin the forward pass to produce a solution.\nIt computes the derivative of the solution with respect to\nthe parameters in the backward pass. \n This library accompanies our  NeurIPS 2019 paper \non differentiable convex optimization layers.\nFor an informal introduction to convex optimization layers, see our\n blog post . \n Our package uses  CVXPY  for specifying\nparametrized convex optimization problems. \n \n Installation \n Usage \n Examples \n Contributing \n Projects using cvxpylayers \n License \n Citing \n \n Installation \n Use the package manager  pip  to install\ncvxpylayers. \n bash\npip install cvxpylayers \n Our package includes convex optimization layers for\nPyTorch, JAX, and TensorFlow 2.0;\nthe layers are functionally equivalent. You will need to install\n PyTorch ,\n JAX , or\n TensorFlow \nseparately, which can be done by following the instructions on their websites. \n cvxpylayers has the following dependencies:\n* Python 3\n*  NumPy \n*  CVXPY  >= 1.1.a4\n*  PyTorch  >= 1.0,  JAX  >= 0.2.12, or  TensorFlow  >= 2.0\n*  diffcp  >= 1.0.13 \n Usage \n Below are usage examples of our PyTorch, JAX, and TensorFlow layers.\nNote that the parametrized convex optimization problems must be constructed\nin CVXPY, using\n DPP . \n PyTorch \n ```python\nimport cvxpy as cp\nimport torch\nfrom cvxpylayers.torch import CvxpyLayer \n n, m = 2, 3\nx = cp.Variable(n)\nA = cp.Parameter((m, n))\nb = cp.Parameter(m)\nconstraints = [x >= 0]\nobjective = cp.Minimize(0.5 * cp.pnorm(A @ x - b, p=1))\nproblem = cp.Problem(objective, constraints)\nassert problem.is_dpp() \n cvxpylayer = CvxpyLayer(problem, parameters=[A, b], variables=[x])\nA_tch = torch.randn(m, n, requires_grad=True)\nb_tch = torch.randn(m, requires_grad=True) \n solve the problem \n solution, = cvxpylayer(A_tch, b_tch) \n compute the gradient of the sum of the solution with respect to A, b \n solution.sum().backward()\n``` \n Note:  CvxpyLayer  cannot be traced with  torch.jit . \n JAX \n ```python\nimport cvxpy as cp\nimport jax\nfrom cvxpylayers.jax import CvxpyLayer \n n, m = 2, 3\nx = cp.Variable(n)\nA = cp.Parameter((m, n))\nb = cp.Parameter(m)\nconstraints = [x >= 0]\nobjective = cp.Minimize(0.5 * cp.pnorm(A @ x - b, p=1))\nproblem = cp.Problem(objective, constraints)\nassert problem.is_dpp() \n cvxpylayer = CvxpyLayer(problem, parameters=[A, b], variables=[x])\nkey = jax.random.PRNGKey(0)\nkey, k1, k2 = jax.random.split(key, 3)\nA_jax = jax.random.normal(k1, shape=(m, n))\nb_jax = jax.random.normal(k2, shape=(m,)) \n solution, = cvxpylayer(A_jax, b_jax) \n compute the gradient of the summed solution with respect to A, b \n dcvxpylayer = jax.grad(lambda A, b: sum(cvxpylayer(A, b)[0]), argnums=[0, 1])\ngradA, gradb = dcvxpylayer(A_jax, b_jax)\n``` \n Note:  CvxpyLayer  cannot be traced with the JAX  jit  or  vmap  operations. \n TensorFlow 2 \n ```python\nimport cvxpy as cp\nimport tensorflow as tf\nfrom cvxpylayers.tensorflow import CvxpyLayer \n n, m = 2, 3\nx = cp.Variable(n)\nA = cp.Parameter((m, n))\nb = cp.Parameter(m)\nconstraints = [x >= 0]\nobjective = cp.Minimize(0.5 * cp.pnorm(A @ x - b, p=1))\nproblem = cp.Problem(objective, constraints)\nassert problem.is_dpp() \n cvxpylayer = CvxpyLayer(problem, parameters=[A, b], variables=[x])\nA_tf = tf.Variable(tf.random.normal((m, n)))\nb_tf = tf.Variable(tf.random.normal((m,))) \n with tf.GradientTape() as tape:\n  # solve the problem, setting the values of A, b to A_tf, b_tf\n  solution, = cvxpylayer(A_tf, b_tf)\n  summed_solution = tf.math.reduce_sum(solution) \n compute the gradient of the summed solution with respect to A, b \n gradA, gradb = tape.gradient(summed_solution, [A_tf, b_tf])\n``` \n Note:  CvxpyLayer  cannot be traced with  tf.function . \n Log-log convex programs \n Starting with version 0.1.3, cvxpylayers can also differentiate through log-log convex programs (LLCPs), which generalize geometric programs. Use the keyword argument  gp=True  when constructing a  CvxpyLayer  for an LLCP. Below is a simple usage example \n ```python\nimport cvxpy as cp\nimport torch\nfrom cvxpylayers.torch import CvxpyLayer \n x = cp.Variable(pos=True)\ny = cp.Variable(pos=True)\nz = cp.Variable(pos=True) \n a = cp.Parameter(pos=True, value=2.)\nb = cp.Parameter(pos=True, value=1.)\nc = cp.Parameter(value=0.5) \n objective_fn = 1/(x y z)\nobjective = cp.Minimize(objective_fn)\nconstraints = [a (x y + x z + y z) <= b, x >= y**c]\nproblem = cp.Problem(objective, constraints)\nassert problem.is_dgp(dpp=True) \n layer = CvxpyLayer(problem, parameters=[a, b, c],\n                   variables=[x, y, z], gp=True)\na_tch = torch.tensor(a.value, requires_grad=True)\nb_tch = torch.tensor(b.value, requires_grad=True)\nc_tch = torch.tensor(c.value, requires_grad=True) \n x_star, y_star, z_star = layer(a_tch, b_tch, c_tch)\nsum_of_solution = x_star + y_star + z_star\nsum_of_solution.backward()\n``` \n Examples \n Our  examples  subdirectory contains simple applications of convex optimization\nlayers in IPython notebooks. \n Contributing \n Pull requests are welcome. For major changes, please open an issue first to\ndiscuss what you would like to change. \n Please make sure to update tests as appropriate. \n Please lint the code with  flake8 .\n bash\npip install flake8  # if not already installed\nflake8 \n Running tests \n cvxpylayers uses the  pytest  framework for running tests.\nTo install  pytest , run:\n bash\npip install pytest \n Execute the tests from the main directory of this repository with:\n bash\npytest cvxpylayers/{torch,jax,tensorflow} \n Projects using cvxpylayers \n Below is  a list of projects using cvxpylayers. If you have used cvxpylayers in a project, you're welcome to make a PR to add it to this list.\n*  Learning Convex Optimization Control Policies \n*  Learning Convex Optimization Models \n License \n cvxpylayers carries an Apache 2.0 license. \n Citing \n If you use cvxpylayers for research, please cite our accompanying  NeurIPS paper : \n @inproceedings{cvxpylayers2019,\n  author={Agrawal, A. and Amos, B. and Barratt, S. and Boyd, S. and Diamond, S. and Kolter, Z.},\n  title={Differentiable Convex Optimization Layers},\n  booktitle={Advances in Neural Information Processing Systems},\n  year={2019},\n} \n If you use cvxpylayers to differentiate through a log-log convex program, please cite the accompanying  paper : \n @article{agrawal2020differentiating,\n  title={Differentiating through log-log convex programs},\n  author={Agrawal, Akshay and Boyd, Stephen},\n  journal={arXiv},\n  archivePrefix={arXiv},\n  eprint={2004.12553},\n  primaryClass={math.OC},\n  year={2020},\n}", 'QNSTOP: quasi-Newton stochastic optimization algorithm. \n The package  QNSTOP  is a suite of serial and parallel Fortran 95/2003 codes\nfor deterministic global optimization and stochastic optimization, with\nthe serial driver subroutine  QNSTOPS  and the parallel driver subroutine\nQNSTOPP. The organization of this repository is: \n \n src  contains the ACT TOMS code. See the  README  here for further details. \n docs  contains website docs. \n', "This repository contains the slides behind  my  major\npresentations with a  CC-BY  license. \n \n [2022] Learning with differentiable and amortized optimization \n Powerpoint  |\n PDF \n Optimization has been a transformative modeling and decision-making paradigm over the past century that computationally encodes non-trivial reasoning operations.  Developments in optimization foundations alongside domain experts have resulted in breakthroughs for 1) controlling robotic, autonomous, mechanical, and multi-agent systems, 2) making operational decisions based on future predictions, 3) efficiently transporting or matching resources, information, and measures, 4) allocating budgets and portfolios, 5) designing materials, molecules, and other structures, 6) solving inverse problems to infer underlying hidden costs, incentives, geometries, terrains, and other structures, and 7) learning and meta-learning the parameters of predictive and statistical models. These settings often analytically specify the relevant models of the world along with an explicit objective to optimize for. Once these are specified, computational optimization solvers are able to search over the space of possible solutions or configurations and return the best one. \n The magic of optimization stops when 1) the relevant models of the world are too difficult or impossible to specify, leading to inaccurate or incomplete representations of the true setting, and 2) solving the optimization problem is computationally challenging and takes too long to return a solution on today's hardware. Machine learning methods help overcome both of these by providing fast predictive models and powerful latent abstractions of the world. In this talk, I will cover two ways of tightly integrating optimization and machine learning methods:] \n \n \n Differentiable optimization  characterizes how the solution to an optimization problem changes as the inputs change. In machine learning settings, differentiable optimization provides an implicit layer that integrates optimization-based domain knowledge into the model and enables unknown parts of the optimization problem to be learned. I will cover the foundations of learning these layers with implicit differentiation and highlight applications in robotics and control settings. \n \n \n Amortized optimization  rapidly predicts approximate solutions to optimization problems and is useful when repeatedly solving optimization problems. Traditional optimization methods typically solve every new problem instance from scratch, ignoring shared structures and information when solving a new instance. In contrast, a solver augmented with amortized optimization learns the shared structure present in the solution mappings and better-searches the domain. I will cover the foundations of amortized optimization and highlight new applications in control and optimal transport. \n \n \n [2022] Amortized optimization for computing optimal transport maps \n Powerpoint  |\n PDF \n [2022] Differentiable optimization \n Powerpoint  |  PDF \n [2022] Differentiable control \n Powerpoint  |  PDF \n [2022] Amortized optimization \n Powerpoint  |  PDF \n [2021] On the model-based stochastic value gradient for continuous RL \n Powerpoint  |  PDF \n [2021] Riemannian Convex Potential Maps \n Keynote  |  PDF \n [2020] Differentiable cross-entropy method \n Powerpoint  |  PDF \n [2019] Ph.D. Thesis: Differentiable optimization-based modeling for machine learning \n Powerpoint  |  PDF \n [2018] PyTorch libraries for linear algebra, optimization, and control \n Powerpoint  |  PDF \n [2018] OptNet, end-to-end task-based learning, and control \n Powerpoint  |  PDF \n [2018] Differentiable MPC \n Powerpoint  |  PDF ] |  Poster Powerpoint  |  Poster PDF \n [2017] OptNet \n Powerpoint  |  PDF \n [2017] ICNN \n Powerpoint  |  PDF"]
jekbradbury,['\n Swift Programming Language \n | |  Architecture  |  Master  |  Package  |\n|---|:---:|:---:|:---:|\n|  macOS         | x86_64 | | |\n|  Ubuntu 14.04  | x86_64 |  | |\n|  Ubuntu 16.04  | x86_64 |  | |\n|  Ubuntu 18.04  | x86_64 |  | | \n Swift Community-Hosted CI Platforms \n |  OS  |  Architecture  |  Build  |\n|---|:---:|:---:|\n| Debian 9.1 (Raspberry Pi)  | ARMv7 |  |\n| Fedora 27  | x86_64 | |\n| Ubuntu 16.04  | x86_64 | |\n| Ubuntu 16.04   | PPC64LE | |\n| Ubuntu 16.04   | AArch64 | |\n| Ubuntu 16.04 (Android)  | x86_64 | |\n| Ubuntu 16.04 (TensorFlow)  | x86_64 | |\n| macOS 10.13 (TensorFlow)  | x86_64 | |\n| Ubuntu 16.04 (TensorFlow with GPU)  | x86_64 | | \n Welcome to Swift \n Swift is a high-performance system programming language.  It has a clean\nand modern syntax, offers seamless access to existing C and Objective-C code\nand frameworks, and is memory safe by default. \n Although inspired by Objective-C and many other languages, Swift is not itself a\nC-derived language. As a complete and independent language, Swift packages core\nfeatures like flow control, data structures, and functions, with high-level\nconstructs like objects, protocols, closures, and generics. Swift embraces\nmodules, eliminating the need for headers and the code duplication they entail. \n To learn more about the programming language, visit  swift.org . \n Contributing to Swift \n Contributions to Swift are welcomed and encouraged! Please see the\n Contributing to Swift guide . \n To be a truly great community,  Swift.org  needs to welcome\ndevelopers from all walks of life, with different backgrounds, and with a wide\nrange of experience. A diverse and friendly community will have more great\nideas, more unique perspectives, and produce more great code. We will work\ndiligently to make the Swift community welcoming to everyone. \n To give clarity of what is expected of our members, Swift has adopted the\ncode of conduct defined by the Contributor Covenant. This document is used\nacross many open source communities, and we think it articulates our values\nwell. For more, see the  Code of Conduct . \n Getting Started \n These instructions give the most direct path to a working Swift development\nenvironment. To build from source you will need about 2 GB of disk space for the\nsource code and up to 70 GB of disk space for the build artifacts with full\ndebugging. Depending on your machine, a clean build can take a few minutes to\nseveral hours. Naturally, incremental builds are much faster. \n System Requirements \n macOS, Ubuntu Linux LTS, and the latest Ubuntu Linux release are the current\nsupported host development operating systems. \n macOS \n To build for macOS, you need  Xcode 10.0 .\nThe required version of Xcode changes frequently, and is often a beta release.\nCheck this document or the host information on  https://ci.swift.org  for the\ncurrent required version. \n You will also need  CMake  and  Ninja ,\nwhich can be installed via a package manager: \n Homebrew \n brew install cmake ninja\n \n MacPorts \n sudo port install cmake ninja\n \n Instructions for installing CMake and Ninja directly can be found  below . \n Linux \n For Ubuntu, you\'ll need the following development dependencies: \n sudo apt-get install git cmake ninja-build clang python uuid-dev libicu-dev icu-devtools libbsd-dev libedit-dev libxml2-dev libsqlite3-dev swig libpython-dev libncurses5-dev pkg-config libblocksruntime-dev libcurl4-openssl-dev systemtap-sdt-dev tzdata rsync\n \n Note:  LLDB currently requires at least  swig-1.3.40  but will successfully build\nwith version 2 shipped with Ubuntu. \n Build instructions for Ubuntu 14.04 LTS can be found  here . \n Getting Sources for Swift and Related Projects \n First create a directory for all of the Swift sources: \n mkdir swift-source\ncd swift-source\n \n Note:  This is important since update-checkout (see below) checks out\nrepositories next to the Swift source directory. This means that if one clones\nSwift and has other unrelated repositories, update-checkout may not clone those\nrepositories and will update them instead. \n Via HTTPS   For those checking out sources as read-only, HTTPS works best: \n git clone https://github.com/apple/swift.git\n./swift/utils/update-checkout --clone\n \n Via SSH   For those who plan on regularly making direct commits,\ncloning over SSH may provide a better experience (which requires\n uploading SSH keys to GitHub ): \n git clone git@github.com:apple/swift.git\n./swift/utils/update-checkout --clone-with-ssh\n \n Building Swift \n The  build-script  is a high-level build automation script that supports basic\noptions such as building a Swift-compatible LLDB, building the Swift Package\nManager, building for various platforms, running tests after builds, and more. \n There are two primary build systems to use: Xcode and Ninja. The Xcode build\nsystem allows you to work in Xcode, but Ninja is a bit faster and supports\nmore environments. \n To build using Ninja, run: \n utils/build-script --release-debuginfo\n \n When developing Swift, it helps to build what you\'re working on in a debug\nconfiguration while building the rest of the project with optimizations. Below\nare some examples of using debug variants: \n utils/build-script --release-debuginfo --debug-swift # Swift frontend built in debug\nutils/build-script --release-debuginfo --debug-swift-stdlib # Standard library built in debug\nutils/build-script --release-debuginfo --debug-swift --force-optimized-typechecker # Swift frontend sans type checker built in debug\n \n Limiting the amount of debug code in the compiler has a very large impact on\nSwift compile times, and in turn the test execution time. If you want to build\nthe entire project in debug, you can run: \n utils/build-script --debug\n \n For documentation of all available arguments, as well as additional usage\ninformation, see the inline help: \n utils/build-script -h\n \n Xcode \n To build using Xcode, specify the  --xcode  argument on any of the above commands.\nXcode can be used to edit the Swift source code, but it is not currently\nfully supported as a build environment for SDKs other than macOS. The generated\nXcode project does not integrate with the test runner, but the tests can be run\nwith the \'check-swift\' target. \n Build Products \n All of the build products are placed in  swift-source/build/${TOOL}-${MODE}/${PRODUCT}-${PLATFORM}/ .\nIf macOS Swift with Ninja in DebugAssert mode was built, all of the products\nwould be in  swift-source/build/Ninja-DebugAssert/swift-macosx-x86_64/ . It\nhelps to save this directory as an environment variable for future use. \n export SWIFT_BUILD_DIR="~/swift-source/build/Ninja-DebugAssert/swift-macosx-x86_64"\n \n Ninja \n Once the first build has completed, Ninja can perform fast incremental builds of\nvarious products. These incremental builds are a big timesaver when developing\nand debugging. \n cd ${SWIFT_BUILD_DIR}\nninja swift\n \n This will build the Swift compiler, but will not rebuild the standard library or\nany other target. Building the  swift-stdlib  target as an additional layer of\ntesting from time to time is also a good idea. To build just the standard\nlibrary, run: \n ninja swift-stdlib\n \n It is always a good idea to do a full build after using  update-checkout . \n Using Xcode \n To open the Swift project in Xcode, open  ${SWIFT_BUILD_DIR}/Swift.xcodeproj .\nIt will auto-create a  lot  of schemes for all of the available targets. A\ncommon debug flow would involve: \n \n Select the \'swift\' scheme. \n Pull up the scheme editor (⌘⇧<). \n Select the \'Arguments\' tab and click the \'+\'. \n Add the command line options. \n Close the scheme editor. \n Build and run. \n \n Another option is to change the scheme to "Wait for executable to be launched",\nthen run the build product in Terminal. \n Swift Toolchains \n Building \n Swift toolchains are created using the script\n build-toolchain . This\nscript is used by swift.org\'s CI to produce snapshots and can allow for one to\nlocally reproduce such builds for development or distribution purposes. E.x.: \n $ ./utils/build-toolchain $TOOLCHAIN_PREFIX \n where  $TOOLCHAIN_PREFIX  is a string that will be prepended to the swift\npackage name in the produced tar ball. For instance, if  $TOOLCHAIN_PREFIX \nwas  macOS , the produced archive will have the name\n swift-macOS.tar.gz . \n Beyond building the toolchain,  build-toolchain  also supports the following\n(non-exhaustive) set of useful options:: \n \n --dry-run : Perform a dry run build. This is off by default. \n --test : Test the toolchain after it has been compiled. This is off by default. \n --distcc : Use distcc to speed up the build by distributing the c++ part of\n  the swift build. This is off by default. \n \n More options may be added over time. Please pass  --help  to\n build-toolchain  to see the full set of options. \n Installing into Xcode \n On macOS if one wants to install such a toolchain into Xcode: \n \n Untar and copy the toolchain to one of  /Library/Developer/Toolchains/  or\n    ~/Library/Developer/Toolchains/ . E.x.: \n \n $ tar -xzf swift-macOS.tar.gz -C /\n  $ tar -xzf swift-macOS.tar.gz -C ~/ \n \n Specify the local toolchain for Xcode\'s use via  Xcode->Toolchains . \n \n Build Failures \n Make sure you are using the  correct release  of Xcode. \n If you have changed Xcode versions but still encounter errors that appear to\nbe related to the Xcode version, try passing  --clean  to  build-script . \n When a new version of Xcode is released, you can update your build without\nrecompiling the entire project by passing the  --reconfigure  option. \n Make sure all repositories are up to date with the  update-checkout  command\ndescribed above. \n Testing Swift \n See  docs/Testing.md , in particular the section on  lit.py . \n Learning More \n Be sure to look through the  docs \ndirectory for more information about the compiler. In particular, the documents\ntitled  Debugging the Swift Compiler  and\n Continuous Integration for Swift  are very\nhelpful to understand before submitting your first PR. \n Building Documentation \n To read the compiler documentation, start by installing the\n Sphinx  documentation generator tool by running the\ncommand: \n easy_install -U Sphinx\n \n Once complete, you can build the Swift documentation by changing directory into\n docs  and typing  make . This\ncompiles the  .rst  files in the  docs \ndirectory into HTML in the  docs/_build/html  directory. \n Many of the docs are out of date, but you can see some historical design\ndocuments in the  docs  directory. \n Another source of documentation is the standard library itself, located in\n stdlib . Much of the language is actually implemented in the library\n(including  Int ), and the standard library gives some examples of what can be\nexpressed today. \n Build Dependencies \n CMake \n CMake  is the core infrastructure used to configure builds of \nSwift and its companion projects; at least version 3.4.3 is required.  \n On macOS, you can download the  CMake Binary Distribution ,\nbundled as an application, copy it to  /Applications , and add the embedded\ncommand line tools to your  PATH : \n export PATH=/Applications/CMake.app/Contents/bin:$PATH\n \n On Linux, if you have not already installed Swift\'s  development\ndependencies , you can download and install the CMake\npackage separately using the following command: \n sudo apt-get install cmake\n \n Ninja \n Ninja  is the current recommended build system\nfor building Swift and is the default configuration generated by CMake.  Pre-built\npackages \nare available for macOS and Linux distributions. You can also clone Ninja\nnext to the other projects and it will be bootstrapped automatically: \n Via HTTPS \n git clone https://github.com/ninja-build/ninja.git && cd ninja\ngit checkout release\ncat README\n \n Via SSH \n git clone git@github.com:ninja-build/ninja.git && cd ninja\ngit checkout release\ncat README\n', 'revtok \n Reversible tokenization in Python.', 'Revtok', 'Minibatch \n   \n Write neural net code that operates on individual data samples and autobatch it using dispatch.', 'Transformer', 'SpaCy.jl \n Julia interface to  SpaCy , the Cython NLP package. Work in progress.', 'FunctionalControlFlow.jl \n Provides macro  @functionalize , which converts  if ,  while ,  for ,  && , and  ||  to functional control flow  _if  and  _while  compatible with dataflow graph semantics as found in e.g. XLA or ONNX. Those can then be overdubbed in Cassette to support various tracing, compilation, or nonstandard interpretation behaviors. \n Currently does not support  break ,  continue , or  return  inside a control flow block.', '\n \n \n JAX: Autograd and XLA  \n JAX is  Autograd  and\n XLA ,\nbrought together for high-performance machine learning research. \n With its updated version of  Autograd ,\nJAX can automatically differentiate native\nPython and NumPy functions. It can differentiate through loops, branches,\nrecursion, and closures, and it can take derivatives of derivatives of\nderivatives. It supports reverse-mode differentiation (a.k.a. backpropagation)\nvia  grad  as well as forward-mode differentiation,\nand the two can be composed arbitrarily to any order. \n What’s new is that JAX uses\n XLA \nto compile and run your NumPy programs on GPUs and TPUs. Compilation happens\nunder the hood by default, with library calls getting just-in-time compiled and\nexecuted. But JAX also lets you just-in-time compile your own Python functions\ninto XLA-optimized kernels using a one-function API,\n jit . Compilation and automatic differentiation can be\ncomposed arbitrarily, so you can express sophisticated algorithms and get\nmaximal performance without leaving Python. \n Dig a little deeper, and you\'ll see that JAX is really an extensible system for\n composable function transformations . Both\n grad  and  jit \nare instances of such transformations. Another is  vmap \nfor automatic vectorization, with more to come. \n This is a research project, not an official Google product. Expect bugs and\nsharp edges. Please help by trying it out,  reporting\nbugs , and letting us know what you\nthink! \n ```python\nimport jax.numpy as np\nfrom jax import grad, jit, vmap\nfrom functools import partial \n def predict(params, inputs):\n  for W, b in params:\n    outputs = np.dot(inputs, W) + b\n    inputs = np.tanh(outputs)\n  return outputs \n def logprob_fun(params, inputs, targets):\n  preds = predict(params, inputs)\n  return np.sum((preds - targets)**2) \n grad_fun = jit(grad(logprob_fun))  # compiled gradient evaluation function\nperex_grads = jit(vmap(grad_fun, in_axes=(None, 0, 0)))  # fast per-example grads\n``` \n JAX started as a research project by  Matt Johnson ,\n Roy Frostig ,  Dougal\nMaclaurin , and  Chris\nLeary , and is now developed  in the\nopen  by a growing number of\n contributors . \n Contents \n \n Quickstart: Colab in the Cloud \n Installation \n Running the tests \n Reference documentation \n A brief tour \n What\'s supported \n Transformations \n Random numbers are different \n Mini-libraries \n How it works \n What we\'re working on \n Current gotchas \n \n Quickstart: Colab in the Cloud \n Jump right in using a notebook in your browser, connected to a Google Cloud GPU:\n-  The basics: NumPy on accelerators,  grad  for differentiation,  jit  for compilation, and  vmap  for vectorization \n-  Training a Simple Neural Network, with PyTorch Data Loading \n-  Training a Simple Neural Network, with TensorFlow Dataset Data Loading \n-  The Autodiff Cookbook: easy and powerful automatic differentiation in JAX \n-  MAML Tutorial with JAX .\n-  Directly using XLA in Python \n Installation \n JAX is written in pure Python, but it depends on XLA, which needs to be\ncompiled and installed as the  jaxlib  package. Use the following instructions\nto build JAX from source or install a binary package with pip. \n Building JAX from source \n First, obtain the JAX source code, and make sure  scipy  is installed. \n bash\ngit clone https://github.com/google/jax\ncd jax\npip install scipy \n If you are building on a Mac, make sure XCode and the XCode command line tools\nare installed. \n To build XLA with CUDA support, you can run \n bash\npython build/build.py --enable_cuda\npip install -e build  # install jaxlib (includes XLA)\npip install -e .      # install jax (pure Python) \n See  python build/build.py --help  for configuration options, including ways to\nspecify the paths to CUDA and CUDNN, which you must have installed. The build\nalso depends on NumPy, and a compiler toolchain corresponding to that of\nUbuntu 16.04 or newer. \n To build XLA without CUDA GPU support (CPU only), drop the  --enable_cuda : \n bash\npython build/build.py\npip install -e build  # install jaxlib (includes XLA)\npip install -e .      # install jax \n To upgrade to the latest version from GitHub, just run  git pull  from the JAX\nrepository root, and rebuild by running  build.py  if necessary. You shouldn\'t have\nto reinstall because  pip install -e  sets up symbolic links from site-packages\ninto the repository. \n pip installation \n Installing XLA with prebuilt binaries via  pip  is still experimental,\nespecially with GPU support. Let us know on  the issue\ntracker  if you run into any errors. \n To install a CPU-only version, which might be useful for doing local\ndevelopment on a laptop, you can run \n bash\npip install --upgrade jax jaxlib  # CPU-only version \n If you want to install JAX with both CPU and GPU support, using existing CUDA\nand CUDNN7 installations on your machine (for example, preinstalled on your\ncloud VM), you can run \n ```bash \n install jaxlib \n PYTHON_VERSION=cp27  # alternatives: cp27, cp35, cp36, cp37\nCUDA_VERSION=cuda92  # alternatives: cuda90, cuda92, cuda100\nPLATFORM=linux_x86_64  # alternatives: linux_x86_64\nBASE_URL=\'https://storage.googleapis.com/jax-wheels\'\npip install --upgrade $BASE_URL/$CUDA_VERSION/jaxlib-0.1.11-$PYTHON_VERSION-none-$PLATFORM.whl \n pip install --upgrade jax  # install jax\n``` \n The library package name must correspond to the version of the existing CUDA\ninstallation you want to use, with  cuda100  for CUDA 10.0,  cuda92  for CUDA\n9.2, and  cuda90  for CUDA 9.0. To find your CUDA and CUDNN versions, you can\nrun commands like these, depending on your CUDNN install path: \n bash\nnvcc --version\ngrep CUDNN_MAJOR -A 2 /usr/local/cuda/include/cudnn.h  # might need different path \n The Python version must match your Python interpreter. There are prebuilt wheels\nfor Python 2.7, 3.6, and 3.7; for anything else, you must build from source. \n Running the tests \n To run all the JAX tests, from the repository root directory run \n bash\nnosetests tests \n JAX generates test cases combinatorially, and you can control the number of\ncases that are generated and checked for each test (default 10): \n bash\nJAX_NUM_GENERATED_CASES=100 nosetests tests \n You can run a more specific set of tests using\n nose \'s built-in selection\nmechanisms, or alternatively you can run a specific test file directly to see\nmore detailed information about the cases being run: \n bash\npython tests/lax_numpy_test.py --num_generated_cases=5 \n Reference documentation \n For details about the JAX API, see the\n reference documentation . \n A brief tour \n ```python\nIn [1]: import jax.numpy as np \n In [2]: from jax import random \n In [3]: key = random.PRNGKey(0) \n In [4]: x = random.normal(key, (5000, 5000)) \n In [5]: print(np.dot(x, x.T) / 2)  # fast!\n[[  2.52727051e+03   8.15895557e+00  -8.53276134e-01 ...,  # ... \n In [6]: print(np.dot(x, x.T) / 2)  # even faster! \n JIT-compiled code is cached and reused in the 2nd call \n [[  2.52727051e+03   8.15895557e+00  -8.53276134e-01 ...,  # ...\n``` \n What’s happening behind-the-scenes is that JAX is using XLA to just-in-time\n(JIT) compile and execute these individual operations on the GPU. First the\n random.normal  call is compiled and the array referred to by  x  is generated\non the GPU. Next, each function called on  x  (namely  transpose ,  dot , and\n divide ) is individually JIT-compiled and executed, each keeping its results on\nthe device.\nIt’s only when a value needs to be printed, plotted, saved, or passed into a raw\nNumPy function that a read-only copy of the value is brought back to the host as\nan ndarray and cached. The second call to  dot  is faster because the\nJIT-compiled code is cached and reused, saving the compilation time. \n The fun really starts when you use  grad  for automatic differentiation and\n jit  to compile your own functions end-to-end. Here’s a more complete toy\nexample: \n ```python\nfrom jax import grad, jit\nimport jax.numpy as np \n def sigmoid(x):\n    return 0.5 * (np.tanh(x / 2.) + 1) \n Outputs probability of a label being true according to logistic model. \n def logistic_predictions(weights, inputs):\n    return sigmoid(np.dot(inputs, weights)) \n Training loss is the negative log-likelihood of the training labels. \n def loss(weights, inputs, targets):\n    preds = logistic_predictions(weights, inputs)\n    label_logprobs = np.log(preds) * targets + np.log(1 - preds) * (1 - targets)\n    return -np.sum(label_logprobs) \n Build a toy dataset. \n inputs = np.array([[0.52, 1.12,  0.77],\n                   [0.88, -1.08, 0.15],\n                   [0.52, 0.06, -1.30],\n                   [0.74, -2.49, 1.39]])\ntargets = np.array([True, True, False, True]) \n Define a compiled function that returns gradients of the training loss \n training_gradient_fun = jit(grad(loss)) \n Optimize weights using gradient descent. \n weights = np.array([0.0, 0.0, 0.0])\nprint("Initial loss: {:0.2f}".format(loss(weights, inputs, targets)))\nfor i in range(100):\n    weights -= 0.1 * training_gradient_fun(weights, inputs, targets) \n print("Trained loss: {:0.2f}".format(loss(weights, inputs, targets)))\n``` \n To see more, check out the  quickstart\nnotebook ,\na  simple MNIST classifier\nexample \nand the rest of the  JAX\nexamples . \n What\'s supported \n If you’re using JAX just as an accelerator-backed NumPy, without using  grad  or\n jit  in your code, then in principle there are no constraints, though some\nNumPy functions haven’t been implemented yet. A list of supported functions can\nbe found in the  reference documentation . \n Generally using  np.dot(A, B)  is\nbetter than  A.dot(B)  because the former gives us more opportunities to run the\ncomputation on the device. NumPy also does a lot of work to cast any array-like\nfunction arguments to arrays, as in  np.sum([x, y]) , while  jax.numpy \ntypically requires explicit casting of array arguments, like\n np.sum(np.array([x, y])) . \n For automatic differentiation with  grad , JAX has the same restrictions\nas  Autograd . Specifically, differentiation\nworks with indexing ( x = A[i, j, :] ) but not indexed assignment ( A[i, j] =\nx ) or indexed in-place updating ( A[i] += b ). You can use lists, tuples, and\ndicts freely: JAX doesn\'t even see them. Using  np.dot(A, B)  rather than\n A.dot(B)  is required for automatic differentiation when  A  is a raw ndarray. \n For compiling your own functions with  jit  there are a few more requirements.\nBecause  jit  aims to specialize Python functions only on shapes and dtypes\nduring tracing, rather than on concrete values, Python control flow that depends\non concrete values won’t be able to execute and will instead raise an error. If\nyou want compiled control flow, use structured control flow primitives like\nlax.cond and lax.while. Some indexing features, like slice-based indexing\n A[i:i+5]  for argument-dependent  i , or boolean-based indexing  A[bool_ind] \nfor argument-dependent  bool_ind , produce abstract values of unknown shape and\nare thus unsupported in  jit  functions. \n In general, JAX is intended to be used with a functional style of Python\nprogramming. Functions passed to transformations like  grad  and  jit  are\nexpected to be free of side-effects. You can write print statements for\ndebugging but they may only be executed once if they\'re under a  jit  decorator. \n \n TLDR  Do use \n \n Functional programming \n Many  of NumPy’s\n    functions (help us add more!) \n Some  SciPy functions \n Indexing and slicing of arrays like  x = A[[5, 1, 7], :, 2:4] \n Explicit array creation from lists like  A = np.array([x, y]) \n \n Don’t use \n \n Assignment into arrays like  A[0, 0] = x \n Implicit casting to arrays like  np.sum([x, y])  (use  np.sum(np.array([x,\n    y])  instead) \n A.dot(B)  method syntax for functions of more than one argument (use\n     np.dot(A, B)  instead) \n Side-effects like mutation of arguments or mutation of global variables \n The  out  argument of NumPy functions \n Dtype casting like  np.float64(x)  (use  x.astype(\'float64\')  or\n     x.astype(np.float64)  instead). \n \n For jit functions, also don’t use \n \n Control flow based on dynamic values  if x > 0: ... . Control flow based\n    on shapes is fine:  if x.shape[0] > 2: ...  and  for subarr in array . \n Slicing  A[i:i+5]  for dynamic index  i  (use  lax.dynamic_slice  instead)\n    or boolean indexing  A[bool_ind]  for traced values  bool_ind . \n \n \n You should get loud errors if your code violates any of these. \n Transformations \n At its core, JAX is an extensible system for transforming numerical functions.\nWe currently expose three important transformations:  grad ,  jit , and  vmap . \n Automatic differentiation with grad \n JAX has roughly the same API as  Autograd .\nThe most popular function is  grad  for reverse-mode gradients: \n ```python\nfrom jax import grad\nimport jax.numpy as np \n def tanh(x):  # Define a function\n  y = np.exp(-2.0 * x)\n  return (1.0 - y) / (1.0 + y) \n grad_tanh = grad(tanh)  # Obtain its gradient function\nprint(grad_tanh(1.0))   # Evaluate it at x = 1.0 \n prints 0.41997434161402603 \n ``` \n You can differentiate to any order with  grad . \n For more advanced autodiff, you can use  jax.vjp  for reverse-mode\nvector-Jacobian products and  jax.jvp  for forward-mode Jacobian-vector\nproducts. The two can be composed arbitrarily with one another, and with other\nJAX transformations. Here\'s one way to compose\nthose to make a function that efficiently computes full Hessian matrices: \n python\nfrom jax import jit, jacfwd, jacrev\ndef hessian(fun):\n  return jit(jacfwd(jacrev(fun))) \n As with Autograd, you\'re free to use differentiation with Python control\nstructures: \n ```python\ndef abs_val(x):\n  if x > 0:\n    return x\n  else:\n    return -x \n abs_val_grad = grad(abs_val)\nprint(abs_val_grad(1.0))   # prints 1.0\nprint(abs_val_grad(-1.0))  # prints -1.0 (abs_val is re-evaluated)\n``` \n Compilation with jit \n You can use XLA to compile your functions end-to-end with  jit , used either as\nan  @jit  decorator or as a higher-order function. \n ```python\nimport jax.numpy as np\nfrom jax import jit \n def slow_f(x):\n  # Element-wise ops see a large benefit from fusion\n  return x * x + x * 2.0 \n x = np.ones((5000, 5000))\nfast_f = jit(slow_f)\n%timeit -n10 -r3 fast_f(x)  # ~ 4.5 ms / loop on Titan X\n%timeit -n10 -r3 slow_f(x)  # ~ 14.5 ms / loop (also on GPU via JAX)\n``` \n You can mix  jit  and  grad  and any other JAX transformation however you like. \n Auto-vectorization with vmap \n vmap  is the vectorizing map.\nIt has the familiar semantics of mapping a function along array axes, but\ninstead of keeping the loop on the outside, it pushes the loop down into a\nfunction’s primitive operations for better performance. \n Using  vmap  can save you from having to carry around batch dimensions in your\ncode. For example, consider this simple  unbatched  neural network prediction\nfunction: \n python\ndef predict(params, input_vec):\n  assert input_vec.ndim == 1\n  for W, b in params:\n    output_vec = np.dot(W, input_vec) + b  # `input_vec` on the right-hand side!\n    input_vec = np.tanh(output_vec)\n  return output_vec \n We often instead write  np.dot(inputs, W)  to allow for a batch dimension on the\nleft side of  inputs , but we’ve written this particular prediction function to\napply only to single input vectors. If we wanted to apply this function to a\nbatch of inputs at once, semantically we could just write \n python\nfrom functools import partial\npredictions = np.stack(list(map(partial(predict, params), input_batch))) \n But pushing one example through the network at a time would be slow! It’s better\nto vectorize the computation, so that at every layer we’re doing matrix-matrix\nmultiplies rather than matrix-vector multiplies. \n The  vmap  function does that transformation for us. That is, if we write \n ```python\nfrom jax import vmap\npredictions = vmap(partial(predict, params))(input_batch) \n or, alternatively \n predictions = vmap(predict, in_axes=(None, 0))(params, input_batch)\n``` \n then the  vmap  function will push the outer loop inside the function, and our\nmachine will end up executing matrix-matrix multiplications exactly as if we’d\ndone the batching by hand. \n It’s easy enough to manually batch a simple neural network without  vmap , but\nin other cases manual vectorization can be impractical or impossible. Take the\nproblem of efficiently computing per-example gradients: that is, for a fixed set\nof parameters, we want to compute the gradient of our loss function evaluated\nseparately at each example in a batch. With  vmap , it’s easy: \n python\nper_example_gradients = vmap(partial(grad(loss), params))(inputs, targets) \n Of course,  vmap  can be arbitrarily composed with  jit ,  grad , and any other\nJAX transformation! We use  vmap  with both forward- and reverse-mode automatic\ndifferentiation for fast Jacobian and Hessian matrix calculations in\n jax.jacfwd ,  jax.jacrev , and  jax.hessian . \n Random numbers are different \n JAX needs a  functional pseudo-random number generator (PRNG) system  to provide\nreproducible results invariant to compilation boundaries and backends, while\nalso maximizing performance by enabling vectorized generation and\nparallelization across random calls. The  numpy.random  library doesn’t have\nthose properties. The  jax.random  library meets those needs: it’s functionally\npure, but it doesn’t require you to pass stateful random objects back out of\nevery function. \n The  jax.random  library uses\n count-based PRNGs \nand a functional array-oriented\n splitting model .\nTo generate random values, you call a function like  jax.random.normal  and give\nit a PRNG key: \n ```python\nimport jax.random as random \n key = random.PRNGKey(0)\nprint(random.normal(key, shape=(3,)))  # [ 1.81608593 -0.48262325  0.33988902]\n``` \n If we make the same call again with the same key, we get the same values: \n python\nprint(random.normal(key, shape=(3,)))  # [ 1.81608593 -0.48262325  0.33988902] \n The key never gets updated. So how do we get fresh random values? We use\n jax.random.split  to create new keys from existing ones. A common pattern is to\nsplit off a new key for every function call that needs random values: \n ```python\nkey = random.PRNGKey(0) \n key, subkey = random.split(key)\nprint(random.normal(subkey, shape=(3,)))  # [ 1.1378783  -1.22095478 -0.59153646] \n key, subkey = random.split(key)\nprint(random.normal(subkey, shape=(3,)))  # [-0.06607265  0.16676566  1.17800343]\n``` \n By splitting the PRNG key, not only do we avoid having to thread random states\nback out of every function call, but also we can generate multiple random arrays\nin parallel because we can avoid unnecessary sequential dependencies. \n There\'s a gotcha here, which is that it\'s easy to unintentionally reuse a key\nwithout splitting. We intend to add a check for this (a sort of dynamic linear\ntyping) but for now it\'s something to be careful about. \n For more detailed information on the design and the reasoning behind it, see the\n PRNG design doc . \n Mini-libraries \n JAX provides some small, experimental libraries for machine learning. These\nlibraries are in part about providing tools and in part about serving as\nexamples for how to build such libraries using JAX. Each one is only a few\nhundred lines of code, so take a look inside and adapt them as you need! \n Neural-net building with Stax \n Stax  is a functional neural network building library. The basic idea is that\na single layer or an entire network can be modeled as an  (init_fun, apply_fun) \npair. The  init_fun  is used to initialize network parameters and the\n apply_fun  takes parameters and inputs to produce outputs. There are\nconstructor functions for common basic pairs, like  Conv  and  Relu , and these\npairs can be composed in series using  stax.serial  or in parallel using\n stax.parallel . \n Here’s an example: \n ```python\nimport jax.numpy as np\nfrom jax.experimental import stax\nfrom jax.experimental.stax import Conv, Dense, MaxPool, Relu, Flatten, LogSoftmax \n Use stax to set up network initialization and evaluation functions \n net_init, net_apply = stax.serial(\n    Conv(32, (3, 3), padding=\'SAME\'), Relu,\n    Conv(64, (3, 3), padding=\'SAME\'), Relu,\n    MaxPool((2, 2)), Flatten,\n    Dense(128), Relu,\n    Dense(10), LogSoftmax,\n) \n Initialize parameters, not committing to a batch shape \n in_shape = (-1, 28, 28, 1)\nout_shape, net_params = net_init(in_shape) \n Apply network to dummy inputs \n inputs = np.zeros((128, 28, 28, 1))\npredictions = net_apply(net_params, inputs)\n``` \n First-order optimization \n JAX has a minimal optimization library focused on stochastic first-order\noptimizers. Every optimizer is modeled as an  (init_fun, update_fun)  pair. The\n init_fun  is used to initialize the optimizer state, which could include things\nlike momentum variables, and the  update_fun  accepts a gradient and an\noptimizer state to produce a new optimizer state. The parameters being optimized\ncan be ndarrays or arbitrarily-nested list/tuple/dict structures, so you can\nstore your parameters however you’d like. \n Here’s an example, using  jit  to compile the whole update end-to-end: \n ```python\nfrom jax.experimental import optimizers\nfrom jax import jit, grad \n Define a simple squared-error loss \n def loss(params, batch):\n  inputs, targets = batch\n  predictions = net_apply(params, inputs)\n  return np.sum((predictions - targets)**2) \n Use optimizers to set optimizer initialization and update functions \n opt_init, opt_update = optimizers.momentum(step_size=1e-3, mass=0.9) \n Define a compiled update step \n @jit\ndef step(i, opt_state, batch):\n  params = optimizers.get_params(opt_state)\n  g = grad(loss)(params, batch)\n  return opt_update(i, g, opt_state) \n Dummy input data stream \n data_generator = ((np.zeros((128, 28, 28, 1)), np.zeros((128, 10)))\n                  for _ in range(10)) \n Optimize parameters in a loop \n opt_state = opt_init(net_params)\nfor i in range(10):\n  opt_state = step(i, opt_state, next(data_generator))\nnet_params = optimizers.get_params(opt_state)\n``` \n How it works \n Programming in machine learning is about expressing and transforming functions.\nTransformations include automatic differentiation, compilation for accelerators,\nand automatic batching. High-level languages like Python are great for\nexpressing functions, but usually all we can do with them is apply them. We lose\naccess to their internal structure which would let us perform transformations. \n JAX is a tool for specializing and translating high-level Python+NumPy functions\ninto a representation that can be transformed and then lifted back into a Python\nfunction. \n \n JAX specializes Python functions by tracing. Tracing a function means monitoring\nall the basic operations that are applied to its input to produce its output,\nand recording these operations and the data-flow between them in a directed\nacyclic graph (DAG). To perform tracing, JAX wraps primitive operations, like\nbasic numerical kernels, so that when they’re called they add themselves to a\nlist of operations performed along with their inputs and outputs. To keep track\nof how data flows between these primitives, values being tracked are wrapped in\ninstances of the  Tracer  class. \n When a Python function is provided to  grad  or  jit , it’s wrapped for tracing\nand returned. When the wrapped function is called, we abstract the concrete\narguments provided into instances of the  AbstractValue  class, box them for\ntracing in instances of the  Tracer  class, and call the function on them.\nAbstract arguments represent sets of possible values rather than specific\nvalues: for example,  jit  abstracts ndarray arguments to abstract values that\nrepresent all ndarrays with the same shape and dtype. In contrast,  grad \nabstracts ndarray arguments to represent an infinitesimal neighborhood of the\nunderlying\nvalue. By tracing the Python function on these abstract values, we ensure that\nit’s specialized enough so that it’s tractable to transform, and that it’s still\ngeneral enough so that the transformed result is useful, and possibly reusable.\nThese transformed functions are then lifted back into Python callables in a way\nthat allows them to be traced and transformed again as needed. \n The primitive functions that JAX traces are mostly in 1:1 correspondence with\n XLA HLO  and are defined\nin  lax.py . This 1:1\ncorrespondence makes most of the translations to XLA essentially trivial, and\nensures we only have a small set of primitives to cover for other\ntransformations like automatic differentiation. The  jax.numpy \nlayer  is written in pure\nPython simply by expressing NumPy functions in terms of the LAX functions (and\nother NumPy functions we’ve already written). That makes  jax.numpy  easy to\nextend. \n When you use  jax.numpy , the underlying LAX primitives are  jit -compiled\nbehind the scenes, allowing you to write unrestricted Python+Numpy code while\nstill executing each primitive operation on an accelerator. \n But JAX can do more: instead of just compiling and dispatching to a fixed set of\nindividual primitives, you can use  jit  on larger and larger functions to be\nend-to-end compiled and optimized. For example, instead of just compiling and\ndispatching a convolution op, you can compile a whole network, or a whole\ngradient evaluation and optimizer update step. \n The tradeoff is that  jit  functions have to satisfy some additional\nspecialization requirements: since we want to compile traces that are\nspecialized on shapes and dtypes, but not specialized all the way to concrete\nvalues, the Python code under a  jit  decorator must be applicable to abstract\nvalues. If we try to evaluate  x > 0  on an abstract  x , the result is an\nabstract value representing the set  {True, False} , and so a Python branch like\n if x > 0  will raise an error: it doesn’t know which way to go! \nSee  What’s supported  for more\ninformation about  jit  requirements. \n The good news about this tradeoff is that  jit  is opt-in: JAX libraries use\n jit  on individual operations and functions behind the scenes, allowing you to\nwrite unrestricted Python+Numpy and still make use of a hardware accelerator.\nBut when you want to maximize performance, you can often use  jit  in your own\ncode to compile and end-to-end optimize much bigger functions. \n What we\'re working on \n \n Documentation! \n Cloud TPU support \n Multi-GPU and multi-TPU support \n Full NumPy coverage and some SciPy coverage \n Full coverage for vmap \n Make everything faster \n Lowering the XLA function dispatch overhead \n Linear algebra routines (MKL on CPU, MAGMA on GPU) \n \n \n cond  and  while  primitives with efficient automatic differentiation \n \n Current gotchas \n Some things we don\'t handle that might surprise NumPy users:\n1. No in-place mutation syntax. JAX requires functional code. You can use\n   lax.dynamic_update_slice for slice updating that, under  @jit , will be\n   optimized to in-place buffer updating.\n2. PRNGs can be awkward, and linearity is not checked with a warning. \n Contributors \n So far, JAX includes lots of help and contributions from\n Jamie Townsend ,\n Peter Hawkins ,\n Jonathan Ragan-Kelley ,\n Alex Wiltschko ,\nGeorge Dahl,\n Stephan Hoyer ,\nSam Schoenholz,\n Eli Bendersky ,\nZak Stone,\n Alexey Radul ,\nMichael Isard,\nSkye Wanderman-Milne,\nand many others.', 'Swift for TensorFlow  APIs \n Deep learning library for Swift for TensorFlow. \n Requirements \n \n A latest Swift for TensorFlow toolchain. \n \n Usage \n A  Swift for TensorFlow toolchain \nis required to use this package. Add the following to your Swift package manifest. \n swift\npackages: [\n  .package(url: "https://github.com/tensorflow/swift-apis.git")\n] \n To get started, simply import  TensorFlow  in your Swift code. \n ```swift\nimport TensorFlow \n // Define a model.\nstruct Classifier: Layer {\n    var l1, l2: Dense \n func applied(to input: Tensor<Float>) -> Tensor<Float> {\n    let h1 = sigmoid(l1.applied(to: input))\n    return sigmoid(l2.applied(to: h1))\n}\n \n } \n var model = Classifier(...)\nlet optimizer = SGD (learningRate: 0.02)\nfor _ in 0..<1000 {\n    let (loss, 𝛁model) = model.valueWithGradient { model in\n        let ŷ = model.applied(to: x)\n        print("Prediction: (ŷ)")\n        return (y - ŷ).squared().mean()\n    }\n    print("Loss: (loss)")\n    optimizer.update(&model.allDifferentiableVariables,\n                     along: 𝛁model)\n}\n``` \n Building \n bash\nswift build \n Bugs \n Please report bugs and feature requests using GitHub issues in this repository. \n Community \n Discussion about Swift for TensorFlow happens on the\n swift@tensorflow.org \nmailing list. \n Contributing \n We welcome contributions: please read the  Contributor Guide \nto get started. It\'s always a good idea to discuss your plans on the mailing\nlist before making any major submissions. \n Code of Conduct \n In the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, gender identity and expression, level of\nexperience, education, socio-economic status, nationality, personal appearance,\nrace, religion, or sexual identity and orientation. \n The Swift for TensorFlow community is guided by our  Code of\nConduct , which we encourage everybody to read before\nparticipating.']
dargor,['postgres_rainbow_tables \n PoC of rainbow tables using PostgreSQL. \n Setup \n Caution:  these commands will  overwrite  any database named  rainbows , be careful ! \n cd sql/\nvim roles.sql # create the user in psql, and add its password to your ~/.pgpass file\n./drop_db.sh # if you have a previous rainbows database\n./create_db.sh\ncd ../\n./import.py \n After some time (OWASP SecLists took 27 minutes for 13M passwords), your database is ready. \n Queries \n Use  ./query.py -v <<hash>>  to find a password from its hash. \n With the test sample, you can do this : \n cat test.txt\necho -n 123123 | sha224sum -\n./query.py -v 23d5c51afade7a8701186250777f3c055c94984ce3d4aaed11438c0c \n This should give you  123123  back. \n Debug \n You might want to run this to get some useful (or not) messages : \n export DEBUG=Y \n License \n ISC.', "Colors, algorithms and fun \n A standard  #RRGGBB  color is a 24-bit number, meaning it has  256^3  =  16777216  possible combinations (!). \n I wondered how genetic algorithms worked, and how many iterations would be needed to find a given color. \n I thought it was a good choice, because colors are easy to compare, right ? \n \n Quick note about terminal emulators \n I lost some time wondering why my algorithms sometimes returned me colors that did not match, so I warn you : terminal emulators more or less correctly implement true colors, which are needed in our use case. \n You can test your terminal emulator by running  ./test_color.py  :\n \n If you don't have a nice output, try  st  : it's easy to compile and run locally, no need to install it as root !\n bash\nwget http://dl.suckless.org/st/st-0.6.tar.gz\ntar xzf st-0.6.tar.gz\ncd st-0.6/\nmake\ntic -s st.info\n./st & \nAnd voilà, ready to run with some nice colors ! \n run.py \n This is the benchmark main entry point. \n ```\n$ ./run.py -h\nusage: run.py [-h] -a {random,brute,genetic1,genetic2,genetic3} [-c COLOR]\n              [-d DELAY] [-f FITNESS] [-i ITERATIONS] [-p POPULATION_SIZE]\n              [-s SEED] \n optional arguments:\n  -h, --help            show this help message and exit\n  -a {random,brute,genetic1,genetic2,genetic3}, --algorithm {random,brute,genetic1,genetic2,genetic3}\n                        Algorithm to run\n  -c COLOR, --color COLOR\n                        Color to find\n  -d DELAY, --delay DELAY\n                        Delay between each iteration\n  -f FITNESS, --fitness FITNESS\n                        ΔE considered acceptable\n  -i ITERATIONS, --iterations ITERATIONS\n                        Number of iterations\n  -p POPULATION_SIZE, --population-size POPULATION_SIZE\n                        Size of population at each iteration\n  -s SEED, --seed SEED  Seed to initialize the random number generator\n``` \n Play with the various options, but the only really useful one is  -a . \n But what's a ΔE, my precious ? \n It's a measure of distance between two colors, lesser being closer. \n The  wikipedia  page and  python-colormath  documentation explain everything :\n* https://en.wikipedia.org/wiki/Color_difference\n* http://python-colormath.readthedocs.io/en/latest/ \n ΔE are noted below their corresponding colors in the output of  run.py . \n The  -f  option sets the minimum ΔE value required to pass the test, and default to  1.0 . \n Algo*.py \n All our algorithms derive an abstract base class  Algo , which is used by  run.py  to abstract annoying details. \n The base population of all algorithms is randomly chosen. \n AlgoBrute.py \n Brute force, the worst possible thing to do when you have 16 millions of combinations. \n $ ./run.py -a brute -s 42\n[...]\nColor to find   : ████ #390C8C\nColor found     : ████ #330A8B (ΔE = 0.980)\nIterations      : 139377\nPopulation size : 24\nColors tested   : 3345048\nAll that in     : 789.034 seconds \n This one erases the default random population to set its own, which allows it to try  N  ( N  being the population size) variations at each iteration. \n AlgoRandom.py \n Random, not necessarily a bad idea. \n $ ./run.py -a random -s 42\n[...]\nColor to find   : ████ #390C8C\nColor found     : ████ #3A0892 (ΔE = 0.896)\nIterations      : 516\nPopulation size : 24\nColors tested   : 12384\nAll that in     : 2.984 seconds \n This one has absolutely no subtlety : it generates a new random population at each iteration. \n AlgoGenetic.py \n Base abstract class for genetic algorithms : selection, reproduction and mutations are taken care of by this abstract class. \n The fittest parents get many mating possibilities, while the lesser ones get only a few mating opportunities (but they still get at least one, to preserve our genetic pool diversity). \n When we have enough children, we keep the best ones to form a new generation. \n After some iterations, the algorithm converges to a quite good solution. \n See this  page  for more details on genetic algorithms. \n An actual genetic algorithm must at least define  OPS  (available crossover operations) and  crossover  (their implementations). \n AlgoGenetic1.py \n $ ./run.py -a genetic1 -s 42\n[...]\nColor to find   : ████ #390C8C\nColor found     : ████ #360D8A (ΔE = 0.539)\nAlgorithm       : AlgoGenetic1\nIterations      : 15\nPopulation size : 26\nColors tested   : 390\nAll that in     : 0.149 seconds \n This one simply combines bits together, trying to maximize diversity. \n Works quite well. \n AlgoGenetic2.py \n $ ./run.py -a genetic2 -s 42\n[...]\nColor to find   : ████ #390C8C\nColor found     : ████ #391090 (ΔE = 0.861)\nAlgorithm       : AlgoGenetic2\nIterations      : 13\nPopulation size : 26\nColors tested   : 338\nAll that in     : 0.131 seconds \n This one is an artificial equivalent of how natural crossover works, trying to enhance each generation fitness. \n Can be really slow to converge, but its solutions are quite good. \n AlgoGenetic3.py \n $ ./run.py -a genetic3 -s 42\n[...]\nColor to find   : ████ #390C8C\nColor found     : ████ #36078C (ΔE = 0.717)\nAlgorithm       : AlgoGenetic3\nIterations      : 10\nPopulation size : 26\nColors tested   : 260\nAll that in     : 0.096 seconds \n This one combines both approaches, merging their strengths. \n Best average performance. \n Durations over 100 runs \n Data and scripts used in  bench.* . \n \n $ ./bench.sh\n[...]\nAlgoGenetic1 : .290 seconds (mean time)\nAlgoGenetic2 : .393 seconds (mean time)\nAlgoGenetic3 : .212 seconds (mean time) \n License \n ISC.", 'Setup \n Setup an access token and a  ~/.private_token.json  file, as specified in  repos_gitlab.py . \n Then you should be able to get a list of your available repos and wikis, if desired. \n Usage \n To save/update the list, do  ./repos_gitlab.py > REPOS . \n Then to clone/pull, do  ./UP . \n GitLab API \n https://docs.gitlab.com/ee/api/ \n License \n ISC.', 'I know, I know... chase a fly with a bazooka. \n May this code help you understand what Keras does under the hood.', 'May be useful for a plugin system, in a bot or something. \n', 'My custom Gentoo overlay \n Personal overlay for some packages I care of. \n Installation \n Install  app-eselect/eselect-repository  (new way) or  app-portage/layman  (old way), then: \n ```sh \n for app-eselect/eselect-repository \n eselect repository enable dargor \n for app-portage/layman \n layman -a dargor \n for both \n emaint sync -r dargor \n finally, if you use app-portage/eix (you really should) \n eix-postsync\neix-diff\n``` \n Now you can use  emerge  like usual. \n (optional) Setup GPG signature verification \n Install  sec-keys/openpgp-keys-dargor , then edit  /etc/portage/repos.conf/dargor.conf  to add : \n ini\nsync-git-verify-commit-signature = yes\nsync-openpgp-key-path = /usr/share/openpgp-keys/dargor.asc \n License \n GNU General Public License, version 2.', 'Redpanda in Docker \n Try  Redpanda  in Docker. \n Please note that upstream provides docker images as well : https://vectorized.io/docs/quick-start-docker \n Build \n sh\nmake build \n Beware: the build process is  very  resource intensive ! \n I tried very hard to enforce some reasonable limits, where "reasonable" is tailored for an 8 cores / 16GB laptop. \n The main problem here is the inability to limit the first  cmake  workers, which are forced to the available number of cores by upstream, including in a runtime-fetched dependency ( Seastar ) that can not be patched beforehand. \n As these workers are in fact  cc1plus  instances, a program known for its frugality, spawning 8 of them is almost guaranteed to end with an OOM, unless you are doing nothing else with your computer. \n docker build  seems to happily ignore any cpu/memory restriction, leaving no choice but a two step build:\n  - build a base image with all required dependencies\n  - spawn a builder container (with constrained resources thanks to  LXCFS , if available) to do the heavy work itself, and commit the resulting container as our Redpanda image \n Of course none of this should be necessary if upstream did not enforce  -j$(nproc)  without any override possibility (see  redpanda/cmake/oss.cmake.in  and  seastar/cooking_recipe.cmake  for details, search for  build_concurrency_factor ). Docker could also include LXCFS stuff to have reliable resource limits, it would be nice. \n Run \n sh\nmake run \n Redpanda should be accessible from ports  19092  (kafka) and  19644  (admin). \n This command will show Redpanda logs, use  ^C  to quit. \n Beware: data will  not  be persisted, but you may use a volume if needed. \n Usage \n This assume standard Kafka tools (tested with  net-misc/kafka-bin-2.6.0 ). \n Tools \n ```sh\n$ ./kafka-acls.sh --bootstrap-server localhost:19092 --list\nError while executing ACL command: org.apache.kafka.common.errors.UnsupportedVersionException: The broker does not support DESCRIBE_ACLS \n $ ./kafka-topics.sh --bootstrap-server localhost:19092 --topic test --create --partitions 1 --replication-factor 1\nCreated topic test. \n $ ./kafka-topics.sh --bootstrap-server localhost:19092 --list\ntest \n $ ./kafka-topics.sh --bootstrap-server localhost:19092 --topic test --describe\nTopic: test    PartitionCount: 1    ReplicationFactor: 1    Configs: partition_count=1,replication_factor=1\n    Topic: test    Partition: 0    Leader: 1    Replicas: 1    Isr: 1\n``` \n Consumer \n ```sh\n$ ./kafka-console-consumer.sh --bootstrap-server localhost:19092 --topic test \n no output yet, but should show producer messages \n ``` \n Producer \n ```sh\n$ ./kafka-console-producer.sh --bootstrap-server localhost:19092 --topic test --compression-codec snappy \n \n hello there !\n^D\n``` \n']
bunelr,['Haskell99Problems \n Solving the Ninety-Nine Haskell Problems', "To use:\nInstall  Virtual Box \nInstall  Vagant 1.4.3 \n Install Ruby and the Dev-Kit available  here  version 1.9.3\nInstall Bundle and run bundle install \n (Using berkshelf 2.0 and vagrant-berkshelf 1.3.7 due to not being able to build libgecode necessary for berkshelf v3) \n Install the vagrant plugin berkshelf and omnibus \n  vagrant plugin install vagrant-berkshelf --plugin-version 1.3.7\n vagrant plugin install vagrant-omnibus\n \n Then run  \n vagrant up\n \n to create your vm. You can re-provision by running  vagrant --provision \n Warning:\nIf you're using it from a windows machine, beware of the line endings in the config files which can make the provisioning fail.", 'Work in progress \n \n Set not tested \n Histogram not implemented \n UDP event not implemented \n No automated tests at all \n No support for proxy to push to datadog \n No control for log level modification \n No support for counter reporting zero until expiry \n No support for hostname detection / setting by configuration \n No support for monometric-multi point \n No support for forwarding \n No good command interaction (start/stop/status) \n No support for metric compression \n', 'discrete-optimization-assignment \n Webapp to collect, compile and validate OMA 2015\'s optimization assignment \n Futur employer of Rudy and Alban, please don\'t judge us on this, it was thrown together in an afternoon to help out the class \n TODO: \n Stop being so blatantly insecure (ie, running arbitrary code uploaded by people on the internet) \n Installation \n This is currently run on a Digital Ocean Machine.\nThe description of how we run everything follows. It may not be the simplest but it consisted in what we were familiar with. \n \n Clone the repo on your server \n Create a python virtualenv in the app folder\n cd discrete-optimization-assignment\nvirtualenv . \n Activate the virtualenv  source bin/activate \n Install all the dependencies  pip install -r requirements.txt \n Install a front end facing web server (such as Nginx), you can obtain it from your distribution\'s package manager \n Put the following configuration\'s snippet in  /etc/nginx/sites-available/assignment  and  /etc/nginx/sites-enabled/assignment  (It authorizes submission to take as much as 5 minutes to run, without the page erroring) \n \n ````\nserver {\n    location / {\n        proxy_pass http://127.0.0.1:8000; \n     proxy_connect_timeout       300;\n    proxy_send_timeout          300;\n    proxy_read_timeout          300;\n    send_timeout                300;\n}\n \n }\n```` \n \n Restart the webserver so that it takes the changes into account:  /etc/init.d/nginx restart \n In order to run the app even when we\'re not logged in the servor, we used superviord. To install it:  pip install supervisor \n Drop the following snippet into  /etc/supervisor/conf.d : \n \n [program:assignment]\ncommand = /home/user/discrete-optimization-assignment/run.sh\nuser = user                                                          ; User to run as\nstdout_logfile = /home/user/log.txt\nredirect_stderr = true                                                ; Save stderr in the same log\nenvironment=LANG=en_US.UTF-8,LC_ALL=en_US.UTF-8 \n* Install all the things that may be required by the students: make, gcc, ... \n Some notes: \n \n In the repo, the app is configured in debug mode. To deploy it, you should change the last line from \n \n python\n    app.run(debug = True, use_reloader = False, port=8000) \n to \n python\n    app.run(port=8000) \n \n You should also change the password in the  config.py  file. \n \n Before the first run, you may have to create an empty halloffame file and submission directory:  echo "{}" > halloffame.json && mkdir submission \n \n \n There is some basic monitoring going on, if you want to use it, you can install an Agent from Datadog. It is not required. \n \n', "Display the images of two folders on a webpage and allow to navigate through them. \n Register an alias to use it easily: \n $ alias imgdirdiff='~/workspace/imgdir_diff/server.py'\n \n And them use it simply with \n $ imgdirdiff img_folder1 img_folder2\n", "Karinr buffer \n Karinr-buffer  is an elisp-function for WhEn yOu wAnT To wRiTe lIkE ThIs but you are not 14 years old so you don't want it to take all day. \n If a region is selected, it will operate only on the region. Otherwise it will operate on the whole buffer.", 'Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis \n This repository contains the code used for the paper  Leveraging Grammar and\nReinforcement Learning for Neural Program\nSynthesis . \n Requirements \n We recommend installing this code into a virtual environment. In order to run\nthe code, you first need to install pytorch, following the instructions from\n the pytorch website . Once this is done, you can install\nthis package and its dependencies by running: \n bash\npip install cython\npython setup.py install \n The experiments in the original paper were run using the dataset found at  the\nKarel dataset webpage . We\nrecommend you download and extract them into the `./data** directory. \n Commands \n The code can be interacted with using two commands:  train_cmd.py  to perform\ntraining of a model and  eval_cmd.py  to perform testing. This section\nintroduces the possible option, you can also use  --help  to see what is\navailable. \n Train \n \n --kernel_size ,  --conv_stack ,  --fc_stack ,  --tgt_embedding_size ,\n   --lstm_hidden_size ,  --nb_lstm_layers  are flags to specify the architecture\n  of the model to learn. See  nps/network.py  to see how they are used.\n   --nb_ios  specifies how many of the IO pairs should be used as inputs to the\n  encoder (note that due to the architecture, even a model trained with  x  IO\n  can be used to do prediction, even if a different number of IOs is available\n  at test time). \n --use_grammar  makes the model use the handwritten syntax checker, found in\n   syntax/checker.pyx .  --learn_syntax  adds a Syntax LSTM to the model that\n  attempts to learn a syntax checker, jointly with the rest of the model. The\n  importance of this objective is controlled by the  --beta  parameter. \n --signal  allows to choose the loss, between  supervised ,  rl  and\n   beam_rl . Supervised attempts to reproduce the ground truth program, while\n   rl  and  beam_rl  try to maximize expected rewards. What rewards are used is\n  specified using the  --environment  argument (it can be Consistency to\n  evaluate coherence of the programs with the observed IO grids, Generalization\n  to also take into account the held out pair, or Perf to additionally include\n  consideration about number of steps taken.) In the case where the beam search\n  approximation is used, it is also possible to specify a Reward Combination\n  Function using  --reward_comb . The default one is  RenormExpected  but the\n  "bag of samples" version can be used by choosing  X1m1BagExpected  for 1/-1\n  rewards or  XBagExpected  for the general case. In order to be able to fit\n  experiments in a single GPU, you may need to adjust  --nb_rollouts  (how many\n  samples are taken from the model to estimate a gradient when using  rl ) or\n   --rl_beam  (the size of the beam search when using  beam_rl ). There is also\n  the  --rl_inner_batch  option that splits the computation of a batch into\n  several minibatches that are separately evaluated before doing a gradient\n  step. \n --optim_alg  chooses the optimization algorithm used,  --batch_size  allows\n  to choose the size of the mini batches.  --learning_rate  adjusts the learning\n  rate.  --init_weights  can be used to specify a \'.model\' file from which to\n  load weights. \n --train_file  specify the json file where to look for the training samples\n  and  --val_file  indicates a validation set. The validation set is used to\n  keep track of the best model seen so far, so as to perform early stopping. The\n   --vocab  file is there to give a correspondence between tokens and indices in\n  the learned predictions. Setting  --nb_samples  allows to train on only part\n  of the dataset (0, the default, trains on the whole dataset.).\n   --result_folder  allows to indicate where the results of the experiment\n  should be stored. Changing  --val_frequency  allows to evaluate accuracy on\n  the validation set less frequently.  \n Specify  --use_cuda  to run everything on a GPU. You can use the\n   CUDA_VISIBLE_DEVICES  to run on a specific GPU. \n \n ```bash \n Train a simple supervised model, using the handcoded syntax checker \n train_cmd.py --kernel_size 3 \\\n             --conv_stack "64,64,64" \\\n             --fc_stack "512" \\\n             --tgt_embedding_size 256 \\\n             --lstm_hidden_size 256 \\\n             --nb_lstm_layers 2 \\\n             \\\n             --signal supervised \\\n             --nb_ios 5 \\\n             --nb_epochs 100 \\\n             --optim_alg Adam \\\n             --batch_size 128 \\\n             --learning_rate 1e-4 \\\n             \\\n             --train_file data/1m_6ex_karel/train.json \\\n             --val_file data/1m_6ex_karel/val.json \\\n             --vocab data/1m_6ex_karel/new_vocab.vocab \\\n             --result_folder exps/supervised_use_grammar \\\n             \\\n             --use_grammar \\\n             \\\n             --use_cuda \n Train a supervised model, learning the grammar at the same time \n train_cmd.py --kernel_size 3 \\\n             --conv_stack "64,64,64" \\\n             --fc_stack "512" \\\n             --tgt_embedding_size 256 \\\n             --lstm_hidden_size 256 \\\n             --nb_lstm_layers 2 \\\n             \\\n             --signal supervised \\\n             --nb_ios 5 \\\n             --nb_epochs 100 \\\n             --optim_alg Adam \\\n             --batch_size 128 \\\n             --learning_rate 1e-4 \\\n             --beta 1e-5 \\\n             \\\n             --train_file data/1m_6ex_karel/train.json \\\n             --val_file data/1m_6ex_karel/val.json \\\n             --vocab data/1m_6ex_karel/new_vocab.vocab \\\n             --result_folder exps/supervised_learn_grammar \\\n             \\\n             --learn_syntax \\\n             \\\n             --use_cuda \n Use a pretrained model, to fine-tune it using simple Reinforce \n Change the --environment flag if you want to use a reward including performance. \n train_cmd.py  --signal rl \\\n              --environment BlackBoxGeneralization \\\n              --nb_rollouts 100 \\\n              \\\n              --init_weights exps/supervised_use_grammar/Weights/best.model \\\n              --nb_epochs 5 \\\n              --optim_alg Adam \\\n              --learning_rate 1e-5 \\\n              --batch_size 16 \\\n              \\\n              --train_file data/1m_6ex_karel/train.json \\\n              --val_file data/1m_6ex_karel/val.json \\\n              --vocab data/1m_6ex_karel/new_vocab.vocab \\\n              --result_folder exps/reinforce_finetune \\\n              \\\n              --use_grammar \\\n              \\\n              --use_cuda \n Use a pretrained model, fine-tune it using BS Expected reward \n Change the --environment flag if you want to use a reward including performance. \n Change the --reward_comb flag if you want to use one of the "bag of samples" loss \n Remove the --rl_use_ref flag if you don\'t want to make use of the known ground truth in \n the bag. \n train_cmd.py  --signal beam_rl \\\n              --environment BlackBoxGeneralization \\\n              --reward_comb RenormExpected \\\n              --rl_inner_batch 8 \\\n              --rl_use_ref \\\n              --rl_beam 64 \\\n              \\\n              --init_weights exps/supervised_use_grammar/Weights/best.model \\\n              --nb_epochs 5 \\\n              --optim_alg Adam \\\n              --learning_rate 1e-5 \\\n              --batch_size 16 \\\n              \\\n              --train_file data/1m_6ex_karel/train.json \\\n              --val_file data/1m_6ex_karel/val.json \\\n              --vocab data/1m_6ex_karel/new_vocab.vocab \\\n              --result_folder exps/beamrl_finetune \\\n              \\\n              --use_grammar \\\n              \\\n              --use_cuda \n ``` \n Evaluation \n The evaluation command is fairly similar. Any flags non-specified has the same\nrole as for the  train_cmd.py  command. The relevant file is  nps/evaluate.py . \n \n --model_weights  should point to the model to evaluate. \n --dataset  should point to the json file containing the dataset you want to\n  evaluate against. \n --output_path  points to where the results should be written. This should be\n  a prefix for all the names of the files that will be generated, followed  \n --dump_programs  can be used to investigate by dumping the programs returned\n  by the model. \n --eval_nb_ios  is analogous to  --nb_ios  during training, how many IO pairs\n  should be used as input to the model. \n --val_nb_samples  is analogous to  --nb_samples , can be used to do\n  evaluation on only part of the dataset. \n --eval_batch_size  specifies the batch size to use during decoding. This\n  doesn\'t affect accuracies and batching operations only allows to go faster. \n --beam_size  controls the size of the beam search to run when decoding the\n  programs and  --top_k  should be the largest integer for which the accuracies\n  should be computed. \n \n This will generate a set of files. If  --dump_programs  is passed, the  --top_k \nmost likely programs for each element of the dataset will be dumped, with their\nrank and their log-probability in the  generated  subfolder. This will also\ninclude the reference program, under the name  target .  \n The values at various ranks are reported in the generated files.  exactmatch \ncorresponds to exactly reproducing the input,  semantic  corresponds to\ngenerating a program being correct on the observed IOs,  fullgeneralize  means\ngenerating a program correct on the observed AND held-out IOs.  syntax  simply\nindicates that the program was synctatically correct. If the file,\n semantic_top3.txt  contains the number 75.00, this means that for 75.00% of the\nsamples, one of the top 3 programs according to the model will be semantically\ncorrect on the observed samples. \n ```bash \n Evaluate a trained model on the validation set, dumping programs to allow for debugging. \n eval_cmd.py --model_weights exps/supervised_use_grammar/Weights/best.model \\\n            \\\n            --vocabulary data/1m_6ex_karel/new_vocab.vocab \\\n            --dataset data/1m_6ex_karel/val.json \\\n            --eval_nb_ios 5 \\\n            --eval_batch_size 8 \\\n            --output_path exps/supervised_use_grammar/Results/ValidationSet_ \\\n            \\\n            --beam_size 64 \\\n            --top_k 10 \\\n            --dump_programs \\\n            --use_grammar \\\n            \\\n            --use_cuda \n Evaluate a trained model on the test set \n eval_cmd.py --model_weights exps/beamrl_finetune/Weights/best.model \\\n            \\\n            --vocabulary data/1m_6ex_karel/new_vocab.vocab \\\n            --dataset data/1m_6ex_karel/test.json \\\n            --eval_nb_ios 5 \\\n            --eval_batch_size 8 \\\n            --output_path exps/beamrl_finetune/Results/TestSet_ \\\n            \\\n            --beam_size 64 \\\n            --top_k 10 \\\n            --use_grammar \\\n            \\\n            --use_cuda \n ``` \n Citation \n If you use this code in your research, consider citing: \n @Article{Bunel2018,\n  author        = {Bunel, Rudy and Hausknecht, Matthew and Devlin, Jacob and Singh, Rishabh and Kohli, Pushmeet},\n  title        =  {Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis},\n  journal      = {ICLR},\n  year         = {2018},\n}', 'jax_verify: Neural Network Verification in JAX \n \n \n Jax_verify is a library containing JAX implementations of many widely-used neural network verification techniques. \n Overview \n If you just want to get started with using jax_verify to verify your neural\nnetworks, the main thing to know is we provide a simple, consistent interface\nfor a variety of verification algorithms: \n python\noutput_bounds = jax_verify.verification_technique(network_fn, input_bounds) \n Here,  network_fn  is any JAX function,  input_bounds  define bounds over\npossible inputs to  network_fn , and  output_bounds  will be the computed bounds\nover possible outputs of  network_fn .  verification_technique  can be one of\nmany algorithms implemented in  jax_verify , such as  interval_bound_propagation \nor  crown_bound_propagation . \n The overall approach is to use JAX’s powerful  program transformation system , \nwhich allows us to analyze general network structures defined by  network_fn \nand then to define corresponding functions for calculating\nverified bounds for these networks. \n Verification Techniques \n The methods currently provided by  jax_verify  include: \n \n SDP-FO (first-order SDP verification,  Dathathri et al 2020 ) \n Non-convex ( Bunel et al 2020 ) \n Interval Bound Propagation ( Gowal et al 2018 ,  Mirman et al 2018 ) \n Fast-Lin ( Wong and Kolter 2017 ,  Weng et al 2018 ) \n CROWN ( Zhang et al 2018 ) \n CROWN-IBP ( Zhang et al 2019 ) \n Planet (also known as the "LP" or "triangle" relaxation,  Ehlers 2017 ), currently using  CVXPY  as the LP solver \n \n Installation \n Stable : Just run  pip install jax_verify  and you can  import jax_verify  from any of your Python code. \n Latest : Clone this directory and run  pip install .  from the directory root. \n Getting Started \n We suggest starting by looking at the minimal examples in the  examples/  directory.\nFor example, all the bound propagation techniques can be run with the  run_boundprop.py  script: \n bash\ncd examples/\npython3 run_boundprop.py --boundprop_method=interval_bound_propagation \n For documentation, please refer to the  API reference page . \n Notes \n Contributions of additional verification techniques are very welcome. Please open\nan issue first to let us know. \n This is not an official Google product.']
albanD,['To use (make sure to have g++ installed): \n git clone git@github.com:albanD/sudoku-solver.git \n cd sudoku-solver \n make \n ./run.sh', 'c in lua \n This repo presents a simple example on how to embed c code in lua/ torch . \n To do so we will present how to create a new Torch layer that will just multiply the input by a constant. This repository contains 3 different implementation:\n*  luaModule  shows how to create the module with only lua code.\n*  cModule  shows how to create the module with some functions implemented in C. Here C functions are implemented for a fixed tensor type.\n*  cGenericModule  shows how to create the module with some functions implemented in C. Here we use the macro tricks fom  TH  to generate the functions for all needed types (Float and Double). \n It also contains  test.lua  that is a simple example that will load the custom module and try to use it with both FloatTensor and DoubleTensor and verify that the outputs are correct.', 'Adaptive Neural Compilation \n This repository contains the code associated to the Adaptive Neural Compilation paper that can be found on  arxiv . \n In this repository, you can find two projects:\n* The compiler in the  compilation/  folder allows to transform a program written in a low level language (examples can be found  here ) into a set of weights for our model. These weights are represented as a configuration file that will be used by the adaptation project.\n* The learning part in the  adaptation/  folder provides the execution model, the training script and all the utilities to run the experiments. Examples of configurations files can be found  here  and examples of training commands can be found  here \n If you use this work, please cite:\n @article{anc,\n    title={Adaptive Neural Compilation},\n    author={Bunel, Rudy and Desmaison, Alban and Kohli, Pushmeet and Torr, Philip H.S and Kumar, M. Pawan},\n    journal={arXiv preprint arXiv:1605.07969},\n    year={2016}\n}', "lua-dogstatsd \n Lua implementation of  datadog's python dogstatsd interface . \n Difference \n \n dogstatsd.new  method that replaces the  initialize . \n Use of default route from  /proc/net/route  for Linux is not implemented. \n event  reporting is not implemented. \n", 'Crayon    \n Crayon is a framework that gives you access to the visualisation power\nof\n TensorBoard  with\n any language . Currently it provides a Python and a Lua interface, however\nyou can easily implement a wrapper around the\nprovided  RESTful API . \n \n This system is composed of two parts:\n* A server running on a given machine that will be used to display tensorboard\n  and store all the data.\n* A client embedded inside your code that will send the datas to the server. \n Note that the server and the client  do not  have to be on the same machine. \n Install \n Server machine \n The machine that will host the server needs to\nhave  docker  installed. The server is completely\npackaged inside a docker container. To get it, run: \n bash\n$ docker pull alband/crayon \n Client machine \n The client machine only need to install the client for the required language.\nDetailed instructions can be read by nagivating to\ntheir  respective directories . \n TL;DR: \n \n Lua / Torch -  $ luarocks install crayon \n Python 2 -  $ pip install pycrayon \n Python 3 -  $ pip3 install pycrayon \n \n Usage \n Server machine \n To start the server, run the following: \n bash\n$ docker run -d -p 8888:8888 -p 8889:8889 --name crayon alband/crayon \n Tensorboard is now accessible on a browser at  server_machine_address:8888 . The\nclient should send the data at  server_machine_address:8889 . \n Client \n See the documentation for the required language: \n \n Lua \n Python \n', 'pytorch_dev_env_setup \n Small script to setup a full dev-env for different python versions \n What it will create \n HOME=~/local is the base of everything\nHOME/installs all the local installs\nHOME/installs_source\nHOME/pytorch/PY_VERSION contains a pytorch install for that particular python version \n WIP: \n Brand new install \n Cmake \n wget whatever from https://cmake.org/download/\ntar zxvf cmake-3. \ncd cmake-3. \n./bootstrap --prefix=/home/albandes/local/cmake_install\nmake -j$(nproc)\nmake install\nadd cmake_install to PATH \n OpenBLAS \n git clone https://github.com/xianyi/OpenBLAS\nmake\nmake PREFIX=/home/albandes/local/openblas_install install\nadd /home/albandes/local/openblas_install/lib to LD_LIBRARY_PATH \n Ccache \n Get autoconf from http://ftp.gnu.org/gnu/autoconf/\ninstall\nGet ccache https://github.com/ccache/ccache\n./autogen.sh\n./configure\nmake install prefix=/home/albandes/local/ccache_install\nln -s ~/local/tmp_ccache_install/bin/ccache ~/local/tmp_ccache_install/bin/cc\nln -s ~/local/tmp_ccache_install/bin/ccache ~/local/tmp_ccache_install/bin/c++\nln -s ~/local/tmp_ccache_install/bin/ccache ~/local/tmp_ccache_install/bin/gcc\nln -s ~/local/tmp_ccache_install/bin/ccache ~/local/tmp_ccache_install/bin/g++\nln -s ~/local/tmp_ccache_install/bin/ccache ~/local/tmp_ccache_install/bin/nvcc\nexport PATH=/home/albandes/local/tmp_ccache_install/bin:$PATH \n lld (not possible in devserver) \n direnv \n Python stuff \n cpython download\ncpython local install\nuse local pip to install virtualenv\nuse virtualenv to create env for this python\ndirenv is either provided or can be downloaded from they github\nadd  eval "$(direnv hook bash)"  at the end of bashrc and allow all the necessary folders \n Pytorch \n git clone\nsubmodule update init recursive\ngit remote add alban git@github.com:albanD/pytorch.git\npip install requirements.txt\npip install ninja\npython setup.py develop \n Pytorch update \n git checkout\nsubmodule update\npython setup.py clean\npython setup.py develop', 'subclass zoo \n This repository contains a number of examples of Tensor subclasses in PyTorch,\nspecifically using  __torch_dispatch__  to integrate deeply into PyTorch\'s\nexisting subsystems (there\'s also some use of modes as well).  We\'re still\nworking out good APIs for working with Tensor subclasses, and this repository\nis here to tell you about what we\'ve figured out so far!  To run these\nexamples, you will want a recent nightly of PyTorch. \n Here\'s what\'s in the repo so far: \n \n inner_autograd_tensor.py  shows how to override autograd from\n   __torch_dispatch__ , by deferring autograd to the inner tensor on a\n  subclass. \n negative_tensor.py  is a reimplementation of negative tensor views as\n  implemented in PyTorch core (https://github.com/pytorch/pytorch/pull/56058) \n python_meta_tensor.py  is a demonstration of how to extend an existing\n  tensor (meta tensor) with some extra behavior (in this case, implementations\n  of meta functions for operations that don\'t support it natively) \n sparse_output.py \n tracer_tensor.py \n trivial_tensors.py  is a comparison for two ways how to "wrap" tensors,\n  one using inheritance (is-a) and one using composition (has-a) (so called\n  wrapper tensors) \n verifier_tensor.py \n \n There are also some utility files: \n \n base_tensor.py  contains a common superclass that most of our tensors\n  inherit from, that fixes up some problems with directly inheriting from\n  torch.Tensor.  We intend to upstream these changes so that this superclass\n  is not necessary. \n utils.py  contains some handy utility functions that we found ourselves\n  repeatedly using in our implementations. \n \n We\'re still working on the APIs in questions, so sometimes there will be bugs.\n bug_zoo.py  contains repros for known bugs we\'re tracking in PyTorch proper. \n TODO \n \n CUDA sanitizer in Python (hard cuz no event hooks) \n Sparse gradients / outputs per Christian (using modes; gradients hard cuz\n  need torch function mode) \n SSD tensor \n Reimplement functionalization tensor \n Nested tensor \n Custom allocator mode (albanD) \n Lazy tensor \n Immutable tensor \n Various ways of writing FX passes https://gist.github.com/1c640ea30fd7451b08e90e34461459c1 \n \n Work plan \n \n TODO: merge BaseTensor into Tensor \n TODO: torch function disable https://github.com/pytorch/pytorch/pull/73942 \n \n Get rid of  fill_defaults \n \n \n Compositionality \n \n TODO: suppress elem in init \n \n Developer notes \n \n This repo is formatted with ufmt and autoflakes.  Use  ./format.sh  to\n  reformat all the files in this repository. \n']
klgraham,['naive_bayes_classifier \n This is simple Naive Bayes classifier, for labeling emails as spam or not spam. \n Right now, I\'m building like this: \n scalac -d classes src/NaiveBayes.scala \n And running like this: \n scala -classpath classes/ NaiveBayes /path/to/your/data/directory \n This was tested on the lemm-stop subset of the Lingspam Dataset. Punctuation, numbers, and the "Subject" header were deleted. Tab and newline characters were replaced by a single space. See http://csmining.org/index.php/ling-spam-datasets.html for more info.', 'One-dimensional Particle Physics and BubbleSort \n I recently perused  Donald Knuth\'s website  and saw that he once had a lecture called  "Constructing bubblesort at random: one-dimensional particle physics" . As a physicist and software engineer, I naturally found this interesting. So this short bit of code takes a chain of 1s and 0s, as specified below, and sorts them at random. It also tells you how many passes it takes. Also, I\'m not yet clear on how this relates to physics, at least not in a detailed sense. I\'ll have to think about that some more. \n Here\'s a note from one of the papers Knuth linked to: \n Start with infinitely many 1s followed by infinitely many 0s; then randomly interchange adjacent elements that are out of order. \n 1D physics \n The physics connection becomes a bit more clear if you imagine a 1D lattice, where the 1s are particles and the 0s are vacancies. Now, pick any site k and ask how many particles are to the right of site k. Call the result $R_k$. The number of 0s to the left of site k is $L_k$. Note that if $k=N/2$, then $R_k=L_k$. \n It might be interesting to track how $R_k$ and $L_k$ change with "time". Here, time means the number of random swaps needed to sort the list. For a long lattice, it might also be interesting to compute the distribution of 1s or 0s as a function of lattice site index. Then we could watch it change from all 1s on the left to all 1s on the right. Might look like a Heaviside Step Function inverting bit by bit.', "Deep Learning and Neural Networks \n I'm learning about deep learning and neural networks. I'm also playing with Prismatic's Graph library to see how I can use it to structure a computation. This repo is my sandbox. \n Usage \n Perhaps something will go here later... \n License \n Copyright © 2013 Ken Graham \n Distributed under the MIT License.", 'Watershed \n \n Watershed provides a framework for working with Probabilistic Graphical\nModels (PGMs) in Clojure. It\'s designed with experimentation and learning in\nmind, so you can quickly define your own PGM and perform queries. At\npresent, this is setup mainly for analyzing Bayesian Networks, which are also\nDirected Acyclic Graphs. \n Probability distributions currently included: \n * Uniform\n* Standard Normal\n* Boolean (true/false)\n* Bernoulli\n* Exponential\n* Discrete Uniform\n* Poisson\n* Binomial\n* Fair coin\n* Biased coin\n* Pair of N-Sided Dice\n \n Bayesian Networks currently included: \n * Rain example\n \n Planned updates: \n * Convert all basic distribution code to call Incanter functions. No need to reinvent the wheel.\n* Add Bayesian Networks:\n  * Student example from Koller & Friedman\'s PGM textbook\n \n Installation \n Leiningen \n Add the following to your  :dependencies  in  project.clj : \n clj\n[watershed "0.2.0-SNAPSHOT"] \n Maven \n xml\n<dependency>\n  <groupId>watershed</groupId>\n  <artifactId>watershed</artifactId>\n  <version>0.2.0-SNAPSHOT</version>\n</dependency> \n Usage \n For now, please look at the examples in examples.clj. They should all be\nstraightforward. There are two PGM examples: \n 1. the common "Rain" example\n2. the Student\'s example\n \n License \n Copyright © 2015 Kenneth Graham \n Distributed under the  Eclipse Public License , same as Clojure.', "Descent \n As projects grow in complexity, their dependencies often do as well. When release time comes, it may help to know how your various libraries depend on one another. Descent lets you visualize project dependencies. \n \n Installation \n \n Download from https://github.com/klgraham/descent. \n cd  to the descent directory. \n Run  lein uberjar  if you want to run using a jar, then look at the Usage below. \n Run  lein run <args> , if you don't want to use a jar. See Usage example for arguments. \n \n Usage \n $ java -jar descent-0.1.0-standalone.jar <directory with poms> <group-id prefix to filter on OR use empty string> <image-name>\n$ lein run <directory with poms> <group-id prefix to filter on OR use empty string> <image-name>\n \n License \n Copyright © 2014 Kenneth L. Graham \n Distributed under the  Eclipse Public License , same as Clojure.", 'markov-text \n FIXME: description \n Installation \n Download from http://example.com/FIXME. \n Usage \n FIXME: explanation \n $ java -jar markov-text-0.1.0-standalone.jar [args]\n \n Options \n FIXME: listing of options this app accepts. \n Examples \n ... \n Bugs \n ... \n Any Other Sections \n That You Think \n Might be Useful \n License \n Copyright © 2015 FIXME \n Distributed under the Eclipse Public License either version 1.0 or (at\nyour option) any later version.', 'Introduction \n Percolation models are used to study the physics of disordered systems and critical phenomena. Percolation is one of the simplest probabilistic models that exhibits "critical phenomena", which is abrupt changes in a system\'s properties are triggered by changes in the value of an adjustable parameter. \n Here are a few nice intros to percolation theory: \n \n http://wwwf.imperial.ac.uk/~mgastner/percolation/percolation.html \n http://www.mit.edu/~levitov/8.334/notes/percol_notes.pdf (textbook) \n \n Usage: \n The object Percolator takes two inputs: the lattice size (N x N lattice) and fraction of lattice sites which are open. Usage instructions follow. \n \n $ sbt \n > run <lattice-size> <% open lattice sites> \n', 'This is an example of how to display attributed strings with an NSTextView that lives inside of an NSScrollView.', 'Minibrain: A Simple Java Neural Network \n This library is the product of working through a couple of neural network and deep learning tutorials:\n- http://ufldl.stanford.edu/tutorial/\n- http://neuralnetworksanddeeplearning.com/index.html', 'Swift implementation of the probability monad.', 'simple-neural-networks \n A one input Perceptron in Swift.', "Reference Library \n This is a collection of research papers related to topics I'm interested in. The topics generally fall into things at the intersection of machine learning, natural language processing, probability and statistics, and physics (mainly statistical physics).", 'lua-c-embedding \n Here are some notes/instructions for embedding Lua in C. Initially, we follow along with  Marek Vavrusa\'s blog post . Later, we move beyond and see how to interact with things in Torch. \n Note: This was initially setup to embed the LuaJIT, but I then discovered the issue with building 64-bit LuaJIT library to use with the JNI. So, I switched to plain Lua instead. \n Install Torch and Lua \n This repo includes the Torch distribution. Installation instrutions follow: \n \n git clone https://github.com/klgraham/luajit-c-embedding.git ~/lua-c-embedding --recursive \n cd ~/lua-c-embedding/torch-distro \n install dependencies:  bash install-deps \n install Lua, LuaRocks, and Torch:  TORCH_LUA_VERSION=LUA51 ./install.sh \n cd .. \n creating an environment variable  $TORCH_HOME  pointing to your torch installation, which is  ~/lua-c-embedding/torch-distro/install . Note that if you skip this step, you\'ll need to update the makefiles in the JNI examples. \n \n Hello, World! \n In the hello-world directory you\'ll notice a Lua script named hello.lua which contains  print(\'Hello, World!\')  and a C file, hello.c, which looks like this (modified from the above blog post): \n ``` \n include  \n include  \n include  \n include  \n int main()\n{\n  int status;\n  lua_State *L; \n L = luaL_newstate(); // open Lua\n  if (!L) {\n    return -1; // Checks that Lua started up\n  } \n luaL_openlibs(L); // load Lua libraries \n status = luaL_loadfile(L, argv[1]);  // load Lua script\n  int ret = lua_pcall(L, 0, 0, 0); // tell Lua to run the script\n  if (ret != 0) {\n    fprintf(stderr, "%s\\n", lua_tostring(L, -1)); // tell us what mistake we made\n    return 1;\n  } \n lua_close(L); // Close Lua\n  return 0;\n}\n``` \n To compile the C, use: \n gcc hello.c -pagezero_size 10000 -image_base 100000000 -I$TORCH_HOME/include -I/usr/local/include -L/usr/local/lib -L$TORCH_HOME/lib -llua -lm -o hello.out , noting that  -pagezero_size 10000 -image_base 100000000  only needs to be included on macOS systems. Also note how the LuaJIT include directory has been included in the compiler search path.  \n You can now execute the Lua script with  ./hello.out hello.lua . \n Understanding the C Code \n To interact with Lua from C we will use  Lua\'s C API . See http://pgl.yoyo.org/luai/i/_ for great documentation on the functions in Lua\'s C API. Let\'s look at the each part of hello-luajit.c.  \n \n luaL_newstate : Starts up Lua and returns a new Lua state \n luaL_openlibs : loads the Lua standard libraries \n luaL_loadfile : Loads the Lua script \n lua_pcall : runs the Lua script and returns an error code \n lua_close : closes Lua \n \n Using the Lua Stack \n Lua uses a stack to exchange values with C. To get a value from Lua, you call Lua to push the value onto the stack. To pass a value from C to Lua, you call Lua to push the value onto the stack and then pop the value. When pushing a value onto the stack, it\'s important to know if the stack has room for the new value. The lua-users wiki has  a nice example of this . Let\'s look a couple of examples. \n Example 1 \n Suppose we want to compute the factorial of a number in Lua and make the result of the computation available in C. Go into the  factorial  directory. The Lua code is in the file factorial.lua: \n function factorial(n)\n  local result = 1\n  for i = 1, n do\n    result = result * i\n  end\n  return result\nend \n From C, we\'ll call factorial.lua and then get the result from the Lua stack and print to stdout. \n ``` \n include  \n include  \n include  \n include  \n include  \n int main(int argc, char  argv[])\n{\n  lua_State  L; \n L = luaL_newstate(); // open Lua \n  luaL_openlibs(L); // load Lua libraries\n  int n = atoi(argv[1]); \n luaL_loadfile(L, "factorial.lua");\n  lua_pcall(L, 0, 0, 0); // Execute script once to create and assign functions \n lua_getglobal(L, "factorial"); // function to be called\n  lua_pushnumber(L, n); // push argument \n if (lua_pcall(L, 1, 1, 0) != 0) // 1 argument, 1 return value\n  {\n    fprintf(stderr, "%s\\n", lua_tostring(L, -1));\n    return 1;\n  } \n int result = lua_tonumber(L, -1);\n  lua_pop(L, 1); // pop returned value\n  printf("%d! is %d\\n", n, result);   \n lua_close(L); \n  return 0;\n}\n``` \n This example is simple: \n \n First, we load the Lua file containing the factorial function \n Call  lua_pcall  to execute the script and create the functions \n Call  lua_getglobal  to specify which function you wish to call \n Push the function arguments to the stack, in order \n Call  lua_pcall  again, specifying the number of arguments and return values \n Use  lua_tointeger  to get the return value from the top of the stack \n \n You\'ll notice that at the end we pop the returned value from the stack with  lua_pop . For such a simple example, this is not needed since we\'re killing Lua afterwards, but for more complex uses you\'ll definitely want to clean up the stack before moving on. \n You can compile with  gcc factorial.c -pagezero_size 10000 -image_base 100000000 -I$TORCH_HOME/include -I/usr/local/include -L/usr/local/lib -L$TORCH_HOME/lib -llua -lm -o fact.out . You can run with  ./fact.out <some integer> \n Example 2 \n Now we\'ll use Torch to compute the trace of a matrix. For this, we\'ll simply execute a Lua script that requires Torch: \n ```\nrequire(\'torch\') \n function cosine(theta)\n  return torch.cos(theta)\nend\n``` \n This example is functionally identical to the previous one. You should be able to do this on your own now. If you run into any trouble, just look at torch-test.c. \n Using Lua From Java \n This requires the JNI. So, first, we\'ll look at how to use it for a simple example. \n Hello, World, v2.0 \n First, we make a Java class, HelloWorldJNI.java, that loads a native (C) library, helloworld, that contains a method  hello() . \n ```\npublic class HelloWorldJNI \n{\n  private native void hello(); \n public static void main(String[] args) \n  {\n    System.load(args[0]);\n    new HelloWorldJNI().hello(); \n  }\n}\n``` \n Next steps are to compile the java code with  javac HelloWorldJNI.java  to get the a class file, HelloWorld.class, and then generate the JNI header file with  javah HelloWorldJNI . \n Then we create the JNI C implementation, HelloWorldJNI.c: \n ``` \n include  \n include  \n include "HelloWorldJNI.h" \n JNIEXPORT void JNICALL Java_HelloWorldJNI_hello(JNIEnv *env, jobject thisObj)\n{\n  printf("Hello, World!\\n");\n  return;\n}\n``` \n Now we need to compile the C code and link everything up. For this we can use: \n gcc -I"$JAVA_HOME/include" -shared -o libhelloworld.dylib HelloWorldJNI.c \n Notice that we\'re including the location of JAVA_HOME/include, are creating a shared library, and are naming the library libhelloworld.dylib. To run the java application, you use  java HelloWorldJNI <full path to helloworld.dylib> . \n Using Torch from Java \n We\'re going to adapt the earlier example where we compute the factorial of a number. This is on the jni-example branch. You can build and run in the  jni-example  directory by using this: \n make && java FactorialJNI <full path to libfactorial.dylib> <int of your choice> \n Note, there is an issue using 64-bit LuaJIT on macOS: \n \n http://comments.gmane.org/gmane.comp.lang.lua.luajit/4817 \n http://stackoverflow.com/questions/14840569/sigsegv-error-in-some-lua-c-code \n http://stackoverflow.com/questions/13400660/binding-lua-in-static-library-segfault?rq=1 \n http://nticformation.com/solutions_problems.php?tuto=59091&subCategory=c+lua+jni+luajit&Category=C+Language \n \n Based on the last of the above links, I think the issue is that  -pagezero_size 10000 -image_base 100000000  is not included when compiling the JNI stuff. The problem is that when it is included, the compiler says  \n \n -pagezero_size option can only be used when linking a main executable\nwhich means those params can\'t be used for shared libraries (which JNI needs). \n \n I\'ve confirmed that the above is the problem via  a thread  involving Lua\'s maintainer, Mike Pall. This is specifically an issue on 64-bit macOS. If I compile a 32-bit LuaJIT from source, then I should be able to get things to work. But, a 32-bit version might not be as useful for our purposes. To build the 32-bit LuaJIT binary, you can run the following commands inside the LuaJIT directory: \n CFLAGS="-arch i386" GCCFLAGS="-arch i386" LDFLAGS="-arch i386" make && sudo make install \n Configuring Lua State \n Let\'s say that you want to set a variable to a constant and then use that constant multiple times in a calculation. One way to do this in Lua is to set a global variable and then refer to that global by name in later calculations. A simple example of this is in the directory  config-example . It computes the factorial of a number and then computes the sum of integers from some  m  up to that number.  \n The Lua code uses the earlier factorial function and adds a sum function. This time, you\'ll notice that the sum references a variable  N . We\'ll set that variable from Java. \n ```\nfunction factorial(n)\n  local result = 1\n  for i = 1, n do\n    result = result * i\n  end\n  return result\nend \n -- computes $\\sum_{i=m}^N i$\nfunction sum(m)\n  local result = 0\n  for i = m, N do\n    result = result + i\n  end\n  return result\nend\n``` \n The Java code is shown below. Notice that the constructor has methods that start and close Lua.\n``` \n import java.lang.Throwable; \n public class ComputationJNI\n{\n  private native int sum(int m);\n  private native String factorial(int n);\n  private native void startLua();\n  private native void closeLua(); \n public ComputationJNI()\n  {\n    this.startLua();\n  } \n @Override\n  protected void finalize() throws Throwable\n  {\n    this.closeLua();\n  } \n public static void main(String[] args)\n  {\n    System.load(args[0]);\n    int n = Integer.valueOf(args[1]); \n ComputationJNI jni = new ComputationJNI();\nString fact = jni.factorial(n);\nSystem.out.println(n + "! = " + fact + "\\n");\nint sum = -1;\n\nfor (int m = 0; m < 10; m++)\n{\n  sum = jni.sum(m);\n  System.out.println("Sum(" + m + ", " + fact + ") = " + sum);\n}\n \n }\n}\n``` \n The JNI code has functions for starting and closing Lua, which are nice to have on the Java side when instantiating and garbage-collecting JNI instances.\n``` \n include  \n include  \n include "ComputationJNI.h" \n include  \n include  \n include  \n include  \n include  \n include  \n include  \n include  \n lua_State *L; \n JNIEXPORT void JNICALL Java_ComputationJNI_startLua(JNIEnv *env, jobject thisObj)\n{\n  L = luaL_newstate(); \n if (!L)\n  {\n    fprintf(stderr, "%s\\n", "Cannot create new Lua state!");\n    exit(-1);\n  }\n  luaL_openlibs(L);\n} \n JNIEXPORT void JNICALL Java_ComputationJNI_closeLua(JNIEnv *env, jobject thisObj)\n{\n  lua_close(L);\n} \n JNIEXPORT jstring JNICALL Java_ComputationJNI_factorial(JNIEnv *env, jobject thisObj, jint n)\n{\n  // load factorial function from script onto stack\n  luaL_loadfile(L, "computation.lua");\n  lua_pcall(L, 0, 0, 0);\n  lua_getglobal(L, "factorial"); \n lua_pushnumber(L, (int)n); // push parameter onto stack \n if (lua_pcall(L, 1, 1, 0) != 0)\n  {\n    fprintf(stderr, "%s\\n", lua_tostring(L, -1));\n    luaT_stackdump(L);\n    exit(-1);\n  } \n // pop output off stack and store as a global variable\n  lua_setglobal(L, "N"); \n jstring result = (*env)->NewStringUTF(env, "N"); // pass global var name to Java\n  printf("Lua global var: %s", "N");\n  return result;\n} \n JNIEXPORT jint JNICALL Java_ComputationJNI_sum(JNIEnv  env, jobject thisObj, jint m)\n{\n  // convert name of global var holding function input to C string\n  // const char  var_name = (*env)->GetStringUTFChars(env, n, NULL);\n  // fprintf(stderr, "Global name: %s", var_name); \n // load factorial function from script onto stack\n  luaL_loadfile(L, "computation.lua");\n  lua_pcall(L, 0, 0, 0);\n  lua_getglobal(L, "sum");\n  lua_pushnumber(L, (int)m); \n if (lua_pcall(L, 1, 1, 0) != 0)\n  {\n    fprintf(stderr, "%s\\n", lua_tostring(L, -1));\n    luaT_stackdump(L);\n    return 1;\n  } \n // convert result to Java int\n  jint result = (jint)lua_tonumber(L, -1);\n  lua_pop(L, 1); // remove from stack \n // (*env)->ReleaseStringUTFChars(env, n, var_name); // release mem from C string\n  return result; // return int to Java\n}\n``` \n Returning Function References \n Above, we stored a value in a global variable so that we could use it again. That would be find for storing a constant, such as Avogadro\'s number. But what if you want to call a function multiple times? Is a global variable the best way to go? Probably not. Lua lets you create a reference to a function. Lua\'s C API has a function  luaL_ref  that will store the value on top of the stack in the registry and return an integer that refers to the value in the registry. So, you can load the function onto the stack, call  lua_ref  to store it in the registry, and then use  lua_rawgeti  to push the function onto the stack again. See the example in the  function-ref-example  directory. Build with the  make  and run with  java LuaJNI <path to compiled library> .', "Functional Programming in Scala \n Code from Coursera's Functional Programming in Scala class. https://class.coursera.org/progfun-2012-001/class/index", "swift-lm \n A simple language model in Swift \n Am writing a simple spelling corrector modeled after Peter Norvig's from http://norvig.com/spell-correct.html. ", 'exercism-solutions-clojure', 'Longest Common Substring \n The  longest common substring problem  is where you want to find the longest string that is a substring of two other strings. For example, the longest common substring of "encyclopedia" and "encyclical" is "ncycl". \n This repo contains implementations of the longest common substring algorithm in Swift. There\'s an implementation based on  suffix arrays  and a dynamic programming implementation. The suffix array implementation is largely ported from Robert Sedgewick and Kevin Wayne\'s textbook  Algorithms , 4th edition . The suffix array algorithm should run in  O(m + n)  time, while the dynamic programming version should run in  O(mn)  time, where  m  and  n  are the lengths of the input strings.', "aima-swift \n Swift implementation of algorithms from Russell And Norvig's  Artificial Intelligence - A Modern Approach, Third Edition \n Build \n swift build \n Run Tests \n swift test", 'annotations \n Experimenting with annotations on top of text, in Rust.', 'fasttextrs \n A Rust port of FastText', 'graph-rs \n This simple library was written to help me learn Rust. It uses an adjacency list, rather than an adjacency matrix.', 'ray-tracing-weekend \n A Rust implementation of the raytracer from  Ray tracing in one weekend .']
eugenium,['', 'This is an implementation of a structured sparsity regularization method with total variation and l2 constraints. It can be used to obtain sparse and smooth linear models. \n The method is described in https://hal.inria.fr/hal-01167861/file/ConvexECML2015.pdf \n To run the example in python: \n 1) Make sure you have installed scipy,numpy,sklearn, and nilearn python packages\nNote: these can all be easily installed with the python pip utility \n 2) Run the Synthetic_Example.py  \n 3) Run the FMRI_Example.py this might take 5-10 minutes to complete.  \n The matlab example is very basic. \n For any questions email eugene.belilovsky@inria.fr', 'Minimal Mistakes \n Minimal Mistakes  is a two column responsive Jekyll theme perfect for powering your GitHub hosted blog.  \n Minimal Mistakes is all about: \n \n Responsive templates. Looking good on mobile, tablet, and desktop. \n Gracefully degrading in older browsers. Compatible with Internet Explorer 8+ and all modern browsers.  \n Minimal embellishments -- content first. \n Optional large feature images for posts and pages. \n Simple and clear permalink structure. \n Custom 404 page  to get you started. \n Support for Disqus Comments \n \n \n See a  live version of Minimal Mistakes  hosted on GitHub. \n Getting Started \n Minimal Mistakes takes advantage of Sass and data files to make customizing easier. These features require Jekyll 2.x and will not work with older versions of Jekyll. \n To learn how to install and use this theme check out the  Setup Guide  for more information.', 'k-support-solver \n Efficient python solver for the k-support norm regularized risk minimization. Coming soon..', 'Relative MMD \n Code for computing the MMD and Relative MMD test. Please refer to http://arxiv.org/pdf/1511.04581.pdf \n An example is provided under Example_Vae.py which trains two varational auto-encoders and then compares their samples to a holdout set using MMD. \n The Relative MMD computations need to be exact to assure their validity. Thus matrix operations can be costly. Please make sure you have a properly configured numpy installation (linked to optimized blas libraries like openblas). \n For a matlab version of the Relative MMD test please see https://github.com/wbounliphone/relative_similarity_test \n For questions and or bug reports please dont hesitate to contact eugene.belilovsky@inria.fr', 'EdgeDifferenceTest \n Testing for Differences. This has code and routines associated with the paper https://arxiv.org/pdf/1512.08643.pdf \n Dependencies \n This code is written in python with wrappers on cvxopt,mosek, and R packages. \n The following packages and their python bindings are needed: cvxopt and mosek , rpy2\nR should be installed as well as the package genlasso which is used\nThe current project was developed using versions cvxopt 1.1.4 and mosek 7\nRecent version of these packages (can be installed with acanaconda more easily) but have not thoroughly tested yet \n Run \n Difference_Test_Sim.py to get started \n Note:  Current code has not been optimized and requires further packaging and documentation. mosek produces blank output that takes over the command line thus intermediate printing is best done to a file, this may be resolved in the future by switching and optimizing the QP solver  \n email: eugene.belilovsky@inria.fr ', 'LearnGraphDiscovery \n Example code for the paper https://arxiv.org/abs/1605.06359\nPlease cite our ICML paper if you find this code useful \n Basic python dependencies needed:\nsklearn\ntheano\nkeras \n Basic Example \n Run Example.py to execute the 39 node model and comparisons to sklearn glasso. \n Evaluating on Other data \n The example is simply using our own data generator. In order to evaluate on similar data as in the paper (not the training data generator). You will need to install R, the r-package BDGraph, and rpy2 python package. Then change the variable R_Install in Example.py to True. The script should work automatically \n Train and model specification \n To train your own model or see the model specifications run Train_39_Node_Net.py \n For questions or bug reports please contact eugene.belilovsky@umontreal.ca', 'PyScatWave \n CuPy/PyTorch Scattering implementation \n A scattering network is a Convolutional Network with filters predefined to be wavelets that are not learned and it can be used in vision task such as classification of images. The scattering transform can drastically reduce the spatial resolution of the input (e.g. 224x224->14x14) with demonstrably neglible loss in dicriminative power.    \n The software uses PyTorch + NumPy FFT on CPU, and PyTorch + CuPy + CuFFT on GPU. \n Previous (lua-based) versions of the code can be found at  https://github.com/edouardoyallon/scatwave \n If using this code for your research please cite our paper: \n E. Oyallon, E. Belilovsky, S. Zagoruyko  Scaling the Scattering Transform: Deep Hybrid Networks \n You can find experiments from the paper in the following repository:\nhttps://github.com/edouardoyallon/scalingscattering/ \n We used PyTorch for running experiments in  https://arxiv.org/abs/1703.08961 ,\nbut it is possible to use scattering with other frameworks (e.g. Chainer, Theano or Tensorflow) if one copies Scattering outputs to CPU (or run on CPU and convert to  numpy.ndarray  via  .numpy() ). \n Benchmarks \n We do some simple timings and comparisons to the previous (multi-core CPU) implementation of scattering (ScatnetLight). We benchmark the software using a 1080 GPU. Below we show input sizes (WxHx3xBatchSize) and speed: \n 32 × 32 × 3 × 128 (J=2)- 0.03s (speed of 8x vs ScatNetLight) \n 256 × 256 × 3 × 128 (J=2) - 0.71 s (speed up of 225x vs ScatNetLight) \n Installation \n The software was tested on Linux with anaconda Python 2.7 and\nvarious GPUs, including Titan X, 1080s, 980s, K20s, and Titan X Pascal. \n The first step is to install pytorch following instructions from\n http://pytorch.org , then you can run  pip : \n pip install -r requirements.txt\npython setup.py install \n Usage \n Example: \n ```python\nimport torch\nfrom scatwave.scattering import Scattering \n scat = Scattering(M=32, N=32, J=2).cuda()\nx = torch.randn(1, 3, 32, 32).cuda() \n print scat(x).size()\n``` \n Contribution \n All contributions are welcome. \n Authors \n Edouard Oyallon, Eugene Belilovsky, Sergey Zagoruyko', 'Scaling The Scattering Transform : Deep Hybrid Networks \n This repository contains the experiments found in the paper: https://arxiv.org/abs/1703.08961\n \n Requirements \n In order to run our experiments you will need at minimum the following python packages: pytorch,opencv,pyscatwave package.\nThe simplest way to install pytorch and opencv is through anaconda. We recommend python 2.7 + anaconda.\nThe pyscatwave package can be found here https://github.com/edouardoyallon/pyscatwave \n Imagenet \n We provide a pre-trained model similar to the one described in the paper.  \n To run the trained model of scattering+resnet on imagenet ILSVRC validation set: \n 1) Make sure you have downloaded at least the validation set of ILSVRC2012 and have it organized by class categories\n Note : due to problems with pytorch dataset constructors make sure your imagenet directory has no hidden files, or extra directories besides the 1000 ILSVRC categories.. otherwise all the images will be mislabeled\n2) Download the model file from  http://www.di.ens.fr/~oyallon/scatter_resnet_10_model.pt7\n3) Add this to the imagenet/ directory\n4) Run the script main_test.py to evaluate on the ILSVRC validation set specifying --imagenetpath to point to your imagenet directory \n Training scripts for imagenet and SLE feature extractor will be added soon \n STL-10 \n Simply run python main_STL.py script in the STL directory \n CIFAR-10 \n To run the small sample experiments\nExample: \n bash\npython main_small_sample_class_normalized.py --model resnet12_8_scat --save "test"  --seed 1 --sampleSize 500 --mul 20', 'Kymatio: wavelet scattering in PyTorch \n Kymatio is a Python package for wavelet scattering transforms, built on top of PyTorch. \n \n \n \n Use Kymatio if you need a library that:\n* integrates wavelet scattering in a deep learning architecture,\n* supports 1-D, 2-D, and 3-D wavelets, and\n* runs seamlessly on CPU and GPU hardware. \n Website:  https://kymatio.github.io \n Installation \n Dependencies \n Kymatio requires: \n \n Python (>= 3.6) \n PyTorch (>= 0.4) \n SciPy (>= 0.13) \n \n We also strongly recommend running Kymatio in a Conda environment since this\nsimplifies installation of PyTorch. \n Linux \n conda install pytorch torchvision -c pytorch\npip install -i https://test.pypi.org/simple/ kymatio==0.0.1 \n macOS \n conda install pytorch torchvision -c pytorch\npip install -i https://test.pypi.org/simple/ kymatio==0.0.1 \n The software was tested on Linux with Anaconda Python 3 and\nvarious GPUs, including Titan X, 1080s, 980s, K20s, and Titan X Pascal. \n The software uses PyTorch + NumPy FFT on CPU, and PyTorch + CuPy + CuFFT on GPU. \n If you use this code in your work please cite our paper: \n The scattering authors,  Kymatio: Fast Scattering in 1-D,2-D,3-D \n This code unifies multiple previous efforts:\n    - PyScatWave/ScatWave,\n    - ScatNetLight,\n    - ScatNet, and others \n Optimized package \n If you have a CUDA-enabled GPU, you may run \n pip install -r requirements_optional_cuda.txt \n after installation to install the optimized  skcuda  backend. To enable it, set\nthe  KYMATIO_BACKEND  environment variable to  skcuda . For more information,\nsee the documentation. \n Documentation \n To build the documentation, please run \n pip install -r requirements_optional.txt\ncd doc; make clean; make html \n You may then read the documentation in  doc/build/html/index.html .', "Decoupled Greedy Learning \n This contains the source code for key experiments associated with the paper https://arxiv.org/abs/1901.08164. If you find this code helpful please cite our paper \n @article{belilovsky2019decoupled,\n  title={Decoupled Greedy Learning of CNNs},\n  author={Belilovsky, Eugene and Eickenberg, Michael and Oyallon, Edouard},\n  journal={arXiv preprint arXiv:1901.08164},\n  year={2019}\n} \n In all experiments there is a log file which is generated in the local directory containing the per epoch training/val accuracy and other useful data.  \n For questions or comments please contact at: eugene.belilovsky@umontreal.ca \n dni_comparisons \n contains experiments used in Section 5\nThis relies on the package https://github.com/koz4k/dni-pytorch . Which is also copied in the directory for completness \n end to end baseline \n python cifar_cnn_dni.py \n DNI \n python cifar_cnn_dni.py --dni \n DNI with context \n python cifar_cnn_dni.py --dni --context \n DGL (our method) \n python cifar_dgl.py \n imagenet_dgl \n this contains our source code for the ImageNet experiments. Additionally this implementation provides a preliminary interface for specifying and learning greedy models, which will be released along with the paper. \n VGG-13 K=10 layer by layer  \n python imagenet_dgl.py IMAGENET_DIR  --arch vgg13 --block_size 1 --half --dynamic-loss-scale -j THREADS \n VGG-13 K=4  \n python imagenet_dgl.py IMAGENET_DIR  --arch vgg13 --block_size 3 --half --dynamic-loss-scale -j THREADS \n VGG-19 K=4  \n python imagenet_dgl.py IMAGENET_DIR  --arch vgg19 --block_size 4 --half --dynamic-loss-scale -j THREADS \n VGG-19 K=2 \n python imagenet_dgl.py IMAGENET_DIR  --arch vgg19 --block_size 8 --half --dynamic-loss-scale -j THREADS \n ResNet152 K=2 \n python imagenet_dgl.py IMAGENET_DIR  --arch resnet152 --half --dynamic-loss-scale -j THREADS \n run baseline (end-to-end) code \n Resnet152 \n python imagenet.py IMAGENET_DIR  --arch resnet152 --half --dynamic-loss-scale -j THREADS \n vgg13,vgg19. note: we use batchnorm versions of the pytorch baseline model repo for vgg, the models trained in our code above uses the batchnorm versions as well  \n python imagenet.py IMAGENET_DIR  --arch vgg13_bn --half --dynamic-loss-scale -j THREADS \n python imagenet.py IMAGENET_DIR  --arch vgg19_bn --half --dynamic-loss-scale -j THREADS \n ddg_comparisons \n Comparison to DDG, separate README provided in directory \n auxiliary_nets_study \n From Sec 5 Auxiliary nets \n compares auxiliary networks, to evaluate MLP-SR-aux \n python cifar_dgl.py --type_aux mlp-sr \n CNN auxiliary is given by 'cnn' and MLP auxiliary by 'mlp' \n Learning curves \n From Sec 5 Sequential vs Parallel. \nContains notebook comparing the learnign curves for sequential greedy and dgl. To generate the logs run cifar_sequential_greedy.py for sequential and cifar_dgl.py for dgl with default arguments.  \n buffer_experiments \n This repostory contains experiments associated with Section 5.3 that evaluate the asynchronous versions of DGL proposed in our work \n An example can be run as follows with M=30, slowing down layer 3 (of 6 layers) by a factor 1.11\nnote: the slowdown factor reported in the paper is 1/(1-noise) \n python cifar_buffer.py --buffer 30 --noise 0.1 --layer_noise 3 --seed 0", 'Layerwise Learned CNN \n This is code associated with the paper https://arxiv.org/abs/1812.11446\nThis is a peliminary research code and some more refinements are needed. \n Imagenet \n Imagenet experiments for 1-hidden layer use the standalone imagenet_single_layer.py \n Imagenet experiments for k=2+ can be run with imagenet.py \n Note k in the paper corresponds to nlin in the code \n To obtain the results for Imagenet \n k=3 \n```\npython imagenet.py IMAGENER_DIR -j THREADS  --ncnn 8 --nlin 2  \n ``` \n k=2  \n ```\npython imagenet.py IMAGENER_DIR -j THREADS --ncnn 8 --nlin 1  \n ``` \n k=1 model\n```\npython imagenet_single_layer.py IMAGENER_DIR -j THREADS  --ncnn 8 \n ``` \n VGG-11 \n The VGG-11 model was trained with a new refactored and more modular codebase different from the codebase used for the above models and is thus run from the standalone directory \nrefactored_imagenet/ \n To train the VGG-11 with k=3 \n ```\npython imagenet_greedy.py IMAGENER_DIR -j THREADS --arch vgg11_bn --half --dynamic-loss-scale \n ```\nto train the baseline: \n ```\npython imagenet.py IMAGENER_DIR -j THREADS --arch vgg11_bn --half --dynamic-loss-scale \n ``` \n CIFAR experiments \n CIFAR experiments can be reproduced using cifar.py \n The CIFAR-10 models can be trained: \n k=3 (~91.7) \n```\npython cifar.py --ncnn 4 --nlin 2 --feature_size 128 --down [1] --bn 1 \n ``` \n k=2 (~90.4) \n ```\npython cifar.py --ncnn 4 --nlin 1 --feature_size 128 --down [1] --bn 1 \n ``` \n k=1 (~88.3) \n```\npython cifar.py --ncnn 5 --nlin 0 --feature_size 256  \n ``` \n Refactored (similar to imagenet_refactored) to train CIFAR-10 coming soon with some improvements in accuracy.  \n Contact: please send questions/comments/issues to eugene.belilovsky@umontreal.ca', "Continual Learning Reading Group \n Schedule \n Monday 1:30PM \n Categories \n \n Classics (papers before Deep Learning area. Motivates and sets the stage for Continual Learning) \n Influencials (well cited papers. More general contributions like proposing new desideratas, frameworks, evaluations, etc) \n Rookies (new papers. e.g. NeurIPS 2019, ICLR 2020 submissions) \n Prior-Focused Methods \n Dynamic Architectures \n Rehearsal Methods \n Meta-Continual Learning \n Lifelong Reinforcement Learning \n Relevants (non-continual learning papers that can help us. Generalization, Biology, Psychology, etc) \n \n Abreviations \n Continual Learning (CL)\nCatastrophic Forgetting (CF) \n Papers \n Classics \n Title | Year | tl;dr\n--- | --- | --- \n Influencials \n Title | Year | tl;dr\n--- | --- | ---\n An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks  | 2013 | Investigates CF in neural networks\n Elastic Weight Consolidation (EWC)  | 2017 | Introduces prior-focused methods\n Gradient Episodic Memory (GEM)  | 2017 | a model that alliviates CF via constrained optimization (doesn't increase loss on previous stored data)\n Efficient Lifelong Learning with A-GEM  | 2018 | More efficient GEM; Introduces online continual learning \n Rookies \n Title | Year | tl;dr\n--- | --- | --- \n Prior-focused Methods \n Title | Year | tl;dr\n--- | --- | ---\n Elastic Weight Consolidation (EWC)  | 2017 | Introduces prior-focused methods \n Dynamic Architectures \n Title | Year | tl;dr\n--- | --- | --- \n Rehearsal Methdods \n Title | Year | tl;dr\n--- | --- | ---\n Gradient Episodic Memory (GEM)  | 2017 | a model that alliviates CF via constrained optimization (doesn't increase loss on previous stored data)\n Efficient Lifelong Learning with A-GEM  | 2018 | More efficient GEM; Introduces online continual learning \n Meta Continual Learning \n Title | Year | tl;dr\n--- | --- | --- \n Lifelong Reinforcement Learning \n Title | Year | tl;dr\n--- | --- | --- \n Relevants \n Title | Year | tl;dr\n--- | --- | --- \n Paper Classification \n Title | Family | Multi-head | Single-Head | Online | Supervised | Generative | RL\n--- | --- | --- | --- | --- | --- | --- | ---"]
lberrada,['AIMS : Autonomous Intelligent Machines & Systems \n Repository for the first year courses of the AIMS program \n Modules in src: \n ML Algorithms \n Belief Propagation Algorithm \nExpectation-Maximization Algorithm for Bayesian Linear Regression and for Gaussian Mixture models   \n Regression \n Gaussian Process Regression \nAuto-Regressive Model \nAuto-Correlated Model \nKalman Filter (on going)   \n NB: Some of the code is simply converted from Matlab (original: http://www.robots.ox.ac.uk/~fwood/teaching/AIMS_CDT_ML_2015/homework/index.html)', "The Translation Game \n What is this? \n The translation game is an idea by @lberrada who wished to train his spanish vocabulary. A dataset was built thanks to wiktionary's public dumps and a simple website allows the user to try to translate a french word in spanish and then see the correct results.  \n How does it work? \n The website can be spun up with docker-compose, a working Traefik setup is assumed in this project. An nginx container serves the static files and a python server will provide an API for word fetching and results aggregation. Later, results should be kept in a database and used to provide the user with words targeted at his weaknesses to help him improve on the words he has had trouble with in the past.  \n License \n The translation dataset was retrieved from Wiktionary, they are available at : https://dumps.wikimedia.org/backup-index.html. Subsequently, the file dict/parsed_wiki_sp2fr.json is made available under the Creatice-Commons CC BY-SA 3.0 License.\nThe rest of the project is under MIT license.", 'jax_verify: Neural Network Verification in JAX \n \n \n Jax_verify is a library containing JAX implementations of many widely-used neural network verification techniques. \n Overview \n If you just want to get started with using jax_verify to verify your neural\nnetworks, the main thing to know is we provide a simple, consistent interface\nfor a variety of verification algorithms: \n python\noutput_bounds = jax_verify.verification_technique(network_fn, input_bounds) \n Here,  network_fn  is any JAX function,  input_bounds  define bounds over\npossible inputs to  network_fn , and  output_bounds  will be the computed bounds\nover possible outputs of  network_fn .  verification_technique  can be one of\nmany algorithms implemented in  jax_verify , such as  interval_bound_propagation \nor  crown_bound_propagation . \n The overall approach is to use JAX’s powerful  program transformation system , \nwhich allows us to analyze general network structures defined by  network_fn \nand then to define corresponding functions for calculating\nverified bounds for these networks. \n Verification Techniques \n The methods currently provided by  jax_verify  include: \n \n Functional Lagrangian (coming)  Berrada et al 2021 \n SDP-FO (first-order SDP verification,  Dathathri et al 2020 ) \n Non-convex ( Bunel et al 2020 ) \n Interval Bound Propagation ( Gowal et al 2018 ,  Mirman et al 2018 ) \n Backward Lirpa bounds such as CAP ( Wong and Kolter 2017 ), FastLin( Weng et al 2018 ) or CROWN ( Zhang et al 2018 ) \n Forward Lirpa bounds ( Xu et al 2020 ) \n CROWN-IBP ( Zhang et al 2019 ) \n Planet (also known as the "LP" or "triangle" relaxation,  Ehlers 2017 ), currently using  CVXPY  as the LP solver \n MIP encoding ( Cheng et al 2017 ,  Tjeng et al 2019 ) \n \n Installation \n Stable : Just run  pip install jax_verify  and you can  import jax_verify  from any of your Python code. \n Latest : Clone this directory and run  pip install .  from the directory root. \n Getting Started \n We suggest starting by looking at the minimal examples in the  examples/  directory.\nFor example, all the bound propagation techniques can be run with the  run_boundprop.py  script: \n bash\ncd examples/\npython3 run_boundprop.py --boundprop_method=interval_bound_propagation \n For documentation, please refer to the  API reference page . \n Notes \n Contributions of additional verification techniques are very welcome. Please open\nan issue first to let us know. \n This is not an official Google product.']
WladimirSidorenko,['Dissertation \n Thesis of Wladimir Sidorenko (Uladzimir Sidarenka).', 'Presentations \n Project Presentations', 'CRFSuite (0.13) \n \n Table of Contents \n \n CRFSuite (0.13) \n Introduction \n Installation \n Version 0.13 \n Format \n Warnings \n Copyright and Licensing \n Acknowledgment \n \n \n \n Introduction \n CRFSuite 0.13 is a fork of  Naoaki Okazaki\'s \nimplementation of conditional random fields (CRFs).  Please refer to\nthe  web site  for more\ninformation about the original software. \n Installation \n In order to install the current version of the program, you need to\nexecute the following commands in the root directory of the\ndownloaded project: \n ```shell\nautoreconf -f -i \n ./configure \n make all check \n sudo make install\n``` \n Version 0.13 \n This version of  CRFSuite  has been extended with the following\nvariants of CRFs: \n \n tree-structured CRFs; \n semi-Markov CRFs of arbitrary orders; \n linear-chain CRFs  of arbitrary orders. \n \n To invoke tree-structured CRFs, you should provide the option\n --type=tree  when running  crfsuite learn  and also specify this\noption when you later envoke  crfsuite tag  with the trained model. \n To use higher-order linear-chain and semi-markov CRFs, you should specify the\noption  --type=semim  both during the training and during the tagging, e.g.: \n crfsuite learn --type=semim -p feature.max_seg_len=-1 -m semim.model tests/test_sm_1.input \n crfsuite tag --type=semim -m semim.model tests/test_sm_1.input \n Setting the option  -p feature.max_seg_len=  to a negative value will envoke the\nsemi-Markov variant of CRF, providing a non-negative integer will activate the\nlinear-chain model.  To specify the maximum order of transition features, use\nthe option  -p feature.max_order= , e.g.: \n crfsuite learn --type=semim -p feature.max_seg_len=1 -p feature.max_order=4\n-m semim.model tests/test_sm_1.input \n will train a 4-th order linear-chain model. \n Format \n The format for the first and higher order linear-chain and semi-Markov\nlooks as follows: \n label1 \\t feat_name1:value1 \\t feat_name2:value2 \\t feat_name3:value3 \n label2 \\t feat_name4:value4 \\t feat_name5:value5 \\t feat_name6:value6 \n label3 \\t feat_name7:value7 \\t feat_name8:value8 \n label4 \\t feat_name9:value9 \\t feat_name10:value10 \\t feat_name11:value11 \n \n For testing, you can either specify a valid label as the first field or\nput any value (e.g  _  underscore ) which does not coincide with any known\ntagset label.  This first field is skipped during the testing.  Empty lines\ndelimit the sequences. \n For the tree-structured CRFs, you should specify the id of the node as the\nsecond field and the id of the parent node as the third field, e.g: \n label1 \\t node_id1 \\t node_id2 \\t feat_name1:value1 \n label2 \\t node_id2 \\t _ \\t feat_name2:value2 \n label3 \\t node_id3 \\t node_id4 \n label4 \\t node_id4 \\t node_id2 \\t feat_name3:value3 \\t feat_name4:value4 \n \n The parent of the root node should be specified as  _  (underscore) (see file\n tests/test_tree_2.input  for an example). \n Warnings \n \n Only  l-BFGS  is supported so far for the higher-order and\n   semi-markov models; \n Semi-Markov and higher-order linear-chain models do not support the options\n -i  and  -p  for tagging yet; \n No speed optimization was done for the higher-order semi-Markov and linear-\nchain models; \n C++ interface has not been updated to support the new types. \n \n Copyright and Licensing \n This program is distributed under the modified BSD license. Refer to\nCOPYING file for the precise description of the license. \n Portions of this software are based on libLBFGS. \n The MIT License \n Copyright (c) 1990 Jorge Nocedal\nCopyright (c) 2007 Naoaki Okazaki \n Permission is hereby granted, free of charge, to any person obtaining a\ncopy of this software and associated documentation files (the "Software"),\nto deal in the Software without restriction, including without limitation\nthe rights to use, copy, modify, merge, publish, distribute, sublicense,\nand/or sell copies of the Software, and to permit persons to whom the\nSoftware is furnished to do so, subject to the following conditions: \n The above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software. \n THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE. \n Portions of this software are based on Constant Quark Database (CQDB). \n The BSD license. \n Copyright (c) 2007, Naoaki Okazaki\nAll rights reserved. \n Redistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n    * Redistributions of source code must retain the above copyright\n      notice, this list of conditions and the following disclaimer.\n    * Redistributions in binary form must reproduce the above copyright\n      notice, this list of conditions and the following disclaimer in the\n      documentation and/or other materials provided with the distribution.\n    * Neither the name of the Northwestern University, University of Tokyo,\n      nor the names of its contributors may be used to endorse or promote\n      products derived from this software without specific prior written\n      permission. \n THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER\nOR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\nEXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\nPROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\nPROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\nLIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\nNEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n Portions of this software are based on RumAVL. \n MIT/X Consortium License. \n Copyright (c) 2005-2007 Jesse Long  jpl@unknown.za.net \nAll rights reserved. \n Permission is hereby granted, free of charge, to any person obtaining a\ncopy of this software and associated documentation files (the "Software"),\nto deal in the Software without restriction, including without limitation\nthe rights to use, copy, modify, merge, publish, distribute, sublicense,\nand/or sell copies of the Software, and to permit persons to whom the\nSoftware is furnished to do so, subject to the following conditions: \n \n The above copyright notice and this permission notice shall be\n      included in all copies or substantial portions of the Software. \n The origin of the Software must not be misrepresented; you must not\n      claim that you wrote the original Software. \n Altered source versions of the Software must be plainly marked as\n      such, and must not be misrepresented as being the original Software. \n \n THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\nFROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\nDEALINGS IN THE SOFTWARE. \n Portions of this software are based on a portable stdint.h (for MSVC). \n Copyright (c) 2005-2007 Paul Hsieh \n Redistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions\nare met: \n Redistributions of source code must retain the above copyright\nnotice, this list of conditions and the following disclaimer.\n\nRedistributions in binary form must not misrepresent the orignal\nsource in the documentation and/or other materials provided\nwith the distribution.\n\nThe names of the authors nor its contributors may be used to\nendorse or promote products derived from this software without\nspecific prior written permission.\n \n THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS\nFOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE\nCOPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,\nINCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\nHOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,\nSTRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\nARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED\nOF THE POSSIBILITY OF SUCH DAMAGE. \n Portions of this software are based on Mersenne Twister. \n Copyright (C) 1997 - 2002, Makoto Matsumoto and Takuji Nishimura,\nAll rights reserved. \n Redistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet: \n \n \n Redistributions of source code must retain the above copyright\n     notice, this list of conditions and the following disclaimer. \n \n \n Redistributions in binary form must reproduce the above copyright\n     notice, this list of conditions and the following disclaimer in the\n     documentation and/or other materials provided with the distribution. \n \n \n The names of its contributors may not be used to endorse or\n     promote products derived from this software without specific\n     prior written permission. \n \n \n THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\nCONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\nEXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\nPROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\nPROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\nLIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\nNEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n Acknowledgment \n Special thanks goes to: \n \n Olivier Grisel \n Andreas Holzbach \n Baoli Li \n Yoshimasa Tsuruoka \n Hiroshi Manabe \n Riza Theresa B. Batista-Navarro \n', "RSTTool for Annotation of Discussions \n This program is a forked version of  Daniel Marcu's\nmodification  of\nthe original  RSTTool from Michael\nO'Donell .  This version provides a\nmodified appearance for annotation of multilogues (i.e. dialogues with\nmultiple participants).  The user can view multilogues in a successive\nfashion (i.e. reply by reply) and can annotate RST-relations both\nwithin and among the multilogue messages. \n \n Description \n After starting  ./RSTTool &  from your shell, you should see a\ngraphical window consisting of three horizontal sub-frames.  The upper\nsub-frame (called  RST Editor ) displays a graphical\nrepresentation of the (partially constructed) RST tree.  The middle\nsub-frame ( Refererence Text Viewer ) shows the message which\nis being answered in the discussion.  The lowest part of the program\nwindows ( Answer Text Editor ) displays the answer to the\nreference message and also allows you to annotate boundaries of\nelementary discourse units in that answer. \n After loading an input file with discussions (cf. Section  Input\nFormat ), you should see the first message of the first\ndiscussion in  Answer Text Editor .  There you can annotate the\nboundaries of its discourse segments.  Since this message is typically\nnot an answer to any other message in your file,  Refererence Text\nViewer  will remain empty by that time. \n Once you have annotated the segments of the message in  Answer Text\nEditor  (cf. Section  Answer Text Editor  for\ndescription), you can construct an RST tree of these segments using\n RST Editor .  In order to link two nodes, you simply need to\nclick one of them, and then make a second click on the node which you\nare going to connect to.  After that, a context menu will be offered\nasking you whether the first node should be linked as a nucleus or a\nsatellite, or, probably, as a part of a multinucleus relation.  Once\nyou have chosen the linkage type, you can choose a particular relation\nfor that link in another menu. \n To proceed to the next message, you simply need to click the button\n Next Message  in the middle part of the window.  If your\ncurrently annotated message has any answers, then its text will move\nto the  Refererence Text Viewer  part and the answer will be\ndisplayed in  Answer Text Editor  instead. \n After you are finished with your annotation, you can save your data\neither by using the menu  File/Save  or by using the hot key\n Ctrl-S  ( Cmd-S  on Mac). \n \n Input Format \n This tool accepts discussion files in tab-separated format where each\nmessage is represented as a sequence of fields delimited by tabulation\ncharacters.  In each message, there shoul be at least one and at most\ntwo non-empty fields.  The first of these fields represents the id of\nthat message (the id should be unique across the file).  The second\nfield contains the text of the message (no tabs are allowed within the\ntext).  The number of tabulation characters at the beginning of the\nstring represents the ``nestedness'' level of the message in a given\ndiscussion.  It means, that message which starts a discussion there\nshould have zero tabs at the beginning whereas an immediate answer to\nit should have one tab and should immediately follow this first\nmessage.  An answer to the answer should begin with two tabs and so\non.  You can see an example of the input file\n here . \n \n Output Format \n Currently, this tool saves all annotations in a tab-separated format\ntoo.  The output file contains all information (including meta-data)\nabout each created EDU segment and the RST relations established\nbetween the segments.  Lines containing description of the segments\nbegin with the word  nid  followed by the id of the EDU node, and\nthe id of its message.  Further fields have the format\n attribute_name\\034attribute_value , where  \\034  is a special\ncharacter (with octal code 34) that rarely occurs in normal text and\nserves here as a delimiter.  The attribute value pairs are delimited\nfrom each other by tab characters.  The particular set of attributes\nand their values mostly depends on the type of the node.  Further\nworks are planned to provide an appropriate exporter for this format\nto other formats like (RST Treebank Lisp Format or RS3). \n \n Bindings \n Particular bindings apply for particular parts of the program window.\nBelow you can see a summary of binding for each of the sub-frames. \n All windows \n Ctrl-O  or  Cmd-O  on Mac -- open new file \n Ctrl-S  or  Cmd-S  on Mac -- save file \n Ctrl-C  or  Cmd-C  on Mac -- copy selection \n Answer Text Editor \n LeftMouseClick  on unannotated text area - set new segment boundary \n Ctrl-RightMouseClick  on segment boundary - move segment boundary \n Ctrl-Alt-RightMouseClick  on segment boundary - delete segment boundary \n \n License \n This program is based on the RSTTool version developed by Daniel Marcu\nand is subject to the same  license agreement\nterms  as the\noriginal program. \n \n Contact \n If you find any issues or inconsistencies in this version of RSTTool\nor in its description, please feel free to  submit a\nbug .", 'German Discourse Parser \n Description \n Installation \n Copyright \n Contact', 'Sentiment Lexicon Generation Suite \n \n This project provides executable files and scripts for generating\nsentiment lexicons from  GermaNet  (a German equivalent of the English\n WordNet ), raw text corpora, and neural word embeddings. \n Building \n For generating a sentiment lexcion from pre-trained word embeddings,\nyou first need to compile the C++ code by running the following\ncommands (please note that the build requires the  Armadillo\nlibrary  to be installed): \n shell\ncd build/\ncmake ../\nmake \n Afterwards, an executable called  vec2dic  wil apper in\nthe subdirectory  bin .  You can exectute this file by envoking: \n shell\n./bin/vec2dic [OPTIONS] --type=TYPE VECTOR_FILE SEED_FILE \n where the  TYPE  argument (an integer from zero to three) will\ndetermine the algorithm to use for inducing a sentiment lexicon,\n VECTORE_FILE  denotes a path to a text file with pre-trained\n word2vec  embeddings (note that the file should be in the raw text\nformat with space separated values), and  SEED_FILE .  We currently\nsupport the following types of algorithms:\n- 0 -- nearest centroids (default);\n- 1 -- KNN;\n- 2 -- PCA. \n Examples \n In addition to the C++ executables, we also provide several\nreimplementations of popular alternative approaches which generate\nsentiment lexcions from lexical taxonomies (e.g.,  GermaNet ) or raw\nunlabeled text corpora.  Please note that in order to use\ndictionary-based methods, you need to download\n GermaNet , which is not\nincluded here by default due to license restrictions, and place its\nfiles in the directory  data/GermaNet_v9.0/ .  For corpus-based\nalgorithms, you need to provide a pre-lemmatized corpus in the format\nsimilar to the one used in  data/snapshot_corpus_data/example.txt .\nAlternatively, for the method of Takamura et al. (2005), you need to\nprovide a list of coordinately conjoined pairs similar to the one\nprovided in  data/corpus/cc_light.txt . \n Below, you can find a short summary and command examples of the\nprovided systems. \n Hu and Liu (2004) \n For generating a sentiment lexicon with the method of  Hu and Liu\n(2004) ,\nyou should envoke the following command: \n ```shell \n ./scripts/generate_lexicon.py hu-liu \\\n --ext-syn-rels --seed-pos=adj \\\n--form2lemma=data/GermaNet_v9.0/gn_form2lemma.txt \\\ndata/seeds/hu_liu_seedset.txt data/GermaNet_v9.0 \n ``` \n Blair-Goldensohn (2008) \n If you want to generate a sentiment lexicon using the method of\n Blair-Goldensohn et\nal. (2008) ,\nyou should envoke the following command: \n ```shell \n ./scripts/generate_lexicon.py blair-goldensohn \\\n --ext-syn-rels --seed-pos=adj \\\n --form2lemma=data/GermaNet_v9.0/gn_form2lemma.txt \\\n data/seeds/hu_liu_seedset.txt data/GermaNet_v9.0/ \n ``` \n Kim-Hovy (2004) \n For generating a sentiment lexicon with the method of  Kim and Hovy,\n(2004) ,\nuse the following command: \n ```shell \n ./scripts/generate_lexicon.py kim-hovy \\\n --ext-syn-rels --seed-pos=adj \\\n --form2lemma=data/GermaNet_v9.0/gn_form2lemma.txt \\\n data/seeds/hu_liu_seedset.txt data/GermaNet_v9.0/ \n ``` \n Takamura et al. (2005) \n To generate a sentiment lexicon with the method of\n Takamura et al. (2005) ,\nuse the following command instead (note that the file\n data/corpus/cc.txt  is not included in this repository due to its big\nsize): \n ```shell \n ./scripts/generate_lexicon.py takamura \\\n    --form2lemma=data/GermaNet_v9.0/gn_form2lemma.txt \\\n    data/seeds/turney_littman_2003.txt data/GermaNet_v9.0/ data/corpus/cc.txt -1 \n ``` \n Esuli and Sebastiani (2006) \n For generating a sentiment lexicon using the  SentiWordNet  method of\n Esuli and Sebastiani (2006) ,\nyou should use the following command: \n ```shell \n ./scripts/generate_lexicon.py esuli --ext-syn-rels \\\n--seed-pos=adj --form2lemma=data/GermaNet_v9.0/gn_form2lemma.txt \\\ndata/seeds/hu_liu_seedset.txt data/GermaNet_v9.0 \n ``` \n Rao and Ravichandran (2009) \n In order to generate a sentiment lexicon with the min-cut approach of\n Rao and Ravichandran (2009) ,\nuse the below command: \n ```shell \n ./scripts/generate_lexicon.py rao-min-cut --ext-syn-rels \\\n--seed-pos=adj --form2lemma=data/GermaNet_v9.0/gn_form2lemma.txt \\\ndata/seeds/hu_liu_seedset.txt data/GermaNet_v9.0 \n ``` \n If you want to test the label propagation algorithm described by these\nauthors, you should specify the following arguments: \n ```shell \n ./scripts/generate_lexicon.py rao-lbl-prop --ext-syn-rels \\\n--seed-pos=adj --form2lemma=data/GermaNet_v9.0/gn_form2lemma.txt \\\ndata/seeds/hu_liu_seedset.txt data/GermaNet_v9.0 \n ``` \n Awdallah and Radev (2010) \n To generate a sentiment lexicon using the method of\n Awdallah and Radev (2010) ,\nyou should use the following command: \n ```shell \n ./scripts/generate_lexicon.py awdallah --ext-syn-rels \\\n--seed-pos=adj --form2lemma=data/GermaNet_v9.0/gn_form2lemma.txt \\\ndata/seeds/hu_liu_seedset.txt data/GermaNet_v9.0/ \n ``` \n Velikovich et al. (2010) \n For generating a sentiment lexicon using the algorithm of\n Velikovich et al. (2010) ,\nyou can use the following command: \n ```shell \n ./scripts/generate_lexicon.py velikovich \\\ndata/seeds/hu_liu_seedset.txt -1 data/snapshot_corpus_data/example.txt \n ``` \n Kiritchenko et al. (2014) \n In order to generate a sentiment lexicon using the system of\n Kiritchenko et al. (2014) ,\nyou should use the following command: \n ```shell \n ./scripts/generate_lexicon.py kiritchenko \\\ndata/seeds/hu_liu_seedset.txt -1 data/snapshot_corpus_data/example.txt \n ``` \n Severyn and Moschitti (2014) \n For generating a sentiment lexicon using the approach of\n Severyn and Moschitti (2014) ,\nyou should use the following command: \n ```shell \n ./scripts/generate_lexicon.py severyn \\\ndata/seeds/hu_liu_seedset.txt -1 data/snapshot_corpus_data/example.txt \n ``` \n Evaluation \n You can evaluate the resulting sentiment lexicon on the\n PotTS  dataset by using\nthe following command and providing a valid path to the downloaded\ncorpus data: \n ```shell \n ./scripts/evaluate.py -l data/form2lemma.txt \\\n    data/results/esuli-sebastiani/esuli-sebastiani.ext-syn-rels.turney-littman-seedset.txt \\\n    ${PATH_TO_PotTS}/corpus/basedata/ ${PATH_TO_PotTS}/corpus/annotator-2/markables/ \n ```', 'Description \n \n This directory contains the data of the Potsdam Twitter Sentiment\nCorpus (ISLRN 714-621-985-491-3).  To open the files of this\ncorpus, you need to download and launch\n MMAX2 —a freely distributed\nannotation tool—and then select one of the *.mmax projects from the\ndirectories  corpus/annotator-1/  or  corpus/annotator-2/ . \n Folder Structure \n The folders of this project are structured as follows: \n \n corpus/  – directory containing corpus files; \n annotator1/  – directory containing MMAX projects for the first\n    annotator; \n markables/  – directory containing annotation files for the\n   first annotator; \n \n \n annotator2/  – directory containing MMAX projects for the second\n    annotator; \n markables/  – directory containing annotation files for the\n   second annotator; \n \n \n basedata/  and  source/  – original corpus tokenization; \n \n custom/ ,  scheme/ , and  style/  – auxiliary MMAX2 data; \n \n \n docs/  – directory containing annotation guidelines and other\n  accompanying documents; \n \n \n scripts/  – directory containing scripts that were used to process\n  corpus data; \n \n examples/  – directory containing examples of input files for\n    the scripts; \n align.py  – auxiliary module used for annotation alignment; \n alt_fio.py  – auxiliary module for AWK-like input/output operations; \n conll.py  – auxiliary module for handling CONLL sentences; \n measure_corpus_agreement.py  – script for measuring corpus\n    agreement; \n merge_conll_mmax.py  – script for aligning annotation from the\n    corpus with the automatically processed CONLL data; \n \n You can see the examples of invocations in the script files or by just\ntyping  --help  to see their usage. \n Note \n I strongly recommend using the  annotation of annotator-2  on the branch  eexpression-revision  (run  git checkout eexpression-revision  after cloning this project).', 'scala-playground \n Personal Scala Exercises']
bhdn,["dockerize-it \n Create a set of ready to use Dockerfiles based on OpenSUSE, and find a nice home for them to live in. Useful containers or just for fun, let's dockerize-it all.", "supybot-telegram-bridge \n A Supybot plugin that implements an IRC-Telegram gateway.  \n It relies on the  Telegram Bot API . \n Setting it up \n \n Create your Telegram bot using  @BotFather \n Set up your supybot bot using  supybot-wizard . \n Clone this repository into the  plugins/  directory and rename the\n   checkout to  TelegramBridge \n Invite the bot to the target Telegram Chat \n Start supybot \n Send a message to the Telegram Chat, and watch the supybot logs for the message\n    Got message from unknown Telegram group: ; the Telegram chat ID will follow. \n Either update the supybot's bot configuration manually or say  config channel\n   <channel_name> plugins.TelegramBridge.tgChatId <chat_id>  to your bot to set\n   the Telegram chat ID. \n Enjoy \n \n Configuration options \n \n supybot.plugins.TelegramBridge.tgToken : The bot's token as provided by\n@BotFather \n supybot.plugins.TelegramBridge.tgChatId : The chatroom ID\n   NB:  this is a per-channel configuration option \n"]
spillai,['fs-python-examples \n Miscellaneous Python Examples', 'fs-scripts \n Automation and general purpose productivity scripts', 'NUMPY <=> OpenCV\'s cv::Mat converter \n Sudeep Pillai  (spillai@csail.mit.edu) Sep 14, 2014 \n A convenience library that registers a boost::python converter to implicitly\nconvert between OpenCV\'s popular cv::Mat datatype and NumPy\'s popular\nnp.array() datatype. This allows a developer to go back and forth between their\nOpenCV C++ API and Python API written using NumPy with relative ease, avoiding the need to\nwrite additional wrappers that handle PyObjects being passed around or returned.  \n This work was mostly inspired by\nhttps://github.com/yati-sagade/opencv-ndarray-conversion.  \n BSD License \n A simple example \n Imagine writing a C++ API using OpenCV as so: \n c++\ncv::Mat process_mat(const cv::Mat& in) { \n   // process matrix, or just plain-simple cloning!\n   cv::Mat out = in.clone();\n   return out;\n} \n Wrap it using Boost::Python in say, cv_module.cpp: \n python\nboost::python::def("process_mat", &process_mat);   \n Call it from Python: \n python\nimport numpy as np\nfrom cv_module import process_mat\nA = np.random.random(shape=(4,3))\nB = process_mat(A)   \n As simple as that! Hope you find it useful! \n Build and Install \n See dependencies below before building. \n git clone git@github.com:spillai/numpy-opencv-converter\nmkdir build\ncd build && cmake ..\nmake \n Usage \n Make sure the built np_opencv_converter.so and np_opencv_module.so\n(currently in numpy-opencv-converter/build/) are in the $LD_LIBRARY_PATH\nenvironment variable, before running the following in python/ipython.  \n python\nIn [1]: import np_opencv_module as npcv\nPYTHON TYPE CONVERTERS exported \nOn succesful import, the cv::Mat <=> np.ndarray converters are exported.  \n We then test  a simple function that takes in a cv::Mat (appropriately converted\nfrom np.ndarray A) as an argument and returns\nanother cv::Mat (again appropriately converted back to np.ndarray B).   \n ```python\nIn [2]: import numpy as np\nIn [3]: A = np.random.random((4,3))\nIn [4]: B = npcv.test_np_mat(A)\nin: [0.2793205402416998, 0.466896711918419, 0.3834843006535923;\n  0.5374426625812107, 0.3737008026047054, 0.3685794034255524;\n  0.993469313797578, 0.2619403678989528, 0.5700175530375297;\n  0.5711496315041438, 0.3286727439294438, 0.1250325059375548]\nsz: [3 x 4] \n In [5]: print A.dtype, B.dtype\nfloat64 float64\n``` \n With default args\n```python\nIn [6]: npcv.test_with_args(A, var1=0, var2=20.0, name=\'test_name2\')\nin: [0.27932054, 0.46689671, 0.3834843;\n  0.53744268, 0.3737008, 0.36857942;\n    0.9934693, 0.26194036, 0.57001758;\n      0.57114965, 0.32867274, 0.1250325]\nsz: [3 x 4]\nReturning transpose \n Out[6]: \narray([[ 0.27932054,  0.53744268,  0.9934693 ,  0.57114965],\n      [ 0.46689671,  0.3737008 ,  0.26194036,  0.32867274],\n      [ 0.3834843 ,  0.36857942,  0.57001758,  0.1250325 ]],\n      dtype=float32)\n``` \n Finally, testing a wrapper class\n```python\nIn [7]: gw = npcv.GenericWrapper(var_int=1, var_float=2.0, var_double=3.0, var_string=\'string\')\nIn [8]: gw.process(A) \n in: [0.2793205402416998, 0.466896711918419, 0.3834843006535923;\n  0.5374426625812107, 0.3737008026047054, 0.3685794034255524;\n    0.993469313797578, 0.2619403678989528, 0.5700175530375297;\n      0.5711496315041438, 0.3286727439294438, 0.1250325059375548]\nsz: [3 x 4]\nReturning transpose \n Out[8]: \narray([[ 0.27932054,  0.53744266,  0.99346931,  0.57114963],\n      [ 0.46689671,  0.3737008 ,  0.26194037,  0.32867274],\n      [ 0.3834843 ,  0.3685794 ,  0.57001755,  0.12503251]])\n``` \n Dependencies \n Currently only Linux is supported, although the code is pretty barebones that\nshould allow you to port it to Windows, or Mac OSX with relative ease. \nI personally wouldn\'t recommend installing opencv as indicated below, but I\'ll\nassume you know how to deal with pkg-config and opencv to make any\nmodifications to the CMakeLists.txt file.  \n $ sudo apt-get install libboost-python-dev libopencv-dev', "Build \n Check  software/tobuild.txt  and  software/externals/tobuild.txt \n Build system \n shell\ncd ~/articulation_learning/software\nmake -j 8 \n Note: \n- Look at  externals/pcl-pod/pcl_common_include.patch  - you might have to copy over some files from\n   software/externals/pcl-pod/pcl-1.7.0/common/include/pcl/  to\n   software/build/include/pcl/ (conversions.h, register_point_struct.h, point_types_conversion.h) \n General setup \n Terminal 1 \n shell\nbot-param-server config/articulation_learning.cfg \n Terminal 2 \n shell\ncd ~/articulation_learning/software/python/apps\npython articulation_learning_demo.py \n Directory layout \n \n \n config:  \n config scripts such as articulation_learning.cfg for bot-param-server \n \n \n \n \n \n software:  \n compilation of pods, and modules for articulation learning  \n Within the software folder, code is organized as follows \n Comment/Uncomment pods in the  tobuild.txt  file to add/remove unnecessary\n   dependencies \n apps:  \n mostly test apps that are used to check/run various implementations \n \n \n \n \n \n drivers:  \n kinect driver for reading/publishing kinect data over lcm \n \n \n \n \n \n externals:  \n external dependencies include opencv, pcl, apriltags, eigen, isam, lcm,\n libbot, and a few other visualization tools \n \n \n \n \n \n modules:  \n articulation: main articulation learning module \n birchfield's klt \n dbow2: bag of words implementation \n more modules \n \n \n \n \n \n python:  \n notebooks:  \n ipython notebooks for articulation learning  \n run articulation_learning.ipynb \n \n \n utils:  \n all the different components in the articulation learning implmented in\n  python with certain modules wrapped in C++ (see pywrappers) \n look at  al_lfd.py  and  articulation_learning_demo.py  initially for articulation learning \n \n \n \n \n \n pywrappers:  \n apriltags, articulation, pcl, and fs_utils wrapped utilities for python access \n utils/: provides containers, and registered conversions from/to opencv<=>numpy  \n \n \n \n \n \n utilities:  \n general purpose utilities for opencv, pcl, lcm, and opencv-based rgbd containers. Also a few visualization utilities \n \n \n \n \n \n viewer:  \n al-viewer for visualizing results/data \n \n \n \n BASHRC setup \n shell\n  export ALEARN=$HOME/articulation_learning\n  export PATH=$ALEARN/bin:$PATH\n  export LD_LIBRARY_PATH=$ALEARN/software/build/lib/:$ALEARN/software/build/lib64:$LD_LIBRARY_PATH\n  export PYTHONPATH=$ALEARN/software/python:$ALEARN/software/build/lib:$PYTHONPATH\n  export PYTHONPATH=$ALEARN/software/build/lib/python2.7/dist-packages:$ALEARN/software/build/lib/python2.7/site-packages:$PYTHONPATH\n  export DYLD_LIBRARY_PATH=$ALEARN/software/build/lib:$DYLD_LIBRARY_PATH\n  export PKG_CONFIG_PATH=/usr/lib/pkgconfig:/usr/share/pkgconfig/:/usr/share/cmake/Modules:$PKG_CONFIG_PATH\n  export PKG_CONFIG_PATH=$ALEARN/software/build/lib64/pkgconfig/:$ALEARN/software/build/lib64/pkgconfig:$PKG_CONFIG_PATH \n DATA \n shell\nssh spillai@virgo2", 'recognition-utils \n A collection of files that I regularly use in my research for object recognition/classification \n Author(s):  Sudeep Pillai \n Related Publications: \n Monocular SLAM Supported Object     Recognition \n Sudeep Pillai  and John. J. Leonard.  \n Robotics: Science and Systems 2015.  pdf \n Dependencies: \n OpenCV 2.4.9 \n License: \n MIT', 'halide-experiments \n A few ongoing experiments with  Halide   \n Stereo Block Matching with Halide \nThis work is a direct python-port to the work done in https://github.com/mzhang94/stereo. ', "Build GTSAM and iSAM with conda \n Author:  Sudeep Pillai \nLicense: MIT \n \n Install \n Install miniconda if you don't have it and add to path\n sh\nwget https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh\nsh Miniconda2-latest-Linux-x86_64.sh -b -p $HOME/anaconda\nexport PATH=$HOME/anaconda/bin:$PATH \n Get the newest version of conda, as well as some conda build tools.\n sh\nconda update conda -y\nconda install conda-build anaconda-client -y \n Create a new environment, and build all packages.\nBoth  PYTHONPATH  and  LD_LIBRARY_PATH  should have been set automatically on environment activation. See activate.sh / deactivate.sh.\n sh\nconda create -n slam_env python=2\nsource activate slam_env\nbuild_all.sh", 'conda-recipes-vision \n Building/Packaging Computer Vision Libraries with conda', 'pybot \n Research tools for autonomous systems using Python \nAuthor:  Sudeep Pillai   (spillai@csail.mit.edu) \nLicense: MIT \n \n Modules \n geometry:  General-purpose tools for computing rigid-body\ntransformations. This is a preliminary version that currently deals\nmostly with  SE(3)  or 6-DOF (3-DoF Rotational + 3-DoF translation)\nand some support for  Sim(3)  motions. \n vision:  Computer vision package with several tools including\n  camera, tracking, 2d features, 3d features, optical flow,\n  recognition, object proposals, caffe, classifier training,\n  bag-of-words training, geometry, stereo, drawing etc. \n utils:  Basic tooling that includes attribute dictionaries,\ndatabase-utils including incremental hdf5 tables, dataset readers\n[ImageDatasets, StereoDatasets, VelodyneDatasets etc], dataset helpers\n[ KITTI , itertools\nrecipes, timing/profiling tools, io utils\n[video/image writing, mkdirs, find_files, config parsers, joblib utils, stdout tee-ing, JSON],\nother misc tools including pretty prints, progressbars, colored\nprints, counters, accumulators (indexed deques), accumulators with\nperiodic callbacks etc. \n externals:  ROS/LCM drawing tools, ROS/LCM log readers log reader \n Installation \n See  INSTALL \n Quick install \n Install pybot and its dependencies into a new conda environment.\n sh\nconda config --add channels menpo\nconda create --name pybot-py35 python=3.5\nconda install --name pybot-py35 -c s_pillai pybot \n Contributing \n We appreciate all contributions. If you would like to contribute new\nfeatures or fix existing bugs, please open an issue and discuss them\nwith us first. ', "conda-recipes-robot \n Building/Packaging Robot Libraries with conda \n Author:  Sudeep Pillai \nLicense: MIT \n \n See my  anaconda repo  for more packages.  \n \n Install \n Install miniconda if you don't have it and add to path\n sh\nwget https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh\nsh Miniconda2-latest-Linux-x86_64.sh -b -p $HOME/anaconda\nexport PATH=$HOME/anaconda/bin:$PATH \n Create a new environment.\n sh\nconda create -n robot_env python=2\nsource activate robot_env \n Install the packages\n bash\nconda install -c s_pillai pygtsam=3.2.1 -y\nconda install -c s_pillai lcm=1.3.1 -y\nconda install -c s_pillai libbot=2 -y \n Install  pybot \n bash\nconda install --file https://raw.githubusercontent.com/spillai/pybot/master/conda_requirements.txt\npip install -i https://pypi.anaconda.org/s_pillai/simple pybot \n \n Pre-built packages \n \n lcm \n libbot \n pybot \n pygtsam \n", 'Geodesic Object Proposals \n Dependencies \n Required:\n * cmake\n * c++11 compiler( g++-4.7 or higher, vc++2013 or higher, clang might work too )\n * Eigen 3.2 (optionally you can download eigen and put in in external/eigen such that external/eigen/Eigen is a valid directory)\n * libpng and libjpg (needed by cimg) \n Optional for python3 bindings:\n * python3 (python 2.7 should work too, but I didn\'t test it extensively)\n * numpy\n * boost-python\n * matio (optional to load datasets)\n * matplotlib for some visualizations\n * MATLAB (r2013a on Ubuntu 14.04 tested, others might work too. You might have to specify a new gcc version in mexopts) \n How to compile \n \n create an build directory (eg. build) \n call cmake: cmake -DCMAKE_BUILD_TYPE=Release relative_path_to_source \n to compile the matlab bindings add the flag \'-DUSE_PYTHON=3\' (for python3), \'-DUSE_PYTHON=2\' (for python2.7) \n to compile the matlab bindings add the flag \'-DUSE_MATLAB=On\'\nYou can define several variables to load various datasets from the python bindings: \n BERKELEY_DIR: Directory of the BSD500 dataset (point to where \'groundTruth\' and \'images\' directories are located) \n VOC_DIR: Directory of the VOCdevkit, with subfolders VOC2010, VOC2012, ... (point to where \'VOC2010\', \'VOC2011\', \'VOC2012\' directories are located) \n \n Example:\n$ mkdir build\n$ cd build\n$ cmake .. -DCMAKE_BUILD_TYPE=Release -DVOC_DIR=/home/user/Downloads/VOCdevkit -DBERKELEY_DIR=/home/user/Downloads/BSDS500 \n How to use/run \n There is a small cpp example to illustate how to use the object proposals. The majority of examples are in python however. \n eval_baseline.py : Reproduces the baseline numbers reported in the paper\neval_bnd.py: Compares various boundary detectors (fig 7 in paper).\neval_box.py: Runs GOP on the VOC 2012 detection dataset and evaluates the box overlap (run as:"python eval_box.py place_to_store_result.dat")\neval_coco.py: Evaluates GOP on the COCO dataset. This will take a while ~6 hours.\neval_learned.py : Trains a model, if none is provided and reproduces the learned numbers reported in the paper\neval_seed.py: Computes the number of undiscovered objects by various seed functions (fig 6a )\neval_size.py: Produces the accuracy vs size plot in the paper (fig 8 in paper). It does not produce the CPMC results.\nexample.py: A simple example that visualizes some proposals (required matplotlib)\nplot_box.py: Plots the bounding box results and evaluates the VUS for 2000 windows (run as "python plot_box.py place_to_store_result.dat output.pdf")\ntrain_seed.py: Trains a seed function (see eval_learned for example how to use)\ntrain_unary.py: Trains a set of foreground and background masks (see eval_learned for example how to use) \n Example (c++):\n$ build/examples/example path-to-image.png \n Example (python):\n$ cd src\n$ python eval_baseline.py\nResults in Table 1\n$ python eval_learned.py\nResults in Table 1 (or very close, depends on what you learn) \n Example (matlab):\nSee example.m\nMake sure to compile the code with \'-DUSE_MATLAB=On\' \n Seed proposals \n I added seed proposals in version 1.2 (a proposals mask containing just the seed itself). Those proposals seem to help with smaller objects for both the COCO and VOC dataset. If you\'re insterested in just bouding boxes I\'d recomment NOT to use them, as most small bounding boxes are labeled as difficult and wont be evaluated! \n License \n All code here is under a BSD license and can be used freely for academic and non-academic purposes. One exception are the saliency based seeds (saliency.cpp), which use the patented saliency filter.', "pygtsam \n Python wrappers for  GTSAM 3 .  \n This previously lived  here  but it made sense to move it a separate repo.  \n Build Instructions \n GTSAM 3.2.1 (https://research.cc.gatech.edu/borg/sites/edu.borg/files/downloads/gtsam-3.2.1.zip)\n - There is a bug with Factor removal in GTSAM 3.2.1, see: https://bitbucket.org/gtborg/gtsam/pull-requests/272/fix-recalculate-batch-step-when-variables/diff\n - replace ..../gtsam-3.2.1/gtsam/nonlinear/ISAM2.cpp with https://github.com/spillai/pygtsam/blob/dev/fixedlag/factor_removal_fix/ISAM2.cpp; this is critical to enable fixed-lag smoothing\n - replace ..../gtsam-3.2.1/gtsam/nonlinear/ISAM2.h with https://github.com/spillai/pygtsam/blob/dev/fixedlag/factor_removal_fix/ISAM2.h; this allows us to enable/disable factor ID reuse after factor removal\n - note: you may have issues with C++ Boost, especially with ver. 1.58 or greater; you may have to build ver. 1.57 from source at: https://sourceforge.net/projects/boost/files/boost/1.57.0/, using instructions at: http://www.boost.org/doc/libs/1_61_0/more/getting_started/unix-variants.html; the required Boost libraries are: python,serialization,system,filesystem,thread,program_options,date_time,regex,timer,chrono\n - build from source \n   - cd gtsam-3.2.1; mkdir build; cd build\n   - cmake -DCMAKE_INSTALL_PREFIX=..../gtsam-3.2.1/ -DGTSAM_WITH_EIGEN_MKL=OFF -DGTSAM_WITH_EIGEN_MKL_OPENMP=OFF -DGTSAM_BUILD_EXAMPLES_ALWAYS=OFF -DGTSAM_BUILD_TESTS=OFF ..\n   - make install\n - export LIBRARY and INCLUDE paths to add include and lib directories of CMAKE_INSTALL_PREFIX \n Eigen 3 (sudo apt-get install libeigen3-dev) \n Boost-Python (sudo apt-get install libboost-python-dev) \n SuiteSparse (sudo apt-get install libsuitesparse-dev) \n Google GLOG (sudo apt-get install libgoogle-glog-dev) \n pygtsam\n - git clone https://github.com/spillai/pygtsam.git\n - build from source\n   - cd pygtsam; mkdir build; cd build\n   - cmake -DCMAKE_INSTALL_PREFIX=..../gtsam-3.2.1 -DCMAKE_PREFIX_PATH=..../gtsam-3.2.1/lib/cmake/GTSAM ..\n   - make install\n - export PYTHONPATH to add lib directory of CMAKE_INSTALL_PREFIX \n Contribute \n Feel free to contribute new factors and python-based applications for GTSAM on this branch, I'm sure  Frank Dellaert  will be ecstatic that more people are using factor graphs. ", 'Towards Visual Ego-Motion Learning in Robots \n Arxiv: https://arxiv.org/abs/1705.10279  \nWebsite: http://people.csail.mit.edu/spillai/learning-egomotion  \nStay tuned for code release. ', 'tex-scripts \n Latex scripts used for compiling, editing, etc.  \n To bundle files to tar.gz file, see  bundle_latex.sh', 'Sudeep Pillai \n Aspiring technologist, thinker, innovator. \n Interests  🤓 \n \n TL;DR: Self-supervised visual intelligence in autonomous vehicles that learn from experience. \n High-impact ventures leveraging ML/CV technologies in autonomous systems \n Machine Learning, Computer Vision, Autonomous Vehicles/Robots etc \n \n Work  💻 \n \n Present : Working on something new! 🚀 🚀 🚀  \n Jan 2020 - Mar 2021 : Team Lead, Machine Learning Engineering @  Toyota Research Institute \n May 2019 - Jan 2020 : Manager, Machine Learning @  Toyota Research Institute \n Oct 2017 - May 2019 : Research Scientist @  Toyota Research Institute \n May 2014 - Aug 2014 : Research Intern @  Mitsubishi Electric Research Laboratories \n May 2009 - May 2011 : Computer Vision Developer @  PhaseSpace Motion Capture \n May 2008 - Aug 2008 : R & D Engineer Intern @  Segway Inc. \n \n Education  🎓 \n \n 2014 - 2017 : Ph.D. in Computer Science @  CSAIL, MIT \n 📕 Thesis :  SLAM-aware, Self-Supervised Perception in Mobile Robots \n 2011 - 2014 : S.M. in Computer Science @  CSAIL, MIT \n 2005 - 2008 : B.S.E in Mechanical Engineering @  University of Michigan, Ann Arbor \n \n Invited Talks 🎙 \n \n TWIMLcon 2021 :  MLOps for High-Stakes Environments \n ODSC West 2019 :  World-scale Deep Learning for Automated Driving \n Auto AI 2019 :  World-scale Deep Learning for Automated Driving \n NVIDIA GTC Silicon Valley 2019 :  Beyond Supervised Driving  (jointly with Adrien Gaidon) \n RE·WORK Deep Learning Summit Boston 2018 :  Self-supervision in Mobile Robots in the Deep Learning Era \n \n Get in touch  💬 \n \n Ask me about  Autonomous Vehicles ,  Machine Learning ,  Perception ,  Computer Vision ,  SLAM \n Social media:  Twitter ,  LinkedIn ,  Medium \n Other links:  Personal Site ,  CV / Resume ,  Google Scholar \n Email: sudeep "dot" pillai "at" gmail "dot" com \n']
tqchen,['ML-SGHMC \n Experiment codes for Paper: \n Tianqi Chen, Emily B. Fox, Carlos Guestrin "Stochastic Gradient Hamiltonian Monte Carlo"  ICML 2014 \n Informations:\n* arxiv link: http://arxiv.org/abs/1402.4102\n* matlab: contains scripts for simulated experiments\n* bayesnn: contains a numpy implementation of baysian neural network \n* mf: contains a c++ implementation of bayesian matrix factorization using SGHMC', 'systems-homework \n repo for homework on systems', "YARN EC2 \n This is a script to help you quickly setup a dynamic YARN cluster on EC2.\nIt adapts a cloud workflow to use S3 for distributed data storage and YARN for computing. \n Features \n- Dynamically add or remove slave nodes from the cluster.\n- Customized installation of packages. \n How to Use \n To start a cluster, the script follows two steps: (1) start master machine; (2) add slaves.\nThis two step procedure allows you to add and remove slaves on the fly. \n \n Start your master machine \n ./yarn-ec2 -k mykey -i mypem.pem launch cluster-name \n Add slaves to the cluster \n ./yarn-ec2 -k mykey -i mypem.pem -s nslave addslave cluster-name \n Alternatively, you can add spot instance to the cluster \n ./yarn-ec2 -k mykey -i mypem.pem -s nslave addspot cluster-name \n On demand price is used by default, you can change it by  --spot-price  option. \n Both addslave and addspot will send request to EC2 and may not be fullfilled immediately \n They will connect to the master node after one bootstrap (which takes around 1 minimute). \n You can browse the yarn resource manager for the status of the cluster. \n Shutdown the machines manually in ec2 panel \n \n Distributed Storage \n Because the cluster is dynamic, all the nodes are only used as computing nodes.\nHDFS is only started on the master machine for temp code transfer.\nNormally S3 is used instead for distributed storage. \n Customize Installation \n You can modify  custom_master_install  and  custom_all_nodes_install  in  bootstrap.py \nto add the packages you like to install on each machine. \n Restart Master Machine \n In case you stopped the master and restart it on the EC2. There is no need to do the launch step again.\nInstead, log into the master machine, and run  startup.sh  on the home folder.\nAfter the startup is finished, you can continue with the steps of adding slaves. \n Acknowledgement \n Part of yarn-ec2 is adopted from  spark-ec2  script. \n Note on Implementation \n Most existing cluster launch script follows a start and deploy way.\n- First start all the nodes, copy the master's credentials to the slaves.\n- Deploy the slaves by using ssh or pssh command from master. \n These scripts requires master to be aware of the slaves and are hard to dynamically add or remove nodes.\nyarn-ec2 uses another way, where the master does not need to be aware of slaves beforehand.\n- First start the master and listen requests from slaves.\n- When a slave get started, it runs the bootstrap script, install the dependencies and report to master.\n  - The YARN will then dynamically add the slave to the cluster.\n- When a slave get removed, the master will detect the event with cluster health check, and remove it from the cluster.", 'gtc_tutorial', 'MXNet GAN \n MXNet  module implementation of multi GPU compatible generative models. \n List of Methods \n \n Unsupervised Training \n Semisupervised Training \n Minibatch discrimation \n \n Usage \n ```python\nimport logging\nimport numpy as np\nimport mxnet as mx \n from mxgan import module, generator, encoder, viz \n def ferr(label, pred):\n    pred = pred.ravel()\n    label = label.ravel()\n    return np.abs(label - (pred > 0.5)).sum() / label.shape[0] \n lr = 0.0005\nbeta1 = 0.5\nbatch_size = 100\nrand_shape = (batch_size, 100)\nnum_epoch = 100\ndata_shape = (batch_size, 1, 28, 28)\ncontext = mx.gpu() \n logging.basicConfig(level=logging.DEBUG, format=\'%(asctime)-15s %(message)s\')\nsym_gen = generator.dcgan28x28(oshape=data_shape, ngf=32, final_act="sigmoid") \n gmod = module.GANModule(\n    sym_gen,\n    symbol_encoder=encoder.lenet(),\n    context=context,\n    data_shape=data_shape,\n    code_shape=rand_shape) \n gmod.init_params(mx.init.Xavier(factor_type="in", magnitude=2.34)) \n gmod.init_optimizer(\n    optimizer="adam",\n    optimizer_params={\n        "learning_rate": lr,\n        "wd": 0.,\n        "beta1": beta1,\n}) \n data_dir = \'./../../mxnet/example/image-classification/mnist/\'\ntrain = mx.io.MNISTIter(\n    image = data_dir + "train-images-idx3-ubyte",\n    label = data_dir + "train-labels-idx1-ubyte",\n    input_shape = data_shape[1:],\n    batch_size = batch_size,\n    shuffle = True) \n metric_acc = mx.metric.CustomMetric(ferr) \n for epoch in range(num_epoch):\n    train.reset()\n    metric_acc.reset()\n    for t, batch in enumerate(train):\n        gmod.update(batch)\n        gmod.temp_label[:] = 0.0\n        metric_acc.update([gmod.temp_label], gmod.outputs_fake)\n        gmod.temp_label[:] = 1.0\n        metric_acc.update([gmod.temp_label], gmod.outputs_real) \n     if t % 100 == 0:\n        logging.info("epoch: %d, iter %d, metric=%s", epoch, t, metric_acc.get())\n        viz.imshow("gout", gmod.temp_outG[0].asnumpy(), 2)\n        diff = gmod.temp_diffD[0].asnumpy()\n        diff = (diff - diff.mean()) / diff.std() + 0.5\n        viz.imshow("diff", diff)\n        viz.imshow("data", batch.data[0].asnumpy(), 2)\n \n ```', 'TinyFlow: Build Your Own DL System in 2K Lines \n TinyFlow is  "example code" for  NNVM . \n It demonstrates how can we build a clean, minimum and powerful computational\ngraph based deep learning system with same API as TensorFlow.\nThe operator code are implemented with  Torch7  to reduce the effort to write operators while still demonstrating the concepts of the system (and Embedding Lua in C++ is kinda of fun:). \n TinyFlow is a real deep learning system that can run on GPU and CPUs.\nTo support the examples, it takes.\n- 927 lines code for operators\n- 734 lines of code for execution runtime\n- 71 lines of code for API glue\n- 233 lines of code for front-end \n Note that more code in operators can easily be added to make it as\nfeature complete as most existing deep learning systems. \n What is it for \n As explained in the goal of  NNVM ,\nit is important to make modular and reusable components for to enable us to build\ncustomized learning system easily. \n \n Course Material for teaching DL system. TinyFlow can be used to teach student the concepts in deep learning systems. \n e.g. design homeworks on implementing symbolic differentiation, memory allocation, operator fusion. \n Experiment bed for learning system researchers. TinyFlow allows easy addition with new system features with\n  the modular design being portable to other system that reuses NNVM. \n Showcase of intermediate representation usecase. It demonstrates how intermediate representation like NNVM to be able to\n  target multiple front-ends(TF, MXNet) and backends(Torch7, MXNet) with common set of optimizations. \n Test bed on common reusable modules for DL system. TinyFlow, together with other systems(e.g. MXNet) can be used as testbed on the\n  common reusable modules in deep learning to encourage front-end, optimization module and backends\n  that are shared across frameworks. \n Just for fun :) \n \n We believe the Unix Philosophy can building learning system more fun and everyone can be able to build\nand understand learning system better. \n Deep Learning System Course \n If you are interested in learning how to build deep learning system from scratch, checkout  CSE 599G1: Deep Learning System  from University of Washington.  \n The Design \n \n The graph construction API is automatically reused from NNVM \n We choose Torch7 as the default operator execution backend. \n So TinyFlow can also be called "TorchFlow" since it is literally TensorFlow on top of Torch:) \n This allows us to quickly implement the operators and focus code on the system part. \n We intentionally choose to avoid using  MXNet  as front or backend,\n  since MXNet already uses NNVM as intermediate layer, and it would be more fun to try something different. \n \n Although it is minimum. TinyFlow still comes with many advanced design concepts in Deep Learning system.\n- Automatic differentiation.\n- Shape/type inference.\n- Static memory allocation for graph for memory efficient training/inference. \n The operator implementation is easy Thanks to Torch7. More fun demonstrations will be added to the project. \n Dependencies \n Most of TinyFlow\'s code is self-contained.\n- TinyFlow depend on Torch7 for operator supports with minimum code.\n  - We use a lightweight lua bridge code from dmlc-core/dmlc/lua.h\n- NNVM is used for graph representation and optimizations \n Build \n \n Install Torch7 \n For OSX User, please install Torch with Lua 5.1 instead of LuaJIT,\n    i.e.  TORCH_LUA_VERSION=LUA51 ./install.sh \n Set up environment variable  TORCH_HOME  to root of torch \n Type  make \n Setup python path to include tinyflow and nnvm\n bash\nexport PYTHONPATH=${PYTHONPATH}:/path/to/tinyflow/python:/path/to/tinyflow/nnvm/python \n Try example program  python example/mnist_softmax.py \n \n Enable Fusion in TinyFlow \n \n Build NNVM with Fusion: uncomment fusion plugin part in config.mk, then  make \n Build TinyFlow: enable  USE_FUSION  in Makefile, then  make \n Try Example program  example/mnist_lenet.py , change the config of session from  tf.Session(config=\'gpu\')  to  tf.Session(config=\'gpu fusion\') \n', 'TVM Project Homepage \n Serve Locally \n bash\n./serve_local.sh \n AutoBuild Branch \n The  asf-site  branch is built automatically do not modify manually.', 'FFI Navigator \n \n Most modern IDEs support find function definition within the same language(e.g. python or c++). However, it is very hard to do that for cross-language FFI calls. While solving this general problem can be very technically challenging, we can get around it by building a project-specific analyzer that matches the FFI registration code patterns and recovers the necessary information. \n This project is an example of that. Currently, it supports the PackedFunc FFI in the Apache TVM project. It is implemented as a  language server  that provides getDefinition function for FFI calls and returns the location of the corresponding C++ API in the TVM project. It complements the IDE tools that support navigation within the same language. We also have preliminary support for MXNet, DGL, and PyTorch, so we can do goto-definition from Python to C++ in these projects too. \n Installation \n Install python package\n bash\npip install --user ffi-navigator \n VSCode \n See  vscode-extension \n Emacs \n Install  lsp-mode \n Add the following configuration\n el\n(lsp-register-client\n (make-lsp-client\n  :new-connection (lsp-stdio-connection \'("python3" "-m" "ffi_navigator.langserver"))\n  :major-modes \'(python-mode c++-mode)\n  :server-id \'ffi-navigator\n  :add-on? t)) \n \n Use commands like  M-x   lsp-find-definition  and  M-x   lsp-find-references \n \n If you use eglot instead, check out  this PR .\neglot does not support multiple servers per language at the moment, the PR above contains a workaround. \n Other editors/IDEs \n It should be straightforward to add support for other editors or IDEs that have a LSP client implementation.\nPlease refer to this  site  for the availability of clients. \n Since ffi-navigator is intended to be used with other general purpose servers such as  pyls , your LSP client needs to be able to talk to multiple servers per language. If your client does not have such feature, check out  this PR  and the discussion there for a workaround. This  branch  has fallback to pyls and it is always kept up to date with the master branch. \n Features \n TVM FFI \n \n Find definition/references of FFI objects(e.g. PackedFunc in TVM) on the python and c++ side. \n Jump from a python PackedFunc into  TVM_REGISTER_GLOBAL ,  @register_func \n Find definition/references of FFI objects on the python and c++ side. \n move cursor to the object class name on the python side. \n move cursor to the  _type_key  string on the c++ side. \n \n PyTorch \n \n Jump to C10 registered ops. In python they corresponds to functions under  torch.ops  namespace. \n Example:  torch.ops.quantized.conv2d (py)  ->  c10::RegisterOperators().op("quantized::conv2d", ...) (cpp) \n Jump to cpp functions wrapped by pybind. \n Example:  torch._C._jit_script_class_compile (py)  ->  m.def( "_jit_script_class_compile", ...) (cpp) \n \n Development \n For developing the python package locally, we can just make sure ffi_navigator is in your python path in bashrc.\n bash\nexport PYTHONPATH=${PYTHONPATH}:/path/to/ffi-navigator/python \n Project Structure \n \n python/ffi_navigator The analysis code and language server \n python/ffi_navigator/dialect Per project dialects \n vscode-extension language server extension for vscode \n \n Adding Support for New FFI Patterns \n Add your FFI convention to  dialect namespace . \n Demo \n VSCode \n See  vscode-extension \n Emacs \n Goto definition from Python to C++ \n \n Goto definition from C++ to Python \n \n Find reference across Python and C++ \n \n Goto definition in PyTorch \n', 'TVM WebGPU Example \n This is an example project for building a tvm webgpu-backed module and deploy it to the web. \n Try it out \n \n You can try it out at https://tqchen.com/tvm-webgpu-example/ \n You will need a WebGPU enabled browser \n \n Build from Source \n You will need the latest version of TVM on the master.\nPlease install the TVM via source build(by setting the PYTHONPATH).\nCheckout the steps under  tvm/web  to enable webgpu support. \n Build the dependent files in the  tvm/web \n bash\ncd /path/to/tvm/web\nmake\nnpm run bundle \n Then run the build script to build the model specific data. \n bash\ncd /path/to/tvm-webgpu-examples\npython3 build.py \n You can now serve the content under  dist \n bash\ncd /path/to/tvm-webgpu-examples/dist\npython3 http.server \n You can also publish to github using the  publish_gh_pages  script.', 'Lecture14 \n This repo contains the infrastructure code needed for lecture14.\nIt can be safely replaced by hw3 repo eventually.']
deshraj,['SkyNetBrowser \n It is a web browser that enables the windows phone user to get the fast accessing to the web sites and some quick links .', 'projecteuler', 'Chineese_checkers \n This is the game build on C language and somewhat similar to the Chineese Checkers game.\nIn this game, one player will be the computer and the other will be the real user . \n User can choose the color Red or Blue so as to play .\nThe moves are aready initialized by the user(in the form of code in C language) and can be edited by changing the code of the final.c file.', 'Hackerrank \n my subissions on Hackerrank', 'This project is made using the Annotator JS library available on the annotatorjs.org . \n The technologies used in making this project are Django Framework, Javascript, Dajax, Dajaxice. \nFor setting the project, you need to start with the django installation.\nThe instructions for the django installation is given on the official website of Django https://www.djangoproject.com/download/  \n After setting the django , you need to install dajaxice and dajax  from their corresponding websites. \n https://github.com/jorgebastida/django-dajax/ \n https://github.com/jorgebastida/django-dajaxice/ \n Then copy this whole repository to any folder .  \n Open this folder in the terminal nad do the following things. \n Run these commands in sequence given through the terminal \n python manage.py collectstatic --noinput \n python manage.py runserver --insecure  \n then open ', 'Machine-Learning', 'gdg-website-', 'GDG-Website \n This is the whole django code of the website of the Google Developers Group JSS Noida. \n For further queries and questions, you can mail us at desh.py@gmail.com', 'My-Portfolio', 'scrapy-projects \n This repository contains all my work that is done using the scrapy framework.  \n About Scrapy: \n Scrapy is a fast high-level screen scraping and web crawling framework, used to crawl websites and extract structured data from their pages. It can be used for a wide range of purposes, from data mining to monitoring and automated testing. \n For more information including a list of features check the Scrapy homepage at: http://scrapy.org', 'deshraj.xyz \n For more details, visit  http://deshraj.xyz', 'pyUrl \n Url shortner using Python \n This is a django project that serves as the url shortner for shortening the links.  \n The technologies used in this project are: \n \n Django framework \n MySql Database \n Python scripting \n Html,Css,Javascript \n \n Features:\n- Web-based Configuration\n- Public or private, depending on how you configure it.\n- Links can be randomly generated, sequentially generated.\n- Compatible with any database supported by Django. Uses SQLite by default.\n- XML-RPC & JSON API with associated Documentation.\n- Abuse detection and blocking.\n- Once the shortened url is created, it is automatically copied to the browsers clipboard, allowing users to easily share it anywhere, such as:\n    * Twitter\n    * Facebook\n    * Google+\n    * Blogger\n    * Delicio.us\n    * Digg\n    * MySpace?\n    * Anywhere!', 'Audio-Scraper \n This project is made using Scrapy and django framework. It scrapes data from the website http://songspk.name and display the list of songs of new movies and links to download htem directly without getting any advertisement.  \n Technologies used :\n* Shell Scripting \n* Python\n* Django\n* Scrapy for crawling data\n* Html5\n* Css3\n* Javascript \n* Bootstrap', 'Youtube-Downloader \n Download the youtube videos through this project. Its very much simple. Currently, the website is in beta phase at http://getvids.herokuapp.com . Very soon, the website will be officially launched.  \n Installation Instructions \n git clone https://github.com/DESHRAJ/Youtube-Downloader.git ytd \n cd ytd/ytd \n virtualenv env \n source env/bin/activate \n pip install -r requirements.txt \n python manage.py runserver', 'College-Hostel-Management \n This is the Hostel Allotment project for Our College that will make the Hostel Allotment Process very easy and efficient. Currently the project si made using Django Framework , HTML, CSS and Javascript.', 'profile-gen \n This project automatically creates a page for every student of the JSSATE Noida College. The page for each student acts as a Portfolio page provided by their own college. There is a unique link for every student to to access their page.  \n For example, for a student with the regitration number 12cse020, he will be having his link on http://student.jssaten.ac.in/12cse020 (currently link is not active).  \n The technologies used for developing this project are : \nDjango Framework, MongoDb NoSql Database, Html5, CSS3, Javascript, Bootstrap', 'easyComm', 'letsGetScrapy', 'A documentation of what I do', 'Docs-For-Help', 'ncsAttendance \n Attendance Module of NCS', 'ncsUpload \n A django application that put the csv content to the MySql Database automatically. No need to manually enter the details into db.  \n This web applicaiton is live at https://ncsupload.herokuapp.com/ \n Guidelines to use this django Application: \n \n Created for the MCQ module used by the Nibble Computer Society. \n If you want to use this application for your own purpose, then see the ncsUpload/views.py file to change the content according to yourself. \n If you have any queries or you want to contact me, then feel free to ping me at deshrajdry@gmail.com. \n \n How to Use: \n \n Create an excel file.  \n In the first column, enter the question one by one.  \n In the second, third, fourth and fifth column, enter the choices accordingly.  \n In the fifth question of every column, enter the correct answer.  \n Save the file in .csv format.( Here python reads the .csv format and not .xls format) \n Upload the file on this applicaiton and click "Generate Query". \n You will get the result successfully.  \n \n Example of storing data in the Excel sheet: \n What is the core of Linux Operating System? | Shell | Kernel    | Command   | Terminal  | b', 'h1n1 \n Project X', 'SocialAuth \n Usage: \n SocialAuth  is a UI implementation of CLoudCV. It is made using the  Materializecss framework . It involves the UI implemetation of Training a class, Testing a Model adn classification of models.  \n Also, the social authentication is done using the library using django-allauth library for Dropbox, Google and Github.  \n Note: \n The current implemetation is done using Django as backend and will be updated further and will be integrated in CloudCV repository. \n Developed By: \n \n Deshraj \n', 'ml-pro-1 \n This repository contains the work done by me on Deep Learning using Theano.  \n All the code that resides in the repository is basic since it is written at the time of learning tutorials.  \n The source of learning is from : http://deeplearning.net/software/theano/tutorial ', 'djangular \n This repository defines the django + angular and the best possible ways of integrating both of these. ', 'flask-SocketIO-Progressbar', 'Beacon-Navigation', 'django-socketio-example \n A sample django web application using django-socketio for creating socket connections \n Note: Since the current version of django-socketio does not work with the latest version of gevent and gevent-socketio, so you need to install the previous version of gevent and gevent-socketio. Its already mentioned in requirements file.  \n Steps to setup the project: \n \n \n Clone the repository using the command \n git clone https://github.com/DESHRAJ/django-socketio-example.git \n \n \n Get inside the virtual environment \n soruce env/bin/activate \n \n \n Create virtual environment \n cd django-socketio-example && virtualenv env \n \n \n Install the dependenices \n pip install -r requirements.txt   \n \n \n Start the server \n python manage.py runserver_socketio \n \n \n Thats it. You are all set and ready to ROCK!', 'brahma \n brahma is intended to be a set of utilities to manage aws\ncloud infrastructure. \n Setup AWS authentication credentials \n aws configure', 'Google-Forms-Mailer', 'pycodechef', 'CodeTable \n A web application to compile and run code online on browser.  \n About this project \n Compile and run code. You just need to host the project on your machine and you are good to go.  \n This project uses the HackerEarth Compile and run API for code compilation. For more details about the API, visit the link  https://www.hackerearth.com/docs/api/developers/code/v3/ \n Instructions to setup the project on local machine \n Setting up the project on your local machine is very easy. Just three step installation after cloning the repository.  \n \n \n Create virtual environment by using following command:  \n virtualenv env\n \n \n \n If you dont have virtualenv the install using the command  \n pip install virtualenv\n \n \n \n Run the virtaulenv by  \n source env/bin/activate\n \n \n \n Install the dependencies using the command:  \n pip install -r requirements.txt\n \n \n \n Run migrations using the command:  \n python manage.py migrate\n \n \n \n Enter the HackerEarth API Key( HE_CLIENT_SECRET) in settings.py file. \n \n \n Run the development server:  \n python manage.py runserver\n \n \n \n Finally open http://127.0.0.1:8000 to see the magic into action.  \n If you want to contribute to this project, then simply: \n * Fork the repository\n* Clone on your machine \n* Make changes\n* Send Pull requests\n', 'my-cron \n Scripts that I run with cron', 'TestOAuth \n Project based on Django Rest Framework for Social Authentication using Token Based Authentication Scheme', 'Image-Search-Engine', "Diverse Beam Search \n \n Beam search, the standard work-horse for decoding outputs from neural sequence models like RNNs produces generic and uninteresting sequences. This is inadequate for AI tasks with inherent ambiguity — for example, there can be multiple correct ways of describing the contents of an image. To overcome this we propose a diversity-promoting replacement, Diverse Beam Search that produces sequences that are significantly different — with runtime and memory requirements comparable to beam search. \n Arxiv Paper link : https://arxiv.org/abs/1610.02424 \n Diverse Beam Search Demo : http://dbs.cloudcv.org/captioning \n \n Installing / Getting started \n We use RabbitMQ to queue the submitted jobs. Also, we use Redis as backend for realtime communication using websockets. \n All the instructions for setting diverse beam search from scratch can be found   here \n Interested in Contributing? \n Cloud-CV always welcomes new contributors to learn the new cutting edge technologies. If you'd like to contribute, please fork the repository and use a feature branch. Pull requests are warmly welcome. \n If you have more questions about the project, then you can talk to us on our  Gitter Channel .   \n Acknowledgements \n \n Original Diverse Beam Search Code \n NeuralTalk2 \n PyTorch \n", 'CloudCV Cookbook \n TODO: Enter the cookbook description here. \n e.g.\nThis cookbook makes your favorite breakfast sandwich. \n Requirements \n TODO: List your cookbook requirements. Be sure to include any requirements this cookbook has on platforms, libraries, other cookbooks, packages, operating systems, etc. \n e.g. \n Platforms \n \n SandwichOS \n \n Chef \n \n Chef 12.0 or later \n \n Cookbooks \n \n toaster  - cloudcvapp needs toaster to brown your bagel. \n \n Attributes \n TODO: List your cookbook attributes here. \n e.g. \n cloudcvapp::default \n \n \n Key \n Type \n Description \n Default \n \n \n [\'cloudcvapp\'][\'bacon\'] \n Boolean \n whether to include bacon \n true \n \n \n Usage \n cloudcvapp::default \n TODO: Write usage instructions for each cookbook. \n e.g.\nJust include  cloudcvapp  in your node\'s  run_list : \n json\n{\n  "name":"my_node",\n  "run_list": [\n    "recipe[cloudcvapp]"\n  ]\n} \n Contributing \n TODO: (optional) If this is a public cookbook, detail the process for contributing. If this is a private cookbook, remove this section. \n e.g.\n1. Fork the repository on Github\n2. Create a named feature branch (like  add_component_x )\n3. Write your change\n4. Write tests for your change (if applicable)\n5. Run the tests, ensuring they all pass\n6. Submit a Pull Request using Github \n License and Authors \n Authors: TODO: List authors', 'cvfy-test', 'RGB2GRAYSCALE \n A CVfy demo that converts the RGB image into GRAYSCALE ', 'Grad-CAM: Gradient-weighted Class Activation Mapping \n \n Usage \n Download Caffe model(s) and prototxt for VGG-16/VGG-19/AlexNet using  sh models/download_models.sh . \n Classification \n th classification.lua -input_image_path images/cat_dog.jpg -label 243 -gpuid 0\nth classification.lua -input_image_path images/cat_dog.jpg -label 283 -gpuid 0 \n Options \n \n proto_file : Path to the  deploy.prototxt  file for the CNN Caffe model. Default is  models/VGG_ILSVRC_16_layers_deploy.prototxt . \n model_file : Path to the  .caffemodel  file for the CNN Caffe model. Default is  models/VGG_ILSVRC_16_layers.caffemodel . \n input_image_path : Path to the input image. Default is  images/cat_dog.jpg . \n input_sz : Input image size. Default is 224 (Change to 227 if using AlexNet). \n layer_name : Layer to use for Grad-CAM. Default is  relu5_3  (use  relu5_4  for VGG-19 and  relu5  for AlexNet). \n label : Class label to generate grad-CAM for (-1 = use predicted class, 283 = Tiger cat, 243 = Boxer). Default is -1. These correspond to ILSVRC synset IDs. \n out_path : Path to save images in. Default is  output/ . \n gpuid : 0-indexed id of GPU to use. Default is -1 = CPU. \n backend : Backend to use with  loadcaffe . Default is  nn . \n save_as_heatmap : Whether to save heatmap or raw Grad-CAM. 1 = save heatmap, 0 = save raw Grad-CAM. Default is 1. \n \n Examples \n \n \n \n \'boxer\' (243) \n \n \n \n \'tiger cat\' (283) \n Visual Question Answering \n Clone the  VQA  ( http://arxiv.org/abs/1505.00468 ) sub-repository ( git submodule init && git submodule update ), and download and unzip the provided extracted features and pretrained model. \n ```\nth visual_question_answering.lua -input_image_path images/cat_dog.jpg -question \'What animal?\' -answer \'dog\' -gpuid 0\nth visual_question_answering.lua -input_image_path images/cat_dog.jpg -question \'What animal?\' -answer \'cat\' -gpuid 0 \n ``` \n Options \n \n proto_file : Path to the  deploy.prototxt  file for the CNN Caffe model. Default is  models/VGG_ILSVRC_19_layers_deploy.prototxt . \n model_file : Path to the  .caffemodel  file for the CNN Caffe model. Default is  models/VGG_ILSVRC_19_layers.caffemodel . \n input_image_path : Path to the input image. Default is  images/cat_dog.jpg . \n input_sz : Input image size. Default is 224 (Change to 227 if using AlexNet). \n layer_name : Layer to use for Grad-CAM. Default is  relu5_4  (use  relu5_3  for VGG-16 and  relu5  for AlexNet). \n question : Input question. Default is  What animal? . \n answer : Optional answer (For eg. "cat") to generate Grad-CAM for (\'\' = use predicted answer). Default is \'\'. \n out_path : Path to save images in. Default is  output/ . \n model_path : Path to VQA model checkpoint. Default is  VQA_LSTM_CNN/lstm.t7 . \n gpuid : 0-indexed id of GPU to use. Default is -1 = CPU. \n backend : Backend to use with  loadcaffe . Default is  cudnn . \n save_as_heatmap : Whether to save heatmap or raw Grad-CAM. 1 = save heatmap, 0 = save raw Grad-CAM. Default is 1. \n \n Examples \n \n \n \n What animal? Dog \n \n \n \n What animal? Cat \n \n \n \n What color is the hydrant? Yellow \n \n \n \n What color is the hydrant? Green \n Image Captioning \n Clone the  neuraltalk2  sub-repository. Running  sh models/download_models.sh  will download the pretrained model and place it in the neuraltalk2 folder. \n Change lines 2-4 of  neuraltalk2/misc/LanguageModel.lua  to the following: \n local utils = require \'neuraltalk2.misc.utils\'\nlocal net_utils = require \'neuraltalk2.misc.net_utils\'\nlocal LSTM = require \'neuraltalk2.misc.LSTM\' \n ```\nth captioning.lua -input_image_path images/cat_dog.jpg -caption \'a dog and cat posing for a picture\' -gpuid 0\nth captioning.lua -input_image_path images/cat_dog.jpg -caption \'\' -gpuid 0 \n ``` \n Options \n \n input_image_path : Path to the input image. Default is  images/cat_dog.jpg . \n input_sz : Input image size. Default is 224 (Change to 227 if using AlexNet). \n layer : Layer to use for Grad-CAM. Default is 30 (relu5_3 for vgg16) \n caption : Optional input caption. No input will use the generated caption as default. \n out_path : Path to save images in. Default is  output/ . \n model_path : Path to captioning model checkpoint. Default is  neuraltalk2/model_id1-501-1448236541.t7 . \n gpuid : 0-indexed id of GPU to use. Default is -1 = CPU. \n backend : Backend to use with  loadcaffe . Default is  cudnn . \n save_as_heatmap : Whether to save heatmap or raw Grad-CAM. 1 = save heatmap, 0 = save raw Grad-CAM. Default is 1. \n \n Examples \n \n \n \n a dog and cat posing for a picture \n \n \n \n a bathroom with a toilet and a sink \n License \n BSD \n 3rd-party \n \n VQA_LSTM_CNN , BSD \n neuraltalk2 , BSD \n', 'slackbot \n A Grad-CAM Slack that answers the questions about an image ', 'VQA Chatbot \n A Chatbot based on Visual Question Answering(VQA) \n How to setup on local machine \n \n \n Setting up VQA Chatbot on your local system is really easy. Follow the following steps: \n shell\ngit clone https://github.com/deshraj/vqa_chatbot.git\ncd vqa_chatbot\nvirtualenv env\nsource env/bin/activate\npip install -requirements.txt \n \n \n Install pytorch using following commands: \n shell\ngit clone https://github.com/hughperkins/pytorch.git\ncd pytorch\nsource ~/torch/install/bin/torch-activate\n./build.sh \n \n \n Run the local django server using following command: \n shell\npython manage.py runserver \n \n \n Open new terminal window to run the RabbitMQ worker and run the following command: \n shell\nsource env/bin/activate\npython worker.py \n \n \n Now, you are good to go. Visit http://127.0.0.1:8000 \n Interested in contributing \n If you want to contribute to the project, then fork the repository and work on feature branch and create pull requests. If you find some issues, please open issues. ', "It Takes Two to Tango: Towards Theory of AI's Mind \n Authors \n Arjun Chandrashekharan ,  Deshraj Yadav ,  Prithvijit Chattopadhyay ,  Viraj Prabhu ,  Devi Parikh \n Abstract \n Theory of Mind is the ability to attribute mental states (beliefs, intents, knowledge, perspectives, etc.) to others and recognize that these mental states may differ from one's own. Theory of Mind is critical to effective communication and to teams demonstrating higher collective performance. To effectively leverage the progress in Artificial Intelligence (AI) to make our lives more productive, it is important for humans and AI to work well together in a team. Traditionally, there has been much emphasis on research to make AI more accurate, and (to a lesser extent) on having it better understand human intentions, tendencies, beliefs, and contexts. The latter involves making AI more human-like and having it develop a theory of our minds. \n In this work, we argue that for human-AI teams to be effective, humans must also develop a theory of AI's mind -- get to know its strengths, weaknesses, beliefs, and quirks. \n We instantiate these ideas within the domain of Visual Question Answering (VQA). We find that using just a few examples (50), lay people can be trained to better predict responses and oncoming failures of a complex VQA model. Surprisingly, we find that having access to the model's internal states -- its confidence in its top-k predictions, explicit or implicit attention maps which highlight regions in the image (and words in the question) the model is looking at (and listening to) while answering a question about an image -- do not help people better predict its behavior. \n Arxiv Link \n https://arxiv.org/abs/1704.00717 \n Citing this work \n If you find this work useful in your research, please consider citing: \n @article{toaim,\n  author = {Arjun Chandrasekaran and Deshraj Yadav and Prithvijit Chattopadhyay and Viraj Prabhu and Devi Parikh},\n  title = {{I}t {T}akes {T}wo to {T}ango: {T}owards {T}heory of {AI}'s {M}ind},\n  journal = {arXiv preprint arXiv:1704.00717},\n  year = {2017}\n} \n License \n MIT", "How to create a challenge on EvalAI? \n If you are looking for a simple challenge configuration that you can replicate to create a challenge on EvalAI, then you are at the right place. Follow the instructions given below to get started. \n Directory Structure \n .\n├── README.md\n├── annotations                                 # Contains the annotations for Dataset splits\n│\xa0\xa0 ├── test_annotations_devsplit.json          # Annotations of dev split\n│\xa0\xa0 └── test_annotations_testsplit.json         # Annotations for test split\n├── challenge_data                              # Contains scripts to test the evalautaion script locally\n│\xa0\xa0 ├── challenge_1                             # Contains evaluation script for the challenge\n|        ├── __init__.py                        # Imports the main.py file for evaluation\n|\xa0 \xa0     └── main.py                            # Challenge evaluation script\n│\xa0\xa0 └── __init__.py                             # Imports the modules which involve evaluation script loading\n├── challenge_config.yaml                       # Configuration file to define challenge setup\n├── evaluation_script                           # Contains the evaluation script\n│\xa0\xa0 ├── __init__.py                             # Imports the modules that involve annotations loading etc\n│\xa0\xa0 └── main.py                                 # Contains the main `evaluate()` method\n├── logo.jpg                                    # Logo image of the challenge\n├── submission.json                             # Sample submission file\n├── run.sh                                      # Script to create the challenge configuration zip to be uploaded on EvalAI website\n└── templates                                   # Contains challenge related HTML templates\n    ├── challenge_phase_1_description.html      # Challenge Phase 1 description template\n    ├── challenge_phase_2_description.html      # Challenge Phase 2 description template\n    ├── description.html                        # Challenge description template\n    ├── evaluation_details.html                 # Contains description about how submissions will be evalauted for each challenge phase\n    ├── submission_guidelines.html              # Contains information about how to make submissions to the challenge\n    └── terms_and_conditions.html               # Contains terms and conditions related to the challenge\n├── worker                                      # Contains the scripts to test evaluation script locally\n│\xa0\xa0 ├── __init__.py                             # Imports the module that ionvolves loading evaluation script\n│\xa0\xa0 └── run.py                                  # Contains the code to run the evaluation locally \n Create challenge using github \n \n \n Use this repository as  template . \n \n \n Generate your  github personal acccess token  and copy it in clipboard. \n \n \n Add the github personal access token in the forked repository's  secrets  with the name  AUTH_TOKEN . \n \n \n Now, go to  EvalAI  to fetch the following details - \n \n evalai_user_auth_token  - Go to  profile page  after logging in and click on  Get your Auth Token  to copy your auth token. \n host_team_pk  - Go to  host team page  and copy the  ID  for the team you want to use for challenge creation. \n \n evalai_host_url  - Use  https://eval.ai  for production server and  https://staging.eval.ai  for staging server. \n \n \n Create a branch with name  challenge  in the forked repository from the  master  branch.\n Note: Only changes in  challenge  branch will be synchronized with challenge on EvalAI. \n \n \n Add  evalai_user_auth_token  and  host_team_pk  in  github/host_config.json . \n \n \n Read  EvalAI challenge creation documentation  to know more about how you want to structure your challenge. Once you are ready, start making changes in the yaml file, HTML templates, evaluation script according to your need. \n \n \n Commit the changes and push the  challenge  branch in the repository and wait for the build to complete. View the  logs of your build . \n \n \n If challenge config contains errors then a  issue  will be opened automatically in the repository with the errors otherwise the challenge will be created on EvalAI. \n \n \n Go to  Hosted Challenges  to view your challenge. The challenge will be publicly available once EvalAI admin approves the challenge. \n \n \n To update the challenge on EvalAI, make changes in the repository and push on  challenge  branch and wait for the build to complete. \n \n \n Create challenge using config \n \n \n Fork this repository. \n \n \n Read  EvalAI challenge creation documentation  to know more about how you want to structure your challenge. Once you are ready, start making changes in the yaml file, HTML templates, evaluation script according to your need. \n \n \n Once you are done making changes, run the command  ./run.sh  to generate the  challenge_config.zip . \n \n \n Upload the  challenge_config.zip  on  EvalAI  to create a challenge on EvalAI. Challenge will be available publicly once EvalAI Admin approves the challenge. \n \n \n To update the challenge on EvalAI, use the UI to update the details. \n \n \n Test your evaluation script locally \n In order to test the evaluation script locally before uploading it to  EvalAI  server, please follow the below instructions - \n \n \n Copy the evaluation script i.e  __init__.py  ,  main.py  and other relevant files from  evaluation_script/  directory to  challenge_data/challenge_1/  directory. \n \n \n Now, edit  challenge_phase  name,  annotation file  name and  submission file  name in the  worker/run.py  file to the challenge phase codename (which you want to test for), annotation file name in the  annotations/  folder (for specific phase) and corresponding submission file respectively. \n \n \n Run the command  python -m worker.run  from the directory where  annotations/   challenge_data/  and  worker/  directories are present. If the command runs successfully, then the evaluation script works locally and will work on the server as well. \n \n \n Facing problems in creating a challenge? \n Please feel free to open issues on our  GitHub Repository  or contact us at team@cloudcv.org if you have issues.", 'Django ECS Deployment \n How to deploy Django based application to AWS ECS? \n This repository describes how to deploy a django based web application to ECS. \n How to setup \n TODOs \n \n [ ] Add support for separate celery worker \n [ ] Add separate container for PostgreSQL Database \n [ ] Add support to use SQS as message broker \n [ ] Add support to create the containers on the fly \n [ ] Add support to reload the container using python code (use AWS Lambda) \n \n References \n \n Deploy Notebook \n Deploying Django app on AWS ECS using Docker \n', "Namastey, I am Deshraj 👋 \n \n**deshraj/deshraj** is a ✨ _special_ ✨ repository because its `README.md` (this file) appears on your GitHub profile.\n \n I'm an engineer. I was born in a small town called Prayagraj (formerly known as Allahabad) in India. I like to tinker with new technologies and build useful tools. \n \n 🔭 I'm currently working as a Machine Learning Engineer at  Tesla Autopilot . \n 🌱 I had built  EvalAI  as my  masters thesis  at Georgia Tech, which is an open source platform for evaluating and comparing machine learning and artificial intelligence algorithms at scale. \n ⚡ I co-founded  Caliper . Failed but learned a lot. \n 😅 Fun fact: I once booked the train tickets from destination to source instead of source to destination and I realized about this after reaching the railyway station.  \n \n 📫 I'm best reached via  email . I'm always open to interesting collaborations and hacking open source projects over the weekends. \n \n"]
yuandong-tian,['Usage:  \n \n \n Download the two .mat files:\n   ./download.sh \n \n \n Then run the matlab script.\n   runme.m \n \n', './schedule_plan.sh sample_todo.org', 'DataDrivenDescent \n C++ Implementation of the following two papers: \n Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation, ICCV 2013  [PDF] \nYuandong Tian, Srinivasa G. Narasimhan \n A Globally Optimal Data-Driven Approach for Image Distortion Estimation, CVPR 2010  [PDF]   \nYuandong Tian, Srinivasa G. Narasimhan \n Usage \n cmake ./\nmake\n./deformation_tracker.py\n', '', '', "lufei_homepage \n Lufei's homepage", 'scheduler_new \n New version of scheduler. See https://github.com/yuandong-tian/scheduler for the old version. \n The motivation of this project is to schedule a set of todo tasks, possibly dependent, for the rest of the day. The program takes a list of tasks as the input, and output a schedule, starting from right now. \n Example Input and Output \n Input: \n (Assuming it is 8:00am in the morning) \n [8:30a=1h~10m] Task 1\n[9:30a=30m~15m] Task 2\n[1h] Task 3\n[3h<10:00] Task 4\n[2h>12:00] Task 5\n[x11:20] Task 6 \n Output:\n Task 1 [     8:30a =1h ~10m ]   08:20 - 09:20\n Task 2 [    9:30a =30m ~15m ]   09:25 - 09:55\n Task 4 [          3h <10:00 ]   10:00 - 13:00\n Task 3 [                 1h ]   13:05 - 14:05\n Task 5 [          2h >12:00 ]   14:10 - 16:10 \n Compilation and Example \n bash\nmake\n./run.sh \n Usage \n Each row looks like the following: \n [Scheduling][Dependency] Task name \n Scheduling: \n | Symbol | Meaning | Example\n|--------|---------|--------\n| a      |  morning | 7:00a\n| p      | afternoon| 3:15p\n| m      | minutes  | 30m\n| h      | hours    | 1h\n| s      | seconds  | 30s\n| >      | Start after | >12:00\n| <      | Start before| <15:00\n| =      | How long this task takes | =1h\n| ~      | Uncertainty of the starting time | ~20m\n| +      | Cool down before next dependent task | +20m\n| $      | Deadline (finish before)  | $16:00\n| x      | The task is finished      | x12:30\n| c      | The task is cancelled     | c14:20 \n Dependency: \n | Symbol | Meaning | Example\n|--------|---------|--------\n| #      | tag     | #task1\n| ,      | dependent tags.| ,task2 \n Note that dependency means the current task starts only after  all  tasks with the dependent tags are completed (plus their respective cooldown). \n Example:   \n \n [8:30a=1h~10m] Task 1   \nTask 1 starts at 8:20-8:40am, and last for 1 hour. \n [1h30m] Task 2 \nTask 2 lasts for 1.5 hours. Starting time is flexible. \n [30m<13:00] Task 3   \nTask 3 lasts for 30 minutes and needs to start before 13:00 \n [50m$16:00] Task 4 \nTask 4 lasts for 50 minutes and shall be completed before 16:00. \n \n Example that uses dependency:   \n \n [20m+10m][#first] Task 1   \n [30m][#second,first] Task 2   \nTask 2 starts after at least 10 minutes after the completion of Task 1, which takes 20 minutes. \n \n License \n MIT License', 'ICML17_ReLU \n Relevant paper: \n An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis \nYuandong Tian \nICML, 2017   \n link \n Usage \n Simply run  python convergence.py  to check the convergence with population gradient.\n fig_L.py  and  fig_verify.py  can be used to regenerate the figures in the paper. \n Dependence \n Python, Matplotlib \n License \n MIT', 'MiniRTS-pretrain-models \n Pretrained models in ELF/MiniRTS \n ELF can be found  here .', 'Example Usage \n Assuming you have already installed hydra.  \n Suppose your sweep directory is in  /checkpoint/$USER/outputs/2019-12-27/08-38-43 , then you can run: \n path=2019-12-27/08-38-43; python analyze.py --logdirs $path; python stats.py --logdir $path --key_stats eval_score \n And it will automatically save to  /checkpoint/$USER/summary/2019-12-27_08-38-43.pkl , which is a pandas DataFrame that contains all statistics. \n Usage without  /checkpoint \n For other usage, try the following with a folder generated by hydra\n python analyze.py --logdirs [your saved directory with hydra]  --output_dir [your output folder] --num_process 1  --no_sub_folder --path_outside_checkpoint \nFor sweep generated by hydra, run the following:\n python analyze.py --logdirs [your sweep directory]  --output_dir [your output folder] --num_process 1  --path_outside_checkpoint \n Automatic generate md file to visualize result \n Prepare a file (e.g.,  check_list.txt ) with the content like:\n```\nTest hyper parameter a\n[your sweep filename] \n Test hyper parameter b\n[your sweep filename]\n``` \n Then you run in your current code repo:\n python ~/tools2/serve_result.py check_list.txt --match_file `pwd`/match_log3.json --output_prefix progress --key_stats acc --descending --check_freq 60 \nThen it will create  progress.md  in your current folder every 60 sec. You can use  markserv  to serve the file for web browsing, e.g., \n markserv -p 5000 progress.md \nPlease check https://github.com/markserv/markserv \n Using distributed sweep tools \n Install Ray and then run the following: \n On the host machine.\n sh ./start_ray_server.sh \n On other machines (note that the ip address of the host machine is hard-coded for now and would need to change)\n sh ./start_ray_client.sh \n Then prepare a task list (called  tasks.txt ) and run\n RAY_ADDRESS="auto" python [your path to tools2]/distribute_ray.py tasks.txt \n Installation of  common_utils  package \n Just run install \n pip install . \nAfter that you can use the package (together with  hydra )\n```python\nimport common_utils\nimport hydra\nimport logging\nlog = logging.getLogger()  \n @hydra.main(config_path="config", config_name="config.yaml")\ndef main(args):\n    log.info(common_utils.print_info(args))\n    common_utils.set_all_seeds(args.seed)\n    log.info(common_utils.pretty_print_args(args))\n```', 'ELF-examples \n Example environments in ELF platform.  \n \n Atari \n', "A simple UI and backend for scheduling tasks \n Summary \n The core idea is to have one markdown file to store status of multiple projects, including reading summary, meeting notes, todo list, idea and thoughts, etc, and create read-only views (e.g., a calendar) for different purposes.  \n This avoids having many scattered documents in multiple places (e.g., some in dropbox, some in google doc, etc.), which often causes headache and frustration.  \n Usage \n The tools is a combination of vscode plus python.  \n VSCode Side  In your VSCode setting, add the snippets as shown in  .vscode/snippet.code-snippets . Then you can write your markdown file (see  ./example.md  as an example) with shortcut like  !t[tab] . It will expand to  !TODO[start=05:44 2021/12/31;deadline=] , which automatically records the current timestamp. \n+ You can add content after the closing bracket. \n+ You can add the deadline. which can be absolute timestamp like  Nov. 23  or relative like  1d  (in a day),  1w  (in a week) or  eonw  (end of next week).\n+ You can add who will do the task. E.g.,  !TODO[start=06:16 2021/12/31;deadline=1w;who=yuandong]  means that  yuandong  will finish the task in a week. \n+ You can also add notes (by typing  !n[tab] ), or just type plain markdown texts. These won't be shown in the calendar view. Future work plans to show them to provide a complete project view.  \n Of course you can also use your favorite editor to provide this functionality.   \n Python Side  Finally you can run the following script to create a webpage to review. \n python run_plan.py upload  \nIt will create  data.js . View your calender by opening  run.html . The UI is borrowed from  here .  \n Main functionality includes:\n+ Check what tasks are due in a certain date. \n+ Check which tasks are past due \n+ Check the status of one project (specified by hashtag)\n+ Check the todo lists for one project member \n   \n If you want to deploy it to the http server that support php, replace  run_ftp  function with the uploading code snippets in  run_plan.py . Also use  run.php  instead. It allows adding notes on the webpage and save it on the server side.   \n License \n MIT"]
pricco,['Pablo\'s dotfiles \n \n \n Requirements \n \n zsh \n tmux \n iterm2 (optional) \n ag (optional) \n \n Tests \n \n shellcheck \n \n Installation \n To install these dotfiles: \n bash\nsh -c "`curl -fsSL https://raw.github.com/pricco/dotfiles/master/bootstrap.sh`" \n Fix slow vim on mac+iterm+tmux \n defaults write -g KeyRepeat -int 1 \n Source: https://github.com/tmux/tmux/issues/353#issuecomment-419286706 \n Post Instllation \n \n Under tmux press  Ctrl+a I  to install the tmux\'s plugins. \n Set iTerm2 to read the configuration file from user\'s home then run\n   killall cfprefsd  and restart iTerm2 (cmd+q) to reload the configuration. \n Patch the Menlo font to support Powerline characters. \n Set your username and e-mail in ~/.gitconfig \n \n Fonts (zsh/vim/tmux) \n \n http://powerline.readthedocs.org/en/latest/installation/linux.html#patched-font-installation \n http://powerline.readthedocs.org/en/latest/installation/linux.html#fontconfig \n https://gist.github.com/baopham/1838072 \n https://github.com/Lokaltog/powerline/issues/96 \n \n Inspiration \n \n https://github.com/skwp/dotfiles \n http://vim.spf13.com/ \n http://amix.dk/vim/vimrc.html \n https://github.com/mathiasbynens/dotfiles \n http://sheerun.net/2014/03/21/how-to-boost-your-vim-productivity/ \n https://github.com/kien/ctrlp.vim \n http://nvie.com/posts/how-i-boosted-my-vim/ \n https://github.com/grota/dotfiles \n', 'Vim and NeoVim configuration \n \n Installation \n To install these dotfiles: \n bash\nsh -c "`curl -fsSL https://raw.github.com/pricco/dotvim/master/bootstrap.sh`"', 'BrickLink Helper Chrome Extension (BHCE) \n Features \n \n Bulk delete and duplicate for Wanted Lists \n Merge wanted lists \n Force page size for Wanted List\'s detail page \n \n Development \n The  yarn build  command will "compile" the extension for Google Chrome.\nLoad the  build  directory in Google Chrome as unpacked extension.\nTo test it, run  yarn build  and the reload the extension in Google Chrome. \n Tip: Use  yarn start  in background to fix syntax errors quickly. \n Publish \n 0) Set version in manifest.json\n1)  yarn build \n2) Create zip with the build folder content\n3) Go to https://chrome.google.com/webstore/developer/\n4) Upload new version\n5) Done.']
a-covar,['Kualia \n Kualia is an android app that help people see the traffic of the international bridges of Juarez-Lincon and Puerta de las Americas Bridge', 'titanic \n This package is designed to run different algorithms for Titanic data. Here are\nthe steps we need: \n \n \n Run R-script,  ./src/R/titanic.R \n \n \n it will do data pre-processing \n \n will printout correlation matrix and make correlation plot \n will create mosaic plot \n \n write out model.arff and model.csv for further processing \n \n \n Run  bin/run_weka_alg model.arff \n \n \n it will invoke multiple Weka algorithms, see src/Weka directory and\n     will store results into the log area \n \n \n it will also print on stdout Correctly identified results and kappa\n     statistics for traninig set nad cross-validation steps \n \n \n Two perform modeling and/or prediction run  bin/run_weka  \n \n \n it will invoke given Weka algorithm, e.g. RandomForest, and\n     will store results into the log area \n \n it will also print on stdout full model statistics and create final file\n     for kaggle submission. \n \n References: \n \n Stacking Multiple Classifiers, http://www.youtube.com/watch?v=Nje8mblA7bs \n Ensemble in R, http://vikparuchuri.com/blog/intro-to-ensemble-learning-in-r/ \n', 'Android101 \n Android 101 is a training material develop in Spanish .. \n Contenido \n 1 - Android antes y ahora\n2 - The Stack\n    * Linux\n    * Libreria Nativas\n    * Dalvik\n    * Android y Java\n    * Application Framework\n    * Applications\n3 - Hello World!\n4 - Hands-on!\n5 - Recursos y Recomendaciones \n Demos \n a) Hello World \n b) Where Am I \n``` \n Emulador ver lista posible \n android list targets \n Emular posiciones GPS en el emulador \n Utilicen una coneccion de Telnet \n telnet localhost   or telnet 127.0.0.1  \n geo fix    \n telnet localhost:5554  |  telnet 127.0.0.1 5554\ngeo fix -121.45356 46.51119 4392 \n ``` \n Eclipse Shurtocuts \n Ctrl + Shift + O   -> Organizar Imports\nCtrl + Shift + F   -> Format Code \nCTRL + SPACE       -> Auto Complete \nALT + ↑ or ↓       -> mover line abajo o arriba \n Installar Android \n ```\nDescarga Java SDK \n Asegurate de descargar Java SE 6 SDK \nhttp://www.oracle.com/technetwork/java/javasebusiness/downloads/java-archive-downloads-javase6-419409.html \n Descarga el ADT Bundel en el siguiente link \n http://developer.android.com/sdk/index.html \n Descomprime el zip y coloca el folder en el directorio de tu preferencia  \n Windows \n    C:\\ apps\\ android-sdk-windows \nLinux \n    /home/ YourUserName/ android-sdk-linux_86 \nMac OS X \n    /Users/ YourUserName/ android-sdk-mac_86 \n agrega a tus variables de path los siguientes folder  \n Para Windows en la linea de coomandos ejecuta las siguientes lineas :\n    SET PATH=%PATH%; /sdk/platform-tools/\n    SET PATH=%PATH%; /sdk/tools/ \n Para Mac and Linux \n    sudo nano /etc/paths \n        agregar en el archivo\n              /sdk/platform-tools/\n              /sdk/tools/\nlisto\n```', 'JavaScript-101 \n Training Material for the aptecic.org training ', 'Working with Node.js Packages \n One of the most powerful features of the Node.js framework is the ability to easily extend it with additional Node Packaged Modules (NPMs). \n A Node Packaged Module is a packaged library that can easily be shared, reused, and installed in different projects. There are many different modules available for a variety of purposes. \n Node.js modules are created by various third-party organizations to provide important features that Node.js lacks out of the box. \n Package.json \n Each Node Packaged Module includes a package.json file that defines the packages. The package.json file includes informational metadata such as the name, version, author, and contributors, as well as control metadata such as dependencies and other requirements that the Node Package Manager will use when performing actions such as installation and publishing. \n Package.json \n ```\n{ \n   "name": "my_module", \n   "version": "0.1.0", \n   "description": "a simple node.js module", \n   "dependencies" : { \n        "express" : "latest" \n    } \n} \n ``` \n Dependecies and devDependecies hash \n Dependencies are specified with a simple hash of package name to version range. The version range is a string which has one or more space-separated descriptors. Dependencies can also be identified with a tarball or git URL. \n \n version Must match version exactly \n >version Must be greater than version \n >=version etc \n <version \n <=version \n ~version "Approximately equivalent to version" See semver(7) \n ^version "Compatible with version" See semver(7) \n 1.2.x 1.2.0, 1.2.1, etc., but not 1.3.0 \n http://... See \'URLs as Dependencies\' below \n * Matches any version \n "" (just an empty string) Same as * \n version1 - version2 Same as >=version1 <=version2. \n range1 || range2 Passes if either range1 or range2 are satisfied. \n git... See \'Git URLs as Dependencies\' below \n user/repo See \'GitHub URLs\' below \n path/path/path See Local Paths below  \n \n ``` \n { "dependencies" :\n  { "foo" : "1.0.0 - 2.9999.9999"\n  , "bar" : ">=1.0.2 <2.1.2"\n  , "baz" : ">1.0.2 <=2.3.4"\n  , "boo" : "2.0.1"\n  , "qux" : "<1.0.0 || >=2.3.1 <2.4.5 || >=2.5.2 <3.0.0"\n  , "asd" : "http://asdf.com/asdf.tar.gz"\n  , "til" : "~1.2"\n  , "elf" : "~1.2.3"\n  , "two" : "2.x"\n  , "thr" : "3.3.x"\n  , "lat" : "latest"\n  , "dyl" : "~/projects/dyl"\n  }\n} \n ``` \n If someone is planning on downloading and using your module in their program, then they probably don\'t want or need to download and build the external test or documentation framework that you use. \n In this case, it\'s best to list these additional items in a devDependencies hash. \n ``` \n { "name": "ethopia-waza",\n  "description": "a delightfully fruity coffee varietal",\n  "version": "1.2.3",\n  "devDependencies": {\n    "coffee-script": "~1.6.3"\n  },\n  "scripts": {\n    "prepublish": "coffee -o lib/ -c src/waza.coffee"\n  },\n  "main": "lib/waza.js"\n} \n ``` \n To view the full  package.json specification . Also visit the  interactive guide . \n One of the things I love about the Node modules is that there is a managed location called the Node Package Registry where packages are registered. The registry allows you to publish your own packages in a location where others can use them as well as download packages that others have created. The Node Package Registry is located at http:// npmjs.org . From this location, you can view the newest and most popular modules as well as search for specific packages. \n Command-line options \n | Options         | Description                                     | Example               |\n|---------------|-----------------------------------------------|-----------------------|\n| search        | Finds module packages in the repository       | npm search express    |\n| install         | Installs a package either using a package.json file from the repository or from a local location        | npm install , npm install express , npm install express@0.1.1 , npm install ../Module.tgz         |\n| install -g    | Installs a package in a globally accessible   | npm install -g express-generator|\n| remove            | Removes module                                              | npm remove express      |', '', 'Start Bootstrap  -  Creative \n Creative  is a one page creative theme for  Bootstrap  created by  Start Bootstrap . \n Getting Started \n To use this theme, choose one of the following options to get started:\n* Download the latest release on Start Bootstrap\n* Fork this repository on GitHub \n Bugs and Issues \n Have a bug or an issue with this theme?  Open a new issue  here on GitHub or leave a comment on the  template overview page at Start Bootstrap . \n Creator \n Start Bootstrap was created by and is maintained by  David Miller , Managing Partner at  Iron Summit Media Strategies . \n \n https://twitter.com/davidmillerskt \n https://github.com/davidtmiller \n \n Start Bootstrap is based on the  Bootstrap  framework created by  Mark Otto  and  Jacob Thorton . \n Copyright and License \n Copyright 2013-2015 Iron Summit Media Strategies, LLC. Code released under the  Apache 2.0  license.', 'JS Tools and Frameworks \n This app is a simple example on how to use React and Redux. The API Server has only 1 endpoint that returns an array of the names of JavaScript build tooling choices (e.g. Webpack, etc), JavaScript frameworks (e.g React, etc). \n The React App allows the user to mark their opinion of each item with a positive or negative reaction (no need to persistent data). Once they have made at least 3 reactions, fetch the remainder of your items from the API and add them to the view. \n \n TRY IT !!! \n Run the Server\n bash\ncd src/server\nnpm install\nnpm test\nnpm start \n Run the UI\n bash\ncd src/public\nnpm install\nnpm start \n Lighthouse Score \n', 'trading_momentum']
tiborpilz,['DrawMe! \n A Python based Telegram bot that applies hand drawn styles to photos. \n Prerequisites: \n \n Python Telegram Bot \n Imagemagick \n \n Installation: \n \n Install prerequisites \n Clone this repo \n Create telegram bot via @botfather \n Put the token in "token.txt" \n \n Run: \n python bot.py \n Have fun!', 'Screenshot Mailer \n A small tool to set up automatic screenshots that are mailed to a specified mail address and/or FTP server \n Config \n Copy config.json.example to config.json and fill in the data (email, password etc.).', 'base16-scheme-generator (WIP) \n Base 16 Color scheme generation', 'dotfiles \n Arch / Bspwm / Zsh / Vim / ... more \n Usage \n \n Clone this repo with its submodules:  git clone https://github.com/tiborpilz/dotfiles --recursive \n Run install script  cd dotfiles && ./install \n', 'PKGBUILDS for lnc.lt custom arch repository \n This repository contains PKGBUILDs for all the packages on the https://arch.lnc.lt/repo repository. The packages are mostly meta-packages, allowing for easy, modular scaffolding of different Arch Linux installs. \n Usage \n \n Via Docker \n \n #### Dependencies:\n     * Docker \n Run  docker run -v "$PWD/repo:/repo" lnclt/arch-repo . \n \n Per hand \n \n #### Dependencies:\n      * Arch Linux\n      * base-devel, wget, and a non-root user for installing packages\n      * aurutils (aur) \n Run  ./build_packages.sh . Ensure the proper pkg and repo paths are set inside the script.\n  By default, it assumes a  pkg  folder full of package folders, which in turn contain the PKGBUILDS,\n  as well as a  repo  folder, which is where the repostiory files and package binaries are saved.', "A PKGBUILD builder as a Docker image \n This Docker image is a base Arch Linux with a custom builduser, a few dependencies and config tweaks\nso it's ready out-of-the box to build Arch Linux packages and add them to a custom repository.\nIts primary use is for the  lnclt packages  but\nit can easily be tweaked for any other automated package building - be it AUR packages or your own \ncustom meta packages.", "lnclt Custom Arch Linux Repository \n These are build tools, PKGBUILDs and an install script to make the installation of Arch Linux from a live medium\nto a familiar system environment as quick and painless as possible. \n Acknowledgements \n mdaffin , whose blog post inspired me to do this, and who's repository \nwas a tremendous help in getting the first version of this project started.", 'Arch-Build Docker image \n A basic Arch Linux custom repository build system.', 'VirtuAW \n VirtuaW is a Virtual Audio Workstation with a flow/node based workflow - powered by Angular, Typescript and Tone.js. \n \n \n \n Dependencies/Building \n git clone git@github.com:tiborpilz/virtuaw-web.git \n cd virtuaw-web \n yarn  (or  npm install )\n ng serve  for the angular-cli development server. \n Current Status \n This is a very very early proof of concept - a basic synthesizer is working, as well as harmonics and an arpeggiator. Things might, can and will break and change rapidly.', 'CSS Houdini - Presentation & Example Code \n The presentation is written in markdown files and converted via pandoc.\nRun  ./ops/build  to generate a reveal.js presentation. All sources are in  bibliography.bib  and get appended to the end of the presentation.', 'NixOS \n Declarative, immutable os configuration', 'Gitlab CI/CD Introduction \n The presentation is written in markdown files and converted via pandoc.\nRun  ./ops/build  to generate a reveal.js presentation.', 'Pandoc Presentation Template', 'Polyrhythm Generator \n Small web app to generate polyrhythms. The app is written using\n Svelte  and  Tone.js . \n Try the app']
laurentalsina,[]
fjordan,["Arduino Remote Deploy \n Arduino-powered remote deploy script execution \n Based on Niall O'Higgins's project found here: https://github.com/niallo/arduino-deploybutton \n Execute your deploy script on a remote host via ssh and an arduino button. \n \n", 'User Pack Template \n This is a template for your own user (or other purpose) pack. \n init.el \n Use the file  init.el  for your own configuration elisp. If this starts\ngetting unwieldy then you might want to break out the config into\nseparate files which you can store in the config directory. \n config \n Files placed in the  config  dir may then be referenced and pulled into\nyour  init.el  via the fn  live-load-config-file . For example, if you\nhave the file config/foo.el then you may load it in with: \n (live-load-config-file "foo.el")\n \n lib \n If you want to pull in external libraries into your pack, then you\n should place the libraries within the lib dir. To add a directory\n within the pack\'s lib directory to the Emacs load path (so that it\'s\n contents are available to require) you can use the fn\n  live-add-pack-lib . For example, if you have the external library bar\n stored in lib which contains the file  baz.el  which you wish to\n require, this may be achieved by: \n (live-add-pack-lib "bar")\n(require \'baz)\n \n Have fun!', 'Openark Kit - common utilities for MySQL \n Fork of https://code.google.com/p/openarkkit/ \n The openark kit provides common utilities to administer, diagnose and audit MySQL databases. \n Please refer to the openark kit documentation for details. \n The available tools are: \n \n oak-apply-ri: apply referential integrity on two columns with parent-child relationship. \n oak-block-account: block or release MySQL users accounts, disabling them or enabling them to login. \n oak-chunk-update: perform long, non-blocking UPDATE/DELETE operation in auto managed small chunks. \n oak-get-slave-lag: print slave replication lag and terminate with respective exit code. \n oak-hook-general-log: hook up and filter general log entries based on entry type or execution plan criteria. \n oak-kill-slow-queries: terminate long running queries. \n oak-modify-charset: change the character set (and collation) of a textual column. \n oak-online-alter-table: perform a non-blocking ALTER TABLE operation. \n oak-prepare-shutdown: make for a fast and safe MySQL shutdown. \n oak-purge-master-logs: purge master logs, depending on the state of replicating slaves. \n oak-repeat-query: repeat query execution until some condition holds. \n oak-security-audit: audit accounts, passwords, privileges and other security settings. \n oak-show-limits: show AUTO_INCREMENT “free space”. \n oak-show-replication-status: show how far behind are replicating slaves on a given master. \n \n All tools are coded in Python, require Python 2.3 or newer, and the python-mysqldb driver. Some tools require MySQL 5.0 or higher; see the docs for each tool.']
sseffa,['Fully CMS \n Laravel 5.1 Content Managment System \n not stable! \n \n Features \n \n Laravel 5.1 \n Bootstrap \n Authentication Sentinel \n Ckeditor \n Bootstrap Code Prettify \n File Manager \n Dropzone.js \n Backend \n Manage menu (nested) \n Manage article (category, tag) \n Manage tag \n Manage article category \n Manage page \n Manage news \n Manage photo gallery \n CKEditor for post creation and editing (filemanager) \n Form post manage \n Site settings \n Log view page \n Frontend \n Article (momentjs) \n Page \n News \n Photo Gallery (Lazy load image, responsive fancybox) \n Contact Form \n Breadcrumbs \n \n Installation \n Please check the system requirements before installing sf CMS. \n \n You may install by cloning from github, or via composer. \n Github:  git clone git@github.com:sseffa/fullycms.git \n From a command line open in the folder, run  composer install . \n Enter your database details into  config/database.php . \n Run  php artisan app:install  to setup and seed your database and admin information \n Settings  config/fully.php . (optional) \n Cache enable / disable \n image folder \n post per page \n ... \n \n Credits \n \n http://almsaeedstudio.com/preview/ \n http://www.dropzonejs.com/ \n http://ckeditor.com/ \n http://www.eyecon.ro/bootstrap-datepicker/ \n http://fancyapps.com/fancybox/ \n https://github.com/simogeo/Filemanager \n https://github.com/dbushell/Nestable \n http://momentjs.com/ \n https://github.com/tuupola/jquery_lazyload \n https://github.com/mikemand/logviewer/ \n... \n \n Screenshots \n \n \n \n Licence \n MIT license', 'Asque (Ask Question) \n Sistem Analizi ve Tasarımı Dersi Proje Ödevi \n Soru-Cevap Uygulaması (stackoverflow tarzında) \n Installation \n Please check the system requirements before installing Asque (Ask Question). \n \n You may install by cloning from github, or via composer. \n Github:  git clone git@github.com:sseffa/asque.git \n Composer:  composer create-project sseffa/asque --prefer-dist \n From a command line open in the folder, run  composer install . \n Enter your database details into  app/config/database.php . \n Run  php artisan migrate  to setup and  php artisan db:seed  seed your database. (php artisan app:install) \n \n Screenshots \n \n \n \n \n \n \n', 'Laravel Video API \n \n \n \n Installation \n 1. Install with Composer \n bash\ncomposer require "sseffa/video-api": "dev-master" \n Laravel  | video-api\n:---------|:----------\n 4.x.x    | 1.x\n 5.x.x    | 2.x \n 2. Add to  app/config/app.php \n php\n    \'providers\' => array(\n        // ...\n        \'Sseffa\\VideoApi\\VideoApiServiceProvider\',\n    ), \n And: \n php\n    \'aliases\' => array(\n        // ...\n        \'VideoApi\'          => \'Sseffa\\VideoApi\\Facades\\VideoApi\',\n    ), \n Usage \n Youtube API Key \n ```php\n<?php \n Route::get(\'video/youtube/{id}\', function ($id) { \n //$data = VideoApi::setType(\'youtube\')->setKey(\'api-key\')->getVideoDetail($id); // video detail\n$data = VideoApi::setType(\'youtube\')->setKey(\'api-key\')->getVideoList($id);     // video list\n\nvar_dump($data);\n \n }); \n Route::get(\'video/vimeo/{id}\', function ($id) { \n //$data = VideoApi::setType(\'vimeo\')->getVideoDetail($id);\n$data = VideoApi::setType(\'vimeo\')->getVideoList($id);\n\nvar_dump($data);\n \n }); \n ``` \n Licence \n MIT license', "Doconverter \n With the help of this project, you can esaily convert your PDF and Office files to images by using command lines. \n Warning \n Doesn't work very stable, yet. \n Requirements \n Firstly requirse app  ghostscript  for PDF files. \n Again you must have Office programs installed on your system. * You might get an error if you try to convert a file which was created in a newer version of Office than the one you have on your system. \n How to use \n Just type  doconverter.exe file_toconvert.doc|xls|ppt|pdf  in your command line. System will create a file called  export  where your file is located and produce the images there. \n Configs \n You can make configurations with the help of  doconverter.exe.config  file. \n \n Ghostscript  file location settings \n Image resolution settings for Word and PowerPoint \n Resolution settings for PDF files ( Ex : 100,200,...) \n Excel works in limits which are already set. This will be editable in settings in coming versions. \n", 'aspnet-core-api', 'asp.netcore-efcore \n \n DI \n Repository \n Seeder \n One2One \n Many2Many \n', 'kotlin-w-spring', 'image-processing \n image processing lesson examples', 'functional-programming', 'algorithm-analysis', 'socketio-chess', 'Hi there 👋 \n \n**sseffa/sseffa** is a ✨ _special_ ✨ repository because its `README.md` (this file) appears on your GitHub profile.\n\nHere are some ideas to get you started:\n\n- 🔭 I’m currently working on ...\n- 🌱 I’m currently learning ...\n- 👯 I’m looking to collaborate on ...\n- 🤔 I’m looking for help with ...\n- 💬 Ask me about ...\n- 📫 How to reach me: ...\n- 😄 Pronouns: ...\n- ⚡ Fun fact: ...\n \n My GitHub Stats \n \n', 'XYZSports', 'Proje kodlarını içerir']
100,['/r/csgobetting Simulator Bot \n (Although functional and complete, this project was primarily an exercise to learn the resources that it uses.) \n This bot runs on the /r/csgobetting subreddit (https://www.reddit.com/r/csgobetting) on the popular website https://www.reddit.com. It interacts with the portion of the Counter-Strike: Global Offensive that is involved in betting in-game items on the results of professional tournaments and games, and allows users to make \'simulated\' bets and monitor their money through this bot based on these real match events. \n In order to use the bot, users must include their command to the bot in a particular format, and in the reddit submission that correlates with the match on which they wish to place their bets on. The bot is not case-sensitive for the most part; however, the portion of the command that conveys the type of command must be in lowercase (e.g. "bet" or "check). In addition, the bot only recognizes team names in the form that CSGL (csgolounge.com) uses on its match page (but is still case-insensitive).  \n This application uses: \n \n Python \n SQLAlchemy ORM \n PRAW API (Reddit API) \n CSGOLounge API \n \n Current Commands: \n Commands may be in any portion of a reddit post to the submission, so please, feel free to add it to the end of analysis posts. \n \n To place a bet: "!CSGOBettingSimBot bet [amount] [team]" \n To check one\'s current status: "!CSGOBettingSimBot check" \n', 'geotwitter \n (Although functional and complete, this project was primarily an exercise to learn the resources that it uses.) \n This web-app uses Twitter information to provide insight into a particular zip-code. It provides various metrics, such as a showcase of the most recent tweets, the most used hashtags, and the average sentiment that result from the search. It is a valuable tool when visiting or moving to a new location, as one can quickly get a snapshot of particular facets of the community in a convenient, concise fashion. \n This app uses: \n \n Python \n Flask web framework \n Tweepy API \n AlchemyAPI Sentiment Analysis API \n Google Maps Geolocation API \n HTML \n Bootstrap CSS Framework \n \n This app implements: \n \n The Haversine Formula \n', 'Lore \n This web-app utilizes Markov Chains, a single-state "machine", in order to create psuedo-realistic filler texts. Useful for various types of developers, this application improves upon the typical "lorem ipsum" texts and instead provides filler, or dummy, text that is both tailored to a specific length, and from a specific type of source (e.g. old literature, modern literature, or social media) - which will be partially contributed to by users. Additionally, using its API, users will be able to quickly request for texts with their desired parameters in their own code, as an alternative to the web-based interface. \n This app uses: \n \n Python \n Flask web framework \n Tweepy API \n PRAW (Reddit API) \n SQLAlchemy ORM \n HTML \n Bootstrap CSS Framework \n \n This app implements: \n \n Markov Chain \n REST API \n', "Talk Stock \n This web-app allows users to rate relevant news articles via the /r/talkstock subreddit, and determines the most pressing articles via Reddit's upvote system. Additionally, users may post comments on each article and vote on the analyses of others, and maintain a 'karma' score based on the score of their comments on the respective reddit submissions. Users within the top 10% of analysts will be able to join a private subreddit for the top analysts, and hear exclusive insights from the most-trusted members of the community. \n This app uses: \n \n Python \n Flask web framework \n Twilio API \n PRAW (Reddit API) \n New York Times API \n PyMongo (MongoDB) \n HTML \n Skeleton CSS Framework \n", "Subreddit-recommender \n The data: \n I originally found a subset of what is probably my favorite dataset, the collection of all Reddit comments, on Kaggle; it provided May 2015's comments in a SQLite database. However, I wanted some experience reading through files in Python and really dealing with more-difficulty-formatted data, so I googled around, and found another form of this dataset in a plain text file. It was a 30GB text file of JSON blobs. \n Process: \n I tried to open the text file, but was unable to due to its large size. Instead, I split the file into 30 smaller text files, which proved to be much more feasible. Then, I iterated through each .txt file in the directory, and obtained each JSON block, delimited by newlines. Some of the blocks were not valid (either due to issues in the original collection process or uneven splitting by me), so I had to ensure that each block was valid and had the required fields. Luckily, very few, if any, of the JSON were invalid. \n The code: \n I wanted to do something simple that I could implement from scratch, so I decided to use a very simple machine learning model that used the k-Nearest-Neighbor method. I began by getting a list of all the users in the dataset and all of the subreddits in the union of their subscriptions (only subreddits with 2 or more subscribers were included), operating under the naive assumption that commenting in a subreddit meant that a user was subscribed and that not commenting meant that a user was not subscribed. To further this method, I then used praw to get that user's past comments. This has clear flaws and gravitates towards comment-oriented subreddits, but under the circumstances, was the most logical assumption to make. I then created vectors for each subreddit, with binary values depending on whether or not each user subscribed to it. \n Assuming that there were N users, and M subreddits: I had M [Nx1] vectors. \n Then, since the calculations are impossibly time-consuming to do online, I had to index the similarities of every subreddit against every other subreddit. I used Jaccard Similarity in this case, because the vectors were incredibly sparse, and using something such as Euclidean distance or Cosine similarity would simply not capture the intended overlaps. \n I therefore created an [MxM] matrix S, where S[i][j] is the similarity between the ith subreddit and the jth subreddit. In order to account for comparing a subreddit with itself, I set every entry where i=j to 0. \n Finally, in order to find the kth nearest subreddits to the nth subreddit, I simply needed to retrieve the nth column, and extract the kth highest values and their corresponding row labels. \n After-thoughts: \n This method is horribly inefficient, and struggles to run at any scale remotely near the 50 million-large dataset that I have on my personal computer. It was definitely more of a proof-of-concept learning experience for me, but even then, it was only able to process ~30 comments per minute. This could definitely be improved by using more functions of pandas, which are hyperoptimized compared to my implementations, or obviously, by just using scikit-learn's kNN module.", 'Predicting and Analyzing Reddit Comment Karma \n SEE IT LIVE  HERE ! \n Uses various data-science and NLP methods to analyze comment karma throughout Reddit and provide this information with visualizations and a RESTful API. \n Dependencies: \n \n Flask (0.9) \n Flask-Limiter (0.9.1) \n WTForms (2.1) \n gunicorn (0.17.2) \n textblob (0.11.0) \n numpy (1.10.1) \n scikit-learn (0.17) \n Python 2.7.11 \n [Optional] Pickle files present in pickles folder \n \n Implements: \n \n K-means unsupervised clustering \n Linear SVM supervised classification \n RESTful API \n Documented \n Rate-limited \n \n \n Extensive JSON parsing \n Various facets of Natural Language Processing \n Graph visualization \n JS (cytoscape.js) \n \n \n Various facets of web-design \n HTML \n CSS (Bootstrap) \n \n \n \n Inspired by the work done  here  and  here , this application utilizes two classifiers and k-means clustering to provide insights on comment karma on Reddit. The first classifier used n-grams to classify comments as either positive or negative, and the second, which uses the classification from the first as one of its features, classifies comments into one of five bins based on score ranges based on various metadata and calculated NLP-related features. The application also, independent from the aforementioned tasks, parsed the aggregate data to cluster comments by average karma per comment, and created a clustered graph visualization of the top subreddits in the data-set. Lastly, the application implemented a fully-documented and rate-limited RESTful API to allow developers to use the prediction service in their own applications.', 'Analyzing Trends in Stock Movements using Markov Chains and Monte Carlo Simulation of Random Walks \n SEE DEMONSTRATIONS  HERE ! \n Download the executable .jar file  here . \n Dependencies: \n \n Java SE 8 recommended (should work on anything 6+) \n markovstock.jar file to use right away \n Files in repo if building from source, including .jar files in /lib directory: \n Apache Commons Math \n JFreeChart (and JCommon) \n JSoup \n OpenCSV \n SQLite3 Java Driver \n \n \n \n Implements: \n \n Markov Chain \n Monte Carlo Simulation of Random Walks \n Interaction with SQLite3 Embedded Relational Database \n Reading and Writing to CSV \n Web-Scraping \n Use of Yahoo Finance API \n Time-Series Visualization \n Desktop Application Development \n GUI Creation with Swing \n Concurrency \n \n \n \n \n', 'Tab-Organizer 1.0 by Devin Soni \n Download it  here ! \n Featured on  Product Hunt  and  Lifehacker , among other sites. \n Chrome extension that rearranges tabs in groups of matching URLs. Groups tabs in the order that the group\'s first member originally appeared. \n Version 1.0 \n Added options page, and now allows users to auto-sort and use the keyboard shortcut "Ctrl + Shift + A" to sort. \n Version 0.1 \n Initial version.', 'Subreddit-Recommendation \n Work in Progress', 'My humble dotfiles', 'Atom-things \n Themes \n To convert themes to Atom packages: \n apm init --package ~/.atom/packages/language-r \\\n  --convert {{ PATH TO .tmTheme FILE }}', '\n \n \n \n \n \n Cranium  is a portable, header-only, feedforward artificial neural network library written in vanilla C99. \n It supports fully-connected networks of arbitrary depth and structure, and should be reasonably fast as it uses a matrix-based approach to calculations. It is particularly suitable for low-resource machines or environments in which additional dependencies cannot be installed. \n Cranium supports CBLAS integration. Simply uncomment line 7 in  matrix.h  to enable the BLAS  sgemm  function for fast matrix multiplication. \n Check out the detailed documentation  here  for information on individual structures and functions. \n \n Features \n \n Activation functions \n sigmoid \n ReLU \n tanh \n softmax ( classification ) \n linear ( regression ) \n \n \n Loss functions \n Cross-entropy loss ( classification ) \n Mean squared error ( regression ) \n \n \n Optimization algorithms   \n Batch Gradient Descent \n Stochastic Gradient Descent \n Mini-Batch Stochastic Gradient Descent \n \n \n L2 Regularization \n Learning rate annealing \n Simple momentum \n Fan-in weight initialization \n CBLAS support for fast matrix multiplication \n Serializable networks \n \n \n Usage \n Since Cranium is header-only, simply copy the  src  directory into your project, and  #include "src/cranium.h"  to begin using it.  \n Its only required compiler dependency is from the  <math.h>  header, so compile with  -lm . \n If you are using CBLAS, you will also need to compile with  -lcblas  and include, via  -I , the path to wherever your particular machine\'s BLAS implementation is. Common ones include  OpenBLAS  and  ATLAS . \n It has been tested to work perfectly fine with any level of gcc optimization, so feel free to use them.  \n \n Example \n ```c \n include "cranium.h" \n / \nThis basic example program is the skeleton of a classification problem.\nThe training data should be in matrix form, where each row is a data point, and\n    each column is a feature. \nThe training classes should be in matrix form, where the ith row corresponds to\n    the ith training example, and each column is a 1 if it is of that class, and\n    0 otherwise. Each example may only be of 1 class.\n / \n // create training data and target values (data collection not shown)\nint rows, features, classes;\nfloat  training;\nfloat  classes; \n // create datasets to hold the data\nDataSet  trainingData = createDataSet(rows, features, training);\nDataSet  trainingClasses = createDataSet(rows, classes, classes); \n // create network with 2 input neurons, 1 hidden layer with sigmoid\n// activation function and 5 neurons, and 2 output neurons with softmax \n// activation function\nsrand(time(NULL));\nsize_t hiddenSize[] = {5};\nActivation hiddenActivation[] = {sigmoid};\nNetwork* net = createNetwork(2, 1, hiddenSize, hiddenActivation, 2, softmax); \n // train network with cross-entropy loss using Mini-Batch SGD\nParameterSet params;\nparams.network = net;\nparams.data = trainingData;\nparams.classes = trainingClasses;\nparams.lossFunction = CROSS_ENTROPY_LOSS;\nparams.batchSize = 20;\nparams.learningRate = .01;\nparams.searchTime = 5000;\nparams.regularizationStrength = .001;\nparams.momentumFactor = .9;\nparams.maxIters = 10000;\nparams.shuffle = 1;\nparams.verbose = 1;\noptimize(params); \n // test accuracy of network after training\nprintf("Accuracy is %f\\n", accuracy(net, trainingData, trainingClasses)); \n // get network\'s predictions on input data after training\nforwardPass(net, trainingData);\nint* predictions = predict(net);\nfree(predictions); \n // save network to a file\nsaveNetwork(net, "network"); \n // free network and data\ndestroyNetwork(net);\ndestroyDataSet(trainingData);\ndestroyDataSet(trainingClasses); \n // load previous network from file\nNetwork* previousNet = readNetwork("network");\ndestroyNetwork(previousNet);\n``` \n \n Building and Testing \n To run tests, look in the  tests  folder.  \n The  Makefile  has commands to run each batch of unit tests, or all of them at once. \n \n Contributing \n Feel free to send a pull request if you want to add any features or if you find a bug. \n Check the issues tab for some potential things to do.', '\n \n \n \n \n \n Solid  is a Python framework for gradient-free optimization. \n It contains basic versions of many of the most common  optimization algorithms that do not require the calculation of gradients , and allows for very rapid development using them. \n It\'s a very versatile library that\'s great for learning, modifying, and of course, using out-of-the-box. \n See the detailed documentation  here . \n \n Current Features: \n \n Genetic Algorithm \n Evolutionary Algorithm \n Simulated Annealing \n Particle Swarm Optimization \n Tabu Search \n Harmony Search \n Stochastic Hill Climb \n \n \n Usage: \n \n pip install solidpy   \n Import the relevant algorithm \n Create a class that inherits from that algorithm, and that implements the necessary abstract methods \n Call its  .run()  method, which always returns the best solution and its objective function value \n \n \n Example: \n ```python\nfrom random import choice, randint, random\nfrom string import lowercase\nfrom Solid.EvolutionaryAlgorithm import EvolutionaryAlgorithm \n class Algorithm(EvolutionaryAlgorithm):\n    """\n    Tries to get a randomly-generated string to match string "clout"\n    """\n    def _initial_population(self):\n        return list(\'\'.join([choice(lowercase) for _ in range(5)]) for _ in range(50)) \n def _fitness(self, member):\n    return float(sum(member[i] == "clout"[i] for i in range(5)))\n\ndef _crossover(self, parent1, parent2):\n    partition = randint(0, len(self.population[0]) - 1)\n    return parent1[0:partition] + parent2[partition:]\n\ndef _mutate(self, member):\n    if self.mutation_rate >= random():\n        member = list(member)\n        member[randint(0,4)] = choice(lowercase)\n        member = \'\'.join(member)\n    return member\n \n def test_algorithm():\n    algorithm = Algorithm(.5, .7, 500, max_fitness=None)\n    best_solution, best_objective_value = algorithm.run() \n ``` \n \n Testing \n To run tests, look in the  tests  folder.  \n Use  pytest ; it should automatically find the test files.  \n \n Contributing \n Feel free to send a pull request if you want to add any features or if you find a bug. \n Check the issues tab for some potential things to do.']
thefunkjunky,['', 'thinkful-projects', 'Abuse_queue_SpamFilter \n This is designed to identify spam tickets in our spam/abuse queue \n at the datacenter I work at.  The difficulty in using traditional\n spam filtering methods is that they will always select legitimate abuse tickets,\n because they always contain the full, original spam text in the body. \n The task is further complicated by the fact that I am not given access to the support api, so I\n used Selenium to automate the actual browser (not ideal, but it works). \n This script only works (well, worked) on the particular abuse queue at my work, \n I leave it here as a reference for my methods for future Selenium projects.\n It no longer functions at work, as management found out about it and promptly changed the api\n and ticketing web interface to break the script.   \n When it was working, it consistently selected NO false positives, and missed NO false negatives.\n It saved approximately 2 hours of work that was typically assigned to system admins, who are too\n valuable to be wasting time on such a simple and tedius task.', 'Quick-N-Dirty', '', 'dnsgetarecords \n General Information \n Name: dnsgetarecords \n Version: v0.1 \nRelease: Review \nAuthor: J Garrett Anderson \nDescription:  a Python command-line utility that accepts a domain URL, \nand returns a list of DNS A-record IPv4 addresses for the domain \n usage:    \n \n Install .egg via easy_install  \n dnsgetarecords [-h] [-t TIMEOUT] [-m MAXITEMS] [-v] domain \n \n positional arguments: \n* domain: Target domain, from which A-Record IP addresses will be returned. \n optional arguments: \n  * -h, --help            show this help message and exit\n  * -t TIMEOUT, --timeout TIMEOUT\n                        Sets DNS query timeout length in seconds. Default =\n                        5.0 s.\n  * -m MAXITEMS, --maxitems MAXITEMS\n                        Sets maximum number of IPs to be returned. Default =\n                        3.\n  * -v, --verbose         Toggles Human-Readable format. Default is False, which\n                        returns IPs formatted as a Python list.', 'eLect \n eLect - an online elections platform \n (Note: This is a project I started years ago to teach myself how to program, \nand is currently still a work in progress.)   \n One only has to look at some grim statistics to know that the state of democracy is failing on almost every front,\nincluding the act of voting itself.  Voter turnout is abysmal, whatever voters do turn out are routinely disenfranchised by discriminatory and arcane laws, inadequate polling stations and resources, and easily hackable voting machines and election software,  with little to no accountability in regards to the software, hardware, and source code, or verifiable paper trails for the results. \n This software will attempt to solve a few problems: the problem of voter turnout and ease of registration, the problem of voter identification, and the problem of security and accountability for online election systems.  The proposed solutions will rely on the power and transparency of distributed ledgers, or blockchain technology. It will also attempt to unify election systems under an easy-to-use and secure system, both for creating and managing elections, but of also of voter registration, user participation and communication/community building, and clean, simple, and effective UX design.  You know, someday.   \n In the meantime, the project can be cloned and run in its current state by first cloning the repository: \n git clone git@github.com:thefunkjunky/eLect.git  \n  or   \n git clone https://github.com/thefunkjunky/eLect.git \n You will need PostgreSQL 9.4+ (preferably 9.5) installed, set up, and running, and two databases created: one for the normal eLect project (ex. "eLect-db"), and a test database with the same name with "-test" appended (ex. "eLect-db-test").  See online documentation for instructions on how to do this on your system.   \n create a python3 virtual environment in the project folder \n python3 -m venv env \n change source to virtual environment \n source env/bin/activate \n pip install requirements (make sure you have the python libs and compilers you need to make and install python3 packages) \n pip install -r requirements.txt \n set up database access via wizard: \n python config_wizard.py \n populate database (default database used : *-test): \n python run.py seed_db \n run flask server/project \n python run.py run \n run tests: \n nosetests   \n More info to come as project is fleshed out.', '', 'Code SF Brigade Suggestion Box \n (c) Garrett Anderson \n j.garrett.anderson@gmail.com', '', 'SublimeText-ToThird', 'gs-dotfiles \n My configuration scripts for newly installed systems, adapted from several sources: \n https://github.com/Juxtaposedwords/dotfiles \nhttps://github.com/twolfson/sexy-bash-prompt \nhttps://github.com/mrzool/bash-sensible \nhttps://github.com/amix/vimrc (basic version)   \n NOTE: This is still a work in progress.  Some things need to be deprecated, removed, modified, and added.  Use at your own risk. \n Usage \n Must be run as sudo in project root dir: \n $ chmod +x install.sh\n$ sudo ./install.sh', 'bishop-gobot \n Go/Baduk AI based on AlphaGo', 'shoreline_devops_takehome \n Take home project for Shoreline devops position. \n Iterates through a list of base firewall rules and applies them\nto multiple providers.  Supports GCP Firewalls and AWS Networking Firewalls. \n Installation/Execution \n \n Create Python3 virtual environment\n```bash \n \n python3 -m venv venv \n 2. Source virtual environment bash \n source venv/bin/activate \n 3. Install dependencies bash\n(venv)# pip install -r requirements.txt \n 4. Execute script against firewall YAML config bash\n(venv)# python apply_firewalls.py firewalls.yaml\n``` \n Setup \n Before the script can be run, the firewall rules YAML and your provider environments must be configured. \n Firewall config YAML \n The firewalls YAML contains both the firewalls rules and relevant provider configurations.  Example:\n```yaml\nfirewall_rules:\n  firewall001:\n    action: allow\n    cidrs:\n      - 10.1.0.0/21\n      - 192.168.0.0/23\n    protocol_ports:\n      tcp:\n        - "22"\n        - "80"\n        - "443-8080"\n      udp:\n        - "5555"\n        - "90-180"\n    providers:\n      gcloud:\n        networks:\n          temp-interview-vpc:\n            name: firewall-001\n            description: "A test firewall rule"\n            priority: 1000\n            target_service_accounts: []\n            target_tags:\n              - test-firewall \n      aws:\n        network_firewall:\n          rule_group: firewall-test-group\n          priority_start: 100\n          priority_jump: 100\n          add_to_capacity: 10 \n additional_provider_configs:\n  aws:\n    policies:\n      firewall-test-policy:\n        rule_groups:\n          firewall-test-group:\n            priority: 100\n    firewalls:\n      test-firewall:\n        vpc_id: vpc-00000000000000\n        subnet_ids:\n          - subnet-00000000000000000\n          - subnet-058d0000000000000\n        policy: firewall-test-policy \n ``` \n The  firewall_rules  section is a hash map of each ruleset with a unique name as the key, followed by the following parameters:\n- action: Either "allow" or "deny"\n- cidrs: A list of CIDR blocks\n- protocol_ports: A map of protocols (tcp and udp) to ports (strings representing single integers, or a range of integers.)\n- providers: which providers to apply the firewall rules to, along with their relative configurations. "gcloud" and "aws" currently supported. More information is available below. \n Due to the limitations of this structure, configurations for resources that must be decoupled from each individual firewall rule are located separately in the  additional_provider_configs  section.  More information is provided below. \n Provider configuration \n Google Cloud \n To use Google Cloud as a provider, credentials and a project must be configured.  The script first checks to see if these are configured on a local  gcloud  cli installation.  If no gcloud configuration is found, it will then attempt to locate credentials using the Application Default Credentials. This is typically set up by creating a service account with the necessary roles, exporting the service account key to the target machine, and setting the  GOOGLE_APPLICATION_CREDENTIALS  environment variable to the key file location. See https://cloud.google.com/docs/authentication/production \n An example of the firewall configuration for the gcloud provider is provided above. The keys under  networks  represents a gcloud vpc network name, and the values set the firewall rules.  Of these, only  name  is required. \n Because of good API/library design and a relatively flat structure for firewalls, it is possible to configure most available firewall options by simply adding the desired parameters to the YAML provider map.  All of these key:value pairs will be unpacked and loaded as-is into the client library object.  For a list of available fields, see https://googleapis.dev/python/compute/0.5.0/compute_v1/types.html#google.cloud.compute_v1.types.Firewall. \n AWS \n AWS providers utilize the active profile ( AWS_PROFILE= ) and the associated config/credentials in the  ~/.aws  directory. See https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html. \n The AWS provider currently offers limited compatibility with AWS Network Firewalls.  Plans to additionally support Security Groups, another type of firewall, were abandoned due to time constraints. \n Due to the fact that AWS Networking Firewalls utilize several different components that often have one-to-many relationships with each other, configurations are limited and some had to be decoupled from the firewall rules into their own data structure, located in  additional_provider_configs .  For the current implementation, the provider config on each firewall ruleset is used to generate or update AWS Networking Firewall rule groups, and policies and firewalls are configured in the  additional_provider_configs .  See YAML above for an example of these configs. \n Documentation regarding these resources can be found here: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/network-firewall.html\nhttps://docs.aws.amazon.com/network-firewall/latest/developerguide/what-is-aws-network-firewall.html \n Discussion \n This project highlighted the difficulties in designing data structures that can be used across multiple platforms.  Ultimately the core structure needs to be relatively simple to ensure compatibility between all platforms, but it does so at the cost of limiting the unique features of each platform that can be utilized. \n \n \n "How do you map the network rules to each platform i.e. how are the rules applied on each platform and to what types of resources on that platform?" \n \n \n GCloud : Each firewall is associated with a vpc network. All defined protocols and rules are included in a single stateful firewall object.  By default the firewall will be applied to all GCE, GKE, app engine flex nodes, instance groups, or other resources that live on the network.  Source/Destination targets can easily, flexibly, and dynamically be assigned via the use of tagging and/or service accounts.  These rules cannot be used for serverless or managed resources that do not utilize VPC networking, nor does it offer the ability to manage inheritable firewall policies that span across an organization. \n \n \n AWS : There are multiple firewall products that provide some differing and overlapping functionality. The two main products used for this are Security Groups, and VPC Networking Firewalls.  I only had time to implement the latter. The Networking Firewall provides broad protections at the point of a VPC\'s subnets in each availability zone, where it lives. By default it will apply to every resource on the VPC, including EC2 instances, SQL database nodes, EKS, etc (no serverless). Once created, routing tables must be updated to point to the firewall on each subnet, which I do not believe are handled by the APIs used in this script. \n \n \n AWS : Firewalls are associated with a firewall policy, which can be shared between multiple firewalls.  Each policy, in turn, is associated with a number of firewall rule groups.  Each rule group can be either stateful or stateless (only stateless is supported here), and offers a number of options for routing traffic.  The script creates or updates rule groups per firewall rule.  Rule groups are then assigned to policies, which have to be created and managed separately from the firewall rules.  Firewalls themselves also have to be managed separately, and are assigned a policy, which applies all of its rule groups based on their priority settings. \n \n \n "What are the differences between the platforms from a networking perspective? How does this impact your ability to create an abstraction across the platforms?": \n \n \n Gcloud: Much better designed and programmatic.  Firewall rules are consolidated into a single object with relatively flat design, which means that configuring it doesn\'t require the management of multiple separate resources. However, this also means that rules can\'t be independently managed and shared across multiple firewalls without the use of inheritable policies, which this script doesn\'t support. Firewall rules can be dynamically filtered and routed to a fine degree of precision via the use of tagging or service accounts, which are very intuitive and easy to manage. Routing tables are unnecessary to manage, as the firewall doesn\'t live on any subnets.  It filters by subnet simply as another field of filtering rules, which can be updated later or discarded entirely. The firewall is stateful by default, which means that response traffic for outgoing requests are automatically allowed through without the need for creating custom deep-packet inspection rules. I could go on. \n \n \n AWS: Networking Firewalls are much more hands-on, which makes setup and maintenance a pain. Offers more flexibility over decoupled resources that can be shared with other resources, and independently managed. Offers a lot more options for defining rulesets from a traditional networking admin perspective, but less flexible in terms of targeting specific resources without the need for complex stateful deep-packet inspection rules. Good for those who desire a traditional approach to networking, not great for those who value ease of automation and programmatic control. Firewall is locked to the subnets it was created on and cannot be changed. Routing tables have to be updated to route traffic to the firewalls. Separate resources with one-to-many relationships prevent the ability to use a data structure that starts with the firewall rules and moves outward in terms of resource definitions, as configuring these resources for each rule will clobber and conflict with configurations for the same resources elsewhere. I could go on. \n \n \n The takeaway here is that the more similar the products are, the more overlap that can be abstracted.  Differences between them require different logic to handle, which  limits the ability to effectively leverage their unique features and best-practices. \n \n \n "How fine grained do you provide control over network flow? How would you go about extending this for finer grain control?  Per instance? Per group? Per network? " \n \n \n Both platforms support egress rules, which are unsupported by this script. \n \n Both platforms support more protocols than this script allows for. \n GCP: Supports fine grained control over sources and destinations via tagging and service accounts. These tags can be applied from single instances up to entire groups. Rules are limited to a single network, but can be applied to inheritable policies at both the project and organization level (not supported here). \n GCP: Script offers broad coverage over the API, so many features can be used. \n AWS: Offers far more options for stateless and stateful rules than this script is designed to deal with. Fine grained control per instance/group is primarily handled with clunky CIDR definitions, which should generally be avoided due to their static and opaque nature.  Stateful rules can be used to filter traffic based on domains, which is not supported in the script. For finer grained control at the instance, group, or load balancer level, it is highly advised to use security groups instead.  These are not only easier to use, but they also offer more dynamic control over sources and targets, as firewall rules can be applied to other security groups instead of static IP addresses. \n AWS: Custom actions are supported on the platform, but not this script. \n AWS: Firewall rules can be shared with other firewalls across multiple networks in a given project due to the decoupled nature of rule groups and policies.  Applying a single policy or rule group to multiple firewalls across different networks is not only possible, but allows for one to update all of them simultaneously by modifying a single rule group or policy.  However, there is no clear way to apply firewall policies across multiple accounts in an organization, or even different AWS regions in a single account. \n There\'s honestly way too much to list on this subject, so I\'ll stop here. \n \n Final Thoughts \n Ultimately, I think the entire approach here is ill-advised.  It is difficult to implement, and cannot fully leverage each platform\'s unique strengths and best practices.  Furthermore, it is dangerously close to running afoul of IaC principles, which are paramount in infrastructure design and implementation principles. \n Instead, I would recommend using Terraform to define and manage the infrastructure.  One could create a data struture that contains similar parameters as the firewall object in this code, which can then be consumed by modules that manage the firewall resources for each different provider. That way you are still gleaning the benefits of abstracted data, but with better support for the different platforms and a sound IaC implementation, without the need to write all the API logic yourself. \n Author \n \n Garrett Anderson  garrett@devnull.rip \n', 'GCloud/Terraform "Hello World" \n Take-home Gcloud devops assignment. \n Requirements \n The provided instructions were short on explicit instructions, leaving much to interpretation.  The basic stated requirements are as follows: \n \n Create web app that returns the message "Hello World" from a database. \n App should be publicly accessible and highly scalable. \n Application and associated infrastructure must be deployed using IaC (Terraform). \n Include monitoring for observability. \n Additional services may be added as needed. \n \n Options \n The following were options I considered for the implementation details: \n Language(s) \n Just about any language with a web framework will work here. There aren\'t any usage requirements that demand special features of any language in particular, so I opted for Python3, the language I\'m most familiar with.  It also has a long history of support on Google Cloud runtimes and client libraries, so it is reasonable to assume it has wide product and feature coverage. \n The downside to Python is that it is slower than other languages, and requires larger container runtime sizes due to its requirements for an OS.  Golang would be a good alternative choice, as it is not only fast and lightweight, but provides a good web framework and other libraries natively. But, I don\'t know it, and the benefits aren\'t worth the time spent trying to learn it here.  \n App Infrastructure \n I had the following potential strategies in mind: \n \n \n Static website hosted on multi-region GCS bucket. Use cloud load balancer to route https traffic to the static content, cloud CDN to cache the content globally, and cloud monitoring to observe traffic and errors.  The "Hello World" message doesn\'t change according to the description, so hosting it statically would be the simplest option, and you get some nice features with the LB and CDN.  However, categorizing object store as a "database", while technically true, doesn\'t seem to be in the spirit of this exercise.  It also doesn\'t offer any future dynamism.  Pass. \n \n \n Flask/NGINX web application run on GCE instances, autoscaled in instance groups, using cloud load balancing.  This requires more setup as there\'s a whole OS and VM to deal with, which have to be configured via configuration management, or I have to build and deploy an image. Scaling is slow and based on simpler metrics like CPU and Memory. Networking is something I have to manage. Not as many metrics or logs available, unless I roll my own solutions.  Upside is that I have more control over my infrastructure, although nothing in the requirements demands this level of customization.  A lot of work for less performance and features, I\'ll pass. \n \n \n Build Flask app in a docker container and deploy it to Cloud Run.  This is one of Google\'s serverless options that will run docker containers.  Might be a good option, but I\'m unfamiliar with it, and learning it is likely to be a time sink. \n \n \n Google App Engine (standard/flex). Google Cloud\'s oldest product, this is their flagship web application platform.  Standard is designed to use extremeley light runtime environments for your code on automatically scaled workers.  It is extremeley fast and designed for large and unpredictable scaling needs.  It can also scale down to 0 workers, making it affordable for me.  It comes pre-baked with log aggregation (cloud logging), lots of useful metrics (cloud monitoring), cloud trace, cloud debugger, a NoSQL document store, caching, and a lot of other features.  Flex engine allows one to easily deploy their own images and runtimes, but is designed for more steady usage patterns.  I don\'t need anything that isn\'t already offered in the standard runtime, so I\'m leaning toward that.  It seems like the most features offered, and the most production ready, with the smallest amount of effort required.  The downside is that it is a heavily managed service, offering less control over the backend.  It is also very unique, requiring more learning investment, and is less portable. \n \n \n Flask app on GKE - The heaviest weight option here. Great for managing a complex array of services on scaled infrastructure.  Offers all kinds of abilities and services, but a complex endeavor to manage. I believe GCloud offers a package that helps it integrate into cloud monitoring and logging, although one can also run Prometheus, Grafana, etc.  Requires container images to be built and deployed via cloud build/container registry. There\'s only really one simple service that I\'m running, and getting this to production readiness requires a lot more time and moving parts.  I believe that this is overkill for such a simple app, and the extra moving parts just means more things that can go wrong. Also, it\'s easy to accidentally incur costs using it, since it requires running nodes.  However, I have a nagging suspicion that this may be the better option, since it is more portable, better supported in the community, and offers more control. \n \n \n Believing that the pre-baked monitoring features of GAE, along with the excellent scaling abilities, request tracing, and simple app requirements, to be the best value/time ratio, I decide on App Engine Standard (python39).  In hindsight, this may have been a mistake.  The "auto-magic" nature of its execution, along with the weird rules it enforces, makes it extremely unkind to Terraform. More details are provided below. \n Database \n The DB only really needs to be written to once, and must be production hardened for a demanding read load.  The following db solutions were considered: \n \n \n Object storage (GCS) - blob file store for static web content.  Offers multi-region storage, object versioning, and global CDN caching abilities.  Since there is only one static message to be read, this could be a good option, but it limits the ability to create dynamic content down the line. \n \n \n SQL Cluster - one write and multiple read heads.  SQL dbs, like postgres, offer strong consistency and complex relational relationships between entities.  They are not designed for the scaling and HA needs of modern distributed web architecture, and should be avoided unless strong consistency, relational data, or complex queries are needed.  In this case I only have one point of data to write once and read a million more, so this doesn\'t seem like an ideal option. \n \n \n NoSQL - NoSQL dbs leverage non-relational data and eventual consistency so that they can be automatically distributed and scaled for high read/write demands and high availability. There are different kinds of NoSQL dbs available for different types of data and usage patterns.  The simplest would be a key/value store, which is all that is needed for this example.  Bigtable, Redis, and Consul are examples that could function here.  However, I decided to use the google-managed Firestore db, which is an advanced, strongly consistent NoSQL document store. It offers more flexibility in the data it can handle (although it does key:value just fine), has a serverless design, is highly scalable, and is less prone to creating hot spots on the backend.  It also has great python client library support, and integrates with App Engine nicely. \n \n \n Implementation \n The State Backend \n First thing\'s first, I need to create a state backend for the Terraform, and create the project resource.  In my experience, it is important to keep this separate from other parts of the terraform, so they were given their own folder,  00-backend . Placing it together with other resources creates issues when trying to tear down other environments and configurations, since it will also try to tear down the bucket it is storing the state in. \n This was my first time using Terraform with GCP, and there were some challenges here. Notably, there\'s a chicken and egg problem when using a GCS state backend.  The Terraform needs to create the project the bucket will reside in first, without using backend state. Then the user needs to set their gcloud project to the new project, authenticate using their default application credentials, create the state bucket, then update the config to use the backend bucket, and export the local state to the remote backend.  I attempted to write a  bootstrap.sh  script to handle all of this. \n First set the variables in  terraform.tfvars , and double check to make sure your org ID and billing account name are correct.  Then, execute  bootstrap.sh . Part of it will require you to authenticate your Google account, please do so. \n If you run into errors and need to start over, you will need to delete any remaining  *.tfstate*  files and the  .terraform  directory first. \n Ensure that it runs successfully and outputs the  project  resource.  This is required for the other Terraform resources.  They will also use the project_id set here, so you won\'t have to change it in multiple places. \n The GAE Application \n Before we get into the terraform for the application, we need to write the application itself.  The code for the Python application is located in  01-hello-world/app .  It is a simple Flask app that uses gunicorn WSGI for the http service/entrypoint. It checks Firestore to see if the greeting data is present, and if not, it writes the "Hello World!" message to the database, and attempts to read from it again.  Once present, it will display the contents of the greeting entity in the database on a simple webpage.  I confirmed that this worked locally before moving on to GAE. After confirming that it worked with GAE using the command-line instructions provided in the docs, I started a new project and attempted to implement this using terraform.  Then all of the problems started to happen. \n Terraform/GAE Problems \n Although there are terraform resources available for managing GAE and deploying the application code to new services/versions, in practice it is very error-prone. \n To start, there are rules for GAE that aren\'t conducive to Terraform IaC: \n1. There can only be one  google_app_engine_application  per project, and once created, it cannot be deleted (you have to delete the entire project).\n1. No service can be created without deploying the  default  service first, which also can\'t be deleted.\n1. The oldest version for each service also can\'t be removed, which terraform will try to do if certain changes are made to the version resource. \n I tried to get around this by creating a separate resource for the default service, and another resource for a user-managed  helloworld  service, which depends on the  default  service being created first.  In theory this should work, but I experienced a host of different issues and errors when running it for the first time.  Many of the API errors aren\'t clear about what the problem is, and I eventually figured out that the real error messages are hidden in Cloud Build and Cloud Logging for the "behind the scenes" build and deployment steps. \n Furthermore, most of the issues appear to either be transient in nature, and go away on subsequent runs of the terraform code, or require that the app be deployed once using the gcloud command line tool to "set up" whatever needs to be initialized on the backend. \n To deploy the GAE Terraform, execute the  bootstrap.sh  script in the  01-hello-world  directory.  This should be working now, but I can\'t run any more tests on it because I\'ve run out of my projects quota, so I have to present it as-is for now. \n If you see errors when applying the terraform here, try waiting 5-10 minutes after creation of the  google_app_engine_application.helloworld  resource and applying the terraform again.  If this doesn\'t work, try running:\n bash\ngcloud app deploy --version init \nin the  app  directory, which seems to fix many of the transient issues, and subsequent runs of the terraform should work. \n Once the terraform has been deployed, you may test the app by clicking on the "helloworld" service in your project\'s App Engine dashboard.  It\'s possible to assign the service a more idiomatic url based on a domain you control, but that is beyond the scope of this assignment. \n Monitoring \n The good news is that once the app has been deployed, a world of monitoring metrics are ready to go.  They can be viewed in the Cloud Monitoring tool, and provide access to common metrics like CPU and Memory usage (good for spotting load or memory leaks), but also other really useful metrics like http response codes, and request latency, which is probably the most useful of the bunch. \n There is also Cloud Trace, which tracks requests through the application and measures the latency at each hop and service, which helps identify bottlenecks.  There is cloud debugging for debugging purposes, and you can also set up memcache to reduce load on the DB and serve common requests quicker.  Cloud Logging aggregates the application and request/health check logs, providing a powerful search interface. Alerts can be set up for both metrics and logs.  There\'s a lot to work with here. \n Final thoughts \n Ultimately this is a project that can potentially go on ad infintum, constantly adding more features, monitoring, and production-hardening.  It also needs a CI/CD system for testing and deployments, which I did not have the time to implement here. \n However, when trying to implement everything at once, it becomes easy to get lost, resulting in a lot of work expended on premature optimization and personal rabbit-holes that lead to dead-ends.  It is better to start with simple solutions, and to gradually improve upon them in incremental steps. \n I wanted to save time and deliver a robust solution using pre-baked technology, but I ended up spending more time trying to fix obscure bugs.  Next time I would consider something with more direct control, like GCE or GKE, before migrating to a serverless option. \n Author \n Garrett Anderson  garrett@devnull.rip', "neustar-interview \n take-home interview for Neustar \n Runs a script on servers via Ansible, collects and parses a sample of log entries matching a certain pattern (in this case, a specific application's launch latency), and returns the mean and standard deviation of the launch latencies on each server. \n Installation/Requirements \n Requires Python 3 and Ansible to run.  Requires Vagrant and virtualbox to deploy the test environment. \n \n https://www.virtualbox.org/manual/ch02.html \n https://www.vagrantup.com/docs/installation \n \n It is recommended that you use a virtual environment for Python and Ansible:\n bash\n$ python3 -m venv venv\n$ source venv/bin/activate\n(venv)$ pip install ansible\n(venv)$ ...(whatever)\n(venv)$ deactivate\n$ \n Deploying the test environment \n Since the instructions presume this is being run against servers that don't actually exist, it was essential to devise a local test environment to mimick them.  In this case, the 5 servers are deployed as VMs locally on Virtualbox (or whatever virtualization platform you choose) via Vagrant and Ansible. \n Presuming you have your virtual environment activated and Ansible installed (see above), you may provision the the local test environment by navigating to the  vagrant  directory and running  vagrant up .\n bash\n(venv)$ vagrant up \n This will create 5 VMs configured with the  vagrant/playbook.yaml  ansible playbook, which seeds each VM with randomly generated logs using the  scripts/gen-log.py  script:\n```bash\n$ python3 gen-log.py --help\nusage: gen-log.py [-h] [--dest_log_file DEST_LOG_FILE]\n                  [--min_latency MIN_LATENCY] [--max_latency MAX_LATENCY]\n                  [--max_logs_range MAX_LOGS_RANGE]\n                  [--match_string MATCH_STRING]\n                  sample_log_path \n Randomly generates log files from a sample log. \n positional arguments:\n  sample_log_path       Location of sample log file. \n optional arguments:\n  -h, --help            show this help message and exit\n  --dest_log_file DEST_LOG_FILE\n                        Location of generated log file.\n  --min_latency MIN_LATENCY\n                        Minimum random latency.\n  --max_latency MAX_LATENCY\n                        Max random latency.\n  --max_logs_range MAX_LOGS_RANGE\n                        Max number of random match entry logs.\n  --match_string MATCH_STRING\n                        Log entries string to match.\n``` \n It will only do this once, so if you want to regenerate the logs on the VMs, run:\n bash\n(venv)$ ansible-playbook regenerate-random-logs-playbook.yaml -i .vagrant/provisioners/ansible/inventory/vagrant_ansible_inventory \n NOTE:  Vagrant will generate an Ansible inventory for the VMs at the  .vagrant/provisioners/ansible/inventory/vagrant_ansible_inventory  location.  You will need this when running Ansible playbooks on these servers outside of the Vagrant provisioner. \n SSH Login issues \n When running playbooks outside of the Vagrant Ansible provisioner, you may need the following added to your  ~/.ssh/config  to avoid ssh login problems:\n Host 127.0.0.1\n    IdentitiesOnly yes \n Destroy \n To destroy the test environment, go to  vagrant/  dir and: \n bash\n(venv)$ vagrant destroy -f \n Gathering application launch metrics \n To gather the application launch metrics from each server, head to the  metrics/  folder and run the appropriate playbook: \n Remote servers:\n bash\n(venv)$ ansible-playbook playbook-remote.yaml -i hosts.ini \n Local Vagrant VMS:\n bash\n(venv)$ ansible-playbook playbook-local.yaml -i ../vagrant/.vagrant/provisioners/ansible/inventory/vagrant_ansible_inventory \n The playbook will run the  scripts/ultradns-latency.py   script on each server, and return the results from  stdout :\n```bash\n$ python3 ultradns-latency.py --help\nusage: ultradns-latency.py [-h] [--match_string MATCH_STRING]\n                           [--sample_size SAMPLE_SIZE] [--json]\n                           log_path \n Parse log and return mean/stddev of application launch latencies. \n positional arguments:\n  log_path              Location of log file to parse. \n optional arguments:\n  -h, --help            show this help message and exit\n  --match_string MATCH_STRING\n                        Log entries string to match.\n  --sample_size SAMPLE_SIZE\n                        Number of most recent log entries to sample.\n  --json                Output mean and stddev as machine readable json.\n``` \n Design Notes \n Language \n I chose Python for several reasons: \n1. It's by far my strongest language.\n1. It already comes installed on most VM images.\n1. It doesn't have to be compiled.\n1. Other scripting languages are poorly designed, imo.  Bash isn't designed to be a true programming language and the syntax is incredibly arcane, and I have a personal distaste for Ruby and Javascript.\n1. Ansible is written in Python, which means the two scripts can be ported into Ansible modules down the line. \n Infrastructure \n Given that the assignment only refers to remote servers that don't actually exist, the primary infrastructure design considerations were for the local test environment.  This means my choices were down to two options: VMs or Containers. \n \n Containers are meant to be immutable, and only changed at the image build stage.  Although they can be used kind-of similarly as a VM, this isn't really what they are for.  This makes randomizing the logs for each instance tricky. \n Since VMs are mutable and configured after the base image is deployed, it makes generating random log files for each server much easier. \n The remote servers are not described as containers, and  are therefore better represented as full VMs. \n Vagrant not only makes configuring and deploying local VMs fairly easy, but it comes pre-baked with an Ansible provisioner that sets up everything you need to use Ansible automatically. \n \n So, Vagrant VMs configured with the Ansible provisioner are my choice for the local test environment. We can make-believe about the remote servers since they don't exist, and I'm not messing around with my /etc/hosts to route the URLs to something I'm already running locally. \n Config managment and script execution \n The assignment specified Ansible as the means by which this code is to be executed, so alternatives were not considered.  Ansible is good enough, we shall use Ansible. \n Issues \n I had the most problems with, and spent the most time on just trying to get the test enviornment working.  The code for setting up multiple VMs in Vagrant was challenging in Ruby because Vagrant evaluates outer contexts first, and using nested config structures resulted in strange behavior, with certain configs being ignored while others at the same context level being used.  Of particular note was setting up a private local network with unique IPs for each machine, which it seemed to ignore no matter what I did. \n Also, most of the code found on the internet averaged around 5 years old, and didn't work.  Vagrant and VMs are kind of old-hat in the industry I guess.  Ultimately I found that I didn't need to change most of the default configurations, so I stripped out everything that was unnecessary and let Vagrant do its thing, and it worked out fine. \n The biggest challenge was trying to get playbooks to work with the VMs outside of the Vagrant Ansible provisioner.  It took quite a while of research to figure out two key pieces of information: \n1. That the Vagrant Ansible provisioner generates its own inventory file  .vagrant/provisioners/ansible/inventory/vagrant_ansible_inventory , which includes the location of the individual ssh private keys that Vagrant generates for each host, along with each host's ssh port mapping.\n1. That I needed to add  IdentitiesOnly yes  to host  127.0.0.1  in  ~/.ssh/config . Otherwise, Ansible would try every identity file when connecting to each sever, then each server would block access due to too many invalid login attempts.  This ensures that only the specified identity and cert are used. \n Other approaches \n Although this is fine for demonstration purposes and a semi-ad hoc workflow, it isn't ideal. It's too manual, the information isn't being saved anywhere, and it isn't being presented in an ideal context. \n A better approach would use Ansible purely for the configuration management of the servers, and modify the script to be run as a regular cron job that returns the host and metrics information to an HA time-series nosql DB, like Cloudwatch Metrics, Google metrics, Influxdb, or Prometheus.  You could also use microservices to deduplicate entries and write/read them from a message queue first, to ensure higher reliability by decoupling the direct connection between the parser and the time-series db. From there, the metrics could be viewed from a metrics dashboard like Grafana, and automatic alerts configured through your pager system of choice. \n Author \n \n Garrett Anderson  garrett@devnull.rip \n", "resonance-interview \n Take-home interview for Resonance \n NOTE:  I attempted the first challenge to try to broaden my current K8 experience, but it became clear that I wasn't going to complete it in time.  I requested extra time to switch to Challenge 2, since I already had boilerplate Terraform to draw from.  However, I was still strapped for time, so I had to make some concessions in the execution. \n Challenge 2 - Terraform/Cloud  \nSelect one of the 3 major cloud providers (AWS, Azure, GCP) which you feel comfortable using in Terraform. Don't worry about applying these configurations, a plan will suffice. \n Using Terraform create a 3-tiered system: \n \n a compute instance to hold a UI which is publicly available. \n a compute instance for an API which is not publicly available. \n a database instance which is not publicly available\nEnsure the UI instance has access to the API instance. The API and database are both in the same subnet and have access to one another. You can assume IAM policies are already in place for the instances but security groups are not. \n \n Key Concepts: \n \n Create a VPC \n Ensure there is a public subnet \n Ensure there is a private subnet \n Security groups are in place \n Create a UI compute instance \n Deploy in a public subnet \n Create an API compute instance \n Deployed in a private subnet but has access to communicate with UI \n Ensure the instance has access to the database \n Create a database \n Deploy this is in a private subnet \n \n Installation / destruction \n \n Install Terraform - https://www.terraform.io/downloads \n Configure AWS in  ~/.aws/config  and  ~/.aws/credentials  - https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html \n Configure parameters in  vars.tf . \n Initialize Terraform:\n   bash\n  terraform init \n Generate Terraform Plan:\n   bash\n  terraform plan \n Apply terraform:\n   bash\n  terraform apply \n To destroy:\n   bash\n  terraform destroy \n \n General overview of resources \n \n Creates a VPC with a public subnet in a single Availability zone, and 1 private subnet in a zone. The module used is located in the  modules  folder.  Because this creates only 1 instance of the UI, API, and RDS instances, I didn't need the other availability zones. More can be added by appending the appropriate hash maps in the variables.  \n Creates the networking resources required for communication between resources in the VPC.  Also allows for outgoing communication in the private subnet, so that instances inside can communicate outward and receive responses, but will not allow incoming public requests except for instances on the VPC. \n Creates a UI EC2 instance in the public subnet, with ports open on 443 and 80 to the public. \n Creates an API EC2 instance in the private subnet, which allows incoming requests from the UI server on port 443. \n Creates an RDS postgresql instance in the private subnet, which allows incoming requests from the API server. \n \n Configuration \n Configure the resources by modifying the variables in  vars.tf .  Normally I would list each one and describe what they are, and what data types they expect, but I'm already over time on this challenge.  Most of them should be fairly obvious by the name. \n Concessions \n I am way over time due to switching challenges, so I had remove some features I normally would use. \n \n Normally I would set up a remote state backend in S3 with DynamoDB state locking in a separate directory to prevent corruption by multiple users. This creates a single source of truth safely stored on AWS, and allows for multiple users to collaborate. \n \n An interesting solution to the chicken-and-egg problem of\nsetting up state backends in Terraform can be seen at\nhttps://github.com/thefunkjunky/gcloud-takehome/tree/main/00-backend.\nIt is configured for Google Cloud, but the idea is similar. \n \n \n I don't like secrets stored in plaintext, so I usually only set a temporary RDS password in the variables, create an ACM key, encrypt a new password, and then update the terraform to store the hashed password in a new variable, and to decrypt it. \n \n \n Authentication for the EC2 instances has not been configured. As such, they do not accept SSH connections.  Furthermore, I did not configure a VPN or bastion host to be able to connect to the private instances. \n \n \n Although I think the security groups are set up correctly, I don't have a way to actually check them.  Some modifications may be required. \n \n \n It's not clear to me whether the private instances have the correct networking in place to allow connections from the public subnets.  I have my doubts, but it may already be ok as-is. \n \n \n Botched 1st challenge \n My incomplete attempt at the first challenge can be seen on this branch:\nhttps://github.com/thefunkjunky/resonance-interview/tree/dev \n I'm sure with enough time I could have solved it, but I miscalculated how much I would need to complete it.  I wanted to challenge myself, which was a mistake given the circumstances. \n I would have loved to solve challenge 3, which was the most interesting of the bunch.  However, I knew right away that would take too long to complete. \n Authors \n \n Garrett Anderson  garrett@devnull.rip \n", 'funny_names \n Funny name generator', 'DevOps Practical Exercise - AWS \n Primary Task: \n Create a very simple REST API that\xa0when its endpoint is called, it returns one object from S3 that is a JSON file.\xa0 For example\xa0GET\xa0 /api/foo \xa0returns contents of JSON file with something like\xa0 { "greeting": "I am the Foo" } . But really do whatever you want there as long as it accesses S3 to get the content.\xa0 You can create this in any development language with any framework, or you can use a sample app from somewhere online (note your source if you do so).\xa0 If you need help with this, let me know.  Keep this simple, its a 10-20 line app at most. \n \n Use infrastructure as code (i.e. Terraform, Pulumi, or other) \n You may use containers or instances for the application backend, or make a case for other options \n Deploy the backend to VPC / private network \n Create your own S3 bucket for the JSON files \n Make the service autoscale \n Expose the service through a termination on publicly accessible network \n \n Bootstrapping instructions \n \n Install the latest terraform, docker, and awscli applications to your local machine. \n configure aws access credentials in ~/.aws per https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html \n Edit all  config.tf  and  variables.tf  files in each folder under  global  and  environments  directories to match the desired bucket names, regions, and other parameters for your environment.  realm  referrs to either the account name, or the application name.  It is used along with  company  to generate unique names for AWS resources.  The defaults should work once I clean up and destroy everything in my AWS . \n Copy  config.tf  to  config.tf.remote  under the  global/backend  folder. \n Execute  ./bootstrap.sh  under the  global/backend  folder. Copy the json bucket id from the output. \n Execute  ./bootstrap.sh  under the  global/ecr  folder.  Copy the  base_ecr_url  from the outputs. \n cd to app/ \n Update  config.json  to the json bucket id above. \n Log in to your AWS console, set the region to where the ECR repo is located, and view the Elastic Container Registry dashboard.  Select the repo, and hit the "View Push Commands" button.  Follow the instructions to build and deploy the application container to the repo created above.  It should look something like this:\n  ```\n  aws ecr get-login-password --region   | docker login --username AWS --password-stdin  .dkr.ecr. .amazonaws.com \n \n docker build -t theoremone-interview . \n docker tag theoremone-interview  .dkr.ecr. .amazonaws.com/theoremone-interview:latest \n docker push  .dkr.ecr. .amazonaws.com/theoremone-interview:latest\n   ``\n1. cd to environments/prod and run ./bootstrap.sh .  This will launch all VPC, IAM, and ECS resources.  The ECS cluster will launch a service that runs the python application task.\n1. Copy the output value for lb_dns . Wait a few minutes, then run curl [lb_dns]/api/foo` to see json message. \n Design consideration \n I created a Python flask app for the application, since it is the language and framework I\'m most proficient in. \n All infrastructure is managed by Terraform, the most robust platform for IaC.  Also the one I\'m most proficient at. The terraform state is hosted on an S3 backend, with state locking provided by DynamoDB tables. The state backend bucket is managed in Terraform, and its  bootstrap.sh  script mitigates the chicken-and-egg problem. \n The primary options for hosting the app would be EC2, Lambda, ECS, or EKS.  EC2 is too heavyweight for this kind of app, and there is no benefit to using a full linux OS.  Furthermore, it would require configuration management of some kind, which not only complicates things, but introduces problems due to its imperative nature and mutability. Lambda would be ok, but that uses too much "automagic" on the backend, without the ability to designate a VPC subnet to host it in. \n A lightweight, immutable container would be ideal for this kind of app, so it raises the question of which container scheduler to use: ECS, or EKS.  EKS is the heavyweight option, with a ton of features and the benefit of portability.  However, it is more complex to operate, especially in Terraform. I also feel its implementation on AWS leaves something to be desired, in terms of features and ease of use.  ECS is a more established product on AWS, and comes with the benefit of simple (or sometimes automatic) integration with AWS\'s Cloudwatch/Logging, load balancers, ECR, autoscaling, IAM, etc. \n Therefore, the app would be containerized using Docker, uploaded to an AWS ECR repository, and scheduled via ECS. \n The best structure for the Terraform that I have found is to separate globally shared resources such as the state backend and ECR repository, from environment specific resources such as the VPC and ECS.  This ensures that upstream dependencies are handled first, with no cyclic dependency issues.  Furthermore, it allows one to destroy and create environments without affecting the state backend or ECR repositories. \n Due to a tricky problem I encountered, I moved the ECS services to the public subnet for troubleshooting purposes.  The issue resolution had nothing to do with this, but I was told it was fine to leave it public.  Therefore I will leave it, but it is not difficult to move it back to private if need be. \n Although not specified in the requirements, I set up application logging in Cloudwatch.  ECS automatically set up metrics and alarms, including resource under-utilization, which is nice. \n Autoscaling is set to 80% of cpu or memory. \n Nice to have \n Things that should be in place, but time constraints prevent me from implementing it:\n- Python app should be using production hardened Gunicorn WSGI instead of the default.\n- Python app should only output logs in structured JSON, not plaintext.\n- ECS should be moved back to private, although current security groups should prevent any mischief.\n- Static, hard coded values for Terraform state backend should be auto-generated using Python Mako templates, not changed manually.  It is not possible to use variables here.\n- Bootstrapping, docker build and push, and terraform plan/apply should be automated using Python script, and set in a CI/CD pipeline.\n- Needs a CI/CD pipeline.\n- Prod should not be automatically pulling "latest" tagged docker images, and should be use something more specific (like ":prod") to allow for control over deployments in multiple environments.\n- Needs tests for both python app and terraform code, which should be part of the CI/CD pipeline.\n- JSON data used in this context would be better suited for a NoSQL key:value store, or a document db. \n Authors \n \n Garrett Anderson  garrett@devnull.rip \n', 'Terraform Realm Utility \n This utility script is used to configure realms (AWS accounts) and realm deployments ("environments"), generate terraform templates from the configs, bootstrap new realms and environments, and plan, apply, or destroy realms/environments in the correct order of operations. \n Video Demo \n In progress... \n Installation \n \n python3 -m venv venv \n source venv/bin/activate \n pip3 install -r requirements.txt \n \n Be sure to  source venv/bin/activate  before running the script \n deactivate  to exit the virtual environment. \n Basic usage \n Basic overview of script usage and arguments. \n NOTE:  Be sure to first set the correct AWS_PROFILE for the account being targeted. Configure profiles in your ~/.aws directory per https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html#cli-configure-files-where and set the AWS_PROFILE environment variable (i.e.  export AWS_PROFILE=mycompany-dev ).\n```\n└─▶ ./terraform-realm-utility.py -h\nusage: terraform-realm-utility.py [-h] [--modules_source MODULES_SOURCE]\n                                  [--modules_dest MODULES_DEST]\n                                  [--templates TEMPLATES]\n                                  [--action {bootstrap,plan,apply,destroy}]\n                                  [--skip_templates] [--target_env TARGET_ENV]\n                                  [--mako_modules MAKO_MODULES] [--dry_run]\n                                  realm_config destination \n Generate and/or execute new terraform environments. \n positional arguments:\n  realm_config          YAML config file for realm.\n  destination           Location of the root destination terraform config directory. \n optional arguments:\n  -h, --help            show this help message and exit\n  --modules_source MODULES_SOURCE\n                        The source modules folder. Default=\'../modules/\'\n  --modules_dest MODULES_DEST\n                        Copy source modules to this target directory.\n  --templates TEMPLATES\n                        Location of the templates directory. Default=templates\n  --action {bootstrap,plan,apply,destroy}\n                        Optional terraform action to take for realm.\n  --skip_templates      Skip template generation.\n  --target_env TARGET_ENV\n                        Only act on target environment in realm.\n  --mako_modules MAKO_MODULES\n                        Troubleshooting: generate Mako template modules in this\n                        directory.\n  --dry_run             Troubleshooting: displays work_order. Skips template generation and action scripts.\n``` \n Configuration \n Before going into configuration and operational details, it is important to understand the base organization of the terraform and key terms:\n-  [root]/  - The root directory for all Terraform configuration files.\n-  [root]/[realm]  - Realm imples an AWS account. Ex.: "development", "production".\n-  [root]/[realm]/global  - Resources that are global to a realm and/or shared amongst multiple environments, or shared across multiple accounts. For example, this contains the S3  backend/ , which stores terraform state for the realm and all of its environments.  Other examples would be ECR repositories, shared IAM resources, Route53 zones, etc.\n-  [root]/[realm]/[environment]  - Environment is a specific deployment of resources that can be separated from other environments under the same realm. This is where the majority of non-global, unshared resources go, such as EKS clusters. Ex.: "dev", "integration", "performance", "staging", "prod". \n Example:\n ../deployments/sandbox/lab \n Realm configs \n The realm configs are the source of truth for your realm configuration, including all realm environments.  These are merged with  defaults.yaml  in specific ways to produce a final configuration in memory. \n Configuration is broadly split into:\n-  globals  - variables and configs for  global  template resources, and/or shared variables required by all environment deployments.\n-  environments  - Configurations for environment deployments\n-  action_order  - Overrides the default order of operations for [bootstrap/plan/apply]/destroy actions. \n Example:\n mycompany-[realm].yaml \n```yaml\nglobals:\n  company: mycompany\n  realm: demo\n  region: us-west-2\n  required_terraform_version: ">= 0.13.5"\n  terraform_providers:\n    aws:\n      source: hashicorp/aws\n      version: "3.35.0"\n  zone_domains: ["demo.mycompany.com"] # optional\n  outside_sub_domains: # optional\n    lab.demo.mycompany.com:\n      root_domain: demo.mycompany.com # must be present in zone_domains\n      nameservers:\n        - whatever.aws.com\n        - whatever02.aws.com\n        - whatever03.aws.com\n        - whatever04.aws.com \n environments: \n Each object listed under environments will be templated into separate environments named after the key. \n lab:\n    # VPC configs\n    vpc_domain_name: lab.demo.mycompany.com\n    vpc_cidr: 10.1.0.0/16\n    vpc_private_cidr: 10.1.0.0/20\n    vpc_private_subnets:\n      private_az_a:\n        cidr: 10.1.0.0/24\n        az_suffix: a\n      private_az_b:\n        cidr: 10.1.1.0/24\n        az_suffix: b\n      private_az_c:\n        cidr: 10.1.2.0/24\n        az_suffix: c\n    vpc_public_cidr: 10.1.16.0/20\n    vpc_public_subnets:\n      public_az_a:\n        cidr: 10.1.16.0/24\n        az_suffix: a\n      public_az_b:\n        cidr: 10.1.17.0/24\n        az_suffix: b\n      public_az_c:\n        cidr: 10.1.18.0/24\n        az_suffix: c\n    vpc_tags: {} # If not setting a dict or list, either set it to its respective empty version, or omit the parameter entirely.\n    private_subnet_tags: {}\n    public_subnet_tags: {}\n    # DB Configs\n    db_storage_size: 100\n    db_max_allocated_storage: 1000\n    db_instance_class: db.m5.large\n    db_engine_version: "11.10"\n    db_publicly_accesible: "false"\n    db_skip_final_snapshot: "true"\n    db_backup_retention_period: 7\n    db_temp_password: TempPassword00!!\n    db_password_encrypted: null # This is a special case.  See below.\n    k8_clusters:\n    # For each object listed here, an EKS cluster will be created.\n      main:\n        version: "1.17"\n        node_groups:\n        # For each object listed here, an EKS Node Group will be created for the parent cluster.\n          default:\n            capacity_type: "ON_DEMAND"\n            public: "false"\n            instance_types: ["m5.large"]\n            min_nodes: 1\n            max_nodes: 20\n            desired_nodes: 20\n            vol_size: 50\n            vol_type: gp2\n            node_tags: {}\n            node_labels: {}\n      argo:\n        node_groups:\n          spot:\n            capacity_type: "SPOT"\n            public: "true"\n            instance_types:\n              - m5.xlarge\n              - m5n.xlarge\n              - m5d.xlarge\n              - m5dn.xlarge\n              - m5a.xlarge\n              - m4.xlarge\n            min_nodes: 1\n            max_nodes: 100\n            desired_nodes: 20\n            vol_size: 100\n            vol_type: gp2\n            node_tags:\n              k8s.io/cluster-autoscaler/node-template/label/lifecycle: Ec2Spot\n            node_labels:\n              lifecycle: Ec2Spot\n              aws.amazon.com/spot: "true" \n action_order:\n  apply:\n    - global/backend\n    - global/ecr\n    - global/route53-zones-acm\n    - global/route53-zones-acm/acm_validation\n    - environments # expanded to each environment name \n destroy:\n    - environments\n    - global/route53-zones-acm/acm_validation\n    - global/route53-zones-acm\n    - global/ecr\n``` \n Special configs \n Some parameters require special instructions to configure: \n \n outside_sub_domains  - This sets up subdomain NS records with a base zone domain configured in the realm.  For example, if the prod realm/account manages the  mycompany.com  domain, and you want to configure  dev.mycompany.com  for use in the dev account: \n Set up and apply  mycompany.com  in the  zone_domains  in the prod config. \n Set up  dev.mycompany.com  in the  zone_domains  in the dev config. \n Apply the terraform in the  mycompany/dev/global/route53-zones-acm  directory, and copy the NS records for the subdomain from the outputs. \n Then, update the prod config\'s  outside_sub_domains  with the following:\n      yaml\n     outside_sub_domains:\n         dev.mycompany.com:\n           root_domain: mycompany.com\n           nameservers:\n             - ns.aws.com\n             - ns02.aws.com\n             - ns03.aws.com\n             - ns04.aws.com \n     and apply prod. \n \n Eventually, once the ACM has been issued, re-apply dev\'s  route53-zones-acm , then  route53-zones-acm/acm_validation  to validate the ACM. \n \n \n db_password_encrypted  - This is the KMS encrypted db password.  Because the KMS key must be created before encrypting the password, this must be set to  null  when bootstrapping a new environment, and the db will be created with the  db_temp_password .  After bootstrapping is complete, run the following command to encrypt a new password:\n bash\n$ aws kms encrypt --output text --query CiphertextBlob --key-id "alias/${environment}-main" --plaintext $(echo -n "[password]" | base64) \n(replace [password] with password and ${environment} with environment).  Once you have the encrypted password, set  db_password_encrypted , re-generate the templates, and apply to update the RDS. \n \n \n Defaults \n To save effort, there are a number of default settings in  defaults.yaml  that are selectively merged with the realm configs:\n- defaults["globals"] - Merged with realm["globals"].\n- defaults["environment_defaults"] - Merged with each realm["environments"][env].\n- defaults["k8_defaults"] - Merged with each realm["environments"][env]["k8_clusters"][cluster].\n- defaults["node_group_defaults"] - Merged with each realm["environments"][env]["k8_clusters"][cluster]["node_groups"][node_group].\n- defaults["action_order"] - Merged with realm["action_order"]. \n However, it is important to note that this is not a simple Python dict merge (or  .update() ), as doing so with empty nested structures in the realm configs would blow out the desired defaults. The exact procedure for each merge operation can be found in the  merge_defaults()  function in the script. \n Here is an example of a shortened realm config that will be merged with default values:\n yaml\nglobals:\n  company: mycompany\n  realm: sandbox\n  zone_domains: ["sandbox.mycompany.com"]\nenvironments:\n  lab: {}\n  sandbox:\n    vpc_domain_name: sandbox.mycompany.com\n    public_subnet_tags:\n      test: tag\n    k8_clusters:\n      main:\n        node_groups:\n          default: {}\n      argo:\n        node_groups:\n          default: {}\n          spot:\n            capacity_type: "SPOT"\n            public: true\n            instance_types:\n              - m5.xlarge\n              - m5n.xlarge\n              - m5d.xlarge\n              - m5dn.xlarge\n              - m5a.xlarge\n              - m4.xlarge\n            min_nodes: 1\n            max_nodes: 100\n            desired_nodes: 20\n            vol_size: 100\n            vol_type: gp2\n            node_tags:\n              k8s.io/cluster-autoscaler/node-template/label/lifecycle: Ec2Spot\n            node_labels:\n              lifecycle: Ec2Spot\n              aws.amazon.com/spot: true \n action_order \n action_order  configures the default order of action script operations, so that dependencies are resolved and updated in the correct order. \n IMPORTANT NOTES:\n- All entries are appended to [root]/[realm]/\n-  apply  list specifies the bootstrap, plan, and apply actions.\n- Any item with the word  environments  will expand the realm config\'s "environments" config.  If there were "dev" and "prod" environments configured, that would resolve to [root]/[realm]/dev, and [root]/[realm]/prod.\n- For the destroy action, you must leave out the  global/backend , as Terraform still needs to store state if resources are being destroyed. If you wish to destroy the state bucket, you must do so manually.\n- When "--target_env" is used, the apply section remains the same so that higher dependencies in the globals are resolved and updated first before executing the target environment configs.  However, for a similar reason, the destroy list is reduced to just entries containing "environments", in order to prevent the script from destroying global resources that other environments depend on. \n Operation \n Details about how the script works \n Templates \n This script uses Python  Mako  templates to generate the Terraform configs, bash scripts, and other files necessary for each realm and environment.  These are located in the  templates  folder. \n Work Order \n Before generating any templates or taking any actions, a  work_order  is compiled in memory.  The  work_order  maps each subdirectory under  templates , expands environments, lists mako files to be templated and regular files to be copied, separately maps config variables from the realm configs for each subdirectory, and generates an action plan. \n The templates folder and file structure map to the structure generated by the script.\n-  global  - maps to [root]/[realm]/global\n-  deployment  - maps to each configured environment.  For example, [root]/[realm]/dev, and [root]/[realm]/perf. \n Any file with the extention .mako will be treated as a template, and the .mako extention will be removed.  Any other file will be copied as-is. \n Each subfolder must be included in  action_order  configuration for actions to apply to it. \n For  global  templates, realm_configs["globals"] is merged to root.  environments  and  action_order  keys are deleted, as they are not needed. \n For  deployment  templates, realm_configs["globals"] is merged to root.   deployment  is expanded to each environment in  environments , and realm_configs["environments"][env] is merged to root for each environment.  This means that global overrides can be configured here for each environment. Also, root["environment"] is set to  env , and  environments  and  action_order  are deleted. \n For troubleshooting purposes, you may include the  --dry_run  flag to the command to view the proposed work order without generating any templates or taking any actions. \n Best practices \n \n Avoid hard-coding values in resources. Place them in terraform variables, locals, outputs, etc, and refer to them using standard terraform syntax. \n Use modules wherever possible, and do not overly rely on templating to generate resource definitions. The eventual goal is to use modules for most of the terraform, and templates simply to set variables and bootstrap resources that cannot be modularized (like the S3 backend). \n Be careful when naming resources.  Avoid possible naming conflicts with other environments/resources, but also keep them generic enough to be applicable and consistent across other company/realm/environment configs. \n All dependencies should flow downward from globals to environments, and from parent directories to subdirectories.  Avoid creating circular dependencies from child to parent.  If necessary, use terraform  for_each  with  if  conditional that sets empty list or map if resource/output/config is not found.  See  templates/global/route53-zones-acm/acm_validation/locals.tf  as an example. \n To create new dynamic resources: \n create new nested maps in the realm config yaml. \n use a combination of mako  if ,  for , and terraform  for_each  syntax in new templates to check for maps.  This will generate template code in a  for  loop, and utilize terraform  for_each  to create 0..n resources without error (making the resources optional).  See  templates/deployment/eks.tf.mako  for an example. \n To utilize defaults, create a new defaults key for each dynamic level in  defaults.yaml , and update  merge_defaults()  in  terraform-realm-utility.py  to intelligently merge the defaults into each realm config layer. \n \n Troubleshooting \n If templates are causing issues, set  --mako_modules  to a troubleshooting folder. \n Now when templates are generated, the Python modules generated for each one are cached in this directory.  You may evaluate the code and compare to the traceback in order to troubleshoot any issues you are having. \n WARNING  - DO NOT CHECK IN THE MAKO MODULES DIRECTORY TO GIT! \n Actions \n The  --action  argument has the following options:\n-  bootstrap  - Bootstraps new realm/environments. Avoids state locking and other chicken/egg scenarios for new configs.\n-  plan  - Runs terraform plan on all configured directories.  Good for running pre-commit MR pipeline actions to prevent faulty configs from being checked in.\n-  apply  - Runs terraform apply on all configured directories.\n-  destroy  - Runs terraform destroy on all configured directories.   Will delete directories after destroying them. \n The script will exit if the  AWS_PROFILE  environment variable has not been set. \n The order in which these are executed on each directory is determined by the  action_order  configuration mentioned earlier.  \n The specified action executes a list of commands found in  defaults.yaml  -  default_actions[action] .   HOWEVER , the default actions can be overridden if an {action}.sh script is found in the directory.  See  templates/globals/backend/bootstrap.sh.mako  as an example. \n If  --target_env  is set, the  apply  actions will still execute the global resources first, as they hold resources each environment depends on.  However, it will only act on the target_env environment.  Counter to this, if the action is to  destroy  with  --target_env  set, it will skip the globals and ONLY destroy the target_env, as the globals are still required for other environments to function. \n Running any action other than  plan  will prompt the user for confirmation.   HOWEVER , the script will skip confirmation if it does not have a tty (terminal) attached.  This is designed for running the script from a pipeline, although it will also apply if using cli nohup, redirection, or pipes (|). \n NEW TERRAFORM DIRS  When creating a new terraform config folder with a new backend state (such as creating a new environment), the  bootstrap  action  MUST  be run before attempting  apply  or  destroy .  Keep this in mind when checking in new configs with a pipeline that automatically runs  apply .  First  bootstrap  the new configuration directory before merging it. \n BOOTSTRAP  - The  bootstrap  action is idempotent and can be run multiple times on the same realms/environments.  The primary difference between this and  apply  (in most cases) is that  bootstrap  skips the state locking check, which makes it more dangerous to use if other people are applying terraform to the same backends at the same time. \n bootstrap  for the global backend, however, is more complicated.  If the backend bucket already exists, it runs a normal  apply  action.  Otherwise, it goes through a complex procedure to bootstrap a realm for the first time. See templates/global/backend/bootstrap.sh.mako. \n IMPORTANT  DO NOT include  global/backend  in destroy actions, as the backend is still required to store state regardless if all of the other configs are destroyed.  Remove this manually if you must, but it is not recommended. \n To remove backend manually:\n1. Empty the backend bucket from the console/cli, including all past versions of any files.\n1. Delete the bucket.\n1. Delete the backend state DynamoDB table. \n General Troubleshooting \n Use the  --dry_run  flag with any other combination of arguments to display the  work_order , which contains information on what folders are being targeted, what templates sources are being used, which mako template files are being generated vs. which regular files are being copied, what the merged config variables are for each folder, and what actions will be taken.  Using this flag will skip template generation along with any actions. \n Authors \n \n Garrett Anderson  garrett@devnull.rip \n', 'SpectroCloud-interview \n Vagrant/ansible project to run "hello world" in a VM \n Requirements \n You must have Vagrant, Python3, pip, and ansible installed on your system. Use homebrew to install vagrant, python3, and pip if you are on macos.  Otherwise, follow the installation instructions for your OS. \n Installing ansible:\n $ pip3 install ansible \n Bootstrapping VM \n To bootstrap and provision the VM: \n \n cd to  vagrant  directory. \n Edit Vagrantfile line  config.vm.network \'s  host  to the local port you want to forward. Default is 8080. \n Run  vagrant up \n Once provisioned, check if the application is running by running  curl localhost:8080  (be sure to change the port number if you edited it). It should return a "Hello World" page. \n \n Design \n This is a simple Flask application that returns a static "Hello World" website.  It runs inside of the default VM for Vagrant.  Vagrant provisions the VM, using the Ansible provisioner to set up the guest system.  The system uses NGINX for the web server, and a number of Gunicorn workers to connect the NGINX server to the application.  Details can be found by reading the  Vagrantfile  and  playbook.yml  in the  vagrant  directory. \n Issues encountered \n \n \n The biggest problem I had was due to the Ansible provisioner. By default, it uses the local ansible on your system, but when installing apt resources, it would fail due to 404s on the ubuntu package resources.  When changing the provisioner to  ansible_local , it would install and run ansible on the guest machine, which worked.  However,  ansible_local  is incapable of copying files from the local system, so I was originally forced to use a vagrant  file  provisioner for copying files, which is terrible practice.  Eventually I figured out that the reason I was getting 404s was because the apt cache had to be updated first, which fixed the problem and allowed me to use the regular ansible provisioner. \n \n \n Another big issue I had was the gunincorn service failing due to an inability to read the application.sock socket required to connect nginx to the application.  Eventually I realized that the application.sock was set to the parent directory of where it should have been, and that I needed to ensure that all folders and files were owned by the  www-data  user and groups used by the service. \n \n \n Finally, after everything was provisioned and running correctly, testing showed that the nginx server was returning a default ngix page, not hello world.  Everything appeared to be set up correctly, which made troubleshooting it challenging.  After some time, I noticed that there was a  default  site configured in  /etc/nginx/sites-enabled/default . I tried deleting it, after which the site was working as expected. \n \n \n Authors \n \n Garrett Anderson  garrett@devnull.rip   \n', 'concatenate-word-docs \n Concatenates word docs into a single file \n Installation \n python3 -m venv venv\nsource venv/bin/activate\npip3 install -r requirements.txt \n Usage \n ```\n$ source venv/bin/activate \n $ python3 concatenate-word.py --help\nusage: concatenate-word.py [-h] [-f FILES [FILES ...]] output_file \n Concatenate word docs. \n positional arguments:\n  output_file           Concatenated output file. \n optional arguments:\n  -h, --help            show this help message and exit\n  -f FILES [FILES ...], --files FILES [FILES ...]\n                        Files to concatenate. \n $ deactivate\n``` \n Example:\n python3 concatenate-word.py output.docx -f test1.docx test2.docx']
tmitchel2,['Hyper \n A hypermedia api framework for building fully REST compliant internet scale web applications. \n Features \n \n Discoverability through hypertext aware media types. \n Transport protocol independence (Tcp, Websocket, etc). \n Transfer format independence (Json, Xml, Html, etc). \n Client / Server independence enabling internet scale versioning across multiple domains. \n Stateless communication (No server side session state) to enable internet level scaling. \n Caching for improved network efficiency. \n \n References \n \n http://roy.gbiv.com/untangled/2008/rest-apis-must-be-hypertext-driven \n \n Example \n Client \n using (var client = new HyperClient(Application.GetTypes()))\n{\n  // Get Api\n  var api = await client.Get<Api>(@"http://localhost:80");\n\n  // Login - Create a session object using HTTP POST\n  var session = await api.Sessions.Post(new Session { Username = "username", Password = "password" }, client);\n\n  // Logout - Delete the session object using HTTP DELETE\n  await session.Delete(client, HttpStatusCode.Unauthorized);\n}\n \n Server \n // Configuration\nvar config = new HyperHttpSelfHostConfiguration"http://localhost:80", false);\nconfig.AuthenticationCookieName = "hyper-auth";\nconfig.WebApiUserNamePasswordValidator = new HyperUserNamePasswordValidator();\nconfig.Routes.MapHttpRouteLowercase(\n    name: "DefaultApi",\n    routeTemplate: "{controller}/{id}",\n    defaults: new { id = RouteParameter.Optional, controller = "Root" });\n\nvar types = GetTypes();\nconfig.Formatters.Remove(config.Formatters.JsonFormatter);\nconfig.Formatters.Add(new HyperJsonMediaTypeFormatter(types));\nconfig.Formatters.Add(new HyperXmlMediaTypeFormatter(types));\nconfig.MessageHandlers.Add(new RestQueryParameterHandler());\nconfig.MessageHandlers.Add(new AuthenticationHandler(config));\nconfig.Filters.Add(new BasicAuthenticationAttribute(config));\nconfig.Services.Replace(typeof(ITraceWriter), new SimpleTracer());\n\n// Start server\nvar server = new HttpSelfHostServer(config);\nserver.OpenAsync().Wait();\nConsole.ReadKey();\n \n Server Model \n [HyperContract(Name = "api", MediaType = "application/vnd.hypertests.api", Version = "1.0.0.0")]\npublic class Api : IHyperEntity<Api>\n{\n    [HyperLink(Rel = "self")]\n    public HyperLink<Api> Self { get; set; }\n\n    [HyperMember(Name = "name")]\n    public string Name { get; set; }\n\n    [HyperMember(Name = "version")]\n    public Version Version { get; set; }\n\n    [HyperLink(Rel = "types")]\n    public HyperListLink<HyperType> Types { get; set; }\n\n    [HyperLink(Rel = "sessions")]\n    public HyperListLink<Session> Sessions { get; set; }\n\n    [HyperLink(Rel = "users")]\n    public HyperListLink<User> Users { get; set; }\n\n    [HyperLink(Rel = "messages")]\n    public HyperListLink<Message> Messages { get; set; }\n}\n \n Server Controller \n public class RootController : ApiController\n{\n    [AllowAnonymous]\n    public Api Get()\n    {\n        return new Api\n            {\n                Self = new HyperLink<Api>(ControllerContext.Request.RequestUri.ToString()),\n                Name = Assembly.GetExecutingAssembly().GetName().Name,\n                Version = Assembly.GetExecutingAssembly().GetName().Version,\n                Types = new HyperListLink<HyperType>(GetRoute("Type")),\n                Sessions = new HyperListLink<Session>(GetRoute("Session")),\n                Users = new HyperListLink<User>(GetRoute("User")),\n                Messages = new HyperListLink<Message>(GetRoute("Message"))\n            };\n    }\n\n    private string GetRoute(string controller, string id = null)\n    {\n        return new Uri(Request.RequestUri, Url.Route("DefaultApi", new { controller, id })).ToString();\n    }\n}\n \n Hypermedia Json \n Run the HyperTests project and navigate to http://localhost/?accept=application/json \n {\n    "name": "HyperTests",\n    "version": "0.0.1.0",\n    "self": {\n        "href": "http://localhost/?accept=application/json"\n    },\n    "types": {\n        "href": "http://localhost/type?accept=application/json"\n    },\n    "sessions": {\n        "href": "http://localhost/session?accept=application/json"\n    },\n    "users": {\n        "href": "http://localhost/user?accept=application/json"\n    },\n    "messages": {\n        "href": "http://localhost/message?accept=application/json"\n    }\n}\n \n Status \n Hyper is work in progress but development is active.  Questions on usage accepted.', "PaxosSharp \n A C# port / rewrite of Marco Primi's Paxos algorithm implementation.  Original available at http://libpaxos.sourceforge.net/ \n This implementation is still a work in progress.", 'dynamodb-lambda-autoscale \n Autoscale AWS DynamoDB using an AWS Lambda function \n \n 5 minute setup process \n Serverless design \n Flexible code over configuration style \n Autoscale table and global secondary indexes \n Autoscale multiple tables \n Autoscale by fixed settings \n Autoscale by provisioned capacity utilisation \n Autoscale by throttled event metrics \n Optimised for large spikes in usage and hotkey issues by incorporating throttled event metrics \n Optimised performance using concurrent queries \n RateLimitedDecrement as imposed by AWS \n Statistics via \'measured\' \n AWS credential configuration via \'dotenv\' \n Optimised lambda package via \'webpack\' \n ES7 code \n 100%  Flow  static type checking coverage \n \n Disclaimer \n Any reliance you place on dynamodb-lambda-autoscale is strictly at your own\nrisk. \n In no event will we be liable for any loss or damage including without\nlimitation, indirect or consequential loss or damage, or any loss or damage\nwhatsoever arising from loss of data or profits arising out of, or in\nconnection with, the use of this code. \n Getting started \n Note: dynamodb-lambda-autoscale uses  Flow  extensively for static type\nchecking, we highly recommend you use  Nuclide  when making modification to code /\nconfiguration.  Please see the respective websites for advantages / reasons. \n \n Build and package the code \n Fork the repo \n Clone your fork \n Create a new file in the root folder called \'config.env.production\' \n \n Put your AWS credentials into the file in the following format, only if you want to run a local test (not needed for lambda) \n javascript\nAWS_ACCESS_KEY_ID="###################"\nAWS_SECRET_ACCESS_KEY="###############" \n \n \n Update  Region.json  to match the region of your DynamoDB instance \n \n Run \'npm install\' \n Run \'npm run build\' \n Verify this has created a \'dist.zip\' file \n Optionally, run a local test by running \'npm run start\' \n \n Running on AWS Lambda \n \n Follow the steps in \'Running locally\' \n Create an AWS Policy and Role \n Create a policy called \'DynamoDBLambdaAutoscale\' \n \n Use the following content to give access to dynamoDB, cloudwatch and lambda logging \n javascript\n  {\n    "Version": "2012-10-17",\n    "Statement": [\n      {\n        "Action": [\n          "dynamodb:ListTables",\n          "dynamodb:DescribeTable",\n          "dynamodb:UpdateTable",\n          "cloudwatch:GetMetricStatistics",\n          "logs:CreateLogGroup",\n          "logs:CreateLogStream",\n          "logs:PutLogEvents"\n        ],\n        "Effect": "Allow",\n        "Resource": "*"\n      }\n    ]\n  } \n \n \n Create a role called \'DynamoDBLambdaAutoscale\' \n \n Attach the newly created policy to the role \n Create a AWS Lambda function \n Skip the pre defined functions step \n Set the name to \'DynamoDBLambdaAutoscale\' \n Set the runtime to \'Node.js 4.3\' \n Select upload a zip file and select \'dist.zip\' which you created earlier \n Set the handler to \'index.handler\' \n Set the Role to \'DynamoDBLambdaAutoscale\' \n Set the Memory to the lowest value initially but test different values at a later date to see how it affects performance \n Set the Timeout to approximately 5 seconds (higher or lower depending on the amount of tables you have and the selected memory setting) \n Once the function is created, attach a \'scheduled event\' event source and make it run every minute.  Event Sources > Add Event Source > Event Type = Cloudwatch Events - Schedule. Set the name to \'DynamoDBLambdaAutoscale\' and the schedule expression to \'rate(1 minute)\' \n \n Configuration \n The default setup in the  Provisioner.js  allows for a quick no touch setup.\nA breakdown of the configuration behaviour is as follows:\n- AWS region is set to \'us-east-1\' via  Region.json  configuration\n- Autoscales all tables and indexes\n- Autoscaling \'Strategy\' settings are defined in  DefaultProvisioner.json  and are as follows\n  - Separate \'Read\' and \'Write\' capacity adjustment strategies\n  - Separate asymmetric \'Increment\' and \'Decrement\' capacity adjustment strategies\n  - Read/Write provisioned capacity increased\n    - when capacity utilisation > 75% or throttled events in the last minute > 25\n    - by 3 + (0.7 * throttled events) units or by 30% + (0.7 * throttled events) of provisioned value or to 130% of the current consumed capacity, which ever is the greater\n    - with hard min/max limits of 1 and 100 respectively\n  - Read/Write provisioned capacity decreased\n    - when capacity utilisation < 30% AND\n    - when at least 60 minutes have passed since the last increment AND\n    - when at least 60 minutes have passed since the last decrement AND\n    - when the adjustment will be at least 5 units AND\n    - when we are allowed to utilise 1 of our 4 AWS enforced decrements\n    - to the consumed throughput value\n    - with hard min/max limits of 1 and 100 respectively \n Strategy Settings \n The strategy settings described above uses a simple schema which applies to both Read/Write and to\nboth the Increment/Decrement.  Using the options below many different strategies can be constructed:\n- ReadCapacity.Min : (Optional) Define a minimum allowed capacity, otherwise 1\n- ReadCapacity.Max : (Optional) Define a maximum allowed capacity, otherwise unlimited\n- ReadCapacity.Increment : (Optional) Defined an increment strategy\n- ReadCapacity.Increment.When : (Required) Define when capacity should be incremented\n- ReadCapacity.Increment.When.ThrottledEventsPerMinuteIsAbove : (Optional) Define a threshold at which throttled events trigger an increment\n- ReadCapacity.Increment.When.UtilisationIsAbovePercent : (Optional) Define a percentage utilisation upper threshold at which capacity is subject to recalculation\n- ReadCapacity.Increment.When.UtilisationIsBelowPercent : (Optional) Define a percentage utilisation lower threshold at which capacity is subject to recalculation, possible but non sensical for increments however.\n- ReadCapacity.Increment.When.AfterLastIncrementMinutes : (Optional) Define a grace period based off the previous increment in which capacity adjustments should not occur\n- ReadCapacity.Increment.When.AfterLastDecrementMinutes : (Optional) Define a grace period based off the previous decrement in which capacity adjustments should not occur\n- ReadCapacity.Increment.When.UnitAdjustmentGreaterThan : (Optional) Define a minimum unit adjustment so that only capacity adjustments of a certain size are allowed\n- ReadCapacity.Increment.By : (Optional) Define a \'relative\' value to change the capacity by\n- ReadCapacity.Increment.By.ConsumedPercent : (Optional) Define a \'relative\' percentage adjustment based on the current ConsumedCapacity\n- ReadCapacity.Increment.By.ProvisionedPercent : (Optional) Define a \'relative\' percentage adjustment based on the current ProvisionedCapacity\n- ReadCapacity.Increment.By.Units : (Optional) Define a \'relative\' unit adjustment\n- ReadCapacity.Increment.By.ThrottledEventsWithMultiplier : (Optional) Define a \'multiple\' of the throttled events in the last minute which are added to all other \'By\' unit adjustments\n- ReadCapacity.Increment.To : (Optional) Define an \'absolute\' value to change the capacity to\n- ReadCapacity.Increment.To.ConsumedPercent : (Optional) Define an \'absolute\' percentage adjustment based on the current ConsumedCapacity\n- ReadCapacity.Increment.To.ProvisionedPercent : (Optional) Define an \'absolute\' percentage adjustment based on the current ProvisionedCapacity\n- ReadCapacity.Increment.To.Units : (Optional) Define an \'absolute\' unit adjustment \n A sample of the strategy setting json is...\n javascript\n{\n  "ReadCapacity": {\n    "Min": 1,\n    "Max": 100,\n    "Increment": {\n      "When": {\n        "UtilisationIsAbovePercent": 75,\n        "ThrottledEventsPerMinuteIsAbove": 25\n      },\n      "By": {\n        "Units": 3,\n        "ProvisionedPercent": 30,\n        "ThrottledEventsWithMultiplier": 0.7\n      },\n      "To": {\n        "ConsumedPercent": 130\n      }\n    },\n    "Decrement": {\n      "When": {\n        "UtilisationIsBelowPercent": 30,\n        "AfterLastIncrementMinutes": 60,\n        "AfterLastDecrementMinutes": 60,\n        "UnitAdjustmentGreaterThan": 5\n      },\n      "To": {\n        "ConsumedPercent": 100\n      }\n    }\n  },\n  "WriteCapacity": {\n    "Min": 1,\n    "Max": 100,\n    "Increment": {\n      "When": {\n        "UtilisationIsAbovePercent": 75,\n        "ThrottledEventsPerMinuteIsAbove": 25\n      },\n      "By": {\n        "Units": 3,\n        "ProvisionedPercent": 30,\n        "ThrottledEventsWithMultiplier": 0.7\n      },\n      "To": {\n        "ConsumedPercent": 130\n      }\n    },\n    "Decrement": {\n      "When": {\n        "UtilisationIsBelowPercent": 30,\n        "AfterLastIncrementMinutes": 60,\n        "AfterLastDecrementMinutes": 60,\n        "UnitAdjustmentGreaterThan": 5\n      },\n      "To": {\n        "ConsumedPercent": 100\n      }\n    }\n  }\n} \n Advanced Configuration \n This project takes a \'React\' style code first approach over declarative configuration traditionally\nused by other autoscaling community projects.  Rather than being limited to a structured\nconfiguration file or even the \'strategy\' settings above you have the option to extend the  ProvisionerBase.js \nabstract base class for yourself and programmatically implement any desired logic. \n The following three functions are all that is required to complete the provisioning functionality. \nAs per the \'React\' style, only actual updates to the ProvisionedCapacity will be sent to AWS. \n ```javascript\ngetDynamoDBRegion(): string {\n  // Return the AWS region as a string\n} \n async getTableNamesAsync(): Promise  {\n  // Return the table names to apply autoscaling to as a string array promise\n} \n async getTableUpdateAsync(\n  tableDescription: TableDescription,\n  tableConsumedCapacityDescription: TableConsumedCapacityDescription):\n  Promise<?UpdateTableRequest> {\n  // Given an AWS DynamoDB TableDescription and AWS CloudWatch ConsumedCapacity metrics\n  // return an AWS DynamoDB UpdateTable request\n}\n```\n DescribeTable.ResponseSyntax \n UpdateTable.RequestSyntax \n Flexibility is great, but implementing all the logic required for a robust autoscaling\nstrategy isn\'t something everyone wants to do.  Hence, the default \'Provisioner\' builds upon the base\nclass in a layered approach.  The layers are as follows:\n-  Provisioner.js  concrete implementation which provides very robust autoscaling logic which can be manipulated with a \'strategy\' settings json object\n-  ProvisionerConfigurableBase.js  abstract base class which breaks out the \'getTableUpdateAsync\' function into more manageable abstract methods\n-  ProvisionerBase.js  the root abstract base class which defines the minimum contract \n Throttled Events \n Throttled events are now taken into account as part of the provisioning calculation.  A multiple of the events can be added to the existing calculation so that both large spikes in usage and hot key issues are both dealt with. \n Rate Limited Decrement \n AWS only allows 4 table decrements in a calendar day.  To account for this we have included\nan algorithm which segments the remaining time to midnight by the amount of decrements we have left.\nThis logic allows us to utilise each 4 decrements as efficiently as possible.  The increments on the\nother hand are unlimited, so the algorithm follows a unique \'sawtooth\' profile, dropping the\nprovisioned capacity all the way down to the consumed throughput rather than gradually.  Please see\n RateLimitedDecrement.js  for full implementation. \n Capacity Calculation \n As well as implementing the correct Provisioning logic it is also important to calculate the\nConsumedCapacity for the current point in time.  We have provided a default algorithm in\n CapacityCalculator.js  which should be good enough for most purposes\nbut it could be swapped out with perhaps an improved version.  The newer version could potentially\ntake a series of data points and plot a linear regression line through them for example. \n Dependencies \n This project has the following main dependencies (n.b. all third party dependencies are compiled\ninto a single javascript file before being zipped and uploaded to lambda):\n+ aws-sdk - Access to AWS services\n+ dotenv - Environment variable configuration useful for lambda\n+ measured - Statistics gathering \n Licensing \n The source code is licensed under the MIT license found in the\n LICENSE  file in the root directory of this source tree.', '']
edran,['\n Unimon! The battle for the lost lab! \n Inspired by the PokemonⒸ series, the OP AWESOME group presents "Unimon! The Battle For The Lost Lab!" \n Have you ever wanted to be able to make your own companions of adventures but never had the possibility because Nintendo never liked Open Source projects? Well, that\'s why we are here!  \n \n CREATE YOUR PERSONAL UNIMONS, ATTACKS AND ITEMS! \n Battle with your friends Online from wherever you want, whenever you want, tasting the joy of being able to make fun of them with a UBEROVERPOWERED!11!!!eleven! StackOverFlow against their beloved monsters! \n Game mode for FOREVERALONE™s avalaible in localhost! \n Bacon! \n MOAR BACON! \n \n Play your favorites professors (We are going to be good!) against your classmates and decide who will win the glory!\nSend new rules as pull requests and we\'ll try to implement them!\nMAKE SCHOOL-WIDE TOURNAMENTS! \n There are a lot of things that we are going to implement Soon™. Just wait for them and in the meanwhile...  \n \n CHALLENGE THE WHOLE WORLD! \n', '.emacs.d \n Personal emacs configuration. Currently very slow and potentially\nunstable. Use at your own risk. Meant to be used with Ubuntu 14.04;\ncheckout  precise  branch for 12.04 compatibility. \n Requirements \n \n xcape (for god-mode esc switch) \n \n LaTeX \n \n dvipng \n \n Python \n \n pip (for anaconda mode) \n virtualenv (for virtualenvwrapper) \n \n Installation \n git clone --recursive git@github.com/edran/emacs.d ~/.emacs.d \nor  git submodule update --init --recursive  if you have already\ncloned the repository. \n init.sh is used to disable caps lock and add Esc with xcape \n TODOs \n \n Fix themes import routine \n Refactor  everything  to use-package. \n Install elpy (python) \n Install writegood-mode for essay writing \n', 'xmonad \n My (bad) xmonad config', 'Mega Project List \n Numbers - 8/18 \n Find PI to the Nth Digit \n- Enter a number and have the program generate PI up to that many\ndecimal places. Keep a limit to how far the program will go. \n DONE - Fibonacci Sequence \n- Enter a number and have the program generate the Fibonacci sequence\nto that number or to the Nth number. \n DONE - Prime Factorization \n- Have the user enter a number and find all Prime Factors (if there\n  are any) and display them. \n DONE - Next Prime Number \n- Have the program find prime numbers until the user chooses to stop\n  asking for the next one. \n DONE - Find Cost of Tile to Cover W x H Floor \n- Calculate the total cost of tile it would take to cover a floor plan\n  of width and height, using a cost entered by the user. \n Mortgage Calculator \n- Calculate the monthly payments of a fixed term mortgage over given\n  Nth terms at a given interest rate. Also figure out how long it will\n  take the user to pay back the loan. \n Change Return Program \n- The user enters a cost and then the amount of money given. The\n  program will figure out the change and the number of quarters,\n  dimes, nickels, pennies needed for the change. \n DONE - Binary to Decimal and Back Converter \n- Develop a converter to convert a decimal number to binary or a\n  binary number to its decimal equivalent. \n DONE - Calculator \n- A simple calculator to do basic operators. Make it a scientific\n  calculator for added complexity. \n DONE - Unit Converter (temp, currency, volume, mass and more) \n- Converts various units between one another. The user enters the type\n  of unit being entered, the type of unit they want to convert to and\n  then the value. The program will then make the conversion. \n DONE - Alarm Clock \n- A simple clock where it plays a sound after X number of\n  minutes/seconds or at a particular time. \n DONE - Distance Between Two Cities \n- Calculates the distance between two cities and allows the user to\n  specify a unit of distance. This program may require finding\n  coordinates for the cities like latitude and longitude. \n DONE - Credit Card Validator \n- Takes in a credit card number from a common credit card vendor\n  (Visa, MasterCard, American Express, Discoverer) and validates it to\n  make sure that it is a valid number (look into how credit cards use\n  a checksum). \n Tax Calculator \n- Asks the user to enter a cost and either a country or state tax. It\n  then returns the tax plus the total cost with tax. \n DONE - Factorial Finder \n- The Factorial of a positive integer, n, is defined as the product of\n  the sequence n, n-1, n-2, ...1 and the factorial of zero, 0, is\n  defined as being 1. Solve this using both loops and recursion. \n DONE - Complex Number Algebra \n- Show addition, multiplication, negation,\n  and inversion of complex numbers in separate functions. (Subtraction\n  and division operations can be made with pairs of these operations.)\n  Print the results for each operation tested. \n DONE - Happy Numbers \n- A happy number is defined by the following\n  process. Starting with any positive integer, replace the number by\n  the sum of the squares of its digits, and repeat the process until\n  the number equals 1 (where it will stay), or it loops endlessly in a\n  cycle which does not include 1. Those numbers for which this process\n  ends in 1 are happy numbers, while those that do not end in 1 are\n  unhappy numbers. Display an example of your output here. Find first\n  8 happy numbers. \n DONE - Number Names \n- Show how to spell out a number in English. You can\n  use a preexisting implementation or roll your own, but you should\n  support inputs up to at least one million (or the maximum value of\n  your language\'s default bounded integer type, if that\'s\n  less).  Optional: Support for inputs other than positive integers\n  (like zero, negative integers, and floating-point numbers). \n Classic Algorithms \n DONE - Collatz Conjecture \n- Start with a number  n > 1 . Find the number\n  of steps it takes to reach one using the following process: If  n \n  is even, divide it by 2. If  n  is odd, multiply it by 3 and add 1. \n DONE - Sorting \n- Implement two types of sorting algorithms: Merge sort\n  and bubble sort. \n DONE - Closest pair problem \n- The closest pair of points problem or\n  closest pair problem is a problem of computational geometry: given\n   n  points in metric space, find a pair of points with the smallest\n  distance between them. \n DONE - Sieve of Eratosthenes \n- The sieve of Eratosthenes is one of the\n  most efficient ways to find all of the smaller primes (below 10\n  million or so). \n Graph \n Graph from links \n- Create a program that will\ncreate a graph or network from a series of links. \n Eulerian Path \n- Create a program which will take as an input a\n  graph and output either a Eulerian path or a Eulerian cycle, or\n  state that it is not possible.  A Eulerian Path starts at one node\n  and traverses every edge of a graph through every node and finishes\n  at another node.  A Eulerian cycle is a eulerian Path that starts\n  and finishes at the same node. \n Connected Graph \n- Create a program which takes a graph as an input\n  and outputs whether every node is connected or not. \n Dijkstra’s Algorithm \n- Create a program that finds the shortest\n  path through a graph using its edges. \n Data Structures \n Inverted index \n- An  Inverted Index  is a\n  data structure used to create full text search. Given a set of text\n  files, implement a program to create an inverted index. Also create\n  a user interface to do a search using that inverted index which\n  returns a list of files that contain the query term / terms. The\n  search index can be in memory. \n Text \n Reverse a String \n- Enter a string and the program will reverse it\n  and print it out. \n Pig Latin \n- Pig Latin is a game of alterations played on the\n  English language game. To create the Pig Latin form of an English\n  word the initial consonant sound is transposed to the end of the\n  word and an ay is affixed (Ex.: "banana" would yield\n  anana-bay). Read Wikipedia for more information on rules. \n Count Vowels \n- Enter a string and the program counts the number of\n  vowels in the text. For added complexity have it report a sum of\n  each vowel found. \n Check if Palindrome \n- Checks if the string entered by the user is\n  a palindrome. That is that it reads the same forwards as backwards\n  like “racecar” \n Count Words in a String \n- Counts the number of individual words in\n  a string. For added complexity read these strings in from a text\n  file and generate a summary. \n Text Editor \n- Notepad style application that can open, edit, and\n  save text documents.  Optional: Add syntax highlighting and other\n  features. \n RSS Feed Creator \n- Given a link to RSS/Atom Feed, get all posts\n  and display them. \n Post it Notes Program \n- A program where you can add text reminders\n  and post them.  Optional: You can have the program also add popup\n  reminders. \n Quote Tracker (market symbols etc) \n- A program which can go out\n  and check the current value of stocks for a list of symbols entered\n  by the user. The user can set how often the stocks are checked. For\n  CLI, show whether the stock has moved up or down.  Optional: If GUI,\n  the program can show green up and red down arrows to show which\n  direction the stock value has moved. \n Guestbook / Journal \n- A simple application that allows people to\n  add comments or write journal entries. It can allow comments or not\n  and timestamps for all entries. Could also be made into a shout\n  box.  Optional: Deploy it on Google App Engine or Heroku or any\n  other PaaS (if possible, of course). \n Fortune Teller (Horoscope) \n- A program that checks your horoscope\n  on various astrology sites and puts them together for you each day. \n Vigenere / Vernam / Ceasar Ciphers \n- Functions for encrypting and\n  decrypting data messages. Then send them to a friend. \n Random Gift Suggestions \n- Enter various gifts for certain people\n  when you think of them. When its time to give them a gift (xmas,\n  birthday, anniversary) it will randomly pick one.  Optional: Suggest\n  places you can get it (link to Amazon page?). \n Markdown to HTML Converter \n- Converts Markdown formatted text into\n  HTML files. Implement basic tags like  p ,  strong ,  em \n  etc.  Optional: Implement all tags from\n   Markdown Syntax Docs . \n Regex Query Tool \n- A tool that allows the user to enter a text\n  string and then in a separate control enter a regex pattern. It will\n  run the regular expression against the source text and return any\n  matches or flag errors in the regular expression. \n Networking \n FTP Program \n- A file transfer program which can transfer files\n  back and forth from a remote web sever. \n Bandwidth Monitor \n- A small utility program that tracks how much\n  data you have uploaded and downloaded from the net during the course\n  of your current online session. See if you can find out what periods\n  of the day you use more and less and generate a report or graph that\n  shows it. \n Port Scanner \n- Enter an IP address and a port range where the\n  program will then attempt to find open ports on the given computer\n  by connecting to each of them. On any successful connections mark\n  the port as open. \n Mail Checker (POP3 / IMAP)  - The user enters various account\n  information include web server and IP, protocol type (POP3 or IMAP)\n  and the application will check for email at a given interval. \n Country from IP Lookup \n- Enter an IP address and find the country\n  that IP is registered in.  Optional: Find the Ip automatically. \n Whois Search Tool \n- Enter an IP or host address and have it look\n  it up through whois and return the results to you. \n Site Checker with Time Scheduling  - An application that attempts\n  to connect to a website or server every so many minutes or a given\n  time and check if it is up. If it is down, it will notify you by\n  email or by posting a notice on screen. \n Classes \n Product Inventory Project \n- Create an application which manages an\n  inventory of products. Create a product class which has a price, id,\n  and quantity on hand. Then create an  inventory  class which keeps\n  track of various products and can sum up the inventory value. \n Airline / Hotel Reservation System \n- Create a reservation system\n  which books airline seats or hotel rooms. It charges various rates\n  for particular sections of the plane or hotel. Example, first class\n  is going to cost more than coach. Hotel rooms have penthouse suites\n  which cost more. Keep track of when rooms will be available and can\n  be scheduled. \n Bank Account Manager  - Create a class called Account which will be\n  an abstract class for three other classes called CheckingAccount,\n  SavingsAccount and BusinessAccount. Manage credits and debits from\n  these accounts through an ATM style program. \n Patient / Doctor Scheduler \n- Create a patient class and a doctor\n  class. Have a doctor that can handle multiple patients and setup a\n  scheduling program where a doctor can only handle 16 patients during\n  an 8 hr work day. \n Recipe Creator and Manager \n- Create a recipe class with\n  ingredients and a put them in a recipe manager program that\n  organizes them into categories like deserts, main courses or by\n  ingredients like chicken, beef, soups, pies etc. \n Image Gallery \n- Create an image abstract class and then a class\n  that inherits from it for each image type. Put them in a program\n  which displays them in a gallery style format for viewing. \n Shape Area and Perimeter Classes \n- Create an abstract class called\n  Shape and then inherit from it other shapes like diamond, rectangle,\n  circle, triangle etc. Then have each class override the area and\n  perimeter functionality to handle each shape type. \n Flower Shop Ordering To Go \n- Create a flower shop application\n  which deals in flower objects and use those flower objects in a\n  bouquet object which can then be sold. Keep track of the number of\n  objects and when you may need to order more. \n Family Tree Creator \n- Create a class called Person which will have\n  a name, when they were born and when (and if) they died. Allow the\n  user to create these Person classes and put them into a family tree\n  structure. Print out the tree to the screen. \n Threading \n Create A Progress Bar for Downloads \n- Create a progress bar for\n  applications that can keep track of a download in progress. The\n  progress bar will be on a separate thread and will communicate with\n  the main thread using delegates. \n Bulk Thumbnail Creator \n- Picture processing can take a bit of time\n  for some transformations. Especially if the image is large. Create\n  an image program which can take hundreds of images and converts them\n  to a specified size in the background thread while you do other\n  things. For added complexity, have one thread handling re-sizing,\n  have another bulk renaming of thumbnails etc. \n Web \n Page Scraper \n- Create an application which connects to a site and\n  pulls out all links, or images, and saves them to a list.  Optional:\n  Organize the indexed content and don’t allow duplicates. Have it put\n  the results into an easily searchable index file. \n Web Browser with Tabs \n- Create a small web browser that allows you\n  to navigate the web and contains tabs which can be used to navigate\n  to multiple web pages at once. For simplicity don’t worry about\n  executing Javascript or other client side code. \n Online White Board \n- Create an application which allows you to\n  draw pictures, write notes and use various colors to flesh out ideas\n  for projects.  Optional: Add feature to invite friends to\n  collaborate on a white board online. \n Get Atomic Time from Internet Clock \n- This program will get the\n  true atomic time from an atomic time clock on the Internet. Use any\n  one of the atomic clocks returned by a simple Google search. \n Fetch Current Weather \n- Get the current weather for a given\n  zip/postal code.  Optional: Try locating the user automatically. \n Scheduled Auto Login and Action \n- Make an application which logs\n  into a given site on a schedule and invokes a certain action and\n  then logs out. This can be useful for checking web mail, posting\n  regular content, or getting info for other applications and saving\n  it to your computer. \n E-Card Generator \n- Make a site that allows people to generate\n  their own little e-cards and send them to other people. Do not use\n  Flash. Use a picture library and perhaps insightful mottos or\n  quotes. \n Content Management System \n- Create a content management system\n  (CMS) like Joomla, Drupal, PHP Nuke etc. Start small.  Optional:\n  Allow for the addition of modules/addons. \n Web Board (Forum) \n- Create a forum for you and your buddies to\n  post, administer and share thoughts and ideas. \n CAPTCHA Maker \n- Ever see those images with letters a numbers when\n  you signup for a service and then asks you to enter what you see? It\n  keeps web bots from automatically signing up and spamming. Try\n  creating one yourself for online forms. \n Files \n Quiz Maker \n- Make an application which takes various questions\n  form a file, picked randomly, and puts together a quiz for\n  students. Each quiz can be different and then reads a key to grade\n  the quizzes. \n File Explorer \n- Create your own simple windows explorer\n  program. Add feature(s) you always thought are missing from MS\n  Windows Explorer or Mac Finder. \n Sort Excel/CSV File Utility \n- Reads a file of records, sorts them,\n  and then writes them back to the file. Allow the user to choose\n  various sort style and sorting based on a particular field. \n Create Zip File Maker \n- The user enters various files from\n  different directories and the program zips them up into a zip\n  file.  Optional: Apply actual compression to the files. Start with\n  Huffman Algorithm. \n PDF Generator \n- An application which can read in a text file, html\n  file or some other file and generates a PDF file out of it. Great\n  for a web based service where the user uploads the file and the\n  program returns a PDF of the file.  Optional: Deploy on GAE or\n  Heroku if possible. \n Mp3 Tagger \n- Modify and add ID3v1 tags to MP3 files. See if you\n  can also add in the album art into the MP3 file’s header as well as\n  other ID3v2 tags. \n Code Snippet Manager \n- Another utility program that allows coders\n  to put in functions, classes or other tidbits to save for use\n  later. Organized by the type of snippet or language the coder can\n  quickly look up code.  Optional: For extra practice try adding\n  syntax highlighting based on the language. \n Databases \n SQL Query Analyzer \n- A utility application which a user can enter\n  a query and have it run against a local database and look for ways\n  to make it more efficient. \n Remote SQL Tool \n- A utility that can execute queries on remote\n  servers from your local computer across the Internet. It should take\n  in a remote host, user name and password, run the query and return\n  the results. \n Report Generator \n- Create a utility that generates a report based\n  on some tables in a database. Generates a sales reports based on the\n  order/order details tables or sums up the days current database\n  activity. \n Event Scheduler and Calendar \n- Make an application which allows\n  the user to enter a date and time of an event, event notes and then\n  schedule those events on a calendar. The user can then browse the\n  calendar or search the calendar for specific events.  Optional:\n  Allow the application to create re-occurrence events that reoccur\n  every day, week, month, year etc. \n Budget Tracker \n- Write an application that keeps track of a\n  household’s budget. The user can add expenses, income, and recurring\n  costs to find out how much they are saving or losing over a period\n  of time.  Optional: Allow the user to specify a date range and see\n  the net flow of money in and out of the house budget for that time\n  period. \n Address Book \n- Keep track of various contacts, their numbers,\n  emails and little notes about them like a Rolodex in the database. \n TV Show Tracker \n- Got a favorite show you don’t want to miss?\n  Don’t have a PVR or want to be able to find the show to then PVR it\n  later? Make an application which can search various online TV Guide\n  sites, locate the shows/times/channels and add them to a database\n  application. The database/website then can send you email reminders\n  that a show is about to start and which channel it will be on. \n Travel Planner System \n- Make a system that allows users to put\n  together their own little travel itinerary and keep track of the\n  airline / hotel arrangements, points of interest, budget and\n  schedule. \n Graphics and Multimedia \n Slide Show \n- Make an application that shows various pictures in a\n  slide show format.  Optional: Try adding various effects like fade\n  in/out, star wipe and window blinds transitions. \n Stream Video from Online \n- Try to create your own online streaming\n  video player. \n Mp3 Player \n- A simple program for playing your favorite music\n  files. Add features you though are missing from your favorite music\n  player. \n Watermarking Application \n- Have some pictures you want copyright\n  protected? Add your own logo or text lightly across the background\n  so that no one can simply steal your graphics off your site. Make a\n  program that will add this watermark to the picture.  Optional: Use\n  threading to process multiple images simultaneously. \n Turtle Graphics \n- This is a common project where you create a\n  floor of 20 x 20 squares. Using various commands you tell a turtle\n  to draw a line on the floor. You have move forward, left or right,\n  lift or drop pen etc. Do a search online for "Turtle Graphics" for\n  more information.  Optional: Allow the program to read in the list\n  of commands from a file. \n Security \n Caesar cipher \n- Implement a Caesar cipher, both encoding and\n  decoding. The key is an integer from 1 to 25. This cipher rotates\n  the letters of the alphabet (A to Z). The encoding replaces each\n  letter with the 1st to 25th next letter in the alphabet (wrapping Z\n  to A). So key 2 encrypts "HI" to "JK", but key 20 encrypts "HI" to\n  "BC". This simple "monoalphabetic substitution cipher" provides\n  almost no security, because an attacker who has the encoded message\n  can either use frequency analysis to guess the key, or just try all\n  25 keys. \n =============================================== \n Sources ======= \n *\n   Martyr2’s Mega Project List \n*  Rosetta Code', "compsoc-hoodie \n This is incredibly silly - It's here just for historical purposes.", 'hc-zenburn-emacs \n A higher contrast version of the popular zenburn theme created by  Bozhidar Batsov . \n This theme uses the new built-in theming support available starting\nwith Emacs 24. \n Screenshot \n \n Installation \n package.el \n hc-zenburn is available in  MELPA . \n You can install  hc-zenburn  with the following command: \n M-x package-install hc-zenburn-theme \n To load it automatically on Emacs startup add this to your init file: \n lisp\n(load-theme \'hc-zenburn t) \n Manual \n Download  hc-zenburn-theme.el  to the directory  ~/.emacs.d/themes/ . Add this to your\n .emacs : \n lisp\n(add-to-list \'custom-theme-load-path "~/.emacs.d/themes/") \n Now you can load the theme with the interactive function  load-theme  like this: \n M-x load-theme RET hc-zenburn \n To load it automatically on Emacs startup add this to your init file: \n lisp\n(load-theme \'hc-zenburn t) \n Ugly colors in the terminal Emacs version \n If your Emacs looks considerably uglier in a terminal (compared to the\nGUI version) try adding this to your  .bashrc  or  .zshrc : \n bash\nexport TERM=xterm-256color \n Source the  .bashrc ( .zshrc ) file and start Emacs again. \n Bugs & Improvements \n Please, report any problems that you find at the\n project integrated tracker .\nIf you\'ve added some improvements and you want them included upstream\ndon\'t hesitate to send me a patch or even better - a GitHub pull\nrequest.', "edran's dotfiles \n \n These dotfiles are compatible with  MacOS Mojave  and  Ubuntu 20.04  (and\npossibly  18.04  too). Most of the configuration files can be found in  dots/ . \n Feel free to check things out, and copy whatever is needed, but please don't run\nit as-is, as it might mess up your system. \n Installation \n There are two install levels,  basic  and  full . The former is for systems\nwhere I require a quick installation and setup of basic tooling (like shell +\neditor), and where I most likely do not have root access. The latter for my own\nmachines. \n The automatic installation requires the following: \n \n ssh authentication setup for github \n python3  +  ansible \n \n If these are done, then it's just a matter of running: \n sh\n$ git clone git@github.com:edran/.dotfiles ~/.dotfiles\n$ cd .dotfiles/\n$ ansible-playbook --ask-become-pass -i ansible/inventory ansible/<basic|full>.yml \n After installation \n There are a couple of things that need to be done, after the first installation\n(but not necessarily subsequent playbook runs). \n \n Reload the shell to make sure everything is sourced. \n Run  zinit self-update  to finish configuring  zsh . \n Run  fast-theme clean  to set zsh highlighting. \n", 'nthlisp \n Yet another lisp-like hack \n Requirements \n \n libedit-dev \n', 'GRE2Anki \n Chooses a GRE word from a list of nearly 4000 words and outputs its\ndescription. You can then copy that in Anki or other SRS software.\nEvery time you run the script, the random word will be added to\n archive.txt , which will stop the word from appearing again. In case\nyou want to start over, just delete  archive.txt . \n Requirements \n \n python3 \n \n Instructions \n bash\n$ python gre2anki.py \n Hopefully you will see something like this: \n \n The HTML will be automatically parsed by Anki. By default it will read\n share/GRE_WORDS.txt , but you can specify a path doing: \n bash\n$ python gre2anki.py -f my_file \n Acknowledgements \n The list of words has been exported from\n this deck .', 'Coursera-ml \n Solution to exercises from the Machine Learning Coursera 2014 class.', 'code-exercises \n Repository containing solutions to exercises from different sources.', 'Twitter2Speech \n Read tweets aloud. \n Requirements \n \n python 2.7+ \n tweepy \n festival \n pyglet \n gtts \n pygame \n \n Usage \n \n Create all the Twitter Tokens at https://apps.twitter.com/ \n Record them in  test_config.txt \n Run  $ python display.py -f test_config.txt \n', 'Twitter2Robot \n Move Archie using tweets!', 'SDP-milestone-one-tester \n Code for testing the communications task of SDP milestone 1 \n randomFileGenerator <file size(bytes)> \ngenerates a file of random bytes with the specified size, for testing \n The arduino code just forwards data received to the serial port \n SDP-milestone-one-tester <port ("/dev/ttyACM0")> <filename ("randomFile.bin")> \nlistens for incoming data and compares it to the input file', 'ros_sdp \n Example  rospy  package for the System Design Project course at the University\nof Edinburgh.  \n Instructions (DICE) \n See  wiki', "pyrsk \n Uuh, what? \n This is a pedagogic experiment, made primarily for 4 reasons: \n \n To teach Python (and with it, programming, modern engineering tooling, and\n   algorithmic thinking) to a group of friends, who all happened to have grown\n   up together playing Risk, and somehow have ended up doing all sort of\n   important jobs. \n To end up with a fully functional, fun, multi-player experience for Risk-like\n   games. \n To have a side project that we can hack on to distract ourselves from this\n   quite terrible year. \n For me, @edran, to learn and develop some technical jargon in my\n   mothertongue, Italian. \n \n The engineering goal is to end up with a cross-compatible & native-feeling\nclient, and a game server that doesn't suck and that can be easily hosted on\nmodern platforms. \n This is an ambitious project, and quite likely to fail, due to the participants\nhaving almost no software engineering experience. @edran will act as mentor and\nsupervisor, and will hopefully not write any line of code whatsoever beyond the\ninitial setup. \n There will probably be nothing to see here :) You have been warned. Also, most\nof the interaction on this repo will be in Italian, so please forgive us in\nadvance for the colourful language. \n FAQs \n Is this a good idea? \n I don't know. YOLO."]
thomasopsomer,['repo', 'python-spark', 'Kelemen', 'InSpark', 'RapportiveLookup \n RapportiveLookup \n 1. Description \n Ask me for the tip \n 2. Usage \n \n Clone the repository :) \n In the repository\n     pip install -r requirement.txt \n In command line example python.py /path/to/my/csv/of/emails.csv /path/to/csv/of/results.csv --header --email-position 2\n```\nusage: app.py [-h] [--header] [--no-header] [--email-position EMAIL_POSITION]\n              INPUT_PATH OUTPUT_PATH \n \n positional arguments:\n  INPUT_PATH\n  OUTPUT_PATH \n optional arguments:\n  -h, --help            show this help message and exit\n  --header\n  --no-header           (default: True)\n  --email-position EMAIL_POSITION, -e EMAIL_POSITION\n                        (default: 2)\n```\nThen the program will ask for a token to start, and each time it needs a new one.', 'Maestro \n Python API Usage \n ```python \n from maestro import Maestro\nimport os \n init maestro \n m = Maestro() \n set conf for driver \n MASTER_DRIVER_CONF = {\n    "security_group": "default",\n    "access_key": os.environ["AWS_ACCESS_KEY_ID"],\n    "secret_key": os.environ["AWS_SECRET_ACCESS_KEY"],\n    "instance_type": "r4.xlarge",\n    "region": "eu-west-1",\n    "ssh_keypath": "/Users/thomasopsomer/AWS/asgard.pem",\n    "request_spot_instance": True,\n    "spot_price": 0.2\n    "zone": "a",\n    "iam_instance_profile": "asgard",\n    "root_size": "50"\n} \n set conf for workers \n WORKER_DRIVER_CONF = {\n    "security_group": "default",\n    "access_key": os.environ["AWS_ACCESS_KEY_ID"],\n    "secret_key": os.environ["AWS_SECRET_ACCESS_KEY"],\n    # "instance_type": "m4.large",\n    "region": "eu-west-1",\n    "instance_type": "r4.4xlarge",\n    "ssh_keypath": "/Users/thomasopsomer/AWS/asgard.pem",\n    "zone": "a",\n    "iam_instance_profile": "asgard",\n    "root_size": "50"\n    "request_spot_instance": True,\n    "spot_price": 0.4\n} \n cluster name and docker image \n cluster_name = "ac"\nspark_image = "asgard/asgard:develop" \n launch a spark cluster \n m.add_spark_cluster(cluster_name,\n                    spark_image=spark_image,\n                    provider="amazonec2",\n                    n_workers=4,\n                    master_driver_conf=MASTER_DRIVER_CONF,\n                    worker_driver_conf=WORKER_DRIVER_CONF) \n run a task on the cluster \n cmd = "spark-submit --master spark://master:7077 --class org.apache.spark.examples.SparkPi /usr/spark/lib/spark-examples-1.6.0-hadoop2.6.0.jar" \n m.run_on_cluster(cluster_name=cluster_name,\n                 docker_image=spark_image,\n                 command=cmd) \n ``` \n REST API Usage \n \n \n Start Rabbitmq and Celery\n bash\ncelery -A maestro.api.task worker --loglevel=info \n \n \n start maestro server\n python maestro/api/server.py \n \n \n TODO \n \n \n Allow to install stuff on each machine on request \n \n \n Try flask + celery \n \n \n Add database layer with sqlite for cluster data persistence and finding all created cluster ... \n \n \n Issue with pydm: be sure that when machine is delete, the created keypair is also deleted \n \n \n Specify network for each cluster base on cluster name \n \n', 'MVA \n Course \n \n Graph in Machine Learning \n Reinforcement Learning \n Object Recogniation and Computer Vision \n Sparse Representations, Wavelets and Classification \n \n Final Projects \n Summary \n \n Learning Graph Embeddings ( Graph in ML ) \n Counter Factual Regret Minimization for Extensive Games ( RL ) \n Joint Representation of Text and Images ( RecVis ) \n Plume (challengedata) ( Mallat ) \n \n Deadlines \n | Project       | Report deadline  | Pres Deadline        |\n| ------------- |:----------------:| --------------------:|\n| Graphs        | 05 January 2017  | 09 January 2017      |\n| RL            | 10 January 2017  | 17 January 2017      |\n| RecVis        | 16 January 2017  | 9-10-11 January 2017 |\n| Mallat        | ?? February 2017 | ?? February 2017     | \n Graphs in Machine Learning \n Course website \n Piazza \n TPs \n website \n \n \n TP1: Spectral Clustering (due to 7/11/2015) \n \n \n TP2: Face Recognition (due to 28/11/2015) \n \n \n TP3: Large Scale Graph Learning (due on 12/12/2016) \n \n \n Project (review, or more :) \n Deadline \n \n \n finding project 21/11 \n \n \n report 05/01 then presentation... \n \n \n Ideas \n Learning Graph Embeddings \n \n DeepWalks (Graph embedding with random walks and skip-gram)\nsee  paper  and  code \n \n Reinforcement Learning \n course website \n TPs \n website \n \n \n TP1: Dynamic Programming and Reinforcement Learning (due for 13/11/2016) \n \n \n TP2: Q-Learning / Stochastic Bandit algorithms (due for 27/11/2016) \n \n \n TP3: Bandit algorithm for Game Theory and RL (due on 20/12/2016) \n \n \n Object recognition and computer vision \n Course website \n Assignments \n \n \n Assignment 1: Instance-level recognition - Due to 02/11\n    see  here \n \n \n Assignment 2: Image classification - Due to 08/11\n    see  here \n \n \n Assignment 3: Neural networks - Due on 22/11\n    see  here \n \n \n Sparse Representations, Wavelets and Classification \n Course website \n Project: Plume \n https://challengedata.ens.fr/en/challenge/16/predict_air_quality_at_the_street_level.html', 'ImagoTexto \n Word Embedding \n toto :) \n Image Embedding \n Imane :) \n Canonical Correlation Analysis \n TODO', '\n TODO \n \n \n Faire un Dev Set (avec des station différentes du train set) \n \n \n Tester modèle classique sur feature brut (just un peu de preprocessing genre scaling, centring)\n(on se fait plaisir sur scikit learn) (brutal) \n \n \n Reflechir aux features (lire un peu), en particulier "the regular structure of the time series (strong daily and seasonal cycles)" \n \n \n Recommencer avec des nouvelles features :) \n \n', 'Kimnd \n Secret :)', 'Dockerized Brat Server \n Easy deployment of the  brat  server for text annotation using docker \n Usage \n \n \n Configuration: Fill  .env  and / or  config.py \n \n \n Add users in  userlist.txt  with one user + " " + password per line \n \n \n Build image:  docker-compose build \n \n \n Run the server:  docker-compose up -d \n \n \n Import data collection \n Given a corpus of txt files to be annotated. Put the files in a folder in the data directory.\nBefore starting annotation,  .ann  files needs to be created. You can create them with the following command \n bash\nfind data -name \'*.txt\' | sed -e \'s|\\.txt|.ann|g\' | xargs touch \n Brat usage \n For information about using the tool, see the  manual', 'uspto-parser \n Spark application to parse patents from the USPTO. It mainly consists of a wrapper of the USPTO parser  USPTO/PatentPublicData , so thanks to them for the dirty work :) \n Build \n Using  sbt docker  to directly get a docker image with the jar:\n sbt docker \n Usage \n show usage: \n docker run --rm asgard/uspto-parser:latest spark-submit /home/uspto-parser/uspto-parser-assembly-0.0.1.jar \n --folderPath <value>\n        path to folder containing patent archives to process\n  --outputPath <value>\n        path to output parquet file\n  --numPartitions <value>\n        Number of partitions of rdd to process\n  --test\n        Flag to test the software, process only 2 patent archive\n  --from <value>\n        Starting date, in string format, will be infered. For instance 20010101\n  --to <value>\n        Ending date, in string format: yyyyMMdd or yyMMdd \n Data Schema \n root\n |-- type: string (nullable = true)\n |-- kind: string (nullable = true)\n |-- patentId: string (nullable = true)\n |-- patentNb: string (nullable = true)\n |-- applicationId: string (nullable = true)\n |-- applicationDate: string (nullable = true)\n |-- publicationDate: string (nullable = true)\n |-- relatedIds: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- otherIds: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- abstract: string (nullable = true)\n |-- briefSummary: string (nullable = true)\n |-- detailedDescription: string (nullable = true)\n |-- inventors: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- name: struct (nullable = true)\n |    |    |    |-- type: string (nullable = true)\n |    |    |    |-- raw: string (nullable = true)\n |    |    |    |-- firstName: string (nullable = true)\n |    |    |    |-- middleName: string (nullable = true)\n |    |    |    |-- lastName: string (nullable = true)\n |    |    |    |-- abbreviated: string (nullable = true)\n |    |    |-- address: struct (nullable = true)\n |    |    |    |-- street: string (nullable = true)\n |    |    |    |-- city: string (nullable = true)\n |    |    |    |-- state: string (nullable = true)\n |    |    |    |-- country: string (nullable = true)\n |    |    |    |-- zipCode: string (nullable = true)\n |    |    |    |-- email: string (nullable = true)\n |    |    |    |-- phone: string (nullable = true)\n |    |    |-- residency: string (nullable = true)\n |    |    |-- nationality: string (nullable = true)\n |-- applicants: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- name: struct (nullable = true)\n |    |    |    |-- type: string (nullable = true)\n |    |    |    |-- raw: string (nullable = true)\n |    |    |    |-- firstName: string (nullable = true)\n |    |    |    |-- middleName: string (nullable = true)\n |    |    |    |-- lastName: string (nullable = true)\n |    |    |    |-- abbreviated: string (nullable = true)\n |    |    |-- address: struct (nullable = true)\n |    |    |    |-- street: string (nullable = true)\n |    |    |    |-- city: string (nullable = true)\n |    |    |    |-- state: string (nullable = true)\n |    |    |    |-- country: string (nullable = true)\n |    |    |    |-- zipCode: string (nullable = true)\n |    |    |    |-- email: string (nullable = true)\n |    |    |    |-- phone: string (nullable = true)\n |-- assignees: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- name: struct (nullable = true)\n |    |    |    |-- type: string (nullable = true)\n |    |    |    |-- raw: string (nullable = true)\n |    |    |    |-- firstName: string (nullable = true)\n |    |    |    |-- middleName: string (nullable = true)\n |    |    |    |-- lastName: string (nullable = true)\n |    |    |    |-- abbreviated: string (nullable = true)\n |    |    |-- address: struct (nullable = true)\n |    |    |    |-- street: string (nullable = true)\n |    |    |    |-- city: string (nullable = true)\n |    |    |    |-- state: string (nullable = true)\n |    |    |    |-- country: string (nullable = true)\n |    |    |    |-- zipCode: string (nullable = true)\n |    |    |    |-- email: string (nullable = true)\n |    |    |    |-- phone: string (nullable = true)\n |    |    |-- role: string (nullable = true)\n |    |    |-- roleDesc: string (nullable = true)\n |-- ipcs: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- type: string (nullable = true)\n |    |    |-- main: boolean (nullable = true)\n |    |    |-- normalized: string (nullable = true)\n |    |    |-- section: string (nullable = true)\n |    |    |-- class: string (nullable = true)\n |    |    |-- subClass: string (nullable = true)\n |    |    |-- group: string (nullable = true)\n |    |    |-- subGroup: string (nullable = true)\n |-- claims: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- id: string (nullable = true)\n |    |    |-- type: string (nullable = true)\n |    |    |-- text: string (nullable = true)\n |    |    |-- parentIds: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |-- priorities: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- docNumber: string (nullable = true)\n |    |    |-- kind: string (nullable = true)\n |    |    |-- date: string (nullable = true)\n |    |    |-- country: string (nullable = true)\n |    |    |-- id: string (nullable = true)\n |-- publicationRef: struct (nullable = true)\n |    |-- docNumber: string (nullable = true)\n |    |-- kind: string (nullable = true)\n |    |-- date: string (nullable = true)\n |    |-- country: string (nullable = true)\n |    |-- id: string (nullable = true)\n |-- applicationRef: struct (nullable = true)\n |    |-- docNumber: string (nullable = true)\n |    |-- kind: string (nullable = true)\n |    |-- date: string (nullable = true)\n |    |-- country: string (nullable = true)\n |    |-- id: string (nullable = true)\n |-- citations: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- num: string (nullable = true)\n |    |    |-- documentId: struct (nullable = true)\n |    |    |    |-- docNumber: string (nullable = true)\n |    |    |    |-- kind: string (nullable = true)\n |    |    |    |-- date: string (nullable = true)\n |    |    |    |-- country: string (nullable = true)\n |    |    |    |-- id: string (nullable = true)', "wikipedia-spark \n This repo is mainly a wrapper of  idio/json-wikipedia , which itself bring some improvement over  diegoceccarelli/json-wikipedia . \n It combines  databricks/spark-xml  to read wikipedia dump (without decompressing the whole files :) and the  ArticleParser  from json-wikipedia to parse wikipedia articles. It brings the speed from Spark and the ability to save easily in parquet or json format, in a distributed maner for instance on S3 :) \n build \n \n Build json-wikipedia using the command  mvn assembly:assembly  (for more information see their  readme ) \n \n Actually you shoud remove the spark dependency from the  pom.xml  because it's big and is still at version 1.3 ... \n Once the  json-wikipedia-1.2.0-jar-with-dependencies.jar  is build, put it in a  lib  folder at the root of this repo. \n \n Build wikipedia-spark using  sbt assembly  or  sbt docker  (which also builds you an image ready to go) \n \n usage \n bash\ndocker run --rm asgard/wikipedia-spark:latest spark-submit /home/wikipedia-spark/wikipedia-spark-assembly-0.0.1.jar ... \n ```\nUsage: SparkWikipediaParser [options] \n --inputPath  \n        path to xml dump or folder with xml dumps files\n  --outputPath  \n        path to output file / folder\n  --outputFormat  \n        Format for the ouput: parquet or json\n  --lang  \n        language of the dump\n  --test\n        Flag to test on a few lines\n```"]
awentzonline,["coolmit \n Don't just commit; coolmit. Make your commits cooler with coolmit today!\n \n \n Eliminate boring commit hashes! With  coolmit  you choose\nthe beginning of the commit hash, giving your commits that\n coolmit  edge. Impress your boss and aggressively flex\nyour toned, bronze code muscle in the faces of your peers. \n This was inspired by one of the problems in Stripe's CTF3. \n Here's a nice list of words you can make with hexadecimal characters. \n Installation \n pip install coolmit  or  python setup.py install \n Usage \n To update the hash of the current HEAD:\n git coolmit DESIRED_PREFIX \n E.g.:  git coolmit beef \n License \n MIT", 'js1k template \n Usage \n \n Fork this repo \n Install dependencies with  npm install \n Create interesting javascript in js1k.js \n Run  grunt  to make an optimized build in  js1k.crushed.js \n Run  grunt serve  to start a local webserver at http://localhost:8000/ \n Point your browser at  index.html  or  index-crushed.html  to view in the contest shim \n Submit to  js1k \n \n License \n MIT', 'botbits \n Some bits for building bots.', 'portraithell \n Portraithell detects poorly oriented videos which have apparently been\naccidentally uploaded to the internet and alerts their creator. \n NOTE \n Under construction. \n Installation \n \n python setup.py install \n \n Usage \n \n Copy  vars.sh.example  to  vars.sh  (or something) and source it \n Run  portraithell  to start things up. \n \n Develop \n \n Clone \n Run  python setup.py develop \n Achieve success \n', 'BRthur \n Overview \n The aim of this project is to waste a bunch of time writing a componentized\nweb editor. \n Namesake \n \n Getting started \n Install dependencies: \n npm install\nnpm install -g bower\nbower install\n \n Run  grunt karma  to run tests\nRun  grunt  to build \n License \n MIT', 'gtwittools \n Some tools for making twitter-related things with gevent. \n Installation \n \n python setup.py install \n \n Examples \n \n lolstream.py: print out a live stream of tweets containing lol \n plottin.py: create matplotlib images using gipc to create gevent-friendly child processes \n tweetplot.py: plot a graph of how frequently a phrase is tweeted \n opencv/twittercv.py: detect faces in images from twitter \n \n Develop \n \n Clone \n Run  python setup.py develop \n Achieve success \n', 'lolpersec \n Stats on the number of times "lol" is tweeted.\nSee it in action  @lolpersec \n Installation \n \n python setup.py install \n \n Usage \n \n Copy  vars.sh.example  to  vars.sh  (or something), fill it out, and source it \n Run  lps  to start things up. \n \n Develop \n \n Clone \n Run  python setup.py develop \n Achieve success \n', "bearbones \n Some really simple CMS tools for django. \n About \n The bulk of this project is found in the example. It's a simple\nCMS which uses an AngularJS app for the backend. \n License \n MIT", 'hubot-rapture \n Reports the current rapture index as defined by http://www.raptureready.com \n Usage \n To get the current rapture index, say  hubot rapture me \n Running hubot-rapture Locally \n You can test your hubot by running the following. \n You can start hubot-rapture locally by running: \n % bin/hubot\n \n You\'ll see some start up output about where your scripts come from and a\nprompt: \n [Sun, 04 Dec 2011 18:41:11 GMT] INFO Loading adapter shell\n[Sun, 04 Dec 2011 18:41:11 GMT] INFO Loading scripts from /home/tomb/Development/hubot/scripts\n[Sun, 04 Dec 2011 18:41:11 GMT] INFO Loading scripts from /home/tomb/Development/hubot/src/scripts\nHubot>\n \n Then you can interact with hubot-rapture by typing  hubot-rapture help . \n hubot-rapture> hubot-rapture help\n\nhubot-rapture> animate me <query> - The same thing as `image me`, except adds a few\nconvert me <expression> to <units> - Convert expression to given units.\nhelp - Displays all of the help commands that Hubot knows about.\n...\n \n Scripting \n An example script is included at  scripts/example.coffee , so check it out to\nget started, along with the  Scripting Guide . \n For many common tasks, there\'s a good chance someone has already one to do just\nthe thing. \n hubot-scripts \n There will inevitably be functionality that everyone will want. Instead\nof writing it yourself, you can check\n hubot-scripts  for existing scripts. \n To enable scripts from the hubot-scripts package, add the script name with\nextension as a double quoted string to the  hubot-scripts.json  file in this\nrepo. \n external-scripts \n Hubot is able to load scripts from third-party  npm  package. Check the package\'s documentation, but in general it is: \n \n Add the packages as dependencies into your  package.json \n npm install  to make sure those packages are installed \n Add the package name to  external-scripts.json  as a double quoted string \n \n You can review  external-scripts.json  to see what is included by default. \n Persistence \n If you are going to use the  hubot-redis-brain  package\n(strongly suggested), you will need to add the Redis to Go addon on Heroku which requires a verified\naccount or you can create an account at  Redis to Go  and manually\nset the  REDISTOGO_URL  variable. \n % heroku config:add REDISTOGO_URL="..."\n \n If you don\'t require any persistence feel free to remove the\n hubot-redis-brain  from  external-scripts.json  and you don\'t need to worry\nabout redis at all. \n Adapters \n Adapters are the interface to the service you want your hubot to run on. This\ncan be something like Campfire or IRC. There are a number of third party\nadapters that the community have contributed. Check\n Hubot Adapters  for the available ones. \n If you would like to run a non-Campfire or shell adapter you will need to add\nthe adapter package as a dependency to the  package.json  file in the\n dependencies  section. \n Once you\'ve added the dependency and run  npm install  to install it you can\nthen run hubot with the adapter. \n % bin/hubot -a <adapter>\n \n Where  <adapter>  is the name of your adapter without the  hubot-  prefix. \n Deployment \n % heroku create --stack cedar\n% git push heroku master\n \n If your Heroku account has been verified you can run the following to enable\nand add the Redis to Go addon to your app. \n % heroku addons:add redistogo:nano\n \n If you run into any problems, checkout Heroku\'s  docs . \n You\'ll need to edit the  Procfile  to set the name of your hubot. \n More detailed documentation can be found on the\n deploying hubot onto Heroku  wiki page. \n Deploying to UNIX or Windows \n If you would like to deploy to either a UNIX operating system or Windows.\nPlease check out the  deploying hubot onto UNIX  and\n deploying hubot onto Windows  wiki pages. \n Campfire Variables \n If you are using the Campfire adapter you will need to set some environment\nvariables. Refer to the documentation for other adapters and the configuraiton\nof those, links to the adapters can be found on  Hubot Adapters . \n Create a separate Campfire user for your bot and get their token from the web\nUI. \n % heroku config:add HUBOT_CAMPFIRE_TOKEN="..."\n \n Get the numeric IDs of the rooms you want the bot to join, comma delimited. If\nyou want the bot to connect to  https://mysubdomain.campfirenow.com/room/42 \nand  https://mysubdomain.campfirenow.com/room/1024  then you\'d add it like this: \n % heroku config:add HUBOT_CAMPFIRE_ROOMS="42,1024"\n \n Add the subdomain hubot should connect to. If you web URL looks like\n http://mysubdomain.campfirenow.com  then you\'d add it like this: \n % heroku config:add HUBOT_CAMPFIRE_ACCOUNT="mysubdomain"\n \n Restart the bot \n You may want to get comfortable with  heroku logs  and  heroku restart \nif you\'re having issues.', "hubot-drudgesiren \n This hubot script checks http://www.drudgereport.com to see if  siren.gif \nexists and, if so, echos the current headline to chat. \n Usage \n Specify output room with HUBOT_DRUDGESIREN_ROOM env var. \n Running hubot-drudgesiren Locally \n You can test your hubot by running the following. \n You can start hubot-drudgesiren locally by running: \n % bin/hubot\n \n You'll see some start up output about where your scripts come from and a\nprompt: \n [Sun, 04 Dec 2011 18:41:11 GMT] INFO Loading adapter shell\n[Sun, 04 Dec 2011 18:41:11 GMT] INFO Loading scripts from /home/tomb/Development/hubot/scripts\n[Sun, 04 Dec 2011 18:41:11 GMT] INFO Loading scripts from /home/tomb/Development/hubot/src/scripts\nHubot>\n \n Then you can interact with hubot-drudgesiren by typing  hubot-drudgesiren help . \n hubot-drudgesiren> hubot-drudgesiren help\n\nhubot-drudgesiren> animate me <query> - The same thing as `image me`, except adds a few\nconvert me <expression> to <units> - Convert expression to given units.\nhelp - Displays all of the help commands that Hubot knows about.\n...\n", 'base-phaser-game \n A skeletal starting point for  Phaser  games.\nKicked off with  generator-phaser-official . \n Usage \n \n fork this project \n clone your fork \n npm install \n bower install \n grunt serve  to view \n \n Deploy to github pages \n This project uses  grunt-gh-pages \nto deploy the  dist  directory to the github pages branch of your\nrepo. This is a real easy way to share your game with other people. \n \n grunt build \n grunt gh-pages \n View your game at http://YOUR_GITHUB_USER.github.io/YOUR_PROJECT_NAME/ \n \n For instance, you can view base-phaser-game at http://awentzonline.github.io/base-phaser-game/', 'phaser-tunnel-runner \n A game where you run in a tunnel. Made with  Phaser .\nAdapted from the  phaser tunnel example . \n Usage \n \n fork this project \n clone your fork \n npm install \n bower install \n grunt serve  to view \n \n Deploy to github pages \n This project uses  grunt-gh-pages \nto deploy the  dist  directory to the github pages branch of your\nrepo. This is a real easy way to share your game with other people. \n \n grunt build \n grunt gh-pages \n View your game at http://YOUR_GITHUB_USER.github.io/YOUR_PROJECT_NAME/ \n \n For instance, you can view phaser-tunnel-game at http://awentzonline.github.io/phaser-tunnel-runner/', 'base-phaser-game \n A skeletal starting point for  Phaser  games.\nKicked off with  generator-phaser-official . \n Usage \n \n fork this project \n clone your fork \n npm install \n bower install \n grunt serve  to view \n \n Deploy to github pages \n This project uses  grunt-gh-pages \nto deploy the  dist  directory to the github pages branch of your\nrepo. This is a real easy way to share your game with other people. \n \n grunt build \n grunt gh-pages \n View your game at http://YOUR_GITHUB_USER.github.io/YOUR_PROJECT_NAME/ \n \n For instance, you can view base-phaser-game at http://awentzonline.github.io/base-phaser-game/', 'Some snippets for Sublime Text which generate common things for browserified angularjs. \n Install \n \n Install Sublime Text \n Install Package Control for Sublime Text \n Package Control: Add Repository https://github.com/awentzonline/AngularBrowserified \n Package Control: Install Package "AngularBrowserified" \n', 'foldimation \n Combine multiple images on to one page which is folded to provide views of each image independently. \n', 'hubot-tweet-quote \n This bot keeps track of messages which consist of a message between quotes: "MESSAGE"\nWhen it sees the same message twice, it tweets that message. \n You must set environment variables for the twitter account:\n  TWITTER_CONSUMER_KEY\n  TWITTER_CONSUMER_SECRET\n  TWITTER_ACCESS_TOKEN\n  TWITTER_ACCESS_TOKEN_SECRET \n Default README: \n tweet-quote is a chat bot built on the  Hubot  framework. It was\ninitially generated by  generator-hubot , and configured to be\ndeployed on  Heroku  to get you up and running as quick as possible. \n This README is intended to help get you started. Definitely update and improve\nto talk about your own instance, how to use and deploy, what functionality he\nhas, etc! \n Running tweet-quote Locally \n You can test your hubot by running the following, however some plugins will not\nbehave as expected unless the  environment variables  they rely\nupon have been set. \n You can start tweet-quote locally by running: \n % bin/hubot\n \n You\'ll see some start up output and a prompt: \n [Sat Feb 28 2015 12:38:27 GMT+0000 (GMT)] INFO Using default redis on localhost:6379\ntweet-quote>\n \n Then you can interact with tweet-quote by typing  tweet-quote help . \n tweet-quote> tweet-quote help\ntweet-quote animate me <query> - The same thing as `image me`, except adds [snip]\ntweet-quote help - Displays all of the help commands that tweet-quote knows about.\n...\n \n Configuration \n A few scripts (including some installed by default) require environment\nvariables to be set as a simple form of configuration. \n Each script should have a commented header which contains a "Configuration"\nsection that explains which values it requires to be placed in which variable.\nWhen you have lots of scripts installed this process can be quite labour\nintensive. The following shell command can be used as a stop gap until an\neasier way to do this has been implemented. \n grep -o \'hubot-[a-z0-9_-]\\+\' external-scripts.json | \\\n  xargs -n1 -I {} sh -c \'sed -n "/^# Configuration/,/^#$/ s/^/{} /p" \\\n      $(find node_modules/{}/ -name "*.coffee")\' | \\\n    awk -F \'#\' \'{ printf "%-25s %s\\n", $1, $2 }\'\n \n How to set environment variables will be specific to your operating system.\nRather than recreate the various methods and best practices in achieving this,\nit\'s suggested that you search for a dedicated guide focused on your OS. \n Scripting \n An example script is included at  scripts/example.coffee , so check it out to\nget started, along with the  Scripting Guide . \n For many common tasks, there\'s a good chance someone has already one to do just\nthe thing. \n external-scripts \n There will inevitably be functionality that everyone will want. Instead of\nwriting it yourself, you can use existing plugins. \n Hubot is able to load plugins from third-party  npm  packages. This is the\nrecommended way to add functionality to your hubot. You can get a list of\navailable hubot plugins on  npmjs.com  or by using  npm search : \n % npm search hubot-scripts panda\nNAME             DESCRIPTION                        AUTHOR DATE       VERSION KEYWORDS\nhubot-pandapanda a hubot script for panda responses =missu 2014-11-30 0.9.2   hubot hubot-scripts panda\n...\n \n To use a package, check the package\'s documentation, but in general it is: \n \n Use  npm install --save  to add the package to  package.json  and install it \n Add the package name to  external-scripts.json  as a double quoted string \n \n You can review  external-scripts.json  to see what is included by default. \n Advanced Usage \n It is also possible to define  external-scripts.json  as an object to\nexplicitly specify which scripts from a package should be included. The example\nbelow, for example, will only activate two of the six available scripts inside\nthe  hubot-fun  plugin, but all four of those in  hubot-auto-deploy . \n json\n{\n  "hubot-fun": [\n    "crazy",\n    "thanks"\n  ],\n  "hubot-auto-deploy": "*"\n} \n Be aware that not all plugins support this usage and will typically fallback\nto including all scripts. \n hubot-scripts \n Before hubot plugin packages were adopted, most plugins were held in the\n hubot-scripts  package. Some of these plugins have yet to be\nmigrated to their own packages. They can still be used but the setup is a bit\ndifferent. \n To enable scripts from the hubot-scripts package, add the script name with\nextension as a double quoted string to the  hubot-scripts.json  file in this\nrepo. \n Persistence \n If you are going to use the  hubot-redis-brain  package (strongly suggested),\nyou will need to add the Redis to Go addon on Heroku which requires a verified\naccount or you can create an account at  Redis to Go  and manually\nset the  REDISTOGO_URL  variable. \n % heroku config:add REDISTOGO_URL="..."\n \n If you don\'t need any persistence feel free to remove the  hubot-redis-brain \nfrom  external-scripts.json  and you don\'t need to worry about redis at all. \n Adapters \n Adapters are the interface to the service you want your hubot to run on, such\nas Campfire or IRC. There are a number of third party adapters that the\ncommunity have contributed. Check  Hubot Adapters  for the\navailable ones. \n If you would like to run a non-Campfire or shell adapter you will need to add\nthe adapter package as a dependency to the  package.json  file in the\n dependencies  section. \n Once you\'ve added the dependency with  npm install --save  to install it you\ncan then run hubot with the adapter. \n % bin/hubot -a <adapter>\n \n Where  <adapter>  is the name of your adapter without the  hubot-  prefix. \n Deployment \n % heroku create --stack cedar\n% git push heroku master\n \n If your Heroku account has been verified you can run the following to enable\nand add the Redis to Go addon to your app. \n % heroku addons:add redistogo:nano\n \n If you run into any problems, checkout Heroku\'s  docs . \n You\'ll need to edit the  Procfile  to set the name of your hubot. \n More detailed documentation can be found on the  deploying hubot onto\nHeroku  wiki page. \n Deploying to UNIX or Windows \n If you would like to deploy to either a UNIX operating system or Windows.\nPlease check out the  deploying hubot onto UNIX  and  deploying\nhubot onto Windows  wiki pages. \n Campfire Variables \n If you are using the Campfire adapter you will need to set some environment\nvariables. If not, refer to your adapter documentation for how to configure it,\nlinks to the adapters can be found on  Hubot Adapters . \n Create a separate Campfire user for your bot and get their token from the web\nUI. \n % heroku config:add HUBOT_CAMPFIRE_TOKEN="..."\n \n Get the numeric IDs of the rooms you want the bot to join, comma delimited. If\nyou want the bot to connect to  https://mysubdomain.campfirenow.com/room/42 \nand  https://mysubdomain.campfirenow.com/room/1024  then you\'d add it like\nthis: \n % heroku config:add HUBOT_CAMPFIRE_ROOMS="42,1024"\n \n Add the subdomain hubot should connect to. If you web URL looks like\n http://mysubdomain.campfirenow.com  then you\'d add it like this: \n % heroku config:add HUBOT_CAMPFIRE_ACCOUNT="mysubdomain"\n \n Restart the bot \n You may want to get comfortable with  heroku logs  and  heroku restart  if\nyou\'re having issues.', 'hubot-tumblr-video \n tumblr-video is a chat bot built on the  Hubot  framework. It was\ninitially generated by  generator-hubot , and configured to be\ndeployed on  Heroku  to get you up and running as quick as possible. \n This README is intended to help get you started. Definitely update and improve\nto talk about your own instance, how to use and deploy, what functionality he\nhas, etc! \n Running tumblr-video Locally \n You can test your hubot by running the following, however some plugins will not\nbehave as expected unless the  environment variables  they rely\nupon have been set. \n You can start tumblr-video locally by running: \n % bin/hubot\n \n You\'ll see some start up output and a prompt: \n [Sat Feb 28 2015 12:38:27 GMT+0000 (GMT)] INFO Using default redis on localhost:6379\ntumblr-video>\n \n Then you can interact with tumblr-video by typing  tumblr-video help . \n tumblr-video> tumblr-video help\ntumblr-video animate me <query> - The same thing as `image me`, except adds [snip]\ntumblr-video help - Displays all of the help commands that tumblr-video knows about.\n...\n \n Configuration \n A few scripts (including some installed by default) require environment\nvariables to be set as a simple form of configuration. \n Each script should have a commented header which contains a "Configuration"\nsection that explains which values it requires to be placed in which variable.\nWhen you have lots of scripts installed this process can be quite labour\nintensive. The following shell command can be used as a stop gap until an\neasier way to do this has been implemented. \n grep -o \'hubot-[a-z0-9_-]\\+\' external-scripts.json | \\\n  xargs -n1 -I {} sh -c \'sed -n "/^# Configuration/,/^#$/ s/^/{} /p" \\\n      $(find node_modules/{}/ -name "*.coffee")\' | \\\n    awk -F \'#\' \'{ printf "%-25s %s\\n", $1, $2 }\'\n \n How to set environment variables will be specific to your operating system.\nRather than recreate the various methods and best practices in achieving this,\nit\'s suggested that you search for a dedicated guide focused on your OS. \n Scripting \n An example script is included at  scripts/example.coffee , so check it out to\nget started, along with the  Scripting Guide . \n For many common tasks, there\'s a good chance someone has already one to do just\nthe thing. \n external-scripts \n There will inevitably be functionality that everyone will want. Instead of\nwriting it yourself, you can use existing plugins. \n Hubot is able to load plugins from third-party  npm  packages. This is the\nrecommended way to add functionality to your hubot. You can get a list of\navailable hubot plugins on  npmjs.com  or by using  npm search : \n % npm search hubot-scripts panda\nNAME             DESCRIPTION                        AUTHOR DATE       VERSION KEYWORDS\nhubot-pandapanda a hubot script for panda responses =missu 2014-11-30 0.9.2   hubot hubot-scripts panda\n...\n \n To use a package, check the package\'s documentation, but in general it is: \n \n Use  npm install --save  to add the package to  package.json  and install it \n Add the package name to  external-scripts.json  as a double quoted string \n \n You can review  external-scripts.json  to see what is included by default. \n Advanced Usage \n It is also possible to define  external-scripts.json  as an object to\nexplicitly specify which scripts from a package should be included. The example\nbelow, for example, will only activate two of the six available scripts inside\nthe  hubot-fun  plugin, but all four of those in  hubot-auto-deploy . \n json\n{\n  "hubot-fun": [\n    "crazy",\n    "thanks"\n  ],\n  "hubot-auto-deploy": "*"\n} \n Be aware that not all plugins support this usage and will typically fallback\nto including all scripts. \n hubot-scripts \n Before hubot plugin packages were adopted, most plugins were held in the\n hubot-scripts  package. Some of these plugins have yet to be\nmigrated to their own packages. They can still be used but the setup is a bit\ndifferent. \n To enable scripts from the hubot-scripts package, add the script name with\nextension as a double quoted string to the  hubot-scripts.json  file in this\nrepo. \n Persistence \n If you are going to use the  hubot-redis-brain  package (strongly suggested),\nyou will need to add the Redis to Go addon on Heroku which requires a verified\naccount or you can create an account at  Redis to Go  and manually\nset the  REDISTOGO_URL  variable. \n % heroku config:add REDISTOGO_URL="..."\n \n If you don\'t need any persistence feel free to remove the  hubot-redis-brain \nfrom  external-scripts.json  and you don\'t need to worry about redis at all. \n Adapters \n Adapters are the interface to the service you want your hubot to run on, such\nas Campfire or IRC. There are a number of third party adapters that the\ncommunity have contributed. Check  Hubot Adapters  for the\navailable ones. \n If you would like to run a non-Campfire or shell adapter you will need to add\nthe adapter package as a dependency to the  package.json  file in the\n dependencies  section. \n Once you\'ve added the dependency with  npm install --save  to install it you\ncan then run hubot with the adapter. \n % bin/hubot -a <adapter>\n \n Where  <adapter>  is the name of your adapter without the  hubot-  prefix. \n Deployment \n % heroku create --stack cedar\n% git push heroku master\n \n If your Heroku account has been verified you can run the following to enable\nand add the Redis to Go addon to your app. \n % heroku addons:add redistogo:nano\n \n If you run into any problems, checkout Heroku\'s  docs . \n You\'ll need to edit the  Procfile  to set the name of your hubot. \n More detailed documentation can be found on the  deploying hubot onto\nHeroku  wiki page. \n Deploying to UNIX or Windows \n If you would like to deploy to either a UNIX operating system or Windows.\nPlease check out the  deploying hubot onto UNIX  and  deploying\nhubot onto Windows  wiki pages. \n Campfire Variables \n If you are using the Campfire adapter you will need to set some environment\nvariables. If not, refer to your adapter documentation for how to configure it,\nlinks to the adapters can be found on  Hubot Adapters . \n Create a separate Campfire user for your bot and get their token from the web\nUI. \n % heroku config:add HUBOT_CAMPFIRE_TOKEN="..."\n \n Get the numeric IDs of the rooms you want the bot to join, comma delimited. If\nyou want the bot to connect to  https://mysubdomain.campfirenow.com/room/42 \nand  https://mysubdomain.campfirenow.com/room/1024  then you\'d add it like\nthis: \n % heroku config:add HUBOT_CAMPFIRE_ROOMS="42,1024"\n \n Add the subdomain hubot should connect to. If you web URL looks like\n http://mysubdomain.campfirenow.com  then you\'d add it like this: \n % heroku config:add HUBOT_CAMPFIRE_ACCOUNT="mysubdomain"\n \n Restart the bot \n You may want to get comfortable with  heroku logs  and  heroku restart  if\nyou\'re having issues.', "sticky-theatre \n Virtually have a shitty experience at a movie theater.\nTested with Google Cardboard. \n Customize your own venue \n Make your own shitty theater with your favorite software that exports\nthree.js JSON (I'm using Blender). \n There must be a mesh named 'screen'. This is the screen. It should probably\nbe a plane but use your imagination. \n There must also be another mesh named 'viewer'. The origin of this mesh shall\nbe the viewpoint from which you (un)enjoy the movie. This mesh will be\nhidden by the code.", 'doppelchat \n A peer-to-peer chat app where message relevance is determined\nby similarity of the user image avatars. The working title\n"doppelchat" reflects that I wanted to make a chat application\nthat enabled people to talk with a doppelgänger. \n P2P \n The P2P networking uses WebRTC so it currently limits browsers\nto Chrome and Firefox. The great  PeerJS \nlibrary is used to smooth things over in the client and I\'m\nalso running an instance of their PeerServer to enable more\nconnections and an index of peers. \n Image similarity \n convnetjs  is used to\nextract a feature vector from the user image. I\'m using the demo\n cifar10-solving CNN  \nwith the 80% accurate pre-trained weights. \n UI \n I\'m using this project to try out React. There\'s probably a\nbunch of rookie mistakes in there.', 'neural image analogies \n \n \n \n This is basically an implementation of this  "Image Analogies" paper , In our case, we use feature maps from VGG16. The patch matching and blending is inspired by the method described in  "Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis" . Effects similar to that paper can be achieved by turning off the analogy loss (or leave it on!)  --analogy-w=0  and turning on the B/B\' content weighting via the  --b-content-w  parameter. Also, instead of using brute-force patch matching\nwe use the  PatchMatch algorithm \nto approximate the best patch matches. Brute-force matching can be re-enabled by setting\n --model=brute \n The initial code was adapted from the Keras "neural style transfer" example. \n The example arch images are from the  "Image Analogies" website .\nThey have some other good examples from their own implementation which\nare worth a look. Their paper discusses the various applications of image\nanalogies so you might want to take a look for inspiration. \n Installation \n This requires either   TensorFlow \nor  Theano . If you don\'t\nhave a GPU you\'ll want to use TensorFlow. GPU users may find to Theano to be\nfaster at the expense of longer startup times. Here\'s the  Theano GPU guide . \n Here\'s how to  configure the backend with Keras  and\nset your default device (e.g. cpu, gpu0). \n To install via  virtualenv  run the following commands. \n virtualenv venv\nsource venv/bin/activate\npip install neural-image-analogies \n If you have trouble with the above method, follow these directions to  Install latest keras and theano or TensorFlow \n The script  make_image_analogy.py  should now be on your path. \n Before running this script , download the  weights for the VGG16 model . This file contains only the convolutional layers of VGG16 which is 10% of the full size.  Original source of full weights .\nThe script assumes the weights are in the current working directory. If you place\nthem somewhere else make sure to pass the  --vgg-weights=<location-of-the-weights.h5>  parameter or set the  VGG_WEIGHT_PATH  environment variable. \n Example script usage:\n make_image_analogy.py image-A image-A-prime image-B prefix_for_output \n e.g.: \n make_image_analogy.py images/arch-mask.jpg images/arch.jpg images/arch-newmask.jpg out/arch \n The examples directory has a script,  render_example.sh  which accepts an example\nname prefix and, optionally the location of your vgg weights. \n ./render_example.sh arch /path/to/your/weights.h5 \n Currently, A and A\' must be the same size, the same holds for B and B\'.\nOutput size is the same as Image B, unless specified otherwise. \n It\'s too slow \n If you\'re not using a GPU, use TensorFlow. My Macbook Pro with with can render a\n512x512 image in approximately 12 minutes using TensorFlow and --mrf-w=0. Here\nare some other options which mostly trade quality for speed. \n \n If you\'re using Theano enable openmp threading by using env variables  THEANO_FLAGS=\'openmp=1\'   OMP_NUM_THREADS=<cpu_num> . You can read more about multi-core support  here . \n set  --mrf-w=0  to skip optimization of local coherence \n use fewer feature layers by setting  --mrf-layers=conv4_1  and/or  --analogy-layers=conv4_1  (or other layers) which will consider half as many feature layers. \n generate a smaller image by either using a smaller source Image B, or setting\n  the  --width  or  --height  parameters. \n ensure you\'re not using  --model=brute  which needs a powerful GPU \n \n I want it to look better \n The default settings are somewhat lowered to give the average user a better chance\nat generating something on whatever computer they may have. If you have a powerful GPU\nthen here are some options for nicer output:\n *  --model=brute  will turn on brute-force patch-matching and will be done on GPU. This is Theano-only (default=patchmatch)\n *  --patch-size=3  this will allow for much nicer-looking details (default=1)\n *  --mrf-layers=conv1_1,conv2_1,...  add more layers to the mix (also  analogy-layers  and  content-layers ) \n Parameters \n \n --width Sets image output max width \n --height Sets image output max height \n --scales Run at N different scales \n --iters Number of iterations per scale \n --min-scale Smallest scale to iterate \n --mrf-w Weight for MRF loss between A\' and B\' \n --analogy-w Weight for analogy loss \n --b-content-w Weight for content loss between B and B\' \n --tv-w Weight for total variation loss \n --vgg-weights Path to VGG16 weights \n --a-scale-mode Method of scaling A and A\' relative to B \n \n \n \'match\': force A to be the same size as B regardless of aspect ratio (former default) \n \n \n \n \n \'ratio\': apply scale imposed by width/height params on B to A (current default) \n \n \n \n \n \'none\': leave A/A\' alone \n \n \n --a-scale Additional scale factor for A and A\' \n --pool-mode Pooling style used by VGG \n \n \n \'avg\': average pooling - generally smoother results \n \n \n \n \n \'max\': max pooling - more noisy but maybe that\'s what you want (original default) \n \n \n --contrast adjust the contrast of the output by removing the bottom x percentile\n    and scaling by the (100 - x)th percentile. Defaults to 0.02 \n --output-full Output all intermediate images at full size regardless of actual scale \n --analogy-layers Comma-separated list of layer names to be used for the analogy loss (default: "conv3_1,conv_4_1") \n --mrf-layers Comma-separated list of layer names to be used for the MRF loss (default: "conv3_1,conv_4_1") \n --content-layers Comma-separated list of layer names to be used for the content loss (default: "conv3_1,conv_4_1") \n --patch-size Patch size used for matching (default: 1) \n --use-full-analogy match on all of the analogy patches, instead of combining\n    them into one image (slower/more memory but maybe more accurate) \n --model Select the patch matching model (\'patchmatch\' or \'brute\') patchmatch is\n  the default and requires less GPU memory but is less accurate then brute. \n --nstyle-w Weight for neural style loss between A\' and B\' \n --nstyle-layers Comma-separated list of layer names to be used for the neural style\nThe analogy loss is the amount of influence of B -> A -> A\' -> B\'. It\'s a\nstructure-preserving mapping of Image B into A\' via A. \n \n The MRF loss (or "local coherence") is the influence of B\' -> A\' -> B\'. In the\nparlance of style transfer, this is the style loss which gives texture to the image. \n The B/B\' content loss is set to 0.0 by default. You can get effects similar\nto CNNMRF by turning this up and setting analogy weight to zero. Or leave the\nanalogy loss on for some extra style guidance. \n If you\'d like to only visualize the analogy target to see what\'s happening,\nset the MRF and content loss to zero:  --mrf-w=0 --content-w=0  This is also\nmuch faster as MRF loss is the slowest part of the algorithm. \n License \n The code for this implementation is provided under the MIT license. \n The suggested VGG16 weights are originally from  here  and are\nlicensed http://creativecommons.org/licenses/by-nc/4.0/ Open a ticket if you\nhave a suggestion for a more free-as-in-free-speech license. \n The attributions for the example art can be found in  examples/images/ATTRIBUTIONS.md', "Bob Loss, neural painter \n We don't make mistakes, we just have happy accidents.", "Keras VGG buddy \n Lends a hand in adding a trained VGG16 network to your  Keras  model. \n Pre-trained model \n Here's a download \nwhich contains only the convolutional layers of VGG16, cutting it down to 10% of the full size.\nIf you want to use the fully-connected layers download the full set of parameters from:\n original source of weights . \n This code was originally adapted from the Keras neural style example. The weights\nare available under the  CC BY-NC 4.0 \nlicense.", 'Real-Time Style Transfer with Keras \n \n This is an attempt at implementing something like  Real-Time Style Transfer \nwith the  Keras  framework. \n Install \n \n pip install keras-rtst \n Download  pre-trained VGG16 weights  You\'ll need to pass its path as a parameter to the scripts. \n Training currently only supported on Theano backend, but texturing can be done\n with either. \n \n Usage \n After installation you\'ll find  train-rtst.sh ,  render-rtst.sh  and  rtst.py  on your path.\nThe shell scripts are just wrappers around  rtst.py  to demonstrate usage and maybe be a little convenient. There\'s also a script  rtst-download-training-images.sh  that\nwill download a small batch of images randomly selected from a subset of ImageNet 2012. \n Examples \n There\'s an examples folder. Example of an example: \n Train a brick texturizer:  ./make-example-texturizer.sh bricks0 path/to/training/images path/to/evaluation/images path/to/vgg16/weights.h5 \n Texturize a gif with that brick texturizer:  VGG_WEIGHTS=/path/to/vgg.h5 ./texturize-gif.sh path/to/your.gif bricks0 out/bricks0gif \n Differences from the paper \n \n This code doesn\'t use strided convolutions for upsampling as it doesn\'t seem to be implemented in Keras/Theano. \n The learning rate starts at 0.1 and decays at a rate of 0.9 every 200 iterations until it reaches 0.001. \n Also similarly to "Texture Networks" I\'m using a really small training set. \n I\'ve added  MRFRegularizer  and  AnalogyRegularizer  which add losses for patch-wise\n markov random fields and image analogies. Use  --style-map-path=/your/image.jpg \n to specify "image A" in image analogy parlance ( --style-path  corresponds to\n "Image A prime") \n --model=girthy  adds a series of residual blocks at each depth instead of just\n the bottom-most scale. Set maximum depth with  --depth  and the peak number of\n convolution filters with  --num-res-filters . The number of filters is halved\n at each larger scale. \n', 'Visual-Semantic Embedding with Keras \n Keras  implementation of https://github.com/ryankiros/visual-semantic-embedding \n Pre-trained model \n \n Visual-Semantic Embedding \n', 'VR hands for Unity3D \n Some placeholder hands I made with MakeHuman for VR purposes. \n Usage \n Download a  pre-built package \n Contribute \n Make a PR with new hands/textures/animations and a demo scene.\nNothing fancy, just copy the default one and swap in your new stuff. ', "qbrush \n Draw images using reinforcement learning. \n Results \n The computer was asked to draw a simple diagonal line.\nAfter each step, a reward based on the mean squared error between feature vectors\nextracted with VGG16 from the current canvas and the target image was given. \n \n \n Nice try, Picasso. \n \n Don't quit your day job, Renoir.", 'yarp \n Not much to see here.', 'gym-qbrush \n Drawing environments for OpenAI Gym. \n Environments \n The state consists of the channel-wise concatenation of the current canvas,\nthe target image, and the "position map": a single channel with the same spatial dimensions as the images which represents the history of the write head. \n The agent may move in the cardinal directions or choose to stop drawing. \n QBrush-Final-v0 \n At each step, a reward of -1 is given. Another reward is given when the drawing\nis complete based on how many times less error there is between the generated\nimage and the target image compared to a random image. \n QBrush-Step-v0 \n At each step, if the current canvas error is less than the previous best error\nthen a reward proporitional to the ratio of the previous best to current best error\nis given. Otherwise a small negative reward is given.', "deepascii \n Generates ASCII art from images using features extracted from a deep neural net (VGG16) \n Install \n pip install deepascii \n Usage \n Basic usage:  make_deepascii.py --font=<FONT_FILENAME> <IMAGE_TO_TRANSFORM> \n A monospaced font (DroidSansMono.ttf) is included in the examples directory. \n There are a number of options: \n \n --font-size=<INT>  Tweak font size to fit your terminal \n --layer=<LAYER_NAME>  Name of VGG16 layer to take features from \n --pool=<INT>  Integer downscaling factor \n --pool-type=<avg|max>  Type of pooling (avg or max) \n --invert-color  Invert to assume black text on a white background \n \n Examples \n All of the following are using the  --invert-color  to correspond with github's\nrendering of this readme file. \n \n Predicting...\n                         _ydfP___aa#NPPRKWZT$NT$NPTTPPW$RCWMTPRVPT5mKD#jaMRT`` _.wRN$TTTTTTTZZNNF$F$L___j_                   \n                    _y/ _d$FagW#NPPj_____ZDRDRRRP\\7F`q7RTq$PTuqFuWPLV_y/CDF_~_a#T$aKKVP__ap#PT$m#PNf55mw#kg___               \n                   _y/ _4PLapg__.aappVPHMDkRNkP``3k_q7RP`W$W aPNP$DK_dRNPFyhWK_pw#F____a#P__z_I___apAPPTNNNNhkL__            \n                  _V`_j4A$MPP+__IIZ$NDhm.TRRPFW8WXC\\QFWa+d$WWT $HZLWMRMF5WGPhW4PF_a/udPF_udRP__aAMPC_appppLZZ7Mmp_           \n                 _/ _yd$pPF5aam##RRRRTTTl5FF\\W3L~`\\VRC.88d$LW$Wd9PNZFPT$MN$KWMBFWdFWdPFuWZD/__Z$#F_JN$NNNNNNHNNMNL_          \n                 4 _d4$PCLp#T`Sa-___.2g#F5Bb8F#\\uc 7bc. 7FW8 3TW7TF qfRd$W7P Ld9FWTFWTRMRbdM$PFWp1N$EZNNNNELNNM$k   \n /  /$$$gdP__aw4$W#PR$PRFFPFR88ka8#h+W88GQmmL. 3``3TP\\W3$WTR$F _d$PF7PWdP TW$R9RFWdLWdPN$pNNNILLNN$Mk_       \n               J  /AdPNPCkd#RDPRF_aMRkPF~ys@8K 5su.3a389RGZ2~~~~9P~8.WR$g3TFWd8TRs```TRWa7TF` TPPPW75WfN$NNNNhpZNM$gL       \n                _|RN$kCpMP_awPF5a#F5DPF_we-@FW.P38P`3 .. \\wmmmwa.``_.IP\\3PW88$@~-d8P8`58Q. 7TP~Rd9R  7R$ENN$L__9$D$L      \n               _d$W4$u#FF_dT5k `_ g$$kWPF\\s.....`` as.s.`.` T @+W``\\.P~_ TRk__ 3L \\ac aZh888 7T$NNPPFg_jEN$L_    \n               jTDfj#F_pNMFW__aw/5dPFSEWmc```v~j. s@`\\```` .`  _ a  ` -` ~mmm 3#hK + T$ . T .38~39am  T$$ggg_\\UAVA$k  x \n              W|K|WZ$pP__Jqam#FLp#F_gRFF sc..aeL.8L ... . umwm.           ```__``` _ ~-   3...`\\87P W4$ZNNmWZ7RU$$L   \n              jfN$_jMFWdRLd$DBkMRkW@RFF.`.`L..`I.$.````   ` ``  .    +     `         ~ . +~  `` ```38.`3888``T$pgZPHLW$Z7$L  \n             W4md$WRCW4FPH$ZT5pPF_DF`8W8@F_j8 .3BF`L `    v v/ `                       `   .. ....``~~.`38~8 W$ZNRLW4kP$qZk  \n             N$Pd$WLWfR$ggPK_4F_g#Lar`WGsoag`W/F_ -_j/    ` _               ~           v  `  + .  ``.~33888  T$EPh__$LLN$L  \n             jT_J$LjWAgN#FLW$FLNRLkFgsLsL.`be``W+ ~++  v-   `             |      +   .  `       `._ `_`3.3.`3 W$$LWAkZTNhA\\L \n             | qmdK4Ld$$FWZMP|P_pPF@g@@F`bc@P`_.  ..  ..  ~-            _ `          `  .  .3-  3As-~~~``oa.`  f$Rk`7$mZEW4\\L\n            WlL_$pL_jZECaZF_amu#F5pa#F`scC@\\_ `F  j/ _``    ~           ++ +     +    ._` j_     _` o-am.``..q d$KC_4Z$N$hg$k\n            W$LW$LAW$PjZP_W#$PF`_$KFLsL.@RLaxkc. Wf  ~ .. a. .`_               _     ``~+Wd+ .   q. ..`.3s``~3 d$L7qgJ$d$NN$k\n            d$L j$p4RWP5a#C.`__dRRF@#RW/`.QZ`_38L      `` ```  +              vA         +   .  _3`.`\\...3-_``3W`_ 4OhLN$LQ$ \n            J$E W$LTNZLdPhdFWdPLpFL8ks8FW.Q3FWx         _.. $ w- -    . .  ..... ` -  ___y  `` .`` .`.Z/`.um~\\.Wq__W4N$LA$4K \n            |4$LWA$PHNHRLdRkdP5#FWK5#F`.`.`$y/` `     m /`      a-    ` ` ````P`~     y~+\\.....____` `$._-``_.~ \\qg_ZZZ$W$$F \n            d4ZNLWT___ammFC_SpKPFSWR$mcL_`.38@~ `y  ` ``_.__  . `   ~~   o     y         a/ ``~+~W4kwa/`~-`~Wfb Wd$LpA$Pj$Kk \n             TmkwK_+hg_ILaKWPF_5aBBBFFRbLWam/_~  /      W``` ``V~++-w..    _             `..mm    `_ ``..  `_.. q_TqLWjqmMK  \n              $$_aaL_q#KjFW`_gpRBRBF`.@PF-`$@~+v .wx         v       ```   ~    ~am e .  .`` .. .`2~_.ae`` ~W.3 W$.7$hq$RT   \n         _.ae-3lqZ__gZ$WRL_ga#RBPF`L`8F- g.$E`. .`   _y                             `. ~ 3P`` ao -|qoW|$W4qjPN/   \n         ylL_```X$mn##FP_W#P$kKF WsEs.oo.j8F .   _`je _____                _      P~~ - -      . _ .ac` ..q/`W$gLWaBk/    \n         fRRk_  ``5VmN$wmpPKJFFF~~388L`.aaF y____--x~y/_yr@\\ss--.______                         .`_ 3 ``\\__..3. W4$LW7`      \n        |1`T9kL \\ /T$_$W7F_aRkKL`o8@8F.`3`L_ggaA@@x`amwW6wwMQ$LL_gpaggg-___               _____ __yx_____g@ `3. W4$$KK       \n        |1 j7R\\L@~ ~ff__gamBRBF@F@Fo8k`WZ_.dmRGw#K`W__________99#RPPNMRkMk`.          ____yx_gggg___ggssgggg.`3 W$$$C        \n        |_ j$W`\\L  \\|aadRBP5RF .s-.o8q.3aW8P557`___awm########mk`#~W8IL__W@L`kamk.g_  j+~WAGWd6BMm#mBbBP@8@8bL. W$$R\\x.      \n        4$ 3RL d$L `TT57T75KyL@L.._`-3bW8F`qs__ua#P`__________`\\\\___`TfRbg._@@``.A6~  |_.ggL_`_9PPPPPRBb#mP@96b W$BK@8k      \n         \\yF__ `f$L `fF\\4uiWRk````ak d$./_-daWWT`__aawm#@####ww._9bc__gg2RMk~~   .  __i##@@aawmmmmw.L___``\\wg3$ W$BN/F       \n         |$kqg wvqL._`3q$dNKKWgL@s`` aT`~+.....aa#P`\\4Famwrd$ `\\bL_9MMMRmK3L       _adBBaa#F`________9huL__y98F d$N/`        \n         |F 3$wwWMRkW/ dN$KT jF@L`spa.`   `39#P\\$C   d$CZ_a4P  ``9Aq./`_j;`$k      dBRF~_`wwqy#@N|N#hwL29kkom8sWd$/          \n        W| _g__3F`37F\\m~WfF\\W`Ssk@P_3\\    `f.. `3$k-__`~#PF` -w++su7PL-dfF3$      WTCawe##F WfGvwmW$L`T\\mZW8R8FW4$/          \n         \\yW6mg_  T$ |F3``58Log8EE-aE`. _   3$w~``TM@mwwx aaGG-wm``GxK3MF       W$F~wmmxc  \\KZZ_2PF ud7FWGW8.W$$\\ |  A$k  q$WjL .2``5Lgg@\\@``.   3.~sv```___ _I$a@#8WP``qf~xL_    3\\ ```Z`` ``TPP``___23mL.`. q$R\\          \n          f$L_3mk.j$ fP@ZL.g~888bGc__`@x`    `am`~swwmmmawawmmW#BB@#7`` .3: ``~   q$P\\WA8Rb2``+wmwmwmmd8FZF``~ d$W$L         \n          |7\\WZ `\\vgL`$@8L`s.o38F@@@6kE_   _  `3+~j ``TT`TTTTT`~~``     ``.3s.    d$L`\\L``3Rmag_____amR$@Pk `.WfF@4L         \n          \\|`P\\k `TN$W~ss@-qs.qsLpoa8F@@k- 4   ` w4+  ``   3P~       ``o.`38P`    `$EW/`\\k_`TTRRMM##RF~f\\. ..`qILj/          \n           1L_`$`  ``|gFF`.@3PW85Rj@8F@EL..` /    `   7`           ._.._`~_$L@+  |`3$g`  `8._. `s.`\\s#ae/` ..gd$w4/          \n           \\$eW3k@~wWfEwaoE@8baoGggsg@8hC@WE`` .      `            `X|g@sud#F_   +wm8$W   ` ``\\``C./Rs/`   |igdRa/           \n            A$_`  ./ q$PE`G888@@@Ry8@bs`2EL``  ``  v             _j.w36kE.``WmL  _``$$K `\\o  am  2``_`     |lqp@/`           \n             `\\q_____j$EFos.X`..~W8@s..W6hsF```   -   v       .-gg3RWP``8$@\\. aL`+~vd$\\L  `. `. 3~ -yx  ~-y3W#R/             \n              `3\\wwwweAF@@@FWo88L`amgL..`a$F-`_      -     _ amWQyF``` _`$ @/  j;  .3$8\\   `  `   ..o`  _dQB8gj/ T$L@s-~.. 3bs``qL 3b@3L @#m-  i __   ad@#  q$____________ _  ___j8$WkL   v . ~~ydB8B3Q4k I$F@@s@8FqsP .g3F. 8@P\\c ``\\  ~~   @#F    \\qL_ga..___  x y g_g_yF sy ....q8P89@3/             \n                    WT$bs#c@@oqE oqgL@r. \\-@+k      3#F   ~   7P##PPPNmmL._____aapmK  jE x      _..d$qaaa/               \n                     |$8G.Lss.3$W3QZEL``_aoo- .__   __am/ ___     __      T7Rkgwa##9PPF   \\``.  j ~~.|:3gj/   \n                     |$QB8L@@s@8WCq8RLou/C. ..   g@#F .WA\\L .aw#F aw- ``IPd$NPR#~F k ..Ay__  v- - y3qj/                 \n                     |$8E8EL@@8q. Q$E``8FyL_.2 __aPjsKua#Ay ~j.___x_ awmP . $C j_  aF-u$F yg   3~q3F                 \n                     |$88Bgb@E@@FWd$6k@Ga6@W/ umP\\aeLL T__g~w+y+W## W#/``9+KCWF``fyg-__.2P\\ Wjy   aq./8qj/                  \n                     |$83Gaeggpqswq8L@oGGq.. /8  yP\\L.xq _ .sus....as.. _ ymc@Q@P~~ssq$pL. 39~qa7F                   \n                     j$~3dB$gBL@$L3ak-.q.3$L.s. j$.____uaam@@P_____````Q$kxggawmmmi.g__\\vcg__W#FF ~~~qg4F    ___.A.88366bg@9kW3$L_P\\d$L@/  ||P####F TRmm@@#mwmwmauLZ___________@~q___8W#$F   _._j3/   \n               ___ad#KL.8a8Q$qgLLssd$Lg T$L_s  d$L__  ~.. RRmL___```TP############~wa#RkKZa$k   yyg38F                     \n              _yfRRN$K ~~8`3B9#$LK8L38\\c j3\\ggL _3+6Mux```\\._`3RRMmwcuau.a._____gygdmWT T7 7F   _ydR8F   __d$PPNZ$L _ooq8R879$Lgb@#8DWA\\`Ak6W@F`sZ qe_ 3mu___ 7PPP99PRR##M9RmmB#P\\Q8F _y/ ___d#Ry/      .                \n           _y/$$$ppN4k .AsP\\8\\8dRMELj@88L 3+W $6ue. ~_ WmG 36wae_________________.ua#F yac _ap#P_yF      ``                \n          _y/ 4$$NNN4k ~W ods 3SdG3gpg___s._g_``T g.~-~mug.. .. TR#RW6k@Wm6kgpwamwmBPF\\guwQ$w#PCSu#F __~se  _yfP d$$NNN4k @@anom9+8879$FgLgk~g__g_sgge- Av.2@9kW 3~~~~sWIBW07FWPRBPTFFF g38Kq3F uaWQ8F  @$@                   \n        _jfFL_W$$Z$g$k `.2L`\\.ao8\\qiW9RkgL@##6W@gsL__.``~+F\\.`+~- ``338RL$aC2c`_`__aa88`_3$L/#`j/F``sF``8` `       `         \n       _df$kP5WT$$$N$k -E38k`3GamWdRq$P9RLL_7588F8ka#CW.~y~`_x__~.P`````~5#R0##ha#~R3P~_g6ECk_a/F `W8L..`                 `  \n      _d$PFLLZ W$$$g$L-L@8L`ag`T388WT$L_2#kgL_9P__gZ$W/_we_3Q@Aka/_;qs_.~j. `` `3.~~` wygjyKu3F``L`.ZSE@`  `       -      `  \n    __dPREppNbLW4M##Rk`E.@sW@9F@R888WTR$L.2$W6k_~##KKLw/`7_`2\\..3k__`3#kW4k .`__``..xuaqy/AW__ d8@o@@@E_ `         ``        \n   _y/RLZR$BBFlWZT` $kWP@sC`~~~s_`\\.d8WTR9K2RSLgg~sLW$$.__s8CW3K _@X@__. `____@@P 2$ /`d6k/_jL. -``.@8@@@`                   \n  _j4$FBbWBKILgZRL _jLk@~@Rkg~`aeL388``5dR5qLZFqe___WW8\\_y/_.3Aku/\\-.g___axw~~C -.__..amZT Wf$L. .@@88@@~-.`.`~              \n _jfP$WZFNELIZd$d$Ld$KkL@+`3RbL``fC.3~39R9P9R$bd55#FgpggamL_AL-.___3C._y#P`\\azA.____L$EZRL j$RNL__@@8@@@@@@@``@oo~- .        \n_a4$RRLZNBBKNBKDU5LW4kF\\L`\\C``AkqZ.X...`3P\\82P9RHKLWRCPbpEgigL.gggyK8kyg__._____iL2N7FDPF W4FENRkL_..@``-`````` . Q. ` ` \nP$BELd$LLL$qZIBLBB$Ld$k@6L@F@g 3RLW+@#R$Wa7 TTd9hWZLP5RKDR6KdE@ggSgpwe_iuamam#RPBDP_g~_d$ENENNkL__   ````3``.`.`.3R  `    \nW$RBPHRR#HFRbRBBRERlU$LyF@sL3EWGs.`vq._`I.`3owamR8C3PPRMREEpP$PPFPN9FAkuaMRMR#P5L_9RLW#F_W4$NNNENRkwq____  ..  __````    .`. \nMBR$pqgpqgLgIqpgpqWRKRmkL@@g.\\.`36mc`\\mmQ8L8`g`5a8@``g`57T9Rbmbumwmmmn##F5BEP`56mggF##PW W$EEEN$EFKW7Nkpg___________    .`@` \n Predicting...\n                                                   __.wd/APPP_____.aawMMMMMNNNNRKKCZZTT7$$TTT77$$WP7TR7TD7PFW`$V7P`1W7$MRTNPRRN7M7TCCS.W/C5DWMA__$WM#PPTT````` __$$w/PNZ$$7TNTPTTTTTTTTTZZZ$$NWWFFFFFFRZlL__I|_ jy                                         \n                                           __yv/ __ygdfPRC$__LaaawamdRZZ2PPPT7TT59+hKwZDRNNLWWNB$kKF57RRPP`__9M#$FKWWJTR$$PRII2fF___am/FFWW/K_SaV#R$$TP  ..____aam##C$$9PP__$TPPP1____gpmmmwmPKTCZ$IDNaMAkkggLL_4LL_______                                 \n                                         __yy// __gd4TCC5aamww##MMMPPFF+_________ZZ2PhhVPN##PPR#PFKQzPPP``_aa/PR$PR\\j|PPRTCuqmFK__aW/PFLWJF__az/fC$7PK_ g+__.aW##T$_jaZKKC5V/K______apmRRRPR5$DqnmmhhWfff55mmekuamkug_______                               \n                                        __yy/` _yd4$PA_ggd$$DNPP_______ZlLLpauu.pammLZ7$kWT5TRP7TF5$rF``  WdfF57TKWW41F   +jj$FhaadKFCLW/___a/KC$W/KCL___Cwv/fC`5_qua#FF_yVKC___uaam#PFF__5wmnd$PPP______Wd$MRNNMRN#mmmeeL___                              \n                                `     __yax/`___d4PPCLLappm+______.aaagggpgg#hhmVPQ$hkWRNNEPPPTP` W7TL.__y77TDWP` Wdf1L  dSS7PhWdPFCLpW___ud/R$UV/Fh\\xAuyW/KC____aa#PF`_y/____aad##PF___.gP_j$7_______ppwMMPPPNTNNNNNNNNhkecL___                           \n                                     __yV/F __yddf$$kaamKKZ$Sawwwm####KKNDRKCZZP5NR7PNNNBRRPF  WdQPk\\LutC ~fC__Wd7FLW\\qgdfFNVZC5mmK__uadfCDV#PFLW/Z$VKCC_aapmw#fFC__y/__aamm#PPF__.agmKLLgj___gqapp#PPPT$___ZZZIZZTT7PNRkkLL_.__  .                     \n                                    __y//` __jd4PA$adMRPRNW++#____IIIII$NN$DhmuL.PR$$RRMPPRKLW8sLWX4CF\\YdD$FW`TwweWd$FLWW\\TT`WW$$WVPF5_WdfR$NVPFFLW/AGVKFkudd#RPPP`__av/CuWd#RPFF__uam#6$Pk___aVmANM#PFCL__aapppuuEZZLZZZ77NMhkmp-__                       \n                                   _yWd/ ___j4lZADdmmRPL_______.aawwmmmmmHHNRRhhkKW7PPPPFF \\sPAL. 3\\Ld3XQZFW~fd#fR3$$LWW\\3rL_J7qd$KwWamMRRMfTFFWWdAU7fFWWdd$$FF``_ag#FKWW4RPFF___ad/R$D/F__agdfNUMPFF5arqjpHNNNNNNNhLLLLZZ77NNRkLL_                       \n                                  _y/4f __yad4ACgd9PPF5SuaaawwmmM##RRRRRRBTFFT7PPLW7FFF \\CL 3TFF~~F \\VPA\\xLL 87`Wd3$LLWd7$bWdAN7PPWN$PFDPTFT__WMRNV$FKWd9$B$FF WWd$FFWdfPFFF_uaddZC5V/F__jjM$$NVPF__JfANIZRNNNNNNNNNhHpHWPRN7$$kLL_                      \n                                  yd4/ __y/d7Z$$VPPFC5WaMM##PP__________ ~5SCZZZ$WWRB8LL8sbWd$LL__. 3+FP3\\L@.AX8WQ3$FWW389VAAWd$PNda$FFWK q$VMRWd$FFWWVR#RPF WWd$$kWW4$FF1WWd#MR$WV/F__ad$$$NPFL_ W4$Nd$$PNNNNNNNNNNNEUDKNNTNN5$LL_  .                  \n                                 y/4f _yWd4Z$$DPRLL_gWdPP7TTCS....__________ggg++P~@P~;F@9FF@#+KuggL````\\ 7l\\Vcc j97FWW4P~ \\7TRWd7TFF`  qqVPFhWdTELWdTPT7`L__dd$$kWdfPFPWWdTRT$NMRKWWaddm$RKFL_j;AN1Wd$$LDZNNNNNNNNNhEELWd$NNM$AkL._                   \n                               __y/ `__y/A9fPPKC5aappPFF____ggggttppaaawwmmmmKTTTFW88DRK88GC@``\\WmmkLL.._~+f3fFk-C.2Z`_ `\\\\GasddTTWdT7R\\CL  \\7TFRWd7TR|W7TF`` Wa4W7TPFW4TFF qU4PW7PPTRNWMNTN$$PFFWW4;PN1Nd$$hqqENNNNNNNNNNNNNNNNNN7PR\\L.__                 \n                               jy/  __y/ANI$Z$$kWd$PP_____.JZZ$$Dwm##PPRB$NNRPkP~~##~~PR888sLL.22###kwwmWd3T8$RQQbmmmecL_``~~\\8``~3d$TP~\\kcu37TFWd8TRR$V/F \\\\ d9$MRTFWW7TkWwqdfFP`TTTWd$RN$9RRFF5Wd4LWW1N4PRRN$$EZNNNNNNZEEgPHNNON$EO$kk__                 \n                              qy/F __j/A$J$mmgcp9PKCSLaaam+Hjh9MMNPTTCC5ZkKE$DKL```````F~s@~~~+eKCZ````9\\~~~38bWWQQD@@hwwwwwD88mL`Q7~\\C`7bWWW3$R@7RT8d~~kLW`\\$dd9PP`5WWTF~\\dd$BLL`` ad4TPRTTPT1WdMMR$LWWd$TPTNq$LLDNNNNN$$ELL___W4RNU4$LL__                \n                              W// __W/AAjZPFN$$PZ$5wwd##RKCDDPPEB$___ggdfNkkPPPkWW@smme@@8KC`7`55\\sCL...`33.339P#9+KKC22@@~~~+++--480DW+R8CWW3ARq;g`_WT7FFWWd+d#7FR\\sW W773$F~L qQ7W#TF TT \\$d#PP#PFWW77$VWWA$$NLpNNNNNNNhbLLL2T55MM$kkgL._              \n                              V  _yy/f$N4$ELD7K_jgd6DPP ggp$FF___paMRFRRBKFFF __.3BB@@@/F~~~so@@9bbwmmm@am@P8``7 5~hmmu.C ______ ~~__ 59+k/ \\q|Ks.P~\\sLbWdT$88FW~\\s. q|``3TR8WW88CPRTLW`` \\TTTTT7TFWWTTD7 Tf$$$LENNNNNNNhhkKuWd$N$DN$kL__            \n                              j  j|/ARNdZ$hkZ__aJZPP _.J$PPP S$ammPP55$DPPPF samrF~@#SFW..28~8888PPPP9938c5.a. P~~+wwwwmwwwwGG... ```____3$P~\\CQ2/hWWd888$bW@@\\-WWa.D7P~~~~~5$WR\\y.  .7T7TF~+RhWd4RNTL  `TTf$N$EpNNNN$ILL__PR9M$EN$$\\L..            \n                               __j|fP_jd$PPj$uW#AKLCGaam+#~fC5aa##FCC5.gPFF``_aa##RCL@@P``h~~~.``33.-``_..``.z~~~. V#######~+++~~ .. aawdtC.DWXs$D@~~#f888cc ~~++s/`\\\\T`j73WW4$L  |j37TW``55d97P5~~  J77TT$$$LpNNNN$hppLL277$$UZA$kkL_           \n                               _W4ZFLW4$PKuuW#KFC5__WdfTC55k W/T``5 g+jC$.kLWW##F`5\\Wcc....`...  `/`   agg_ sEC``___`~~  . ````````TTT``W3+WWdfF~~+ssZ5R+-_ T3fRtg____ 33iL_~\\qaZP~~~fL `q33/A.x88888 7 __W````TTR$$NNNNNNPPFkkgg_j$EENR$\\LL_          \n                              _jdfFhWdd$$uW#PFCLWKqWJZFEggcC____../KCS$mrKk@#KC L.g~h\\F~~-- -si.  ___44$..`~~--w-. `` -/`      ___`   W.`__`--``T3axc`5$.._`33RR$$ssL.../~A\\..AX4$kC   wd4+F@7RR388~~WsaaaW__   TTT$NDNRRZZIILNKL__A$CZN$$\\LL_        \n j|TT$DYKNjV#PFC_grKhWdfRNWK____..wr/CGam#RFLLSCCkWmxK WvF~~-..  qy.`#~sE ~  .. - ------  ~-.....    ~\\Wwmmme-@`~~39Mmmmv// 3\\F 3f$+-  . 3T 5 I.3338883\\d9Rhavg   7T$$RCILgggggWA\\kLJA$iu3$$kkL.   x.  \n ||I$jJTNU7TC__apKFLWjI$L____aaw##KKCWW/PFF5$mmzh@PR k.L_....``__ ..  atc ```|   .  .. wv/`______   8..   ~~~    3f$9##K. .7TPTPPPF   ``_ ```T`` ---Wd`  ~~-qT~~~~`33T833R$bW  TTT$$$mm4EELL225+V7A$VZNM$kkL_   \\v  \n                             jj|$qjZLWJZ$SaW/PF___J$g_uuaw##FPCL_am#PF`_ggpKRKF```@~Ls.p...`wae/LL.pd$F  __..__.  +-_``\\ aamwwmm.   ~~       ``        .``` ``~__ ```````````  --  ``   ````T`~````3I.....7`\\\\77B3fP   dd$q$PNNNhhmmwZZ1W7ANVN4$$ELL   `   \n                            Wd4$kq|FWW4$ud#PF__aappAquV##K$$L_uam#FKCLWd4BBDPKLW-G.`WseF~~- WXfFWW/P@`F  WdakW-  ``\\W/` W4#PP~~~\\-  ` `  `_       ..  `\\..`    ....        .. ```\\..`..  ..aT0`` .o++--qsmF``38T87``   W7fN$$ZZZTNNRNhLZ_7R$$P25$R\\LL      \n                            W4fFKdlLL_j$M#FFLWdd#KL_W4ZC$2apKWWfRK$DWWW#9RkFFk-c_g;L@~|L``` ``ILL2$L.``  ~~``~` .  /~~  ```   ``2l    .  ax-     3s;  `3+       ~+     _  ~+u    q+~~~-  ~s````  `````33f$.-W`8888888````TT$$mELLZZZFHAqeLLJALU77R$\\L      \n                           _JJ$$Wd$kkud4PFFWWdMPR$LWdOAhm#PPhWdRkk##KKL27PFF`5\\kwaxL`5$bax-  `js-------- ``..``   ```   ___.   --     +  /`       `    /`      ``.  `     ``  -- dT ``...`--``- ````.-+`T$...`33888888   WWT$$$hmqppKP25$gWZA\\VKC24$kL     \n                           Wd4$kWd$PhWMfFL_W4$RFDhHgggPPPPC5LZPPPF_`5k~P~CQ.WW4bW#8kWdts##K .d3$8D8FD\\.`_  z;  .   ws.  ../-  ```    ``          ``              .         .  `` `1   ....`  .` ... ````.\\+~~..`3388@88`  W4T$$PNNMPLWW44N$LLZ$5agZRkk  \\  \n                          W/f$$KNd$LKIZP$WWdAPPNWZZ$$$kk _qapPF___.../`C5pKrW@8LL_____$s./___a9###~___s.. asL a;   ~++  ~~``                        .    .      `              ___`  ----  __.  -.- .....33ss 533   WT$$LZP$KL__7f5$$LL_Wd$$LLL     \n                         WWN|$$NWd$|Wq:Pwq4PRNILggggPRPL_WdfPL__ggmmKLwaePP\\W/cSS--aaeeA~LL.27/F___\\..__jyP+- -+   ``` _``     .                         +                  .. aa..  ````  uv. Q. .`` ..~--3338.oW88   Wdf$$EEDPAkL 7T$lL__UA$kkL     \n                          _jjyPWT$$|WN1LWd$FLjggdNNTFF_qug#FFkwqdBRRLLW/CL5./ @@/hyW#8FE .W~/``_...vttC.2/`````__      xu.                      .--4|.  ``        ..            tt    .  `````  j:    . .q.~``TTPRqs-~~88   WT$$$hkPRNqLL_9$$kuLL2J\\\\LL   |7TC_Jd4$LN$iWJ1hWZ$NNBPKL__Wd$EkkgWN$RK$pkKF\\WgcL_ggL..PQfbsvrK v++o `~++mG    vx.      A+Y`                    +-` j|            v--      ..  . ``       -- `   `l.___.  ~|_ . 338``3\\o  W7T$$ElL WA$kgZ7PRNhhV#A4kL_   \n                         _j|/ ___ZZ$|Wj$LPAjI$NgPFF5yrWd$Rkhh$WPPE5V/FLLzL5$kW.Gbe-__@038EL  .@``$L  ````\\\\ .W/~~  q.____    d$.            .   ++  .      ..Q..  .  as...  .3l...Wo----.. X\\   Wd$$RmLL `1NKLLZZIIPRVM4$LL   \n                         Wj|. \\4amdj1Kd$:KuZ$N$$FF_WZCN4fP5VPP__$W#PFLWmyk@R@vFW`CCW.@@@9FK ___. /` __-__  aYL___  ~+---                    ______  +                     `1  1  .   __ .ms.  . 3+ms...--+~~. ..aas.``33  Wd$$PRAkL_`T9$buudZZD1Wd$\\LL  \n                         Wj|; Wff|Wd|NNfFWd$$PPP__j4AkMPK__l__jppKFFEL_Q$DLDR/LL`sso~G..   ..  `` asswv. \\y-   ts.                     wq.__` __  `         . `      `  ax     ``3$ W    ```9+~``````...`3o9P~~..W3  W7T$EPP1kWW_771NN$$LgZL23$\\L  \n                         Wj$;L T__p4$LPA|V4PPCCCLWdfFP__uammuam#PFL5ppaaxPFF``L..2E@`9xE`__  s:L    jj$PP __```1       ++~ ____                 A\\vv- wv         wvm\\     ` ____``  .__         ____``  .o.uoasm@/````...qqo WW4$FKKC__X\\VZT$RNq$EhmW23$k  \n                         W4fFk qqipR$LL_jZ$$kquwwm#PL__aWd$$WMRFFLWmpWW#RKC..`~+nss.-``__y. _`+    .a$/`` .x       ____```  .-                  ```~  `~ ..      ````      --v...___Js..____   .. ___   ~`~WV3P~-.``.~~o.qqy WW7$$PNhmc_2jNVZP$N$RNQNN4$L  \n                        WW|$$  W4$FNAmkWj$PPjjVPPPF__axAA$$PPPF|__4$KKPF`_giLL``D3MKL._Gaxkk ____ WddfF   ++  _..    .. __.J`             ..             ++   _____   .    ``f+hvvwWwm+KL. .      qs..  ..__``.`3\\asc``~~\\qimWWd$$LW`f$K__JA$U2$$EDNNQ$$kL \n                        _jj$k  W4$$LZR$U4$LWdJRKC_S.W/T59#PFF`_SapRRkKLLSamAk-ypW/P\\\\sWdfFF saaxp W```   ``` aamm   +++y----  ...      .  ~              `    a.  .          `` `Z2````F~~yr   `  tam-- oox..-- `~~t_..```jf~ WdffKL WqqaiL_5+H4$$WPN4#Pkk \n                        Wq4$LL  X$$mpNqMPRLWJPFhagm#KC_W/``___gd#RRRFFbW##RRLW.PK```tggZ``` @###F  ``        /``\\   ``` ```   mvr      +  ``         ``_    avm+      `          -mvw .  3ZL    __`3$L````~\\:``_.`33..`  ``T3 ````\\__ dfOAhVZZDA$$LKN4$FF  \n                        Wd$$LL   j$$LN4fE5WVZCLJ4TFC5ap/F__aamKPRILELD88CC`@@m+F__---a4se.___Z``        . - ``      __  --  __``_____  `                      `               v ``2l `;  -+- `  a..`` ..-..3$--oam@P~C_..dd33~Wq___|__JT$2DhLL_d4$ELg4$K   \n                        WT$$$LL Wd4$$WJ$kWPPEWWdR$kWW#PFkWd##PFLjpFRLL.SikxcE3$LW`.W.Q3$~bWax. `          `      ___../Q$$b wwx  `-..       cL_____ _______._____ `   ...    `  __y   ` ```     o~~```jr~~q.Z`@/#~~``uam-W\\Q. W\\\\______d4$MD$iLWdA$kNj$F   \n                       WW|7$$Ek WJ$$$LdZLU$$WWZ$PPCU$$FLWdfFFFLLJPK5pmmR@Q9kkF~~~~.pwaxF`@@#tF  ~~     .         aavm~~j;~   ~~ __y~~w    v +------ awwmm----aaxe.    ~~~   ......ZL__        .  `   yy   aqmg__```.WWf8```\\:.WWqq___jVWdfNN9AkKU2$$N4Kk   \n                        j|N$$$LLWWAf$LJhhmnNmhNRFhWd4RkWW2PFKGVeKLbxPF55$kPF`L```qiL```CSx/``\\ ```     ..  J\\.. @@#KC W4TC.     4$.P` `  `  ``V```   `PP1U``@PPP~~    ```  +yywwm++L_..__... .`____ _j`  ``A\\s... -s.``  ..33~ WW\\qq.gjCZTIZZI$$Wg4$M$FL   \n                        J|7d$$kkW/`T$MFPPTPPI$IZLWJ#PFkW++FC_gXC5WPC55amREL__y// @1F``qqgeELs.L  --   `~ W `A+mm/``   ~`~\\r     df+~     ....   ______```   ``_```         `3T ~y+wwwwmoos.....__.._____3f~sL. \\....qq:B`  `WdtiLppuua$mmmmWd$$$$K    \n                       Wd4447PNNYkWdTTF____5aammwmKFFC KCIDaKCCDPFCSWW#R$\\ksxc___ ___33$FF@~+~  \\y@   ``   ```~~ ______ ``L____```      ++~- .. ----    ```                      ..2 X  +++++wwavkW waa//Fhooo.`~~~~df$Boo WW7d$mLLpWdRNIPNjg4$$Kk    \n                        WA\\$$LL__L_JT___._.ddfR##PFL5._qapmRhbKKCLgg$KC@D#kR6bk--``-qg2Z$FF``C````Y0       ___ `..a--wav/  w--  2i   1          Jt  . `       31   q++~   ....```````W9fFW~~HtfL ```~~.`3/```~~Y~ W\\V7T$hqgPTO4$LLjgQ$RK     \n                    _    JT$$mmnwkW/__axrnyP__5$TK5arKuW4PFF7`C5ggpB$kk###FFR+kLLp WdaeE/  yr    xr         . wWW @@~+  WV#AW44m-- u....     ____       ~-              dT \\L.wammm ~      _  .``FL..`..~~~~~__``.3  W|| 7TNqlLpWA$hqmmm$KK     \n                    4.   WdTff$D#KK___2T$VZLL.pACuJ/KhW/PCL.._aW4BBMRKKBFF $bmkL.Wdf$EELLLZLL.. ____.  `   ````  __      `  ~~~~wv      `...  ..  ..`____  _.____   `  wvW/`f~\\  `    ax......``__y---`...`../+\\.. Wq|__ N4$kKL_2$N$RM7FF     \n                          WTT$$ZF__aaiL.__qampPKjV/FNVXKC_ggppmBBBRRBPFF~L__@@###k-W``$$F@+---uxr `..aax             xv   v              `````~       ~~~  ~   o+-wvv- ae- ... __  .2`` ``....____`2~~~~~j.__a3/P` ~~~ovWq``3s  WjjxL.Wff$bWqg$$TR7TF  `   \n                     _______jjf|__gd##kkuuW4$FLWJZFL_____g4ODRRRBFFR8TF``--o/``````.`33$F```````` yr~~~~   _____     ~t.@3r                      ..   ``` - 3fk ~@@~Wvm ..  --    wwmmmww.--GGGE-u3i.WdtCL___.. __y..g_ WJ|$m;W`TILWj$m7KQJKL      \n                   __..a---`JI$._qgZP`T5Vggj$LWg+FL____aap##RFBBDPF```L_ 88kk....-g.33$L..___. ../   ____y--      jr                        ~         --``_ ~  ```    ``33P ~~~ @++~~W3fF~ $$  aav-``-./|qqs.Wj|$B$kWdq$Wd3fKhV/F       \n                  __yy/##m~WWdAwqqmLL..2Z$$2DkWK$Fk_gapd#KFFPRRP/F jssoz.@/F ~ gggZF~~`oxx. ~~         ygCC_.   __   _   ___   __                                    `amwu.   _______   ```` 37``___j.W@+~    |||3d+mWq4$ZlLWWAqqjJ/KU7FF       \n                  yygZC__``````RXN$$bppAqmntfFPPP_WW4A#PR$LKKDKF`` Wtgggss..L.... qy8RE. X\\L`. .    __ Xt$gg___________        4                        _            4f#~~+   ------     ..    ..   ``  ...zr.```....`qqV/``Wd4$E$LL Waam/FW7/F        \n                 Wyyamme.._   ```T\\4$KKD$TPRTF____W2Z5TFQ$RKkFF`  .d$$EF@g/L@+~~~_Q2ZF~~_..    yy________.__j$pk _ygss--__________   ___         .           .           `  ````  ```         -  ..3+` .... ~++f$L.    .~`33r`` Wdj$$g$L_Wd#fFkW/`         \n                 W|fM9R\\kLL_    ```TT\\\\am1KN$Luaaeu$DkkLj$RKRTF`k~o@33$W@8CL```..aa$$F` yxL_______-------- y4/P__yxm+~~ggggg-__..____________                ~      .                        `  y._``   ass ```\\\\\\____.```d3lL   Wq$$ELLLW77TF //          \n                 ||TTT7R\\\\LL.     ```77$$FFqj;Wd$$V#PFLWJRKLD\\k ~~~`03\\ba8kkc~~~qd3fFL__.`___ggzgg/__Wf`K__2ZCL-W//C___yammmkwggg________________            `_    x+                           4C   ____``  __``\\..../ o.q3$s  WWj$$RmLmy/F`  ``          \n                W||P`TT73$kkL. ``    44TT__5$$KWOPFF`_WafFkkE/L```.os@@@38Fh\\.``2ZZ$C  qygggaxmAF``WW/``uaamwwwW@$GwwwdMQQ$DL______ggsssggggg.._______    `        ``                 ___         ___yy..____....jjyy~`..XqQl.  WW4$$B$$NKK`               \n                W||  ``T77R\\\\L__ \\cW``TTIuaa/PNVf$____V4RKDDKFk W/@@PKGG3ZL@`--uy--~\\ _qispW#PRQLLwWX~Gudd#RRNN99#WW###hwvmmwaaaaaammmw#mqggppccpx---.         .   . .       __     ________________./g__________.2..L___`.3+-  WWd$$BQ##KK       `        \n                J|1   jjT7`R\\\\L._`+~  ~+~~ffF____2aaudZBRR8BFFW@/W@8PGo8#6kW``_2Z`____gdamWR8GGammmKP\\WW/``________________999####RRPPPRNNMMRRkkmmkk_jL __    y    +~~     `\\________yyx_______..-g______ggggggzggggg___.._`I.  WW4$$$$ZEF                 \n                `||   qQ$F``A\\\\kL ``````77T`___aadd##NRkKBBFF`WXCL@--W888$Eb-WWdG..uaadMRRB@@WW#PFR\\ _______...aaaauuuaau...2ZZ7TTTTDDLWW7fPPPMNRRFLGxkW y-  j___ ```.      qqs---__ygg+gggggggggggggggggsp@w###w68Bkaaec-__3+  WW4$$RR$KL__               \n                ||__ -\\j$FWWW3$\\LL_.   \\q__aaamdARN$PPR5RR/Fk .`\\s--W.am@Aq@.@XtqsmWd#PPR5588#7`` ____.aaamwwm###~#######+mmmmeP`~~~~~hWW``Z_____`\\WW+KL`\\kLu3$kL..gg;      j3ss~~vv/A6GWXwddBBBBGwmmmmmmEbL@@PPD@@6@@8RELL.__  Wd4$$B7R\\kg..              \n                qi.. `33$LL  `3$\\kg/   X\\$am+#R$PRR$KCD7Rj/F \\;W`/``./A8-8q\\-_`3398WR8FL53s8.______uaam####PPP``````````TTTPRRRLL WX\\Gamaaeg _ WW3bWW##+~~yvR\\L.    `77` WWMQQ@WMNNN6RRMMMMRBDNMNRMmkkcLgZQ2W88DGqsgp.  Wd4$$$TP`MMbk   .          \n               W44$k _j3$kc   WQ$kLL.  `7ffT`57TTQ7RhyKCQz/L 3iL.`Lg````--d4$bWW3ZFF3$hqye~~___..aw###F _ _ \\\\\\_____ `~~f##9Rhmeeg___ @@@VP````D3XM6b~   __|__..i3Q2Z 7TTR99NNPPRR#PPPPPRBbbwmmm#@~##RR6bL  Wj$$$RKKW@8Nkk            \n f|__yJ/PA\\   X\\$$kgc  W TI$aW/ y/ jZL@zRkWW@Ak--mqC.. .q3$rL@3.F-``-3IEECuuaam##F``_______...uuaau......_____\\\\sg..___````T 75RbbbL. .L..   .x/ 0YF``   _jyggggggais__________________```T88P####KCC 3559$EL Wj$$BRKL@y/PF   \\\\..z/F _ _   jff\\EEL ddf#KS\\a/LqJ$LWZRLW Aave. -44$kL./F_``.WdGSmwWW#PTF_____aaawwammmmww#mmmmmwwwwcL__3#hmmu-_____a.ZZ79RmbkwwWw    .` ..  .  ___jip##R6W@@jaaaaaaamwwwwuuL.._______````T\\\\swaaZ3$EL Wj$$$DKwv//`               \n                  jjyfCL_______```jjyL_ ````\\ Wdj$ujiLVRKFF__`-`--.@3PF~```3$9~+R..._~~~+PP`````___uaww#+KNV7PCCZ___I7R$QPPA\\hxvL_`79RkkuugqzpamqZ7RR\\KK ``    ~  ~  .____.aamRRR___SSDW#######P##~~~~+mwwu.._____``.Q33sBQ8#~~ Wj$$RNNN/F                 \n                  j|$$kkqgge-  u.____L_. ```3Xqq7$VZ$NBRKKk@./L@`pWg/``   `7ZT``~++K...ZZ____.aaaamm#PP``NW4$kwaxuuwrwd$$KL `7f\\bvc__`RWW66amMRRamLPR$$L       `  `   __qggdBBBRuuaam###T``_____________P~~+hmuLL____jGGBRa8F` WWd$$RNV/F`                 \n                  j|rFA\\Wq$EL__++wwaampL___p/A\\\\dqj$DRKkP`Wj;F\\s.P`\\LL____@/A\\s.```~\\++mmwawwwWdQ##RTC`  WdQlLW9AkVPFL_3$DL  ``TfAKuL.._@@@9#####t|Ld4$KL            __qd4BBPRB@@###FF`_____..puaaeuu....ZZ27P\\hmu..._I339P$L~ WWd$$N#/F                   \n                 W|fFF Wd#$kkWd23WWd9RkLuwx/FWW4TN$$LP$LF Wj;L@arL``sg----.```~   ````3399###RRR$CZ``\\   Wd\\$kZZ`ZZ`__a8PFF    ``T9XXmqq.@##F~ _jj$kW3$N\\W           qqdZRRKFB\\VP```__a.wqqymmmWW@@hHmmmpwwe._279\\bkww-33Z8`ss WW4$$V/F                    \n                 W||F  _`3$9XhmmmWW7RR$VW#KLLWd`Wd$$kk\\LL WJrL@@\\L 33pF@WQyL ```     ...2P`````Xq$$L___   `3A\\hwwwmwwe#PF`______l____jjjxv/  __jyy/KW3$//            WdfPFP_____LLaawW#~qjV7RC____.27$$$FPR\\hmeLZ7$RkW@@##B@8eWWd4$$$K                     \n                 4||L____y7``\\3$PP```33#FF\\\\.W~ Wd3fFk\\LLW@fFbsx/LW38FL__j$L        `Xf$.``..  ~33$bb--.___```#####PPF``` ..-wxwAAKCC233PFL  .gddfFFW3$F             W4TFL5wwwmwwe##rF` \\q4/KGGuuammwd$$k ``7R\\kk.ZPFW8BFR8B8FWWd4$8/F                     \n                 d4$L  aaZC__.``    W377F j3rF``Wd7$kW$FkW``53#6FW@/P__..24-_        `TT\\qgg~_..77A9Mkkwwc.._`````````   g~~~```555~~~~`$$.W~~Wd7PFLW3$k\\            WT$Dwxc~~~f5TC``   W4f$W@+XW#fFhd$$kk  ``75$bkKL@3Fb@a8F`WWd$$$CL                     \n                 `4$vkW~fAbggg_    .Vf$` Wj|FW33W7`$58RELL.sg@@8EE`C`.ascc``L  __`    `T1X$$.w~+```TTRMM@hmmk- ...   ..KK$C..-auw-`````5ae+K`3MM8FF ``7`             dj$g/AKGGI.a$u..L  `T\\\\LZZZ_``__2ZPFL  _JJ77PFkWW`GW88RL.Wqd$$4\\L                     \n                  `\\$|. `QQ$Q\\__    |j$  W|||W//```T385bmW@#RG.8AWekWWd6Fk~~s.  x-      1ff$VW`IL__``~~YV7PP`WWm+~wwmm+#+++~WwWMPR\\__aad/RCWWd#RFF _sZZ`__           W4$$C``~~++##++hmm ``3\\+mmwwwwwm#PF`  uW/ART$CDPF@@W~F.~~qqj$R4\\L                     \n                  `jj|._  3$q$kk   ` j$.LWj|;L``_.37`~~PR\\$Lgggg@@\\LW@78FQ.``\\  `v       T3$. -+ms..``3I\\```__````````````I``_3IlL..aa#4QDWW#PFF`  qyFFssxL__        `3$$k  ```IZZZ````` _```T9#####FF``   /`__33$mLF_..`___``Wj$$R44\\L                    \n                   Wq$LL_  ~jjyg__   jyrkWq$;pwwae/~```I338bqa$L--`\\L..``sgW`\\L.@/        ~f\\_``9\\kcL.z~__ggg_.._____________33awaee#Q8$$#W#TF`   qj$F`@3\\\\L..       .22$\\\\L  ~~yymx-__ywwx `````````____..SSaad#A$KwwxKu..a. qjfFF34\\L                    \n                   Wj$$kc.__I$$kkLL. 4$|LWj$$PW@8TF`..g~~+~~qQ8bL@@szg___``N`@e--`   A   ```\\a...`A\\+mwwwamwwmmmssussssaadmmww###88BDa8###F``    .33rL  7P~~~~      vqq$PAX\\WW`QDPPR\\L22@PAhwmmwwxwwa.uwwmmwWd9PF@71P~~~~~~~~WW4$FWW4$LL                   \n                   `JT$$eL__y/#hAmvg__$\\L JI$LL33iL--sC.```33$$Bb@@@AGLq.gE`@@/`    ``     .3++qs.`3$8PP~~~~#99M@#W##m########8B@8.n#P`~~` ``   --+`\\  j.C```      `|33$FFA\\xk~~+600Amecg__````PP+P##+VWfQ5S$3ZFLs$$F`` ``.. Wd$$P@W4$KL                   \n                    W|TR$kWW/$F `AXKk.2___`f$-W##tL@`3~soo..d3$B8R88@@hqsEF@s/`_      ___  /```31k-~~+$````TZ3I3PTTTTTTTTTTFF3~~r~~F``````.    ``````__giL.      +  vdQ$$LW`A\\CC`_~~#R$kksug._______`I__I33m#R$LWa8Pk  .`~q. WdfFF`W3$KL                   \n                    `||7R\\VWQe_  ``\\XmqqiWW3IDK`SS----`\\.g~~d3S$FLszc_@@3\\k@sg-_______jy  `   ./``_``I.      y+-       3sss.. `````       ~~- __  .---y3$kkF    `` `11j$y_W``3+ww..```T99Mmmmmmussguuaaam##KR$$mW#/F`  W `j+ W7$FL__4$k                    \n                    `\\|.`3fRR\\LL  ``T\\\\N$LL`+~c.ss8#`..qzcp.Xq3gLL.Fssa8QDFP`ssmxk-..w3rk  _  ``   avmmm. ```T``_ ..   39P----        ``_````\\ue..``.Q38#FF         `TQ$ELLax/`PX\\v._``3TPP99RRMMMMm####RRFF@+#AKCCL  `|.```_jI$FL_j3/F                    \n                     j||_```R\\$k   ```33$kkW`5z@@@ssooWd6iW-qG3mmm6@@@#ms8LL@zs//` ~~~j1      `__  jfP~~__   \\$-  ~    ``````          .. ` ..~~~s..3d3$F`__s.     ..3df$bW@A\\L ``\\\\... TTT9RRRRTTF5sW Z$aavk  ..._. qq$$Egqjy/                     \n                     d1$L___ j\\LL`\\  ````\\\\Lpp#6@P`\\..@@3$9PWVfF599WVC0@9sFF@@6F______`I  z/    .  ````     y7`   .                  __________```+~~_j$__ wxr    qr~`3$$Qy@/` \\tm.  ___.```IZ2;5~~\\samAwaam#/F   .|..._Wj$$bwa4/F ff$Lg.  \\$LW \\L   jj$s/F`_L...--d3PP~ SS-ZZLj$g@23FWW3RCyg------kxeL   ~~     //    +         ..aqa-----...``..3$kWW/`` | _j33$$L.      ~f\\  w.as.`\\ame-___33rF@@##FF`    -||.saWd$B@@a4K \\$$epWW3fkW@wWmwwWd4$ELLsspwdsL@@36GoaasW#GEu3+sgg/FWWf6qxC@ @@E/`~~   .`         ./`        .      ..  /+9fRQLL@yAv-L.ua##P~      41wavmm3$bwW      dfR\\y /~~`C...//RGmv7```     `|li-Wdd$RGQ3/F                      \n                      `X\\$$LLW3TF@YP`~~~~Wf$$rWaeFF@AhW@A8b8@#8bW3mWdAS88EL.@@PA\\kv-@@+AG.``--mmvw                               +-  `.``\\j$gs.@@@@WM47F``...     `~\\ff6QQ$gyL   Wv  ..  ``_3. ```wwmm+P--~~~``        jj|__j3fRaa//                       \n                       ``\\\\.L_/`L `  .7/``\\q$LW7ZF@``Q3;QQz8F`G@@~A\\y@RrF6bWu`.``$X`L```@sL ~  wu.   _j...w3xmmsL@_@````` wmmc.    `````\\\\Q$$yK \\Lus.  ..aa.    /fT$C _```         |33\\_jyZG@#/F   \n \\iL______yg~   Wj$|L s.._@@+oWo8P-.88CCQQ$;R$E@@@~~\\qa:Fgg___ -  ___ __~~    _ .              ______.jyipWWd89RAbkwec__-_ @PAk..  ..... .lq$$EL      ~~~  ~+~++  uaxZ _  ___    _____yysRqaamKyy/   \n A\\xvL__ --C`____jI$LBskks.28.XQZLL..9~~Wd88b-kcL...Wd6$F\\Gae- \\L -- `    yr      -   __---g-gd3BK@W@PF RQ$$WA\\L.`` NxL  ++mm-wvdd3$$kL  .. . J3++v v- yxe-   -----338dWd#RNj/`                          \n                          ``3\\\\KvL_________J3$FFRcbaggs;qj.gaamrF`VRRQQggspnsvW@RFU@M#Fk ``~x~ `_.  ``   ___/`           ___yy`amq@39PRPWP`~.```~33tL`5ssG.WPA\\L.````|||`Q7$R\\\\L       ++   --  ````  ``_2```  ```\\ydd8$W#7F`34/                           \n                      `     ``3\\\\KwwaLuuaasd3IFL@@W@F66FW3iWW8RELW``5aaenjFP3I.2``W38$LWx/F`\\.  v          .`         ___yaddmWWQQy##FF -  . $$mw@7Qy/  `\\jy.`  \\...3388M\\k  ___ ```  ````      ...a.F`     .yddQBRC __j/F                          \n ~~#+++H#6mmPFFg@g@RG88LWd+~P8S$ggs.@#f\\LjEF~~ssgu@38$LW~LL`ai____   q..        \\   __yyddMMM8KWqQ|ZLL     ``__.__j.  `` `    `3|     +:.33$FR$LL   y        _       q. ~~L......ggd3BBRGqggaj/                            \n                                ```````5TT$$LLLWssL.@~s._j$.``3sbuzcL``5XqCL``3$EW@P3$F_`-WwmmkvC-.   $L   ___      .gd+@d9PPP WWq$LL_        _-_______________j`    ``_j_j$.D$bWwk@/_  ..  ...  ___ j| ``--`~yyqddM88EP@84Q4$K                            \n                          .              \\j$$L@@##bc`@3auxe-W@28@9#EF~~o33$kW@@A$8\\G3$LL`LW@PP~thbx  `3\\k..a...    yad4$BPFF    XX$$______________..--___`--___y   _______jyggj8/F `C . ~~  ~~:  ....4.  ``..`Y|Q38B88GL@3Q$8/k                            \n                                        `jj$$kkFFF@@o@@@#fFLWjs.W/RRC`g@d3TFL`.338@W/Akssc ````` \\\\L ~~+++~ . g__99#KF TA\\$LL _ _ yy  k W4+   yxvvgggy/Ajjy/ \\swy     ~  d|  .....qqd8PP@89sg@34/FF                            \n                                         Wj$$kLL@.g@@pK@ ..qgzpF ....gjP``\\.p@~~ss.``@4\\LL ..  `T   xqs$.//F _   ``3AhmwwwwwwmmmmmuLL_____    ~____  ______LLgy/L   qs.  ____    ..    .y.q.dQQ8FBQ32ZGa3/F   \n                     .                   W4$$EkkssbsvP@~-uoWqa8FF qssqqgEC@jrKC. 3\\kL..#A\\k-~+-   ``  __ ~;    ``aam+KC_    y--   ```T#######PRRPRhhmmmeLL_____________ggipppmm/L j;   ...    +    ../dQTFBqagqaaa// |        ``         W|$$$8bgg@gjL.``@+PHQ$LLWjfFqyxkkW/``b~-``tLz__``\\L```  ..    . -- ___ WV7F``\\v . ````TTTTT```TTTTPPRRkk--_______paaaammhkMRRkk   33$k Wv+~   .`\\...` ... q|.3$8bgqGaa#PF`                 .             \n          `         `     .              W||$$gggGL.zk_ssg..`3$ELWJICQ2gC5L L __yp-op--`  .`__ ~+     ____as.a2/  __   +  --    _     TTT7DMMkL_gpaaamm##N9PPPPPFF\\    /A\\YL``C.     q++mo~~~~..+yQTG3aay##F``                                \n   __   ||$B$bgaaeKw@-kgsLW38$kWz~WdfGsspk-LL_.a/PAVP``L  -  .. . ___yiuuz#+~F`___ .i... \\Y ____j..amk_____.     ~~f$RNaavwW#RRBBDD7TFF````     ``3$Lge~~v    ``_ . . ||338gqay/F   \n   ...                                    ||$BQ$V@8$LLz8KGsL@@8\\bWsLW2Q9R#cL@sLgz2ZL./~ .L....~  .y/A##TF ____..WwmmcL_ 2l.aaaww+rF \\L..w-- W/ II7RWd9$KN7PPRs#P~FF`\\\\L  `_.___@A\\V/`` `   ...  v--`..|33qqqay/F                                    \n   ~~~              `        ..           ||$B39$F@8bk~~@@+ss--@--+sWqyZPA\\F-+#eeFgy;````~~osv- .pzzEC7 ___.aaecL`3$$kWq@+~@@~P`$C. wvr+W`_____uamrF`33T1P#+F5j7K```__j$k____yyyV/`\\tL...    /~~  `````||;Aqqa3Z/                                     \n    `     `                  ~       .    j|$BR3IL@88LLLWSCD3LLGC ~Wj$EL 38FE@7EL  _.`3PF___jspxPAjyec .qzzp/R\\kk  ~~~~~jC`_____jyr `jIL_i..wwvWPP$5.~jf$C```_y/     a4Tks..3$./cL `\\yyg~ .  ``      ``33/Pqqj$/F           ..                        \n                                          j|$P33$aeggbgg@ziL`.ua:k`.W2IEgs.L_@3g_gy/F--.@s-`___.aam/CC22$FF WqmmFF `jlL..``.`3.-uaiL.3/FL_2iuwAAWWMXCC`3xKu77A\\\\WW4IL_____df; ~~WwmmEF~  qjiLL@     ______./33C`q347F   `        ~                         \n                                    `    W4|$BW38Q$b@@6k@esL@@W31kwodd$Epob---AGGa86L@wW@/F`uuwm#PA\\\\axrc``_`7TTF`  jymwwwxwWdxWWAAmm#P WWmmV/`` Xf5axm/`_W//F`~~~~+\\y---._______39R\\kL  W4xyy--   .......p+8`hqqj7/F                                      \n                         ..               ||$BBqq3igg2QL@@8L@.W3|L@8Q39$FFFW@@@@~g@@_@@``` aWW#P``_j3XREL  -y___....3IC`__`L_2`__``T1P____________ ``5$\\\\W//   `..``3ggg;Lu-aaa-u.Z``lLL.23$$L_. Ww~~wwqsiBDD._ja4/F                                       \n                         ~      v         ||$.R8386GaagpL@ggsLWqiL.GwdZ$LL`@ssW8ZSSg.._`` W3#F`   yV/`\\\\YL_..ikqgg_yyg______s.._i..`______....________ _____ ______j4$GG-WW@g@@@@+~Ww-yggcgj$gpcL._ ``W39+kWRqqa3/F                                        \n                               ``  `  `   |||;B33IQ59RBbLgsspLWqsec@#Q$ggL._`~~g~~qaxLL  _/``5   jy/F__j\\Lux+~________.saawwmmWwakwwwammwwwmmmmmeg-_..._______________#j$L03M4kEZLL.`57M8$Wd$$bkkw-- .d``I`__qjaZ/F   v +                                  \n                                          jj$:FWd+GdTR$gLgg8KLW3$$FF_q3$bkk....Q$P@3$kL .___..  WjZCL../_______.aaaapww@@@@PP___``  VPP9YPPPPPRQ55bseeg_gysauaamwmmuLLL2$g__jf\\\\xve__y/__|Wd4RPFF~`~~++``.q.gd38/F   A1```  ..                             \n     .                                  ___jl:P`33888Uq$gggELbWqsELL.338$LLL~~wa.Wd3$LL ~Gaax-  jymmwwdaaaaaaamw####KKLLCCIILLL..._____________wvMWMPkWWWQ@@##RRNM##mwwmKCL.____jSSkWxL_3.2##RR\\F```````...qqjZ7F    `\\                  `                 \n        s                           ______../ .3d889qdd666FkkkW@96kWWW3$$LL..`@X+bdq$$.. tV/P  WW|$|N99#######PPTTTTR\\mmwgg+wwwmmwwwamwwmauuL.___________________________h+wwq.____WdsLuamm6FF ~L     ____jj34$/      `                                    \n                                 ______gaappk @888..d7RRBBEELLLW2PL5gg#3$kkkL_.``\\3A4xkL jZC_   jj$.F TTTTT ``3RRRKCC227PPPP########hmmmwwwaauuu.LLLL.............2 _jjsi...@A#WPQQ$/F ~~  ......ga334/F                                          \n _gaaddfRKL  88uaad8QB$$Sagggp.F-gsLd3$ELLLy_ .P3$ELWjyL.   W4t|L ~~~...```55RhmmqL____``````TT`7PP########++mmm#++#+++#+#++mwuuu339mhmmLZZ5La3$KL   ...ygggga8/F              \n                       .     ___yaaa##RNN$$LL .m@@399AW99RR6$SggLL@@amWd8$bgscL.  \\87$$L_3$k    W $$L.ggs..___ .-~XCC````TR#$bmmeLL..__..  ``````````TTTTT```TTTTTTT IIDaam##PPPPP++eewm4PF   _ g33dQQ$/F    --                                  \n                            __yadf#RRDDNN$$L  ~FWG88``33$99###$ELkWRQ8L@3$FR6bcc ``./A\\kWggELL  _ 33+mmwmkcu..   \\s.__```339RRRkmmmwggc_uggg_..__.____________y_y.ugaspWRTTT```TC57XQ37PF       _jy39#A88/F   `  `  ``+ `                               \n                          ___yddTTRDPNNN4$1k  ```~~~.od38899RR$$LLLLaiL.33$88MkkW uxCD3A\\iLgsx---`2``55999WMmkL____``3+s..__ 7PRR99NR6kWWd6kwmwwmmwwwuaiuaiu22lLzaWWdBBKP \\yW/~~~ _jy/F       _qd39KKQ3/F  `+   `   `$          .                      \n                         ___yd4$$PPNTNIN3$|L   _.o88qq8888q89d9R$kL_jgkLL33R#QQ\\LWWA\\V`0AAkk@FkWW#/```TZ2@@P`\\qus;  ```3asL..```````7TPP##K#9##88NNRRk@@@mm@@@hmmmmw@@#R#PR\\CQ3x/F```_jy//  ______yam#RQjy/F       ajr                                \n                        __ydd4$PR$FZlUggd$|k  ...qz@p388Pq.g T7$RbW.ABkL@Z88\\QLL. A\\L/ $$QQ@WP L~~~amsC _W3+kLg..//A9kqsgg_________   _~~~`TT R9#9PP99####PF _j$3/RCL ___j//_  ___gadd#PP\\ay/F`   __   .  d4E         `                       \n                       __y/dA$$FDlLpppdN4$|k  @.Aj$vPA888W\\tqa27R5MgtL@AKyLR888kk  `33$kW`7A\\$8ELuiL..`P~~~~ _7`A\\Wmmmc.338Q$mmuuag.________________________`___``````S..uaa##PF\\WWqqaiC  ___sidM#PP__yy/F`     ..     .``   ``                            \n                      __y//PT$$DN$NHHHNg4$|L  sLL33tL`3\\sd3\\9aBPRd7PAkCL2gLL_z/LL___ \\L 93fRbvmmks..```..agLL___j99h~~~WdfP9H#6mmwuascuggppax-gggg---_____________.zpwWd#FF```__ygd+k__uwam##PF__qa//F`    .ezs .   .                    .    `         \n                    ___y//F d$$DN$NNNNNN4$KL  ~kW \\xud3oxP 33SSWdICQ$bkcggLy____ s.. jy___```\\YQ$EgeL. ---~aameu..___```.|```7TTRRMM#6kWmmkk@@hwamcpKkgggggwaaappwam6BR#PF`sgg_uuW/Q$$wwW##PFFS__aa#PF` ____~--gs   --+              \n               .    _jd/fF Wd4$$Z$NNNNNd4$kL F@-_@AXQ@8$bq223qqdqamdQDkkqLLLsL.gzgL__7 L_.___________._ \\S23PRhbmmmmu.  \\vmmod8 dTTQ$BPN99RPFWWMM6BkkWam6MMW@M6M#RRPPF~~``5asb@W~jjaa###KC```GaaWd/FF   o...G3Q8Rk.    .  ``` ```              \n                   _jJ/PFkWWW4$$$$NNNNNN4$KL @8ku..A.8ooaeeRW877T9R$PP5qgippmku~--___gsg-_g..3axxwxLL wwm.. 2@9999kkL.235zP~~~9+A-+-KT7TBBPWW99PPFUV#PP##PPRRPPPPFFC ____3$BK Q33PFC`IuasmWWQ9#FF   ~~~~~~@ttF~~                            _         \n           `     __yd/fFPCU WJ$$$NNNNIZg4$kL  W@@@@sL@3+q.P 5533XdCZ2I$LW4$$LLCLL@@sssiggsLLgssgg2Z#~~\\L./ \\+oamuZ.2/R\\.~~+3ZE``````TTRWW4$B.oW077TC 7/ TT7TFF``T`````\\yoaae@#PF Wq4T`` awd##KQ$3/F``  __ FF -.8$L.  ox      ----------              . - \n     __          _jdffFFL___WdT$$ZIZ$$gd4$kk  ~~ .2ZLL. \\x..aaod3~\\aqq$EWWd$$kkLgpL.@#mmwmmkkWg@@gsEF__yg ~++h+mrF a.F`33PA---` .g~~~3f$88mq__S$ggcC2zzg____ ....asz8###F __33ICL@V###PFjjy/PF Asy/ @~@zE~~W``      ``   y; \n ydffFFFW4a.WW4$$pqqgggd4$Kk  @@oaxho; 3\\vd~~j```3dd$kLWd99RRqEppLWP`R9RRkkWqypAA$L-__$.x--_ .2Z 5L_@+-_j```\\IL -aSSvw T99+RaGaammwmWWAGGG.wqq__aaaawmm6G8/F yggg$SWW/L __yJ/F``WxF@jsFL ..  -F           . 22 __yd/A$EDWW#AbWWd$$L$$$$ZN4$kk WWkc@3IB3\\WW AT8G33ao8WWdfhqqiZZPNN$kkLL 77R\\8MMtKQQ$kWuaxrcLhxypwqy.-~K _jx.___+s..P3PK``````P~~~#9###KRQ$##~HGaam#@++RQ$$#PK__.344EP@ESLg_uax/F` W~6\\W3ZEL_... .    .     -+             .     .es$    \n yd/A$$kkPF_99 WWj$ppHNNNN41kk @FF~~gEF ~W````~qdd#888``TTTRqypWW7RRhqpLL.. W \\Wd$kWjtLbW#6Fk__y+F_jj;``qgsW/A\\s.``jy/``.......`````77T``593PF``3W#+PF``533+PF__..id28$S..WqjyV#PF\\  W .W#fA\\E@-e- -    .     ``     ~ ___        3$F $    \n ___y44$$PPFLLLLJZ  W4$$$$$Zgg41LL sL.@@8EE``_ygg_ 3T833338 WWTTQ$LL__25d4kkbp____99PPA______ $LGsV/ Luai.  jyg@@ RtrLu2/ jy-qoee y~~~j.  ./ . ``3$$.w-+```` wwqqy+AjyyrKCGam#F``2/@@`_L`I``\\88@#/`` . ``   . ----       j T    \n yd/f$$PFELgpmq4C  Wd$$$puqdd$1kk 6bW@@8GsL_Wdaqsp ~33----~``\\dd4kuupmW77RN$ELLgg.. C2aaaaiLpmvg~~ yyWAt$LW3XAG.. 1WqsCLW4/``7T$LL./``jyc__``` ~~   y+~~``___.____j____3XKC_WW/PF  W/A-.@..as.@ayp@@ ~~       .          . 3$    \n ydffFRIEEpaiDRN$bL Wd4$mMMN##RFkL R\\LL.85ssW/AA9tKWWW8888888WWWff5HtLLL. 5$gggmmm--~++#####KR\\L_.Wx/FQ7F~~~Z25\\L.. 3$kk_____ j3mmmL W3$FW- ____````_..3axcu...qqyp/A\\Lg____  W48D@bor#@8\\@@TL_`   -   . ~+     \n        __jd4$PPN$BBM#RbWH9HhLWW7#PPFPFT51kkW@@bmcc@@#KKQ23ILL``~~~oq~~~``T77R$bmmmupAAGbKRQDKLL2ZZIZ $LWaxgg/P   /C_.---- hssp-// \\\\..uxu22PF~~ jjf`` \\yy- ...a.---~qxrP~\\Wg+qqazKLLpXqqi.. @- WMTL..s.84L.... ``__   ``         -          --- I.    \n       __ydfR$LL2ZPRBCDBWBTFDL__ZZT TT $Lkk@@P@@@sZF`~~-@A+wv- X.. WddTPU7fPRhbKCO9RRhqLLkeywa+eC2LW A\\$L.. _ssA0XC_@3$MK   ~~+#@@~ _``. 7 ____j ___ypdfKK.Q38$``.X G3W46Gux/ qjjiLL_  .```|_~@ss@@s~@ ..`  -   `          ````   +~   \n      __jdZPPFhbgFKS$pPABPPWM$Kwaeep    j$kk@yFL.@33$L- 3.2`XXq...`\\ssWd3`\\W7TTI$7PhWWZZ_j$iKCLLLZ`5SxkW.Q3ammw-___39W@~hWd2ZF___ysC2`__....    ______....aa.RPC___y yy_qggf$9##FLWWJ$$LL   s..q.L@@38EF EL ~. `. __                `     ``    \n     __jdf$FPT5IELWqmpKRBILLgg2W#$|L` __j$LLk.L@xW@RAkkLgg+~ `\\axk__333o88``~~~9a++B5Nq$Lg22DFhqepL____LbwW/P$MkWjyyFr___239X6C--3frhx...zzgg_____.aaxww++~g~~_~--..________yaaZZZ5T` WW4$$$kL._  .WdikascB:L`Gowv--.` `.L`~~--         --   .             \n    __y4A$$ELWd9PKKd$ELW9+buuaiLZ$$LL _d4$Lkk\\L/`\\..3fAWmCCL__W3Akwa//R8888883777TTbmdd$$kqgELU9RkbLL...._N$C2_L__jLLL_.yx/__`\\\\Wd4C`\\ssg/A`\\L.aaqzd#RVP`__S$.  `\\.yyAqvgqqyg#9fKL@p  Wj$$R$$kLL_  @@$E@6FFso@6@@@@bs.-`----`..__..__   ``                 \n   __jdfPP$ELWZTPDUH$EL_ZIZZ9H$LWd$kL_Wd4$kKLQLL_ax--``35tss..```~tY8CL`33~~~A39R8W9+PW91NN$$kWZP5RhmmmFpL..zggwwaxkL__y/ACL....______yXKC..ggyWA4VPP`\\L..gg+~____________2$$P7TRbVF _Wj$$R$R$LL____`Gb888@@@@@@W@w@@@@@ o+ouomo---  _.___   _       \n j/f$$Z$Dkp#$Lq$N$$LpaqqpZR$EW7A$LWWd3$LLWR6kW \\kW@7PA++qeL__`33Fbug3````7TT~3dTT5W7T5dO#NbLWZID7PRRqggpAiLL@@R\\kLW/L__g____.q..q.2Dkxe__3..P2IL__`WWm#F` yxq..wj...aamAAKLDDA\\  _WJ$EB$NAAkLL.  `B#8@s8F@--@`@380..~~ooW`Q@`P````` v-------   .       \n__.ddf$RRRR$LL2ihdiN#bbKdN$RkUd$WZ5$bLWW4$kLF`\\kLL \\gg/ TRXkkqgg__9XWy.`..``S333A\\W8dQ7PRR$MhhmmKCLDHAKCLDgjppSZLLgsiL__L..__ggiguyrc@3$kwvrc__`_.. `________ji__jrndVPRDDDkkFP` _Wj$$FEZNNN$kLL.. `.@@@-F -.- ~~~``````~ .. @@ CL````~  `        \nwamdf$PPFFRqjppFPPPNj$LLWH4BhWdPhWVA$LWW#$$LL. gg..-sse-  \\XW4Gu./`jj.psaaaqs/F``599d+P`7TRR9RRhameZ255mK6$DPhhmw#Akugbggpqj.pALVZASyg___ZL_..pgg_______-__u3$kW/CDDPFDW#RC55W  WW4$EEN$ERN$NkLL__ ..``.. ```~ G---.oaoo..``s...-@- Gam--   --     --  \n/RRO$DBLL_Z5$ELLLZILd$gLgP5$RI$LDBZR$$LLW3$Lk.ksssLL@38FLggc```33f#MeLuuae@@#RRQQZLWaaW7``5``TTTTT9Wd$$bWWZRLW#AKOIBMKDNVR6K$LNjiF@VKC_gXCL.--waxcL_$LLuaapgqqam####PkWfCDPP__yg~~__Wd$$ENN$ERN$N$kkL_____  L . `` ```~~~sf~~ .@`~~-``` 37R8L  ``` ....``  \nFD2Z$$$umpWd4$kkgggggphqLLJlHjmmqgEW#AbWW4$kLLL@#Fkcs@/kWs6F--gg/`33\\kW`3$S2P`\\34rPW~#QQC`___gg337d9PPR#99Kua.FDaggFF-Wd$NMPAWW4$LWDFkqi$ggggLW3$LLaamWWd6KUUMMPFFFTIIZLFFFLGa8FF  WW4$$ENNRELN#RRN$kLL______       --___ --. .. -. 4/F        ~~~;    \nLqdBRR####N#4RkN##HHBRN4$kRKU2IRN$$W7I$LW3$kkkW.F``@xe.@@38F``WGCL.``\\\\mmqyp```2ZIL_``3\\amwwaaamPA87Z 537PPHtbbWMRELZ2L9PHfKCPH~+F~~NNammKAAkkW aagdRNNMMRM##fR5CL gaamKFFFWgzFL_  Wd4$FRNNNRKNTNDRRkkkqq. _   .....  ``___ .  .`~    j. i.   \nWj$BPRIZZFP2Z55P#ZPBPPU5$KPggggLPAgggp$iW2ARLLLKLL@@8EL. ..@+Aamsg ~A#xkL pGae-u-P3##Bd99fP_g7~\\WW7F5577PR##9hhmmmBFKZggW IIZ___ZZlL22$LLWM#RRPP####$TFC5gg_pW2ZPRLLLdSFF__ _Wj$$EEIDDNNELNRR$EV@#amegL________             `.  .   _   3omaekW  \nNd$BPR$$$FnqjppWqIgZLLggZFhjZZ$ELPA4VWAhhYFR\\bkFL@g@@@ggL_3iL.. 3R6mmuuZ``\\\\wamc@39bV C2 397 \\aZ W8 ~~~P 57TTT99RRKuammLuaamwuaammmwwwmm##PPP55M6TFP7P~Wd6Gbkgg$FFBmbWPAWWa _W4$$ENEEENN$EPEEPAKW77RNhkpppL______________________   . ~@fF~  \nNfBLLN$MlLNqqLLpqqqppLpN$LNqiWRlLLdj$KZDN$L@A\\QLk-6@@@A\\kW/A\\y ___29#+~ ~~++~--`Q.wweqg2sS..33ssb8g __Z aaZDWWTT55####$hH#+FP####PRRU7PPTF _33RR8L 99#RKGZZDPPFRLLLW~ __jl$ENNNLLWNR$LLKKQDkK``7TNNkkkmeeg _ _ _ _ \n``` \n \n 5$..`` jrF~s; G7bxZ_ _~~j$@|L3$F`Z`SWTPL_.7` .wyrnIC../05Q\\W4CW````~````````2|P1bY``````PP``7FF--~~``-``````F``T @~~~~~~F F`````````7````F`````````````````T   ___ ~~`` ZL``55```Y```__```````````\n.3sto _ZIL@3s.gtE3\\L......A.@-AbYn--#Xhya.L.3\\#/APAqyroYd#1# +----C G.u _siL.y ks...-``.a-T```_..``. .. S. @3.F 9--GTG.3SS.w--...V 1WdTNGk---.---- ...o.ux. _~........eE.csp--~~\\u..-L.. .  W\n+P L..e- W41ho$E@@@mgc~dtB\\C`C3;U````5fFop---G`_ 3IERTF ~-``CCD3LL~~L.s...8jH3rLWz~cd .c \\..oaovoor~~+~~~+~`S3cL..```~~+~~~~@`Fq~oiQ.Q~~Qt```5;P`5`.g~~~~_`\\a.P~~~~cc@~~SP~9@P`N. ~~~Qa+uoouarp ~\n gs;__W.3|DW8\\L..SC5.dwGuxo---W...WdTM7PRSEH\\...d$P`\\\\``..~~;~++-`.ee~~~```V7rL~``5s~~o.---ooz~0`bMTK```````W@6~om+nu..`````T``-3TP+~~~ k --:uvrosCQ. ...ctL 7 5S- @25 Z L.q:L .38`P`~X01LW`\ns._o.A-q.g-uicG7FWvw4kdsc09P~8TP\\azsbW/`V``_~~G3zhs0lLLJDD~~n.ST``TP~~GTFFK..JXfLL.`.~`.``o. ~ ___.z;........ T 0tb~~ .o--`Q2/```_.___ZL.`Q|W`6@~~~~~g~~~So-oo/G-``~~-..v:o-~iL.e--`j. ~ ~\nso- T0\\qtC@39F~j;_``5fW FH7 ~~~~~o....`..`33`P`~~eoxc```k~r\\\\v:0.@`````~oxqf;--o.`\\...~\\...F----oowwwymmo~-W...o--_``88.qyP``WXcL.--ae------os..`sG.K`D...-@38`P``````.55Rr8\\0.8\\G.8``\\9--`..---.`\nR`c`___`|k~``CG7ou---. I~-. sso:~~XXG33`````67`L.L_S`lL.:b.ox``` ``7`H|..G8s.`~z~`~r~~C.````P`\\\\.08P`..~~s8`G..o~~~s.L.```~o;n`dt..`$5T@otL__~~uos~~``-o~-_` s...g~``8`@_y@.~~s.L.Z`L~~~``8 W\n0$kWow.o/K`Fsg~fY/`88s. ...``\\.`..___g~~sv/Q.P~+6V`NTR`o------~~~~`\\.8.__..```U1r~of~~o-s..Z```_sL..```.~~s.-o.o````~~oo-`G@arF ....``__`G+--..`~``\\...``2o.L...QZ_.._joosC.L@iso.-----o+n;~o..``_.``W\nB.`~~9|M|h-uf6RMIL`-#9;uwV`__`\\o+----.```~~sr```\\c_y/Cq.````TQTZ`.````\\....`.`W`S\\.```389~~~-u.o+k~~s.-``.z``-:~_..`..```9-`8`L``soo.`q.W7TH.L..`__go~-..sz~~-o.q.gs;u403o9--`--~`s2L0W8CGZL9o\\..a.` W\nF~---3xM/N1MA@PH+K``07---4....P``08ssoW```.___ ``so/A\\G---~..uso--.```qo+---X.qjP~+-_-~~```               _  ``..--_q;.____```___`~~~~s..``3iL..__.d3ZL.@jsL.._~jrF3+-`~G33lL``.  ----Gtcd+-```~~~+h  \nF` yW#`N$M/N`$LP`L.sJA```@s~~s:..~~oz~`\\.os--u.ae~~`````LL.F~_`___\\s..37C````j;````    ________LLLL..L__________   .--o---..o.Q......-q+s.W`FFcoo--P~~sooxoooo-`4|W``.. ~+3;q.n: ``S``H`iE`````.```  W\n   /``../~`.`3.``~~~``~-_`3 Gq.mr``@`G.u@@`#@@+A..`. `---+;L....---o--``bo. `9+o  ______...----------------....______` `@0~+h+or~~~~.`G7F~-8C5SbdZC```3g@jyg|s;WQ.L.sg:.``3\\o.P\\u.or `G3@Es___s----  W\n.L .uav/`.._``-````````GG@P~~d;B. .@.P~~`-.3..`0\\vX...````\\brn~~~XC.`...88L  `` ____.a-em~n#fKNTTTTTTTP2P5~~~~mm--..____ `````8:_______iE3IF~~~`G-eo-ue6G3.oi---~+~33br-~-Q|#|L`~~;` `36TG----`````  W\nF~ AV#\\L.ss__....a....sc@s.``7T-- ss/C``.b;P+~~ `_`~~  `.``jC`````q+~~-.``L  ____.amd##FP``7TPMTFHMMN+m--CCZZZ7PPhhmx-.___   ro\\o..4\\.`-~~+6T`````RkW``~~~+@`````S@P``````M/K`- ``IL.`````PF~os.   - W\nC``````.wygov+-_@9+Fhr~``~+  `l8.W`#--sswv/``.W`---``L.~--3.F _..``TF```   ___.axdfPTTT`   /- `TT7P``TNTNthmmmiCZZ7P9hmxc..__ .`htcss..````\\i. -````~ `.W`F`````.3#F``_.n.2$L.`  `T~~```..`  s3:----  \n.  -`.zsQ\\m@XG.DZ0Z.D...``` .ss..V.```3-0.  `~~`9@--oxr`` d+L _/___`L..  ___..W/fT`___.L___|  _ _...    7ZTTP9+hmmCCZ77Nhmx.__  `\\--~~s.. .g+__S.n..`_yroxF``  ._``\\L.x-n~v-~~-...`F-.......o37F````` \n; _GyX`@#f#f#M+K~~~~~r~:L_._syecc~s...``~` `...````.Y`\\j. ``  XCG --vr~ __jyzdRC__uuamFWWwuwmwuLuwm_____  2ZZZT7TPbpw__77Ph\\L.__ `-_``2; _sd$---:Qo. W2L``L ..qs.``\\r~`. ``````-___`_q.F`-3rF3``   `  \nLW o/0\\v```f#`````\\.`` \\o.---T`` .a-otb. .w.s-o.` _.`. j;  ...5.W```Y  __ya/$DDwNe#+P-~P~~~+++++++A44mmmuzntumcr`57PNKKCZ77DhkL._ ...-o`----+#f``~3s.X+q~~--~-u3r  `` `.s.` ..j...-...tL@@Q.Ks.. -..  \nL __|_``` `` ` __/0|`  @~f```\\S.k/``bwv-8.0:``~`...\\.o.4\\ vrnv----    __yd4PP-~~_`````  .``````````55$91___ZZ55$uZZNZ@5;wdJ77A\\LL__ f`7`.``````  `3+-`TRTF@33.P+`  `..~+o. __.z;q;yr~qs---s.uziL q;   \nk_oa-o.xc.L..```4-Q.__ .7m----A#/Fss./``~~``....n:o~``Wj| `|P`````.  __y4mPKCSS------  .~~              44. q7P+HCCg$F-PP+NTW7A\\\\L.   `\\.  v.``_. T- ~ .cuzs A3.k--XC@ox.. 37 Wtcus1k q.   \n.WY#1k@jvwqxc _y ~\\  v/dI 07F@-+-....  `~-~`0i.`` `j; `iL.   ~  __jd$PFF~~~``````````` ` `````````   ~~5$TPTZZa4lU$LLZ21VW77$\\L_  .-\\L ~~ ..-_.` `4V``````  .rbV+--~~~ Y``~FP39kW3tL  fhr``  N1W  \n ```.G.XC@N|K.u...`\\V/```+-----`s.```.qyk....`` jrF ` `\\c./4k     ___.4$PF   -.   `      T1   Z29HHrhueZ_|A477$\\L.  .A--`..rKGo.   .. C.. . `.3$---`Q\\~w. ``   \n_   `V``\\ AlQ;~+y~ _+~- -~~ vWXQTF s. .7` _.. `$--``L   __uqr#fFF    `  `    .   ``    `` ___         q2ZZZPT$VH$PNCDAXU34\\L   ___ xo.   ______  .._...-\\.g~~    --  ----3-w---````````   `3 \n.. ```` ` `|s:``I  ``.` ._.  _```` / +  ..  ss -uo; `7\\. `  ___ Wd$$$L  `  `    .. i.     _____... J     ntqqEL_2.p1PDtV 5VZ34\\L______@z;__...-G-- qj.ury 0// W.`` ``-````````s..vj-`     3 \n__   j\\.  W--\\_yis.__.._~o-   v ___   ``  -q;.__   Wj_ j;nrK   _y/_ JI41k       ./wyr +:  `  --.axortL_  /`    7+ jqjpniu|hWZAWH4V74\\L  as--_`-Luxmg6YA  jyr \\.. `._s.`-uxv   ``  `.   -o;c`j.    _.  \n.LL..______ \\/Aj;. \\;  X\\ ____....   r @1b.... -G.W4+M1P  _y/jyWd$$FK\\  _ j. /1  \\.   ``~+``90 ---``  ``    T4lZP_____1HqZANV74|W Nj|L..___Q7F@   WV/L jiL ~-oxr  \\ ``.  `y  / ygj|.  .n-P \n~~g qx--....waiQ3\\L \\___ ____jyjy _  .   ``##+-o-W#+P````   W/qV/A$3$L \\k.oWvG:  \\      Y````___        d$$__.4wj|p 4ZnYN3\\L  JiLz:C..mcF`___y/A WJ$;W`Q7    ...  . .4kX . ..ojxr __\n_ G--HlkWaic~~~+~R$L_  .....wvd/QlLs._   ```| `.    W/AW4|Wd4$L   ~~~~~``     ``  ______...    .._______  ZJ1hurdNqiL_n1H1Vd$LL d$Lg@~++AvL ..y/``.q.$:L__``..  .o---o\\u~~ ujx .diy/s.__.__..\n-c2ZCU++Wd$L__```@axkkuo:qyg/0\\~qjywecL....____ `.  j;      _j. dtLLJ3$L     ______     `_____________________     qZZ2J$UWN$LW 1UjVN4|L G3iK.____jb~wv/CLywqrP\\L... ~-.do````8.P`` WJ$bwdsMavwuxwuwmm\n$kvw-/`IV`9ho.A\\oWP1b~W@`5Y``-___4___$gm+kuug_._ -_.J;      qJr dA$Vw4$L_____.LL..L..    __ggspwwwww.....____.p    $ee-Z$.L2J|LLJVd1N4|Lwe/Fh;L....\\.23$LV__JlLLWzs;  4P~~`\\.`~`  _-~~yrHy~~~`\\V3WW/RC\n$MKK6SWw.KIQ/K__XC..```GW@|psFW4..$.W4KQ1NNVhjyp .W/j|   .  W4. _`3TPj$kkwgeeecz__sxcp   qyammeccczzzzzpnmmmmmkW-  7TElPP++HdlkW+wH+Wd$kW+CC/lbgmsrn+_j-RX\\.2Ar__$Q;L ```` qs .    .___`_Z` ..WxPFK` .\n$KCbxKKD1NyV`L..qjrK__./XNYP@hWzznjVM`Qjr __N41L | ______j  W4\\y..`ZLq$kWX46FFGmmmDAVP   dddfRFFFwwmmmmR4NN#PPF``    jET`T7_d$$PP MQV`4|L`5zaGxZE2LL$2ZL_`9Xd$$kW.R\\L   oxc3$e-u .uoo.-W4e.ujsd8T` |x \nZLWYKFWV`QIE Wj6U7Fkwve;WdfK  _jILJl``WJL qqy/|L  __yx.._.   `lzzrH$Vd|L 3fK`@@PP9FR\\L  WJ$PTF```7TPPPP5zfF```   ____$$L__p4$PD|L2AHi_4|k jg_jfFF--k-#j\\u./Q5P~hq:L`L   7`~7+``y  `ttt; `+-W3--`  W/3 \nLLV/L dtL@+F Q$VW/` jj$L.2l___jymm/A_gg/L_jaYC`._  d/A\\ku;L  J/4$FW7$Q|L ``` W4T77F``L  dTT`$    W`TT47PF`        __2Z$LuJMfZ$j|n;_d1mq|L IL.2T7`71K`U2lbYK3ZL``4\\Lu.L_ ``.`` .._../+---```W2ZL$.  _js\n$EFF  ~~W2IL__jfQ$L_.3/Gr_ys.LJ/CELWaaE.L.W31hx:L_WdTP`\\MiLwz/`R$.W$$4|L `` ~~~f``  7k  Jf$9//``~~~~~~~```      _~aqd$$PP|FHa4JwVeL_`W4|kwm++n__________9l_______$L@+k    x...swwmm/```_ ` 3$-P++``32/\n$EKLL..`Wsskwa.kM+kyg_2J$D/A\\W/A\\---f5j+m+W`$V`\\V.u21L 23$hV/ __jrK474|L      ``   zf  W``T``             44$7A1O1Wq_j+NY|L WW3$LW2ZlLas.L.L-.G2ZLWa$..wax-#PLL _--- _  _.. j7`C```_Wg/`\nPF _gs;..ZE3s2:L.__`Ajrp~~`57X$L./___27IL.W2lKW@g@yj|W_d7`N|L .W71U9N4$L    .  ;  ye    | s._        ~;  .      ___jjZQjY___44WN||L_pWd4|F____@9+~+#####A@@##+nNM4NN4mmwww.2``l.......J:L.2..xws/ XI$_\nFF_jywmmm-Fwmjmmmvp-W4fRTCWW/0+h+Kua-neqcppd$;WXGQZAiL____W|kwWdf$___3$LL   :.   jI2   q..$ss.  ```` ``    _q.    4$J1Wj+ku.A\\W44|K4DWM41L_.._ZZ__________________~~~~~~Y~~~  ~____ssgg___y+-`Aj.___j.\nL WVTN$$DK_`TQPRQ|KLWJtL____\\V`7TWWf1U$HmWeA#.U`VX\\VAksu.W4CL.|_yvKqad$$k   ~~~` j|y   _____jyp  s.        ___   dd4ZOJNCLg;___j|CNJ|__N\\kwspd4bw---- a.w.----.....__``  C.___.G-../qjG$...____ji...gg\n;L /F_j3ZL..A4/A3lL--`\\_---.__yg|  `\\J|P`5~F5$__``_ `@@9;WJ$bmKM2PhjfH4$LL  ``  WXi._____.__jiL  ~~ ``  `  `4.._J$ZZ$VccqJA-__\\4.k:q$L_UA\\W$FF#9#1KQLggtt#YMNtgqmn-urn___ywywvgqj+KKy2#thbp-uawwykWd6C\n|k_|u427Pqycy2`WJ+kW.Kq.W9Ahvq|Q.L  qj;_ 7TWW4L___..Ld7F-pqjPrhYqLWATU3$LL       ~A\\u..aammmmrk ```  `` o+oaaawuq_ 47Z5WWT#|KV4t;pTR1NNV`\\VpTW2ZB/N\\uvwkZE@LV$C7TR1PKh..______CW7CLWz;R$GiLWW4#Q$KP`tg\nL_jlWjfKNMAbYK _JT j;Luib.`7_Ntm:___j7lk.d_gzsb-tgp:w~/NTFRlCL.-jYKM1Mf4|L   ee  ```~~+MMPPPPFL___..   `4`P```  `$ZZZCZZR1UrN|PA\\V1MI|N|Wj|P+W~+#-W4#+@@+H-wy++qC2Z$LVtbpmuaaeWdAkwn+FH|N9F__`$Xek_2s.\nLWv/KQTLW7#M1kwxR\\.JFuziy;W4.U4#-L .d/Cb;xee-/`5ymElLZ__g.WjymKW7TPh~NTN$LL  X ______`7_________.pmk~ ```~__.  __jqZZq$KwWMAU___j|LJm1N|WJ|L  3Tk._`$`STP`XO7CIW;wymWd6@P~H9#hVP`5WUIC____C..2/#Yh@eei\nkWfCWv+LjfKC1WJ/hj~__W@mYLy.~V``\\h___`Sg._~SSZ___F5+y-uae-W3TR\\V|`___j:M4\\L_   wax---......aaaa--PFF    _________Z41nj4CWfPW4:kW4|k_OAN|k4ikp-#thtzns.D1L_./_vWHTP7PP`RTC``````L.  ___ua-wvrnyF2|KZQZA\nKQ|kWf$WJAK____L_J`NV#P8IL.c__...Lwa.uzziL_@gyke--3lB$R##PWj;N4\\.r ax/CL3$LL___ 7``f~~~~~~~ffffT5L_      ..4|   41___2q$zK__j~_VJ$VN7PN1WJ$kK ``Uz:@zr__u---_J$``K\\.` `` _______ss...kWffKZCDZ-NvkW4CF\n___ `jamrF`\\.L-ees_______~j:.g-+~ssz+vwmik-##2R5F~gjyrK`` qJr_`\\lL_dt$FWdA\\kL___     wGGww77T     -   ______|    _jaa4q|K_..____gJtdlLN|W/`\\VW _gg_W/#kW.f$__q.._.g; s..._.__..Gaarp~cq$p+q~~~9M|K``\\_\nkZ|W.W7R\\L qiL@9F9x--w4eFWxmsWSZ@zFEZ__3$K8htrFRmW7TILLuimW4O --4+WZ$$W7Z79\\hkL___  PP#####PP\\\\V`~~ ______aa: ____dfMWUIuJdtuaW4hf$jAWW4Kk.2.L aa4$PA##n~~qy/wywveu..qzpyqsuva@@@MFK7wMTC````___`  d$W\nhhipWN.Z$x-d+K_Z`2Z`5f#f_j$ELgsg__E3ss.g|__~j;______~-____`j|L_`7$$;#1hYKUZ75$kmLL__ ```````   ________app#A\\L_.qjptPNM|gf9+Wfq|FTQZDUU$rj~ggL ~99tKV u/KQIDN9Rq~d@D@P+H@@@ _4L. 2lL. ._uxg.p@n+W\n|N$KLurWymWUIKKCLpy- y/ war#mE###--d7kcpiL..2iL.....ZILZ..WJ$._..d7Z___| ~jiQ7PNhmcL____  _________--apmk_____uaa$FFCd$$p4Z__|N+L_jgp N4lL_jyk- 77 _ / W4+F TN$TP~sC``E3ILg..~~uaxmkwzzLzsq;K_Llu\nkU$AW/LU|NykrH@hyM1LUlL WdfPTPRI5tCW_GSPkko~d+H@@+++++H+++-H\\|u+PHAL.-___ 2lJ$LZ7N\\kL______....aavmyA_____aaamMPPfKH1W4+Pntwu/U\\Lu$4L.N$$KwjYKL@____.L..... _ |L _| ___~y+#g--  W441N@+Aeee3:L.a-W\nLN$FA;WJrD/FILJQ1KL_.Au.____``FwdmcuadRlLzWW2ZELZZZZZIZ TT J\\VAL__jrKNikWaa4cLp7PAFhkwwppgggg__2l__L..a.w+HRQKC_ZlU$PT$CUZDJ\\Wg|Htgp;W7I_UJ|L_-__wyrh~njc_ui..L.u.amnwyxK..ZT$CCLL..#zcL`___~FP+h~4T_\nPP\\KJ1W`|K`GakW/AGvm/W3;b----wVMTF~H~PP+~+--+-F@+++--+- __  d\\rF_yWM$.#tLLV7$KWPKN4TNNNMMAMNMwammwaawmmdfNTPN|ww.m+PF   qywyTkWd1NmYn|W4$Wd4$LWJDWMZDN@N4P-H+++mmWdf@UV/|Utcpxdamwumr__~_ss..7 TW q.W\nK__Wd..  Wdf1L 2f5_W2E@ZZCLL_________`Z`W2ZDF7T`5T LQ$E~ WJ1LUJ+HnrKFwv;P I5$PWf+HWMN|PPNfMMRNHMPNPPP1R1_d1N+PP``   qnIOJKUdjKUJlU1NjtMd#+FF++~~~~p####_K 777 NjF#aKC.W FR;5ZILU$L..22@@ss;LaxcwyxW\nk4vwyyXn.WWd$$k__j$wmvwF------am--yg--eg~~~~~~;   zc5.ua+F WdA\\q|$IDP  M1h__ZJPF__T I$3lL  T7T$V7RTTR1 d1K`1  . __|MlU;hj4ZFH`$P`WU|KCZ|F`7STG77Mmmua.uus...U$MVP~~~nMAGmmmvFw@++n+nEE7R\\Lg~~W/`5\n_j;LWJDM+_@A4lLLWd$MRbUT`RV#@@P855f###```TF`FS.._yew-+_3IL.___ _jywj|L...ID.ngr___._w$Zd$L       77P+~\\o\\MmWd.  ~~~ _ Jqj$E__7njCDZj. W44QaaiL_;###fP ~~~~~~~++~~YARKK77`7`#fF~9lKW``CUTDTM8OD+nyy7`qy\n.axee+N$CWVP@$h+#+FE5zzszz; ____ g| ______ ` 3$Lu.LF`5-qip~q_. Wj|K7|wynyp~4UZ$Luq4|W4RN4\\L  `   ```` --PP`~s:K``  .-W4NTPAkkcQ$$kqqrWWthY44$kVA_____________``_2TP`~~j_..__`_______..z;.`55;``Ul.  j/\nV2$EEI2esz:W`TZTR`555FF-mmeswa.csq._q.a....___j____________jxr_Wj|L\\1U1U|P_ydtj;Wdj:P4UNM$LL___ ``  -- T ~~-----`\\NtCZ7fhWd#fWj|  \\VIDZQ$LLwiepxcpzaaui_..________y-- --W.G---_ywmmr~_j$.wamcL_Jw\nmEFFFW~F~~~~~~~_____ZC2ZC5./Wdthwj:udtp+hmxL..4_..-j....4..24\\WWJ1kvWM\\M|L.wOID$MWH+NmMMPP\\ku-c   .`#-8..`~T`   W7$\\xL.944OTN4|   a;PA#AhYH4#M@@H+++HGmrKC..wua-cZCL.2z5ttVXKODPPTF`wv/KC7$FwW/A\nNTF```T````\\ W4y/kwa--uyx/ KU7FHd|P`N7AhywcLzd5zamwwqtmv`|WdfF XL qYAhtu++HAN$NDPPFFF``\\VA~ __. ``@3s `...-n.     q|7T$kqy__y Wj|LWq$IP 2TN\\U|RKQZCZZI23XCLzidWMQDhkua:n--s __yr~CI. WVfC yq$V/P`U\nP+L_._L.L.Q.LLL2iL___$DV/FL_.qiW/``N7IL   d9fFhykke@9$KWVTR\\L.W7ILw__wdfkUTNTF2NDPPP`````  \\.   .......`  --_..3s. ..-`1  _4Qywq.  j|k d71kW3|P`MiP  wwwmwmZFwWdAPPA94BDHRFF`@__jy/`\\y/-_ 7jrdIN3rK  Y\n``5igqzg~~~+__ggm-u.p/#fC__z~9gg__ Wj$kW/`W4$WWTF5SE73YW`__9+wu/A|PA4VJ1U9$Nwm.PPK``   ..__`\\y ~~~+~--~`  ``s:.`~c.z: ____;njfN3iLW3;K 2fE|W31LW7AKL2Z229P___ ``F`TP7FF```E__S.u3+FhW/CL._|JlM$M7FL_uJ\nyyd+AHxEZ____yWZOTNXKFk\\ywaGW34Gmk_WJ/Nj|Wdf$W/A\\V|NMy|N ../`lM|U1L`tY`$V`AM#PF```    --ee..4.. ```````  .  ~~  .--~~ w.wyrd3$W4|kW71h|d$$. 31W+K7hxvw----..L._.   __```__y-wg+d`C_W/\\L_wwammW`N|Kjy-W\nDTRTP71NS----tQ$kWUI\\L_$VWH+P@/##EbV/`_|Wyd4$PFP`\\$Z$JFW m+hqrM1H|;uq$LWlKQPF``    `  ``Ts;.~~~       ..    .  J/ WWfQ/AWd|L5|W $|W|j|P1W___I$WMAKCC| ggmmwu;L__..  am4/KW4$ZL..W $V4WUARL  q|WJ/K7\nK\\h. F T3M|NWMfFk+F  q+K__ $Z 77F __u/qiDNTF W fNmg|LW|7 Nj$7TN|F_dlVPA$VK   ```+   ...-~    .--`` \\41L N$wy;W yL__JrP\\V___yPAJ7PhqiL. @MRMlL----LL999$k@@#+WXAkW4fPP _ j;W/CW/\nL _g __JJR|WXTTIL Wd1PwwaxkWw.L.WayMWM+FH\\y.L d7$MkkW| __ym_|U1LudrKCUJ/L        .`  ````  `. .r~~~    XJ \\.   `J\\y.2JQ$LWd|hYd41LgXuJ#/L_ZlTNN$kp_____@K__ZLEE..2LL.LL.22L.sJ`__`-4.-W7/P\\V/A\nL`\\aa.w-eJD| ``$|k__ wvrhV23$LP~+bWddtPW`` .2rhV`Q72f5\\W___J4Qp+HAM/KC5xg;F      `       `..  -  4C ```  - `~``` ----- __3zeu39/L  |Mj|$$Vd$V/R1k 44- 7A\\VLL.__A\\WyA-@yFnwve~~qymmw~~cL.._gatmW/C._/__\nL ~m4CP`jjM|L qy|N\\..___  wG7K `TFWW#A\\S.kaedTP` W|`F___uxmdONPKN2MCLWYK5|K       ..      ~~ `` ``\\.   `````` __. ```W\\u.d4FWWf$\\W_|_qimw$wM/____________$ywtgc_jy/C@@TCP`XT`77ARKFRGkx+nvW4PPP~9X44--\nk ``7\\._31M| _jy_Uq\\Lu..._ 7`\\.W4LD/N`#V|__TP1   J\\___yWWffPPK$NN/FAWPA\\a;b   .   +--     ``.... .3:`      `` -a..    WtcOA\\U5TRtLWlmM1L@2LLLL.wwwwwyww-444M$mmWCZ_k-s:L..LL__``______````5``````07f#W\nK___44xWdfRLLWdImWd$kWajc_____~-++c__;Z$$W./CL  __j_WdfPNPT$TWjmRCD2KK@M4FF  _- -````       ~~~-~~~`    . ``   G+- 3 Wd$yK`_Wy__$WW@B9DV#-F++~~c#f5zzz2222$5$@RhhmwcCg~g~+----__j..--..  _.____.g~~___\nL--uV/1Nf#hhvWdmmWW``W74\\k--..__`T-wxeu2d9fhfK___..a+HTFZL2Z$U41Nkc@@XY/``   ...`    -  ..  G7``.``    o   `` ```_. __23:D\\aiZ$W/##FE-zz5$7TFFFF____~~~~~~~___.T71Namu_TTTN7Rkkcgz_@g++__aip--uiL...``\nk`#P~` 7`__7~HTR7__.  W4#Mfhmmw._____| `A-CD/__yymmPPPTF-- jjmmKPFFP`55;     z:~ ..```  `  ```                  sv- _.wrcUx/A+-_CL55www y~_____C...ZT7TTT77AvwwzZT7RRNVLD|_/55MkWCwV/Z\\wwmwkM4MAk+etcg\nL_LCC____4V/``T_\\sz/hYWNTRTU`D9kn---s...``F __yV/RZLLJ/K`CLJ4ZRL__   W++ .   `E..~   `       . /       ...      +`  V/Q$FP`CS3$Dw+--~~__``_.-vygggg~sg__tf___|Hj$L_2294bKC. W`TN9\\Wd6kzcEE____C```R\\.u\nkmkwwx/kc...___zg~++WnfZ$5wWv@Z$WWO#mnu;L  __y/f$Iqykc` qqgd$$kcsL._ ```      `s./`_    .     ``          `    ``` W4$~+_SSwvP~~_`C___.agwaddsUN$$7F--4vwmwuxw.71kyw-CZf5\\iCL_-W2l5f~j+Fm--.____ __~~V\nP|F PR\\LqgqipwvcCZ`__2 wa-RM$KGWMtWMODPP___.J/$PP~jTCL. jjTR$FhfhmiL___       ``+  x-  y  .  \\.       _..__        _-``$vg+______aaawwmmN_A+F~~~~t_|222LL2|gfh;nTDXCD\\K4.J$hiL.hmmwTZZI229h~m-C..___`3\nL.L___spwGQvW#fF9sw.--_______@@@_--___`___ua+AN|L_21k+c_j7FQ|EWAW7\\bxL._____   `j.`` .N/  . ._    ..  ---..    ____`` _gfC5._yaadpW#PPEE;KZZZZIZZ$axmmnu-wxmwj|Nj;h\\V4DPXd$QbL__.DD|hd+w-ZZZ`5~~wxcc_g\nh+c.pwm/@99KC``33#c~5`\\.....__-`ST`5___yaWdTF 4|uamPK`_WyZLUlkcL___2zhnwv...____y__          ...  z  ```     ___.g__s..`_yaapMANVP_5I5wmmnmmmhrn~+H1NTPNTP1Fh4:LJ;F+~j|K. 9fRKu.n ..W4OT#$~-- 7`Dh\\cc_\n7$\\kL_5S_2P`  ./A\\__g_gstr~tss.W/oV/_GV/##PLLWJ$MfPL .wV/AWgmkh WjayF`2Dth~~---uiLL____________          ___..aaasv-____J$#PRCC2$_wyrK4O1UPR$UTF`TN1N\\CUWM$|WrAWj|_`W.Z|L.Z`$kU$LwXt.p~ ___4Vg_  ``\\bx\n_$7hk--A4CC  zc__X4a.WW2PL__aobKGV` uW4fPR\\.wWd$$ZLLWd47CU/APT__yWfF5yW/`F``DDX#~mmwv--u......L.__________uizs/4_2;LLww--+S$L__ga4U/PNf#$F__7`|L_|p UJq:L$q|KCDJA1bwn+g+P+KL2ggyKL`\\___ q.._tp@.  `$7t\nutcL2___25k-_a\\x..~~+##X4LWd3KLL2LLudd$FK_J+F`jB;h\\y_AM$kW47/`\\v/N|_yZKD._qyWKCTZ22DN@@h~~~mmwmwww-----wwg@@___u$v@e~~f$ZZ3JaKNkdW/`_V/h7 WyrL|L_|P`4d6$FwjlL___Mj||NIU5Z1kks55$EkWu\\L..zjy_TF#\\L .Jf$\nM4kwx-.._`___@P~~~`7IL`5~~~~~@@@@--y49PFWW71LW4TFFjYL_J$kuWXC_mtKN$y/+P__Lq:F\\g+pyw....Z7TZZZDR@@@#N#X@@VVPP..zW+KC55wmmmmWdfPhfN7K\\./A  WWfFW+Hd|k_______2$k.L__q|.__W|g..F++@AKCWWmm~~yA\\McLL______t\nM`U7P5XiL..2##C____.~L..```____`---3TP$LWJr VrdfFWj1K42LLY`\\jy``Wjy/ACLqyzd$k M|yjyprrngjrwjy/q.p.$.LS./A35wv/~KG-WWdfPRCDN1K  _jTNjYK`_j;//N``N_|Lww44-.Wyrh;N.LuJk__jjupK`Z5$PA\\..NTC71Mf55bn-----qg\nkeK4Q7Q\\kL@@@pnssvurEEEi..._j.G``WW4TF___fF d|d$LW2lNamkH+L2lLL_W3/PR\\bu/FRlL_JnYu/BTTNI7_j7T|N+uizpvWcGSk-_ZC_.Vf$$RTDI$p41C|L..|UJ|L_WyrF`Nlwwm+P-~~55t#fFj1h+mH+Lu.dJTPkweWA\\.aiL_  /q$_`3TRQtWYA\\M\n  `W4tN$kVP--P@@RtU55FFjmpxWdA@9WWW$$DA\\41L_ggg___jydQ$K``WamKLWdffKW5WMTF`ywy/M|#+y/qy+jy/qyp/|NWPP|$W/c55wwxe~+P_AVphmTPj|K_yj;_y/__Wd/C__ym+~~T5`T ___``W`\\P`T``_yg~R4YFA\\Cwq;PA\\L   N1F R|_j|W;n3f\n____J$9ANYKD$k@FAxeL3$UA@R1NTS7____+fKUJPKq2g;G$.W`TPj|k  _7FhYd2$LLW/Z`\\_V/F7Q7/7Q.CV/C.X4U/kpx/C55x/jGwW/NU$CCl__2lL$ZALq.A\\V`\\W/KWW47A\\s./Z```____$$$-`AL2$ _.__.JFPP7\\L2J|LJlW`$LL___`__j\\Vd|Q|L`T\n.b--qdZ$M1kWtNmLUPF--+mQID1K____..2TZhaZC_j$RwMAtbWdNQ1h.Wd7CUwymmrW_|__wv//k_y/k|M+W/rVrn~M6V/```W/6VW/PR$awrnmuupMAmrL__j;D/|V|MKuV4M1LJ#KGwk-KGG--n+#YN@$y___jpaWPA\\L__~+M$WH+kW4$ku.__..___d;HiL__\nFhf`5$NtQ1F $P~+~``@7PR44M4NW44vmn#fC@fA--dIPPXP1VmFWd|N|WdfhW4T$PKW|AYg2/``_y/A\\WN|N|k4|Wj$kK`__W6$k__I._jMPFN7NfPKO7A|WWZ|KdlNxCuVwy/hWd#1k/K`@WdfC2I.`-_jlL$5wMPPLW4\\_W71N4TN|bWMfhhmwMmmW4vH|P1F--\nF7  q4Q$PFFWTC7T-  g```2T`\\.`~jfC57$$-CClUM4F``4ZIL _j|H`Wd1WW7NI.WV|NtEE;u.W/ANJ+/WU|kjwux/F`_y.__I.GwpmwrKC_uJMTC 7CUlL y/FH+P$$W4O7KUJ$P`\\.W_yZ_. -+___4$eW+PPR$L__Vd\\Vp JA1U\\V/NTQ7|WMfFU#XQlN`BW4\nP____jy/F` ``_~`` Y~\\______   _|..;22tbz~~dIlLdJ$EELWJ1N WZ1W2$NjrWj|Hj$PFNWM1L/`lLL/CW/P/`5W v+cL.j+d+R1PFL_gpfPAV/`WJAL./CV4ANW4f#7|h~dA_.p;Wqipu-WZI$wWdATKTCIZjrWNtddtLK.P\\VLJL__Mj|  `__`~N$kmrW@\n_la.uJ/FL   u..`` ```\\..G-.`__jipmmc _d$C ge;_~dfFF+Wd|L.q$FHjiUJLWJ1N4$FF `7DY;.`--qhxfK`y/e/Z`kWdJP`TD/`_WjIPL__JCUJKQyrwVrA1|PQ|K`|W/Rqy/P` J+K_|5armHfKD1kpmmNM1LgjTR$Lp\\V`+HmmWJ74\\L G/K. 31N$LWx\nmm++#/`  y  df\\V `\\ywsg~d|k__yymB$RELW4PL_GSEWT7`TCI_$|p+HILWj|p/kWJ1UZ$FLWJ$V`VWd$$WMPF W/FK`wd49PK` ./__adZLLLuVZhVAQV/CWj|Nq|L-+uyrKC_jfFL__``\\aaW#PTNTFH$1L|$LHJhiuTNQlL`\\V`TRR\\vwd\\LW/fh\\W``5++W4\njTT`________________________udf$PE_5\\VZ$VwH++P+L__wywj|L`7\\YWjfP1L2;hj$ZLWjrA|W`9-qjNPK__JTF_W4$VKC. -AC.prP$|hjVFFWIDW/FWW$kWd;K``U/___WdTL_i.._aMfK$$1N|KN7|NrP__j7AH|N41k_~+_____|_7A\\L__________`_\n_a._a.uuaay_wwaxuw___y.___aWdfFKCS.22F_./67TlLLu.VN1NJ1L.WJ|WJ$CLWy+W3$ELWZIN|L_|AN.P`_../_WW4MfPLp:K`54MMTR4lNJKCZJ$P_|W49fCW71hyx/CL.xcj$uWj+Wdxd$lLNWN|LJNNM|k.wJKC4lLdIL.Z`.a._S-k_7$LL..u.aiu.www\nddfkdthkdf#NM##fU6U4NY:WyzkWdTF`jjtKCFWiEF`~;~~q~+N$Uf$kwu4|K42r__|L df$kWuiv|uaicJfC|WfrNvWUI55$W/|__dJ$N1WjtHtLLqVZ__|wydiLj$LV/R1kW/fKdlVZO$W7A59A$PA\\uxPP_Q|ktgj__dtmjrhiuxcpaumtNWfQ$kuwVfRtHfKWd\nWdfF4AHfR@@~tCLz__zz5fhyzdIL2lLL.2RQkLd$ELL2ZZZSZI2TF~CUPN4C_N4CLq|L_$$TKLUzK|Wj+FF`\\y/CL__$kepv/``L.qjpL__ d$PAV+q|L.q|P/Fj;p$NX|L NVC___j$kMAHA$4TRTL_WffLLwu:nwqlLWdQLJ|N|N|PPj+Fkrd$MTFWPTRUTP1F_j\nZZIL``P\\5j-_9+---___Zl9XAh+mH`_y+R$H+F-@A+-----_jy__ `j|__3iLJfhqd:hJZD$hwV/RYKZZT`_J4$kXq./pFP`_.grndTFWu:udrKV`KdlLW91KC$prn_NA-- _j_u.wdfNfKC2ZN1Nwnxn_jpwVfANyH+E__jxy|N$MRKC4TP4$W4IhKCKTMWOKK.W4\nwqyg___2WdlWKC``_2SS--df$NIL__.ZT__RIL_ ___23.L.._3$LWjmmnwwq7FWj$P WjfFUjqirL_jrPK @@fP____arH|_U1LWfhVR1uYCuJ$FW71hnVKCUaV/1L_j4wgrKj$g;pj$CL__|N|NWVN|KCLJ:NIELLJNYKV|P tCLgfNl$EDL _ _W#\n Q$F---ggg;F .pxWKCCF ~jAL.gg+_uxFn---A....axqz__j+u3;K dTILPWN$KWj| 7 JdfFLWJPC  W7ZC..pxMCGlmN LgPR|D|H|LW44|NTNTD1FuqfKK wurdIZ$LprH$VWArnwu.J|__|hVuwW4:H+rhpw91N4.L _44kg;.44HEB;N..4C...__\n_4lL _qWZZ__.v/ _aYK .JFwxK4$W/AL _ _jjmwUI.Wj$L__j _ 1_WN| q_______g/ . _ w _ | _ _ _| _ _4| _ _ _ _|N$ _ _W _ qL______|___j| @gyg~rj;wy\n``` \n \n `\n````````````````````````````````````````````````````````````T\nkBBBBBBBpp____BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB9\nkBBBBp___wa-A.__BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB9\nkBBBq_w#-   \\.__BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB9\nkBBBJ4C_ . BpBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB9\nkBBBDW \\x_      ``\\.__BpBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB9\nkBBBBBDW \\L_       \\.__BpBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB9\nkBBBBBBBD7`\\L_ .__BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB9\nkBBBBBBBBB77 \\L_      ``\\.__BpBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB9\nkBBBBBBBBBBB77 .   \\v__BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB9\nkBBBBBBBBBBBBBW7`\\._ \\v__BpBBBBBBBBBBBBBBBBBBBBBBBBBBB9\nkBBBBBBBBBBBBBBBW7A.   \\v__BpBBBBBBBBBBBBBBBBBBBBBBBBB9\nkBBBBBBBBBBBBBBBBDW Ax   \\v__BBBBBBBBBBBBBBBBBBBBBBBBB9\nkBBBBBBBBBBBBBBBBBBDW \\x_         \\v__BBBBBBBBBBBBBBBBBBBBBBB9\nkBBBBBBBBBBBBBBBBBBBBD7 \\L_         \\v__BBBBBBBBBBBBBBBBBBBBB9\nkBBBBBBBBBBBBBBBBBBBBBBW7 \\L_         \\v__BpBBBBBBBBBBBBBBBBB9\nkBBBBBBBBBBBBBBBBBBBBBBBBW7 .   \\v._BBBBBBBBBBBBBBBBB9\nkBBBBBBBBBBBBBBBBBBBBBBBBBBW7 .   \\t._BBBBBBBBBBBBBBB9\nkBBBBBBBBBBBBBBBBBBBBBBBBBBBBW7 .   \\\\._BBBBBBBBBBBBB9\nkBBBBBBBBBBBBBBBBBBBBBBBBBBBBBDW7Ax._ \\._BBBBBBBBBBB9\nkBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBDW \\x_ ~._BBBBBBBBB9\nkBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBDW \\x_ ~._BpBBBBB9\nkBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBD7 \\L_ ~._BBBBB9\nkBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB77 \\L_ ~j.bBBB9\nkBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBW7 \\._       _./TPBBB9\nkBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBW7 .   __y/FDBBBBB9\nkBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBW79._y/PDBBBBBBB9\nkBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBDW``W$DBBBBBBBBB9\nkBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBZBBBBBBBBBBBB9\nkBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB9 \n \n ``` \n with flags  --layer=block1_conv1 --pool=3 \n``` \n            __                                                                  \n       __pgmm._                                                                \n   ___pgg#$RPbx._                                                              \n    wm#$RBgg##Pbx._                                                            \n    `7Xqgggggg##Pbm._                                                          \n      `5Rqggggggg#8bm._                                                        \n        `5Rqggggggg#8bmL_                                                      \n          `AmqggggggB#8bmL_                                                    \n           `7AmggBgggg##8bmL_                                                  \n             `7Xmgggggggg#@AmL_                                                \n               `7Xqgggggggg#@AmL_                                              \n                 `7Xqgg##ggg##@Dmg_                                            \n                   `5XqggBgggg##@Dmg_                                          \n                     `5Rqgggggggg#@Dmg_                                        \n                       `5Rqgggggggg#@Dmg_                                      \n                         7AmqgggggggB#@Dmg_                                    \n                          `7Amggggggg###@Dmg_                                  \n                            `7Xmggggggggg#@Dmg_                                \n                              `7Xqggggggggg#@Dbg_                              \n                                `7Xqgggggggg##@Dbg__                           \n                                  `5Xqggggggggg#@Dbg__                         \n                                    `5Rqggggggggg#@Dbg__                       \n                                      `5Kqggggggggg#@Dbg__                     \n                                        7AmBggggggggg#@Dbg__                   \n                                         `7Amgg#gggggg##@Pbg__                 \n                                           `7Xqgggggggggg#@Pbg__               \n                                             `7Xqgggggggggg#@Pbg__             \n                                               `7Xqgg#gggggg#B@Pbg__           \n                                                 `5Rqgg#gggggg##@Pbg._         \n                                                   `5Rqggggggggggg@Qbpw        \n                                                     `5mqggggggggg$9PCW        \n                                                      `7Amggg#gg$9PC           \n                                                        `7Xmgg$9PC             \n                                                          `7$VPC               \n                                                            7T\n \n ```", 'hubot-parking-cop \n Find out who\'s parking in your spot. \n Parking Cop is configured with a Google Sheet:tm: of user names and license plates. \n Usage \n @parkingcop bust <license-plate-number> \ne.g.\n @parkingcop bust ROMAN-1 \n If Parking Cop knows the license plate, it will acknowledge your request and private message the offender. \n Configuration \n The following environment variables are expected to be set:\n * PARKING_SPREADSHEET_ID - the id of your configuration spreadsheet\n * PARKINGCOP_CLIENT_EMAIL - see https://www.npmjs.com/package/google-spreadsheet for how to set up an api user\n * PARKINGCOP_PRIVATE_KEY - see https://www.npmjs.com/package/google-spreadsheet for how to set up an api user \n The following environment variables are optional and specific to your data:\n * PARKINGCOP_NAME_FIELD - field containing user names (default: "name")\n * PARKINGCOP_PLATE_FIELD - field containing license plates (default: "plate")', 'Simple Manual Image Categorizer \n Got unsorted images? Get off your duff and sort them.\nGive SMIC a source path full of unlabeled images and\na destination directory with a subdirectory for each\ncategory. The GUI will use the subdirectory names as\nlabels. For speedy categorization, the numbers 1-9\nare hotkeys to assign their respective category. \n Installation \n pip install simple-manual-image-categorizer \n Example \n To sort a hotdog detector dataset: \n smic path/to/source/hotdog/images path/of/destination \n Where  path/to/destination  has subdirectories  hotdog  and  not_hotdog', "PyTorch-CNS \n An implementation of  Generalized Compressed Network Search \nfor PyTorch models. \n This creates a genome per layer, rather than a single one for the entire model as is described in the paper. \n There are two optimizers:\n * Asynchronous Gene Pool: a master list of genomes, sorted by fitness, is worked\n on by a pool of agents which draw fit genomes for mutation and evaluation.\n * Synchronous Score Swap: each worker maintains a full copy of all genomes and\n only fitness scores are exchanged. \n Installation \n Install python package with  pip install pytorch-cns \n Install  redis  which is used as the datastore. The examples\nexpect a redis instance listening on localhost at the default port. You can change\nthis by passing a JSON dict of  StrictRedis  kwargs via the  --redis-params  command\nline option. \n AI Gym Examples \n \n aigym.py : CartPole! \n atari.py : Atari ram-based games \n atari_pix.py : Atari pixel-based games \n atari_pixrnn_gpa.py : Atari pixel-based games with a recurrent neural network,\n using the asynchronous gene pool optimizer. \n atari_pixrnn_ss.py : Atari pixel-based games with a recurrent neural network,\n using the synchronous \n \n Install additional requirements:  pip install gym atari_py box2d \n To run a pool of workers with default settings simply run the python file\n(e.g.  python atari_pix.py ). If you make any changes to the hyperparameters\nyou'll to use the  --clear-store  flag which deletes the old gene pool upon start.\nUse  --num-agents  to customize the number of child processes spawned. \n Invoke the example with  python atari_pix.py --render --best  to run the simulation\nwith the fittest genome. This can be done at the same time as the workers are\nrunning to monitor progress. \n Image generation \n These do not converge on anything at the moment. Maybe, if you are a real GANimal,\nyou can find the right configuration. The code here is hacked together from\nthe pytorch example repo. \n cnsdcgan.py : DCGAN adapted from the PyTorch DCGAN example. Attempts to train\nboth the discriminator and generator with compressed network search. \n vggmse.py : An autoencoder which uses VGG16 to calculate the loss.", 'Twitter Code Listener \n Find codes (QR, UPC, and more) in images posted on Twitter. \n Usage \n \n pip install -r requirements.txt \n Make sure the following environment variables are set:\n TWITTER_CONSUMER_KEY\nTWITTER_CONSUMER_SECRET\nTWITTER_ACCESS_TOKEN\nTWITTER_ACCESS_TOKEN_SECRET \n python code_listener.py \n \n I had issues installing  zbar  on OSX so there is a simple\ndocker compose setup:\n* Make sure the following environment variables are set (or edit  docker-compose.yml ):\n TWITTER_CONSUMER_KEY\nTWITTER_CONSUMER_SECRET\nTWITTER_ACCESS_TOKEN\nTWITTER_ACCESS_TOKEN_SECRET \n*  docker-compose up', "mindstate \n A toolkit for machine learning models powered by compressed\nnetwork search. \n Install \n Download this repo and  pip install -e . \n Examples \n Start a redis server:\n *  redis-server \n Install example requirements:\n *  pip install -r example-requirements.txt \n Start a group of workers: \n \n cd examples \n python aigym_pix_att.py --num-agents=10 --clear-store \n \n Don't use  --clear-store  if you want to preserve the current genepool state. \n View the performance of the current best genome: \n \n python aigym_pix_att.py --exhibition \n", 'numpy-binnn \n Some codes for binary neural networks in numpy. WIP \n Install \n pip install numpy-binnn \n Usage \n Cython-accelerated bitwise XNOR product: \n from binnn.xnor_product import xnor_product \n Per-bit spectrum of a vector: \n from binnn.utils import bit_spectrum', 'nyeat \n NEAT + CPPN implemented with numpy and networkx.', "kiqn \n Implicit Quantile Network  layer for Keras. Could use some work--PRs welcome. \n Usage \n ```\nfrom kiqn.layers import IQN\nfrom kiqn.losses import iqn_loss \n ...your good codes... \n x_in = Input(batch_shape=(batch_size, input_dims,))\ny_in = Input(batch_shape=(batch_size, output_dims,)) \n ...more good codes... \n x_tau  contains the input features conditioned on the quantile embeddings \n taus  are the quantile samples \n x_tau, taus = IQN(\n    num_quantiles=32,\n    embedding_dims=64,\n    name='iqn'\n)(x_features) \n ... y_out = f(x_tau) ... \n model = Model(inputs=[x_in, y_in], outputs=[y_out])\nmodel.add_loss(iqn_loss(y_in, y_out, taus))\nmodel.compile(optimizer='adam') \n Since we don't assign loss functions in  compile , don't send any targets \n model.fit([train_x, train_y], []) \n Use it \n f_output = K.function(\n    [model.inputs[0]],\n    [model.get_layer('y_out').output]\n)\n```", "bigmouth \n Render images from text. \n Data \n \n Download word vectors from https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download \n Flickr8k \n \n Use  bigmouth.preprocess  to find BigGAN input codes which approximate\nthe dataset images. Here's a  pre-trained one to get started .\nUse  bigmouth.train_preprocessed  to learn a mapping from document vectors\nto BigGAN inputs.", "tartangan \n Use a GAN to model tartan patterns. Big ol' work in progress. \n Preparing a Dataset \n Here's  an archive of tartan images  to get you started. \n There are two ways of providing your dataset: \n ImageFolderDataset: This loader will lazily resize the images the first time\nthey're used in training. This is easy to use but is slower during the first\nepoch due to the loading and resizing being done at run-time. Simply pass the\nroot path of your images to the trainer as the  dataset  argument. \n ImageBytesDataset: Loads an archive of images which have already been resized\nso it doesn't have the first epoch slowness. To prepare your images for use with\nthis loader run: \n python -m tartangan.image_bytes_dataset --resize=$SIZE $PATH_TO_IMAGES $OUTPUT_FILENAME \n Then pass  $OUTPUT_FILENAME  to the trainer as the  dataset  argument. \n Training \n \n Clone and install this repo \n Optionally pre-resize your dataset as described in the  Preparing Dataset  section \n python -m tartangan.trainers.cnn $DATASET  to train a SA-GAN alike model \n python -m tartangan.trainers.iqn $DATASET  to train a SA-GAN-IQN model \n There are many CLI options for the trainer. Run with  --help  for more information. \n \n Test metrics \n I've adapted code from the excellent  BigGAN  to calculate the Inception Score and Frechet Inception Distance, popular measures of how closely the generator output matches the real data. \n First precalculate the inception moments of the dataset with: \n python3 -m tartangan.calculate_inception_moments $DATASET $IM_FILENAME \n Then run the trainer with the  --fid --inception-moments=$IM_FILENAME  arguments. Calculating these takes a bit of time. You can choose how frequently to run the tests with  --fid-freq  and the number of samples drawn from the generator with  --n-inception-imgs . \n The scores will be printed during training and you can choose to have it output final scores to a JSON-encoded file with  --metrics-path . You can switch between metrics backends with the\n --metrics-collector  arg which takes  katib ,  kubeflow , or  tensorboard . \n Resume from checkpoint \n To resume training, specify the CLI arguments  --run-id  and  --resume-training-step \n run-id  is the path segment which looks like a datetime with a random suffix or\na custom one you've decided upon and  resume-training-step  should be a number that\nappears in the checkpoints directory.", 'ShuffleGen \n Given a shuffled list of image pieces, recreate the original image.', 'rl_reader \n Learn to read using reinforcement learning. \n Development \n Use docker-compose for a reproducible workflow.\n * build image with  docker-compose build \n * run stuff in container with  ./dc_run COMMAND AND WHAT HAVE YOU']
durandtibo,['heatmap-matlab \n MATLAB code to generate heatmap. \n  |   |  \n:---:|:---:|:---:\nOriginal image | Heatmap | image + heatmap \n Visual results for different values of transparency \n  |   |  \n:---:|:---:|:---:\nalpha=0.45 | alpha=0.65 | alpha=0.85', 'MANTRA \n Install \n When you are in the directory mantra-python, you can install mantra package with the following command:\n python\npython3 setup.py install \nThe code was developped with Python3. \n Demo \n A demo file is in directory demo. To run the demo, execute the command\n python\npython3 demo.py \nYou need to change the path to the data. \n[Download toys data] (http://webia.lip6.fr/~durandt/data/mantra/data.tar.gz) \n Citing this repository \n If you find this code useful in your research, please consider citing us: \n @inproceedings{Durand_MANTRA_ICCV_2015,\nAuthor = {Thibaut Durand and Nicolas Thome and Matthieu Cord},\nTitle = {{MANTRA: Minimum Maximum Latent Structural SVM for Image Classification and Ranking}},\nbooktitle = {IEEE International Conference on Computer Vision (ICCV)},\nYear = {2015}\n} \n Licence \n MIT License', 'Spatial Pooling for Torch7 \n This repositery proposed the implementation of several spatial poolings used for weakly supervised learning of deep ConvNets. \n Installation \n $ git clone https://github.com/durandtibo/spatial-pooling.torch.git\n$ cd spatial-pooling.torch\n$ luarocks make rocks/spatial-pooling-scm-1.rockspec \nTo test the installation, you can run\n $ th test/test.lua \n Modules \n \n GlobalMaxPooling \n GlobalkMaxPooling \n GlobalAveragePooling \n LogSumExpPooling \n WeldonPooling \n \n \n GlobalMaxPooling (GMP) \n Global Max Pooling is a spatial pooling strategy used in  "Is object localization for free? – Weakly Supervised Object Recognition with Convolutional Neural Networks " . \n lua\nmodule = nn.GlobalMaxPooling() \n Applies 2D max-pooling operation on the whole image. The number of output features is equal to the number of input planes. \n If the input image is a 4D tensor  nBatchImage x nInputPlane x w x h , the output image size will be  nBatchImage x nInputPlane x 1 x 1  where  w  and  h  are spatial image dimensions. \n If the input image is a 3D tensor  nInputPlane x w x h , the output image size will be  nInputPlane x 1 x 1  where  w  and  h  are spatial image dimensions. \n Equation \n We note  z^c  the c-th map of input, and  s^c  the c-th map of the output.\n s^c = max_{i,j} z^c_{i,j} \n References \n @inproceedings{Oquab_DeepMIL_CVPR15,\nauthor = "Oquab, M. and Bottou, L. and Laptev, I. and Sivic, J.",\ntitle = "Is object localization for free? – Weakly-supervised learning with convolutional neural networks",\nbooktitle =  "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",\nyear = "2015"\n} \n \n GlobalkMaxPooling \n GlobalkMaxPooling is a generalization of GlobalMaxPooling to multiple maximums. This spatial pooling strategy is inspired from Top Instances model  "Multiple Instance Learning for Soft Bags via Top Instances" . \n lua\nmodule = nn.GlobalkMaxPooling(kMax) \nApplies 2D k-max-pooling operation on the whole image. The number of output features is equal to the number of input planes. \n The parameter is the following:\n  *  kMax : The number of top instances.  kMax  can defined the number of selected regions ( kMax >= 1 ) or the proportion of selected regions ( 0 < kMax < 1 ). If  kMax <= 0 , all the regions are selected.\n  Default is  kMax = 1 . \n If the input image is a 4D tensor  nBatchImage x nInputPlane x w x h , the output image size will be  nBatchImage x nInputPlane x 1 x 1  where  w  and  h  are spatial image dimensions. \n If the input image is a 3D tensor  nInputPlane x w x h , the output image size will be  nInputPlane x 1 x 1  where  w  and  h  are spatial image dimensions. \n Equation \n We note  z^c  the c-th map of input, and  s^c  the c-th map of the output.\n s^c = max_{h in H_kMax} 1 / kMax sum_{i,j} h_{i,j} z^c_{i,j} \nwhere  H_k  is such that  h in H_k  satisfies  h_{i,j} in {0, 1}  and  sum_{i,j} h_{i,j} = k \n Special cases \n \n GlobalMaxPooling: if  kMax = 1 \n GlobalAveragePooling: if  kMax = h x w \n \n \n GlobalAveragePooling (GAP) \n Global Max Pooling is a spatial pooling strategy used in  "Learning Deep Features for Discriminative Localization " . \n lua\nmodule = nn.GlobalAveragePooling() \n Applies 2D average-pooling operation on the whole image. The number of output features is equal to the number of input planes. \n If the input image is a 4D tensor  nBatchImage x nInputPlane x w x h , the output image size will be  nBatchImage x nInputPlane x 1 x 1  where  w  and  h  are spatial image dimensions. \n If the input image is a 3D tensor  nInputPlane x w x h , the output image size will be  nInputPlane x 1 x 1  where  w  and  h  are spatial image dimensions. \n Equation \n We note  z^c  the c-th map of input, and  s^c  the c-th map of the output.\n s^c = 1 / (h * w) sum_{i,j} z^c_{i,j} \n References \n @inproceedings{Zhou_2016_CVPR,\nauthor = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},\ntitle = {{Learning Deep Features for Discriminative Localization}},\nbooktitle = {CVPR},\nyear = {2016}\n} \n \n LogSumExpPooling \n LogSumExpPooling is a spatial pooling strategy used in  "From Image-level to Pixel-level Labeling with Convolutional Networks " . \n lua\nmodule = nn.LogSumExpPooling(beta) \n Applies 2D LogSumExp-pooling operation on the whole image. The number of output features is equal to the number of input planes. \n The parameter is the following:\n  *  beta : The anti-temperature parameter. Default is  beta=1 . \n If the input image is a 4D tensor  nBatchImage x nInputPlane x w x h , the output image size will be  nBatchImage x nInputPlane x 1 x 1  where  w  and  h  are spatial image dimensions. \n If the input image is a 3D tensor  nInputPlane x w x h , the output image size will be  nInputPlane x 1 x 1  where  w  and  h  are spatial image dimensions. \n Equation \n We note  z^c  the c-th map of input, and  s^c  the c-th map of the output.\n s^c = log 1 / (h * w) sum_{i,j} exp(beta * z^c_{i,j}) \n Special cases \n \n GlobalMaxPooling: if  beta = +inf \n GlobalAveragePooling: if  beta = 0 \n \n References \n @inproceedings{pinheiro_weak_seg_cvpr15,\nAuthor = {Pedro O. Pinheiro and Ronan Collobert},\nTitle = {{From Image-level to Pixel-level Labeling with Convolutional Networks}},\nbooktitle = {CVPR},\nYear = {2015}\n} \n \n WeldonPooling \n WeldonPooling is a spatial pooling module used in  "WELDON: Weakly Supervised Learning of Deep Convolutional Neural Networks" . \n lua\nmodule = nn.WeldonPooling(kMax, kMin) \n Applies 2D WELDON-pooling operation on the whole image. The number of output features is equal to the number of input planes. \n The parameters are the following:\n  *  kMax : The number of top instances. It is possible to define the number of selected regions ( kMax >= 1 ) or the proportion of selected regions ( 0 <= kMax < 1 ). If  kMax < 0 ,  kMax  is set to  0 . Default is  kMax = 1 .\n  *  kMin : The number of low instances. It is possible to define the number of selected regions ( kMin >= 1 ) or the proportion of selected regions ( 0 <= kMin < 1 ). If  kMin < 0 ,  kMin  is set to  0 . Default is  kMin = 1 . \n If the input image is a 4D tensor  nBatchImage x nInputPlane x w x h , the output image size will be  nBatchImage x nInputPlane x 1 x 1  where  w  and  h  are spatial image dimensions. \n If the input image is a 3D tensor  nInputPlane x w x h , the output image size will be  nInputPlane x 1 x 1  where  w  and  h  are spatial image dimensions. \n Equation \n We note  z^c  the c-th map of input, and  s^c  the c-th map of the output.\n s^c = max_{h in H_kMax} 1 / kMax sum_{i,j} h_{i,j} z^c_{i,j} + min_{h in H_kMin} 1 / kMin sum_{i,j} h_{i,j} z^c_{i,j} \nwhere  H_k  is such that  h in H_k  satisfies  h_{i,j} in {0, 1}  and  sum_{i,j} h_{i,j} = k \n Special cases \n \n GlobalMaxPooling: if  kMax = 1  and  kMin = 0 \n GlobalAveragePooling: if  kMax = h x w  and  kMin = 0 \n MantraPooling: if  kMax = 1  and  kMin = 1 \n \n References \n @inproceedings{Durand_WELDON_CVPR_2016,\nauthor = {Durand, Thibaut and Thome, Nicolas and Cord, Matthieu},\ntitle = {{WELDON: Weakly Supervised Learning of Deep Convolutional Neural Networks}},\nbooktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\nyear = {2016}\n} \n Licence \n MIT License', 'Weakly Supervised Learning of ResNet \n This implements weakly supervised learning of residual networks.\nThis  Torch  implementation is based on  fb.resnet.torch . \n Download pretrained models \n The  download_pretrained_models  script downloads pretrained models in  data/pretrained_models  directory. \n th download_pretrained_models.lua \n Packages \n This implementation uses the following packages:\n* torch\n* nn\n* cunn\n* cudnn\n* optim\n* paths\n* csvigo\n* matio \n You also need to install  spatial-pooling.torch  package to have spatial pooling modules. \n Training \n To train ResNet-101 with WELDON pooling on VOC 2007 dataset, run  main.lua  with\n th main.lua -optim sgd -LR 1e-2 -netType resnet101-weldon -batchSize 40 -imageSize 224 -data /path_dataset/VOCdevkit/VOC2007/ -dataset voc2007-cls -loss MultiLabel -train multilabel -k 15 -nEpochs 20 \n*  LR : initial learning rate.\n*  imageSize : size of the image.\n*  batchSize : number of images per batch\n*  k : number of regions for WELDON pooling.\n*  nEpochs : number of training epochs. \n To train ResNet-101 with GlobalMaxPooling on VOC 2007 dataset, run  main.lua  with\n th main.lua -optim sgd -LR 1e-2 -netType resnet101-gmp -batchSize 40 -imageSize 224 -data /path_dataset/VOCdevkit/VOC2007/ -dataset voc2007-cls -loss MultiLabel -train multilabel -k 15 -nEpochs 20 \n To train ResNet-101 with GlobalAveragePooling on VOC 2007 dataset, run  main.lua  with\n th main.lua -optim sgd -LR 1e-2 -netType resnet101-gap -batchSize 40 -imageSize 224 -data /path_dataset/VOCdevkit/VOC2007/ -dataset voc2007-cls -loss MultiLabel -train multilabel -k 15 -nEpochs 20 \n License \n MIT License', 'heatmap \n Python package to combine image and heatmap \n Install \n python\npython3 setup.py install \nThe code was developped with Python3. \n Demo \n A demo file is in directory demo. To run the demo, execute the command in demo directory\n python\npython3 demo_heatmap.py \n Visual results for different values of alpha \n  |   |  \n:---:|:---:|:---:\nalpha=0.0 | alpha=0.1 | alpha=0.2\n  |   |  \nalpha=0.3 | alpha=0.4 | alpha=0.5\n  |   |  \nalpha=0.6 | alpha=0.7 | alpha=0.8\n  |   | \nalpha=0.9 | alpha=1.0 |  \n Licence \n MIT License', 'weldon.resnet.pytorch \n @inproceedings{Durand_WELDON_CVPR_2016,\nauthor = {Durand, Thibaut and Thome, Nicolas and Cord, Matthieu},\ntitle = {{WELDON: Weakly Supervised Learning of Deep Convolutional Neural Networks}},\nbooktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\nyear = {2016}\n}', 'wildcat.pytorch \n PyTorch implementation of "WILDCAT: Weakly Supervised Learning of Deep ConvNets for Image Classification, Pointwise Localization and Segmentation", CVPR 2017 (http://webia.lip6.fr/~durandt/pdfs/2017_CVPR/Durand_WILDCAT_CVPR_2017.pdf) \n Requirements \n Please, install the following packages\n- numpy\n- torch\n- torchnet\n- torchvision\n- tqdm \n Options \n \n k : number of regions for the spatial pooling. If  k  is larger than 1,  k  is the number of regions, otherwise  k  is the proportion of selected regions.  k=0.2  means that 20% of the regions are used.   \n maps : number of maps for each class \n alpha : weight for minimum regions \n lr : learning rate \n lrp : factor for learning rate of pretrained layers. The learning rate of the pretrained layers is  lr * lrp \n batch-size : number of images per batch \n image-size : size of the image \n epochs : number of training epochs \n \n Demo VOC 2007 \n sh\npython3 -m wildcat.demo_voc2007 ../data/voc --image-size 448 --batch-size 16 --lrp 0.1 --lr 0.01 --epochs 20 --k 0.2 --maps 8 --alpha 0.7 \n Demo MIT67 \n sh\npython3 -m wildcat.demo_mit67 ../data/mit67 --image-size 448 --batch-size 16 --lrp 0.1 --lr 0.001 --epochs 20 --k 0.4 --maps 8 \n Citing this repository \n If you find this code useful in your research, please consider citing us: \n @inproceedings{Durand_WILDCAT_CVPR_2017,\nauthor = {Durand, Thibaut and Mordan, Taylor and Thome, Nicolas and Cord, Matthieu},\ntitle = {{WILDCAT: Weakly Supervised Learning of Deep ConvNets for Image Classification, Pointwise Localization and Segmentation}},\nbooktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\nyear = {2017}\n} \n Licence \n MIT License', 'deep_archi_latex \n VGG16 \n \n @inproceedings { Simonyan2015,\n    year = {2015},\n    booktitle = {International Conference on Learning Representations (ICLR)},\n    title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},\n    author = {Karen Simonyan and Andrew Zisserman}\n} \n WELDON \n \n @inproceedings{Durand2016,\n  author = {Durand, Thibaut and Thome, Nicolas and Cord, Matthieu},\n  title = {{WELDON: Weakly Supervised Learning of Deep Convolutional Neural Networks}},\n  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year = {2016}\n} \n WILDCAT \n \n @inproceedings{Durand2017,\n  author = {Durand, Thibaut and Mordan, Taylor and Thome, Nicolas and Cord, Matthieu},\n  title = {{WILDCAT: Weakly Supervised Learning of Deep ConvNets for Image Classification, Pointwise Localization and Segmentation}},\n  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year = {2017}\n} \n VGG-M \n \n @inproceedings { Chatfield2014,\n    year = {2014},\n    booktitle = {British Machine Vision Conference (BMVC)},\n    title = {Return of the Devil in the Details: Delving Deep into Convolutional Nets},\n    author = {Ken Chatfield and Karen Simonyan and Andrea Vedaldi and Andrew Zisserman}\n}', 'segmentation_sol \n Installation \n \n Install  conda  (https://conda.io/miniconda.html) or update it\n conda update conda \n Tp create a new environment  segsol , run the command:\n conda create -n segsol python=3.6 \n To activate this environment, run the command:\n source activate segsol \n To install  pytorch  package (http://pytorch.org/), run the command:\n conda install pytorch torchvision -c pytorch \n To install other package, run the command: \n pip install tqdm \n To clone this repository, run the command: \n git clone https://github.com/durandtibo/segmentation_sol.git \n \n Evaluation \n From the folder  segmentation_sol , to predict the mask of one image, run the command: \n python -m segmentation.main --image data/test.JPG --max_size 1000 --output_dir outputs \n From the folder  segmentation_sol , to predict the mask of all the images in a directory, run the command: \n python -m segmentation.main --image data --max_size 1000 --output_dir outputs \n List of options\n-  image : the filename or the directory of the image\n-  checkpoint_dir : the directory where the checkpoint is\n-  output_dir : the directory where to save the results \n-  max_size : the size of the smaller edge of the image', "\n \n Bootstrap  is a high-level framework for starting deep learning projects.\nIt aims at accelerating research projects and prototyping by providing a powerful workflow focused on your dataset and model only. \n And it is: \n \n Scalable \n Modular \n Shareable \n Extendable \n Uncomplicated \n Built for reproducibility \n Easy to log and plot anything \n \n It's not a wrapper over pytorch, it's a powerful extension. \n Quick tour \n To run an experiment (training + evaluation):\n python -m bootstrap.run\n       -o myproject/options/sgd.yaml \n To display parsed options from the yaml file:\n python -m bootstrap.run\n       -o myproject/options/sgd.yaml\n       -h \n Running an experiment will create 4 files, here is an example with  mnist : \n \n options.yaml  contains the options used for the experiment, \n logs.txt  contains all the information given to the logger. \n logs.json  contains the following data: train_epoch.loss, train_batch.loss, eval_epoch.accuracy_top1, etc. \n view.html  contains training and evaluation curves with javascript utilities (plotly). \n \n To save the next experiment in a specific directory:\n python -m bootstrap.run\n       -o myproject/options/sgd.yaml\n       --exp.dir logs/custom \n To reload an experiment:\n python -m bootstrap.run\n       -o logs/custom/options.yaml\n       --exp.resume last \n Documentation \n The package reference is available on the  documentation website . \n It also contains some notes: \n \n Installation \n Concepts \n Quickstart \n Directories \n Examples \n \n Official project modules \n \n mnist.bootstrap.pytorch  is a useful example for starting a quick project with bootstrap \n vision.bootstrap.pytorch  contains utilities to train image classifier, object detector, etc. on usual datasets like imagenet, cifar10, cifar100, coco, visual genome, etc. \n recipe1m.bootstrap.pytorch  is a project for image-text retrieval related to the Recip1M dataset developped in the context of a  SIGIR18 paper . \n block.bootstrap.pytorch  is a project focused on fusion modules related to the VQA 2.0, TDIUC and VRD datasets developped in the context of a  AAAI19 paper . \n \n Poster \n"]
jravetch,['StarCloud \n An Android Cloud Testing Framework', 'test', 'Welcome to GitHub Training \n Class Goals \n During this class, you will learn:\n- How git and GitHub work together to version control your projects.\n- A baseline of information on how GitHub enables collaboration within your work team.\n- How to use the GitHub platform and GitHub desktop applications. \n Class Activities \n Here are the activities you will complete in this class. If you get stuck or have any questions, please refer to the documents linked here for more information. \n \n Comment on  this issue  with one thing you are hoping to learn in todays training. \n Create an issue to add your bio to the class site.  Instructions \n Add a bio to the class site.  Instructions \n Comment on a Pull Request. \n Make a change to your bio on the class site.  Instructions \n Merge your Pull Request.  Instructions \n Fork this repository. \n Clone this repository to your desktop.   Instructions \n Make a change to your bio using the desktop apps. [Instructions] (https://github.com/certify/template-everyone/blob/master/instructions/make-changes-on-desktop.md) \n Create a new file using the desktop apps.  Instructions \n Push your changes to GitHub.  Instructions \n Merge your changes.  Instructions \n']
mkgobaco,['upgrade \n API docs via Swagger\nhttp://localhost:8080/swagger-ui.html \n Highlights:\n1.  The main Spring Boot Application class is CampsiteApplication.java.\n2.  The main and only RestController class is CampsiteController.java.\n3.  There are two tables.\n    a.  Reservations table \n    b.  Schedules table (One date per row.   Many-to-one relation to Reservations table via BookingId)\n4.  Pessimistic locking on the persistence layer is used to maintain data integrity during concurrent requests.\n    The com.upgrade.campsite.services.CampsiteServiceTest.testAsynchronous test is used to test if\n    pessimistic locking is working.\n5.  Sample REST calls in upgrade.postman_collection.json can be imported to PostMan\n6.  The response objects will have a non-empty errors json array if there are any errors.\n7.  A file-based H2 database is used.  To get to it while the rest service is running, \ngo to http://localhost:8080/h2-console, username: sa, blank password.\n8. For testing purposes, two months worth of sample data are created from the date the microservice is started. \n Sample REST calls Quick Start:\n1.  Making a reservation \n URL: \nhttp://localhost:8080/reserve\nPOST Body: \n{\n    "firstName": "Michael",\n    "lastName": "Jordan",\n    "email": "michael@jordan.com",\n    "checkInDate": "2020-07-01",\n    "checkOutDate": "2020-07-03"\n} \n \n \n Cancelling a reservation\nURL: http://localhost:8080/cancel\nPOST Body: \n{\n   "bookingId": "DOOVO"\n} \n \n \n Modifying a reservation\nPOST URL: http://localhost:8080/modify\nBody:\n{\n    "firstName": "Michael",\n    "lastName": "Jordan",\n    "email": "michael@jordan.com",\n    "checkInDate": "2020-07-06",\n    "checkOutDate": "2020-07-09",\n    "bookingId": "BSFJH"\n} \n \n \n Requesting for available dates\nPOST URL: http://localhost:8080/available\nBody:\n{\n   "startDate": "2020-07-01",\n   "endDate": "2020-09-31"\n} \n \n \n Get info about given bookingId\nGET URL: http://localhost:8080/reservation?bookingId=BSFJH \n \n \n Get all reservations\nGET URL: http://localhost:8080/reservations \n \n \n Get all schedules\nGET URL: http://localhost:8080/schedules \n \n', "A Next.js starter for the  JAMstack \n This is a boilerplate for using  Next.js  as a static site generator. \n \n Usage \n Getting started \n To start your project, either: \n \n Deploy to Netlify using the button above, or \n Clone this repository and run: \n \n bash\nnpm install \n This will take some time and will install all packages necessary to run the starter. \n Development \n While developing your website, use: \n bash\nnpm start \n Then visit http://localhost:3000/ to preview your new website. The Next.js development server will automatically reload the CSS or refresh the whole page, when stylesheets or content changes. \n Static build \n To build a static version of the website inside the  /dist  folder, run: \n bash\nnpm run build \n See  package.json  for all tasks. \n Basic Concepts \n You can read more about building sites and apps with Next.js in their documentation here: \n https://nextjs.org/docs \n Doing dynamic things \n A few resources for doing anything you can imagine with a 100% static site/app on the JAMstack\nusing Next.js. If you would like to add more resources please open a pull request! \n \n Using Next.js as a Static Site Generator for Netlify  -  Shawn Wang \n Serverless Next.js 9 on Netlify Functions  -  Shawn Wang \n \n Deploying to Netlify \n The deploy to Netlify button above will create a new site and repo in one click. If you've created your repo manually, you can deploy to Netlify as follows: \n \n Push your clone to your own GitHub repository. \n Create a new site on Netlify  and link the repository. \n \n Now Netlify will build and deploy your site whenever you push to git.", 'mkgobaco.github.io', "jhipsterSampleApplication \n This application was generated using JHipster 7.1.0, you can find documentation and help at  https://www.jhipster.tech/documentation-archive/v7.1.0 . \n Development \n Before you can build this project, you must install and configure the following dependencies on your machine: \n \n Node.js : We use Node to run a development web server and build the project.\n   Depending on your system, you can install Node either from source or as a pre-packaged bundle. \n \n After installing Node, you should be able to run the following command to install development tools.\nYou will only need to run this command when dependencies change in  package.json . \n npm install \n We use npm scripts and  Angular CLI  with  Webpack  as our build system. \n Run the following commands in two separate terminals to create a blissful development experience where your browser\nauto-refreshes when files change on your hard drive. \n ./mvnw\nnpm start \n Npm is also used to manage CSS and JavaScript dependencies used in this application. You can upgrade dependencies by\nspecifying a newer version in  package.json . You can also run  npm update  and  npm install  to manage dependencies.\nAdd the  help  flag on any command to see how you can use it. For example,  npm help update . \n The  npm run  command will list all of the scripts available to run for this project. \n PWA Support \n JHipster ships with PWA (Progressive Web App) support, and it's turned off by default. One of the main components of a PWA is a service worker. \n The service worker initialization code is disabled by default. To enable it, uncomment the following code in  src/main/webapp/app/app.module.ts : \n typescript\nServiceWorkerModule.register('ngsw-worker.js', { enabled: false }), \n Managing dependencies \n For example, to add  Leaflet  library as a runtime dependency of your application, you would run following command: \n npm install --save --save-exact leaflet \n To benefit from TypeScript type definitions from  DefinitelyTyped  repository in development, you would run following command: \n npm install --save-dev --save-exact @types/leaflet \n Then you would import the JS and CSS files specified in library's installation instructions so that  Webpack  knows about them:\nEdit  src/main/webapp/app/app.module.ts  file: \n import 'leaflet/dist/leaflet.js'; \n Edit  src/main/webapp/content/scss/vendor.scss  file: \n @import '~leaflet/dist/leaflet.css'; \n Note: There are still a few other things remaining to do for Leaflet that we won't detail here. \n For further instructions on how to develop with JHipster, have a look at  Using JHipster in development . \n Using Angular CLI \n You can also use  Angular CLI  to generate some custom client code. \n For example, the following command: \n ng generate component my-component \n will generate few files: \n create src/main/webapp/app/my-component/my-component.component.html\ncreate src/main/webapp/app/my-component/my-component.component.ts\nupdate src/main/webapp/app/app.module.ts \n Doing API-First development using openapi-generator \n OpenAPI-Generator  is configured for this application. You can generate API code from the  src/main/resources/swagger/api.yml  definition file by running: \n bash\n./mvnw generate-sources \n Then implements the generated delegate classes with  @Service  classes. \n To edit the  api.yml  definition file, you can use a tool such as  Swagger-Editor . Start a local instance of the swagger-editor using docker by running:  docker-compose -f src/main/docker/swagger-editor.yml up -d . The editor will then be reachable at  http://localhost:7742 . \n Refer to  Doing API-First development  for more details. \n Building for production \n Packaging as jar \n To build the final jar and optimize the jhipsterSampleApplication application for production, run: \n ./mvnw -Pprod clean verify \n This will concatenate and minify the client CSS and JavaScript files. It will also modify  index.html  so it references these new files.\nTo ensure everything worked, run: \n java -jar target/*.jar \n Then navigate to  http://localhost:8080  in your browser. \n Refer to  Using JHipster in production  for more details. \n Packaging as war \n To package your application as a war in order to deploy it to an application server, run: \n ./mvnw -Pprod,war clean verify \n Testing \n To launch your application's tests, run: \n ./mvnw verify \n Client tests \n Unit tests are run by  Jest . They're located in  src/test/javascript/  and can be run with: \n npm test \n For more information, refer to the  Running tests page . \n Code quality \n Sonar is used to analyse code quality. You can start a local Sonar server (accessible on http://localhost:9001) with: \n docker-compose -f src/main/docker/sonar.yml up -d \n Note: we have turned off authentication in  src/main/docker/sonar.yml  for out of the box experience while trying out SonarQube, for real use cases turn it back on. \n You can run a Sonar analysis with using the  sonar-scanner  or by using the maven plugin. \n Then, run a Sonar analysis: \n ./mvnw -Pprod clean verify sonar:sonar \n If you need to re-run the Sonar phase, please be sure to specify at least the  initialize  phase since Sonar properties are loaded from the sonar-project.properties file. \n ./mvnw initialize sonar:sonar \n For more information, refer to the  Code quality page . \n Using Docker to simplify development (optional) \n You can use Docker to improve your JHipster development experience. A number of docker-compose configuration are available in the  src/main/docker  folder to launch required third party services. \n For example, to start a postgresql database in a docker container, run: \n docker-compose -f src/main/docker/postgresql.yml up -d \n To stop it and remove the container, run: \n docker-compose -f src/main/docker/postgresql.yml down \n You can also fully dockerize your application and all the services that it depends on.\nTo achieve this, first build a docker image of your app by running: \n ./mvnw -Pprod verify jib:dockerBuild \n Then run: \n docker-compose -f src/main/docker/app.yml up -d \n For more information refer to  Using Docker and Docker-Compose , this page also contains information on the docker-compose sub-generator ( jhipster docker-compose ), which is able to generate docker configurations for one or several JHipster applications. \n Continuous Integration (optional) \n To configure CI for your project, run the ci-cd sub-generator ( jhipster ci-cd ), this will let you generate configuration files for a number of Continuous Integration systems. Consult the  Setting up Continuous Integration  page for more information.", "Tic Tac Toe Game \n Let's learn about CD while using GitHub Actions and the GitHub Package Registry! \n Head over to your first  Pull Request  to get started"]
WenchenLi,['Unsupervised-Feature-Learning-and-Deep-Learning \n a repository for  UFLDL course', 'liveCamera \n a google glass camera app with browse and zoom functionality.', 'recommend_system \n dependencies:\npymongo\ngraphlab\nflask\nd3.js \n This is the repo for recording the process of building the complete \nrecommendation system from the backend based on mongodb using \ngraphlab and pymongo. \n recommend.py is the recommendation system I implemented based on graphlab, it is \nmongodb.py is the python script I used to setup the mongodb and specify the schema of the database,\nThose file are well documented with examples. \nrecommend.ipynb and recommend.ipynb is ipython notebook file that is easy for me to debug my code in \nthe corresponding .py file. \n server.py is based on flask that I can used to make function call to my database and recommend.py to fetch \nthe recommend results. I\'m still working on this part and hopefully I will get this running after a week. \n recommend.html is my UI of the recommdation system. Currently everything is static you can run that by first\nsetup the server on local from command line "python -m SimpleHTTPServer" and it can render the top five \nrecommended item for the user. Based on each item if you move your mouse to the item, it can render the \nitem based recommendation based on the similarity of item.', 'Gatech Android Printing \n Goal \n The goal for this app is to help you using georgia tech\'s printing Service at ease\nwith you android phone. \n Usage \n \n \n If you print from the within Georgia Tech network, you can print directly once you have printer configured.(TODO) \n \n \n Otherwise, you have to install  AnyConnect  first,\nand choose Add New VPN Connection with Server Address anyc.vpn.gatech.edu, and then start Connection to Georgia Tech network. Once connected,\nyou can print as you are on campus. \n \n \n Notes \n This plugin unpacks minimal Debian installation with CUPS included, and launches it using PRoot, then uses it for actual printing.\nPRoot can be downlaoded from http://proot.me/\nThere is no JNI - Java code just calls lp, lpinfo, lpadmin etc commandline tools. \n TODO: \n \n \n Add gatech printer to the CUPS server( now you can do it manually through advanced interface with the help from  here ). One thing need to mention is on configuring the queue,\ninstead of doing lpd://pharosQ3.ad.gatech.edu/laptop_black_xerox5550, you should add you gatech id like this lpd://wli345@pharosQ3.ad.gatech.edu/laptop_black_xerox5550.) \n \n \n Remove redundancy of this app. \n \n Add status update once sent to printer. \n Add support for 64-bit architectures. \n Implement AdvancedPrintOptionsActivity: \n Page margins \n Collate copies \n Resize print content to fit into less amount of pages, by reporting bigger paper size and rescaling it back when printing \n All options configurable by lpoptions: \n Paper type \n Printer tray select \n Print resolution \n Booklet print \n Print preview: \n Show or hide print preview in advanced options \n Change page margins inside print preview \n Select and de-select pages for print preview \n USB printers support, by forwaring calls from libusb to Java \n Print on older devices and from older apps, using \'Share\' button \n Connect to non-local CUPS server: https://www.cups.org/documentation.php/doc-1.7/ref-client-conf.html \n Command-line interface into CUPS installation \n Add non-Samba printer by URL, share non-Samba printer \n Search for IPP and Bonjour printers using cups-browsed \n When printing to CUPS server,  CUPS web interface shows an error: "Failed to connect to system bus" \n Set LANG to make CUPS print localized error messages \n When printing page range and multiple copies, CUPS collates pages and we don\'t want that \n', "Hybrid \n link to showcase video \n hybrid pic of two image from high frequency to low frequency, such that people can interpret the picture differently based on the size or distance from the picture.More on this technique please refer to  this . \n Please add opencv to android studio according to [this] (http://stackoverflow.com/questions/27406303/opencv-in-android-studio), you can skip step 6 since jniLibs is already in the repo. \n The below part is for my own reference in the future:\nlambda * v  = c, where c is the speed of the light, lambda is the wavelength and v is the frequency of the lightwave. When sensor capture the image,it capture the complicated waves distribution of that moment through aperture. It's a discrete approximation of the natural light given the  Wave–particle duality  of light. \n We have high pass filter and low pass filter to filter the image to high frequency part and low frequency part. \nin 2d, Gaussian filter is a good example of low pass filter, all the energy concentrate at the low frequency domain.\n . \n The opposite of Gaussian filter make an example of high frequncy filter like this one. \n I remember from the lecture James mention natural light is more in low frequency domain.", 'FlashCard \n glassware help me remember things as flashcard:\nspeech to text as input, internet connection required. Not working well in the noisy enviroment.', 'deprecated \n We are the highlighted projects for advance computer vision class. Our result page \n We have moved to  here .  \n PhotoRestoration \n \n proposal slides \n Progress \n our current focus:  \n \n \n data: taking existing face dataset like celebA with cropped face as ground truth training data,\nand randomly assign a rectangular mask on the face as the training data, so that is a tuple (masked_image, ground_truth_image)\nas training data. \n \n \n model : convolutional autoencoder/decoder followed by a discriminator taking masked image and groundtruth image to discriminate\nthe restoration. \n \n \n current possible solution still requires "photoshop" \n \n facetune \n retouch \n retoucher \n \n Ideas \n \n \n Inspired by the network in network architecture proprosed by GoogLeNet as well as the R-CNN transition to faster-R-CNN, we want to design an architecture of the network based on the Generative Adversarial Networks (GAN) and convert the pipline of photo restoration within the network. For inpainting, we\'ve found two CNN related papers.   \n \n \n For inpainting to work, our algorithm need to apply inpainting from the nearest neighbor of the missing part or creases, inpainting will not work if the area is big, in other words, our algorithm need to shrink the size of user defined problematic area until it\'s good enough to do inpainting. \n \n \n GAN in the sense generator now is the restored instance of image and discriminator classify whether current  restored instance is a good repairing. The problem is this network needs so many data to train the generator at least, but we can have databases of faces and fake the problematic area. \n \n \n Train faster RCNN to detect problematic area of the image in real time.(TODO: solve let the user select the area to fix) \n \n \n Our Approach \n \n Non-face \n Small tears, folds:Image Inpainting [4] \n Missing patches: \n Fragment-based Image Completion[8] \n Scene completion(background?)  \n \n \n \n \n Face \n Non-fine-grained features(cheeks, forehead): \n Small (Image Inpainting [4]) \n Patch (Segmented-based image completion[8]) \n \n \n Fine-grained features (Missing patches): \n Graph Laplace for Occluded Face Completion[6][7] \n Stronger conditional Generative Adversarial Network [5] \n The condition is no longer semantic description of faces like age or races, but the faces with “problematic” area. \n \n \n \n \n \n resources might be useful \n general ideas \n \n cvpr 2015 Image and Video Processing and Restoration \n Generate image analogies using neural matching and blending \n Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artworks \n \n datasets \n \n Aditya Khosla, Wilma A. Bainbridge, Antonio Torralba and Aude Oliva "Modifying the Memorability of Face Photographs."\nInternational Conference on Computer Vision (ICCV), 2013. \n \n inpainting related \n \n Shepard Convolutional Neural Networks \n Image Denoising and Inpainting with Deep Neural Networks \n \n GAN github resources \n \n original work   \n tensorflow implementation \n Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks \n Conditional generative adversarial networks for convolutional face generation \n \n CNN image search \n 1. Cross-dimensional Weighting (CroW) aggregation(with repo,search based on CroW extracted by CNN) \n reference \n 1. [Scene Completion Using Millions of Photographs. James Hays, Alexei A. Efros. ACM Transactions on Graphics (SIGGRAPH 2007). August 2007, vol. 26, No. 3.](http://graphics.cs.cmu.edu/projects/scene-completion/) \n 2. [Sketch2Photo: Internet Image Montage. ACM SIGGRAPH ASIA 2009, ACM Transactions on Graphics. Tao Chen, Ming-Ming Cheng, Ping Tan, Ariel Shamir, Shi-Min Hu.](http://cg.cs.tsinghua.edu.cn/montage/main.htm) \n 3. [Supervised Learning of Semantics-Preserving Hashing via Deep Neural Networks for Large-Scale Image Search Huei-Fang Yang, Kevin Lin, Chu-Song Chen arXiv preprint arXiv:1507.00101](http://arxiv.org/abs/1507.00101) \n \n "Generative Adversarial Networks." Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio. ArXiv 2014. \n Dale, Kevin, et al. "Image restoration using online photo collections."Computer Vision, 2009 IEEE 12th International Conference on. IEEE, 2009. \n Geman, Stuart, and Donald Geman. "Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images." Pattern Analysis and Machine Intelligence, IEEE Transactions on 6 (1984): 721-741. \n Dong, Chao, et al. "Learning a deep convolutional network for image super-resolution." Computer Vision–ECCV 2014. Springer International Publishing, 2014. 184-199. \n Bertalmio, Marcelo, et al. "Image inpainting." Proceedings of the 27th annual conference on Computer graphics and interactive techniques. ACM Press/Addison-Wesley Publishing Co., 2000. \n M. Mirza and S. Osindero. Conditional Generative Adversarial Nets. arXiv:1411.1784 [cs, stat], Nov. 2014. arXiv: 1411.1784  \n Deng, Yue, Qionghai Dai, and Zengke Zhang. "Graph Laplace for occluded face completion and recognition." Image Processing, IEEE Transactions on 20.8 (2011): 2329-2338. \n J. Wright, A. Yang, A. Ganesh, S. Sastry, and Y. Ma, “Robust face recognition via sparse representation,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 31, no. 2, pp. 210–227, Feb. 2009.                              8.  Drori, Iddo, Daniel Cohen-Or, and Hezy Yeshurun. "Fragment-based image completion." ACM Transactions on Graphics (TOG). Vol. 22. No. 3. ACM, 2003. \n', 'This repository keeps track of my working progress in the fullstack nanodegree @ Udacity. misc for Some practises along the way before each project.', "paper-notes \n paper notes will be moved to  medium  for easier publishing \n paper notes on machine learning \n \n \n Neural Enquirer: Learning to Query Tables with Natural Language \n \n \n Modeling documents with Generative Adversarial\nNetworks \n \n \n Key-Value Memory Networks for Directly Reading Documents \n \n \n Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation [ arXiv ] \n \n \n Glove \n \n \n 2016-11 (ICLR 2017) \n Reinforcement Learning: \n \n A Connection between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models [ arXiv ] \n The Predictron: End-To-End Learning and Planning [ OpenReview ] \n Third-Person Imitation Learning [ OpenReview ] \n Generalizing Skills with Semi-Supervised Reinforcement Learning [ OpenReview ] \n Sample Efficient Actor-Critic with Experience Replay [ OpenReview ] \n [Reinforcement Learning with Unsupervised Auxiliary Tasks][ OpenReview ] \n Neural Architecture Search with Reinforcement Learning [ OpenReview ] \n Towards Information-Seeking Agents [ OpenReview ] \n Multi-Agent Cooperation and the Emergence of (Natural) Language [ OpenReview ] \n Improving Policy Gradient by Exploring Under-appreciated Rewards [ OpenReview ] \n Stochastic Neural Networks for Hierarchical Reinforcement Learning [ OpenReview ] \n Tuning Recurrent Neural Networks with Reinforcement Learning [ OpenReview ] \n RL^2: Fast Reinforcement Learning via Slow Reinforcement Learning [ arXiv ] \n Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning [ OpenReview ] \n Learning to Perform Physics Experiments via Deep Reinforcement Learning [ OpenReview ] \n Reinforcement Learning through Asynchronous Advantage Actor-Critic on a GPU [ OpenReview ] \n Learning to Compose Words into Sentences with Reinforcement Learning[ OpenReview ] \n Deep Reinforcement Learning for Accelerating the Convergence Rate [ OpenReview ] \n Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning [ OpenReview ] \n Learning to Compose Words into Sentences with Reinforcement Learning [ OpenReview ] \n Learning to Navigate in Complex Environments [ arXiv ] \n Unsupervised Perceptual Rewards for Imitation Learning [ OpenReview ] \n Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic [ OpenReview ] \n Neural Architecture Search with Reinforcement Learning [ OpenReview ] \n \n NLP \n \n Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation [ arXiv ] \n [Neural Machine Translation with Reconstruction][ arXiv ] \n Iterative Refinement for Machine Translation [ OpenReview ] \n A Convolutional Encoder Model for Neural Machine Translation [ arXiv ] \n Improving Neural Language Models with a Continuous Cache [ OpenReview ] \n Vocabulary Selection Strategies for Neural Machine Translation [ OpenReview ] \n Towards an automatic Turing test: Learning to evaluate dialogue responses [ OpenReview ] \n Dialogue Learning With Human-in-the-Loop [ OpenReview ] \n Batch Policy Gradient Methods for Improving Neural Conversation Models [ OpenReview ] \n Learning through Dialogue Interactions [ OpenReview ] \n [Dual Learning for Machine Translation][ arXiv ] \n Unsupervised Pretraining for Sequence to Sequence Learning [ arXiv ] \n On orthogonality and learning recurrent networks with long term dependencies[ OpenReview ] \n Efficient Vector Representation for Documents through Corruption\n[ OpenReview ] \n Generating Long and Diverse Responses with Neural Conversation Models[ OpenReview ] \n A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks[ OpenReview ] \n Structured Sequence Modeling with Graph Convolutional Recurrent Networks[ OpenReview ] \n Wav2Letter: an End-to-End ConvNet-based Speech Recognition System[ OpenReview ] \n LEARNING A NATURAL LANGUAGE INTERFACE WITH NEURAL PROGRAMMER\n[ OpenReview ] \n Dynamic Coattention Networks For Question Answering [ OpenReview ] \n Quasi-Recurrent Neural Networks [ OpenReview ] \n A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks [ OpenReview ] \n Adversarial Training Methods for Semi-Supervised Text Classification[ OpenReview ] \n A Neural Knowledge Language Model[ OpenReview ] \n \n One-shot learning/ transfer-learning\n- Optimization as a Model for Few-Shot Learning[ OpenReview ]\n- Learning to Remember Rare Events[ OpenReview ]\n- Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks[ OpenReview ] \n CNN\n- LOCALLY CONNECTED CONVOLUTIONAL NEURAL\nNETWORKS ON GRAPH-STRUCTURED DATA[ OpenReview ]\n- Semi-Supervised Classification with Graph Convolutional Networks[ OpenReview ]\nsemi-supervised learning\n-Neural Graph Machines: Learning Neural Networks Using Graphs[ OpenReview ] \n misc\n- An Analysis of Deep Neural Network Models for Practical Applications[ OpenReview ]", 'GloVe PyWrapper : python wrapper for Global Vectors for Word Representation \n This module is a thin python wrapper module built upon original code from  GloVe . \n you can easily make this a submodule to your project by altering built path in  __init__ \n Usage \n bash\n./get_text8.sh\nmake \n```python \nimport glove_pywrapper \n CORPUS = "text8"\nglove = glove_pywrapper.GloveWrapper(CORPUS, "text8") \n prepare vocabulary count \n glove.vocab_count() \n prepare co-occurrence matrix \n glove.cooccur() \n reshuffle \n glove.shuffle() \n glove train \n glove.glove() \n ``` \n License \n All work contained in this package is licensed under the Apache License, Version 2.0. See the include LICENSE file.', 'util \n utils script made life easier \n include \n \n install openai gym, universe and govncdriver.   \n', 'mail \n a simple python script for sending email \n install \n bash\nsudo python setup.py install \n run \n ```python \n please define all constants in the constant.py file \n from constant import * \n send_mail(sender_address, to_address_dict, subject, mail_body, send_mail_server, ps,\n              send_mail_port=587, send_file_name_as=send_file_name_as, send_file_path=send_file_path)\n``` \n TODO /issues', 'n2nSRL \n deep learning approach(s) on Semantic Role Labeling  \n Semantic Role Labeler with Recurrent Neural Networks \n run \n ```bash \n you can ignore the gzip warning \n chmod +x *.sh\n./prepare_data.sh\n./train.sh\n``` \n training output \n notice the fact with limited data and wsj corpus and brown corpus are different\n bash\nEpoch: 160\n    Index:  100 Time: 57.226235 seconds\n    Average Negative Log Likelihood: 0.111926\n    Train Accuracy: 0.999139\n    Index:  1000 2000 3000 4000 5000    Time: 40.484402 seconds\n    Dev Accuracy: 0.999099\n    Props: 5267 P total: 14055.000000   R total: 14077.000000   Correct: 13975.000000\n    Precision: 0.994308 Recall: 0.992754    F1: 0.993530\n    ### Best Dev F Score: 0.993708  Epoch: 157 ###\n    Index:  Time: 5.371451 seconds\n    Test Accuracy: 0.737695\n    Props: 804  P total: 2060.000000    R total: 2177.000000    Correct: 1008.000000\n    Precision: 0.489320 Recall: 0.463023    F1: 0.475808\n    ### Best Test F Score: 0.482759  Epoch: 157 ### \n Data \n \n CoNLL-2005 Shared Task (http://www.cs.upc.edu/~srlconll/) \n Word Embedding: SENNA (http://ronan.collobert.com/senna/) \n \n Notice \n \n \n the original conll05 is not public available, the training are performed on test-wsj set and can achieve F1 score around 99%, \nwhich shows the learning capability of the model. \n \n \n you can try your own word embedding, the required format is 1st colunm is the word, the rest is the corresponding vector. \n \n \n With your own training: to avoid overfitting, try use higher L2 Reg rate, larger batch size and more annotated data. \n \n \n how to label your own data for training \n please follow  CoNLL-2005 Shared Tasks Semantic Role Labeling \nto label your training data. \n Notes on chinese training data, \naccording to  this page , on Format section Target column, \nchinese predicates are the same with the word itself.  \n how to run prediction \n under predict.txt, we have 2 same sentences with 1st one the labeled sentence for training, dev and testing,\n 2nd one as the prediction format. Under main.py, the default argv is for prediction, you can simply run main\n to see how it predict the result given the input as in predict.txt(output as Predict-result.txt). \n you can easily write a script to pipline the prediction process by :\n1. has a tagger do POS finding the verbs(predicated)\n2. put the base form(finds->find) to the target column 5th column as shown in predict.txt example, for chinese it is\n    the same as the word itself. \n reference \n \n Zhou, Jie and Wei Xu. “End-to-end learning of semantic role labeling using recurrent neural networks.” ACL (2015). \n', "VDCNN \n tensorflow implementation of Very Deep Convolutional Networks\nfor Text Classification \n RUN \n train \n I have toy dataset rt_data_all.txt to get you started with the model. \n ```bash \n config the Training flags in main \n python main.py\n``` \n predict \n please take a look at predict.py. I have example for both english and chinese\n```bash \n config the Training flags in main \n python predict.py\n``` \n model and implementation details \n \n Convolutional block \n \n \n \n the basic whole model \n \n \n \n shortcut  \n \n as shows in paper shortcut is not always helping, so we dont implement shortcut here yet, put it \nas future TODO. \n \n above table :\nEvolution of the train/test error on the Yelp Review Full data set for all depths, and with or without shortcut connections (ResNet). \n \n different depth,(K)max pooling  \n \n as shows in the table, k max pooling not always helps, so keep to max pooling for now, mark KmaxPooling as TODO  \n \n above Table : Testing error of our models on the 8 data sets. The deeper the networks the lower the error for all pooling types. No data preprocessing or augmentation is used. \n how to train your own model \n training input data format \n each sentence should be separated by line.\nfor each line, it starts with the training sentence, followed by the label.\nlabel should be started with    __label__  just to be consistent with  fasttext  input format\nfollowing is a few examples, where 'pos' and 'neg'  are labels.\n the thing looks like a made-for-home-video quickie . __label__neg\neffective but too-tepid biopic __label__pos \n TODO \n \n uniform sample from different category[weighted sample][precision/recall/f1][give different weights to the positive and negative examples in the training criterion.] \n \n add  n-gram cnn as comparision \n \n \n prototxt config \n \n optimize readin data queue \n add shortcut \n add kMaxPooling \n fold for dynamic graph \n \n reference \n [1] Conneau, Alexis et al. “Very Deep Convolutional Networks for Natural Language Processing.” CoRR abs/1606.01781 (2016): n. pag.", "join syntactic and semantic parser \n Transition-based joint syntactic dependency parser and semantic role labeler using stack LSTM RNN architecture (with the focus on chinese corpus) \n trained model download \n official conll 2009 trained model \ncorresponding working_dir: wang_old_works_mkl \n our conll 2009 format data trained model \ncorresponding working_dir: ours \n wang2vec embedding \nembedding file suppose to be under data/word_embedding \n Required software \n \n A C++ compiler supporting the  C++11 language standard , g++4.8.5 tested working \n Boost  libraries, 1.60 compiled with g++4.8.5  \n Eigen  (newer versions strongly recommended, 3.3.3) \n CMake \n intel mkl [default required]recommended,speed up parser training 1.5~2 faster \n This runs with the older version of dynet :  commit:814f1819e4b7a75cb490b6a242a40e55f5ce7a1b notice: might not work with dynet 2.0 (not tested) \n \n Checking out the project for the first time \n The first time you clone the repository, you need to sync the  dynet/  submodule.(eigen and boost install included) \n sh scripts/prepare_submodule.sh\n \n Prepare other dependencies \n ```\nsh scripts/prepare_eigen.sh\nsh scripts/prepare_boost.sh\nsh scripts/prepare_mkl.sh\n```\n \n Build instructions \n sh scripts/build.sh\n \n Train a parsing model \n Once you successfully build lstm-parse, we can continue to train the model: \n ```\nsh scripts/prepare_conll_data_and_train_zh.sh $WORKING_DIR $EMBEDDING \n ```\n$WORKING_DIR keeps reference to the trained model(joint.model), as well as all the necessary train/dev data\n$EMBEDDING to specify the word embedding to train the model,  wang2vec  suggested.  \n to resume training: \n sh scripts/resume_training.sh $WORKING_DIR $WORD_EMBEDDING $MODEL \n Testing \n sh scripts/test.sh $WORKING_DIR $WORD_EMBEDDING $MODEL \n Predicting \n sh scripts/predict.sh $WORKING_DIR $WORD_EMBEDDING $MODEL \n current results \n result on official conll 2009 dev data with wang2vec trained with wiki dump, use mkl on single thread. \n ```\n[epoch=69 eta=0.0153374 clips=85 updates=100] update #15394 (epoch 69.1072)  llh: 4980.81 ppl: 1.51411 err: 0.0898642\n[epoch=69 eta=0.0153374 clips=85 updates=100] update #15395 (epoch 69.1116)  llh: 4838.37 ppl: 1.49128 err: 0.0852399\n[epoch=69 eta=0.0153374 clips=75 updates=100] update #15396 (epoch 69.1161)  llh: 4759.13 ppl: 1.4869 err: 0.0904393\n[epoch=69 eta=0.0153374 clips=84 updates=100] update #15397 (epoch 69.1206)  llh: 5141.25 ppl: 1.47943 err: 0.0887484\n[epoch=69 eta=0.0153374 clips=89 updates=100] update #15398 (epoch 69.1251)  llh: 5748.75 ppl: 1.52395 err: 0.0948333\n[epoch=69 eta=0.0153374 clips=85 updates=100] update #15399 (epoch 69.1296)  llh: 4799.43 ppl: 1.49627 err: 0.0897565\n[epoch=69 eta=0.0153374 clips=81 updates=100] update #15400 (epoch 69.1341)  llh: 5530.8 ppl: 1.52136 err: 0.0899021\n  **dev (iter=15400 epoch=69.1341)   llh=0 ppl: 1 err: 1 las: 79.01 semF1: 79.04 macro:79.04    [1762 sents in 39816.8 ms] \n ``` \n result on our own conll 2009 data  with wang2vec trained with wiki dump, use mkl on single thread. \n ``` \n [epoch=70 eta=0.0151515 clips=56 updates=100] update #12996 (epoch 70.6858)  llh: 1139.36 ppl: 1.37206 err: 0.0816213\n[epoch=70 eta=0.0151515 clips=57 updates=100] update #12997 (epoch 70.6912)  llh: 1017.06 ppl: 1.33381 err: 0.0705183\n[epoch=70 eta=0.0151515 clips=52 updates=100] update #12998 (epoch 70.6967)  llh: 982.284 ppl: 1.34805 err: 0.0741867\n[epoch=70 eta=0.0151515 clips=46 updates=100] update #12999 (epoch 70.7021)  llh: 886.387 ppl: 1.29362 err: 0.06535\n[epoch=70 eta=0.0151515 clips=56 updates=100] update #13000 (epoch 70.7076)  llh: 1011.8 ppl: 1.31518 err: 0.074736\n  **dev (iter=13000 epoch=70.7076)   llh=0 ppl: 1 err: 1 las: 88.01 semF1: 76.9 macro:82.46 [3294 sents in 13458.6 ms]\n[epoch=70 eta=0.0151515 clips=56 updates=100] update #13001 (epoch 70.713)   llh: 1010.82 ppl: 1.3321 err: 0.0729078\n[epoch=70 eta=0.0151515 clips=49 updates=100] update #13002 (epoch 70.7184)  llh: 989.865 ppl: 1.33044 err: 0.0695125\n[epoch=70 eta=0.0151515 clips=49 updates=100] update #13003 (epoch 70.7239)  llh: 842.52 ppl: 1.29604 err: 0.0627886\n[epoch=70 eta=0.0151515 clips=48 updates=100] update #13004 (epoch 70.7293)  llh: 960.099 ppl: 1.3176 err: 0.0669348\n[epoch=70 eta=0.0151515 clips=56 updates=100] update #13005 (epoch 70.7348)  llh: 916.987 ppl: 1.30957 err: 0.0682353 \n ``` \n online predict \n Here is the example input to the parsing system: \n under example/ predict_example.transition:\nwe have input[to get the input, please see predicate_identification project]:\n``` \n 中国-NR, 最大-JJ, 氨纶丝-NN, 生产-NN, 基地-NN, 在-P, 连云港-NR, 建成-VV~建成, ROOT-NOTAG \n 王翔-NR, 虽-CS, 年过半百-VV~年过半百, ，-PU, 但-AD, 其-PN, 充沛-VV~充沛, 的-DEC, 精力-NN, 和-CC, 敏捷-NN~敏捷, 的-DEG, 思维-NN, ，-PU, 给-VV~给, 人-NN, 以-P, 一-CD, 个-M, 挑战者-NN, 的-DEG, 印象-NN, 。-PU, ROOT-NOTAG \n and the system output: \npredicates:\n7:PR(建成.01)\nsem-arcs:\n4<- 7[SL(A1)], \n5<- 7[SL(LOC)], \nsyn-arcs:\n4-LA(NMOD)->0\n4-LA(AMOD)->1\n4-LA(NMOD)->2\n4-LA(NMOD)->3\n7-LA(SBJ)->4\n7-LA(LOC)->5\n5-RA(OBJ)->6\n8-LA(ROOT)->7\n-1-ERROR->8\npredicates:\n2:PR(UNK)\n6:PR(充沛.01)\n10:PR(UNK)\n14:PR(给.01)\nsem-arcs:\n0<- 2[SL(A0)], \n1<- 2[SL(DIS)], \n8<- 6[SR(A0)], \n12<- 10[SR(A0)], \n15<- 14[SR(A1)], \nsyn-arcs:\n2-LA(SBJ)->0\n2-LA(ADV)->1\n22-LA(ADV)->2\n22-LA(UNK)->3\n22-LA(ADV)->4\n8-LA(NMOD)->5\n7-LA(COMP)->6\n8-LA(RELC)->7\n22-LA(SBJ)->8\n8-RA(CJTN)->9\n11-LA(COMP)->10\n12-LA(RELC)->11\n9-RA(CJT)->12\n22-LA(UNK)->13\n22-LA(ADV)->14\n14-RA(COMP)->15\n22-LA(MNR)->16\n19-LA(DMOD)->17\n17-RA(COMP)->18\n20-LA(COMP)->19\n21-LA(NMOD)->20\n16-RA(OBJ)->21\n23-LA(ROOT)->22\n-1-ERROR->23\n``` \n where parsing sentence results are seperated by -1-ERROR->num, and for each sentence, \nwe have sentence predicate disambiguation:predicates:\nsemantic dependencies: sem-arcs, \nand syntactic dependencies: syn-arcs. \nNotice the order of the sentence start from 0.  \n Below, is an example: \n data input to the parser system: \n```\n中国-NR, 最大-JJ, 氨纶丝-NN, 生产-NN, 基地-NN, 在-P, 连云港-NR, 建成-VV~建成, ROOT-NOTAG \n ``` \n data output by the parser system:\n```\npredicates:\n7:PR(建成.01)\nsem-arcs:\n4<- 7[SL(A1)], \n5<- 7[SL(LOC)], \nsyn-arcs:\n4-LA(NMOD)->0\n4-LA(AMOD)->1\n4-LA(NMOD)->2\n4-LA(NMOD)->3\n7-LA(SBJ)->4\n7-LA(LOC)->5\n5-RA(OBJ)->6\n8-LA(ROOT)->7\n-1-ERROR->8 \n ``` \n data conll 2009 format:\n```\n1   中国  中国  中国  NR  NR  _   _   5   5   NMOD    NMOD    _   _   _\n2   最大  最大  最大  JJ  JJ  _   _   5   5   AMOD    AMOD    _   _   _\n3   氨纶丝 氨纶丝 氨纶丝 NN  NN  _   _   5   5   NMOD    NMOD    _   _   _\n4   生产  生产  生产  NN  NN  _   _   5   5   NMOD    NMOD    _   _   _\n5   基地  基地  基地  NN  NN  _   _   8   8   SBJ SBJ _   _   A1\n6   在   在   在   P   P   _   _   8   8   LOC LOC _   _   LOC\n7   连云港 连云港 连云港 NR  NR  _   _   6   6   OBJ OBJ _   _   _\n8   建成  建成  建成  VV  VV  _   _   0   0   ROOT    ROOT    Y   建成.01   _ \n ``` \n you can compare to the gold standard[data conll 2009 format], the parser correctly \na. disambiguate the predicate\nb. find the semantic dependencies[4<- 7, 5<- 7] and it's role [SL(A1),SL(LOC)], where SL are transition actions.  \n references \n 1. Swayamdipta, Swabha et al. “Greedy, Joint Syntactic-Semantic Parsing with Stack LSTMs.” CoNLL (2016). \n paper presentation ACL \n slides \n 2. Ling, Wang et al. “Two/Too Simple Adaptations of Word2Vec for Syntax Problems.” HLT-NAACL (2015).", 'Q-learning trader \n A technical q trader on SPY given macd. \n run \n python teststratgy.py \n structure \n QLearner.py: an independent tabular (dyna)q-learner. \n StrategyLearner.py: Build upon Qlearner.py to learn the trading strategy \n testStrategy.py: train and test the StrategyLearner \n util.py: some helper functions for the model \n experiment result \n please see  this link \n or the evernote screenshot below:\n', 'nips2017 \n nips 2017 paper abstract  \n #1: Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence Learning \n #2: Concentration of Multilinear Functions of the Ising Model with Applications to Network Data \n #3: Deep Subspace Clustering Network \n Pan Ji,  Tong Zhang,  Hongdong Li,  Mathieu Salzmann,  Ian Reid \n We present a novel deep neural network architecture for unsupervised subspace clustering. This architecture is built upon deep auto-encoders, which non-linearly map the input data into a latent space. Our key idea is to introduce a novel self-expressive layer between the encoder and the decoder to mimic the "self-expressiveness" property that has proven effective in traditional subspace clustering. Being differentiable, our new self-expressive layer provides a simple but effective way to learn pairwise affinities between all data points through a standard back-propagation procedure. Being nonlinear, our neural-network based method is able to cluster data points having complex (often nonlinear) structures. We further propose pre-training and fine-tuning strategies that let us effectively learn the parameters of our subspace clustering networks. Our experiments show that the proposed method significantly outperforms the state-of-the-art unsupervised subspace clustering methods. \n #4: Attentional Pooling for Action Recognition \n #5: On the Consistency of Quick Shift \n #6: Rethinking Feature Discrimination and Polymerization for Large-scale Recognition \n Yu Liu,  Hongyang Li,  Xiaogang Wang \n Feature matters. How to train a deep network to acquire discriminative features across categories and polymerized features within classes has always been at the core of many computer vision tasks, specially for large-scale recognition systems where test identities are unseen during training and the number of classes could be at million scale. In this paper, we address this problem based on the simple intuition that the cosine distance of features in high-dimensional space should be close enough within one class and far away across categories. To this end, we proposed the congenerous cosine (COCO) algorithm to simultaneously optimize the cosine similarity among data. It inherits the softmax property to make inter-class features discriminative as well as shares the idea of class centroid in metric learning. Unlike previous work where the center is a temporal, statistical variable within one mini-batch during training, the formulated centroid is responsible for clustering inner-class features to enforce them polymerized around the network truncus. COCO is bundled with discriminative training and learned end-to-end with stable convergence. Experiments on five benchmarks have been extensively conducted to verify the effectiveness of our approach on both small-scale classification task and large-scale human recognition problem. \n #7: Breaking the Nonsmooth Barrier: A Scalable Parallel Method for Composite Optimization \n Fabian Pedregosa,  Rémi Leblond,  Simon Lacoste-Julien \n Due to their simplicity and excellent performance, parallel asynchronous variants of stochastic gradient descent have become popular methods to solve a wide range of large-scale optimization problems on multi-core architectures. Yet, despite their practical success, support for nonsmooth objectives is still lacking, making them unsuitable for many problems of interest in machine learning, such as the Lasso, group Lasso or empirical risk minimization with convex constraints. In this work, we propose and analyze ProxASAGA, a fully asynchronous sparse method inspired by SAGA, a variance reduced incremental gradient algorithm. The proposed method is easy to implement and significantly outperforms the state of the art on several nonsmooth, large-scale problems. We prove that our method achieves a theoretical linear speedup with respect to the sequential version under assumptions on the sparsity of gradients and block-separability of the proximal term. Empirical benchmarks on a multi-core architecture illustrate practical speedups of up to 12x on a 20-core machine.\n Abstract ,  PDF \n #8: Dual-Agent GANs for Photorealistic and Identity Preserving Profile Face Synthesis \n #9: Dilated Recurrent Neural Networks \n Shiyu Chang,  Yang Zhang,  Wei Han,  Mo Yu,  Xiaoxiao Guo,  Wei Tan,  Xiaodong Cui,  Michael Witbrock,  Mark Hasegawa-Johnson,  Thomas Huang \n Notoriously, learning with recurrent neural networks (RNNs) on long sequences is a difficult task. There are three major challenges: 1) extracting complex dependencies, 2) vanishing and exploding gradients, and 3) efficient parallelization. In this paper, we introduce a simple yet effective RNN connection structure, the DILATEDRNN, which simultaneously tackles all these challenges. The proposed architecture is characterized by multi-resolution dilated recurrent skip connections and can be combined flexibly with different RNN cells. Moreover, the DILATEDRNN reduces the number of parameters and enhances training efficiency significantly, while matching state-of-the-art performance (even with Vanilla RNN cells) in tasks involving very long-term dependencies. To provide a theory-based quantification of the architecture\'s advantages, we introduce a memory capacity measure - the mean recurrent length, which is more suitable for RNNs with long skip connections than existing measures. We rigorously prove the advantages of the DILATEDRNN over other recurrent neural architectures. \n #10: Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs \n #11: Scalable Generalized Linear Bandits: Online Computation and Hashing \n Kwang-Sung Jun,  Aniruddha Bhargava,  Robert Nowak,  Rebecca Willett \n Generalized Linear Bandits (GLBs), a natural extension of the stochastic linear bandits, has been popular and successful in recent years. However, existing GLBs scale poorly with the number of rounds and the number of arms, limiting their utility in practice. This paper proposes new, scalable solutions to the GLB problem in two respects. First, unlike existing GLBs, whose per-time-step space and time complexity grow at least linearly with time $t$, we propose a new algorithm that performs online computations to enjoy a constant space and time complexity. At its heart is a novel Generalized Linear extension of the Online-to-confidence-set Conversion (GLOC method) that takes \\emph{any} online learning algorithm and turns it into a GLB algorithm. As a special case, we apply GLOC to the online Newton step algorithm, which results in a low-regret GLB algorithm with much lower time and memory complexity than prior work. Second, for the case where the number $N$ of arms is very large, we propose new algorithms in which each next arm is selected via an inner product search. Such methods can be implemented via hashing algorithms (i.e., "hash-amenable") and result in a time complexity sublinear in $N$. While a Thompson sampling extension of GLOC is hash-amenable, its regret bound for $d$-dimensional arm sets scales with $d^{3/2}$, whereas GLOC\'s regret bound is linear in $d$. Towards closing this gap, we propose a new hash-amenable algorithm whose regret bound scales with $d^{5/4}$. Finally, we propose a fast approximate hash-key computation (inner product) that has a better accuracy than the state-of-the-art, which can be of independent interest. We conclude the paper with preliminary experimental results confirming the merits of our methods.\n Abstract ,  PDF \n #12: Probabilistic Models for Integration Error in the Assessment of Functional Cardiac Models \n #13:  Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent \n #14: Dynamic Safe Interruptibility for Decentralized Multi-Agent Reinforcement Learning \n #15: Interactive Submodular Bandit \n #16: Scene Physics Acquisition via Visual De-animation \n #17: Label Efficient Learning of Transferable Representations acrosss Domains and Tasks \n #18: Decoding with Value Networks for Neural Machine Translation \n #19: Parametric Simplex Method for Sparse Learning \n Haotian Pang,  Tuo Zhao,  Robert Vanderbei,  Han Liu \n High dimensional sparse learning has imposed a great computational challenge to large scale data analysis. In this paper, we are interested in a broad class of sparse learning approaches formulated as linear programs parametrized by a {\\em regularization factor}, and solve them by the parametric simplex method (PSM). Our parametric simplex method offers significant advantages over other competing methods: (1) PSM naturally obtains the complete solution path for all values of the regularization parameter; (2) PSM provides a high precision dual certificate stopping criterion; (3) PSM yields sparse solutions through very few iterations, and the solution sparsity significantly reduces the computational cost per iteration. Particularly, we demonstrate the superiority of PSM over various sparse learning approaches, including Dantzig selector for sparse linear regression, LAD-Lasso for sparse robust linear regression, CLIME for sparse precision matrix estimation, sparse differential network estimation, and sparse Linear Programming Discriminant (LPD) analysis. We then provide sufficient conditions under which PSM always outputs sparse solutions such that its computational performance can be significantly boosted. Thorough numerical experiments are provided to demonstrate the outstanding performance of the PSM method.\n Abstract ,  PDF \n #20: Group Sparse Additive Machine \n #21: Uprooting and Rerooting Higher-order Graphical Models \n #22: The Unreasonable Effectiveness of Structured Random Orthogonal Embeddings \n #23: From Parity to Preference: Learning with Cost-effective Notions of Fairness \n #24: Inferring Generative Model Structure with Static Analysis \n Paroma Varma,  Bryan He,  Payal Bajaj,  Imon Banerjee,  Nishith Khandwala,  Daniel L. Rubin,  Christopher Ré \n Obtaining enough labeled data to robustly train complex discriminative models is a major bottleneck in the machine learning pipeline. A popular solution is combining multiple sources of weak supervision using generative models. The structure of these models affects training label quality, but is difficult to learn without any ground truth labels. We instead rely on these weak supervision sources having some structure by virtue of being encoded programmatically. We present Coral, a paradigm that infers generative model structure by statically analyzing the code for these heuristics, thus reducing the data required to learn structure significantly. We prove that Coral\'s sample complexity scales quasilinearly with the number of heuristics and number of relations found, improving over the standard sample complexity, which is exponential in $n$ for identifying $n^{\\textrm{th}}$ degree relations. Experimentally, Coral matches or outperforms traditional structure learning approaches by up to 3.81 F1 points. Using Coral to model dependencies instead of assuming independence results in better performance than a fully supervised model by 3.07 accuracy points when heuristics are used to label radiology data without ground truth labels.\n Abstract ,  PDF \n #25: Structured Embedding Models for Grouped Data \n Maja Rudolph,  Francisco Ruiz,  Susan Athey,  David Blei \n Word embeddings are a powerful approach for analyzing language, and exponential family embeddings (EFE) extend them to other types of data. Here we develop structured exponential family embeddings (S-EFE), a method for discovering embeddings that vary across related groups of data. We study how the word usage of U.S. Congressional speeches varies across states and party affiliation, how words are used differently across sections of the ArXiv, and how the co-purchase patterns of groceries can vary across seasons. Key to the success of our method is that the groups share statistical information. We develop two sharing strategies: hierarchical modeling and amortization. We demonstrate the benefits of this approach in empirical studies of speeches, abstracts, and shopping baskets. We show how S-EFE enables group-specific interpretation of word usage, and outperforms EFE in predicting held-out data. \n #26: A Linear-Time Kernel Goodness-of-Fit Test \n Wittawat Jitkrittum,  Wenkai Xu,  Zoltan Szabo,  Kenji Fukumizu,  Arthur Gretton \n We propose a novel adaptive test of goodness-of-fit, with computational cost linear in the number of samples. We learn the test features that best indicate the differences between observed samples and a reference model, by minimizing the false negative rate. These features are constructed via Stein\'s method, meaning that it is not necessary to compute the normalising constant of the model. We analyse the asymptotic Bahadur efficiency of the new test, and prove that under a mean-shift alternative, our test always has greater relative efficiency than a previous linear-time kernel test, regardless of the choice of parameters for that test. In experiments, the performance of our method exceeds that of the earlier linear-time test, and matches or exceeds the power of a quadratic-time kernel test. In high dimensions and where model structure may be exploited, our goodness of fit test performs far better than a quadratic-time two-sample test based on the Maximum Mean Discrepancy, with samples drawn from the model.\n Abstract ,  PDF \n #27: Cortical microcircuits as gated-recurrent neural networks \n #28: k-Support and Ordered Weighted Sparsity for Overlapping Groups: Hardness and Algorithms \n #29: A simple model of recognition and recall memory \n #30: On Structured Prediction Theory with Calibrated Convex Surrogate Losses \n Anton Osokin,  Francis Bach,  Simon Lacoste-Julien \n We provide novel theoretical insights on structured prediction in the context of efficient convex surrogate loss minimization with consistency guarantees. For any task loss, we construct a convex surrogate that can be optimized via stochastic gradient descent and we prove tight bounds on the so-called "calibration function" relating the excess surrogate risk to the actual risk. In contrast to prior related work, we carefully monitor the effect of the exponential number of classes in the learning guarantees as well as on the optimization complexity. As an interesting consequence, we formalize the intuition that some task losses make learning harder than others, and that the classical 0-1 loss is ill-suited for general structured prediction.\n Abstract ,  PDF \n #31: Best of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model \n Jiasen Lu,  Anitha Kannan,  Jianwei Yang,  Devi Parikh,  Dhruv Batra \n We present a novel training framework for neural sequence models, particularly for grounded dialog generation. The standard training paradigm for these models is maximum likelihood estimation (MLE), or minimizing the cross-entropy of the human responses. Across a variety of domains, a recurring problem with MLE trained generative neural dialog models (G) is that they tend to produce \'safe\' and generic responses ("I don\'t know", "I can\'t tell"). In contrast, discriminative dialog models (D) that are trained to rank a list of candidate human responses outperform their generative counterparts; in terms of automatic metrics, diversity, and informativeness of the responses. However, D is not useful in practice since it can not be deployed to have real conversations with users. Our work aims to achieve the best of both worlds -- the practical usefulness of G and the strong performance of D -- via knowledge transfer from D to G. Our primary contribution is an end-to-end trainable generative visual dialog model, where G receives gradients from D as a perceptual (not adversarial) loss of the sequence sampled from G. We leverage the recently proposed Gumbel-Softmax (GS) approximation to the discrete distribution -- specifically, a RNN augmented with a sequence of GS samplers, coupled with the straight-through gradient estimator to enable end-to-end differentiability. We also introduce a stronger encoder for visual dialog, and employ a self-attention mechanism for answer encoding along with a metric learning loss to aid D in better capturing semantic similarities in answer responses. Overall, our proposed model outperforms state-of-the-art on the VisDial dataset by a significant margin (2.67% on recall@10).\n Abstract ,  PDF \n #32: MaskRNN: Instance Level Video Object Segmentation \n #33: Gated Recurrent Convolution Neural Network for OCR \n #34: Towards Accurate Binary Convolutional Neural Network \n #35: Semi-Supervised Learning for Optical Flow with Generative Adversarial Networks \n #36: Learning a Multi-View Stereo Machine \n Abhishek Kar,  Christian Häne,  Jitendra Malik \n We present a learnt system for multi-view stereopsis. In contrast to recent learning based methods for 3D reconstruction, we leverage the underlying 3D geometry of the problem through feature projection and unprojection along viewing rays. By formulating these operations in a differentiable manner, we are able to learn the system end-to-end for the task of metric 3D reconstruction. End-to-end learning allows us to jointly reason about shape priors while conforming geometric constraints, enabling reconstruction from much fewer images (even a single image) than required by classical approaches as well as completion of unseen surfaces. We thoroughly evaluate our approach on the ShapeNet dataset and demonstrate the benefits over classical approaches as well as recent learning based methods.\n Abstract ,  PDF \n #37: Phase Transitions in the Pooled Data Problem \n #38: Universal Style Transfer via Feature Transforms \n Yijun Li,  Chen Fang,  Jimei Yang,  Zhaowen Wang,  Xin Lu,  Ming-Hsuan Yang \n Universal style transfer aims to transfer any arbitrary visual styles to content images. Existing feed-forward based methods, while enjoying the inference efficiency, are mainly limited by inability of generalizing to unseen styles or compromised visual quality. In this paper, we present a simple yet effective method that tackles these limitations without training on any pre-defined styles. The key ingredient of our method is a pair of feature transforms, whitening and coloring, that are embedded to an image reconstruction network. The whitening and coloring transforms reflect a direct matching of feature covariance of the content image to a given style image, which shares similar spirits with the optimization of Gram matrix based cost in neural style transfer. We demonstrate the effectiveness of our algorithm by generating high-quality stylized images with comparisons to a number of recent methods. We also analyze our method by visualizing the whitened features and synthesizing textures via simple feature coloring.\n Abstract ,  PDF \n #39: On the Model Shrinkage Effect of Gamma Process Edge Partition Models \n Iku Ohama,  Issei Sato,  Takuya Kida,  Hiroki Arimura \n The edge partition model (EPM) is a fundamental Bayesian nonparametric model for extracting an overlapping structure from binary matrix. The EPM adopts a gamma process ($\\Gamma$P) prior to automatically shrink the number of active atoms. However, we empirically found that the model shrinkage of the EPM does not typically work appropriately and leads to an overfitted solution. An analysis of the expectation of the EPM\'s intensity function suggested that the gamma priors for the EPM hyperparameters disturb the model shrinkage effect of the internal $\\Gamma$P. In order to ensure that the model shrinkage effect of the EPM works in an appropriate manner, we proposed two novel generative constructions of the EPM: CEPM incorporating constrained gamma priors, and DEPM incorporating Dirichlet priors instead of the gamma priors. Furthermore, all DEPM\'s model parameters including the infinite atoms of the $\\Gamma$P prior could be marginalized out, and thus it was possible to derive a truly infinite DEPM (IDEPM) that can be efficiently inferred using a collapsed Gibbs sampler. We experimentally confirmed that the model shrinkage of the proposed models works well and that the IDEPM indicated state-of-the-art performance in generalization ability, link prediction accuracy, mixing efficiency, and convergence speed. \n #40: Pose Guided Person Image Generation \n Liqian Ma,  Qianru Sun,  Xu Jia,  Bernt Schiele,  Tinne Tuytelaars,  Luc Van Gool \n This paper proposes the novel Pose Guided Person Generation Network (PG$^2$) that allows to synthesize person images in arbitrary poses, based on an image of that person and a novel pose. Our generation framework PG$^2$ utilizes the pose information explicitly and consists of two key stages: pose integration and image refinement. In the first stage the condition image and the target pose are fed into a U-Net-like network to generate an initial but coarse image of the person with the target pose. The second stage then refines the initial and blurry result by training a U-Net-like generator in an adversarial way. Extensive experimental results on both 128$\\times$64 re-identification images and 256$\\times$256 fashion photos show that our model generates high-quality person images with convincing details.\n Abstract ,  PDF \n #41: Inference in Graphical Models via Semidefinite Programming Hierarchies \n Murat A. Erdogdu,  Yash Deshpande,  Andrea Montanari \n Maximum A posteriori Probability (MAP) inference in graphical models amounts to solving a graph-structured combinatorial optimization problem. Popular inference algorithms such as belief propagation (BP) and generalized belief propagation (GBP) are intimately related to linear programming (LP) relaxation within the Sherali-Adams hierarchy. Despite the popularity of these algorithms, it is well understood that the Sum-of-Squares (SOS) hierarchy based on semidefinite programming (SDP) can provide superior guarantees. Unfortunately, SOS relaxations for a graph with $n$ vertices require solving an SDP with $n^{\\Theta(d)}$ variables where $d$ is the degree in the hierarchy. In practice, for $d\\ge 4$, this approach does not scale beyond a few tens of variables. In this paper, we propose binary SDP relaxations for MAP inference using the SOS hierarchy with two innovations focused on computational efficiency. Firstly, in analogy to BP and its variants, we only introduce decision variables corresponding to contiguous regions in the graphical model. Secondly, we solve the resulting SDP using a non-convex Burer-Monteiro style method, and develop a sequential rounding procedure. We demonstrate that the resulting algorithm can solve problems with tens of thousands of variables within minutes, and outperforms BP and GBP on practical problems such as image denoising and Ising spin glasses. Finally, for specific graph types, we establish a sufficient condition for the tightness of the proposed partial SOS relaxation. \n #42: Variable Importance Using Decision Trees \n #43: Preventing Gradient Explosions in Gated Recurrent Units \n #44: On the Power of Truncated SVD for General High-rank Matrix Estimation Problems \n Simon S. Du,  Yining Wang,  Aarti Singh \n We show that given an estimate $\\widehat{A}$ that is close to a general high-rank positive semi-definite (PSD) matrix $A$ in spectral norm (i.e., $\\|\\widehat{A}-A\\| 2 \\leq \\delta$), the simple truncated SVD of $\\widehat{A}$ produces a multiplicative approximation of $A$ in Frobenius norm. This observation leads to many interesting results on general high-rank matrix estimation problems, which we briefly summarize below ($A$ is an $n\\times n$ high-rank PSD matrix and $A_k$ is the best rank-$k$ approximation of $A$): (1) High-rank matrix completion: By observing $\\Omega(\\frac{n\\max{\\epsilon^{-4},k^2}\\mu_0^2\\|A\\|_F^2\\log n}{\\sigma {k+1}(A)^2})$ elements of $A$ where $\\sigma_{k+1}\\left(A\\right)$ is the $\\left(k+1\\right)$-th singular value of $A$ and $\\mu_0$ is the incoherence, the truncated SVD on a zero-filled matrix satisfies $\\|\\widehat{A} k-A\\|_F \\leq (1+O(\\epsilon))\\|A-A_k\\|_F$ with high probability. (2)High-rank matrix de-noising: Let $\\widehat{A}=A+E$ where $E$ is a Gaussian random noise matrix with zero mean and $\\nu^2/n$ variance on each entry. Then the truncated SVD of $\\widehat{A}$ satisfies $\\|\\widehat{A}_k-A\\|_F \\leq (1+O(\\sqrt{\\nu/\\sigma {k+1}(A)}))\\|A-A_k\\| F + O(\\sqrt{k}\\nu)$. (3) Low-rank Estimation of high-dimensional covariance: Given $N$ i.i.d.~samples $X_1,\\cdots,X_N\\sim\\mathcal N_n(0,A)$, can we estimate $A$ with a relative-error Frobenius norm bound? We show that if $N = \\Omega\\left(n\\max{\\epsilon^{-4},k^2}\\gamma_k(A)^2\\log N\\right)$ for $\\gamma_k(A)=\\sigma_1(A)/\\sigma {k+1}(A)$, then $\\|\\widehat{A} k-A\\|_F \\leq (1+O(\\epsilon))\\|A-A_k\\|_F$ with high probability, where $\\widehat{A}=\\frac{1}{N}\\sum {i=1}^N{X_iX_i^\\top}$ is the sample covariance.\n Abstract ,  PDF \n #45: f-GANs in an Information Geometric Nutshell \n Richard Nock,  Zac Cranko,  Aditya Krishna Menon,  Lizhen Qu,  Robert C. Williamson \n Nowozin \\textit{et al} showed last year how to extend the GAN \\textit{principle} to all $f$-divergences. The approach is elegant but falls short of a full description of the supervised game, and says little about the key player, the generator: for example, what does the generator actually converge to if solving the GAN game means convergence in some space of parameters? How does that provide hints on the generator\'s design and compare to the flourishing but almost exclusively experimental literature on the subject? In this paper, we unveil a broad class of distributions for which such convergence happens --- namely, deformed exponential families, a wide superset of exponential families --- and show tight connections with the three other key GAN parameters: loss, game and architecture. In particular, we show that current deep architectures are able to factorize a very large number of such densities using an especially compact design, hence displaying the power of deep architectures and their concinnity in the $f$-GAN game. This result holds given a sufficient condition on \\textit{activation functions} --- which turns out to be satisfied by popular choices. The key to our results is a variational generalization of an old theorem that relates the KL divergence between regular exponential families and divergences between their natural parameters. We complete this picture with additional results and experimental insights on how these results may be used to ground further improvements of GAN architectures, via (i) a principled design of the activation functions in the generator and (ii) an explicit integration of proper composite losses\' link function in the discriminator.\n Abstract ,  PDF \n #46: Multimodal Image-to-Image Translation by Enforcing Bi-Cycle Consistency \n #47: Mixture-Rank Matrix Approximation for Collaborative Filtering \n #48: Non-monotone Continuous DR-submodular  Maximization: Structure and Algorithms \n #49: Learning with Average Top-k Loss \n Yanbo Fan,  Siwei Lyu,  Yiming Ying,  Bao-Gang Hu \n In this work, we introduce the average top-$k$ (AT$_k$) loss as a new ensemble loss for supervised learning, which is the average over the $k$ largest individual losses over a training dataset. We show that the AT$_k$ loss is a natural generalization of the two widely used ensemble losses, namely the average loss and the maximum loss, but can combines their advantages and mitigate their drawbacks to better adapt to different data distributions. Furthermore, it remains a convex function over all individual losses, which can lead to convex optimization problems that can be solved effectively with conventional gradient-based method. We provide an intuitive interpretation of the AT$_k$ loss based on its equivalent effect on the continuous individual loss functions, suggesting that it can reduce the penalty on correctly classified data. We further give a learning theory analysis of MAT$_k$ learning on the classification calibration of the AT$_k$ loss and the error bounds of AT$_k$-SVM. We demonstrate the applicability of minimum average top-$k$ learning for binary classification and regression using synthetic and real datasets.\n Abstract ,  PDF \n #50: Learning multiple visual domains with residual adapters \n Sylvestre-Alvise Rebuffi,  Hakan Bilen,  Andrea Vedaldi \n There is a growing interest in learning data representations that work well for many different types of problems and data. In this paper, we look in particular at the task of learning a single visual representation that can be successfully utilized in the analysis of very different types of images, from dog breeds to stop signs and digits. Inspired by recent work on learning networks that predict the parameters of another, we develop a tunable deep network architecture that, by means of adapter residual modules, can be steered on the fly to diverse visual domains. Our method achieves a high degree of parameter sharing while maintaining or even improving the accuracy of domain-specific representations. We also introduce the Visual Decathlon Challenge, a benchmark that evaluates the ability of representations to capture simultaneously ten very different visual domains and measures their ability to recognize well uniformly.\n Abstract ,  PDF \n #51: Dykstra\'s Algorithm, ADMM, and Coordinate Descent: Connections, Insights, and Extensions \n Ryan J. Tibshirani \n We study connections between Dykstra\'s algorithm for projecting onto an intersection of convex sets, the augmented Lagrangian method of multipliers or ADMM, and block coordinate descent. We prove that coordinate descent for a regularized regression problem, in which the (separable) penalty functions are seminorms, is exactly equivalent to Dykstra\'s algorithm applied to the dual problem. ADMM on the dual problem is also seen to be equivalent, in the special case of two sets, with one being a linear subspace. These connections, aside from being interesting in their own right, suggest new ways of analyzing and extending coordinate descent. For example, from existing convergence theory on Dykstra\'s algorithm over polyhedra, we discern that coordinate descent for the lasso problem converges at an (asymptotically) linear rate. We also develop two parallel versions of coordinate descent, based on the Dykstra and ADMM connections.\n Abstract ,  PDF \n #52: Flat2Sphere: Learning Spherical Convolution for Fast Features from 360° Imagery \n Yu-Chuan Su,  Kristen Grauman \n While 360{\\deg} cameras offer tremendous new possibilities in vision, graphics, and augmented reality, the spherical images they produce make core feature extraction non-trivial. Convolutional neural networks (CNNs) trained on images from perspective cameras yield "flat" filters, yet 360{\\deg} images cannot be projected to a single plane without significant distortion. A naive solution that repeatedly projects the viewing sphere to all tangent planes is accurate, but much too computationally intensive for real problems. We propose to learn a spherical convolutional network that translates a planar CNN to process 360{\\deg} imagery directly in its equirectangular projection. Our approach learns to reproduce the flat filter outputs on 360{\\deg} data, sensitive to the varying distortion effects across the viewing sphere. The key benefits are 1) efficient feature extraction for 360{\\deg} images and video, and 2) the ability to leverage powerful pre-trained networks researchers have carefully honed (together with massive labeled image training sets) for perspective images. We validate our approach compared to several alternative methods in terms of both raw CNN output accuracy as well as applying a state-of-the-art "flat" object detector to 360{\\deg} data. Our method yields the most accurate results while saving orders of magnitude in computation versus the existing exact reprojection solution.\n Abstract ,  PDF \n #53: 3D Shape Reconstruction by Modeling 2.5D Sketch \n #54: Multimodal Learning and Reasoning for Visual Question Answering \n #55: Adversarial Surrogate Losses for Ordinal Regression \n #56: Hypothesis Transfer Learning via Transformation Functions \n Simon Shaolei Du,  Jayanth Koushik,  Aarti Singh,  Barnabas Poczos \n We consider the Hypothesis Transfer Learning (HTL) problem where one incorporates a hypothesis trained on the source domain into the learning procedure of the target domain. Existing theoretical analysis either only studies specific algorithms or only presents upper bounds on the generalization error but not on the excess risk. In this paper, we propose a unified algorithm-dependent framework for HTL through a novel notion of transformation function, which characterizes the relation between the source and the target domains. We conduct a general risk analysis of this framework and in particular, we show for the first time, if two domains are related, HTL enjoys faster convergence rates of excess risks for Kernel Smoothing and Kernel Ridge Regression than those of the classical non-transfer learning settings. Experiments on real world data demonstrate the effectiveness of our framework.\n Abstract ,  PDF \n #57: Adversarial Invariant Feature Learning \n #58: Convergence Analysis of Two-layer Neural Networks with ReLU Activation \n Yuanzhi Li,  Yang Yuan \n In recent years, stochastic gradient descent (SGD) based techniques has become the standard tools for training neural networks. However, formal theoretical understanding of why SGD can train neural networks in practice is largely missing. In this paper, we make progress on understanding this mystery by providing a convergence analysis for SGD on a rich subset of two-layer feedforward networks with ReLU activations. This subset is characterized by a special structure called "identity mapping". We prove that, if input follows from Gaussian distribution, with standard $O(1/\\sqrt{d})$ initialization of the weights, SGD converges to the global minimum in polynomial number of steps. Unlike normal vanilla networks, the "identity mapping" makes our network asymmetric and thus the global minimum is unique. To complement our theory, we are also able to show experimentally that multi-layer networks with this mapping have better performance compared with normal vanilla networks. Our convergence theorem differs from traditional non-convex optimization techniques. We show that SGD converges to optimal in "two phases": In phase I, the gradient points to the wrong direction, however, a potential function $g$ gradually decreases. Then in phase II, SGD enters a nice one point convex region and converges. We also show that the identity mapping is necessary for convergence, as it moves the initial point to a better place for optimization. Experiment verifies our claims.\n Abstract ,  PDF \n #59: Doubly Accelerated Stochastic Variance Reduced Dual Averaging Method for Regularized Empirical Risk Minimization \n Tomoya Murata,  Taiji Suzuki \n In this paper, we develop a new accelerated stochastic gradient method for efficiently solving the convex regularized empirical risk minimization problem in mini-batch settings. The use of mini-batches is becoming a golden standard in the machine learning community, because mini-batch settings stabilize the gradient estimate and can easily make good use of parallel computing. The core of our proposed method is the incorporation of our new "double acceleration" technique and variance reduction technique. We theoretically analyze our proposed method and show that our method much improves the mini-batch efficiencies of previous accelerated stochastic methods, and essentially only needs size $\\sqrt{n}$ mini-batches for achieving the optimal iteration complexities for both non-strongly and strongly convex objectives, where $n$ is the training set size. Further, we show that even in non-mini-batch settings, our method achieves the best known convergence rate for both non-strongly and strongly convex objectives.\n Abstract ,  PDF \n #60: Langevin Dynamics with Continuous Tempering for Training Deep Neural Networks \n Nanyang Ye,  Zhanxing Zhu,  Rafal K. Mantiuk \n Minimizing non-convex and high-dimensional objective functions is challenging, especially when training modern deep neural networks. In this paper, a novel approach is proposed which divides the training process into two consecutive phases to obtain better generalization performance: Bayesian sampling and stochastic optimization. The first phase is to explore the energy landscape and to capture the "fat" modes; and the second one is to fine-tune the parameter learned from the first phase. In the Bayesian learning phase, we apply continuous tempering and stochastic approximation into the Langevin dynamics to create an efficient and effective sampler, in which the temperature is adjusted automatically according to the designed "temperature dynamics". These strategies can overcome the challenge of early trapping into bad local minima and have achieved remarkable improvements in various types of neural networks as shown in our theoretical analysis and empirical experiments.\n Abstract ,  PDF \n #61: Efficient Online Linear Optimization with Approximation Algorithms \n Dan Garber \n We revisit the problem of \\textit{online linear optimization} in case the set of feasible actions is accessible through an approximated linear optimization oracle with a factor $\\alpha$ multiplicative approximation guarantee. This setting is in particular interesting since it captures natural online extensions of well-studied \\textit{offline} linear optimization problems which are NP-hard, yet admit efficient approximation algorithms. The goal here is to minimize the $\\alpha$\\textit{-regret} which is the natural extension of the standard \\textit{regret} in \\textit{online learning} to this setting. We present new algorithms with significantly improved oracle complexity for both the full information and bandit variants of the problem. Mainly, for both variants, we present $\\alpha$-regret bounds of $O(T^{-1/3})$, were $T$ is the number of prediction rounds, using only $O(\\log{T})$ calls to the approximation oracle per iteration, on average. These are the first results to obtain both average oracle complexity of $O(\\log{T})$ (or even poly-logarithmic in $T$) and $\\alpha$-regret bound $O(T^{-c})$ for a constant $c>0$, for both variants.\n Abstract ,  PDF \n #62: Geometric Descent Method for Convex Composite Minimization \n #63: Diffusion Approximations for Online Principal Component Estimation and Global Convergence \n #64:  Avoiding Discrimination through Causal Reasoning \n #65: Nonparametric Online Regression while Learning the Metric \n Ilja Kuzborskij,  Nicolò Cesa-Bianchi \n We study algorithms for online nonparametric regression that learn the directions along which the regression function is smoother. Our algorithm learns the Mahalanobis metric based on the gradient outer product matrix $\\boldsymbol{G}$ of the regression function (automatically adapting to the effective rank of this matrix), while simultaneously bounding the regret ---on the same data sequence--- in terms of the spectrum of $\\boldsymbol{G}$. As a preliminary step in our analysis, we generalize a nonparametric online learning algorithm by Hazan and Megiddo by enabling it to compete against functions whose Lipschitzness is measured with respect to an arbitrary Mahalanobis metric.\n Abstract ,  PDF \n #66: Recycling for Fairness: Learning with Conditional Distribution Matching Constraints \n #67: Safe and Nested Subgame Solving for Imperfect-Information Games \n Noam Brown,  Tuomas Sandholm \n Unlike perfect-information games, imperfect-information games cannot be solved by decomposing the game into subgames that are solved independently. Instead, all decisions must consider the strategy of the game as a whole. While it is not possible to solve an imperfect-information game exactly through decomposition, it is possible to approximate solutions, or improve existing strategies, by solving disjoint subgames. This process is referred to as subgame solving. We introduce subgame solving techniques that outperform prior methods both in theory and practice. We also show how to adapt them, and past subgame solving techniques, to respond to opponent actions that are outside the original action abstraction; this significantly outperforms the prior state-of-the-art approach, action translation. Finally, we show that subgame solving can be repeated as the game progresses down the tree, leading to lower exploitability. Subgame solving is a key component of Libratus, the first AI to defeat top humans in heads-up no-limit Texas hold\'em poker.\n Abstract ,  PDF \n #68: Unsupervised Image-to-Image Translation Networks \n Ming-Yu Liu,  Thomas Breuel,  Jan Kautz \n Most of the existing image-to-image translation frameworks---mapping an image in one domain to a corresponding image in another---are based on supervised learning, i.e., pairs of corresponding images in two domains are required for learning the translation function. This largely limits their applications, because capturing corresponding images in two different domains is often a difficult task. To address the issue, we propose the UNsupervised Image-to-image Translation (UNIT) framework, which is based on variational autoencoders and generative adversarial networks. The proposed framework can learn the translation function without any corresponding images in two domains. We enable this learning capability by combining a weight-sharing constraint and an adversarial training objective. Through visualization results from various unsupervised image translation tasks, we verify the effectiveness of the proposed framework. An ablation study further reveals the critical design choices. Moreover, we apply the UNIT framework to the unsupervised domain adaptation task and achieve better results than competing algorithms do in benchmark datasets.\n Abstract ,  PDF \n #69: Coded Distributed Computing for Inverse Problems \n #70: A Screening Rule for l1-Regularized Ising Model Estimation \n #71: Improved Dynamic Regret for Non-degeneracy Functions \n #72: Learning Efficient Object Detection Models with Knowledge Distillation \n #73: One-Sided Unsupervised Domain Mapping \n Sagie Benaim,  Lior Wolf \n In unsupervised domain mapping, the learner is given two unmatched datasets $A$ and $B$. The goal is to learn a mapping $G_{AB}$ that translates a sample in $A$ to the analog sample in $B$. Recent approaches have shown that when learning simultaneously both $G_{AB}$ and the inverse mapping $G_{BA}$, convincing mappings are obtained. In this work, we present a method of learning $G_{AB}$ without learning $G_{BA}$. This is done by learning a mapping that maintains the distance between a pair of samples. Moreover, good mappings are obtained, even by maintaining the distance between different parts of the same sample before and after mapping. We present experimental results that the new method not only allows for one sided mapping learning, but also leads to preferable numerical results over the existing circularity-based constraint. Our entire code is made publicly available at this https URL .\n Abstract ,  PDF \n #74: Deep Mean-Shift Priors for Image Restoration \n Siavash Arjomand Bigdeli,  Meiguang Jin,  Paolo Favaro,  Matthias Zwicker \n In this paper we introduce a natural image prior that directly represents a Gaussian-smoothed version of the natural image distribution. We include our prior in a formulation of image restoration as a Bayes estimator that also allows us to solve noise-blind image restoration problems. We show that the gradient of our prior corresponds to the mean-shift vector on the natural image distribution. In addition, we learn the mean-shift vector field using denoising autoencoders, and use it in a gradient descent approach to perform Bayes risk minimization. We demonstrate competitive results for noise-blind deblurring, super-resolution, and demosaicing.\n Abstract ,  PDF \n #75: Greedy Algorithms for Cone Constrained Optimization with Convergence Guarantees \n Francesco Locatello,  Michael Tschannen,  Gunnar Rätsch,  Martin Jaggi \n Greedy optimization methods such as Matching Pursuit (MP) and Frank-Wolfe (FW) algorithms regained popularity in recent years due to their simplicity, effectiveness and theoretical guarantees. MP and FW address optimization over the linear span and the convex hull of a set of atoms, respectively. In this paper, we consider the intermediate case of optimization over the convex cone, parametrized as the conic hull of a generic atom set, leading to the first principled definitions of non-negative MP algorithms for which we give explicit convergence rates and demonstrate excellent empirical performance. In particular, we derive sublinear ($\\mathcal{O}(1/t)$) convergence on general smooth and convex objectives, and linear convergence ($\\mathcal{O}(e^{-t})$) on strongly convex objectives, in both cases for general sets of atoms. Furthermore, we establish a clear correspondence of our algorithms to known algorithms from the MP and FW literature. Our novel algorithms and analyses target general atom sets and general objective functions, and hence are directly applicable to a large variety of learning settings.\n Abstract ,  PDF \n #76: A New Theory for Nonconvex Matrix Completion \n #77: Robust Hypothesis Test for Functional Effect with Gaussian Processes \n #78: Lower bounds on the robustness to adversarial perturbations \n #79: Minimizing a Submodular Function from Samples \n #80: Introspective Classification with Convolutional Nets \n #81: Label Distribution Learning Forests \n Wei Shen,  Kai Zhao,  Yilu Guo,  Alan Yuille \n Label distribution learning (LDL) is a general learning framework, which assigns to an instance a distribution over a set of labels rather than a single label or multiple labels. Current LDL methods have either restricted assumptions on the expression form of the label distribution or limitations in representation learning, e.g., to learn deep features in an end-to-end manner. This paper presents label distribution learning forests (LDLFs) - a novel label distribution learning algorithm based on differentiable decision trees, which have several advantages: 1) Decision trees have the potential to model any general form of label distributions by a mixture of leaf node predictions. 2) The learning of differentiable decision trees can be combined with representation learning. We define a distribution-based loss function for a forest, enabling all the trees to be learned jointly, and show that an update function for leaf node predictions, which guarantees a strict decrease of the loss function, can be derived by variational bounding. The effectiveness of the proposed LDLFs is verified on several LDL tasks and a computer vision application, showing significant improvements to the state-of-the-art LDL methods.\n Abstract ,  PDF \n #82: Unsupervised object learning from dense equivariant image labelling \n James Thewlis,  Hakan Bilen,  Andrea Vedaldi \n One of the key challenges of visual perception is to extract abstract models of 3D objects and object categories from visual measurements, which are affected by complex nuisance factors such as viewpoint, occlusion, motion, and deformations. Starting from the recent idea of viewpoint factorization, we propose a new approach that, given a large number of images of an object and no other supervision, can extract a dense object-centric coordinate frame. This coordinate frame is invariant to deformations of the images and comes with a dense equivariant labelling neural network that can map image pixels to their corresponding object coordinates. We demonstrate the applicability of this method to simple articulated objects and deformable objects such as human faces, learning embeddings from random synthetic transformations or optical flow correspondences, all without any manual supervision.\n Abstract ,  PDF \n #83: Compression-aware Training of Deep Neural Networks \n #84: Multiscale Semi-Markov Dynamics for Intracortical Brain-Computer Interfaces \n #85: PredRNN: Recurrent Neural Networks for Video Prediction using Spatiotemporal LSTMs \n #86: Detrended Partial Cross Correlation for Brain Connectivity Analysis \n #87: Contrastive Learning for Image Captioning \n Bo Dai,  Dahua Lin \n Image captioning, a popular topic in computer vision, has achieved substantial progress in recent years. However, the distinctiveness of natural descriptions is often overlooked in previous work. It is closely related to the quality of captions, as distinctive captions are more likely to describe images with their unique aspects. In this work, we propose a new learning method, Contrastive Learning (CL), for image captioning. Specifically, via two constraints formulated on top of a reference model, the proposed method can encourage distinctiveness, while maintaining the overall quality of the generated captions. We tested our method on two challenging datasets, where it improves the baseline model by significant margins. We also showed in our studies that the proposed method is generic and can be used for models with various structures. \n #88: Safe Model-based Reinforcement Learning with Stability Guarantees \n Felix Berkenkamp,  Matteo Turchetta,  Angela P. Schoellig,  Andreas Krause \n Reinforcement learning is a powerful paradigm for learning optimal policies from experimental data. However, to find optimal policies, most reinforcement learning algorithms explore all possible actions, which may be harmful for real-world systems. As a consequence, learning algorithms are rarely applied on safety-critical systems in the real world. In this paper, we present a learning algorithm that explicitly considers safety in terms of stability guarantees. Specifically, we extend control theoretic results on Lyapunov stability verification and show how to use statistical models of the dynamics to obtain high-performance control policies with provable stability certificates. Moreover, under additional regularity assumptions in terms of a Gaussian process prior, we prove that one can effectively and safely collect data in order to learn about the dynamics and thus both improve control performance and expand the safe region of the state space. In our experiments, we show how the resulting algorithm can safely optimize a neural network policy on a simulated inverted pendulum, without the pendulum ever falling down.\n Abstract ,  PDF \n #89: Online multiclass boosting \n #90: Matching on Balanced Nonlinear Representations for Treatment Effects Estimation \n #91: Learning Overcomplete HMMs \n #92: GP CaKe: Effective brain connectivity with causal kernels \n Luca Ambrogioni,  Max Hinne,  Marcel van Gerven,  Eric Maris \n A fundamental goal in network neuroscience is to understand how activity in one region drives activity elsewhere, a process referred to as effective connectivity. Here we propose to model this causal interaction using integro-differential equations and causal kernels that allow for a rich analysis of effective connectivity. The approach combines the tractability and flexibility of autoregressive modeling with the biophysical interpretability of dynamic causal modeling. The causal kernels are learned nonparametrically using Gaussian process regression, yielding an efficient framework for causal inference. We construct a novel class of causal covariance functions that enforce the desired properties of the causal kernels, an approach which we call GP CaKe. By construction, the model and its hyperparameters have biophysical meaning and are therefore easily interpretable. We demonstrate the efficacy of GP CaKe on a number of simulations and give an example of a realistic application on magnetoencephalography (MEG) data.\n Abstract ,  PDF \n #93: Decoupling "when to update" from "how to update" \n #94: Self-Normalizing Neural Networks \n Günter Klambauer,  Thomas Unterthiner,  Andreas Mayr,  Sepp Hochreiter \n Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are "scaled exponential linear units" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.\n Abstract ,  PDF \n #95: Learning to Pivot with Adversarial Networks \n Gilles Louppe,  Michael Kagan,  Kyle Cranmer \n Several techniques for domain adaptation have been proposed to account for differences in the distribution of the data used for training and testing. The majority of this work focuses on a binary domain label. Similar problems occur in a scientific context where there may be a continuous family of plausible data generation processes associated to the presence of systematic uncertainties. Robust inference is possible if it is based on a pivot -- a quantity whose distribution does not depend on the unknown values of the nuisance parameters that parametrize this family of data generation processes. In this work, we introduce and derive theoretical results for a training procedure based on adversarial networks for enforcing the pivotal property (or, equivalently, fairness with respect to continuous attributes) on a predictive model. The method includes a hyperparameter to control the trade-off between accuracy and robustness. We demonstrate the effectiveness of this approach with a toy example and examples from particle physics.\n Abstract ,  PDF \n #96: MolecuLeNet: A continuous-filter convolutional neural network for modeling quantum interactions \n Kristof T. Schütt,  Pieter-Jan Kindermans,  Huziel E. Sauceda,  Stefan Chmiela,  Alexandre Tkatchenko,  Klaus-Robert Müller \n Deep learning has the potential to revolutionize quantum chemistry as it is ideally suited to learn representations for structured data and speed up the exploration of chemical space. While convolutional neural networks have proven to be the first choice for images, audio and video data, the atoms in molecules are not restricted to a grid. Instead, their precise locations contain essential physical information, that would get lost if discretized. Thus, we propose to use continuous-filter convolutional layers to be able to model local correlations without requiring the data to lie on a grid. We apply those layers in SchNet: a novel deep learning architecture modeling quantum interactions in molecules. We obtain a joint model for the total energy and interatomic forces that follows fundamental quantum-chemical principles. This includes rotationally invariant energy predictions and a smooth, differentiable potential energy surface. Our architecture achieves state-of-the-art performance for benchmarks of equilibrium molecules and molecular dynamics trajectories. Finally, we introduce a more challenging benchmark with chemical and structural variations that suggests the path for further work.\n Abstract ,  PDF \n #97: Active Bias: Training a More Accurate Neural Network by Emphasizing High Variance Samples \n Haw-Shiuan Chang,  Erik Learned-Miller,  Andrew McCallum \n Self-paced learning and hard example mining re-weight training instances to improve learning accuracy. This paper presents two improved alternatives based on lightweight estimates of sample uncertainty in stochastic gradient descent (SGD): the variance in predicted probability of the correct class across iterations of mini-batch SGD, and the proximity of the correct class probability to the decision threshold. Extensive experimental results on six datasets show that our methods reliably improve accuracy in various network architectures, including additional gains on top of other popular training techniques, such as residual learning, momentum, ADAM, batch normalization, dropout, and distillation.\n Abstract ,  PDF \n #98: Differentiable Learning of Submodular Functions \n #99: Inductive Representation Learning on Large Graphs \n William L. Hamilton,  Rex Ying,  Jure Leskovec \n Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node\'s local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.\n Abstract ,  PDF \n #100: Subset Selection for Sequential Data \n #101: Question Asking as Program Generation \n #102: Revisiting Perceptron: Efficient and Label-Optimal Learning of Halfspaces \n Songbai Yan,  Chicheng Zhang \n It has been a long-standing problem to efficiently learn a linear separator using as few labels as possible. In this work, we propose an efficient perceptron-based algorithm for actively learning homogeneous linear separators under uniform distribution. Under bounded noise, where each label is flipped with probability at most $\\eta$, our algorithm achieves near-optimal $\\tilde{O}\\left(\\frac{d}{(1-2\\eta)^2}\\log\\frac{1}{\\epsilon}\\right)$ label complexity in time $\\tilde{O}\\left(\\frac{d^2}{\\epsilon(1-2\\eta)^2}\\right)$, and significantly improves over the best known result (Awasthi et al., 2016). Under adversarial noise, where at most $\\nu$ fraction of labels can be flipped, our algorithm achieves near-optimal $\\tilde{O}\\left(d\\log\\frac{1}{\\epsilon}\\right)$ label complexity in time $\\tilde{O}\\left(\\frac{d^2}{\\epsilon}\\right)$, which is better than the best known label complexity and time complexity in Awasthi et al. (2014).\n Abstract ,  PDF \n #103: Gradient Descent Can Take Exponential Time to Escape Saddle Points \n Simon S. Du,  Chi Jin,  Jason D. Lee,  Michael I. Jordan,  Barnabas Poczos,  Aarti Singh \n Although gradient descent (GD) almost always escapes saddle points asymptotically [Lee et al., 2016], this paper shows that even with fairly natural random initialization schemes and non-pathological functions, GD can be significantly slowed down by saddle points, taking exponential time to escape. On the other hand, gradient descent with perturbations [Ge et al., 2015, Jin et al., 2017] is not slowed down by saddle points - it can find an approximate local minimizer in polynomial time. This result implies that GD is inherently slower than perturbed GD, and justifies the importance of adding perturbations for efficient non-convex optimization. While our focus is theoretical, we also present experiments that illustrate our theoretical findings.\n Abstract ,  PDF \n #104: Union of Intersections (UoI) for Interpretable Data Driven Discovery and Prediction \n Kristofer E. Bouchard,  Alejandro F. Bujan,  Farbod Roosta-Khorasani,  Shashanka Ubaru,  Prabhat,  Antoine M. Snijders,  Jian-Hua Mao,  Edward F. Chang,  Michael W. Mahoney,  Sharmodeep Bhattacharyya \n The increasing size and complexity of scientific data could dramatically enhance discovery and prediction for basic scientific applications. Realizing this potential, however, requires novel statistical analysis methods that are both interpretable and predictive. We introduce Union of Intersections (UoI), a flexible, modular, and scalable framework for enhanced model selection and estimation. Methods based on UoI perform model selection and model estimation through intersection and union operations, respectively. We show that UoI-based methods achieve low-variance and nearly unbiased estimation of a small number of interpretable features, while maintaining high-quality prediction accuracy. We perform extensive numerical investigation to evaluate a UoI algorithm ($UoI_{Lasso}$) on synthetic and real data. In doing so, we demonstrate the extraction of interpretable functional networks from human electrophysiology recordings as well as accurate prediction of phenotypes from genotype-phenotype data with reduced features. We also show (with the $UoI_{L1Logistic}$ and $UoI_{CUR}$ variants of the basic framework) improved prediction parsimony for classification and matrix factorization on several benchmark biomedical data sets. These results suggest that methods based on the UoI framework could improve interpretation and prediction in data-driven discovery across scientific fields.\n Abstract ,  PDF \n #105: One-Shot Imitation Learning \n Yan Duan,  Marcin Andrychowicz,  Bradly C. Stadie,  Jonathan Ho,  Jonas Schneider,  Ilya Sutskever,  Pieter Abbeel,  Wojciech Zaremba \n Imitation learning has been commonly applied to solve different tasks in isolation. This usually requires either careful feature engineering, or a significant number of samples. This is far from what we desire: ideally, robots should be able to learn from very few demonstrations of any given task, and instantly generalize to new situations of the same task, without requiring task-specific engineering. In this paper, we propose a meta-learning framework for achieving such capability, which we call one-shot imitation learning. Specifically, we consider the setting where there is a very large set of tasks, and each task has many instantiations. For example, a task could be to stack all blocks on a table into a single tower, another task could be to place all blocks on a table into two-block towers, etc. In each case, different instances of the task would consist of different sets of blocks with different initial states. At training time, our algorithm is presented with pairs of demonstrations for a subset of all tasks. A neural net is trained that takes as input one demonstration and the current state (which initially is the initial state of the other demonstration of the pair), and outputs an action with the goal that the resulting sequence of states and actions matches as closely as possible with the second demonstration. At test time, a demonstration of a single instance of a new task is presented, and the neural net is expected to perform well on new instances of this new task. The use of soft attention allows the model to generalize to conditions and tasks unseen in the training data. We anticipate that by training this model on a much greater variety of tasks and settings, we will obtain a general system that can turn any demonstrations into robust policies that can accomplish an overwhelming variety of tasks. Videos available at this https URL .\n Abstract ,  PDF \n #106: Learning the Morphology of Brain Signals Using Alpha-Stable Convolutional Sparse Coding \n Mainak Jas,  Tom Dupré La Tour,  Umut Şimşekli,  Alexandre Gramfort \n Neural time-series data contain a wide variety of prototypical signal waveforms (atoms) that are of significant importance in clinical and cognitive research. One of the goals for analyzing such data is hence to extract such \'shift-invariant\' atoms. Even though some success has been reported with existing algorithms, they are limited in applicability due to their heuristic nature. Moreover, they are often vulnerable to artifacts and impulsive noise, which are typically present in raw neural recordings. In this study, we address these issues and propose a novel probabilistic convolutional sparse coding (CSC) model for learning shift-invariant atoms from raw neural signals containing potentially severe artifacts. In the core of our model, which we call $\\alpha$CSC, lies a family of heavy-tailed distributions called $\\alpha$-stable distributions. We develop a novel, computationally efficient Monte Carlo expectation-maximization algorithm for inference. The maximization step boils down to a weighted CSC problem, for which we develop a computationally efficient optimization algorithm. Our results show that the proposed algorithm achieves state-of-the-art convergence speeds. Besides, $\\alpha$CSC is significantly more robust to artifacts when compared to three competing algorithms: it can extract spike bursts, oscillations, and even reveal more subtle phenomena such as cross-frequency coupling when applied to noisy neural time series.\n Abstract ,  PDF \n #107: Integration Methods and Optimization Algorithms \n #108: Sharpness, Restart and Acceleration \n Vincent Roulet,  Alexandre d\'Aspremont \n The {\\L}ojasievicz inequality shows that sharpness bounds on the minimum of convex optimization problems hold almost generically. Here, we show that sharpness directly controls the performance of restart schemes. The constants quantifying sharpness are of course unobservable, but we show that optimal restart strategies are fairly robust, and searching for the best scheme only increases the complexity by a logarithmic factor compared to the optimal bound. Overall then, restart schemes generically accelerate accelerated methods.\n Abstract ,  PDF \n #109: Learning Koopman Invariant Subspaces for Dynamic Mode Decomposition \n #110: Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations \n Eirikur Agustsson,  Fabian Mentzer,  Michael Tschannen,  Lukas Cavigelli,  Radu Timofte,  Luca Benini,  Luc Van Gool \n We present a new approach to learn compressible representations in deep architectures with an end-to-end training strategy. Our method is based on a soft (continuous) relaxation of quantization and entropy, which we anneal to their discrete counterparts throughout training. We showcase this method for two challenging applications: Image compression and neural network compression. While these tasks have typically been approached with different methods, our soft-to-hard quantization approach gives results competitive with the state-of-the-art for both.\n Abstract ,  PDF \n #111: Learning spatiotemporal piecewise-geodesic trajectories from longitudinal manifold-valued data \n #112: Improving Regret Bounds for Combinatorial Semi-Bandits with Probabilistically Triggered Arms and Its Applications \n #113: Predictive-State Decoders: Encoding the Future into Recurrent Networks \n Arun Venkatraman,  Nicholas Rhinehart,  Wen Sun,  Lerrel Pinto,  Martial Hebert,  Byron Boots,  Kris M. Kitani,  J. Andrew Bagnell \n Recurrent neural networks (RNNs) are a vital modeling technique that rely on internal states learned indirectly by optimization of a supervised, unsupervised, or reinforcement training loss. RNNs are used to model dynamic processes that are characterized by underlying latent states whose form is often unknown, precluding its analytic representation inside an RNN. In the Predictive-State Representation (PSR) literature, latent state processes are modeled by an internal state representation that directly models the distribution of future observations, and most recent work in this area has relied on explicitly representing and targeting sufficient statistics of this probability distribution. We seek to combine the advantages of RNNs and PSRs by augmenting existing state-of-the-art recurrent neural networks with Predictive-State Decoders (PSDs), which add supervision to the network\'s internal state representation to target predicting future observations. Predictive-State Decoders are simple to implement and easily incorporated into existing training pipelines via additional loss regularization. We demonstrate the effectiveness of PSDs with experimental results in three different domains: probabilistic filtering, Imitation Learning, and Reinforcement Learning. In each, our method improves statistical performance of state-of-the-art recurrent baselines and does so with fewer iterations and less data. \n #114: Posterior sampling for reinforcement learning: worst-case regret bounds \n Shipra Agrawal,  Randy Jia \n We present an algorithm based on posterior sampling (aka Thompson sampling) that achieves near-optimal worst-case regret bounds when the underlying Markov Decision Process (MDP) is communicating with a finite, though unknown, diameter. Our main result is a high probability regret upper bound of $\\tilde{O}(D\\sqrt{SAT})$ for any communicating MDP with $S$ states, $A$ actions and diameter $D$, when $T\\ge S^5A$. Here, regret compares the total reward achieved by the algorithm to the total expected reward of an optimal infinite-horizon undiscounted average reward policy, in time horizon $T$. This result improves over the best previously known upper bound of $\\tilde{O}(DS\\sqrt{AT})$ achieved by any algorithm in this setting, and matches the dependence on $S$ in the established lower bound of $\\Omega(\\sqrt{DSAT})$ for this problem. Our techniques involve proving some novel results about the anti-concentration of Dirichlet distribution, which may be of independent interest.\n Abstract ,  PDF \n #115: Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results \n #116: Matching neural paths: transfer from recognition to correspondence search \n Nikolay Savinov,  Lubor Ladicky,  Marc Pollefeys \n Many machine learning tasks require finding per-part correspondences between objects. In this work we focus on low-level correspondences - a highly ambiguous matching problem. We propose to use a hierarchical semantic representation of the objects, coming from a convolutional neural network, to solve this ambiguity. Training it for low-level correspondence prediction directly might not be an option in some domains where the ground-truth correspondences are hard to obtain. We show how transfer from recognition can be used to avoid such training. Our idea is to mark parts as "matching" if their features are close to each other at all the levels of convolutional feature hierarchy (neural paths). Although the overall number of such paths is exponential in the number of layers, we propose a polynomial algorithm for aggregating all of them in a single backward pass. The empirical validation is done on the task of stereo correspondence and demonstrates that we achieve competitive results among the methods which do not use labeled target domain data.\n Abstract ,  PDF \n #117: Linearly constrained Gaussian processes \n Carl Jidling,  Niklas Wahlström,  Adrian Wills,  Thomas B. Schön \n We consider a modification of the covariance function in Gaussian processes to correctly account for known linear constraints. By modelling the target function as a transformation of an underlying function, the constraints are explicitly incorporated in the model such that they are guaranteed to be fulfilled by any sample drawn or prediction made. We also propose a constructive procedure for designing the transformation operator and illustrate the result on both simulated and real-data examples.\n Abstract ,  PDF \n #118: Fixed-Rank Approximation of a Positive-Semidefinite Matrix from Streaming Data \n Joel A. Tropp,  Alp Yurtsever,  Madeleine Udell,  Volkan Cevher \n Several important applications, such as streaming PCA and semidefinite programming, involve a large-scale positive-semidefinite (psd) matrix that is presented as a sequence of linear updates. Because of storage limitations, it may only be possible to retain a sketch of the psd matrix. This paper develops a new algorithm for fixed-rank psd approximation from a sketch. The approach combines the Nystrom approximation with a novel mechanism for rank truncation. Theoretical analysis establishes that the proposed method can achieve any prescribed relative error in the Schatten 1-norm and that it exploits the spectral decay of the input matrix. Computer experiments show that the proposed method dominates alternative techniques for fixed-rank psd matrix approximation across a wide range of examples.\n Abstract ,  PDF \n #119: Multi-Modal Imitation Learning from Unstructured Demonstrations using Generative Adversarial Nets \n Karol Hausman,  Yevgen Chebotar,  Stefan Schaal,  Gaurav Sukhatme,  Joseph Lim \n Imitation learning has traditionally been applied to learn a single task from demonstrations thereof. The requirement of structured and isolated demonstrations limits the scalability of imitation learning approaches as they are difficult to apply to real-world scenarios, where robots have to be able to execute a multitude of tasks. In this paper, we propose a multi-modal imitation learning framework that is able to segment and imitate skills from unlabelled and unstructured demonstrations by learning skill segmentation and imitation learning jointly. The extensive simulation results indicate that our method can efficiently separate the demonstrations into individual skills and learn to imitate them using a single multi-modal policy. The video of our experiments is available at this http URL\n Abstract ,  PDF \n #120: Learning to Inpaint for Image Compression \n Mohammad Haris Baig,  Vladlen Koltun,  Lorenzo Torresani \n We study the design of deep architectures for lossy image compression. We present two architectural recipes in the context of multi-stage progressive encoders and empirically demonstrate their importance on compression performance. Specifically, we show that: (a) predicting the original image data from residuals in a multi-stage progressive architecture facilitates learning and leads to improved performance at approximating the original content and (b) learning to inpaint (from neighboring image pixels) before performing compression reduces the amount of information that must be stored to achieve a high-quality approximation. Incorporating these design choices in a baseline progressive encoder yields an average reduction of over $60\\%$ in file size with similar quality compared to the original residual encoder. \n #121: Adaptive Bayesian Sampling with Monte Carlo EM \n #122: No More Fixed Penalty  Parameter in ADMM: Faster Convergence with New Adaptive Penalization \n #123: Shape and Material from Sound \n #124: Flexible statistical inference for mechanistic models of neural dynamics \n #125: Online Prediction with Selfish Experts \n Tim Roughgarden,  Okke Schrijvers \n We consider the problem of binary prediction with expert advice in settings where experts have agency and seek to maximize their credibility. This paper makes three main contributions. First, it defines a model to reason formally about settings with selfish experts, and demonstrates that "incentive compatible" (IC) algorithms are closely related to the design of proper scoring rules. Designing a good IC algorithm is easy if the designer\'s loss function is quadratic, but for other loss functions, novel techniques are required. Second, we design IC algorithms with good performance guarantees for the absolute loss function. Third, we give a formal separation between the power of online prediction with selfish experts and online prediction with honest experts by proving lower bounds for both IC and non-IC algorithms. In particular, with selfish experts and the absolute loss function, there is no (randomized) algorithm for online prediction-IC or otherwise-with asymptotically vanishing regret.\n Abstract ,  PDF \n #126: Tensor Biclustering \n #127: DPSCREEN: Dynamic Personalized Screening \n #128: Learning Unknown Markov Decision Processes: A Thompson Sampling Approach \n Yi Ouyang,  Mukul Gagrani,  Ashutosh Nayyar,  Rahul Jain \n We consider the problem of learning an unknown Markov Decision Process (MDP) that is weakly communicating in the infinite horizon setting. We propose a Thompson Sampling-based reinforcement learning algorithm with dynamic episodes (TSDE). At the beginning of each episode, the algorithm generates a sample from the posterior distribution over the unknown model parameters. It then follows the optimal stationary policy for the sampled model for the rest of the episode. The duration of each episode is dynamically determined by two stopping criteria. The first stopping criterion controls the growth rate of episode length. The second stopping criterion happens when the number of visits to any state-action pair is doubled. We establish $\\tilde O(HS\\sqrt{AT})$ bounds on expected regret under a Bayesian setting, where $S$ and $A$ are the sizes of the state and action spaces, $T$ is time, and $H$ is the bound of the span. This regret bound matches the best available bound for weakly communicating MDPs. Numerical results show it to perform better than existing algorithms for infinite horizon MDPs.\n Abstract ,  PDF \n #129: Testing and Learning on Distributions with Symmetric Noise Invariance \n Ho Chung Leon Law,  Christopher Yau,  Dino Sejdinovic \n Kernel embeddings of distributions and the Maximum Mean Discrepancy (MMD), the resulting distance between distributions, are useful tools for fully nonparametric two-sample testing and learning on distributions. However, it is rarely that all possible differences between samples are of interest -- discovered differences can be due to different types of measurement noise, data collection artefacts or other irrelevant sources of variability. We propose distances between distributions which encode invariance to additive symmetric noise, aimed at testing whether the assumed true underlying processes differ. Moreover, we construct invariant features of distributions, leading to learning algorithms robust to the impairment of the input distributions with symmetric additive noise. Such features lend themselves to a straightforward neural network implementation and can thus also be learned given a supervised signal.\n Abstract ,  PDF \n #130: A Dirichlet Mixture Model of Hawkes Processes for Event Sequence Clustering \n Hongteng Xu,  Hongyuan Zha \n We propose an effective method to solve the event sequence clustering problems based on a novel Dirichlet mixture model of a special but significant type of point processes --- Hawkes process. In this model, each event sequence belonging to a cluster is generated via the same Hawkes process with specific parameters, and different clusters correspond to different Hawkes processes. The prior distribution of the Hawkes processes is controlled via a Dirichlet distribution. We learn the model via a maximum likelihood estimator (MLE) and propose an effective variational Bayesian inference algorithm. We specifically analyze the resulting EM-type algorithm in the context of inner-outer iterations and discuss several inner iteration allocation strategies. The identifiability of our model, the convergence of our learning method, and its sample complexity are analyzed in both theoretical and empirical ways, which demonstrate the superiority of our method to other competitors. The proposed method learns the number of clusters automatically and is robust to model misspecification. Experiments on both synthetic and real-world data show that our method can learn diverse triggering patterns hidden in asynchronous event sequences and achieve encouraging performance on clustering purity and consistency.\n Abstract ,  PDF \n #131: Deanonymization in the Bitcoin P2P Network \n #132: Accelerated consensus via Min-Sum Splitting \n #133: Generalized Linear Model Regression under Distance-to-set Penalties \n #134: Adaptive sampling for a population of neurons \n #135: Nonbacktracking Bounds on the Influence in Independent Cascade Models \n Emmanuel Abbe,  Sanjeev Kulkarni,  Eun Jee Lee \n This paper develops upper and lower bounds on the influence measure in a network, more precisely, the expected number of nodes that a seed set can influence in the independent cascade model. In particular, our bounds exploit nonbacktracking walks, Fortuin-Kasteleyn-Ginibre (FKG) type inequalities, and are computed by message passing implementation. Nonbacktracking walks have recently allowed for headways in community detection, and this paper shows that their use can also impact the influence computation. Further, we provide a knob to control the trade-off between the efficiency and the accuracy of the bounds. Finally, the tightness of the bounds is illustrated with simulations on various network models.\n Abstract ,  PDF \n #136: Learning with Feature Evolvable Streams \n Bo-Jian Hou,  Lijun Zhang,  Zhi-Hua Zhou \n Learning with streaming data has attracted much attention during the past few years. Though most studies consider data stream with fixed features, in real practice the features may be evolvable. For example, features of data gathered by limited-lifespan sensors will change when these sensors are substituted by new ones. In this paper, we propose a novel learning paradigm: Feature Evolvable Streaming Learning where old features would vanish and new features will occur. Rather than relying on only the current features, we attempt to recover the vanished features and exploit it to improve performance. Specifically, we learn two models from the recovered features and the current features, respectively. To benefit from the recovered features, we develop two ensemble methods. In the first method, we combine the predictions from two models and theoretically show that with assistance of old features, the performance on new features can be improved. In the second approach, we dynamically select the best single prediction and establish a better performance guarantee when the best model switches. Experiments on both synthetic and real data validate the effectiveness of our proposal.\n Abstract ,  PDF \n #137: Online Convex Optimization with Stochastic Constraints \n Hao Yu,  Michael J. Neely,  Xiaohan Wei \n This paper considers online convex optimization (OCO) with stochastic constraints, which generalizes Zinkevich\'s OCO over a known simple fixed set by introducing multiple stochastic functional constraints that are i.i.d. generated at each round and are disclosed to the decision maker only after the decision is made. This formulation arises naturally when decisions are restricted by stochastic environments or deterministic environments with noisy observations. It also includes many important problems as special cases, such as OCO with long term constraints, stochastic constrained convex optimization, and deterministic constrained convex optimization. To solve this problem, this paper proposes a new algorithm that achieves $O(\\sqrt{T})$ expected regret and constraint violations and $O(\\sqrt{T}\\log(T))$ high probability regret and constraint violations. Experiments on a real-world data center scheduling problem further verify the performance of the new algorithm.\n Abstract ,  PDF \n #138: Max-Margin Invariant Features from Transformed Unlabelled Data \n #139: Cognitive Impairment Prediction in Alzheimer’s Disease with Regularized Modal Regression \n #140: Translation Synchronization via Truncated Least Squares \n #141: From which world is your graph \n #142: A New Alternating Direction Method for Linear Programming \n #143: Regret Analysis for Continuous Dueling Bandit \n #144: Best Response Regression \n #145: TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning \n Wei Wen,  Cong Xu,  Feng Yan,  Chunpeng Wu,  Yandan Wang,  Yiran Chen,  Hai Li \n High network communication cost for synchronizing gradients and parameters is the well-known bottleneck of distributed training. In this work, we propose TernGrad that uses ternary gradients to accelerate distributed deep learning in data parallelism. Our approach requires only three numerical levels {-1,0,1} which can aggressively reduce the communication time. We mathematically prove the convergence of TernGrad under the assumption of a bound on gradients. Guided by the bound, we propose layer-wise ternarizing and gradient clipping to improve its convergence. Our experiments show that applying TernGrad on AlexNet does not incur any accuracy loss and can even improve accuracy. The accuracy loss of GoogLeNet induced by TernGrad is less than 2% on average. Finally, a performance model is proposed to study the scalability of TernGrad. Experiments show significant speed gains for various deep neural networks.\n Abstract ,  PDF \n #146: Learning Affinity via Spatial Propagation Networks \n Sifei Liu,  Shalini De Mello,  Jinwei Gu,  Guangyu Zhong,  Ming-Hsuan Yang,  Jan Kautz \n In this paper, we propose spatial propagation networks for learning the affinity matrix for vision tasks. We show that by constructing a row/column linear propagation model, the spatially varying transformation matrix exactly constitutes an affinity matrix that models dense, global pairwise relationships of an image. Specifically, we develop a three-way connection for the linear propagation model, which (a) formulates a sparse transformation matrix, where all elements can be the output from a deep CNN, but (b) results in a dense affinity matrix that effectively models any task-specific pairwise similarity matrix. Instead of designing the similarity kernels according to image features of two points, we can directly output all the similarities in a purely data-driven manner. The spatial propagation network is a generic framework that can be applied to many affinity-related tasks, including but not limited to image matting, segmentation and colorization, to name a few. Essentially, the model can learn semantically-aware affinity values for high-level vision tasks due to the powerful learning capability of the deep neural network classifier. We validate the framework on the task of refinement for image segmentation boundaries. Experiments on the HELEN face parsing and PASCAL VOC-2012 semantic segmentation tasks show that the spatial propagation network provides a general, effective and efficient solution for generating high-quality segmentation results. \n #147: Linear regression without correspondence \n Daniel Hsu,  Kevin Shi,  Xiaorui Sun \n This article considers algorithmic and statistical aspects of linear regression when the correspondence between the covariates and the responses is unknown. First, a fully polynomial-time approximation scheme is given for the natural least squares optimization problem in any constant dimension. Next, in an average-case and noise-free setting where the responses exactly correspond to a linear function of i.i.d. draws from a standard multivariate normal distribution, an efficient algorithm based on lattice basis reduction is shown to exactly recover the unknown linear function in arbitrary dimension. Finally, lower bounds on the signal-to-noise ratio are established for approximate recovery of the unknown linear function by any estimator.\n Abstract ,  PDF \n #148: NeuralFDR: Learning Discovery Thresholds from Hypothesis Features \n #149: Cost efficient gradient boosting \n #150: Probabilistic Rule Realization and Selection \n Haizi Yu,  Tianxi Li,  Lav R. Varshney \n Abstraction and realization are bilateral processes that are key in deriving intelligence and creativity. In many domains, the two processes are approached through rules: high-level principles that reveal invariances within similar yet diverse examples. Under a probabilistic setting for discrete input spaces, we focus on the rule realization problem which generates input sample distributions that follow the given rules. More ambitiously, we go beyond a mechanical realization that takes whatever is given, but instead ask for proactively selecting reasonable rules to realize. This goal is demanding in practice, since the initial rule set may not always be consistent and thus intelligent compromises are needed. We formulate both rule realization and selection as two strongly connected components within a single and symmetric bi-convex problem, and derive an efficient algorithm that works at large scale. Taking music compositional rules as the main example throughout the paper, we demonstrate our model\'s efficiency in not only music realization (composition) but also music interpretation and understanding (analysis).\n Abstract ,  PDF \n #151: Nearest-Neighbor Sample Compression: Efficiency, Consistency, Infinite Dimensions \n Aryeh Kontorovich,  Sivan Sabato,  Roi Weiss \n We examine the Bayes-consistency of a recently proposed 1-nearest-neighbor-based multiclass learning algorithm. This algorithm is derived from sample compression bounds and enjoys the statistical advantages of tight, fully empirical generalization bounds, as well as the algorithmic advantages of runtime and memory savings. We prove that this algorithm is strongly Bayes-consistent in metric spaces with finite doubling dimension --- the first consistency result for an efficient nearest-neighbor sample compression scheme. Rather surprisingly, we discover that this algorithm continues to be Bayes-consistent even in a certain infinite-dimensional setting, in which the basic measure-theoretic conditions on which classic consistency proofs hinge are violated. This is all the more surprising, since it is known that k-NN is not Bayes-consistent in this setting. We pose several challenging open problems for future research.\n Abstract ,  PDF \n #152: A Scale Free Algorithm for Stochastic Bandits with Bounded Kurtosis \n Tor Lattimore \n Existing strategies for finite-armed stochastic bandits mostly depend on a parameter of scale that must be known in advance. Sometimes this is in the form of a bound on the payoffs, or the knowledge of a variance or subgaussian parameter. The notable exceptions are the analysis of Gaussian bandits with unknown mean and variance by Cowan and Katehakis [2015] and of uniform distributions with unknown support [Cowan and Katehakis, 2015]. The results derived in these specialised cases are generalised here to the non-parametric setup, where the learner knows only a bound on the kurtosis of the noise, which is a scale free measure of the extremity of outliers.\n Abstract ,  PDF \n #153: Learning Multiple Tasks with Deep Relationship Networks \n Mingsheng Long,  Jianmin Wang,  Philip S. Yu \n Deep networks trained on large-scale data can learn transferable features to promote learning multiple tasks. As deep features eventually transition from general to specific along deep networks, a fundamental problem is how to exploit the relationship across different tasks and improve the feature transferability in the task-specific layers. In this paper, we propose Deep Relationship Networks (DRN) that discover the task relationship based on novel tensor normal priors over the parameter tensors of multiple task-specific layers in deep convolutional networks. By jointly learning transferable features and task relationships, DRN is able to alleviate the dilemma of negative-transfer in the feature layers and under-transfer in the classifier layer. Extensive experiments show that DRN yields state-of-the-art results on standard multi-task learning benchmarks.\n Abstract ,  PDF \n #154: Deep Hyperalignment \n #155: Online to Offline Conversions and Adaptive Minibatch Sizes \n Kfir Y. Levy \n We present an approach towards convex optimization that relies on a novel scheme which converts online adaptive algorithms into offline methods. In the offline optimization setting, our derived methods are shown to obtain favourable adaptive guarantees which depend on the harmonic sum of the queried gradients. We further show that our methods implicitly adapt to the objective\'s structure: in the smooth case fast convergence rates are ensured without any prior knowledge of the smoothness parameter, while still maintaining guarantees in the non-smooth setting. Our approach has a natural extension to the stochastic setting, resulting in a lazy version of SGD (stochastic GD), where minibathces are chosen \\emph{adaptively} depending on the magnitude of the gradients. Thus providing a principled approach towards choosing minibatch sizes.\n Abstract ,  PDF \n #156: Stochastic Optimization with Variance Reduction for Infinite Datasets with Finite Sum Structure \n Alberto Bietti (Thoth, MSR - INRIA),  Julien Mairal (Thoth) \n Stochastic optimization algorithms with variance reduction have proven successful for minimizing large finite sums of functions. Unfortunately, these techniques are unable to deal with stochastic perturbations of input data, induced for example by data augmentation. In such cases, the objective is no longer a finite sum, and the main candidate for optimization is the stochastic gradient descent method (SGD). In this paper, we introduce a variance reduction approach for these settings when the objective is composite and strongly convex. The convergence rate outperforms SGD with a typically much smaller constant factor, which depends on the variance of gradient estimates only due to perturbations on a single example.\n Abstract ,  PDF \n #157: Deep Learning with Topological Signatures \n Christoph Hofer,  Roland Kwitt,  Marc Niethammer,  Andreas Uhl \n Inferring topological and geometrical information from data can offer an alternative perspective on machine learning problems. Methods from topological data analysis, e.g., persistent homology, enable us to obtain such information, typically in the form of summary representations of topological features. However, such topological signatures often come with an unusual structure (e.g., multisets of intervals) that is highly impractical for most machine learning techniques. While many strategies have been proposed to map these topological signatures into machine learning compatible representations, they suffer from being agnostic to the target learning task. In contrast, we propose a technique that enables us to input topological signatures to deep neural networks and learn a task-optimal representation during training. Our approach is realized as a novel input layer with favorable theoretical properties. Classification experiments on 2D object shapes and social network graphs demonstrate the versatility of the approach and, in case of the latter, we even outperform the state-of-the-art by a large margin.\n Abstract ,  PDF \n #158: Predicting User Activity Level In Point Process Models With Mass Transport Equation \n #159: Submultiplicative Glivenko-Cantelli and Uniform Convergence of Revenues \n Noga Alon,  Moshe Babaioff,  Yannai A. Gonczarowski,  Yishay Mansour,  Shay Moran,  Amir Yehudayoff \n In this work we derive a variant of the classic Glivenko-Cantelli Theorem, which asserts uniform convergence of the empirical Cumulative Distribution Function (CDF) to the CDF of the underlying distribution. Our variant allows for tighter convergence bounds for extreme values of the CDF. We apply our bound in the context of revenue learning, which is a well-studied problem in economics and algorithmic game theory. We derive sample-complexity bounds on the uniform convergence rate of the empirical revenues to the true revenues, assuming a bound on the $k$th moment of the valuations, for any (possibly fractional) $k>1$. For uniform convergence in the limit, we give a complete characterization and a zero-one law: if the first moment of the valuations is finite, then uniform convergence almost surely occurs; conversely, if the first moment is infinite, then uniform convergence almost never occurs.\n Abstract ,  PDF \n #160: Deep Dynamic Poisson Factorization Model \n #161: Positive-Unlabeled Learning with Non-Negative Risk Estimator \n Ryuichi Kiryo,  Gang Niu,  Marthinus C. du Plessis,  Masashi Sugiyama \n From only positive (P) and unlabeled (U) data, a binary classifier could be trained with PU learning. Unbiased PU learning that is based on unbiased risk estimators is now state of the art. However, if its model is very flexible, its empirical risk on training data will go negative, and we will suffer from overfitting seriously. In this paper, we propose a novel non-negative risk estimator for PU learning. When being minimized, it is more robust against overfitting, and thus we are able to train very flexible models given limited P data. Moreover, we analyze the bias, consistency and mean-squared-error reduction of the proposed risk estimator as well as the estimation error of the corresponding risk minimizer. Experiments show that the non-negative risk estimator outperforms unbiased counterparts when they disagree.\n Abstract ,  PDF \n #162: Optimal Sample Complexity of M-wise Data for Top-K Ranking \n #163: What-If Reasoning using Counterfactual Gaussian Processes \n #164: Communication-Efficient Stochastic Gradient Descent, with Applications to Neural Networks \n #165: On the Convergence of Block Coordinate Descent in Training DNNs with Tikhonov Regularization \n #166: Train longer, generalize better: closing the generalization gap in large batch training of neural networks \n Elad Hoffer,  Itay Hubara,  Daniel Soudry \n Background: Deep learning models are typically trained using stochastic gradient descent or one of its variants. These methods update the weights using their gradient, estimated from a small fraction of the training data. It has been observed that when using large batch sizes there is a persistent degradation in generalization performance - known as the "generalization gap" phenomena. Identifying the origin of this gap and closing it had remained an open problem. Contributions: We examine the initial high learning rate training phase. We find that the weight distance from its initialization grows logarithmically with the number of weight updates. We therefore propose a "random walk on random landscape" statistical model which is known to exhibit similar "ultra-slow" diffusion behavior. Following this hypothesis we conducted experiments to show empirically that the "generalization gap" stems from the relatively small number of updates rather than the batch size, and can be completely eliminated by adapting the training regime used. We further investigate different techniques to train models in the large-batch regime and present a novel algorithm named "Ghost Batch Normalization" which enables significant decrease in the generalization gap without increasing the number of updates. To validate our findings we conduct several additional experiments on MNIST, CIFAR-10, CIFAR-100 and ImageNet. Finally, we reassess common practices and beliefs concerning training of deep models and suggest they may not be optimal to achieve good generalization.\n Abstract ,  PDF \n #167: Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks \n #168: Model evidence from nonequilibrium simulations \n #169: Minimal Exploration in Structured Stochastic Bandits \n #170: Learned D-AMP: Principled Neural-network-based Compressive Image Recovery \n #171: Deliberation Networks: Sequence Generation Beyond One-Pass Decoding \n #172: Adaptive Clustering through Semidefinite Programming \n #173: Log-normality and Skewness of Estimated State/Action Values in Reinforcement Learning \n #174: Repeated Inverse Reinforcement Learning \n Kareem Amin,  Nan Jiang,  Satinder Singh \n How detailed should we make the goals we prescribe to AI agents acting on our behalf in complex environments? Detailed and low-level specification of goals can be tedious and expensive to create, and abstract and high-level goals could lead to negative surprises as the agent may find behaviors that we would not want it to do, i.e., lead to unsafe AI. One approach to addressing this dilemma is for the agent to infer human goals by observing human behavior. This is the Inverse Reinforcement Learning (IRL) problem. However, IRL is generally ill-posed for there are typically many reward functions for which the observed behavior is optimal. While the use of heuristics to select from among the set of feasible reward functions has led to successful applications of IRL to learning from demonstration, such heuristics do not address AI safety. In this paper we introduce a novel repeated IRL problem that captures an aspect of AI safety as follows. The agent has to act on behalf of a human in a sequence of tasks and wishes to minimize the number of tasks that it surprises the human. Each time the human is surprised the agent is provided a demonstration of the desired behavior by the human. We formalize this problem, including how the sequence of tasks is chosen, in a few different ways and provide some foundational results.\n Abstract ,  PDF \n #175: The Numerics of GANs \n Lars Mescheder,  Sebastian Nowozin,  Andreas Geiger \n In this paper, we analyze the numerics of common algorithms for training Generative Adversarial Networks (GANs). Using the formalism of smooth two-player games we analyze the associated gradient vector field of GAN training objectives. Our findings suggest that the convergence of current algorithms suffers due to two factors: i) presence of eigenvalues of the Jacobian of the gradient vector field with zero real-part, and ii) eigenvalues with big imaginary part. Using these findings, we design a new algorithm that overcomes some of these limitations and has better convergence properties. Experimentally, we demonstrate its superiority on training common GAN architectures and show convergence on GAN architectures that are known to be notoriously hard to train. \n #176: Practical Bayesian Optimization for Model Fitting with Bayesian Adaptive Direct Search \n Luigi Acerbi,  Wei Ji Ma \n Computational models in fields such as computational neuroscience are often evaluated via stochastic simulation or numerical approximation. Fitting these models implies a difficult optimization problem over complex, possibly noisy parameter landscapes. Bayesian optimization (BO) has been successfully applied to solving expensive black-box problems in engineering and machine learning. Here we explore whether BO can be applied as a general tool for model fitting. First, we present a novel BO algorithm, Bayesian adaptive direct search (BADS), that achieves competitive performance with an affordable computational overhead for the running time of typical models. We then perform an extensive benchmark of BADS vs. many common and state-of-the-art nonconvex, derivative-free optimizers on a set of model-fitting problems with real data and models from six studies in behavioral, cognitive, and computational neuroscience. With default settings, BADS consistently finds comparable or better solutions than other methods, showing great promise for BO, and BADS in particular, as a general model-fitting tool.\n Abstract ,  PDF \n #177: Learning Chordal Markov Networks via Branch and Bound \n #178: Revenue Optimization with Approximate Bid Predictions \n Andrés Muñoz Medina,  Sergei Vassilvitskii \n In the context of advertising auctions, finding good reserve prices is a notoriously challenging learning problem. This is due to the heterogeneity of ad opportunity types and the non-convexity of the objective function. In this work, we show how to reduce reserve price optimization to the standard setting of prediction under squared loss, a well understood problem in the learning community. We further bound the gap between the expected bid and revenue in terms of the average loss of the predictor. This is the first result that formally relates the revenue gained to the quality of a standard machine learned model.\n Abstract ,  PDF \n #179: Solving (Almost) all Systems of Random Quadratic Equations \n #180: Unsupervised Learning of Disentangled Latent Representations from Sequential Data \n #181: Lookahead  Bayesian Optimization with Inequality Constraints \n #182: Hierarchical Methods of Moments \n #183: Interpretable and Globally Optimal Prediction for Textual Grounding using Image Concepts \n #184: Revisit Fuzzy Neural Network: Demystifying Batch Normalization and ReLU with Generalized Hamming Network \n #185: Speeding Up Latent Variable Gaussian Graphical Model Estimation via Nonconvex Optimization \n Pan Xu,  Jian Ma,  Quanquan Gu \n We study the estimation of the latent variable Gaussian graphical model (LVGGM), where the precision matrix is the superposition of a sparse matrix and a low-rank matrix. In order to speed up the estimation of the sparse plus low-rank components, we propose a sparsity constrained maximum likelihood estimator based on matrix factorization, and an efficient alternating gradient descent algorithm with hard thresholding to solve it. Our algorithm is orders of magnitude faster than the convex relaxation based methods for LVGGM. In addition, we prove that our algorithm is guaranteed to linearly converge to the unknown sparse and low-rank components up to the optimal statistical precision. Experiments on both synthetic and genomic data demonstrate the superiority of our algorithm over the state-of-the-art algorithms and corroborate our theory.\n Abstract ,  PDF \n #186: Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models \n Sergey Ioffe \n Batch Normalization is quite effective at accelerating and improving the training of deep models. However, its effectiveness diminishes when the training minibatches are small, or do not consist of independent samples. We hypothesize that this is due to the dependence of model layer inputs on all the examples in the minibatch, and different activations being produced between training and inference. We propose Batch Renormalization, a simple and effective extension to ensure that the training and inference models generate the same outputs that depend on individual examples rather than the entire minibatch. Models trained with Batch Renormalization perform substantially better than batchnorm when training with small or non-i.i.d. minibatches. At the same time, Batch Renormalization retains the benefits of batchnorm such as insensitivity to initialization and training efficiency.\n Abstract ,  PDF \n #187: Generating steganographic images via adversarial training \n #188: Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration \n #189: PixelGAN Autoencoders \n Alireza Makhzani,  Brendan Frey \n In this paper, we describe the "PixelGAN autoencoder", a generative autoencoder in which the generative path is a convolutional autoregressive neural network on pixels (PixelCNN) that is conditioned on a latent code, and the recognition path uses a generative adversarial network (GAN) to impose a prior distribution on the latent code. We show that different priors result in different decompositions of information between the latent code and the autoregressive decoder. For example, by imposing a Gaussian distribution as the prior, we can achieve a global vs. local decomposition, or by imposing a categorical distribution as the prior, we can disentangle the style and content information of images in an unsupervised fashion. We further show how the PixelGAN autoencoder with a categorical prior can be directly used in semi-supervised settings and achieve competitive semi-supervised classification results on the MNIST, SVHN and NORB datasets.\n Abstract ,  PDF \n #190: Consistent Multitask Learning with Nonlinear Output Relations \n Carlo Ciliberto,  Alessandro Rudi,  Lorenzo Rosasco,  Massimiliano Pontil \n Key to multitask learning is exploiting relationships between different tasks to improve prediction performance. If the relations are linear, regularization approaches can be used successfully. However, in practice assuming the tasks to be linearly related might be restrictive, and allowing for nonlinear structures is a challenge. In this paper, we tackle this issue by casting the problem within the framework of structured prediction. Our main contribution is a novel algorithm for learning multiple tasks which are related by a system of nonlinear equations that their joint outputs need to satisfy. We show that the algorithm is consistent and can be efficiently implemented. Experimental results show the potential of the proposed method.\n Abstract ,  PDF \n #191: Fast Alternating Minimization Algorithms for Dictionary Learning \n #192: Learning ReLUs via Gradient Descent \n Mahdi Soltanolkotabi \n In this paper we study the problem of learning Rectified Linear Units (ReLUs) which are functions of the form $max(0, )$ with $w$ denoting the weight vector. We study this problem in the high-dimensional regime where the number of observations are fewer than the dimension of the weight vector. We assume that the weight vector belongs to some closed set (convex or nonconvex) which captures known side-information about its structure. We focus on the realizable model where the inputs are chosen i.i.d.~from a Gaussian distribution and the labels are generated according to a planted weight vector. We show that projected gradient descent, when initialization at 0, converges at a linear rate to the planted model with a number of samples that is optimal up to numerical constants. Our results on the dynamics of convergence of these very shallow neural nets may provide some insights towards understanding the dynamics of deeper architectures.\n Abstract ,  PDF \n #193: Stabilizing Training of Generative Adversarial Networks through Regularization \n Kevin Roth,  Aurelien Lucchi,  Sebastian Nowozin,  Thomas Hofmann \n Deep generative models based on Generative Adversarial Networks (GANs) have demonstrated impressive sample quality but in order to work they require a careful choice of architecture, parameter initialization, and selection of hyper-parameters. This fragility is in part due to a dimensional mismatch between the model distribution and the true distribution, causing their density ratio and the associated f-divergence to be undefined. We overcome this fundamental limitation and propose a new regularization approach with low computational cost that yields a stable GAN training procedure. We demonstrate the effectiveness of this approach on several datasets including common benchmark image generation tasks. Our approach turns GAN models into reliable building blocks for deep learning.\n Abstract ,  PDF \n #194: Expectation Propagation with Stochastic Kinetic Model in Complex Interaction Systems \n #195: Data-Efficient Reinforcement Learning in Continuous State-Action Gaussian-POMDPs \n Rowan McAllister,  Carl Edward Rasmussen \n We present a data-efficient reinforcement learning algorithm resistant to observation noise. Our method extends the highly data-efficient PILCO algorithm (Deisenroth & Rasmussen, 2011) into partially observed Markov decision processes (POMDPs) by considering the filtering process during policy evaluation. PILCO conducts policy search, evaluating each policy by first predicting an analytic distribution of possible system trajectories. We additionally predict trajectories w.r.t. a filtering process, achieving significantly higher performance than combining a filter with a policy optimised by the original (unfiltered) framework. Our test setup is the cartpole swing-up task with sensor noise, which involves nonlinear dynamics and requires nonlinear control. \n #196: Compatible Reward Inverse Reinforcement Learning \n #197: First-Order Adaptive Sample Size Methods to Reduce Complexity of Empirical Risk Minimization \n #198: Hiding Images in Plain Sight: Deep Steganography \n #199: Neural Program Meta-Induction \n Jacob Devlin,  Rudy Bunel,  Rishabh Singh,  Matthew Hausknecht,  Pushmeet Kohli \n Most recently proposed methods for Neural Program Induction work under the assumption of having a large set of input/output (I/O) examples for learning any underlying input-output mapping. This paper aims to address the problem of data and computation efficiency of program induction by leveraging information from related tasks. Specifically, we propose two approaches for cross-task knowledge transfer to improve program induction in limited-data scenarios. In our first proposal, portfolio adaptation, a set of induction models is pretrained on a set of related tasks, and the best model is adapted towards the new task using transfer learning. In our second approach, meta program induction, a $k$-shot learning approach is used to make a model generalize to new tasks without additional training. To test the efficacy of our methods, we constructed a new benchmark of programs written in the Karel programming language. Using an extensive experimental evaluation on the Karel benchmark, we demonstrate that our proposals dramatically outperform the baseline induction method that does not use knowledge transfer. We also analyze the relative performance of the two approaches and study conditions in which they perform best. In particular, meta induction outperforms all existing approaches under extreme data sparsity (when a very small number of examples are available), i.e., fewer than ten. As the number of available I/O examples increase (i.e. a thousand or more), portfolio adapted program induction becomes the best approach. For intermediate data sizes, we demonstrate that the combined method of adapted meta program induction has the strongest performance. \n #200: Bayesian Dyadic Trees and Histograms for  Regression \n Stephanie van der Pas,  Veronika Rockova \n Many machine learning tools for regression are based on recursive partitioning of the covariate space into smaller regions, where the regression function can be estimated locally. Among these, regression trees and their ensembles have demonstrated impressive empirical performance. In this work, we shed light on the machinery behind Bayesian variants of these methods. In particular, we study Bayesian regression histograms, such as Bayesian dyadic trees, in the simple regression case with just one predictor. We focus on the reconstruction of regression surfaces that are piecewise constant, where the number of jumps is unknown. We show that with suitably designed priors, posterior distributions concentrate around the true step regression function at the minimax rate (up to a log factor). These results do not require the knowledge of the true number of steps, nor the width of the true partitioning cells. Thus, Bayesian dyadic regression trees are fully adaptive and can recover the true piecewise regression function nearly as well as if we knew the exact number and location of jumps. Our results constitute the first step towards understanding why Bayesian trees and their ensembles have worked so well in practice. As an aside, we discuss prior distributions on balanced interval partitions and how they relate to a problem in geometric probability. Namely, we quantify the probability of covering the circumference of a circle with random arcs whose endpoints are confined to a grid, a new variant of the original problem.\n Abstract ,  PDF \n #201: A graph-theoretic approach to multitasking \n #202: Consistent Robust Regression \n #203:  Natural value approximators: learning when to trust past estimates \n #204: Bandits Dueling on Partially Ordered Sets \n #205: Elementary Symmetric Polynomials for Optimal Experimental Design \n Zelda Mariet,  Suvrit Sra \n We revisit the classical problem of optimal experimental design (OED) under a new mathematical model grounded in a geometric motivation. Specifically, we introduce models based on elementary symmetric polynomials; these polynomials capture "partial volumes" and offer a graded interpolation between the widely used A-optimal design and D-optimal design models, obtaining each of them as special cases. We analyze properties of our models, and derive both greedy and convex-relaxation algorithms for computing the associated designs. Our analysis establishes approximation guarantees on these algorithms, while our empirical results substantiate our claims and demonstrate a curious phenomenon concerning our greedy method. Finally, as a byproduct, we obtain new results on the theory of elementary symmetric polynomials that may be of independent interest.\n Abstract ,  PDF \n #206: Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols \n Serhii Havrylov,  Ivan Titov \n Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general AI. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. Unlike previous work, we require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). We compare a reinforcement learning approach and one using a differentiable relaxation (straight-through Gumbel-softmax estimator) and observe that the latter is much faster to converge and it results in more effective protocols. Interestingly, we also observe that the protocol we induce by optimizing the communication success exhibits a degree of compositionality and variability (i.e. the same information can be phrased in different ways), both properties characteristic of natural languages. As the ultimate goal is to ensure that communication is accomplished in natural language, we also perform experiments where we inject prior information about natural language into our model and study properties of the resulting protocol.\n Abstract ,  PDF \n #207: Backprop without Learning Rates Through Coin Betting \n Francesco Orabona,  Tatiana Tommasi \n Deep learning methods achieve state-of-the-art performance in many application scenarios. Yet, these methods require a significant amount of hyperparameters tuning in order to achieve the best results. In particular, tuning the learning rates in the stochastic optimization process is still one of the main bottlenecks. In this paper, we propose a new stochastic gradient descent procedure for deep networks that does not require any learning rate setting. Contrary to previous methods, we do not adapt the learning rates nor we make use of the assumed curvature of the objective function. Instead, we reduce the optimization process to a game of betting on a coin and propose a learning rate free optimal algorithm for this scenario. Theoretical convergence is proven for convex and quasi-convex functions and empirical evidence shows the advantage of our algorithm over popular stochastic gradient algorithms.\n Abstract ,  PDF \n #208: Pixels to Graphs by Associative Embedding \n Alejandro Newell,  Jia Deng \n Graphs are a useful abstraction of image content. Not only can graphs represent details about individual objects in a scene but they can capture the interactions between pairs of objects. We present a method for training a convolutional neural network such that it takes in an input image and produces a full graph. This is done end-to-end in a single stage with the use of associative embeddings. The network learns to simultaneously identify all of the elements that make up a graph and piece them together. We benchmark on the Visual Genome dataset, and report a Recall@50 of 9.7% compared to the prior state-of-the-art at 3.4%, a nearly threefold improvement on the challenging task of scene graph generation.\n Abstract ,  PDF \n #209: Runtime Neural Pruning \n #210: Compressing the Gram Matrix for Learning Neural Networks in Polynomial Time \n #211: MMD GAN: Towards Deeper Understanding of Moment Matching Network \n Chun-Liang Li,  Wei-Cheng Chang,  Yu Cheng,  Yiming Yang,  Barnabás Póczos \n Generative moment matching network (GMMN) is a deep generative model that differs from Generative Adversarial Network (GAN) by replacing the discriminator in GAN with a two-sample test based on kernel maximum mean discrepancy (MMD). Although some theoretical guarantees of MMD have been studied, the empirical performance of GMMN is still not as competitive as that of GAN on challenging and large benchmark datasets. The computational efficiency of GMMN is also less desirable in comparison with GAN, partially due to its requirement for a rather large batch size during the training. In this paper, we propose to improve both the model expressiveness of GMMN and its computational efficiency by introducing adversarial kernel learning techniques, as the replacement of a fixed Gaussian kernel in the original GMMN. The new approach combines the key ideas in both GMMN and GAN, hence we name it MMD-GAN. The new distance measure in MMD-GAN is a meaningful loss that enjoys the advantage of weak topology and can be optimized via gradient descent with relatively small batch sizes. In our evaluation on multiple benchmark datasets, including MNIST, CIFAR- 10, CelebA and LSUN, the performance of MMD-GAN significantly outperforms GMMN, and is competitive with other representative GAN works.\n Abstract ,  PDF \n #212: The Reversible Residual Network: Backpropagation Without Storing Activations \n Aidan N. Gomez,  Mengye Ren,  Raquel Urtasun,  Roger B. Grosse \n Deep residual networks (ResNets) have significantly pushed forward the state-of-the-art on image classification, increasing in performance as networks grow both deeper and wider. However, memory consumption becomes a bottleneck, as one needs to store the activations in order to calculate gradients using backpropagation. We present the Reversible Residual Network (RevNet), a variant of ResNets where each layer\'s activations can be reconstructed exactly from the next layer\'s. Therefore, the activations for most layers need not be stored in memory during backpropagation. We demonstrate the effectiveness of RevNets on CIFAR-10, CIFAR-100, and ImageNet, establishing nearly identical classification accuracy to equally-sized ResNets, even though the activation storage requirements are independent of depth.\n Abstract ,  PDF \n #213: Fast Rates for Bandit Optimization with Upper-Confidence Frank-Wolfe \n #214: Zap Q-Learning \n #215: Expectation Propagation for t-Exponential Family Using Q-Algebra \n Futoshi Futami,  Issei Sato,  Masashi Sugiyama \n Exponential family distributions are highly useful in machine learning since their calculation can be performed efficiently through natural parameters. The exponential family has recently been extended to the t-exponential family, which contains Student-t distributions as family members and thus allows us to handle noisy data well. However, since the t-exponential family is denied by the deformed exponential, we cannot derive an efficient learning algorithm for the t-exponential family such as expectation propagation (EP). In this paper, we borrow the mathematical tools of q-algebra from statistical physics and show that the pseudo additivity of distributions allows us to perform calculation of t-exponential family distributions through natural parameters. We then develop an expectation propagation (EP) algorithm for the t-exponential family, which provides a deterministic approximation to the posterior or predictive distribution with simple moment matching. We finally apply the proposed EP algorithm to the Bayes point machine and Student-t process classication, and demonstrate their performance numerically.\n Abstract ,  PDF \n #216: Few-Shot Learning Through an Information Retrieval  Lens \n Eleni Triantafillou,  Richard Zemel,  Raquel Urtasun \n Few-shot learning refers to understanding new concepts from only a few examples. We propose an information retrieval-inspired approach for this problem that is motivated by the increased importance of maximally leveraging all the available information in this low-data regime. We define a training objective that aims to extract as much information as possible from each training batch by effectively optimizing over all relative orderings of the batch points simultaneously. In particular, we view each batch point as a `query\' that ranks the remaining ones based on its predicted relevance to them and we define a model within the framework of structured prediction to optimize mean Average Precision over these rankings. Our method produces state-of-the-art results on standard few-shot learning benchmarks.\n Abstract ,  PDF \n #217: Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation \n Matthias Hein,  Maksym Andriushchenko \n Recent work has shown that state-of-the-art classifiers are quite brittle, in the sense that a small adversarial change of an originally with high confidence correctly classified input leads to a wrong classification again with high confidence. This raises concerns that such classifiers are vulnerable to attacks and calls into question their usage in safety-critical systems. We show in this paper for the first time formal guarantees on the robustness of a classifier by giving instance-specific lower bounds on the norm of the input manipulation required to change the classifier decision. Based on this analysis we propose the Cross-Lipschitz regularization functional. We show that using this form of regularization in kernel methods resp. neural networks improves the robustness of the classifier without any loss in prediction performance.\n Abstract ,  PDF \n #218: Associative Embedding: End-to-End Learning for Joint Detection and Grouping \n Alejandro Newell,  Zhiao Huang,  Jia Deng \n We introduce associative embedding, a novel method for supervising convolutional neural networks for the task of detection and grouping. A number of computer vision problems can be framed in this manner including multi-person pose estimation, instance segmentation, and multi-object tracking. Usually the grouping of detections is achieved with multi-stage pipelines, instead we propose an approach that teaches a network to simultaneously output detections and group assignments. This technique can be easily integrated into any state-of-the-art network architecture that produces pixel-wise predictions. We show how to apply this method to both multi-person pose estimation and instance segmentation and report state-of-the-art performance for multi-person pose on the MPII and MS-COCO datasets.\n Abstract ,  PDF \n #219: Practical Locally Private Heavy Hitters \n Raef Bassily,  Kobbi Nissim,  Uri Stemmer,  Abhradeep Thakurta \n We present new practical local differentially private heavy hitters algorithms achieving optimal or near-optimal worst-case error and running time -- TreeHist and Bitstogram. In both algorithms, server running time is $\\tilde O(n)$ and user running time is $\\tilde O(1)$, hence improving on the prior state-of-the-art result of Bassily and Smith [STOC 2015] requiring $O(n^{5/2})$ server time and $O(n^{3/2})$ user time. With a typically large number of participants in local algorithms ($n$ in the millions), this reduction in time complexity, in particular at the user side, is crucial for making locally private heavy hitters algorithms usable in practice. We implemented Algorithm TreeHist to verify our theoretical analysis and compared its performance with the performance of Google\'s RAPPOR code.\n Abstract ,  PDF \n #220: Large-Scale Quadratically Constrained Quadratic Program via Low-Discrepancy Sequences \n Kinjal Basu,  Ankan Saha,  Shaunak Chatterjee \n We consider the problem of solving a large-scale Quadratically Constrained Quadratic Program. Such problems occur naturally in many scientific and web applications. Although there are efficient methods which tackle this problem, they are mostly not scalable. In this paper, we develop a method that transforms the quadratic constraint into a linear form by sampling a set of low-discrepancy points. The transformed problem can then be solved by applying any state-of-the-art large-scale quadratic programming solvers. We show the convergence of our approximate solution to the true solution as well as some finite sample error bounds. Experimental results are also shown to prove scalability as well as improved quality of approximation in practice. \n #221: Inhomogoenous Hypergraph Clustering with Applications \n Pan Li,  Olgica Milenkovic \n Hypergraph partitioning is an important problem in machine learning, computer vision and network analytics. A widely used method for hypergraph partitioning relies on minimizing a normalized sum of the costs of partitioning hyperedges across clusters. Algorithmic solutions based on this approach assume that different partitions of a hyperedge incur the same cost. However, this assumption fails to leverage the fact that different subsets of vertices within the same hyperedge may have different structural importance. We hence propose a new hypergraph clustering technique, termed inhomogeneous hypergraph partitioning, which assigns different costs to different hyperedge cuts. We prove that inhomogeneous partitioning produces a quadratic approximation to the optimal solution if the inhomogeneous costs satisfy submodularity constraints. Moreover, we demonstrate that inhomogenous partitioning offers significant performance improvements in applications such as structure learning of rankings, subspace segmentation and motif clustering.\n Abstract ,  PDF \n #222: Differentiable Learning of Logical Rules for Knowledge Base Reasoning \n Fan Yang,  Zhilin Yang,  William W. Cohen \n We study the problem of learning probabilistic first-order logical rules for knowledge base reasoning. This learning problem is difficult because it requires learning the parameters in a continuous space as well as the structure in a discrete space. We propose a framework, Neural Logic Programming, that combines the parameter and structure learning of first-order logical rules in an end-to-end differentiable model. This approach is inspired by a recently-developed differentiable logic called TensorLog, where inference tasks can be compiled into sequences of differentiable operations. We design a neural controller system that learns to compose these operations. Empirically, our method obtains state-of-the-art results on multiple knowledge base benchmark datasets, including Freebase and WikiMovies.\n Abstract ,  PDF \n #223: Deep Multi-task Gaussian Processes for Survival Analysis with Competing Risks \n #224: Masked Autoregressive Flow for Density Estimation \n George Papamakarios,  Theo Pavlakou,  Iain Murray \n Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.\n Abstract ,  PDF \n #225: Non-convex Finite-Sum Optimization Via SCSG Methods \n #226: Beyond normality: Learning sparse probabilistic graphical models in the non-Gaussian setting \n #227: Inner-loop free ADMM using Auxiliary Deep Neural Networks \n #228: OnACID: Online Analysis of Calcium Imaging Data in Real Time \n #229: Collaborative PAC Learning \n #230: Fast Black-box Variational Inference through Stochastic Trust-Region Optimization \n Jeffrey Regier,  Michael I. Jordan,  Jon McAuliffe \n We introduce TrustVI, a fast second-order algorithm for black-box variational inference based on trust-region optimization and the reparameterization trick. At each iteration, TrustVI proposes and assesses a step based on minibatches of draws from the variational distribution. The algorithm provably converges to a stationary point. We implement TrustVI in the Stan framework and compare it to ADVI. TrustVI typically converges in tens of iterations to a solution at least as good as the one that ADVI reaches in thousands of iterations. TrustVI iterations can be more computationally expensive, but total computation is typically an order of magnitude less in our experiments.\n Abstract ,  PDF \n #231: Scalable Demand-Aware Recommendation \n #232: SGD Learns the Conjugate Kernel Class of the Network \n Amit Daniely \n We show that the standard stochastic gradient decent (SGD) algorithm is guaranteed to learn, in polynomial time, a function that is competitive with the best function in the conjugate kernel space of the network, as defined in Daniely, Frostig and Singer. The result holds for log-depth networks from a rich family of architectures. To the best of our knowledge, it is the first polynomial-time guarantee for the standard neural network learning algorithm for networks of depth more that two. As corollaries, it follows that for neural networks of any depth between $2$ and $\\log(n)$, SGD is guaranteed to learn, in polynomial time, constant degree polynomials with polynomially bounded coefficients. Likewise, it follows that SGD on large enough networks can learn any continuous function (not in polynomial time), complementing classical expressivity results.\n Abstract ,  PDF \n #233: Noise-Tolerant Interactive Learning Using Pairwise Comparisons \n #234: Analyzing Hidden Representations in End-to-End Automatic Speech Recognition Systems \n Yonatan Belinkov,  James Glass \n Neural models have become ubiquitous in automatic speech recognition systems. While neural networks are typically used as acoustic models in more complex systems, recent studies have explored end-to-end speech recognition systems based on neural networks, which can be trained to directly predict text from input acoustic features. Although such systems are conceptually elegant and simpler than traditional systems, it is less obvious how to interpret the trained models. In this work, we analyze the speech representations learned by a deep end-to-end model that is based on convolutional and recurrent layers, and trained with a connectionist temporal classification (CTC) loss. We use a pre-trained model to generate frame-level features which are given to a classifier that is trained on frame classification into phones. We evaluate representations from different layers of the deep model and compare their quality for predicting phone labels. Our experiments shed light on important aspects of the end-to-end model such as layer depth, model complexity, and other design choices.\n Abstract ,  PDF \n #235: Generative Local Metric Learning for Kernel Regression \n #236: Information Theoretic Properties of Markov Random Fields, and their Algorithmic Applications \n Linus Hamilton,  Frederic Koehler,  Ankur Moitra \n Markov random fields area popular model for high-dimensional probability distributions. Over the years, many mathematical, statistical and algorithmic problems on them have been studied. Until recently, the only known algorithms for provably learning them relied on exhaustive search, correlation decay or various incoherence assumptions. Bresler gave an algorithm for learning general Ising models on bounded degree graphs. His approach was based on a structural result about mutual information in Ising models. Here we take a more conceptual approach to proving lower bounds on the mutual information through setting up an appropriate zero-sum game. Our proof generalizes well beyond Ising models, to arbitrary Markov random fields with higher order interactions. As an application, we obtain algorithms for learning Markov random fields on bounded degree graphs on $n$ nodes with $r$-order interactions in $n^r$ time and $\\log n$ sample complexity. The sample complexity is information theoretically optimal up to the dependence on the maximum degree. The running time is nearly optimal under standard conjectures about the hardness of learning parity with noise.\n Abstract ,  PDF \n #237: Fitting Low-Rank Tensors in Constant Time \n #238: Deep supervised discrete hashing \n #239: Using Options and Covariance Testing for Long Horizon Off-Policy Policy Evaluation \n #240: How regularization affects the critical points in linear networks \n Amirhossein Taghvaei,  Jin W. Kim,  Prashant G. Mehta \n This paper is concerned with the problem of representing and learning a linear transformation using a linear neural network. In recent years, there has been a growing interest in the study of such networks in part due to the successes of deep learning. The main question of this body of research and also of this paper pertains to the existence and optimality properties of the critical points of the mean-squared loss function. The primary concern here is the robustness of the critical points with regularization of the loss function. An optimal control model is introduced for this purpose and a learning algorithm (regularized form of backprop) derived for the same using the Hamilton\'s formulation of optimal control. The formulation is used to provide a complete characterization of the critical points in terms of the solutions of a nonlinear matrix-valued equation, referred to as the characteristic equation. Analytical and numerical tools from bifurcation theory are used to compute the critical points via the solutions of the characteristic equation. The main conclusion is that the critical point diagram can be fundamentally different even with arbitrary small amounts of regularization.\n Abstract ,  PDF \n #241: Fisher GAN \n Youssef Mroueh,  Tom Sercu \n Generative Adversarial Networks (GANs) are powerful models for learning complex distributions. Stable training of GANs has been addressed in many recent works which explore different metrics between distributions. In this paper we introduce Fisher GAN which fits within the Integral Probability Metrics (IPM) framework for training GANs. Fisher GAN defines a critic with a data dependent constraint on its second order moments. We show in this paper that Fisher GAN allows for stable and time efficient training that does not compromise the capacity of the critic, and does not need data independent constraints such as weight clipping. We analyze our Fisher IPM theoretically and provide an algorithm based on Augmented Lagrangian for Fisher GAN. We validate our claims on both image sample generation and semi-supervised classification using Fisher GAN.\n Abstract ,  PDF \n #242: Information-theoretic analysis of generalization capability of learning algorithms \n Aolin Xu,  Maxim Raginsky \n We derive upper bounds on the generalization error of a learning algorithm in terms of the mutual information between its input and output. The upper bounds provide theoretical guidelines for striking the right balance between data fit and generalization by controlling the input-output mutual information of a learning algorithm. The results can also be used to analyze the generalization capability of learning algorithms under adaptive composition, and the bias-accuracy tradeoffs in adaptive data analytics. Our work extends and leads to nontrivial improvements on the recent results of Russo and Zou.\n Abstract ,  PDF \n #243: Sparse Approximate Conic Hulls \n #244: Rigorous Dynamics and Consistent Estimation in Arbitrarily Conditioned Linear Systems \n Alyson K. Fletcher,  Mojtaba Sahraee-Ardakan,  Philip Schniter,  Sundeep Rangan \n The problem of estimating a random vector x from noisy linear measurements y = A x + w with unknown parameters on the distributions of x and w, which must also be learned, arises in a wide range of statistical learning and linear inverse problems. We show that a computationally simple iterative message-passing algorithm can provably obtain asymptotically consistent estimates in a certain high-dimensional large-system limit (LSL) under very general parameterizations. Previous message passing techniques have required i.i.d. sub-Gaussian A matrices and often fail when the matrix is ill-conditioned. The proposed algorithm, called adaptive vector approximate message passing (Adaptive VAMP) with auto-tuning, applies to all right-rotationally random A. Importantly, this class includes matrices with arbitrarily poor conditioning. We show that the parameter estimates and mean squared error (MSE) of x in each iteration converge to deterministic limits that can be precisely predicted by a simple set of state evolution (SE) equations. In addition, a simple testable condition is provided in which the MSE matches the Bayes-optimal value predicted by the replica method. The paper thus provides a computationally simple method with provable guarantees of optimality and consistency over a large class of linear inverse problems.\n Abstract ,  PDF \n #245: Toward Goal-Driven Neural Network Models for the Rodent Whisker-Trigeminal System \n Chengxu Zhuang,  Jonas Kubilius,  Mitra Hartmann,  Daniel Yamins \n In large part, rodents see the world through their whiskers, a powerful tactile sense enabled by a series of brain areas that form the whisker-trigeminal system. Raw sensory data arrives in the form of mechanical input to the exquisitely sensitive, actively-controllable whisker array, and is processed through a sequence of neural circuits, eventually arriving in cortical regions that communicate with decision-making and memory areas. Although a long history of experimental studies has characterized many aspects of these processing stages, the computational operations of the whisker-trigeminal system remain largely unknown. In the present work, we take a goal-driven deep neural network (DNN) approach to modeling these computations. First, we construct a biophysically-realistic model of the rat whisker array. We then generate a large dataset of whisker sweeps across a wide variety of 3D objects in highly-varying poses, angles, and speeds. Next, we train DNNs from several distinct architectural families to solve a shape recognition task in this dataset. Each architectural family represents a structurally-distinct hypothesis for processing in the whisker-trigeminal system, corresponding to different ways in which spatial and temporal information can be integrated. We find that most networks perform poorly on the challenging shape recognition task, but that specific architectures from several families can achieve reasonable performance levels. Finally, we show that Representational Dissimilarity Matrices (RDMs), a tool for comparing population codes between neural systems, can separate these higher-performing networks with data of a type that could plausibly be collected in a neurophysiological or imaging experiment. Our results are a proof-of-concept that goal-driven DNN networks of the whisker-trigeminal system are potentially within reach.\n Abstract ,  PDF \n #246: Accuracy First: Selecting a Differential Privacy Level for Accuracy Constrained ERM \n Katrina Ligett,  Seth Neel,  Aaron Roth,  Bo Waggoner,  Z. Steven Wu \n Traditional approaches to differential privacy assume a fixed privacy requirement $\\epsilon$ for a computation, and attempt to maximize the accuracy of the computation subject to the privacy constraint. As differential privacy is increasingly deployed in practical settings, it may often be that there is instead a fixed accuracy requirement for a given computation and the data analyst would like to maximize the privacy of the computation subject to the accuracy constraint. This raises the question of how to find and run a maximally private empirical risk minimizer subject to a given accuracy requirement. We propose a general "noise reduction" framework that can apply to a variety of private empirical risk minimization (ERM) algorithms, using them to "search" the space of privacy levels to find the empirically strongest one that meets the accuracy constraint, incurring only logarithmic overhead in the number of privacy levels searched. The privacy analysis of our algorithm leads naturally to a version of differential privacy where the privacy parameters are dependent on the data, which we term ex-post privacy, and which is related to the recently introduced notion of privacy odometers. We also give an ex-post privacy analysis of the classical AboveThreshold privacy tool, modifying it to allow for queries chosen depending on the database. Finally, we apply our approach to two common objectives, regularized linear and logistic regression, and empirically compare our noise reduction methods to (i) inverting the theoretical utility guarantees of standard private ERM algorithms and (ii) a stronger, empirical baseline based on binary search.\n Abstract ,  PDF \n #247: EX2: Exploration with Exemplar Models for Deep Reinforcement Learning \n Justin Fu,  John D. Co-Reyes,  Sergey Levine \n Deep reinforcement learning algorithms have been shown to learn complex tasks using highly general policy classes. However, sparse reward problems remain a significant challenge. Exploration methods based on novelty detection have been particularly successful in such settings but typically require generative or predictive models of the observations, which can be difficult to train when the observations are very high-dimensional and complex, as in the case of raw images. We propose a novelty detection algorithm for exploration that is based entirely on discriminatively trained exemplar models, where classifiers are trained to discriminate each visited state against all others. Intuitively, novel states are easier to distinguish against other states seen during training. We show that this kind of discriminative modeling corresponds to implicit density estimation, and that it can be combined with count-based exploration to produce competitive results on a range of popular benchmark tasks, including state-of-the-art results on challenging egocentric observations in the vizDoom benchmark.\n Abstract ,  PDF \n #248: Multitask Spectral Learning of Weighted Automata \n #249: Multi-way Interacting Regression via Factorization Machines \n Mikhail Yurochkin,  XuanLong Nguyen,  Nikolaos Vasiloglou \n We propose a Bayesian regression method that accounts for multi-way interactions of arbitrary orders among the predictor variables. Our model makes use of a factorization mechanism for representing the regression coefficients of interactions among the predictors, while the interaction selection is guided by a prior distribution on random hypergraphs, a construction which generalizes the Finite Feature Model. We present a posterior inference algorithm based on Gibbs sampling, and establish posterior consistency of our regression model. Our method is evaluated with extensive experiments on simulated data and demonstrated to be able to identify meaningful interactions in applications in genetics and retail demand forecasting.\n Abstract ,  PDF \n #250: Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network \n Wengong Jin,  Connor W. Coley,  Regina Barzilay,  Tommi Jaakkola \n The prediction of organic reaction outcomes is a fundamental problem in computational chemistry. Since a reaction may involve hundreds of atoms, fully exploring the space of possible transformations is intractable. The current solution utilizes reaction templates to limit the space, but it suffers from coverage and efficiency issues. In this paper, we propose a template-free approach to efficiently explore the space of product molecules by first pinpointing the reaction center -- the set of nodes and edges where graph edits occur. Since only a small number of atoms contribute to reaction center, we can directly enumerate candidate products. The generated candidates are scored by a Weisfeiler-Lehman Difference Network that models high-order interactions between changes occurring at nodes across the molecule. Our framework outperforms the top-performing template-based approach with a 10\\% margin, while running orders of magnitude faster. Finally, we demonstrate that the model accuracy rivals the performance of domain experts.\n Abstract ,  PDF \n #251: Practical Data-Dependent Metric Compression with Provable Guarantees \n #252: REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models \n George Tucker,  Andriy Mnih,  Chris J. Maddison,  Dieterich Lawson,  Jascha Sohl-Dickstein \n Learning in models with discrete latent variables is challenging due to high variance gradient estimators. Generally, approaches have relied on control variates to reduce the variance of the REINFORCE estimator. Recent work (Jang et al. 2016; Maddison et al. 2016) has taken a different approach, introducing a continuous relaxation of discrete variables to produce low-variance, but biased, gradient estimates. In this work, we combine the two approaches through a novel control variate that produces low-variance, \\emph{unbiased} gradient estimates. Then, we introduce a novel continuous relaxation and show that the tightness of the relaxation can be adapted online, removing it as a hyperparameter. We show state-of-the-art variance reduction on several benchmark generative modeling tasks, generally leading to faster convergence to a better final log likelihood.\n Abstract ,  PDF \n #253: Nonlinear random matrix theory for deep learning \n #254: Parallel Streaming Wasserstein Barycenters \n Matthew Staib,  Sebastian Claici,  Justin Solomon,  Stefanie Jegelka \n Efficiently aggregating data from different sources is a challenging problem, particularly when samples from each source are distributed differently. These differences can be inherent to the inference task or present for other reasons: sensors in a sensor network may be placed far apart, affecting their individual measurements. Conversely, it is computationally advantageous to split Bayesian inference tasks across subsets of data, but data need not be identically distributed across subsets. One principled way to fuse probability distributions is via the lens of optimal transport: the Wasserstein barycenter is a single distribution that summarizes a collection of input measures while respecting their geometry. However, computing the barycenter scales poorly and requires discretization of all input distributions and the barycenter itself. Improving on this situation, we present a scalable, communication-efficient, parallel algorithm for computing the Wasserstein barycenter of arbitrary distributions. Our algorithm can operate directly on continuous input distributions and is optimized for streaming data. Our method is even robust to nonstationary input distributions and produces a barycenter estimate that tracks the input measures over time. The algorithm is semi-discrete, needing to discretize only the barycenter estimate. To the best of our knowledge, we also provide the first bounds on the quality of the approximate barycenter as the discretization becomes finer. Finally, we demonstrate the practical effectiveness of our method, both in tracking moving distributions on a sphere, as well as in a large-scale Bayesian inference task.\n Abstract ,  PDF \n #255: ELF: An Extensive, Lightweight and Flexible Research Platform for Real-time Strategy Games \n Yuandong Tian,  Qucheng Gong,  Wenling Shang,  Yuxin Wu,  Larry Zitnick \n In this paper, we propose ELF, an Extensive, Lightweight and Flexible platform for fundamental reinforcement learning research. Using ELF, we implement a highly customizable real-time strategy (RTS) engine with three game environments (Mini-RTS, Capture the Flag and Tower Defense). Mini-RTS, as a miniature version of StarCraft, captures key game dynamics and runs at 40K frame-per-second (FPS) per core on a Macbook Pro notebook. When coupled with modern reinforcement learning methods, the system can train a full-game bot against built-in AIs end-to-end in one day with 6 CPUs and 1 GPU. In addition, our platform is flexible in terms of environment-agent communication topologies, choices of RL methods, changes in game parameters, and can host existing C/C++-based game environments like Arcade Learning Environment. Using ELF, we thoroughly explore training parameters and show that a network with Leaky ReLU and Batch Normalization coupled with long-horizon training and progressive curriculum beats the rule-based built-in AI more than $70\\%$ of the time in the full game of Mini-RTS. Strong performance is also achieved on the other two games. In game replays, we show our agents learn interesting strategies. ELF, along with its RL platform, will be open-sourced.\n Abstract ,  PDF \n #256: Dual Discriminator Generative Adversarial Nets \n Tu Dinh Nguyen,  Trung Le,  Hung Vu,  Dinh Phung \n We propose in this paper a novel approach to tackle the problem of mode collapse encountered in generative adversarial network (GAN). Our idea is intuitive but proven to be very effective, especially in addressing some key limitations of GAN. In essence, it combines the Kullback-Leibler (KL) and reverse KL divergences into a unified objective function, thus it exploits the complementary statistical properties from these divergences to effectively diversify the estimated density in capturing multi-modes. We term our method dual discriminator generative adversarial nets (D2GAN) which, unlike GAN, has two discriminators; and together with a generator, it also has the analogy of a minimax game, wherein a discriminator rewards high scores for samples from data distribution whilst another discriminator, conversely, favoring data from the generator, and the generator produces data to fool both two discriminators. We develop theoretical analysis to show that, given the maximal discriminators, optimizing the generator of D2GAN reduces to minimizing both KL and reverse KL divergences between data distribution and the distribution induced from the data generated by the generator, hence effectively avoiding the mode collapsing problem. We conduct extensive experiments on synthetic and real-world large-scale datasets (MNIST, CIFAR-10, STL-10, ImageNet), where we have made our best effort to compare our D2GAN with the latest state-of-the-art GAN\'s variants in comprehensive qualitative and quantitative evaluations. The experimental results demonstrate the competitive and superior performance of our approach in generating good quality and diverse samples over baselines, and the capability of our method to scale up to ImageNet database.\n Abstract ,  PDF \n #257: Dynamic Revenue Sharing \n #258: Decomposition-Invariant Conditional Gradient for General Polytopes with Line Search \n #259: Multi-agent Predictive Modeling with Attentional CommNets \n #260: An Empirical Bayes Approach to Optimizing Machine Learning Algorithms \n #261: Differentially Private Empirical Risk Minimization Revisited: Faster and More General \n Raef Bassily,  Adam Smith,  Abhradeep Thakurta \n In this paper, we initiate a systematic investigation of differentially private algorithms for convex empirical risk minimization. Various instantiations of this problem have been studied before. We provide new algorithms and matching lower bounds for private ERM assuming only that each data point\'s contribution to the loss function is Lipschitz bounded and that the domain of optimization is bounded. We provide a separate set of algorithms and matching lower bounds for the setting in which the loss functions are known to also be strongly convex. Our algorithms run in polynomial time, and in some cases even match the optimal non-private running time (as measured by oracle complexity). We give separate algorithms (and lower bounds) for $(\\epsilon,0)$- and $(\\epsilon,\\delta)$-differential privacy; perhaps surprisingly, the techniques used for designing optimal algorithms in the two cases are completely different. Our lower bounds apply even to very simple, smooth function families, such as linear and quadratic functions. This implies that algorithms from previous work can be used to obtain optimal error rates, under the additional assumption that the contributions of each data point to the loss function is smooth. We show that simple approaches to smoothing arbitrary loss functions (in order to apply previous techniques) do not yield optimal error rates. In particular, optimal algorithms were not previously known for problems such as training support vector machines and the high-dimensional median.\n Abstract ,  PDF \n #262: Variational Inference via $\\chi$ Upper Bound Minimization \n #263: On Quadratic Convergence of DC Proximal Newton Algorithm in Nonconvex Sparse Learning \n Xingguo Li,  Lin F. Yang,  Jason Ge,  Jarvis Haupt,  Tong Zhang,  Tuo Zhao \n We propose a DC proximal Newton algorithm for solving nonconvex regularized sparse learning problems in high dimensions. Our proposed algorithm integrates the proximal Newton algorithm with multi-stage convex relaxation based on difference of convex (DC) programming, and enjoys both strong computational and statistical guarantees. Specifically, by leveraging a sophisticated characterization of sparse modeling structures/assumptions (i.e., local restricted strong convexity and Hessian smoothness), we prove that within each stage of convex relaxation, our proposed algorithm achieves (local) quadratic convergence, and eventually obtains a sparse approximate local optimum with optimal statistical properties after only a few convex relaxations. Numerical experiments are provided to support our theory.\n Abstract ,  PDF \n #264: #Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning \n #265: An Empirical Study on The Properties of Random Bases for Kernel Methods \n #266: Bridging the Gap Between Value and Policy Based Reinforcement Learning \n Ofir Nachum,  Mohammad Norouzi,  Kelvin Xu,  Dale Schuurmans \n We establish a new connection between value and policy based reinforcement learning (RL) based on a relationship between softmax temporal value consistency and policy optimality under entropy regularization. Specifically, we show that softmax consistent action values satisfy a strong consistency property with optimal entropy regularized policy probabilities along any action sequence, regardless of provenance. From this observation, we develop a new RL algorithm, Path Consistency Learning (PCL), that minimizes inconsistency measured along multi-step action sequences extracted from both on- and off-policy traces. We subsequently deepen the relationship by showing how a single model can be used to represent both a policy and its softmax action values. Beyond eliminating the need for a separate critic, the unification demonstrates how policy gradients can be stabilized via self-bootstrapping from both on- and off-policy data. An experimental evaluation demonstrates that both algorithms can significantly outperform strong actor-critic and Q-learning baselines across several benchmark tasks.\n Abstract ,  PDF \n #267: Premise Selection for Theorem Proving by Deep Graph Embedding \n Mingzhe Wang,  Yihe Tang,  Jian Wang,  Jia Deng \n We propose a deep learning-based approach to the problem of premise selection: selecting mathematical statements relevant for proving a given conjecture. We represent a higher-order logic formula as a graph that is invariant to variable renaming but still fully preserves syntactic and semantic information. We then embed the graph into a vector via a novel embedding method that preserves the information of edge ordering. Our approach achieves state-of-the-art results on the HolStep dataset, improving the classification accuracy from 83% to 90.3%. \n #268: A Bayesian Data Augmentation Approach for Learning Deep Models \n #269: Principles of Riemannian Geometry  in Neural Networks \n #270: Cold-Start Reinforcement Learning with Softmax Policy Gradients \n Nan Ding,  Radu Soricut \n Policy-gradient approaches to reinforcement learning have two common and undesirable overhead procedures, namely warm-start training and sample variance reduction. In this paper, we describe a reinforcement learning method based on a softmax policy that requires neither of these procedures. Our method combines the advantages of policy-gradient methods with the efficiency and simplicity of maximum-likelihood approaches. We apply this new cold-start reinforcement learning method in training sequence generation models for structured output prediction problems. Empirical evidence validates this method on automatic summarization and image captioning tasks.\n Abstract ,  PDF \n #271: Online Dynamic Programming \n Holakou Rahmanian,  S.V.N. Vishwanathan,  Manfred K. Warmuth \n We consider the problem of repeatedly solving a variant of the same dynamic programming problem in successive trials. An instance of the type of problems we consider is to find the optimal binary search tree. At the beginning of each trial, the learner probabilistically chooses a tree with the n keys at the internal nodes and the n + 1 gaps between keys at the leaves. It is then told the frequencies of the keys and gaps and is charged by the average search cost for the chosen tree. The problem is online because the frequencies can change between trials. The goal is to develop algorithms with the property that their total average search cost (loss) in all trials is close to the total loss of the best tree chosen in hind sight for all trials. The challenge, of course, is that the algorithm has to deal with exponential number of trees. We develop a methodology for tackling such problems for a wide class of dynamic programming algorithms. Our framework allows us to extend online learning algorithms like Hedge and Component Hedge to a significantly wider class of combinatorial objects than was possible before.\n Abstract ,  PDF \n #272: Alternating Estimation for Structured High-Dimensional Multi-Response Models \n Sheng Chen,  Arindam Banerjee \n We consider learning high-dimensional multi-response linear models with structured parameters. By exploiting the noise correlations among responses, we propose an alternating estimation (AltEst) procedure to estimate the model parameters based on the generalized Dantzig selector. Under suitable sample size and resampling assumptions, we show that the error of the estimates generated by AltEst, with high probability, converges linearly to certain minimum achievable level, which can be tersely expressed by a few geometric measures, such as Gaussian width of sets related to the parameter structure. To the best of our knowledge, this is the first non-asymptotic statistical guarantee for such AltEst-type algorithm applied to estimation problem with general structures.\n Abstract ,  PDF \n #273: Convolutional Gaussian Processes \n Mark van der Wilk,  Carl Edward Rasmussen,  James Hensman \n We present a practical way of introducing convolutional structure into Gaussian processes, making them more suited to high-dimensional inputs like images. The main contribution of our work is the construction of an inter-domain inducing point approximation that is well-tailored to the convolutional kernel. This allows us to gain the generalisation benefit of a convolutional kernel, together with fast but accurate posterior inference. We investigate several variations of the convolutional kernel, and apply it to MNIST and CIFAR-10, which have both been known to be challenging for Gaussian processes. We also show how the marginal likelihood can be used to find an optimal weighting between convolutional and RBF kernels to further improve performance. We hope that this illustration of the usefulness of a marginal likelihood will help automate discovering architectures in larger models.\n Abstract ,  PDF \n #274: Estimation of the covariance structure of heavy-tailed distributions \n Stanislav Minsker,  Xiaohan Wei \n We propose and analyze a new estimator of the covariance matrix that admits strong theoretical guarantees under weak assumptions on the underlying distribution, such as existence of moments of only low order. While estimation of covariance matrices corresponding to sub-Gaussian distributions is well-understood, much less in known in the case of heavy-tailed data. As K. Balasubramanian and M. Yuan write, "data from real-world experiments oftentimes tend to be corrupted with outliers and/or exhibit heavy tails. In such cases, it is not clear that those covariance matrix estimators .. remain optimal" and "..what are the other possible strategies to deal with heavy tailed distributions warrant further studies." We make a step towards answering this question and prove tight deviation inequalities for the proposed estimator that depend only on the parameters controlling the "intrinsic dimension" associated to the covariance matrix (as opposed to the dimension of the ambient space); in particular, our results are applicable in the case of high-dimensional observations.\n Abstract ,  PDF \n #275: Mean Field Residual Networks: On the Edge of Chaos \n #276: Decomposable Submodular Function Minimization: Discrete and Continuous \n Alina Ene,  Huy L. Nguyen,  László A. Végh \n This paper investigates connections between discrete and continuous approaches for decomposable submodular function minimization. We provide improved running time estimates for the state-of-the-art continuous algorithms for the problem using combinatorial arguments. We also provide a systematic experimental comparison of the two types of methods, based on a clear distinction between level-0 and level-1 algorithms.\n Abstract ,  PDF \n #277: Gauging Variational Inference \n #278: Deep Recurrent Neural Network-Based Identification of Precursor microRNAs \n #279: Robust Estimation of Neural Signals in Calcium Imaging \n #280: State Aware Imitation Learning \n #281: Beyond Parity: Fairness Objectives for Collaborative Filtering \n Sirui Yao,  Bert Huang \n We study fairness in collaborative-filtering recommender systems, which are sensitive to discrimination that exists in historical data. Biased data can lead collaborative-filtering methods to make unfair predictions for users from minority groups. We identify the insufficiency of existing fairness metrics and propose four new metrics that address different forms of unfairness. These fairness metrics can be optimized by adding fairness terms to the learning objective. Experiments on synthetic and real data show that our new metrics can better measure fairness than the baseline, and that the fairness objectives effectively help reduce unfairness.\n Abstract ,  PDF \n #282: A PAC-Bayesian Analysis of Randomized Learning with Application to Stochastic Gradient Descent \n Ben London \n We analyze the generalization error of randomized learning algorithms -- focusing on stochastic gradient descent (SGD) -- using a novel combination of PAC-Bayes and algorithmic stability. Importantly, our risk bounds hold for all posterior distributions on the algorithm\'s random hyperparameters, including distributions that depend on the training data. This inspires an adaptive sampling algorithm for SGD that optimizes the posterior at runtime. We analyze this algorithm in the context of our risk bounds and evaluate it empirically on a benchmark dataset.\n Abstract ,  PDF \n #283: Fully Decentralized Policies for Multi-Agent Systems: An Information Theoretic Approach \n Roel Dobbe,  David Fridovich-Keil,  Claire Tomlin \n Learning cooperative policies for multi-agent systems is often challenged by partial observability and a lack of coordination. In some settings, the structure of a problem allows a distributed solution with limited communication. Here, we consider a scenario where no communication is available, and instead we learn local policies for all agents that collectively mimic the solution to a centralized multi-agent static optimization problem. Our main contribution is an information theoretic framework based on rate distortion theory which facilitates analysis of how well the resulting fully decentralized policies are able to reconstruct the optimal solution. Moreover, this framework provides a natural extension that addresses which nodes an agent should communicate with to improve the performance of its individual policy.\n Abstract ,  PDF \n #284: Model-Powered Conditional Independence Test \n Rajat Sen,  Ananda Theertha Suresh,  Karthikeyan Shanmugam,  Alexandros G. Dimakis,  Sanjay Shakkottai \n We consider the problem of non-parametric Conditional Independence testing (CI testing) for continuous random variables. Given i.i.d samples from the joint distribution $f(x,y,z)$ of continuous random vectors $X,Y$ and $Z,$ we determine whether $X \\perp Y | Z$. We approach this by converting the conditional independence test into a classification problem. This allows us to harness very powerful classifiers like gradient-boosted trees and deep neural networks. These models can handle complex probability distributions and allow us to perform significantly better compared to the prior state of the art, for high-dimensional CI testing. The main technical challenge in the classification problem is the need for samples from the conditional product distribution $f^{CI}(x,y,z) = f(x|z)f(y|z)f(z)$ -- the joint distribution if and only if $X \\perp Y | Z.$ -- when given access only to i.i.d. samples from the true joint distribution $f(x,y,z)$. To tackle this problem we propose a novel nearest neighbor bootstrap procedure and theoretically show that our generated samples are indeed close to $f^{CI}$ in terms of total variational distance. We then develop theoretical results regarding the generalization bounds for classification for our problem, which translate into error bounds for CI testing. We provide a novel analysis of Rademacher type classification bounds in the presence of non-i.i.d near-independent samples. We empirically validate the performance of our algorithm on simulated and real datasets and show performance gains over previous methods.\n Abstract ,  PDF \n #285: Deep Voice 2: Multi-Speaker Neural Text-to-Speech \n Sercan Arik,  Gregory Diamos,  Andrew Gibiansky,  John Miller,  Kainan Peng,  Wei Ping,  Jonathan Raiman,  Yanqi Zhou \n We introduce a technique for augmenting neural text-to-speech (TTS) with lowdimensional trainable speaker embeddings to generate different voices from a single model. As a starting point, we show improvements over the two state-ofthe-art approaches for single-speaker neural TTS: Deep Voice 1 and Tacotron. We introduce Deep Voice 2, which is based on a similar pipeline with Deep Voice 1, but constructed with higher performance building blocks and demonstrates a significant audio quality improvement over Deep Voice 1. We improve Tacotron by introducing a post-processing neural vocoder, and demonstrate a significant audio quality improvement. We then demonstrate our technique for multi-speaker speech synthesis for both Deep Voice 2 and Tacotron on two multi-speaker TTS datasets. We show that a single neural TTS system can learn hundreds of unique voices from less than half an hour of data per speaker, while achieving high audio quality synthesis and preserving the speaker identities almost perfectly.\n Abstract ,  PDF \n #286: Variance-based Regularization with Convex Objectives \n #287: Deep Lattice Networks and Partial Monotonic Functions \n Seungil You,  David Ding,  Kevin Canini,  Jan Pfeifer,  Maya Gupta \n We propose learning deep models that are monotonic with respect to a user-specified set of inputs by alternating layers of linear embeddings, ensembles of lattices, and calibrators (piecewise linear functions), with appropriate constraints for monotonicity, and jointly training the resulting network. We implement the layers and projections with new computational graph nodes in TensorFlow and use the ADAM optimizer and batched stochastic gradients. Experiments on benchmark and real-world datasets show that six-layer monotonic deep lattice networks achieve state-of-the art performance for classification and regression with monotonicity guarantees.\n Abstract ,  PDF \n #288: Continual Learning with Deep Generative Replay \n Hanul Shin,  Jung Kwon Lee,  Jaehong Kim,  Jiwon Kim \n Attempts to train a comprehensive artificial intelligence capable of solving multiple tasks have been impeded by a chronic problem called catastrophic forgetting. Although simply replaying all previous data alleviates the problem, it requires large memory and even worse, often infeasible in real world applications where the access to past data is limited. Inspired by the generative nature of hippocampus as a short-term memory system in primate brain, we propose the Deep Generative Replay, a novel framework with a cooperative dual model architecture consisting of a deep generative model ("generator") and a task solving model ("solver"). With only these two models, training data for previous tasks can easily be sampled and interleaved with those for a new task. We test our methods in several sequential learning settings involving image classification tasks.\n Abstract ,  PDF \n #289: AIDE: An algorithm for measuring the accuracy of probabilistic inference algorithms \n Marco F. Cusumano-Towner,  Vikash K. Mansinghka \n Approximate probabilistic inference algorithms are central to many fields. Examples include sequential Monte Carlo inference in robotics, variational inference in machine learning, and Markov chain Monte Carlo inference in statistics. A key problem faced by practitioners is measuring the accuracy of an approximate inference algorithm on a specific dataset. This paper introduces the auxiliary inference divergence estimator (AIDE), an algorithm for measuring the accuracy of approximate inference algorithms. AIDE is based on the observation that inference algorithms can be treated as probabilistic models and the random variables used within the inference algorithm can be viewed as auxiliary variables. This view leads to a new estimator for the symmetric KL divergence between the output distributions of two inference algorithms. The paper illustrates application of AIDE to algorithms for inference in regression, hidden Markov, and Dirichlet process mixture models. The experiments show that AIDE captures the qualitative behavior of a broad class of inference algorithms and can detect failure modes of inference algorithms that are missed by standard heuristics.\n Abstract ,  PDF \n #290: Learning Causal Structures Using Regression Invariance \n AmirEmad Ghassami,  Saber Salehkaleybar,  Negar Kiyavash,  Kun Zhang \n We study causal inference in a multi-environment setting, in which the functional relations for producing the variables from their direct causes remain the same across environments, while the distribution of exogenous noises may vary. We introduce the idea of using the invariance of the functional relations of the variables to their causes across a set of environments. We define a notion of completeness for a causal inference algorithm in this setting and prove the existence of such algorithm by proposing the baseline algorithm. Additionally, we present an alternate algorithm that has significantly improved computational and sample complexity compared to the baseline algorithm. The experiment results show that the proposed algorithm outperforms the other existing algorithms.\n Abstract ,  PDF \n #291: Online Influence Maximization under Independent Cascade Model with Semi-Bandit Feedback \n #292: Minimax Optimal Players for the Finite-Time 3-Expert Prediction Problem \n #293: Reinforcement Learning under Model Mismatch \n Aurko Roy,  Huan Xu,  Sebastian Pokutta \n We study reinforcement learning under model misspecification, where we do not have access to the true environment but only to a reasonably close approximation to it. We address this problem by extending the framework of robust MDPs to the model-free Reinforcement Learning setting, where we do not have access to the model parameters, but can only sample states from it. We define robust versions of Q-learning, SARSA, and TD-learning and prove convergence to an approximately optimal robust policy and approximate value function respectively. We scale up the robust algorithms to large MDPs via function approximation and prove convergence under two different settings. We prove convergence of robust approximate policy iteration and robust approximate value iteration for linear architectures (under mild assumptions). We also define a robust loss function, the mean squared robust projected Bellman error and give stochastic gradient descent algorithms that are guaranteed to converge to a local minimum.\n Abstract ,  PDF \n #294: Hierarchical Attentive Recurrent Tracking \n Adam R. Kosiorek,  Alex Bewley,  Ingmar Posner \n Class-agnostic object tracking is particularly difficult in cluttered environments as target specific discriminative models cannot be learned a priori. Inspired by how the human visual cortex employs spatial attention and separate "where" and "what" processing pathways to actively suppress irrelevant visual features, this work develops a hierarchical attentive recurrent model for single object tracking in videos. The first layer of attention discards the majority of background by selecting a region containing the object of interest, while the subsequent layers tune in on visual features particular to the tracked object. This framework is fully differentiable and can be trained in a purely data driven fashion by gradient methods. To improve training convergence, we augment the loss function with terms for a number of auxiliary tasks relevant for tracking. Evaluation of the proposed model is performed on two datasets: pedestrian tracking on the KTH activity recognition dataset and the more difficult KITTI object tracking dataset.\n Abstract ,  PDF \n #295: Tomography of the London Underground: a Scalable Model for Origin-Destination Data \n #296: Rotting Bandits \n Nir Levine,  Koby Crammer,  Shie Mannor \n The Multi-Armed Bandits (MAB) framework highlights the tension between acquiring new knowledge (Exploration) and leveraging available knowledge (Exploitation). In the classical MAB problem, a decision maker must choose an arm at each time step, upon which she receives a reward. The decision maker\'s objective is to maximize her cumulative expected reward over the time horizon. The MAB problem has been studied extensively, specifically under the assumption of the arms\' rewards distributions being stationary, or quasi-stationary, over time. We consider a variant of the MAB framework, which we termed Rotting Bandits, where each arm\'s expected reward decays as a function of the number of times it has been pulled. We are motivated by many real-world scenarios such as online advertising, content recommendation, crowdsourcing, and more. We present algorithms, accompanied by simulations, and derive theoretical guarantees.\n Abstract ,  PDF \n #297: Unbiased estimates for linear regression via volume sampling \n Michal Derezinski,  Manfred K. Warmuth \n Given a full rank matrix $X$ with more columns than rows consider the task of estimating the pseudo inverse $X^+$ based on the pseudo inverse of a sampled subset of columns (of size at least the number of rows). We show that this is possible if the subset of columns is chosen proportional to the squared volume spanned by the rows of the chosen submatrix (ie, volume sampling). The resulting estimator is unbiased and surprisingly the covariance of the estimator also has a closed form: It equals a specific factor times $X^+X^{+\\top}$. Pseudo inverse plays an important part in solving the linear least squares problem, where we try to predict a label for each column of $X$. We assume labels are expensive and we are only given the labels for the small subset of columns we sample from $X$. Using our methods we show that the weight vector of the solution for the sub problem is an unbiased estimator of the optimal solution for the whole problem based on all column labels. We believe that these new formulas establish a fundamental connection between linear least squares and volume sampling. We use our methods to obtain an algorithm for volume sampling that is faster than state-of-the-art and for obtaining bounds for the total loss of the estimated least-squares solution on all labeled columns.\n Abstract ,  PDF \n #298: An Applied Algorithmic Foundation for Hierarchical Clustering \n #299: Adaptive Accelerated Gradient Converging Method under H\\"{o}lderian Error Bound Condition \n #300: Stein Variational Gradient Descent as Gradient Flow \n Qiang Liu \n Stein variational gradient descent (SVGD) is a deterministic sampling algorithm that iteratively transports a set of particles to approximate given distributions, based on an efficient gradient-based update that guarantees to optimally decrease the KL divergence within a function space. This paper develops the first theoretical analysis on SVGD, discussing its weak convergence properties and showing that its asymptotic behavior is captured by a gradient flow of the KL divergence functional under a new metric structure induced by Stein operator. We also provide a number of results on Stein operator and Stein\'s identity using the notion of weak derivative, including a new proof of the distinguishability of Stein discrepancy under weak conditions.\n Abstract ,  PDF \n #301: Partial Hard Thresholding: A Towards Unified Analysis of Support Recovery \n #302: Shallow Updates for Deep Reinforcement Learning \n Nir Levine,  Tom Zahavy,  Daniel J. Mankowitz,  Aviv Tamar,  Shie Mannor \n Deep reinforcement learning (DRL) methods such as the Deep Q-Network (DQN) have achieved state-of-the-art results in a variety of challenging, high-dimensional domains. This success is mainly attributed to the power of deep neural networks to learn rich domain representations for approximating the value function or policy. Batch reinforcement learning methods with linear representations, on the other hand, are more stable and require less hyper parameter tuning. Yet, substantial feature engineering is necessary to achieve good results. In this work we propose a hybrid approach -- the Least Squares Deep Q-Network (LS-DQN), which combines rich feature representations learned by a DRL algorithm with the stability of a linear least squares method. We do this by periodically re-training the last hidden layer of a DRL network with a batch least squares update. Key to our approach is a Bayesian regularization term for the least squares update, which prevents over-fitting to the more recent data. We tested LS-DQN on five Atari games and demonstrate significant improvement over vanilla DQN and Double-DQN. We also investigated the reasons for the superior performance of our method. Interestingly, we found that the performance improvement can be attributed to the large batch size used by the LS method when optimizing the last layer.\n Abstract ,  PDF \n #303: A Highly Efficient Gradient Boosting Decision Tree \n #304: Adversarial Ranking for Language Generation \n Kevin Lin,  Dianqi Li,  Xiaodong He,  Zhengyou Zhang,  Ming-Ting Sun \n Generative adversarial networks (GANs) have great successes on synthesizing data. However, the existing GANs restrict the discriminator to be a binary classifier, and thus limit their learning capacity for tasks that need to synthesize output with rich structures such as natural language descriptions. In this paper, we propose a novel generative adversarial network, RankGAN, for generating high-quality language descriptions. Rather than train the discriminator to learn and assign absolute binary predicate for individual data sample, the proposed RankGAN is able to analyze and rank a collection of human-written and machine-written sentences by giving a reference group. By viewing a set of data samples collectively and evaluating their quality through relative ranking scores, the discriminator is able to make better assessment which in turn helps to learn a better generator. The proposed RankGAN is optimized through the policy gradient technique. Experimental results on multiple public datasets clearly demonstrate the effectiveness of the proposed approach.\n Abstract ,  PDF \n #305: Regret Minimization in MDPs with Options without Prior Knowledge \n #306: Net-Trim: Convex Pruning of Deep Neural Networks with Performance Guarantee \n #307: Graph Matching via Multiplicative Update Algorithm \n #308: Dynamic Importance Sampling for Anytime Bounds of the Partition Function \n #309: Is the Bellman residual a bad proxy? \n #310: Generalization Properties of Learning with Random Features \n Alessandro Rudi,  Lorenzo Rosasco \n We study the generalization properties of ridge regression with random features in the statistical learning framework. We show for the first time that $O(1/\\sqrt{n})$ learning bounds can be achieved with only $O(\\sqrt{n}\\log n)$ random features rather than $O({n})$ as suggested by previous results. Further, we prove faster learning rates and show that they might require more random features, unless they are sampled according to a possibly problem dependent distribution. Our results shed light on the statistical computational trade-offs in large scale kernelized learning, showing the potential effectiveness of random features in reducing the computational complexity while keeping optimal generalization properties.\n Abstract ,  PDF \n #311: Differentially private Bayesian learning on distributed data \n #312: Learning to Compose Domain-Specific Transformations for Data Augmentation \n Alexander J. Ratner,  Henry R. Ehrenberg,  Zeshan Hussain,  Jared Dunnmon,  Christopher Ré \n Data augmentation is a ubiquitous technique for increasing the size of labeled training sets by leveraging task-specific data transformations that preserve class labels. While it is often easy for domain experts to specify individual transformations, constructing and tuning the more sophisticated compositions typically needed to achieve state-of-the-art results is a time-consuming manual task in practice. We propose a method for automating this process by learning a generative sequence model over user-specified transformation functions using a generative adversarial approach. Our method can make use of arbitrary, non-deterministic transformation functions, is robust to misspecified user input, and is trained on unlabeled data. The learned transformation model can then be used to perform data augmentation for any end discriminative model. In our experiments, we show the efficacy of our approach on both image and text datasets, achieving improvements of 4.0 accuracy points on CIFAR-10, 1.4 F1 points on the ACE relation extraction task, and 3.4 accuracy points when using domain-specific transformation operations on a medical imaging dataset as compared to standard heuristic augmentation approaches.\n Abstract ,  PDF \n #313: Wasserstein Learning of Deep Generative Point Process Models \n Shuai Xiao,  Mehrdad Farajtabar,  Xiaojing Ye,  Junchi Yan,  Le Song,  Hongyuan Zha \n Point processes are becoming very popular in modeling asynchronous sequential data due to their sound mathematical foundation and strength in modeling a variety of real-world phenomena. Currently, they are often characterized via intensity function which limits model\'s expressiveness due to unrealistic assumptions on its parametric form used in practice. Furthermore, they are learned via maximum likelihood approach which is prone to failure in multi-modal distributions of sequences. In this paper, we propose an intensity-free approach for point processes modeling that transforms nuisance processes to a target one. Furthermore, we train the model using a likelihood-free leveraging Wasserstein distance between point processes. Experiments on various synthetic and real-world data substantiate the superiority of the proposed point process model over conventional ones.\n Abstract ,  PDF \n #314: Ensemble Sampling \n Xiuyuan Lu,  Benjamin Van Roy \n Thompson sampling has emerged as an effective heuristic for a broad range of online decision problems. In its basic form, the algorithm requires computing and sampling from a posterior distribution over models, which is tractable only for simple special cases. This paper develops ensemble sampling, which aims to approximate Thompson sampling while maintaining tractability even in the face of complex models such as neural networks. Ensemble sampling dramatically expands on the range of applications for which Thompson sampling is viable. We establish a theoretical basis that supports the approach and present computational results that offer further insight.\n Abstract ,  PDF \n #315: Language modeling with recurrent highway hypernetworks \n #316: Searching in the Dark: Practical SVRG Methods under Error Bound Conditions with  Guarantee \n #317: Bayesian Compression for Deep Learning \n Christos Louizos,  Karen Ullrich,  Max Welling \n Compression and computational efficiency in deep learning have become a problem of great significance. In this work, we argue that the most principled and effective way to attack this problem is by taking a Bayesian point of view, where through sparsity inducing priors we prune large parts of the network. We introduce two novelties in this paper: 1) we use hierarchical priors to prune nodes instead of individual weights, and 2) we use the posterior uncertainties to determine the optimal fixed point precision to encode the weights. Both factors significantly contribute to achieving the state of the art in terms of compression rates, while still staying competitive with methods designed to optimize for speed or energy efficiency.\n Abstract ,  PDF \n #318: Streaming Sparse Gaussian Process Approximations \n #319: VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning \n Akash Srivastava,  Lazar Valkov,  Chris Russell,  Michael Gutmann,  Charles Sutton \n Deep generative models provide powerful tools for distributions over complicated manifolds, such as those of natural images. But many of these methods, including generative adversarial networks (GANs), can be difficult to train, in part because they are prone to mode collapse, which means that they characterize only a few modes of the true distribution. To address this, we introduce VEEGAN, which features a reconstructor network, reversing the action of the generator by mapping from data to noise. Our training objective retains the original asymptotic consistency guarantee of GANs, and can be interpreted as a novel autoencoder loss over the noise. In sharp contrast to a traditional autoencoder over data points, VEEGAN does not require specifying a loss function over the data, but rather only over the representations, which are standard normal by assumption. On an extensive set of synthetic and real world image datasets, VEEGAN indeed resists mode collapsing to a far greater extent than other recent GAN variants, and produces more realistic samples.\n Abstract ,  PDF \n #320: Sparse k-Means Embedding \n #321: Utile Context Tree Weighting \n #322: A Regularized Framework for Sparse and Structured Neural Attention \n Vlad Niculae,  Mathieu Blondel \n Modern neural networks are often augmented with an attention mechanism, which tells the network where to focus within the input. We propose in this paper a new framework for sparse and structured attention, building upon a max operator regularized with a strongly convex function. We show that this operator is differentiable and that its gradient defines a mapping from real values to probabilities, suitable as an attention mechanism. Our framework includes softmax and a slight generalization of the recently-proposed sparsemax as special cases. However, we also show how our framework can incorporate modern structured penalties, resulting in new attention mechanisms that focus on entire segments or groups of an input, encouraging parsimony and interpretability. We derive efficient algorithms to compute the forward and backward passes of these attention mechanisms, enabling their use in a neural network trained with backpropagation. To showcase their potential as a drop-in replacement for existing attention mechanisms, we evaluate them on three large-scale tasks: textual entailment, machine translation, and sentence summarization. Our attention mechanisms improve interpretability without sacrificing performance; notably, on textual entailment and summarization, we outperform the existing attention mechanisms based on softmax and sparsemax.\n Abstract ,  PDF \n #323: Multi-output Polynomial Networks and Factorization Machines \n Mathieu Blondel,  Vlad Niculae,  Takuma Otsuka,  Naonori Ueda \n Factorization machines and polynomial networks are supervised polynomial models based on an efficient low-rank decomposition. We extend these models to the multi-output setting, i.e., for learning vector-valued functions, with application to multi-class or multi-task problems. We cast this as the problem of learning a 3-way tensor whose slices share a common decomposition and propose a convex formulation of that problem. We then develop an efficient conditional gradient algorithm and prove its global convergence, despite the fact that it involves a non-convex hidden unit selection step. On classification tasks, we show that our algorithm achieves excellent accuracy with much sparser models than existing methods. On recommendation system tasks, we show how to combine our algorithm with a reduction from ordinal regression to multi-output classification and show that the resulting algorithm outperforms existing baselines in terms of ranking accuracy.\n Abstract ,  PDF \n #324: Clustering Billions of Reads for DNA Data Storage \n #325: Multi-Objective Non-parametric Sequential Prediction \n Guy Uziel,  Ran El-Yaniv \n Online-learning research has mainly been focusing on minimizing one objective function. In many real-world applications, however, several objective functions have to be considered simultaneously. Recently, an algorithm for dealing with several objective functions in the i.i.d. case has been presented. In this paper, we extend the multi-objective framework to the case of stationary and ergodic processes, thus allowing dependencies among observations. We first identify an asymptomatic lower bound for any prediction strategy and then present an algorithm whose predictions achieve the optimal solution while fulfilling any continuous and convex constraining criterion.\n Abstract ,  PDF \n #326: A Universal Analysis of Large-Scale Regularized Least Squares Solutions \n #327: Deep Sets \n Manzil Zaheer,  Satwik Kottur,  Siamak Ravanbakhsh,  Barnabas Poczos,  Ruslan Salakhutdinov,  Alexander Smola \n In this paper, we study the problem of designing objective functions for machine learning problems defined on finite \\emph{sets}. In contrast to traditional objective functions defined for machine learning problems operating on finite dimensional vectors, the new objective functions we propose are operating on finite sets and are invariant to permutations. Such problems are widespread, ranging from estimation of population statistics \\citep{poczos13aistats}, via anomaly detection in piezometer data of embankment dams \\citep{Jung15Exploration}, to cosmology \\citep{Ntampaka16Dynamical,Ravanbakhsh16ICML1}. Our main theorem characterizes the permutation invariant objective functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and image tagging.\n Abstract ,  PDF \n #328: ExtremeWeather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events \n #329: Process-constrained batch Bayesian optimisation \n #330: Bayesian Inference of Individualized Treatment Effects using Multi-task Gaussian Processes \n Ahmed M. Alaa,  Mihaela van der Schaar \n Predicated on the increasing abundance of electronic health records, we investi- gate the problem of inferring individualized treatment effects using observational data. Stemming from the potential outcomes model, we propose a novel multi- task learning framework in which factual and counterfactual outcomes are mod- eled as the outputs of a function in a vector-valued reproducing kernel Hilbert space (vvRKHS). We develop a nonparametric Bayesian method for learning the treatment effects using a multi-task Gaussian process (GP) with a linear coregion- alization kernel as a prior over the vvRKHS. The Bayesian approach allows us to compute individualized measures of confidence in our estimates via pointwise credible intervals, which are crucial for realizing the full potential of precision medicine. The impact of selection bias is alleviated via a risk-based empirical Bayes method for adapting the multi-task GP prior, which jointly minimizes the empirical error in factual outcomes and the uncertainty in (unobserved) counter- factual outcomes. We conduct experiments on observational datasets for an inter- ventional social program applied to premature infants, and a left ventricular assist device applied to cardiac patients wait-listed for a heart transplant. In both experi- ments, we show that our method significantly outperforms the state-of-the-art.\n Abstract ,  PDF \n #331: Spherical convolutions and their application in molecular modelling \n #332: Efficient Optimization for Linear Dynamical Systems with Applications to Clustering and Sparse Coding \n #333: On Optimal Generalizability in Parametric Learning \n #334: Near Optimal Sketching of Low-Rank Tensor Regression \n Jarvis Haupt,  Xingguo Li,  David P. Woodruff \n We study the least squares regression problem \\begin{align } \\min_{\\Theta \\in \\mathcal{S}_{\\odot D,R}} \\|A\\Theta-b\\|_2, \\end{align } where $\\mathcal{S} {\\odot D,R}$ is the set of $\\Theta$ for which $\\Theta = \\sum {r=1}^{R} \\theta_1^{(r)} \\circ \\cdots \\circ \\theta_D^{(r)}$ for vectors $\\theta_d^{(r)} \\in \\mathbb{R}^{p_d}$ for all $r \\in [R]$ and $d \\in [D]$, and $\\circ$ denotes the outer product of vectors. That is, $\\Theta$ is a low-dimensional, low-rank tensor. This is motivated by the fact that the number of parameters in $\\Theta$ is only $R \\cdot \\sum_{d=1}^D p_d$, which is significantly smaller than the $\\prod_{d=1}^{D} p_d$ number of parameters in ordinary least squares regression. We consider the above CP decomposition model of tensors $\\Theta$, as well as the Tucker decomposition. For both models we show how to apply data dimensionality reduction techniques based on {\\it sparse} random projections $\\Phi \\in \\mathbb{R}^{m \\times n}$, with $m \\ll n$, to reduce the problem to a much smaller problem $\\min_{\\Theta} \\|\\Phi A \\Theta - \\Phi b\\|_2$, for which if $\\Theta\'$ is a near-optimum to the smaller problem, then it is also a near optimum to the original problem. We obtain significantly smaller dimension and sparsity in $\\Phi$ than is possible for ordinary least squares regression, and we also provide a number of numerical simulations supporting our theory.\n Abstract ,  PDF \n #335: Tractability in Structured Probability Spaces \n #336: Model-based Bayesian inference of neural activity and connectivity from all-optical interrogation of a neural circuit \n #337: Gaussian process based nonlinear latent structure discovery in multivariate spike train data \n #338: Neural system identification for large populations separating "what" and "where" \n #339: Certified Defenses for Data Poisoning Attacks \n Jacob Steinhardt,  Pang Wei Koh,  Percy Liang \n Machine learning systems trained on user-provided data are susceptible to data poisoning attacks, whereby malicious users inject false training data with the aim of corrupting the learned model. While recent work has proposed a number of attacks and defenses, little is understood about the worst-case loss of a defense in the face of a determined attacker. We address this by constructing approximate upper bounds on the loss across a broad family of attacks, for defenders that first perform outlier removal followed by empirical risk minimization. Our bound comes paired with a candidate attack that nearly realizes the bound, giving us a powerful tool for quickly assessing defenses on a given dataset. Empirically, we find that even under a simple defense, the MNIST-1-7 and Dogfish datasets are resilient to attack, while in contrast the IMDB sentiment dataset can be driven from 12% to 23% test error by adding only 3% poisoned data.\n Abstract ,  PDF \n #340: Eigen-Distortions of Hierarchical Representations \n Alexander Berardino,  Johannes Ballé,  Valero Laparra,  Eero P. Simoncelli \n We develop a method for comparing hierarchical image representations in terms of their ability to explain perceptual sensitivity in humans. Specifically, we utilize Fisher information to establish a model-derived prediction of sensitivity to local perturbations around a given natural image. For a given image, we compute the eigenvectors of the Fisher information matrix with largest and smallest eigenvalues, corresponding to the model-predicted most- and least-noticeable image distortions, respectively. For human subjects, we then measure the amount of each distortion that can be reliably detected when added to the image, and compare these thresholds to the predictions of the corresponding model. We use this method to test the ability of a variety of representations to mimic human perceptual sensitivity. We find that the early layers of VGG16, a deep neural network optimized for object recognition, provide a better match to human perception than later layers, and a better match than a 4-stage convolutional neural network (CNN) trained on a database of human ratings of distorted image quality. On the other hand, we find that simple models of early visual processing, incorporating one or more stages of local gain control, trained on the same database of distortion ratings, provide substantially better predictions of human sensitivity than both the CNN and all layers of VGG16. \n #341: Limitations on Variance-Reduction and Acceleration Schemes for Finite Sums Optimization \n Yossi Arjevani \n We study the conditions under which one is able to efficiently apply variance-reduction and acceleration schemes on finite sum optimization problems. First, we show that, perhaps surprisingly, the finite sum structure by itself, is not sufficient for obtaining a complexity bound of $\\tilde{\\cO}((n+L/\\mu)\\ln(1/\\epsilon))$ for $L$-smooth and $\\mu$-strongly convex individual functions - one must also know which individual function is being referred to by the oracle at each iteration. Next, we show that for a broad class of first-order and coordinate-descent finite sum algorithms (including, e.g., SDCA, SVRG, SAG), it is not possible to get an `accelerated\' complexity bound of $\\tilde{\\cO}((n+\\sqrt{n L/\\mu})\\ln(1/\\epsilon))$, unless the strong convexity parameter is given explicitly. Lastly, we show that when this class of algorithms is used for minimizing $L$-smooth and convex finite sums, the optimal complexity bound is $\\tilde{\\cO}(n+L/\\epsilon)$, assuming that (on average) the same update rule is used in every iteration, and $\\tilde{\\cO}(n+\\sqrt{nL/\\epsilon})$, otherwise.\n Abstract ,  PDF \n #342: Unsupervised Sequence Classification using Sequential Output Statistics \n #343: Subset Selection under Noise \n #344: Collecting Telemetry Data Privately \n #345: Concrete Dropout \n Yarin Gal,  Jiri Hron,  Alex Kendall \n Dropout is used as a practical tool to obtain uncertainty estimates in large vision models and reinforcement learning (RL) tasks. But to obtain well-calibrated uncertainty estimates, a grid-search over the dropout probabilities is necessary - a prohibitive operation with large models, and an impossible one with RL. We propose a new dropout variant which gives improved performance and better calibrated uncertainties. Relying on recent developments in Bayesian deep learning, we use a continuous relaxation of dropout\'s discrete masks. Together with a principled optimisation objective, this allows for automatic tuning of the dropout probability in large models, and as a result faster experimentation cycles. In RL this allows the agent to adapt its uncertainty dynamically as more data is observed. We analyse the proposed variant extensively on a range of tasks, and give insights into common practice in the field where larger dropout probabilities are often used in deeper model layers.\n Abstract ,  PDF \n #346: Adaptive Batch Size for Safe Policy Gradients \n #347: A Disentangled Recognition and Nonlinear Dynamics Model for Unsupervised Learning \n #348: PASS-GLM: polynomial approximate sufficient statistics for scalable Bayesian GLM inference \n Jonathan H. Huggins,  Ryan P. Adams,  Tamara Broderick \n Generalized linear models (GLMs) -- such as logistic regression, Poisson regression, and robust regression -- provide interpretable models for diverse data types. Probabilistic approaches, particularly Bayesian ones, allow coherent estimates of uncertainty, incorporation of prior information, and sharing of power across experiments via hierarchical models. In practice, however, the approximate Bayesian methods necessary for inference have either failed to scale to large data sets or failed to provide theoretical guarantees on the quality of inference. We propose a new approach based on constructing polynomial approximate sufficient statistics for GLMs (PASS-GLM). We demonstrate that our method admits a simple algorithm as well as trivial streaming and distributed extensions that do not compound error across computations. We provide theoretical guarantees on the quality of point (MAP) estimates, the approximate posterior, and posterior mean and uncertainty estimates. We validate our approach empirically in the case of logistic regression using a quadratic approximation and show competitive performance with stochastic gradient descent, MCMC, and the Laplace approximation in terms of speed and multiple measures of accuracy -- including on an advertising data set with 40 million data points and 20,000 covariates.\n Abstract ,  PDF \n #349: Bayesian GANs \n #350: Off-policy evaluation for slate recommendation \n Adith Swaminathan,  Akshay Krishnamurthy,  Alekh Agarwal,  Miroslav Dudík,  John Langford,  Damien Jose,  Imed Zitouni \n This paper studies the evaluation of policies that recommend an ordered set of items (e.g., a ranking) based on some context---a common scenario in web search, ads and recommender systems. We develop the first practical technique for evaluating page-level metrics of such policies offline using logged past data, alleviating the need for online A/B tests. Our method models the observed quality of the recommended set (e.g., time to success in web search) as an additive decomposition across items. Crucially, the per-item quality is not directly observed or easily modeled from the item\'s features. A thorough empirical evaluation reveals that this model fits many realistic measures of quality and theoretical analysis shows exponential savings in the amount of required data compared with prior off-policy evaluation approaches.\n Abstract ,  PDF \n #351: A multi-agent reinforcement learning model of common-pool resource appropriation \n Julien Perolat,  Joel Z. Leibo,  Vinicius Zambaldi,  Charles Beattie,  Karl Tuyls,  Thore Graepel \n Humanity faces numerous problems of common-pool resource appropriation. This class of multi-agent social dilemma includes the problems of ensuring sustainable use of fresh water, common fisheries, grazing pastures, and irrigation systems. Abstract models of common-pool resource appropriation based on non-cooperative game theory predict that self-interested agents will generally fail to find socially positive equilibria---a phenomenon called the tragedy of the commons. However, in reality, human societies are sometimes able to discover and implement stable cooperative solutions. Decades of behavioral game theory research have sought to uncover aspects of human behavior that make this possible. Most of that work was based on laboratory experiments where participants only make a single choice: how much to appropriate. Recognizing the importance of spatial and temporal resource dynamics, a recent trend has been toward experiments in more complex real-time video game-like environments. However, standard methods of non-cooperative game theory can no longer be used to generate predictions for this case. Here we show that deep reinforcement learning can be used instead. To that end, we study the emergent behavior of groups of independently learning agents in a partially observed Markov game modeling common-pool resource appropriation. Our experiments highlight the importance of trial-and-error learning in common-pool resource appropriation and shed light on the relationship between exclusion, sustainability, and inequality.\n Abstract ,  PDF \n #352: On the Optimization Landscape of Tensor Decompositions \n Rong Ge,  Tengyu Ma \n Non-convex optimization with local search heuristics has been widely used in machine learning, achieving many state-of-art results. It becomes increasingly important to understand why they can work for these NP-hard problems on typical data. The landscape of many objective functions in learning has been conjectured to have the geometric property that "all local optima are (approximately) global optima", and thus they can be solved efficiently by local search algorithms. However, establishing such property can be very difficult. In this paper, we analyze the optimization landscape of the random over-complete tensor decomposition problem, which has many applications in unsupervised learning, especially in learning latent variable models. In practice, it can be efficiently solved by gradient ascent on a non-convex objective. We show that for any small constant $\\epsilon > 0$, among the set of points with function values $(1+\\epsilon)$-factor larger than the expectation of the function, all the local maxima are approximate global maxima. Previously, the best-known result only characterizes the geometry in small neighborhoods around the true components. Our result implies that even with an initialization that is barely better than the random guess, the gradient ascent algorithm is guaranteed to solve this problem. Our main technique uses Kac-Rice formula and random matrix theory. To our best knowledge, this is the first time when Kac-Rice formula is successfully applied to counting the number of local minima of a highly-structured random polynomial with dependent coefficients.\n Abstract ,  PDF \n #353: High-Order Attention Models for Visual Question Answering \n #354: Sparse convolutional coding for neuronal assembly detection \n #355: Quantifying how much sensory information in a neural code is relevant for behavior \n #356: Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks \n Federico Monti,  Michael M. Bronstein,  Xavier Bresson \n Matrix completion models are among the most common formulations of recommender systems. Recent works have showed a boost of performance of these techniques when introducing the pairwise relationships between users/items in the form of graphs, and imposing smoothness priors on these graphs. However, such techniques do not fully exploit the local stationarity structures of user/item graphs, and the number of parameters to learn is linear w.r.t. the number of users and items. We propose a novel approach to overcome these limitations by using geometric deep learning on graphs. Our matrix completion architecture combines graph convolutional neural networks and recurrent neural networks to learn meaningful statistical graph-structured patterns and the non-linear diffusion process that generates the known ratings. This neural network system requires a constant number of parameters independent of the matrix size. We apply our method on both synthetic and real datasets, showing that it outperforms state-of-the-art techniques.\n Abstract ,  PDF \n #357: Reducing Reparameterization Gradient Variance \n Andrew C. Miller,  Nicholas J. Foti,  Alexander D\'Amour,  Ryan P. Adams \n Optimization with noisy gradients has become ubiquitous in statistics and machine learning. Reparameterization gradients, or gradient estimates computed via the "reparameterization trick," represent a class of noisy gradients often used in Monte Carlo variational inference (MCVI). However, when these gradient estimators are too noisy, the optimization procedure can be slow or fail to converge. One way to reduce noise is to use more samples for the gradient estimate, but this can be computationally expensive. Instead, we view the noisy gradient as a random variable, and form an inexpensive approximation of the generating procedure for the gradient sample. This approximation has high correlation with the noisy gradient by construction, making it a useful control variate for variance reduction. We demonstrate our approach on non-conjugate multi-level hierarchical models and a Bayesian neural net where we observed gradient variance reductions of multiple orders of magnitude (20-2,000x).\n Abstract ,  PDF \n #358: Visual Reference Resolution using Attention Memory for Visual Dialog \n Paul Hongsuck Seo,  Andreas Lehrmann,  Bohyung Han,  Leonid Sigal \n Visual dialog is a task of answering a series of inter-dependent questions given an input image, and often requires to resolve visual references among the questions. This problem is different from visual question answering (VQA), which relies on spatial attention (a.k.a. visual grounding) estimated from an image and question pair. We propose a novel attention mechanism that exploits visual attentions in the past to resolve the current reference in the visual dialog scenario. The proposed model is equipped with an associative attention memory storing a sequence of previous (attention, key) pairs. From this memory, the model retrieves previous attention, taking into account recency, that is most relevant for the current question, in order to resolve potentially ambiguous reference(s). The model then merges the retrieved attention with the tentative one to obtain the final attention for the current question; specifically, we use dynamic parameter prediction to combine the two attentions conditioned on the question. Through extensive experiments on a new synthetic visual dialog dataset, we show that our model significantly outperforms the state-of-the-art (by ~16 % points) in the situation where the visual reference resolution plays an important role. Moreover, the proposed model presents superior performance (~2 % points improvement) in the Visual Dialog dataset, despite having significantly fewer parameters than the baselines.\n Abstract ,  PDF \n #359: Joint distribution optimal transportation for domain adaptation \n #360: Multiresolution Kernel Approximation for Gaussian Process Regression \n Yi Ding,  Risi Kondor,  Jonathan Eskreis-Winkler \n Gaussian process regression generally does not scale to beyond a few thousands data points without applying some sort of kernel approximation method. Most approximations focus on the high eigenvalue part of the spectrum of the kernel matrix, $K$, which leads to bad performance when the length scale of the kernel is small. In this paper we introduce Multiresolution Kernel Approximation (MKA), the first true broad bandwidth kernel approximation algorithm. Important points about MKA are that it is memory efficient, and it is a direct method, which means that it also makes it easy to approximate $K^{-1}$ and $\\mathop{\\textrm{det}}(K)$.\n Abstract ,  PDF \n #361: Collapsed variational Bayes for Markov jump processes \n #362: Universal consistency and minimax rates for online Mondrian Forest \n #363: Efficiency Guarantees from Data \n #364: Diving into the shallows: a computational perspective on large-scale shallow learning \n Siyuan Ma,  Mikhail Belkin \n In this paper we first identify a basic limitation in gradient descent-based optimization methods when used in conjunctions with smooth kernels. An analysis based on the spectral properties of the kernel demonstrates that only a vanishingly small portion of the function space is reachable after a polynomial number of gradient descent iterations. This lack of approximating power drastically limits gradient descent for a fixed computational budget leading to serious over-regularization/underfitting. The issue is purely algorithmic, persisting even in the limit of infinite data. To address this shortcoming in practice, we introduce EigenPro iteration, based on a preconditioning scheme using a small number of approximately computed eigenvectors. It can also be viewed as learning a new kernel optimized for gradient descent. It turns out that injecting this small (computationally inexpensive and SGD-compatible) amount of approximate second-order information leads to major improvements in convergence. For large data, this translates into significant performance boost over the standard kernel methods. In particular, we are able to consistently match or improve the state-of-the-art results recently reported in the literature with a small fraction of their computational budget. Finally, we feel that these results show a need for a broader computational perspective on modern large-scale learning to complement more traditional statistical and convergence analyses. In particular, many phenomena of large-scale high-dimensional inference are best understood in terms of optimization on infinite dimensional Hilbert spaces, where standard algorithms can sometimes have properties at odds with finite-dimensional intuition. A systematic analysis concentrating on the approximation power of such algorithms within a budget of computation may lead to progress both in theory and practice.\n Abstract ,  PDF \n #365: End-to-end Differentiable Proving \n Tim Rocktäschel,  Sebastian Riedel \n We introduce neural networks for end-to-end differentiable theorem proving that operate on dense vector representations of symbols. These neural networks are constructed recursively by taking inspiration from the backward chaining algorithm as used in Prolog. Specifically, we replace symbolic unification with a differentiable computation on vector representations of symbols using a radial basis function kernel, thereby combining symbolic reasoning with learning subsymbolic vector representations. By using gradient descent, the resulting neural network can be trained to infer facts from a given incomplete knowledge base. It learns to (i) place representations of similar symbols in close proximity in a vector space, (ii) make use of such similarities to prove facts, (iii) induce logical rules, and (iv) use provided and induced logical rules for complex multi-hop reasoning. We demonstrate that this architecture outperforms ComplEx, a state-of-the-art neural link prediction model, on four benchmark knowledge bases while at the same time inducing interpretable function-free first-order logic rules.\n Abstract ,  PDF \n #366: Influence Maximization with $\\varepsilon$-Almost Submodular Threshold Function \n #367: Inferring The Latent Structure of Human Decision-Making from Raw Visual Inputs \n Yunzhu Li,  Jiaming Song,  Stefano Ermon \n The goal of imitation learning is to match example expert behavior, without access to a reinforcement signal. Expert demonstrations provided by humans, however, often show significant variability due to latent factors that are not explicitly modeled. We introduce an extension to the Generative Adversarial Imitation Learning method that can infer the latent structure of human decision-making in an unsupervised way. Our method can not only imitate complex behaviors, but also learn interpretable and meaningful representations. We demonstrate that the approach is applicable to high-dimensional environments including raw visual inputs. In the highway driving domain, we show that a model learned from demonstrations is able to both produce different styles of human-like driving behaviors and accurately anticipate human actions. Our method surpasses various baselines in terms of performance and functionality.\n Abstract ,  PDF \n #368: Variational Laws of Visual Attention for Dynamic Scenes \n #369: Recursive Sampling for the Nystrom Method \n #370: Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning \n Shixiang Gu,  Timothy Lillicrap,  Zoubin Ghahramani,  Richard E. Turner,  Bernhard Schölkopf,  Sergey Levine \n Off-policy model-free deep reinforcement learning methods using previously collected data can improve sample efficiency over on-policy policy gradient techniques. On the other hand, on-policy algorithms are often more stable and easier to use. This paper examines, both theoretically and empirically, approaches to merging on- and off-policy updates for deep reinforcement learning. Theoretical results show that off-policy updates with a value function estimator can be interpolated with on-policy policy gradient updates whilst still satisfying performance bounds. Our analysis uses control variate methods to produce a family of policy gradient algorithms, with several recently proposed algorithms being special cases of this family. We then provide an empirical comparison of these techniques with the remaining algorithmic details fixed, and show how different mixing of off-policy gradient estimates with on-policy samples contribute to improvements in empirical performance. The final algorithm provides a generalization and unification of existing deep policy gradient techniques, has theoretical guarantees on the bias introduced by off-policy updates, and improves on the state-of-the-art model-free deep RL methods on a number of OpenAI Gym continuous control benchmarks.\n Abstract ,  PDF \n #371: Dynamic Routing Between Capsules \n #372: Incorporating Side Information by Adaptive Convolution \n #373: Conic Scan Coverage algorithm for nonparametric topic modeling \n #374: FALKON: An Optimal Large Scale Kernel Method \n Alessandro Rudi,  Luigi Carratino,  Lorenzo Rosasco \n Kernel methods provide a principled way to perform non linear, nonparametric learning. They rely on solid functional analytic foundations and enjoy optimal statistical properties. However, at least in their basic form, they have limited applicability in large scale scenarios because of stringent computational requirements in terms of time and especially memory. In this paper, we take a substantial step in scaling up kernel methods, proposing FALKON, a novel algorithm that allows to efficiently process millions of points. FALKON is derived combining several algorithmic principles, namely stochastic projections, iterative solvers and preconditioning. Our theoretical analysis shows that optimal statistical accuracy is achieved requiring essentially $O(n)$ memory and $O(n\\sqrt{n})$ time. Extensive experiments show that state of the art results on available large scale datasets can be achieved even on a single machine.\n Abstract ,  PDF \n #375: Structured Generative Adversarial Networks \n #376: Conservative Contextual Linear Bandits \n Abbas Kazerouni,  Mohammad Ghavamzadeh,  Yasin Abbasi-Yadkori,  Benjamin Van Roy \n Safety is a desirable property that can immensely increase the applicability of learning algorithms in real-world decision-making problems. It is much easier for a company to deploy an algorithm that is safe, i.e., guaranteed to perform at least as well as a baseline. In this paper, we study the issue of safety in contextual linear bandits that have application in many different fields including personalized ad recommendation in online marketing. We formulate a notion of safety for this class of algorithms. We develop a safe contextual linear bandit algorithm, called conservative linear UCB (CLUCB), that simultaneously minimizes its regret and satisfies the safety constraint, i.e., maintains its performance above a fixed percentage of the performance of a baseline strategy, uniformly over time. We prove an upper-bound on the regret of CLUCB and show that it can be decomposed into two terms: 1) an upper-bound for the regret of the standard linear UCB algorithm that grows with the time horizon and 2) a constant (does not grow with the time horizon) term that accounts for the loss of being conservative in order to satisfy the safety constraint. We empirically show that our algorithm is safe and validate our theoretical analysis.\n Abstract ,  PDF \n #377: Variational Memory Addressing in Generative Models \n Jörg Bornschein,  Andriy Mnih,  Daniel Zoran,  Danilo J. Rezende \n Aiming to augment generative models with external memory, we interpret the output of a memory module with stochastic addressing as a conditional mixture distribution, where a read operation corresponds to sampling a discrete memory address and retrieving the corresponding content from memory. This perspective allows us to apply variational inference to memory addressing, which enables effective training of the memory module by using the target information to guide memory lookups. Stochastic addressing is particularly well-suited for generative models as it naturally encourages multimodality which is a prominent aspect of most high-dimensional datasets. Treating the chosen address as a latent variable also allows us to quantify the amount of information gained with a memory lookup and measure the contribution of the memory module to the generative process. To illustrate the advantages of this approach we incorporate it into a variational autoencoder and apply the resulting model to the task of generative few-shot learning. The intuition behind this architecture is that the memory module can pick a relevant template from memory and the continuous part of the model can concentrate on modeling remaining variations. We demonstrate empirically that our model is able to identify and access the relevant memory contents even with hundreds of unseen Omniglot characters in memory\n Abstract ,  PDF \n #378: On Tensor Train Rank Minimization : Statistical Efficiency and Scalable Algorithm \n #379: Scalable Levy Process Priors for Spectral Kernel Learning \n #380: Deep Hyperspherical Learning \n #381: Learning Deep Structured Multi-Scale Features using Attention-Gated CRFs for Contour Prediction \n #382: On-the-fly Operation Batching in Dynamic Computation Graphs \n Graham Neubig,  Yoav Goldberg,  Chris Dyer \n Dynamic neural network toolkits such as PyTorch, DyNet, and Chainer offer more flexibility for implementing models that cope with data of varying dimensions and structure, relative to toolkits that operate on statically declared computations (e.g., TensorFlow, CNTK, and Theano). However, existing toolkits - both static and dynamic - require that the developer organize the computations into the batches necessary for exploiting high-performance algorithms and hardware. This batching task is generally difficult, but it becomes a major hurdle as architectures become complex. In this paper, we present an algorithm, and its implementation in the DyNet toolkit, for automatically batching operations. Developers simply write minibatch computations as aggregations of single instance computations, and the batching algorithm seamlessly executes them, on the fly, using computationally efficient batched operations. On a variety of tasks, we obtain throughput similar to that obtained with manual batches, as well as comparable speedups over single-instance learning on architectures that are impractical to batch manually.\n Abstract ,  PDF \n #383: Nonlinear Acceleration of Stochastic Algorithms \n Damien Scieur,  Alexandre d\'Aspremont,  Francis Bach \n Extrapolation methods use the last few iterates of an optimization algorithm to produce a better estimate of the optimum. They were shown to achieve optimal convergence rates in a deterministic setting using simple gradient iterates. Here, we study extrapolation methods in a stochastic setting, where the iterates are produced by either a simple or an accelerated stochastic gradient algorithm. We first derive convergence bounds for arbitrary, potentially biased perturbations, then produce asymptotic bounds using the ratio between the variance of the noise and the accuracy of the current point. Finally, we apply this acceleration technique to stochastic algorithms such as SGD, SAGA, SVRG and Katyusha in different settings, and show significant performance gains.\n Abstract ,  PDF \n #384: Optimized Pre-Processing for Discrimination Prevention \n #385: YASS: Yet Another Spike Sorter \n #386: Independence clustering (without a matrix) \n Daniil Ryabko \n The independence clustering problem is considered in the following formulation: given a set $S$ of random variables, it is required to find the finest partitioning ${U_1,\\dots,U_k}$ of $S$ into clusters such that the clusters $U_1,\\dots,U_k$ are mutually independent. Since mutual independence is the target, pairwise similarity measurements are of no use, and thus traditional clustering algorithms are inapplicable. The distribution of the random variables in $S$ is, in general, unknown, but a sample is available. Thus, the problem is cast in terms of time series. Two forms of sampling are considered: i.i.d.\\ and stationary time series, with the main emphasis being on the latter, more general, case. A consistent, computationally tractable algorithm for each of the settings is proposed, and a number of open directions for further research are outlined.\n Abstract ,  PDF \n #387: Fast amortized inference of neural activity from calcium imaging data with variational autoencoders \n #388: Adaptive Active Hypothesis Testing under Limited Information \n #389: Streaming Weak Submodularity: Interpreting Neural Networks on the Fly \n Ethan R. Elenberg,  Alexandros G. Dimakis,  Moran Feldman,  Amin Karbasi \n In many machine learning applications, it is important to explain the predictions of a black-box classifier. For example, why does a deep neural network assign an image to a particular class? We cast interpretability of black-box classifiers as a combinatorial maximization problem and propose an efficient streaming algorithm to solve it subject to cardinality constraints. By extending ideas from Badanidiyuru et al. [2014], we provide a constant factor approximation guarantee for our algorithm in the case of random stream order and a weakly submodular objective function. This is the first such theoretical guarantee for this general class of functions, and we also show that no such algorithm exists for a worst case stream order. Our algorithm obtains similar explanations of Inception V3 predictions $10$ times faster than the state-of-the-art LIME framework of Ribeiro et al. [2016].\n Abstract ,  PDF \n #390: Successor Features for Transfer in Reinforcement Learning \n André Barreto,  Rémi Munos,  Tom Schaul,  David Silver \n Transfer in reinforcement learning refers to the notion that generalization should occur not only within a task but also across tasks. Our focus is on transfer where the reward functions vary across tasks while the environment\'s dynamics remain the same. The method we propose rests on two key ideas: "successor features," a value function representation that decouples the dynamics of the environment from the rewards, and "generalized policy improvement," a generalization of dynamic programming\'s policy improvement step that considers a set of policies rather than a single one. Put together, the two ideas lead to an approach that integrates seamlessly within the reinforcement learning framework and allows transfer to take place between tasks without any restriction. The proposed method also provides performance guarantees for the transferred policy even before any learning has taken place. We derive two theorems that set our approach in firm theoretical ground and present experiments that show that it successfully promotes transfer in practice.\n Abstract ,  PDF \n #391: Counterfactual Fairness \n Matt J. Kusner,  Joshua R. Loftus,  Chris Russell,  Ricardo Silva \n Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.\n Abstract ,  PDF \n #392: Prototypical Networks for Few-shot Learning \n Jake Snell,  Kevin Swersky,  Richard S. Zemel \n We propose prototypical networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend prototypical networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.\n Abstract ,  PDF \n #393: Triple Generative Adversarial Nets \n Chongxuan Li,  Kun Xu,  Jun Zhu,  Bo Zhang \n Generative Adversarial Nets (GANs) have shown promise in image generation and semi-supervised learning (SSL). However, existing GANs in SSL have two problems: (1) the generator and discriminator may compete in learning; and (2) the generator cannot generate images in a specific class. The problems essentially arise from the two-player formulation, where a single discriminator shares incompatible roles of identifying fake samples and predicting labels and it only estimates the data without considering labels. We address the problems by presenting triple generative adversarial net (Triple-GAN), a flexible game-theoretical framework for classification and class-conditional generation in SSL. Triple-GAN consists of three players---a generator, a discriminator and a classifier, where the generator and classifier characterize the conditional distributions between images and labels, and the discriminator solely focuses on identifying fake image-label pairs. We design compatible utilities to ensure that the distributions characterized by the classifier and generator both concentrate to the data distribution. Our results on various datasets demonstrate that Triple-GAN as a unified model can simultaneously (1) achieve state-of-the-art classification results among deep generative models, and (2) disentangle the classes and styles and transfer smoothly on the data level via interpolation in the latent space class-conditionally.\n Abstract ,  PDF \n #394: Efficient Sublinear-Regret Algorithms for Online Sparse Linear Regression \n #395: Mapping distinct timescales of functional interactions among brain networks \n #396: Multi-Armed Bandits with Metric Movement Costs \n #397: Learning A Structured Optimal Bipartite Graph for Co-Clustering \n #398: Learning Low-Dimensional Metrics \n Lalit Jain,  Blake Mason,  Robert Nowak \n This paper investigates the theoretical foundations of metric learning, focused on three key questions that are not fully addressed in prior work: 1) we consider learning general low-dimensional (low-rank) metrics as well as sparse metrics; 2) we develop upper and lower (minimax)bounds on the generalization error; 3) we quantify the sample complexity of metric learning in terms of the dimension of the feature space and the dimension/rank of the underlying metric;4) we also bound the accuracy of the learned metric relative to the underlying true generative metric. All the results involve novel mathematical approaches to the metric learning problem, and lso shed new light on the special case of ordinal embedding (aka non-metric multidimensional scaling).\n Abstract ,  PDF \n #399: The Marginal Value of Adaptive Gradient Methods in Machine Learning \n Ashia C. Wilson,  Rebecca Roelofs,  Mitchell Stern,  Nathan Srebro,  Benjamin Recht \n Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We show that for simple overparameterized problems, adaptive methods often find drastically different solutions than gradient descent (GD) or stochastic gradient descent (SGD). We construct an illustrative binary classification problem where the data is linearly separable, GD and SGD achieve zero test error, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to half. We additionally study the empirical generalization capability of adaptive methods on several state-of-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often significantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.\n Abstract ,  PDF \n #400: Aggressive Sampling for Multi-class to Binary Reduction with Applications to Text Classification \n Bikash Joshi,  Massih-Reza Amini,  Ioannis Partalas,  Franck Iutzeler,  Yury Maximov \n We address the problem of multi-class classification in the case where the number of classes is very large. We propose a double sampling strategy on top of a multi-class to binary reduction strategy, which transforms the original multi-class problem into a binary classification problem over pairs of examples. The aim of the sampling strategy is to overcome the curse of long-tailed class distributions exhibited in majority of large-scale multi-class classification problems and to reduce the number of pairs of examples in the expanded data. We show that this strategy does not alter the consistency of the empirical risk minimization principle defined over the double sample reduction. Experiments are carried out on DMOZ and Wikipedia collections with 10,000 to 100,000 classes where we show the efficiency of the proposed approach in terms of training and prediction time, memory consumption, and predictive performance with respect to state-of-the-art approaches.\n Abstract ,  PDF \n #401: Deconvolutional Paragraph Representation Learning \n Yizhe Zhang,  Dinghan Shen,  Guoyin Wang,  Zhe Gan,  Ricardo Henao,  Lawrence Carin \n Learning latent representations from long text sequences is an important first step in many natural language processing applications. Recurrent Neural Networks (RNNs) have become a cornerstone for this challenging task. However, the quality of sentences during RNN-based decoding (reconstruction) decreases with the length of the text. We propose a sequence-to-sequence, purely convolutional and deconvolutional autoencoding framework that is free of the above issue, while also being computationally efficient. The proposed method is simple, easy to implement and can be leveraged as a building block for many applications. We show empirically that compared to RNNs, our framework is better at reconstructing and correcting long paragraphs. Quantitative evaluation on semi-supervised text classification and summarization tasks demonstrate the potential for better utilization of long unlabeled text data.\n Abstract ,  PDF \n #402: Random Permutation Online Isotonic Regression \n #403: A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning \n #404: Inverse Filtering for Hidden Markov Models \n #405: Non-parametric Neural Networks \n #406: Learning Active Learning from Data \n #407: VAE Learning via Stein Variational Gradient Descent \n #408: Deep adversarial neural decoding \n Yağmur Güçlütürk,  Umut Güçlü,  Katja Seeliger,  Sander Bosch,  Rob van Lier,  Marcel van Gerven \n Here, we present a novel approach to solve the problem of reconstructing perceived stimuli from brain responses by combining probabilistic inference with deep learning. Our approach first inverts the linear transformation from latent features to brain responses with maximum a posteriori estimation and then inverts the nonlinear transformation from perceived stimuli to latent features with adversarial training of convolutional neural networks. We test our approach with a functional magnetic resonance imaging experiment and show that it can generate state-of-the-art reconstructions of perceived faces from brain activations.\n Abstract ,  PDF \n #409: Efficient Use of Limited-Memory Resources to Accelerate Linear Learning \n Celestine Dünner,  Thomas Parnell,  Martin Jaggi \n We propose a generic algorithmic building block to accelerate training of machine learning models on heterogenous compute systems. The scheme allows to efficiently employ compute accelerators such as GPUs and FPGAs for the training of large-scale machine learning models, when the training data exceeds their memory capacity. Also, it provides adaptivity to any system\'s memory hierarchy in terms of size and processing speed. Our technique builds upon primal-dual coordinate methods, and uses duality gap information to dynamically decide which part of the data should be made available for fast processing. We provide a strong theoretical motivation for our gap-based selection scheme and provide an efficient practical implementation thereof. To illustrate the power of our approach we demonstrate its performance for training of generalized linear models on large scale datasets exceeding the memory size of a modern GPU, showing an order-of-magnitude speedup over existing approaches.\n Abstract ,  PDF \n #410: Temporal Coherency based Criteria for Predicting Video Frames using Deep Multi-stage Generative Adversarial Networks \n #411: Sobolev Training for Neural Networks \n Wojciech Marian Czarnecki,  Simon Osindero,  Max Jaderberg,  Grzegorz Świrszcz,  Razvan Pascanu \n At the heart of deep learning we aim to use neural networks as function approximators - training them to produce outputs from inputs in emulation of a ground truth function or data creation process. In many cases we only have access to input-output pairs from the ground truth, however it is becoming more common to have access to derivatives of the target output with respect to the input - for example when the ground truth function is itself a neural network such as in network compression or distillation. Generally these target derivatives are not computed, or are ignored. This paper introduces Sobolev Training for neural networks, which is a method for incorporating these target derivatives in addition the to target values while training. By optimising neural networks to not only approximate the function\'s outputs but also the function\'s derivatives we encode additional information about the target function within the parameters of the neural network. Thereby we can improve the quality of our predictors, as well as the data-efficiency and generalization capabilities of our learned function approximation. We provide theoretical justifications for such an approach as well as examples of empirical evidence on three distinct domains: regression on classical optimisation datasets, distilling policies of an agent playing Atari, and on large-scale applications of synthetic gradients. In all three domains the use of Sobolev Training, employing target derivatives in addition to target values, results in models with higher accuracy and stronger generalisation.\n Abstract ,  PDF \n #412: Multi-Information Source Optimization \n Matthias Poloczek,  Jialei Wang,  Peter I. Frazier \n We consider Bayesian optimization of an expensive-to-evaluate black-box objective function, where we also have access to cheaper approximations of the objective. In general, such approximations arise in applications such as reinforcement learning, engineering, and the natural sciences, and are subject to an inherent, unknown bias. This model discrepancy is caused by an inadequate internal model that deviates from reality and can vary over the domain, making the utilization of these approximations a non-trivial task. We present a novel algorithm that provides a rigorous mathematical treatment of the uncertainties arising from model discrepancies and noisy observations. Its optimization decisions rely on a value of information analysis that extends the Knowledge Gradient factor to the setting of multiple information sources that vary in cost: each sampling decision maximizes the predicted benefit per unit cost. We conduct an experimental evaluation that demonstrates that the method consistently outperforms other state-of-the-art techniques: it finds designs of considerably higher objective value and additionally inflicts less cost in the exploration process.\n Abstract ,  PDF \n #413: Deep Reinforcement Learning from Human Preferences \n #414: On the Fine-Grained Complexity of Empirical Risk Minimization: Kernel Methods and Neural Networks \n Arturs Backurs,  Piotr Indyk,  Ludwig Schmidt \n Empirical risk minimization (ERM) is ubiquitous in machine learning and underlies most supervised learning methods. While there has been a large body of work on algorithms for various ERM problems, the exact computational complexity of ERM is still not understood. We address this issue for multiple popular ERM problems including kernel SVMs, kernel ridge regression, and training the final layer of a neural network. In particular, we give conditional hardness results for these problems based on complexity-theoretic assumptions such as the Strong Exponential Time Hypothesis. Under these assumptions, we show that there are no algorithms that solve the aforementioned ERM problems to high accuracy in sub-quadratic time. We also give similar hardness results for computing the gradient of the empirical loss, which is the main computational burden in many non-convex learning tasks.\n Abstract ,  PDF \n #415: Policy Gradient With Value Function Approximation For Collective Multiagent Planning \n #416: Adversarial Symmetric Variational Autoencoder \n #417: Tensor encoding and decomposition of brain connectomes with application to tractography evaluation \n #418: A Minimax Optimal Algorithm for Crowdsourcing \n #419: Estimating Accuracy from Unlabeled Data: A Probabilistic Logic Approach \n Emmanouil A. Platanios,  Hoifung Poon,  Tom M. Mitchell,  Eric Horvitz \n We propose an efficient method to estimate the accuracy of classifiers using only unlabeled data. We consider a setting with multiple classification problems where the target classes may be tied together through logical constraints. For example, a set of classes may be mutually exclusive, meaning that a data instance can belong to at most one of them. The proposed method is based on the intuition that: (i) when classifiers agree, they are more likely to be correct, and (ii) when the classifiers make a prediction that violates the constraints, at least one classifier must be making an error. Experiments on four real-world data sets produce accuracy estimates within a few percent of the true accuracy, using solely unlabeled data. Our models also outperform existing state-of-the-art solutions in both estimating accuracies, and combining multiple classifier outputs. The results emphasize the utility of logical constraints in estimating accuracy, thus validating our intuition.\n Abstract ,  PDF \n #420: A Decomposition of Forecast Error in Prediction Markets \n Miroslav Dudík,  Sébastien Lahaie,  Ryan Rogers,  Jennifer Wortman Vaughan \n We introduce and analyze sources of error in prediction market forecasts in order to characterize and bound the difference between a security\'s price and its ground truth value. We consider cost-function-based prediction markets in which an automated market maker adjusts security prices according to the history of trade. We decompose the forecasting error into four components: \\emph{sampling error}, occurring because traders only possess noisy estimates of ground truth; \\emph{risk-aversion effect}, arising because traders reveal beliefs only through self-interested trade; \\emph{market-maker bias}, resulting from the use of a particular market maker (i.e., cost function) to facilitate trade; and finally, \\emph{convergence error}, arising because, at any point in time, market prices may still be in flux. Our goal is to understand the tradeoffs between these error components, and how they are influenced by design decisions such as the functional form of the cost function and the amount of liquidity in the market. We specifically consider a model in which traders have exponential utility and exponential-family beliefs drawn with an independent noise relative to ground truth. In this setting, sampling error and risk-aversion effect vanish as the number of traders grows, but there is a tradeoff between the other two components: decreasing the market maker\'s liquidity results in smaller market-maker bias, but may also slow down convergence. We provide both upper and lower bounds on market-maker bias and convergence error, and demonstrate via numerical simulations that these bounds are tight. Our results yield new insights into the question of how to set the market\'s liquidity parameter, and into the extent to which markets that enforce coherent prices across securities produce better predictions than markets that price securities independently.\n Abstract ,  PDF \n #421: Safe Adaptive Importance Sampling \n #422: Variational Walkback: Learning a Transition Operator as a Stochastic Recurrent Net \n #423: Polynomial Codes: an Optimal Design for High-Dimensional Coded Matrix Multiplication \n Qian Yu,  Mohammad Ali Maddah-Ali,  A. Salman Avestimehr \n We consider a large-scale matrix multiplication problem where the computation is carried out using a distributed system with a master node and multiple worker nodes, where each worker can store parts of the input matrices. We propose a computation strategy that leverages ideas from coding theory to design intermediate computations at the worker nodes, in order to efficiently deal with straggling workers. The proposed strategy, named as \\emph{polynomial codes}, achieves the optimum recovery threshold, defined as the minimum number of workers that the master needs to wait for in order to compute the output. Furthermore, by leveraging the algebraic structure of polynomial codes, we can map the reconstruction problem of the final output to a polynomial interpolation problem, which can be solved efficiently. Polynomial codes provide order-wise improvement over the state of the art in terms of recovery threshold, and are also optimal in terms of several other metrics. Furthermore, we extend this code to distributed convolution and show its order-wise optimality.\n Abstract ,  PDF \n #424: Unsupervised Learning of Disentangled Representations from Video \n Emily Denton,  Vighnesh Birodkar \n We present a new model DrNET that learns disentangled image representations from video. Our approach leverages the temporal coherence of video and a novel adversarial loss to learn a representation that factorizes each frame into a stationary part and a temporally varying component. The disentangled representation can be used for a range of tasks. For example, applying a standard LSTM to the time-vary components enables prediction of future frames. We evaluate our approach on a range of synthetic and real videos, demonstrating the ability to coherently generate hundreds of steps into the future.\n Abstract ,  PDF \n #425: Federated Multi-Task Learning \n Virginia Smith,  Chao-Kai Chiang,  Maziar Sanjabi,  Ameet Talwalkar \n Federated learning poses new statistical and systems challenges in training machine learning models over distributed networks of devices. In this work, we show that multi-task learning is naturally suited to handle the statistical challenges of this setting, and propose a novel systems-aware optimization method, MOCHA, that is robust to practical systems issues. Our method and theory for the first time consider issues of high communication cost, stragglers, and fault tolerance for distributed multi-task learning. The resulting method achieves significant speedups compared to alternatives in the federated setting, as we demonstrate through simulations on real-world federated datasets.\n Abstract ,  PDF \n #426: Is Input Sparsity Time Possible for Kernel Low-Rank Approximation? \n #427: The Expxorcist: Nonparametric Graphical Models Via Conditional Exponential Densities \n #428: Improved Graph Laplacian via Geometric Self-Consistency \n #429: Dual Path Networks \n Yunpeng Chen,  Jianan Li,  Huaxin Xiao,  Xiaojie Jin,  Shuicheng Yan,  Jiashi Feng \n In this work, we present a simple, highly efficient and modularized Dual Path Network (DPN) for image classification which presents a new topology of connection paths internally. By revealing the equivalence of the state-of-the-art Residual Network (ResNet) and Densely Convolutional Network (DenseNet) within the HORNN framework, we find that ResNet enables feature re-usage while DenseNet enables new features exploration which are both important for learning good representations. To enjoy the benefits from both path topologies, our proposed Dual Path Network shares common features while maintaining the flexibility to explore new features through dual path architectures. Extensive experiments on three benchmark datasets, ImagNet-1k, Places365 and PASCAL VOC, clearly demonstrate superior performance of the proposed DPN over state-of-the-arts. In particular, on the ImagNet-1k dataset, a shallow DPN surpasses the best ResNeXt-101(64x4d) with 26% smaller model size, 25% less computational cost and 8% lower memory consumption, and a deeper DPN (DPN-131) further pushes the state-of-the-art single model performance with about 2 times faster training speed. Experiments on the Places365 large-scale scene dataset, PASCAL VOC detection dataset, and PASCAL VOC segmentation dataset also demonstrate its consistently better performance than DenseNet, ResNet and the latest ResNeXt model over various applications.\n Abstract ,  PDF \n #430: Faster and Non-ergodic O(1/K) Stochastic Alternating Direction Method of Multipliers \n #431: A Probabilistic Framework for Nonlinearities in Stochastic Neural Networks \n Qinliang Su,  Xuejun Liao,  Lawrence Carin \n We present a probabilistic framework for nonlinearities, based on doubly truncated Gaussian distributions. By setting the truncation points appropriately, we are able to generate various types of nonlinearities within a unified framework, including sigmoid, tanh and ReLU, the most commonly used nonlinearities in neural networks. The framework readily integrates into existing stochastic neural networks (with hidden units characterized as random variables), allowing one for the first time to learn the nonlinearities alongside model weights in these networks. Extensive experiments demonstrate the performance improvements brought about by the proposed framework when integrated with the restricted Boltzmann machine (RBM), temporal RBM and the truncated Gaussian graphical model (TGGM).\n Abstract ,  PDF \n #432: DisTraL: Robust multitask reinforcement learning \n #433: Online Learning of Optimal Bidding Strategy in Repeated Multi-Commodity Auctions \n Sevi Baltaoglu,  Lang Tong,  Qing Zhao \n We study the online learning problem of a bidder who participates in repeated auctions. With the goal of maximizing his total T-period payoff, the bidder wants to determine the optimal allocation of his fixed budget among his bids for $K$ different goods at each period. As a bidding strategy, we propose a polynomial time algorithm, referred to as dynamic programming on discrete set (DPDS), which is inspired by the dynamic programming approach to Knapsack problems. We show that DPDS achieves the regret order of $O(\\sqrt{T\\log{T}})$. Also, by showing that the regret growth rate is lower bounded by $\\Omega(\\sqrt{T})$ for any bidding strategy, we conclude that DPDS algorithm is order optimal up to a $\\sqrt{\\log{T}}$ term. We also evaluate the performance of DPDS empirically in the context of virtual bidding in wholesale electricity markets by using historical data from the New York energy market.\n Abstract ,  PDF \n #434: Trimmed Density Ratio Estimation \n #435: Training recurrent networks to generate hypotheses about how the brain solves hard navigation problems \n Ingmar Kanitscheider,  Ila Fiete \n Self-localization during navigation with noisy sensors in an ambiguous world is computationally challenging, yet animals and humans excel at it. In robotics, Simultaneous Location and Mapping (SLAM) algorithms solve this problem though joint sequential probabilistic inference of their own coordinates and those of external spatial landmarks. We generate the first neural solution to the SLAM problem by training recurrent LSTM networks to perform a set of hard 2D navigation tasks that include generalization to completely novel trajectories and environments. The hidden unit representations exhibit several key properties of hippocampal place cells, including stable tuning curves that remap between environments. Our result is also a proof of concept for end-to-end-learning of a SLAM algorithm using recurrent networks, and a demonstration of why this approach may have some advantages for robotic SLAM.\n Abstract ,  PDF \n #436: Visual Interaction Networks \n Nicholas Watters,  Andrea Tacchetti,  Theophane Weber,  Razvan Pascanu,  Peter Battaglia,  Daniel Zoran \n From just a glance, humans can make rich predictions about the future state of a wide range of physical systems. On the other hand, modern approaches from engineering, robotics, and graphics are often restricted to narrow domains and require direct measurements of the underlying states. We introduce the Visual Interaction Network, a general-purpose model for learning the dynamics of a physical system from raw visual observations. Our model consists of a perceptual front-end based on convolutional neural networks and a dynamics predictor based on interaction networks. Through joint training, the perceptual front-end learns to parse a dynamic visual scene into a set of factored latent object representations. The dynamics predictor learns to roll these states forward in time by computing their interactions and dynamics, producing a predicted physical trajectory of arbitrary length. We found that from just six input video frames the Visual Interaction Network can generate accurate future trajectories of hundreds of time steps on a wide range of physical systems. Our model can also be applied to scenes with invisible objects, inferring their future states from their effects on the visible objects, and can implicitly infer the unknown mass of objects. Our results demonstrate that the perceptual module and the object-based dynamics predictor module can induce factored latent representations that support accurate dynamical predictions. This work opens new opportunities for model-based decision-making and planning from raw sensory observations in complex physical environments.\n Abstract ,  PDF \n #437: Reconstruct & Crush Network \n Huijie Yang,  Wenxu Wang,  Tao Zhou,  Binghong ang,  Fangcui Zhao \n A number of recent works have concentrated on a few statistical properties of complex networks, such as the clustering, the right-skewed degree distribution and the community, which are common to many real world networks. In this paper, we address the hierarchy property sharing among a large amount of networks. Based upon the eigenvector centrality (EC) measure, a method is proposed to reconstruct the hierarchical structure of a complex network. It is tested on the Santa Fe Institute collaboration network, whose structure is well known. We also apply it to a Mathematicians\' collaboration network and the protein interaction network of Yeast. The method can detect significantly hierarchical structures in these networks.\n Abstract ,  PDF \n #438: Streaming Robust Submodular Maximization:A Partitioned Thresholding Approach \n #439: Simple strategies for recovering inner products from coarsely quantized random projections \n #440: Discovering Potential Influence via Information Bottleneck \n #441: Doubly Stochastic Variational Inference for Deep Gaussian Processes \n Hugh Salimbeni,  Marc Deisenroth \n Gaussian processes (GPs) are a good choice for function approximation as they are flexible, robust to over-fitting, and provide well-calibrated predictive uncertainty. Deep Gaussian processes (DGPs) are multi-layer generalisations of GPs, but inference in these models has proved challenging. Existing approaches to inference in DGP models assume approximate posteriors that force independence between the layers, and do not work well in practice. We present a doubly stochastic variational inference algorithm, which does not force independence between layers. With our method of inference we demonstrate that a DGP model can be used effectively on data ranging in size from hundreds to a billion points. We provide strong empirical evidence that our inference scheme for DGPs works well in practice in both classification and regression.\n Abstract ,  PDF \n #442: Ranking Data with Continuous Labels through Oriented Recursive Partitions \n #443: Scalable Model Selection for Belief Networks \n #444: Targeting EEG/LFP Synchrony with Neural Nets \n #445: Near-Optimal Edge Evaluation in Explicit Generalized Binomial Graphs \n Sanjiban Choudhury,  Shervin Javdani,  Siddhartha Srinivasa,  Sebastian Scherer \n Robotic motion-planning problems, such as a UAV flying fast in a partially-known environment or a robot arm moving around cluttered objects, require finding collision-free paths quickly. Typically, this is solved by constructing a graph, where vertices represent robot configurations and edges represent potentially valid movements of the robot between these configurations. The main computational bottlenecks are expensive edge evaluations to check for collisions. State of the art planning methods do not reason about the optimal sequence of edges to evaluate in order to find a collision free path quickly. In this paper, we do so by drawing a novel equivalence between motion planning and the Bayesian active learning paradigm of decision region determination (DRD). Unfortunately, a straight application of existing methods requires computation exponential in the number of edges in a graph. We present BISECT, an efficient and near-optimal algorithm to solve the DRD problem when edges are independent Bernoulli random variables. By leveraging this property, we are able to significantly reduce computational complexity from exponential to linear in the number of edges. We show that BISECT outperforms several state of the art algorithms on a spectrum of planning problems for mobile robots, manipulators, and real flight data collected from a full scale helicopter.\n Abstract ,  PDF \n #446: Non-Stationary Spectral Kernels \n Sami Remes,  Markus Heinonen,  Samuel Kaski \n We propose non-stationary spectral kernels for Gaussian process regression. We propose to model the spectral density of a non-stationary kernel function as a mixture of input-dependent Gaussian process frequency density surfaces. We solve the generalised Fourier transform with such a model, and present a family of non-stationary and non-monotonic kernels that can learn input-dependent and potentially long-range, non-monotonic covariances between inputs. We derive efficient inference using model whitening and marginalized posterior, and show with case studies that these kernels are necessary when modelling even rather simple time series, image or geospatial data with non-stationary characteristics.\n Abstract ,  PDF \n #447: Overcoming Catastrophic Forgetting by Incremental Moment Matching \n Sang-Woo Lee,  Jin-Hwa Kim,  Jaehyun Jun,  Jung-Woo Ha,  Byoung-Tak Zhang \n Catastrophic forgetting is a problem of neural networks that loses the information of the first task after training the second task. Here, we propose incremental moment matching (IMM) to resolve this problem. IMM incrementally matches the moment of the posterior distribution of neural networks, which is trained for the first and the second task, respectively. To make the search space of posterior parameter smooth, the IMM procedure is complemented by various transfer learning techniques including weight transfer, L2-norm of the old and the new parameter, and a variant of dropout with the old parameter. We analyze our approach on various datasets including the MNIST, CIFAR-10, Caltech-UCSD-Birds, and Lifelog datasets. Experimental results show that IMM achieves state-of-the-art performance in a variety of datasets and can balance the information between an old and a new network.\n Abstract ,  PDF \n #448: Balancing information exposure in social networks \n #449: SafetyNets: Verifiable Execution of Deep Neural Networks on an Untrusted Cloud \n Zahra Ghodsi,  Tianyu Gu,  Siddharth Garg \n Inference using deep neural networks is often outsourced to the cloud since it is a computationally demanding task. However, this raises a fundamental issue of trust. How can a client be sure that the cloud has performed inference correctly? A lazy cloud provider might use a simpler but less accurate model to reduce its own computational load, or worse, maliciously modify the inference results sent to the client. We propose SafetyNets, a framework that enables an untrusted server (the cloud) to provide a client with a short mathematical proof of the correctness of inference tasks that they perform on behalf of the client. Specifically, SafetyNets develops and implements a specialized interactive proof (IP) protocol for verifiable execution of a class of deep neural networks, i.e., those that can be represented as arithmetic circuits. Our empirical results on three- and four-layer deep neural networks demonstrate the run-time costs of SafetyNets for both the client and server are low. SafetyNets detects any incorrect computations of the neural network by the untrusted server with high probability, while achieving state-of-the-art accuracy on the MNIST digit recognition (99.4%) and TIMIT speech recognition tasks (75.22%).\n Abstract ,  PDF \n #450: Query Complexity of Clustering with Side Information \n Arya Mazumdar,  Barna Saha \n Suppose, we are given a set of $n$ elements to be clustered into $k$ (unknown) clusters, and an oracle/expert labeler that can interactively answer pair-wise queries of the form, "do two elements $u$ and $v$ belong to the same cluster?". The goal is to recover the optimum clustering by asking the minimum number of queries. In this paper, we initiate a rigorous theoretical study of this basic problem of query complexity of interactive clustering, and provide strong information theoretic lower bounds, as well as nearly matching upper bounds. Most clustering problems come with a similarity matrix, which is used by an automated process to cluster similar points together. Our main contribution in this paper is to show the dramatic power of side information aka similarity matrix on reducing the query complexity of clustering. A similarity matrix represents noisy pair-wise relationships such as one computed by some function on attributes of the elements. A natural noisy model is where similarity values are drawn independently from some arbitrary probability distribution $f_+$ when the underlying pair of elements belong to the same cluster, and from some $f_-$ otherwise. We show that given such a similarity matrix, the query complexity reduces drastically from $\\Theta(nk)$ (no similarity matrix) to $O(\\frac{k^2\\log{n}}{\\cH^2(f_+\\|f_-)})$ where $\\cH^2$ denotes the squared Hellinger divergence. Moreover, this is also information-theoretic optimal within an $O(\\log{n})$ factor. Our algorithms are all efficient, and parameter free, i.e., they work without any knowledge of $k, f_+$ and $f_-$, and only depend logarithmically with $n$. Along the way, our work also reveals intriguing connection to popular community detection models such as the {\\em stochastic block model}, significantly generalizes them, and opens up many venues for interesting future research.\n Abstract ,  PDF \n #451: QMDP-Net: Deep Learning for Planning under Partial Observability \n Peter Karkus,  David Hsu,  Wee Sun Lee \n This paper introduces the QMDP-net, a neural network architecture for planning under partial observability. The QMDP-net combines the strengths of model-free learning and model-based planning. It is a recurrent policy network, but it represents a policy by connecting a model with a planning algorithm that solves the model, thus embedding the solution structure of planning in a network learning architecture. The QMDP-net is fully differentiable and allows end-to-end training. We train a QMDP-net in a set of different environments so that it can generalize over new ones and "transfer" to larger environments as well. In preliminary experiments, QMDP-net showed strong performance on several robotic tasks in simulation. Interestingly, while QMDP-net encodes the QMDP algorithm, it sometimes outperforms the QMDP algorithm in the experiments, because of QMDP-net\'s increased robustness through end-to-end learning.\n Abstract ,  PDF \n #452: Robust Optimization for Non-Convex Objectives \n Robert Chen,  Brendan Lucier,  Yaron Singer,  Vasilis Syrgkanis \n We consider robust optimization problems, where the goal is to optimize in the worst case over a class of objective functions. We develop a reduction from robust improper optimization to Bayesian optimization: given an oracle that returns $\\alpha$-approximate solutions for distributions over objectives, we compute a distribution over solutions that is $\\alpha$-approximate in the worst case. We show that de-randomizing this solution is NP-hard in general, but can be done for a broad class of statistical learning tasks. We apply our results to robust neural network training and submodular optimization. We evaluate our approach experimentally on corrupted character classification, and robust influence maximization in networks.\n Abstract ,  PDF \n #453: Thy Friend is My Friend: Iterative Collaborative Filtering for Sparse Matrix Estimation \n #454: Adaptive Classification for Prediction Under a Budget \n Feng Nan,  Venkatesh Saligrama \n We propose a novel adaptive approximation approach for test-time resource-constrained prediction. Given an input instance at test-time, a gating function identifies a prediction model for the input among a collection of models. Our objective is to minimize overall average cost without sacrificing accuracy. We learn gating and prediction models on fully labeled training data by means of a bottom-up strategy. Our novel bottom-up method first trains a high-accuracy complex model. Then a low-complexity gating and prediction model are subsequently learned to adaptively approximate the high-accuracy model in regions where low-cost models are capable of making highly accurate predictions. We pose an empirical loss minimization problem with cost constraints to jointly train gating and prediction models. On a number of benchmark datasets our method outperforms state-of-the-art achieving higher accuracy for the same cost.\n Abstract ,  PDF \n #455: Convergence rates of a partition based Bayesian multivariate density estimation method \n #456: Affine-Invariant Online Optimization \n #457: Beyond Worst-case: A Probabilistic Analysis of Affine Policies in Dynamic Optimization \n Omar El Housni,  Vineet Goyal \n Affine policies (or control) are widely used as a solution approach in dynamic optimization where computing an optimal adjustable solution is usually intractable. While the worst case performance of affine policies can be significantly bad, the empirical performance is observed to be near-optimal for a large class of problem instances. For instance, in the two-stage dynamic robust optimization problem with linear covering constraints and uncertain right hand side, the worst-case approximation bound for affine policies is $O(\\sqrt m)$ that is also tight (see Bertsimas and Goyal (2012)), whereas observed empirical performance is near-optimal. In this paper, we aim to address this stark-contrast between the worst-case and the empirical performance of affine policies. In particular, we show that affine policies give a good approximation for the two-stage adjustable robust optimization problem with high probability on random instances where the constraint coefficients are generated i.i.d. from a large class of distributions; thereby, providing a theoretical justification of the observed empirical performance. On the other hand, we also present a distribution such that the performance bound for affine policies on instances generated according to that distribution is $\\Omega(\\sqrt m)$ with high probability; however, the constraint coefficients are not i.i.d.. This demonstrates that the empirical performance of affine policies can depend on the generative model for instances.\n Abstract ,  PDF \n #458: A unified approach to interpreting model predictions \n Scott Lundberg,  Su-In Lee \n Understanding why a model made a certain prediction is crucial in many applications. However, with large modern datasets the best accuracy is often achieved by complex models even experts struggle to interpret, such as ensemble or deep learning models. This creates a tension between accuracy and interpretability. In response, a variety of methods have recently been proposed to help users interpret the predictions of complex models. Here, we present a unified framework for interpreting predictions, namely SHAP (SHapley Additive exPlanations, which assigns each feature an importance for a particular prediction. The key novel components of the SHAP framework are the identification of a class of additive feature importance measures and theoretical results that there is a unique solution in this class with a set of desired properties. This class unifies six existing methods, and several recent methods in this class do not have these desired properties. This means that our framework can inform the development of new methods for explaining prediction models. We demonstrate that several new methods we presented in this paper based on the SHAP framework show better computational performance and better consistency with human intuition than existing methods.\n Abstract ,  PDF \n #459: Stochastic Approximation for Canonical Correlation Analysis \n Raman Arora,  Teodor V. Marinov,  Poorya Mianjy \n We study canonical correlation analysis (CCA) as a stochastic optimization problem. We show that regularized CCA is efficiently PAC-learnable. We give stochastic approximation (SA) algorithms that are instances of stochastic mirror descent, which achieve $\\epsilon$-suboptimality in the population objective in time $\\operatorname{poly}(\\frac{1}{\\epsilon},\\frac{1}{\\delta},d)$ with probability $1-\\delta$, where $d$ is the input dimensionality.\n Abstract ,  PDF \n #460: Investigating the learning dynamics of deep neural networks using random matrix theory \n #461: Sample and Computationally Efficient Learning Algorithms under S-Concave Distributions \n #462: Scalable Variational Inference for Dynamical Systems \n Nico S. Gorbach,  Stefan Bauer,  Joachim M. Buhmann \n Gradient matching is a promising tool for learning parameters and state dynamics of ordinary differential equations. It is a grid free inference approach which for fully observable systems is at times competitive with numerical integration. However for many real-world applications, only sparse observations are available or even unobserved variables are included in the model description. In these cases most gradient matching methods are difficult to apply or simply do not provide satisfactory results. That is why despite the high computational cost numerical integration is still the gold standard in many applications. Using an existing gradient matching approach, we propose a scalable variational inference framework, which can infer states and parameters simultaneously, offers computational speedups, improved accuracy and works well even under model misspecifications in a partially observable system.\n Abstract ,  PDF \n #463: Context Selection for Embedding Models \n #464: Working hard to know your neighbor\'s margins: Local descriptor learning loss \n Anastasiya Mishchuk,  Dmytro Mishkin,  Filip Radenovic,  Jiri Matas \n We introduce a novel loss for learning local feature descriptors which is inspired by the Lowe\'s matching criterion for SIFT. We show that the proposed loss that maximizes the distance between the closest positive and closest negative patch in the batch is better than complex regularization methods; it works well for both shallow and deep convolution network architectures. Applying the novel loss to the L2Net CNN architecture results in a compact descriptor -- it has the same dimensionality as SIFT (128) that shows state-of-art performance in wide baseline stereo, patch verification and instance retrieval benchmarks. It is fast, computing a descriptor takes about 1 millisecond on a low-end GPU.\n Abstract ,  PDF \n #465: Accelerated Stochastic Greedy Coordinate Descent by Soft Thresholding Projection onto Simplex \n #466: Multi-Task Learning for Contextual Bandits \n Aniket Anand Deshmukh,  Urun Dogan,  Clayton Scott \n Contextual bandits are a form of multi-armed bandit in which the agent has access to predictive side information (known as the context) for each arm at each time step, and have been used to model personalized news recommendation, ad placement, and other applications. In this work, we propose a multi-task learning framework for contextual bandit problems. Like multi-task learning in the batch setting, the goal is to leverage similarities in contexts for different arms so as to improve the agent\'s ability to predict rewards from contexts. We propose an upper confidence bound-based multi-task learning algorithm for contextual bandits, establish a corresponding regret bound, and interpret this bound to quantify the advantages of learning in the presence of high task (arm) similarity. We also describe an effective scheme for estimating task similarity from data, and demonstrate our algorithm\'s performance on several data sets.\n Abstract ,  PDF \n #467: Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon \n Xin Dong,  Shangyu Chen,  Sinno Jialin Pan \n How to develop slim and accurate deep neural networks has become crucial for real- world applications, especially for those employed in embedded systems. Though previous work along this research line has shown some promising results, most existing methods either fail to significantly compress a well-trained deep network or require a heavy retraining process for the pruned deep network to re-boost its prediction performance. In this paper, we propose a new layer-wise pruning method for deep neural networks. In our proposed method, parameters of each individual layer are pruned independently based on second order derivatives of a layer-wise error function with respect to the corresponding parameters. We prove that the final prediction performance drop after pruning is bounded by a linear combination of the reconstructed errors caused at each layer. Therefore, there is a guarantee that one only needs to perform a light retraining process on the pruned network to resume its original prediction performance. We conduct extensive experiments on benchmark datasets to demonstrate the effectiveness of our pruning method compared with several state-of-the-art baseline methods.\n Abstract ,  PDF \n #468: Accelerated First-order Methods for Geodesically Convex Optimization on Riemannian Manifolds \n #469: Selective Classification for Deep Neural Networks \n Yonatan Geifman,  Ran El-Yaniv \n Selective classification techniques (also known as reject option) have not yet been considered in the context of deep neural networks (DNNs). These techniques can potentially significantly improve DNNs prediction performance by trading-off coverage. In this paper we propose a method to construct a selective classifier given a trained neural network. Our method allows a user to set a desired risk level. At test time, the classifier rejects instances as needed, to grant the desired risk (with high probability). Empirical results over CIFAR and ImageNet convincingly demonstrate the viability of our method, which opens up possibilities to operate DNNs in mission-critical applications. For example, using our method an unprecedented 2% error in top-5 ImageNet classification can be guaranteed with probability 99.9%, and almost 60% test coverage.\n Abstract ,  PDF \n #470: Minimax Estimation of Bandable Precision Matrices \n #471: Monte-Carlo Tree Search by Best Arm Identification \n Emilie Kaufmann (CNRS, CRIStAL, SEQUEL),  Wouter Koolen (CWI) \n Recent advances in bandit tools and techniques for sequential learning are steadily enabling new applications and are promising the resolution of a range of challenging related problems. We study the game tree search problem, where the goal is to quickly identify the optimal move in a given game tree by sequentially sampling its stochastic payoffs. We develop new algorithms for trees of arbitrary depth, that operate by summarizing all deeper levels of the tree into confidence intervals at depth one, and applying a best arm identification procedure at the root. We prove new sample complexity guarantees with a refined dependence on the problem instance. We show experimentally that our algorithms outperform existing elimination-based algorithms and match previous special-purpose methods for depth-two trees.\n Abstract ,  PDF \n #472: Group Additive Structure Identification for Kernel Nonparametric Regression \n #473: Fast, Sample-Efficient Algorithms for Structured Phase Retrieval \n #474: Hash Embeddings for Efficient Word Representations \n Dan Svenstrup,  Jonas Meinertz Hansen,  Ole Winther \n We present hash embeddings, an efficient method for representing words in a continuous vector form. A hash embedding may be seen as an interpolation between a standard word embedding and a word embedding created using a random hash function (the hashing trick). In hash embeddings each token is represented by $k$ $d$-dimensional embeddings vectors and one $k$ dimensional weight vector. The final $d$ dimensional representation of the token is the product of the two. Rather than fitting the embedding vectors for each token these are selected by the hashing trick from a shared pool of $B$ embedding vectors. Our experiments show that hash embeddings can easily deal with huge vocabularies consisting of millions of tokens. When using a hash embedding there is no need to create a dictionary before training nor to perform any kind of vocabulary pruning after training. We show that models trained using hash embeddings exhibit at least the same level of performance as models trained using regular embeddings across a wide range of tasks. Furthermore, the number of parameters needed by such an embedding is only a fraction of what is required by a regular embedding. Since standard embeddings and embeddings constructed using the hashing trick are actually just special cases of a hash embedding, hash embeddings can be considered an extension and improvement over the existing regular embedding types.\n Abstract ,  PDF \n #475: Online Learning for Multivariate Hawkes Processes \n #476: Maximum Margin Interval Trees \n #477: DropoutNet: Addressing Cold Start in Recommender Systems \n #478: A simple neural network module for relational reasoning \n Adam Santoro,  David Raposo,  David G.T. Barrett,  Mateusz Malinowski,  Razvan Pascanu,  Peter Battaglia,  Timothy Lillicrap \n Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations.\n Abstract ,  PDF \n #479: Q-LDA: Uncovering Latent Patterns in Text-based Sequential Decision Processes \n #480: Online Reinforcement Learning in Stochastic Games \n #481: Position-based Multiple-play Multi-armed Bandit Problem with Unknown Position Bias \n #482: Active Exploration for Learning Symbolic Representations \n Garrett Andersen,  George Konidaris \n We introduce an online active exploration algorithm for data-efficiently learning an abstract symbolic model of an environment. Our algorithm is divided into two parts: the first part quickly generates an intermediate Bayesian symbolic model from the data that the agent has collected so far, which the agent can then use along with the second part to guide its future exploration towards regions of the state space that the model is uncertain about. We show that our algorithm outperforms random and greedy exploration policies on two different computer game domains. The first domain is an Asteroids-inspired game with complex dynamics, but basic logical structure. The second is the Treasure Game, with simpler dynamics, but more complex logical structure.\n Abstract ,  PDF \n #483: Clone MCMC: Parallel High-Dimensional Gaussian Gibbs Sampling \n #484: Fair Clustering Through Fairlets \n #485: Polynomial time algorithms for dual volume sampling \n #486: Hindsight Experience Replay \n Marcin Andrychowicz,  Filip Wolski,  Alex Ray,  Jonas Schneider,  Rachel Fong,  Peter Welinder,  Bob McGrew,  Josh Tobin,  Pieter Abbeel,  Wojciech Zaremba \n Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.\n Abstract ,  PDF \n #487: Stochastic and Adversarial Online Learning without Hyperparameters \n #488: Teaching Machines to Describe Images with Natural Language Feedback \n #489: Perturbative Black Box Variational Inference \n Robert Bamler,  Cheng Zhang,  Manfred Opper,  Stephan Mandt \n Black box variational inference (BBVI) with reparameterization gradients triggered the exploration of divergence measures other than the Kullback-Leibler (KL) divergence, such as alpha divergences. These divergences can be tuned to be more mass-covering (preventing overfitting in complex models), but are also often harder to optimize using Monte-Carlo gradients. In this paper, we view BBVI with generalized divergences as a form of biased importance sampling. The choice of divergence determines a bias-variance tradeoff between the tightness of the bound (low bias) and the variance of its gradient estimators. Drawing on variational perturbation theory of statistical physics, we use these insights to construct a new variational bound which is tighter than the KL bound and more mass covering. Compared to alpha-divergences, its reparameterization gradients have a lower variance. We show in several experiments on Gaussian Processes and Variational Autoencoders that the resulting posterior covariances are closer to the true posterior and lead to higher likelihoods on held-out data.\n Abstract ,  PDF \n #490: GibbsNet: Iterative Adversarial Inference for Deep Graphical Models \n #491: PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space \n #492: Regularizing Deep Neural Networks by Noise: Its Interpretation and Optimization \n #493: Learning Graph Embeddings with Embedding Propagation \n #494: Efficient Modeling of Latent Information in Supervised Learning using Gaussian Processes \n Zhenwen Dai,  Mauricio A. Álvarez,  Neil D. Lawrence \n Often in machine learning, data are collected as a combination of multiple conditions, e.g., the voice recordings of multiple persons, each labeled with an ID. How could we build a model that captures the latent information related to these conditions and generalize to a new one with few data? We present a new model called Latent Variable Multiple Output Gaussian Processes (LVMOGP) and that allows to jointly model multiple conditions for regression and generalize to a new condition with a few data points at test time. LVMOGP infers the posteriors of Gaussian processes together with a latent space representing the information about different conditions. We derive an efficient variational inference method for LVMOGP, of which the computational complexity is as low as sparse Gaussian processes. We show that LVMOGP significantly outperforms related Gaussian process methods on various tasks with both synthetic and real data.\n Abstract ,  PDF \n #495: A-NICE-MC: Adversarial Training for MCMC \n Jiaming Song,  Shengjia Zhao,  Stefano Ermon \n Existing Markov Chain Monte Carlo (MCMC) methods are either based on general-purpose and domain-agnostic schemes which can lead to slow convergence, or hand-crafting of problem-specific proposals by an expert. We propose A-NICE-MC, a novel method to train flexible parametric Markov chain kernels to produce samples with desired properties. First, we propose an efficient likelihood-free adversarial training method to train a Markov chain and mimic a given data distribution. Then, we leverage flexible volume preserving flows to obtain parametric kernels for MCMC. Using a bootstrap approach, we show how to train efficient Markov chains to sample from a prescribed posterior distribution by iteratively improving the quality of both the model and the samples. A-NICE-MC provides the first framework to automatically design efficient domain-specific MCMC proposals. Empirical results demonstrate that A-NICE-MC combines the strong guarantees of MCMC with the expressiveness of deep neural networks, and is able to significantly outperform competing methods such as Hamiltonian Monte Carlo.\n Abstract ,  PDF \n #496: Excess Risk Bounds for the Bayes Risk using Variational Inference in Latent Gaussian Models \n #497: Real-Time Bidding with Side Information \n #498: Saliency-based Sequential Image Attention with Multiset Prediction \n #499: Variational Inference for Gaussian Process Models with Linear Complexity \n Chris Lloyd,  Tom Gunter,  Michael A. Osborne,  Stephen J. Roberts \n We present the first fully variational Bayesian inference scheme for continuous Gaussian-process-modulated Poisson processes. Such point processes are used in a variety of domains, including neuroscience, geo-statistics and astronomy, but their use is hindered by the computational cost of existing inference schemes. Our scheme: requires no discretisation of the domain; scales linearly in the number of observed events; and is many orders of magnitude faster than previous sampling based approaches. The resulting algorithm is shown to outperform standard methods on synthetic examples, coal mining disaster data and in the prediction of Malaria incidences in Kenya. \n #500: K-Medoids For K-Means Seeding \n James Newling,  François Fleuret \n We run experiments showing that algorithm clarans (Ng et al., 2005) finds better K-medoids solutions than the Voronoi iteration algorithm. This finding, along with the similarity between the Voronoi iteration algorithm and Lloyd\'s K-means algorithm, suggests that clarans may be an effective K-means initializer. We show that this is the case, with clarans outperforming other seeding algorithms on 23/23 datasets with a mean decrease over k-means++ of 30% for initialization mse and 3% or final mse. We describe how the complexity and runtime of clarans can be improved, making it a viable initialization scheme for large datasets.\n Abstract ,  PDF \n #501: Identifying Outlier Arms in Multi-Armed Bandit \n #502: Online Learning with Transductive Regret \n #503: Riemannian approach to batch normalization \n Minhyung Cho,  Jaehyung Lee \n Batch Normalization (BN) has proven to be an effective algorithm for deep neural network training by normalizing the input to each neuron and reducing the internal covariate shift. The space of weight vectors in the BN layer can be naturally interpreted as a Riemannian manifold, which is invariant to linear scaling of weights. Following the intrinsic geometry of this manifold provides a new learning rule that is more efficient and easier to analyze. We also propose intuitive and effective gradient clipping and regularization methods for the proposed algorithm by utilizing the geometry of the manifold. The resulting algorithm consistently outperforms the original BN on various types of network architectures and datasets. \n #504: Self-supervised Learning of Motion Capture \n #505: Triangle Generative Adversarial Networks \n Zhe Gan,  Liqun Chen,  Weiyao Wang,  Yunchen Pu,  Yizhe Zhang,  Hao Liu,  Chunyuan Li,  Lawrence Carin \n A Triangle Generative Adversarial Network ($\\Delta$-GAN) is developed for semi-supervised cross-domain joint distribution matching, where the training data consists of samples from each domain, and supervision of domain correspondence is provided by only a few paired samples. $\\Delta$-GAN consists of four neural networks, two generators and two discriminators. The generators are designed to learn the two-way conditional distributions between the two domains, while the discriminators implicitly define a ternary discriminative function, which is trained to distinguish real data pairs and two kinds of fake data pairs. The generators and discriminators are trained together using adversarial learning. Under mild assumptions, in theory the joint distributions characterized by the two generators concentrate to the data distribution. In experiments, three different kinds of domain pairs are considered, image-label, image-image and image-attribute pairs. Experiments on semi-supervised image classification, image-to-image translation and attribute-based image generation demonstrate the superiority of the proposed approach.\n Abstract ,  PDF \n #506: Preserving Proximity and Global Ranking for Node Embedding \n #507: Bayesian Optimization with Gradients \n Jian Wu,  Matthias Poloczek,  Andrew Gordon Wilson,  Peter I. Frazier \n In recent years, Bayesian optimization has proven successful for global optimization of expensive-to-evaluate multimodal objective functions. However, unlike most optimization methods, Bayesian optimization typically does not use derivative information. In this paper we show how Bayesian optimization can exploit derivative information to decrease the number of objective function evaluations required for good performance. In particular, we develop a novel Bayesian optimization algorithm, the derivative-enabled knowledge-gradient (dKG), for which we show one-step Bayes-optimality, asymptotic consistency, and greater one-step value of information than is possible in the derivative-free setting. Our procedure accommodates noisy and incomplete derivative information, and comes in both sequential and batch forms. We show dKG provides state-of-the-art performance compared to a wide range of optimization procedures with and without gradients, on benchmarks including logistic regression, kernel learning, and k-nearest neighbors.\n Abstract ,  PDF \n #508: Second-order Optimization in Deep Reinforcement Learning using Kronecker-factored Approximation \n #509: Renyi Differential Privacy Mechanisms for Posterior Sampling \n #510: Online Learning with a Hint \n #511: Identification of Gaussian Process State Space Models \n Stefanos Eleftheriadis,  Thomas F.W. Nicholson,  Marc Peter Deisenroth,  James Hensman \n The Gaussian process state space model (GPSSM) is a non-linear dynamical system, where unknown transition and/or measurement mappings are described by GPs. Most research in GPSSMs has focussed on the state estimation problem. However, the key challenge in GPSSMs has not been satisfactorily addressed yet: system identification. To address this challenge, we impose a structured Gaussian variational posterior distribution over the latent states, which is parameterised by a recognition model in the form of a bi-directional recurrent neural network. Inference with this structure allows us to recover a posterior smoothed over the entire sequence(s) of data. We provide a practical algorithm for efficiently computing a lower bound on the marginal likelihood using the reparameterisation trick. This additionally allows arbitrary kernels to be used within the GPSSM. We demonstrate that we can efficiently generate plausible future trajectories of the system we seek to model with the GPSSM, requiring only a small number of interactions with the true system.\n Abstract ,  PDF \n #512: Robust Imitation of Diverse Behaviors \n Ziyu Wang,  Josh Merel,  Scott Reed,  Greg Wayne,  Nando de Freitas,  Nicolas Heess \n Deep generative models have recently shown great promise in imitation learning for motor control. Given enough data, even supervised approaches can do one-shot imitation learning; however, they are vulnerable to cascading failures when the agent trajectory diverges from the demonstrations. Compared to purely supervised methods, Generative Adversarial Imitation Learning (GAIL) can learn more robust controllers from fewer demonstrations, but is inherently mode-seeking and more difficult to train. In this paper, we show how to combine the favourable aspects of these two approaches. The base of our model is a new type of variational autoencoder on demonstration trajectories that learns semantic policy embeddings. We show that these embeddings can be learned on a 9 DoF Jaco robot arm in reaching tasks, and then smoothly interpolated with a resulting smooth interpolation of reaching behavior. Leveraging these policy representations, we develop a new version of GAIL that (1) is much more robust than the purely-supervised controller, especially with few demonstrations, and (2) avoids mode collapse, capturing many diverse behaviors when GAIL on its own does not. We demonstrate our approach on learning diverse gaits from demonstration on a 2D biped and a 62 DoF 3D humanoid in the MuJoCo physics environment.\n Abstract ,  PDF \n #513: Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent \n Xiangru Lian,  Ce Zhang,  Huan Zhang,  Cho-Jui Hsieh,  Wei Zhang,  Ji Liu \n Most distributed machine learning systems nowadays, including TensorFlow and CNTK, are built in a centralized fashion. One bottleneck of centralized algorithms lies on high communication cost on the central node. Motivated by this, we ask, can decentralized algorithms be faster than its centralized counterpart? Although decentralized PSGD (D-PSGD) algorithms have been studied by the control community, existing analysis and theory do not show any advantage over centralized PSGD (C-PSGD) algorithms, simply assuming the application scenario where only the decentralized network is available. In this paper, we study a D-PSGD algorithm and provide the first theoretical analysis that indicates a regime in which decentralized algorithms might outperform centralized algorithms for distributed stochastic gradient descent. This is because D-PSGD has comparable total computational complexities to C-PSGD but requires much less communication cost on the busiest node. We further conduct an empirical study to validate our theoretical analysis across multiple frameworks (CNTK and Torch), different network configurations, and computation platforms up to 112 GPUs. On network configurations with low bandwidth or high latency, D-PSGD can be up to one order of magnitude faster than its well-optimized centralized counterparts.\n Abstract ,  PDF \n #514: Local Aggregative Games \n #515: A Sample Complexity Measure with Applications to Learning Optimal Auctions \n Vasilis Syrgkanis \n We introduce a new sample complexity measure, which we refer to as split-sample growth rate. For any hypothesis $H$ and for any sample $S$ of size $m$, the split-sample growth rate $\\hat{\\tau}_H(m)$ counts how many different hypotheses can empirical risk minimization output on any sub-sample of $S$ of size $m/2$. We show that the expected generalization error is upper bounded by $O\\left(\\sqrt{\\frac{\\log(\\hat{\\tau}_H(2m))}{m}}\\right)$. Our result is enabled by a strengthening of the Rademacher complexity analysis of the expected generalization error. We show that this sample complexity measure, greatly simplifies the analysis of the sample complexity of optimal auction design, for many auction classes studied in the literature. Their sample complexity can be derived solely by noticing that in these auction classes, ERM on any sample or sub-sample will pick parameters that are equal to one of the points in the sample.\n Abstract ,  PDF \n #516: Thinking Fast and Slow with Deep Learning and Tree Search \n Thomas Anthony,  Zheng Tian,  David Barber \n Solving sequential decision making problems, such as text parsing, robotic control, and game playing, requires a combination of planning policies and generalisation of those plans. In this paper, we present Expert Iteration, a novel algorithm which decomposes the problem into separate planning and generalisation tasks. Planning new policies is performed by tree search, while a deep neural network generalises those plans. In contrast, standard Deep Reinforcement Learning algorithms rely on a neural network not only to generalise plans, but to discover them too. We show that our method substantially outperforms Policy Gradients in the board game Hex, winning 84.4% of games against it when trained for equal time.\n Abstract ,  PDF \n #517: EEG-GRAPH: A Factor Graph Based Model for Capturing Spatial, Temporal, and Observational Relationships in Electroencephalograms \n #518: Improving the Expected Improvement Algorithm \n Chao Qin,  Diego Klabjan,  Daniel Russo \n The expected improvement (EI) algorithm is a popular strategy for information collection in optimization under uncertainty. The algorithm is widely known to be too greedy, but nevertheless enjoys wide use due to its simplicity and ability to handle uncertainty and noise in a coherent decision theoretic framework. To provide rigorous insight into EI, we study its properties in a simple setting of Bayesian optimization where the domain consists of a finite grid of points. This is the so-called best-arm identification problem, where the goal is to allocate measurement effort wisely to confidently identify the best arm using a small number of measurements. In this framework, one can show formally that EI is far from optimal. To overcome this shortcoming, we introduce a simple modification of the expected improvement algorithm. Surprisingly, this simple change results in an algorithm that is asymptotically optimal for Gaussian best-arm identification problems, and provably outperforms standard EI by an order of magnitude.\n Abstract ,  PDF \n #519: Hybrid Reward Architecture for Reinforcement Learning \n Harm van Seijen,  Mehdi Fatemi,  Joshua Romoff,  Romain Laroche,  Tavian Barnes,  Jeffrey Tsang \n One of the main challenges in reinforcement learning (RL) is generalisation. In typical deep RL methods this is achieved by approximating the optimal value function with a low-dimensional representation using a deep network. While this approach works well in many domains, in domains where the optimal value function cannot easily be reduced to a low-dimensional representation, learning can be very slow and unstable. This paper contributes towards tackling such challenging domains, by proposing a new method, called Hybrid Reward Architecture (HRA). HRA takes as input a decomposed reward function and learns a separate value function for each component reward function. Because each component typically only depends on a subset of all features, the overall value function is much smoother and can be easier approximated by a low-dimensional representation, enabling more effective learning. We demonstrate HRA on a toy-problem and the Atari game Ms. Pac-Man, where HRA achieves above-human performance.\n Abstract ,  PDF \n #520: Approximate Supermodularity Bounds for Experimental Design \n #521: Maximizing Subset Accuracy with Recurrent Neural Networks in Multi-label Classification \n #522: AdaGAN: Boosting Generative Models \n Ilya Tolstikhin,  Sylvain Gelly,  Olivier Bousquet,  Carl-Johann Simon-Gabriel,  Bernhard Schölkopf \n Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) are an effective method for training generative models of complex data such as natural images. However, they are notoriously hard to train and can suffer from the problem of missing modes where the model is not able to produce examples in certain regions of the space. We propose an iterative procedure, called AdaGAN, where at every step we add a new component into a mixture model by running a GAN algorithm on a reweighted sample. This is inspired by boosting algorithms, where many potentially weak individual predictors are greedily aggregated to form a strong composite predictor. We prove that such an incremental procedure leads to convergence to the true distribution in a finite number of steps if each step is optimal, and convergence at an exponential rate otherwise. We also illustrate experimentally that this procedure addresses the problem of missing modes.\n Abstract ,  PDF \n #523: Straggler Mitigation in Distributed Optimization Through Data Encoding \n #524: Multi-View Decision Processes \n #525: A Greedy Approach for Budgeted Maximum Inner Product Search \n Hsiang-Fu Yu,  Cho-Jui Hsieh,  Qi Lei,  Inderjit S. Dhillon \n Maximum Inner Product Search (MIPS) is an important task in many machine learning applications such as the prediction phase of a low-rank matrix factorization model for a recommender system. There have been some works on how to perform MIPS in sub-linear time recently. However, most of them do not have the flexibility to control the trade-off between search efficient and search quality. In this paper, we study the MIPS problem with a computational budget. By carefully studying the problem structure of MIPS, we develop a novel Greedy-MIPS algorithm, which can handle budgeted MIPS by design. While simple and intuitive, Greedy-MIPS yields surprisingly superior performance compared to state-of-the-art approaches. As a specific example, on a candidate set containing half a million vectors of dimension 200, Greedy-MIPS runs 200x faster than the naive approach while yielding search results with the top-5 precision greater than 75\\%.\n Abstract ,  PDF \n #526: SVD-Softmax: Fast Softmax Approximation on Large Vocabulary Neural Networks \n #527: Plan, Attend, Generate: Planning for Sequence-to-Sequence Models \n #528: Task-based End-to-end Model Learning in Stochastic Optimization \n #529: Towards Understanding Adversarial Learning for Joint Distribution Matching \n Chunyuan Li,  Hao Liu,  Changyou Chen,  Yunchen Pu,  Liqun Chen,  Ricardo Henao,  Lawrence Carin \n We investigate the non-identifiability issues associated with bidirectional adversarial training for joint distribution matching. Within a framework of conditional entropy, we propose both adversarial and non-adversarial approaches to learn desirable matched joint distributions for unsupervised and supervised tasks. We unify a broad family of adversarial models as joint distribution matching problems. Our approach stabilizes learning of unsupervised bidirectional adversarial learning methods. Further, we introduce an extension for semi-supervised learning tasks. Theoretical results are validated in synthetic data and real-world applications.\n Abstract ,  PDF \n #530: Finite sample analysis of the GTD Policy Evaluation Algorithms in Markov Setting \n #531: On the Complexity of Learning Neural Networks \n Le Song,  Santosh Vempala,  John Wilmes,  Bo Xie \n The stunning empirical successes of neural networks currently lack rigorous theoretical explanation. What form would such an explanation take, in the face of existing complexity-theoretic lower bounds? A first step might be to show that data generated by neural networks with a single hidden layer, smooth activation functions and benign input distributions can be learned efficiently. We demonstrate here a comprehensive lower bound ruling out this possibility: for a wide class of activation functions (including all currently used), and inputs drawn from any logconcave distribution, there is a family of one-hidden-layer functions whose output is a sum gate, that are hard to learn in a precise sense: any statistical query algorithm (which includes all known variants of stochastic gradient descent with any loss function) needs an exponential number of queries even using tolerance inversely proportional to the input dimensionality. Moreover, this hard family of functions is realizable with a small (sublinear in dimension) number of activation units in the single hidden layer. The lower bound is also robust to small perturbations of the true weights. Systematic experiments illustrate a phase transition in the training error as predicted by the analysis.\n Abstract ,  PDF \n #532: Hierarchical Implicit Models and Likelihood-Free Variational Inference \n #533: Improved Semi-supervised Learning with GANs using Manifold Invariances \n Abhishek Kumar,  Prasanna Sattigeri,  P. Thomas Fletcher \n Semi-supervised learning methods using Generative Adversarial Networks (GANs) have shown promising empirical success recently. Most of these methods use a shared discriminator/classifier which discriminates real examples from fake while also predicting the class label. Motivated by the ability of the GANs generator to capture the data manifold well, we propose to estimate the tangent space to the data manifold using GANs and employ it to inject invariances into the classifier. In the process, we propose enhancements over existing methods for learning the inverse mapping (i.e., the encoder) which greatly improves in terms of semantic similarity of the reconstructed sample with the input sample. We observe considerable empirical gains in semi-supervised learning over baselines, particularly in the cases when the number of labeled examples is low. We also provide insights into how fake examples influence the semi-supervised learning procedure.\n Abstract ,  PDF \n #534: Approximation and Convergence Properties of Generative Adversarial Learning \n Shuang Liu,  Olivier Bousquet,  Kamalika Chaudhuri \n Generative adversarial networks (GAN) approximate a target data distribution by jointly optimizing an objective function through a "two-player game" between a generator and a discriminator. Despite their empirical success, however, two very basic questions on how well they can approximate the target distribution remain unanswered. First, it is not known how restricting the discriminator family affects the approximation quality. Second, while a number of different objective functions have been proposed, we do not understand when convergence to the global minima of the objective function leads to convergence to the target distribution under various notions of distributional convergence. In this paper, we address these questions in a broad and unified setting by defining a notion of adversarial divergences that includes a number of recently proposed objective functions. We show that if the objective function is an adversarial divergence with some additional conditions, then using a restricted discriminator family has a moment-matching effect. Additionally, we show that for objective functions that are strict adversarial divergences, convergence in the objective function implies weak convergence, thus generalizing previous results.\n Abstract ,  PDF \n #535: From Bayesian Sparsity to Gated Recurrent Nets \n Hao He,  Bo Xin,  David Wipf \n The iterations of many first-order algorithms, when applied to minimizing common regularized regression functions, often resemble neural network layers with pre-specified weights. This observation has prompted the development of learning-based approaches that purport to replace these iterations with enhanced surrogates forged as DNN models from available training data. For example, important NP-hard sparse estimation problems have recently benefitted from this genre of upgrade, with simple feedforward or recurrent networks ousting proximal gradient-based iterations. Analogously, this paper demonstrates that more powerful Bayesian algorithms for promoting sparsity, which rely on complex multi-loop majorization-minimization techniques, mirror the structure of more sophisticated long short-term memory (LSTM) networks, or alternative gated feedback networks previously designed for sequence prediction. As part of this development, we examine the parallels between latent variable trajectories operating across multiple time-scales during optimization, and the activations within deep network structures designed to adaptively model such characteristic sequences. The resulting insights lead to a novel sparse estimation system that, when granted training data, can estimate optimal solutions efficiently in regimes where other algorithms fail, including practical direction-of-arrival (DOA) and 3D geometry recovery problems. The underlying principles we expose are also suggestive of a learning process for a richer class of multi-loop algorithms in other domains.\n Abstract ,  PDF \n #536: Min-Max Propagation \n #537: What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision? \n Alex Kendall,  Yarin Gal \n There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model -- uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.\n Abstract ,  PDF \n #538: Gradient descent GAN optimization is locally stable \n Vaishnavh Nagarajan,  J. Zico Kolter \n Despite their growing prominence, optimization in generative adversarial networks (GANs) is still a poorly-understood topic. In this paper, we analyze the "gradient descent" form of GAN optimization (i.e., the natural setting where we simultaneously take small gradient steps in both generator and discriminator parameters). We show that even though GAN optimization does not correspond to a convex-concave game, even for simple parameterizations, under proper conditions, equilibrium points of this optimization procedure are still locally asymptotically stable for the traditional GAN formulation. On the other hand, we show that the recently-proposed Wasserstein GAN can have non-convergent limit cycles near equilibrium. Motivated by this stability analysis, we propose an additional regularization term for gradient descent GAN updates, which is able to guarantee local stability for both the WGAN and for the traditional GAN, and also shows practical promise in speeding up convergence and addressing mode collapse.\n Abstract ,  PDF \n #539: Toward Robustness against Label Noise in Training Deep Discriminative Neural Networks \n Arash Vahdat \n Collecting large training datasets, annotated with high quality labels, is a costly process. This paper proposes a novel framework for training deep convolutional neural networks from noisy labeled datasets. The problem is formulated using an undirected graphical model that represents the relationship between noisy and clean labels, trained in a semi-supervised setting. In the proposed structure, the inference over latent clean labels is tractable and is regularized during training using auxiliary sources of information. The proposed model is applied to the image labeling problem and is shown to be effective in labeling unseen images as well as reducing label noise in training on CIFAR-10 and MS COCO datasets.\n Abstract ,  PDF \n #540: Dualing GANs \n Yujia Li,  Alexander Schwing,  Kuan-Chieh Wang,  Richard Zemel \n Generative adversarial nets (GANs) are a promising technique for modeling a distribution from samples. It is however well known that GAN training suffers from instability due to the nature of its maximin formulation. In this paper, we explore ways to tackle the instability problem by dualizing the discriminator. We start from linear discriminators in which case conjugate duality provides a mechanism to reformulate the saddle point objective into a maximization problem, such that both the generator and the discriminator of this \'dualing GAN\' act in concert. We then demonstrate how to extend this intuition to non-linear formulations. For GANs with linear discriminators our approach is able to remove the instability in training, while for GANs with nonlinear discriminators our approach provides an alternative to the commonly used GAN training algorithm.\n Abstract ,  PDF \n #541: Deep Learning for Precipitation Nowcasting: A Benchmark and A New Model \n Xingjian Shi,  Zhihan Gao,  Leonard Lausen,  Hao Wang,  Dit-Yan Yeung,  Wai-kin Wong,  Wang-chun Woo \n With the goal of making high-resolution forecasts of regional rainfall, precipitation nowcasting has become an important and fundamental technology underlying various public services ranging from rainstorm warnings to flight safety. Recently, the convolutional LSTM (ConvLSTM) model has been shown to outperform traditional optical flow based methods for precipitation nowcasting, suggesting that deep learning models have a huge potential for solving the problem. However, the convolutional recurrence structure in ConvLSTM-based models is location-invariant while natural motion and transformation (e.g., rotation) are location-variant in general. Furthermore, since deep-learning-based precipitation nowcasting is a newly emerging area, clear evaluation protocols have not yet been established. To address these problems, we propose both a new model and a benchmark for precipitation nowcasting. Specifically, we go beyond ConvLSTM and propose the Trajectory GRU (TrajGRU) model that can actively learn the location-variant structure for recurrent connections. Besides, we provide a benchmark that includes a real-world large-scale dataset from the Hong Kong Observatory, a new training loss, and a comprehensive evaluation protocol to facilitate future research and gauge the state of the art.\n Abstract ,  PDF \n #542: Do Deep Neural Networks Suffer from Crowding? \n Anna Volokitin,  Gemma Roig,  Tomaso Poggio \n Crowding is a visual effect suffered by humans, in which an object that can be recognized in isolation can no longer be recognized when other objects, called flankers, are placed close to it. In this work, we study the effect of crowding in artificial Deep Neural Networks for object recognition. We analyze both standard deep convolutional neural networks (DCNNs) as well as a new version of DCNNs which is 1) multi-scale and 2) with size of the convolution filters change depending on the eccentricity wrt to the center of fixation. Such networks, that we call eccentricity-dependent, are a computational model of the feedforward path of the primate visual cortex. Our results reveal that the eccentricity-dependent model, trained on target objects in isolation, can recognize such targets in the presence of flankers, if the targets are near the center of the image, whereas DCNNs cannot. Also, for all tested networks, when trained on targets in isolation, we find that recognition accuracy of the networks decreases the closer the flankers are to the target and the more flankers there are. We find that visual similarity between the target and flankers also plays a role and that pooling in early layers of the network leads to more crowding. Additionally, we show that incorporating the flankers into the images of the training set does not improve performance with crowding.\n Abstract ,  PDF \n #543: Learning from Complementary Labels \n Takashi Ishida,  Gang Niu,  Masashi Sugiyama \n Collecting labeled data is costly and thus is a critical bottleneck in real-world classification tasks. To mitigate the problem, we consider a complementary label, which specifies a class that a pattern does not belong to. Collecting complementary labels would be less laborious than ordinary labels since users do not have to carefully choose the correct class from many candidate classes. However, complementary labels are less informative than ordinary labels and thus a suitable approach is needed to better learn from complementary labels. In this paper, we show that an unbiased estimator of the classification risk can be obtained only from complementary labels, if a loss function satisfies a particular symmetric condition. We theoretically prove the estimation error bounds for the proposed method, and experimentally demonstrate the usefulness of the proposed algorithms.\n Abstract ,  PDF \n #544: More powerful and flexible rules for online FDR control with memory and weights \n #545: Learning from uncertain curves: The 2-Wasserstein metric for Gaussian processes \n #546: Discriminative State Space Models \n #547: On Fairness and Calibration \n Geoff Pleiss,  Manish Raghavan,  Felix Wu,  Jon Kleinberg,  Kilian Q. Weinberger \n The machine learning community has become increasingly concerned with the potential for bias and discrimination in predictive models, and this has motivated a growing line of work on what it means for a classification procedure to be "fair." In particular, we investigate the tension between minimizing error disparity across different population groups while maintaining calibrated probability estimates. We show that calibration is compatible only with a single error constraint (i.e. equal false-negatives rates across groups), and show that any algorithm that satisfies this relaxation is no better than randomizing a percentage of predictions for an existing classifier. These unsettling findings, which extend and generalize existing results, are empirically confirmed on several datasets.\n Abstract ,  PDF \n #548: Imagination-Augmented Agents for Deep Reinforcement Learning \n Théophane Weber,  Sébastien Racanière,  David P. Reichert,  Lars Buesing,  Arthur Guez,  Danilo Jimenez Rezende,  Adria Puigdomènech Badia,  Oriol Vinyals,  Nicolas Heess,  Yujia Li,  Razvan Pascanu,  Peter Battaglia,  David Silver,  Daan Wierstra \n We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.\n Abstract ,  PDF \n #549: Extracting low-dimensional dynamics from multiple large-scale neural population recordings by learning to predict correlations \n #550: Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning \n #551: Gradients of Generative Models for Improved Discriminative Analysis of Tandem Mass Spectra \n #552: Asynchronous Parallel Coordinate Minimization for MAP Inference \n #553: Multiscale Quantization for Fast Similarity Search \n #554: Diverse and Accurate Image Description Using a Variational Auto-Encoder with an Additive Gaussian Encoding Space \n #555: Improved Training of Wasserstein GANs \n Ishaan Gulrajani,  Faruk Ahmed,  Martin Arjovsky,  Vincent Dumoulin,  Aaron Courville \n Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but can still generate low-quality samples or fail to converge in some settings. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to pathological behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.\n Abstract ,  PDF \n #556: Optimally Learning Populations of Parameters \n Kevin Tian,  Weihao Kong,  Gregory Valiant \n Consider the following fundamental estimation problem: there are $n$ entities, each with an unknown parameter $p_i \\in [0,1]$, and we observe $n$ independent random variables, $X_1,\\ldots,X_n$, with $X_i \\sim $ Binomial$(t, p_i)$. How accurately can one recover the "histogram" (i.e. cumulative density function) of the $p_i$s? While the empirical estimates would recover the histogram to earth mover distance $\\Theta(\\frac{1}{\\sqrt{t}})$ (equivalently, $\\ell_1$ distance between the CDFs), we show that, provided $n$ is sufficiently large, we can achieve error $O(\\frac{1}{t})$ which is information theoretically optimal. We also extend our results to the multi-dimensional parameter case, capturing settings where each member of the population has multiple associated parameters. Beyond the theoretical results, we demonstrate that the recovery algorithm performs well in practice on a variety of datasets, providing illuminating insights into several domains, including politics, and sports analytics.\n Abstract ,  PDF \n #557: Clustering with Noisy Queries \n Arya Mazumdar,  Barna Saha \n In this paper, we initiate a rigorous theoretical study of clustering with noisy queries (or a faulty oracle). Given a set of $n$ elements, our goal is to recover the true clustering by asking minimum number of pairwise queries to an oracle. Oracle can answer queries of the form : "do elements $u$ and $v$ belong to the same cluster?" -- the queries can be asked interactively (adaptive queries), or non-adaptively up-front, but its answer can be erroneous with probability $p$. In this paper, we provide the first information theoretic lower bound on the number of queries for clustering with noisy oracle in both situations. We design novel algorithms that closely match this query complexity lower bound, even when the number of clusters is unknown. Moreover, we design computationally efficient algorithms both for the adaptive and non-adaptive settings. The problem captures/generalizes multiple application scenarios. It is directly motivated by the growing body of work that use crowdsourcing for {\\em entity resolution}, a fundamental and challenging data mining task aimed to identify all records in a database referring to the same entity. Here crowd represents the noisy oracle, and the number of queries directly relates to the cost of crowdsourcing. Another application comes from the problem of {\\em sign edge prediction} in social network, where social interactions can be both positive and negative, and one must identify the sign of all pair-wise interactions by querying a few pairs. Furthermore, clustering with noisy oracle is intimately connected to correlation clustering, leading to improvement therein. Finally, it introduces a new direction of study in the popular {\\em stochastic block model} where one has an incomplete stochastic block model matrix to recover the clusters.\n Abstract ,  PDF \n #558: Higher-Order Total Variation Classes on Grids: Minimax Theory and Trend Filtering Methods \n #559: Training Quantized Nets: A Deeper Understanding \n Hao Li,  Soham De,  Zheng Xu,  Christoph Studer,  Hanan Samet,  Tom Goldstein \n Currently, deep neural networks are deployed on low-power embedded devices by first training a full-precision model using powerful computing hardware, and then deriving a corresponding low-precision model for efficient inference on such systems. However, training models directly with coarsely quantized weights is a key step towards learning on embedded platforms that have limited computing resources, memory capacity, and power consumption. Numerous recent publications have studied methods for training quantized network, but these studies have mostly been empirical. In this work, we investigate training methods for quantized neural networks from a theoretical viewpoint. We first explore accuracy guarantees for training methods under convexity assumptions. We then look at the behavior of algorithms for non-convex problems, and we show that training algorithms that exploit high-precision representations have an important annealing property that purely quantized training methods lack, which explains many of the observed empirical differences between these types of algorithms.\n Abstract ,  PDF \n #560: Permutation-based Causal Inference Algorithms with Interventions \n Yuhao Wang,  Liam Solus,  Karren Dai Yang,  Caroline Uhler \n Learning Bayesian networks using both observational and interventional data is now a fundamentally important problem due to recent technological developments in genomics that generate such single-cell gene expression data at a very large scale. In order to utilize this data for learning gene regulatory networks, efficient and reliable causal inference algorithms are needed that can make use of both observational and interventional data. In this paper, we present two algorithms of this type and prove that both are consistent under the faithfulness assumption. These algorithms are interventional adaptations of the Greedy SP algorithm and are the first algorithms using both observational and interventional data with consistency guarantees. Moreover, these algorithms have the advantage that they are non-parametric, which makes them useful also for analyzing non-Gaussian data. In this paper, we present these two algorithms and their consistency guarantees, and we analyze their performance on simulated data, protein signaling data, and single-cell gene expression data.\n Abstract ,  PDF \n #561: Time-dependent spatially varying graphical models, with application to brain fMRI data analysis \n #562: Gradient Methods for Submodular Maximization \n Hamed Hassani,  Mahdi Soltanolkotabi,  Amin Karbasi \n In this paper, we study the problem of maximizing continuous submodular functions that naturally arise in many learning applications such as those involving utility functions in active learning and sensing, matrix approximations and network inference. Despite the apparent lack of convexity in such functions, we prove that stochastic projected gradient methods can provide strong approximation guarantees for maximizing continuous submodular functions with convex constraints. More specifically, we prove that for monotone continuous DR-submodular functions, all fixed points of projected gradient ascent provide a factor $1/2$ approximation to the global maxima. We also study stochastic gradient and mirror methods and show that after $\\mathcal{O}(1/\\epsilon^2)$ iterations these methods reach solutions which achieve in expectation objective values exceeding $(\\frac{\\text{OPT}}{2}-\\epsilon)$. An immediate application of our results is to maximize submodular functions that are defined stochastically, i.e. the submodular function is defined as an expectation over a family of submodular functions with an unknown distribution. We will show how stochastic gradient methods are naturally well-suited for this setting, leading to a factor $1/2$ approximation when the function is monotone. In particular, it allows us to approximately maximize discrete, monotone submodular optimization problems via projected gradient descent on a continuous relaxation, directly connecting the discrete and continuous domains. Finally, experiments on real data demonstrate that our projected gradient methods consistently achieve the best utility compared to other continuous baselines while remaining competitive in terms of computational effort.\n Abstract ,  PDF \n #563: Smooth Primal-Dual Coordinate Descent Algorithms for Nonsmooth Convex Optimization \n #564: Maximizing the Spread of Influence from Training Data \n #565: Multiplicative Weights Update with Constant Step-Size in Congestion Games:  Convergence, Limit Cycles and Chaos \n Gerasimos Palaiopanos,  Ioannis Panageas,  Georgios Piliouras \n The Multiplicative Weights Update (MWU) method is a ubiquitous meta-algorithm that works as follows: A distribution is maintained on a certain set, and at each step the probability assigned to element $\\gamma$ is multiplied by $(1 -\\epsilon C(\\gamma))>0$ where $C(\\gamma)$ is the "cost" of element $\\gamma$ and then rescaled to ensure that the new values form a distribution. We analyze MWU in congestion games where agents use \\textit{arbitrary admissible constants} as learning rates $\\epsilon$ and prove convergence to \\textit{exact Nash equilibria}. Our proof leverages a novel connection between MWU and the Baum-Welch algorithm, the standard instantiation of the Expectation-Maximization (EM) algorithm for hidden Markov models (HMM). Interestingly, this convergence result does not carry over to the nearly homologous MWU variant where at each step the probability assigned to element $\\gamma$ is multiplied by $(1 -\\epsilon)^{C(\\gamma)}$ even for the most innocuous case of two-agent, two-strategy load balancing games, where such dynamics can provably lead to limit cycles or even chaotic behavior.\n Abstract ,  PDF \n #566: Learning Neural Representations of Human Cognition across Many fMRI Studies \n #567: A KL-LUCB algorithm for Large-Scale Crowdsourcing \n #568: Collaborative Deep Learning in Fixed Topology Networks \n Zhanhong Jiang,  Aditya Balu,  Chinmay Hegde,  Soumik Sarkar \n There is significant recent interest to parallelize deep learning algorithms in order to handle the enormous growth in data and model sizes. While most advances focus on model parallelization and engaging multiple computing agents via using a central parameter server, aspect of data parallelization along with decentralized computation has not been explored sufficiently. In this context, this paper presents a new consensus-based distributed SGD (CDSGD) (and its momentum variant, CDMSGD) algorithm for collaborative deep learning over fixed topology networks that enables data parallelization as well as decentralized computation. Such a framework can be extremely useful for learning agents with access to only local/private data in a communication constrained environment. We analyze the convergence properties of the proposed algorithm with strongly convex and nonconvex objective functions with fixed and diminishing step sizes using concepts of Lyapunov function construction. We demonstrate the efficacy of our algorithms in comparison with the baseline centralized SGD and the recently proposed federated averaging algorithm (that also enables data parallelism) based on benchmark datasets such as MNIST, CIFAR-10 and CIFAR-100.\n Abstract ,  PDF \n #569: Fast-Slow Recurrent Neural Networks \n Asier Mujika,  Florian Meier,  Angelika Steger \n Processing sequential data of variable length is a major challenge in a wide range of applications, such as speech recognition, language modeling, generative image modeling and machine translation. Here, we address this challenge by proposing a novel recurrent neural network (RNN) architecture, the Fast-Slow RNN (FS-RNN). The FS-RNN incorporates the strengths of both multiscale RNNs and deep transition RNNs as it processes sequential data on different timescales and learns complex transition functions from one time step to the next. We evaluate the FS-RNN on two character level language modeling data sets, Penn Treebank and Hutter Prize Wikipedia, where we improve state of the art results to $1.19$ and $1.25$ bits-per-character (BPC), respectively. In addition, an ensemble of two FS-RNNs achieves $1.20$ BPC on Hutter Prize Wikipedia outperforming the best known compression algorithm with respect to the BPC measure. We also present an empirical investigation of the learning and network dynamics of the FS-RNN, which explains the improved performance compared to other RNN architectures. Our approach is general as any kind of RNN cell is a possible building block for the FS-RNN architecture, and thus can be flexibly applied to different tasks.\n Abstract ,  PDF \n #570: Learning Disentangled Representations with Semi-Supervised Deep Generative Models \n N. Siddharth,  Brooks Paige,  Jan-Willem Van de Meent,  Alban Desmaison,  Frank Wood,  Noah D. Goodman,  Pushmeet Kohli,  Philip H.S. Torr \n Variational autoencoders (VAEs) learn representations of data by jointly training a probabilistic encoder and decoder network. Typically these models encode all features of the data into a single variable. Here we are interested in learning disentangled representations that encode distinct aspects of the data into separate variables. We propose to learn such representations using model architectures that generalize from standard VAEs, employing a general graphical model structure in the encoder and decoder. This allows us to train partially-specified models that make relatively strong assumptions about a subset of interpretable variables and rely on the flexibility of neural networks to learn representations for the remaining variables. We further define a general objective for semi-supervised learning in this model class, which can be approximated using an importance sampling procedure. We evaluate our framework\'s ability to learn disentangled representations, both by qualitative exploration of its generative capacity, and quantitative evaluation of its discriminative ability on a variety of models and datasets.\n Abstract ,  PDF \n #571: Learning to Generalize Intrinsic Images with a Structured Disentangling Autoencoder \n #572: Exploring Generalization in Deep Learning \n Behnam Neyshabur,  Srinadh Bhojanapalli,  David McAllester,  Nathan Srebro \n With a goal of understanding what drives generalization in deep networks, we consider several recently suggested explanations, including norm-based control, sharpness and robustness. We study how these measures can ensure generalization, highlighting the importance of scale normalization, and making a connection between sharpness and PAC-Bayes theory. We then investigate how well the measures explain different observed phenomena.\n Abstract ,  PDF \n #573: A framework for Multi-A(rmed)/B(andit) Testing with Online FDR Control \n #574: Fader Networks: Generating Image Variations by Sliding Attribute Values \n #575: Action Centered Contextual Bandits \n #576: Estimating Mutual Information for Discrete-Continuous Mixtures \n Weihao Gao,  Sreeram Kannan,  Sewoong Oh,  Pramod Viswanath \n Estimating mutual information from observed samples is a basic primitive, useful in several machine learning tasks including correlation mining, information bottleneck clustering, learning a Chow-Liu tree, and conditional independence testing in (causal) graphical models. While mutual information is a well-defined quantity in general probability spaces, existing estimators can only handle two special cases of purely discrete or purely continuous pairs of random variables. The main challenge is that these methods first estimate the (differential) entropies of X, Y and the pair (X;Y) and add them up with appropriate signs to get an estimate of the mutual information. These 3H-estimators cannot be applied in general mixture spaces, where entropy is not well-defined. In this paper, we design a novel estimator for mutual information of discrete-continuous mixtures. We prove that the proposed estimator is consistent. We provide numerical experiments suggesting superiority of the proposed estimator compared to other heuristics of adding small continuous noise to all the samples and applying standard estimators tailored for purely continuous variables, and quantizing the samples and applying standard estimators tailored for purely discrete variables. This significantly widens the applicability of mutual information estimation in real-world applications, where some variables are discrete, some continuous, and others are a mixture between continuous and discrete components.\n Abstract ,  PDF \n #577: Attention is All you Need \n #578: Recurrent Ladder Networks \n Alexander Ilin,  Isabeau Prémont-Schwarz,  Tele Hotloo Hao,  Antti Rasmus,  Rinu Boney,  Harri Valpola \n We propose a recurrent extension of the Ladder networks whose structure is motivated by the inference required in hierarchical latent variable models. We demonstrate that the recurrent Ladder is able to handle a wide variety of complex learning tasks that benefit from iterative inference and temporal modeling. The architecture shows close-to-optimal results on temporal modeling of video data, competitive results on music modeling, and improved perceptual grouping based on higher order abstractions, such as stochastic textures and motion cues. We present results for fully supervised, semi-supervised, and unsupervised tasks. The results suggest that the proposed architecture and principles are powerful tools for learning a hierarchy of abstractions, learning iterative inference and handling temporal information.\n Abstract ,  PDF \n #579: Parameter-Free Online Learning via Model Selection \n #580: Bregman Divergence for Stochastic Variance Reduction: Saddle-Point and Adversarial Prediction \n #581: Unbounded cache model for online language modeling with open vocabulary \n #582: Predictive State Recurrent Neural Networks \n Carlton Downey,  Ahmed Hefny,  Boyue Li,  Byron Boots,  Geoffrey Gordon \n We present a new model, Predictive State Recurrent Neural Networks (PSRNNs), for filtering and prediction in dynamical systems. PSRNNs draw on insights from both Recurrent Neural Networks (RNNs) and Predictive State Representations (PSRs), and inherit advantages from both types of models. Like many successful RNN architectures, PSRNNs use (potentially deeply composed) bilinear transfer functions to combine information from multiple sources. We show that such bilinear functions arise naturally from state updates in Bayes filters like PSRs, in which observations can be viewed as gating belief states. We also show that PSRNNs can be learned effectively by combining Backpropogation Through Time (BPTT) with an initialization derived from a statistically consistent learning algorithm for PSRs called two-stage regression (2SR). Finally, we show that PSRNNs can be factorized using tensor decomposition, reducing model size and suggesting interesting connections to existing multiplicative architectures such as LSTMs. We applied PSRNNs to 4 datasets, and showed that we outperform several popular alternative approaches to modeling dynamical systems in all cases.\n Abstract ,  PDF \n #583: Early stopping for kernel boosting algorithms: A general analysis with localized complexities \n Yuting Wei,  Fanny Yang,  Martin J. Wainwright \n Early stopping of iterative algorithms is a widely-used form of regularization in statistics, commonly used in conjunction with boosting and related gradient-type algorithms. Although consistency results have been established in some settings, such estimators are less well-understood than their analogues based on penalized regularization. In this paper, for a relatively broad class of loss functions and boosting algorithms (including L2-boost, LogitBoost and AdaBoost, among others), we exhibit a direct connection between the performance of a stopped iterate and the localized Gaussian complexity of the associated function class. This connection allows us to show that local fixed point analysis of Gaussian or Rademacher complexities, now standard in the analysis of penalized estimators, can be used to derive optimal stopping rules. We derive such stopping rules in detail for various kernel classes, and illustrate the correspondence of our theory with practice for Sobolev kernel classes.\n Abstract ,  PDF \n #584: SVCCA: Singular Vector Canonical Correlation Analysis for Deep Understanding and Improvement \n Maithra Raghu,  Justin Gilmer,  Jason Yosinski,  Jascha Sohl-Dickstein \n With the continuing empirical successes of deep networks, it becomes increasingly important to develop better methods for understanding training of models and the representations learned within. In this paper we propose Singular Vector Canonical Correlation Analysis (SVCCA), a tool for quickly comparing two representations in a way that is both invariant to affine transform (allowing comparison between different layers and networks) and fast to compute (allowing more comparisons to be calculated than with previous methods). We deploy this tool to measure the intrinsic dimensionality of layers, showing in some cases needless over-parameterization; to probe learning dynamics throughout training, finding that networks converge to final representations from the bottom up; to show where class-specific information in networks is formed; and to suggest new training regimes that simultaneously save computation and overfit less.\n Abstract ,  PDF \n #585: Convolutional Phase Retrieval \n #586: Estimating High-dimensional Non-Gaussian Multiple Index Models via Stein’s Lemma \n #587: Gaussian Quadrature for Kernel Features \n Tri Dao,  Christopher De Sa,  Christopher Ré \n Kernel methods have recently attracted resurgent interest, matching the performance of deep neural networks in tasks such as speech recognition. The random Fourier features map is a technique commonly used to scale up kernel machines, but employing the randomized feature map means that $O(\\epsilon^{-2})$ samples are required to achieve an approximation error of at most $\\epsilon$. In this paper, we investigate some alternative schemes for constructing feature maps that are deterministic, rather than random, by approximating the kernel in the frequency domain using Gaussian quadrature. We show that deterministic feature maps can be constructed, for any $\\gamma > 0$, to achieve error $\\epsilon$ with $O(e^{\\gamma} + \\epsilon^{-1/\\gamma})$ samples as $\\epsilon$ goes to 0. We validate our methods on datasets in different domains, such as MNIST and TIMIT, showing that deterministic features are faster to generate and achieve comparable accuracy to the state-of-the-art kernel methods based on random Fourier features.\n Abstract ,  PDF \n #588: Value Prediction Network \n Junhyuk Oh,  Satinder Singh,  Honglak Lee \n This paper proposes a novel deep reinforcement learning (RL) architecture, called Value Prediction Network (VPN), which integrates model-free and model-based RL methods into a single neural network. In contrast to typical model-based RL methods, VPN learns a dynamics model whose abstract states are trained to make option-conditional predictions of future values (discounted sum of rewards) rather than of future observations. Our experimental results show that VPN has several advantages over both model-free and model-based baselines in a stochastic environment where careful planning is required but building an accurate observation-prediction model is difficult. Furthermore, VPN outperforms Deep Q-Network (DQN) on several Atari games even with short-lookahead planning, demonstrating its potential as a new way of learning a good state representation.\n Abstract ,  PDF \n #589: On Learning Errors of Structured Prediction with Approximate Inference \n #590: Efficient Second-Order Online Kernel Learning with Adaptive Embedding \n #591: Implicit Regularization in Matrix Factorization \n Suriya Gunasekar,  Blake Woodworth,  Srinadh Bhojanapalli,  Behnam Neyshabur,  Nathan Srebro \n We study implicit regularization when optimizing an underdetermined quadratic objective over a matrix $X$ with gradient descent on a factorization of $X$. We conjecture and provide empirical and theoretical evidence that with small enough step sizes and initialization close enough to the origin, gradient descent on a full dimensional factorization converges to the minimum nuclear norm solution.\n Abstract ,  PDF \n #592: Optimal Shrinkage of Singular Values Under Random Data Contamination \n #593: Delayed Mirror Descent in Continuous Games \n #594: Asynchronous Coordinate Descent under More Realistic Assumptions \n Tao Sun,  Robert Hannah,  Wotao Yin \n Asynchronous-parallel algorithms have the potential to vastly speed up algorithms by eliminating costly synchronization. However, our understanding to these algorithms is limited because the current convergence of asynchronous (block) coordinate descent algorithms are based on somewhat unrealistic assumptions. In particular, the age of the shared optimization variables being used to update a block is assumed to be independent of the block being updated. Also, it is assumed that the updates are applied to randomly chosen blocks. In this paper, we argue that these assumptions either fail to hold or will imply less efficient implementations. We then prove the convergence of asynchronous-parallel block coordinate descent under more realistic assumptions, in particular, always without the independence assumption. The analysis permits both the deterministic (essentially) cyclic and random rules for block choices. Because a bound on the asynchronous delays may or may not be available, we establish convergence for both bounded delays and unbounded delays. The analysis also covers nonconvex, weakly convex, and strongly convex functions. We construct Lyapunov functions that directly model both objective progress and delays, so delays are not treated errors or noise. A continuous-time ODE is provided to explain the construction at a high level.\n Abstract ,  PDF \n #595: Linear Convergence of a Frank-Wolfe Type Algorithm over Trace-Norm Balls \n Zeyuan Allen-Zhu,  Elad Hazan,  Wei Hu,  Yuanzhi Li \n We propose a rank-$k$ variant of the classical Frank-Wolfe algorithm to solve convex optimization over a trace-norm ball. Our algorithm replaces the top singular-vector computation ($1$-SVD) in Frank-Wolfe with a top-$k$ singular-vector computation ($k$-SVD), which can be done by repeatedly applying $1$-SVD $k$ times. Our algorithm has a linear convergence rate when the objective function is smooth and strongly convex, and the optimal solution has rank at most $k$. This improves the convergence rate and the total complexity of the Frank-Wolfe method and its variants.\n Abstract ,  PDF \n #596: Hierarchical Clustering Beyond the Worst-Case \n #597: Invariance and Stability of Deep Convolutional Representations \n #598: Statistical Cost Sharing \n Eric Balkanski,  Umar Syed,  Sergei Vassilvitskii \n We study the cost sharing problem for cooperative games in situations where the cost function $C$ is not available via oracle queries, but must instead be derived from data, represented as tuples $(S, C(S))$, for different subsets $S$ of players. We formalize this approach, which we call statistical cost sharing, and consider the computation of the core and the Shapley value, when the tuples are drawn from some distribution $\\mathcal{D}$. Previous work by Balcan et al. in this setting showed how to compute cost shares that satisfy the core property with high probability for limited classes of functions. We expand on their work and give an algorithm that computes such cost shares for any function with a non-empty core. We complement these results by proving an inapproximability lower bound for a weaker relaxation. We then turn our attention to the Shapley value. We first show that when cost functions come from the family of submodular functions with bounded curvature, $\\kappa$, the Shapley value can be approximated from samples up to a $\\sqrt{1 - \\kappa}$ factor, and that the bound is tight. We then define statistical analogues of the Shapley axioms, and derive a notion of statistical Shapley value. We show that these can always be approximated arbitrarily well for general functions over any distribution $\\mathcal{D}$.\n Abstract ,  PDF \n #599: The Expressive Power of Neural Networks: A View from the Width \n Zhou Lu,  Hongming Pu,  Feicheng Wang,  Zhiqiang Hu,  Liwei Wang \n The expressive power of neural networks is important for understanding deep learning. Most existing works consider this problem from the view of the depth of a network. In this paper, we study how width affects the expressiveness of neural networks. Classical results state that \\emph{depth-bounded} (e.g. depth-$2$) networks with suitable activation functions are universal approximators. We show a universal approximation theorem for \\emph{width-bounded} ReLU networks: width-$(n+4)$ ReLU networks, where $n$ is the input dimension, are universal approximators. Moreover, except for a measure zero set, all functions cannot be approximated by width-$n$ ReLU networks, which exhibits a phase transition. Several recent works demonstrate the benefits of depth by proving the depth-efficiency of neural networks. That is, there are classes of deep networks which cannot be realized by any shallow network whose size is no more than an \\emph{exponential} bound. Here we pose the dual question on the width-efficiency of ReLU networks: Are there wide networks that cannot be realized by narrow networks whose size is not substantially larger? We show that there exist classes of wide networks which cannot be realized by any narrow network whose depth is no more than a \\emph{polynomial} bound. On the other hand, we demonstrate by extensive experiments that narrow networks whose size exceed the polynomial bound by a constant factor can approximate wide and shallow network with high accuracy. Our results provide more comprehensive evidence that depth is more effective than width for the expressiveness of ReLU networks.\n Abstract ,  PDF \n #600: Spectrally-normalized margin bounds for neural networks \n Peter Bartlett,  Dylan J. Foster,  Matus Telgarsky \n This paper presents a margin-based multiclass generalization bound for neural networks which scales with their margin-normalized "spectral complexity": their Lipschitz constant, meaning the product of the spectral norms of the weight matrices, times a certain correction factor. This bound is empirically investigated for a standard AlexNet network on the mnist and cifar10 datasets, with both original and random labels, where it tightly correlates with the observed excess risks.\n Abstract ,  PDF \n #601: Robust and Efficient Transfer Learning with Hidden Parameter Markov Decision Processes \n Taylor Killian,  Samuel Daulton,  George Konidaris,  Finale Doshi-Velez \n We introduce a new formulation of the Hidden Parameter Markov Decision Process (HiP-MDP), a framework for modeling families of related tasks using low-dimensional latent embeddings. We replace the original Gaussian Process-based model with a Bayesian Neural Network. Our new framework correctly models the joint uncertainty in the latent parameters and the state space and has more scalable inference, thus expanding the scope the HiP-MDP to applications with higher dimensions and more complex dynamics.\n Abstract ,  PDF \n #602: Population Matching Discrepancy and Applications in Deep Learning \n #603: Scalable Planning with Tensorflow for Hybrid Nonlinear Domains \n Ga Wu,  Buser Say,  Scott Sanner \n Given recent deep learning results that demonstrate the ability to effectively optimize high-dimensional non-convex functions with gradient descent optimization on GPUs, we ask in this paper whether symbolic gradient optimization tools such as Tensorflow can be effective for planning in hybrid (mixed discrete and continuous) nonlinear domains with high dimensional state and action spaces? To this end, we demonstrate that hybrid planning with Tensorflow and RMSProp gradient descent is competitive with mixed integer linear program (MILP) based optimization on piecewise linear planning domains (where we can compute optimal solutions) and substantially outperforms state-of-the-art interior point methods for nonlinear planning domains. Furthermore, we remark that Tensorflow is highly scalable, converging to a strong policy on a large-scale concurrent domain with a total of 576,000 continuous actions over a horizon of 96 time steps in only 4 minutes. We provide a number of insights that clarify such strong performance including observations that despite long horizons, RMSProp avoids both the vanishing and exploding gradients problem. Together these results suggest a new frontier for highly scalable planning in nonlinear hybrid domains by leveraging GPUs and the power of recent advances in gradient descent with highly optmized toolkits like Tensorflow.\n Abstract ,  PDF \n #604: Boltzmann Exploration Done Right \n Nicolò Cesa-Bianchi,  Claudio Gentile,  Gábor Lugosi,  Gergely Neu \n Boltzmann exploration is a classic strategy for sequential decision-making under uncertainty, and is one of the most standard tools in Reinforcement Learning (RL). Despite its widespread use, there is virtually no theoretical understanding about the limitations or the actual benefits of this exploration scheme. Does it drive exploration in a meaningful way? Is it prone to misidentifying the optimal actions or spending too much time exploring the suboptimal ones? What is the right tuning for the learning rate? In this paper, we address several of these questions in the classic setup of stochastic multi-armed bandits. One of our main results is showing that the Boltzmann exploration strategy with any monotone learning-rate sequence will induce suboptimal behavior. As a remedy, we offer a simple non-monotone schedule that guarantees near-optimal performance, albeit only when given prior access to key problem parameters that are typically not available in practical situations (like the time horizon $T$ and the suboptimality gap $\\Delta$). More importantly, we propose a novel variant that uses different learning rates for different arms, and achieves a distribution-dependent regret bound of order $\\frac{K\\log^2 T}{\\Delta}$ and a distribution-independent bound of order $\\sqrt{KT}\\log K$ without requiring such prior knowledge. To demonstrate the flexibility of our technique, we also propose a variant that guarantees the same performance bounds even if the rewards are heavy-tailed.\n Abstract ,  PDF \n #605: Towards the ImageNet-CNN of NLP: Pretraining Sentence Encoders with Machine Translation \n #606: Neural Discrete Representation Learning \n #607: Generalizing GANs: A Turing Perspective \n #608: Scalable Log Determinants for Gaussian Process Kernel Learning \n #609: Poincaré Embeddings for Learning Hierarchical Representations \n #610: Learning Combinatorial Optimization Algorithms over Graphs \n Hanjun Dai,  Elias B. Khalil,  Yuyu Zhang,  Bistra Dilkina,  Le Song \n The design of good heuristics or approximation algorithms for NP-hard combinatorial optimization problems often requires significant specialized knowledge and trial-and-error. Can we automate this challenging, tedious process, and learn the algorithms instead? In many real-world applications, it is typically the case that the same optimization problem is solved again and again on a regular basis, maintaining the same problem structure but differing in the data. This provides an opportunity for learning heuristic algorithms that exploit the structure of such recurring problems. In this paper, we propose a unique combination of reinforcement learning and graph embedding to address this challenge. The learned greedy policy behaves like a meta-algorithm that incrementally constructs a solution, and the action is determined by the output of a graph embedding network capturing the current state of the solution. We show our framework can be applied to a diverse range of optimization problems over graphs, and learns effective algorithms for the Minimum Vertex Cover, Maximum Cut and Traveling Salesman problems.\n Abstract ,  PDF \n #611: Robust Conditional Probabilities \n Yoav Wald,  Amir Globerson \n Conditional probabilities are a core concept in machine learning. For example, optimal prediction of a label $Y$ given an input $X$ corresponds to maximizing the conditional probability of $Y$ given $X$. A common approach to inference tasks is learning a model of conditional probabilities. However, these models are often based on strong assumptions (e.g., log-linear models), and hence their estimate of conditional probabilities is not robust and is highly dependent on the validity of their assumptions. Here we propose a framework for reasoning about conditional probabilities without assuming anything about the underlying distributions, except knowledge of their second order marginals, which can be estimated from data. We show how this setting leads to guaranteed bounds on conditional probabilities, which can be calculated efficiently in a variety of settings, including structured-prediction. Finally, we apply them to semi-supervised deep learning, obtaining results competitive with variational autoencoders.\n Abstract ,  PDF \n #612: Learning with Bandit Feedback in Potential Games \n #613: Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments \n Ryan Lowe,  Yi Wu,  Aviv Tamar,  Jean Harb,  Pieter Abbeel,  Igor Mordatch \n We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.\n Abstract ,  PDF \n #614: Communication-Efficient Distributed Learning of Discrete Distributions \n #615: Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles \n Balaji Lakshminarayanan,  Alexander Pritzel,  Charles Blundell \n Deep neural networks are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in neural networks is a challenging and yet unsolved problem. Bayesian neural networks, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) neural neural networks. We propose an alternative to Bayesian neural networks, that is simple to implement, readily parallelisable and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian neural networks. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on unseen data. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.\n Abstract ,  PDF \n #616: When Worlds Collide: Integrating Different Counterfactual Assumptions in Fairness \n #617: Matrix Norm Estimation from a Few Entries \n #618: Deep Networks for Decoding Natural Images from Retinal Signals \n #619: Causal Effect Inference with Deep Latent Variable Models \n Christos Louizos,  Uri Shalit,  Joris Mooij,  David Sontag,  Richard Zemel,  Max Welling \n Learning individual-level causal effects from observational data, such as inferring the most effective medication for a specific patient, is a problem of growing importance for policy makers. The most important aspect of inferring causal effects from observational data is the handling of confounders, factors that affect both an intervention and its outcome. A carefully designed observational study attempts to measure all important confounders. However, even if one does not have direct access to all confounders, there may exist noisy and uncertain measurement of proxies for confounders. We build on recent advances in latent variable modelling to simultaneously estimate the unknown latent space summarizing the confounders and the causal effect. Our method is based on Variational Autoencoders (VAE) which follow the causal structure of inference with proxies. We show our method is significantly more robust than existing methods, and matches the state-of-the-art on previous benchmarks focused on individual treatment effects.\n Abstract ,  PDF \n #620: Learning Identifiable Gaussian Bayesian Networks in Polynomial Time and Sample Complexity \n Asish Ghoshal,  Jean Honorio \n Learning the directed acyclic graph (DAG) structure of a Bayesian network from observational data is a notoriously difficult problem for which many hardness results are known. In this paper we propose a provably polynomial-time algorithm for learning sparse Gaussian Bayesian networks with equal noise variance --- a class of Bayesian networks for which the DAG structure can be uniquely identified from observational data --- under high-dimensional settings. We show that $O(k^4 \\log p)$ number of samples suffices for our method to recover the true DAG structure with high probability, where $p$ is the number of variables and $k$ is the maximum Markov blanket size. We obtain our theoretical guarantees under a condition called Restricted Strong Adjacency Faithfulness, which is strictly weaker than strong faithfulness --- a condition that other methods based on conditional independence testing need for their success. The sample complexity of our method matches the information-theoretic limits in terms of the dependence on $p$. We show that our method out-performs existing state-of-the-art methods for learning Gaussian Bayesian networks in terms of recovering the true DAG structure while being comparable in speed to heuristic methods.\n Abstract ,  PDF \n #621: Gradient Episodic Memory for Continuum Learning \n David Lopez-Paz,  Marc\'Aurelio Ranzato \n One major obstacle towards artificial intelligence is the poor ability of models to quickly solve new problems, without forgetting previously acquired knowledge. To better understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model for continual learning, called Gradient of Episodic Memory (GEM), which alleviates forgetting while allowing beneficial transfer of knowledge to previous tasks. Our experiments on variants of MNIST and CIFAR-100 demonstrate the strong performance of GEM when compared to the state-of-the-art.\n Abstract ,  PDF \n #622: Radon Machines: Effective Parallelisation for Machine Learning \n #623: Semisupervised Clustering, AND-Queries and Locally Encodable Source Coding \n #624: Clustering Stable Instances of Euclidean k-means. \n #625: Good Semi-supervised Learning That Requires a Bad GAN \n #626: On Blackbox Backpropagation and Jacobian Sensing \n #627: Protein Interface Prediction using Graph Convolutional Networks \n #628: Solid Harmonic Wavelet Scattering: Predicting Quantum Molecular Energy from Invariant Descriptors of 3D  Electronic Densities \n #629: Towards Generalization and Simplicity in Continuous Control \n Aravind Rajeswaran,  Kendall Lowrey,  Emanuel Todorov,  Sham Kakade \n This work shows that policies with simple linear and RBF parameterizations can be trained to solve a variety of continuous control tasks, including the OpenAI gym benchmarks. The performance of these trained policies are competitive with state of the art results, obtained with more elaborate parameterizations such as fully connected neural networks. Furthermore, existing training and testing scenarios are shown to be very limited and prone to over-fitting, thus giving rise to only trajectory-centric policies. Training with a diverse initial state distribution is shown to produce more global policies with better generalization. This allows for interactive control scenarios where the system recovers from large on-line perturbations; as shown in the supplementary video.\n Abstract ,  PDF \n #630: Random Projection Filter Bank for Time Series Data \n #631: Filtering Variational Objectives \n Chris J. Maddison,  Dieterich Lawson,  George Tucker,  Nicolas Heess,  Mohammad Norouzi,  Andriy Mnih,  Arnaud Doucet,  Yee Whye Teh \n When used as a surrogate objective for maximum likelihood estimation in latent variable models, the evidence lower bound (ELBO) produces state-of-the-art results. Inspired by this, we consider the extension of the ELBO to a family of lower bounds defined by a particle filter\'s estimator of the marginal likelihood, the filtering variational objectives (FIVOs). FIVOs take the same arguments as the ELBO, but can exploit a model\'s sequential structure to form tighter bounds. We present results that relate the tightness of FIVO\'s bound to the variance of the particle filter\'s estimator by considering the generic case of bounds defined as log-transformed likelihood estimators. Experimentally, we show that training with FIVO results in substantial improvements over training with ELBO on sequential data.\n Abstract ,  PDF \n #632: On Frank-Wolfe and Equilibrium Computation \n #633: Modulating early visual processing by language \n Harm de Vries,  Florian Strub,  Jérémie Mary,  Hugo Larochelle,  Olivier Pietquin,  Aaron Courville \n It is commonly assumed that language refers to high-level visual concepts while leaving low-level visual processing unaffected. This view dominates the current literature in computational models for language-vision tasks, where visual and linguistic input are mostly processed independently before being fused into a single representation. In this paper, we deviate from this classic pipeline and propose to modulate the \\emph{entire visual processing} by linguistic input. Specifically, we condition the batch normalization parameters of a pretrained residual network (ResNet) on a language embedding. This approach, which we call MOdulated RESnet (\\MRN), significantly improves strong baselines on two visual question answering tasks. Our ablation study shows that modulating from the early stages of the visual processing is beneficial.\n Abstract ,  PDF \n #634: Learning Mixture of Gaussians with Streaming Data \n Aditi Raghunathan,  Ravishankar Krishnaswamy,  Prateek Jain \n In this paper, we study the problem of learning a mixture of Gaussians with streaming data: given a stream of $N$ points in $d$ dimensions generated by an unknown mixture of $k$ spherical Gaussians, the goal is to estimate the model parameters using a single pass over the data stream. We analyze a streaming version of the popular Lloyd\'s heuristic and show that the algorithm estimates all the unknown centers of the component Gaussians accurately if they are sufficiently separated. Assuming each pair of centers are $C\\sigma$ distant with $C=\\Omega((k\\log k)^{1/4}\\sigma)$ and where $\\sigma^2$ is the maximum variance of any Gaussian component, we show that asymptotically the algorithm estimates the centers optimally (up to constants); our center separation requirement matches the best known result for spherical Gaussians \\citep{vempalawang}. For finite samples, we show that a bias term based on the initial estimate decreases at $O(1/{\\rm poly}(N))$ rate while variance decreases at nearly optimal rate of $\\sigma^2 d/N$. Our analysis requires seeding the algorithm with a good initial estimate of the true cluster centers for which we provide an online PCA based clustering algorithm. Indeed, the asymptotic per-step time complexity of our algorithm is the optimal $d\\cdot k$ while space complexity of our algorithm is $O(dk\\log k)$. In addition to the bias and variance terms which tend to $0$, the hard-thresholding based updates of streaming Lloyd\'s algorithm is agnostic to the data distribution and hence incurs an approximation error that cannot be avoided. However, by using a streaming version of the classical (soft-thresholding-based) EM method that exploits the Gaussian distribution explicitly, we show that for a mixture of two Gaussians the true means can be estimated consistently, with estimation error decreasing at nearly optimal rate, and tending to $0$ for $N\\rightarrow \\infty$.\n Abstract ,  PDF \n #635: Practical Hash Functions for Similarity Estimation and Dimensionality Reduction \n #636: Two Time-Scale Update Rule for Generative Adversarial Nets \n #637: The Scaling Limit of High-Dimensional Online Independent Component Analysis \n #638: Approximation Algorithms for $\\ell_0$-Low Rank Approximation \n #639: The power of absolute discounting: all-dimensional distribution estimation \n #640: Supervised Adversarial Domain Adaptation \n #641: Spectral Mixture Kernels for Multi-Output Gaussian Processes \n Gabriel Parra,  Felipe Tobar \n Initially, multiple-output Gaussian processes models (MOGPs) were constructed as linear combinations of independent, latent, single-output Gaussian processes (GPs). This resulted in cross-covariance functions with limited parametric interpretation, thus conflicting with single-output GPs and their intuitive understanding of lengthscales, frequencies and magnitudes to name but a few. On the contrary, current approaches to MOGP are able to better interpret the relationship between different channels by directly modelling the cross-covariances as a spectral mixture kernel with a phase shift. We propose a parametric family of complex-valued crossspectral densities and then build on Cramer\'s Theorem, the multivariate version of Bochner\'s Theorem, to provide a principled approach to design multivariate covariance functions. The so-constructed kernels are able to model delays among channels in addition to phase differences and are thus more expressive than previous methods, while also providing full parametric interpretation of the relationship across channels. The proposed method is first validated on synthetic data and then compared to existing MOGP methods on two real-world examples.\n Abstract ,  PDF \n #642: Neural Expectation Maximization \n Klaus Greff,  Sjoerd van Steenkiste,  Jürgen Schmidhuber \n Many real world tasks such as reasoning and physical interaction require identification and manipulation of conceptual entities. A first step towards solving these tasks is the automated discovery of distributed symbol-like representations. In this paper, we explicitly formalize this problem as inference in a spatial mixture model where each component is parametrized by a neural network. Based on the Expectation Maximization framework we then derive a differentiable clustering method that simultaneously learns how to group and represent individual entities. We evaluate our method on the (sequential) perceptual grouping task and find that it is accurately able to recover the constituent objects. We demonstrate that the learned representations are useful for predictive coding.\n Abstract ,  PDF \n #643: Online Learning of Linear Dynamical Systems \n #644: Z-Forcing: Training Stochastic Recurrent Networks \n #645: Thalamus Gated Recurrent Modules \n #646: Neural Variational Inference and Learning in Undirected Graphical Models \n Andriy Mnih,  Karol Gregor \n Highly expressive directed latent variable models, such as sigmoid belief networks, are difficult to train on large datasets because exact inference in them is intractable and none of the approximate inference methods that have been applied to them scale well. We propose a fast non-iterative approximate inference method that uses a feedforward network to implement efficient exact sampling from the variational posterior. The model and this inference network are trained jointly by maximizing a variational lower bound on the log-likelihood. Although the naive estimator of the inference model gradient is too high-variance to be useful, we make it practical by applying several straightforward model-independent variance reduction techniques. Applying our approach to training sigmoid belief networks and deep autoregressive networks, we show that it outperforms the wake-sleep algorithm on MNIST and achieves state-of-the-art results on the Reuters RCV1 document dataset.\n Abstract ,  PDF \n #647: Subspace Clustering via Tangent Cones \n #648: The Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point Process \n Hongyuan Mei,  Jason Eisner \n Many events occur in the world. Some event types are stochastically excited or inhibited---in the sense of having their probabilities elevated or decreased---by patterns in the sequence of previous events. Discovering such patterns can help us predict which type of event will happen next and when. We propose to model streams of discrete events in continuous time, by constructing a neurally self-modulating multivariate point process in which the intensities of multiple event types evolve according to a novel continuous-time LSTM. This generative model allows past events to influence the future in complex and realistic ways, by conditioning future event intensities on the hidden state of a recurrent neural network that has consumed the stream of past events. Our model has desirable qualitative properties. It achieves competitive likelihood and predictive accuracy on real and synthetic datasets, including under missing-data conditions.\n Abstract ,  PDF \n #649: Inverse Reward Design \n #650: Structured Bayesian Pruning via Log-Normal Multiplicative Noise \n Kirill Neklyudov,  Dmitry Molchanov,  Arsenii Ashukha,  Dmitry Vetrov \n Dropout-based regularization methods can be regarded as injecting random noise with pre-defined magnitude to different parts of the neural network during training. It was recently shown that Bayesian dropout procedure not only improves generalization but also leads to extremely sparse neural architectures by automatically setting the individual noise magnitude per weight. However, this sparsity can hardly be used for acceleration since it is unstructured. In the paper, we propose a new Bayesian model that takes into account the computational structure of neural networks and provides structured sparsity, e.g. removes neurons and/or convolutional channels in CNNs. To do this, we inject noise to the neurons outputs while keeping the weights unregularized. We established the probabilistic model with a proper truncated log-uniform prior over the noise and truncated log-normal variational approximation that ensures that the KL-term in the evidence lower bound is computed in closed-form. The model leads to structured sparsity by removing elements with a low SNR from the computation graph and provides significant acceleration on a number of deep neural architectures. The model is very easy to implement as it only corresponds to the addition of one dropout-like layer in computation graph.\n Abstract ,  PDF \n #651: Attend and Predict: Understanding Gene Regulation by Selective Attention on Chromatin \n Ritambhara Singh,  Jack Lanchantin,  Arshdeep Sekhon,  Yanjun Qi \n The past decade has seen a revolution in genomic technologies that enable a flood of genome-wide profiling of chromatin marks. Recent literature tried to understand gene regulation by predicting gene expression from large-scale chromatin measurements. Two fundamental challenges exist for such learning tasks: (1) genome-wide chromatin signals are spatially structured, high-dimensional and highly modular; and (2) the core aim is to understand what are the relevant factors and how they work together? Previous studies either failed to model complex dependencies among input signals or relied on separate feature analysis to explain the decisions. This paper presents an attention-based deep learning approach; we call AttentiveChrome, that uses a unified architecture to model and to interpret dependencies among chromatin factors for controlling gene regulation. AttentiveChrome uses a hierarchy of multiple Long short-term memory (LSTM) modules to encode the input signals and to model how various chromatin marks cooperate automatically. AttentiveChrome trains two levels of attention jointly with the target prediction, enabling it to attend differentially to relevant marks and to locate important positions per mark. We evaluate the model across 56 different cell types (tasks) in human. Not only is the proposed architecture more accurate, but its attention scores also provide a better interpretation than state-of-the-art feature visualization methods such as saliency map. Code and data are shared at www.deepchrome.org\n Abstract ,  PDF \n #652: Acceleration and Averaging in Stochastic Descent Dynamics \n Walid Krichene,  Peter L. Bartlett \n We formulate and study a general family of (continuous-time) stochastic dynamics for accelerated first-order minimization of smooth convex functions. Building on an averaging formulation of accelerated mirror descent, we propose a stochastic variant in which the gradient is contaminated by noise, and study the resulting stochastic differential equation. We prove a bound on the rate of change of an energy function associated with the problem, then use it to derive estimates of convergence rates of the function values, (a.s. and in expectation) both for persistent and asymptotically vanishing noise. We discuss the interaction between the parameters of the dynamics (learning rate and averaging weights) and the covariation of the noise process, and show, in particular, how the asymptotic rate of covariation affects the choice of parameters and, ultimately, the convergence rate.\n Abstract ,  PDF \n #653: Kernel functions based on triplet comparisons \n #654: An Error Detection and Correction Framework for Connectomics \n Jonathan Zung,  Ignacio Tartavull,  H. Sebastian Seung \n Significant advances have been made in recent years on the problem of neural circuit reconstruction from electron microscopic imagery. Improvements in image acquisition, image alignment, and boundary detection have greatly reduced the achievable error rate. In order to make further progress, we argue that automated error detection is essential for focusing the effort and attention of both human and machine. In this paper, we report on the use of automated error detection as an attention signal for a flood filling error correction module. We demonstrate significant improvements upon the state of the art in segmentation performance.\n Abstract ,  PDF \n #655: Style Transfer from Non-parallel Text by Cross-Alignment \n #656: Cross-Spectral Factor Analysis \n #657: Stochastic Submodular Maximization: The Case of Coverage Functions \n #658: On Distributed Hierarchical Clustering \n #659: Unsupervised Transformation Learning via Convex Relaxations \n #660: A Sharp Error Analysis for the Fused Lasso, with Implications to Broader Settings  and Approximate Screening \n #661: Efficient Computation of Moments in Sum-Product Networks \n Han Zhao,  Geoff Gordon \n Bayesian online learning algorithms for Sum-Product Networks (SPNs) need to compute moments of model parameters under the one-step update posterior distribution. The best existing method for computing such moments scales quadratically in the size of the SPN, although it scales linearly for trees. We propose a linear-time algorithm that works even when the SPN is a directed acyclic graph (DAG). We achieve this goal by reducing the moment computation problem into a joint inference problem in SPNs and by taking advantage of a special structure of the one-step update posterior distribution: it is a multilinear polynomial with exponentially many monomials, and we can evaluate moments by differentiating. The latter is known as the \\emph{differential trick}. We apply the proposed algorithm to develop a linear time assumed density filter (ADF) for SPN parameter learning. As an additional contribution, we conduct extensive experiments comparing seven different online learning algorithms for SPNs on 20 benchmark datasets. The new linear-time ADF method consistently achieves low runtime due to the efficient linear-time algorithm for moment computation; however, we discover that two other methods (CCCP and SMA) typically perform better statistically, while a third (BMM) is comparable to ADF. Interestingly, CCCP can be viewed as implicitly using the same differentiation trick that we make explicit here. The fact that two of the top four fastest methods use this trick suggests that the same trick might find other uses for SPN learning in the future.\n Abstract ,  PDF \n #662: A Meta-Learning Perspective on Cold-Start Recommendations for Items \n #663: Predicting Scene Parsing and Motion Dynamics in the Future \n #664: Sticking the Landing: Simple, Lower-Variance Gradient Estimators for Variational Inference \n #665: Efficient Approximation Algorithms for Strings Kernel Based Sequence Classification \n #666: Kernel Feature Selection via Conditional Covariance Minimization \n Jianbo Chen,  Mitchell Stern,  Martin J. Wainwright,  Michael I. Jordan \n We propose a framework for feature selection that employs kernel-based measures of independence to find a subset of covariates that is maximally predictive of the response. Building on past work in kernel dimension reduction, we formulate our approach as a constrained optimization problem involving the trace of the conditional covariance operator, and additionally provide some consistency results. We then demonstrate on a variety of synthetic and real data sets that our method compares favorably with other state-of-the-art algorithms.\n Abstract ,  PDF \n #667:  Statistical Convergence Analysis of Gradient EM on General Gaussian Mixture Models \n #668: Real Time Image Saliency for Black Box Classifiers \n Piotr Dabkowski,  Yarin Gal \n In this work we develop a fast saliency detection method that can be applied to any differentiable image classifier. We train a masking model to manipulate the scores of the classifier by masking salient parts of the input image. Our model generalises well to unseen images and requires a single forward pass to perform saliency detection, therefore suitable for use in real-time systems. We test our approach on CIFAR-10 and ImageNet datasets and show that the produced saliency maps are easily interpretable, sharp, and free of artifacts. We suggest a new metric for saliency and test our method on the ImageNet object localisation task. We achieve results outperforming other weakly supervised methods.\n Abstract ,  PDF \n #669: Houdini: Democratizing Adversarial Examples \n #670: Efficient and Flexible Inference for Stochastic Systems \n #671: When Cyclic Coordinate Descent Beats Randomized Coordinate Descent \n #672: Active Learning from Peers \n #673: Learning Causal Graphs with Latent Variables \n #674: Learning to Model the Tail \n #675: Stochastic Mirror Descent for Non-Convex Optimization \n #676: On Separability of Loss Functions, and Revisiting Discriminative Vs Generative Models \n #677: Maxing and Ranking with Few Assumptions \n #678: On clustering network-valued data \n #679: A General Framework for Robust Interactive Learning \n #680: Multi-view Matrix Factorization for Linear Dynamical System Estimation', '\n \n \n \n capricorn \n capricorn is a lightweight library for helping prepare vocabulary from\ncorpus and prepare word embedding ready to be used by learning models. \n \n build vocabulary from corpus \n load necessary word embedding with consistent word index in Vocabulary \n \n getting started \n pip install capricorn \n ```python\nimport capricorn\nimport os \n Specify filepaths \n Vocab_path = "vocab_processor"\nembedding_vector_path = "path/to/embedding" \n Load vocab \n if os.path.isfile(Vocab_path):\n  print("Loading Vocabulary ...")\n  vocab_processor = capricorn.VocabularyProcessor.restore(Vocab_path) \n else:  # build vocab\n  print("Building Vocabulary ...") \n x_text = ["Saudi Arabia Equity Movers: Almarai, Jarir Marketing and Spimaco.",\n            "Orange, Thales to Get French Cloud Computing Funds, Figaro Says.",\n            "Stansted Could Double Passengers on Deregulation, Times Reports."] \n # Build/load vocabulary\n  max_document_length = 11\n  min_freq_filter = 2 \n vocab_processor = capricorn.VocabularyProcessor(max_document_length=max_document_length,\n                                                  min_frequency=min_freq_filter)\n  # only fit\n  # vocab_processor.fit(x_text)\n  # or fit_transform to get the transformed corpus\n  x_text_transformed = vocab_processor.fit_transform(x_text)\n  vocab_processor.save(Vocab_path)\n  print("vocab_processor saved at:", Vocab_path) \n build embedding matrix of which the index is consistent with vocab word2index mapping \n embedding_matrix = vocab_processor.prepare_embedding_matrix_with_dim(embedding_vector_path, 300) \n ``` \n User input \n The library default to use special token __UNK__  and __PAD__, \nif the input sequence lengths below the max_document_length when initial\nVocabularyProcessor, it will automatically pad the sequence use the __PAD__.  \n If user have pre defined special tokens when initialize Vocabulary, user \nneed to pre-process the sequence, namely adding the self defined special \ntokens to the input sequence. For example if user defined __START__\nand __END__ as additional special tokens and max_document_length=11,  User has to process the original\nsentence from:  \n "We like it very much"  \n to: \n "__START__ __PAD__ __PAD__  We like it very much __PAD__ __PAD__ __END__"', 'News titles for stock prediction \n Deep learning for stock long short signal prediction \nbased on chronologically sorted news titles. \n Run \n prepare or train your own embedding for the task \n Trained word embedding based on news corpus provided from above dataset:\nI have a trained word embedding based on the news corpus to get you started on \nthe project under data/embedding/fasttext/model.vec \n If you want to train your own word embedding: \n ```bash\nsh scripts/prepare_fasttext.sh \n train the word embedding and save embedding under data/embedding, notice you might want to change model name \n sh scripts/train_fasttext_embedding.sh data/train_own_word_embedding_example.txt \n``` \n Main code has three part: \n run_preprocess.py: given input dataframe, prepare date_news_embeddings and input_dataframe_with_signal for training.\n            vocabulary_processor and embedding matrix are saved for later use in online prediction.  \n run_training.py: train long mid short term cnn with daily average news embedding.  \n run_predicting.py:  online predictor for trained model. An english version toy example is given \n                  under run_predicting.py. After training without can mode_config.py, run this script get\n                  a prediction result. \n ```python \n preprocess data \n python run_preprocess.py \n traing model \n python run_training.py \n online prediction \n python run_predicting.py\n``` \n Data \n Raw \n \n \n news data \n original dataset news from Bloomberg \\& Reuters from 20061020 to 20131126\ntotal number of trading days:1786\n google drive \nplease refer to network.preprocess.get_data_dict to get raw text into structured\nformat.  \n \n \n market data \n      SPY historical data from 20061020 to 20131126 \n \n \n preprocessed data and data interface to the model \n You can find raw data processed method under network.preprocess.\nFeel free to try by yourself on cleaning up the raw data. \n Now, let\'s focus on the data format sent to model pipeline. \nThe interface to the model is a dataframe header as:\n Date,   Adj Close,  news_title \n An example row: \n 2006-10-20  108.961449 [["news_title_1","00:00:00"],["news_title_2","01:00:00"]] \n In short we need date, adjust close price and  sorted  daily news along reported time from old to new. Currently, the time\nis only used for sorting within a day.  \n You can find complete Bloomberg \\& Reuters data interface example at training_dir/input_dataframe.pickle \n Model \n long mid short term CNN \n \n input to long_mid_short cnn(or deep prediction model mentioned in the paper) has three parts:\nof same essence \n events embedding: Most recent N days of events embedding with all events within a day represents\nby one dense vector(currently averaged events embedding within a day).  \n With long term use N = 30, mid term use N = 7, short term use N =1.  \n Inputs on long term events embedding and mid term events embedding are Convolved and then Max pooled\n. The output of both long and mid term from max pooling merged with short term input to form \na dense vector represents a long_mid_short term embedding for the prediction on next day long \nor short signal.  \n Events Embedding \n Event embedding can be generalized define as a dense vector represents an event.\n(this implementation use word embeddings concatenation of news title sentence as event embedding) \n Originally in paper[1], they follows thoughts in [3] and [4] trying to learn events embedding\nbased on triplet relations.  \n Even though events embedding learned through method in [1] has around 3% ~ 4% improvements over\nword embeddings concatenation of news title sentence as event embedding on the dataset in the paper.\nIt is not clear events embedding learned through Neural Tensor Network [3][4] can scale well at practise.\nThere are several issue considering events embedding learned through neural tensor network: \n \n \n events triplet in the news title extraction can lose information: [1] use openIE and dependency parser \nto extract (Subject,Verb,Object) triplet assuming openIE extraction contains S,V,O and dependency parser\ncan help narrow words within the extractions of openIE to come up with the final SVO. \nHowever, the state-of-the-art openIE [5] can only extract 25%~50% of the news titles, leaving the rest of\nSVO empty. This leads to huge information loss based on this method.  \n \n \n events triplet in the news title extraction is tedious for agile project development:\ngiven above description, to prepare the the event embedding, you need to extract SVO first based\non openIE and dependency parser, then you need to train the Neural Tensor Network to get the event \nembedding and save them to disk. Finally can you train CNN on event embeddings to get prediction \ninput. For now it is not worth around 3% accuracy gain to prepare that much effort.  \n \n \n events triplet in the news title extraction is not fully end to end for the project: It is impossible for the loss to be \nback-propagate to the event extraction part (aka neural tensor network), given our goal is to learn the next day long short signal.\nThe author believe if given enough data(currently only 1786 trading days news title), it is worth a deeper model to learn the\ninformation extraction by a complete model itself instead of separate SVO extraction and prediction.  \n \n \n \n Experiments \n experiments train/val results on average embedding based on word vector concatenation for news titles:\n ...\nEpoch 411/1000\n1404/1404 [==============================] - 1s 570us/step - loss: 0.6815 - acc: 0.6524 - val_loss: 0.7171 - val_acc: 0.6125\nEpoch 412/1000\n1404/1404 [==============================] - 1s 562us/step - loss: 0.6772 - acc: 0.6531 - val_loss: 0.7307 - val_acc: 0.6097\nEpoch 413/1000\n1404/1404 [==============================] - 1s 517us/step - loss: 0.6875 - acc: 0.6524 - val_loss: 0.7249 - val_acc: 0.6125\nEpoch 414/1000\n1404/1404 [==============================] - 1s 542us/step - loss: 0.6916 - acc: 0.6531 - val_loss: 0.7229 - val_acc: 0.6125\nEpoch 415/1000\n1404/1404 [==============================] - 1s 541us/step - loss: 0.6738 - acc: 0.6524 - val_loss: 0.7099 - val_acc: 0.6097\nEpoch 416/1000\n1404/1404 [==============================] - 1s 556us/step - loss: 0.6732 - acc: 0.6538 - val_loss: 0.7173 - val_acc: 0.6125\nEpoch 417/1000\n1404/1404 [==============================] - 1s 553us/step - loss: 0.6858 - acc: 0.6531 - val_loss: 0.7100 - val_acc: 0.6154\nEpoch 418/1000\n1404/1404 [==============================] - 1s 559us/step - loss: 0.6756 - acc: 0.6538 - val_loss: 0.7294 - val_acc: 0.6125\nEpoch 419/1000\n1404/1404 [==============================] - 1s 537us/step - loss: 0.6852 - acc: 0.6524 - val_loss: 0.7037 - val_acc: 0.6154\nEpoch 420/1000\n1404/1404 [==============================] - 1s 546us/step - loss: 0.6800 - acc: 0.6531 - val_loss: 0.7146 - val_acc: 0.6125\nEpoch 421/1000\n1404/1404 [==============================] - 1s 560us/step - loss: 0.6797 - acc: 0.6524 - val_loss: 0.7254 - val_acc: 0.6125\nEpoch 422/1000\n1404/1404 [==============================] - 1s 523us/step - loss: 0.6837 - acc: 0.6524 - val_loss: 0.7187 - val_acc: 0.6125\nEpoch 423/1000\n1404/1404 [==============================] - 1s 548us/step - loss: 0.6918 - acc: 0.6517 - val_loss: 0.7261 - val_acc: 0.6125\nEpoch 424/1000\n1404/1404 [==============================] - 1s 562us/step - loss: 0.6830 - acc: 0.6517 - val_loss: 0.7017 - val_acc: 0.6154\nEpoch 425/1000\n1404/1404 [==============================] - 1s 542us/step - loss: 0.6797 - acc: 0.6510 - val_loss: 0.7323 - val_acc: 0.6125\nEpoch 426/1000\n1404/1404 [==============================] - 1s 552us/step - loss: 0.6935 - acc: 0.6517 - val_loss: 0.7419 - val_acc: 0.6125\nEpoch 427/1000\n1404/1404 [==============================] - 1s 545us/step - loss: 0.6943 - acc: 0.6538 - val_loss: 0.7473 - val_acc: 0.6097\nEpoch 428/1000\n1404/1404 [==============================] - 1s 561us/step - loss: 0.6908 - acc: 0.6524 - val_loss: 0.7482 - val_acc: 0.6125\nEpoch 429/1000\n1404/1404 [==============================] - 1s 541us/step - loss: 0.6899 - acc: 0.6517 - val_loss: 0.7246 - val_acc: 0.6125\nEpoch 430/1000\n1404/1404 [==============================] - 1s 539us/step - loss: 0.6898 - acc: 0.6531 - val_loss: 0.7229 - val_acc: 0.6040\nEpoch 431/1000\n1404/1404 [==============================] - 1s 519us/step - loss: 0.6867 - acc: 0.6546 - val_loss: 0.7166 - val_acc: 0.6125\nEpoch 432/1000\n1404/1404 [==============================] - 1s 541us/step - loss: 0.6871 - acc: 0.6524 - val_loss: 0.7144 - val_acc: 0.6125\nEpoch 433/1000\n1404/1404 [==============================] - 1s 560us/step - loss: 0.6775 - acc: 0.6538 - val_loss: 0.7045 - val_acc: 0.6068\nEpoch 434/1000\n1404/1404 [==============================] - 1s 544us/step - loss: 0.6713 - acc: 0.6524 - val_loss: 0.7042 - val_acc: 0.6068\nEpoch 435/1000\n1404/1404 [==============================] - 1s 550us/step - loss: 0.6733 - acc: 0.6531 - val_loss: 0.7130 - val_acc: 0.6125\nEpoch 436/1000\n1404/1404 [==============================] - 1s 532us/step - loss: 0.6719 - acc: 0.6531 - val_loss: 0.7115 - val_acc: 0.6068\nEpoch 437/1000\n1404/1404 [==============================] - 1s 572us/step - loss: 0.6743 - acc: 0.6517 - val_loss: 0.7140 - val_acc: 0.6154\nEpoch 438/1000\n1404/1404 [==============================] - 1s 564us/step - loss: 0.6838 - acc: 0.6531 - val_loss: 0.7051 - val_acc: 0.6125\nEpoch 439/1000\n1404/1404 [==============================] - 1s 556us/step - loss: 0.6768 - acc: 0.6538 - val_loss: 0.7097 - val_acc: 0.6125 \n Prepare Chinese Version input for the model \n The only difference between chinese and english in the sense of model input is the natural word segmentation on \nthe english side. To prepare the experiments in Chinese, you need : \n A chinese word segmenter \n ```bash \n get THULAC, compile and download model \n sh scripts/prepare_chinese_word_segmenter.sh    \n A toy input example at data/chinese_word_seg_example.txt \n sh scripts/run_chinese_word_segmenter.sh data/chinese_word_seg_example.txt\n``` \n Now you have a segmented chinese corpus, you can train your own chinese word\nembedding following the above instructions under train your own embedding. \n chinese counterpart input_dataframe.pickle \n As mentioned above: the input_dataframe.pickle loads as a dataframe with header\n Date,   Adj Close,  news_title \n For chinese, you only need use chinese word segmenter to segment each chinese news\ntitle with a space \\\' \\\' \n An example row: \n 2006-10-20  108.961449 [["将 句子 从 繁体 转化 为 简体","00:00:00"],["将 句子 从 繁体 转化 为 简体\n","01:00:00"]] \n references: \n [1]  Ding, Xiao, Yue Zhang, Ting Liu and Junwen Duan. “Deep Learning for Event-Driven Stock Prediction.” IJCAI (2015). \n[2]  Bojanowski, Piotr, Edouard Grave, Armand Joulin and Tomas Mikolov. “Enriching Word Vectors with Subword Information.” TACL 5 (2017): 135-146. \n[3]  Socher, Richard, Danqi Chen, Christopher D. Manning and Andrew Y. Ng. “Reasoning With Neural Tensor Networks for Knowledge Base Completion.” NIPS (2013). \n[4]  Mikolov, Tomas, Ilya Sutskever, Kai Chen, Gregory S. Corrado and Jeffrey Dean. “Distributed Representations of Words and Phrases and their Compositionality.” NIPS (2013). \n[5]  OpenIE 5.0 \n[6]  Zhongguo Li, Maosong Sun. Punctuation as Implicit Annotations for Chinese Word Segmentation. Computational Linguistics, vol. 35, no. 4, pp. 505-512, 2009.']
tensortalk,[]
jrf,['Anki decks for amateur radio exams \n \n \n Technician class: Valid from 2018-07-01 until 2022-06-30 \n \n \n General class: Valid from 2019-07-01 until 2023-06-30 \n \n \n Extra class: Valid from 2020-07-01 until 2024-06-30 \n \n', 'Monty Hall Game Simulation Code \n Code in a few different languages for your edification and enjoyment to simulate the Monty Hall Game.']
dink,['vagrant-nodedev \n vagrant-nodedev', 'metalsmith-starter \n Metalsmith Project Starter', 'Inception \n \n Boilerplate for front-end projects with scss, js, html. \n Prerequisites: Node.js and Bower \n First run:\n npm install && bower install \n Then run:\n gulp \n To-do: \n Better docs']
dusenberrymw,["Pine: Python Neural Networks \n A simple ANN library written entirely in vanilla Python 3 \n This has been a fun project that was started in order to further my understanding of neural nets and machine learning in general.  It's also being used for research in Emergency Medicine. \n Requirements \n Python3 installed (and accessible at  /usr/bin/env python3 ) \n Library \n View the  pine/  directory -> everything should be pretty well documented \n Use  python3 setup.py install  to install globally \n Command line script \n Use  ./bin/pineCLI.py  (add  -h  for help) \n Install Shell Command: \n To access pineCLI.py globally, run  ./installShellCommand.py .  This will install  pine  to /usr/local/bin/ as a soft link, allowing  bin/pineCLI.py  to be run as  pine  from any directory, assuming /usr/local/bin/ is on your path. \n Demo \n Use  ./demo/demo.py  for a quick, old demo.  Also, the bottom of demo.py can be edited to run other sample demo projects.   \n Workflow \n Checkout the  Pine Data Tools  repo for a sample workflow for running real projects.  Be sure to install the shell command first, as seen above.", "Pine-Data-Tools \n Suite of scripts for working with data files (.csv & .txt) and the  Pine ANN\nlibrary . \n Based partially on the FastML Phraug project (https://github.com/zygmuntz/phraug) \n Requirements \n \n Python3 installed (and accessible at '/usr/bin/env python3') \n Pine installed ('git clone https://github.com/dusenberrymw/Pine.git') \n Install the Pine shell command ('./installShellCommand.py' from within the Pine\nproject) \n \n Folders \n \n lib/ -> contains all the scripts that are controlled by pipeline.sh \n data/ -> your data.csv file, as well as created data files \n results/ -> the binary 'model', predictions, and stats \n \n Workflow \n \n Clone this to a directory for the project you're working on \n Open up pipeline.sh -> This is the main script.  Run this to go from start\n(the original data) to finish (trained model, testing stats, predictions).\nCustomize this to meet your workflow needs. \n lib/preprocess.py -> Use this as a first pass through your data for bulk\nconversions, such as converting text features to numeric ones.  Customize the\n'process()' method for your data. \n lib/csv2pine.py -> Use this to convert the original data into Pine's data format,\nwhich is very similar to that of Vowpal Wabbit.  Customize the\n'construct_line()' method for your data.  By this point, you'll want all your data to\nbe numeric. \n lib/split.py -> Use this to randomly split your data up into train and test sets.\nDefault is split 90/10.  Will print out the random seed used, and then can add\nthat to the pipeline.sh file to reproduce the same split. \n pine -> Use Pine for training and testing.  Pass in a network layout, where\nthe number of inputs neurons is equal to the number of features in your data,\nand the number of output neurons is equal to the size of your output vector.\nFor testing, you can just pass in and underscore instead of a layout.  An\nexample of all of this is in pipeline.sh. \n lib/classify.py -> Use this to convert raw prediction probabilities from Pine into\nclasses.  Can customize this to change threshold.  Only need this for\nclassification projects. \n lib/stats.py -> Use this to calculate stats on how well the model is performing.\nCustomize this as needed.  Can redirect output into file as well. \n Run with './pipeline.sh' \n \n Others \n \n lib/automate_passes_pine.py -> Can use this to determine ideal number of\npasses through the data \n lib/run_pine.py -> An older version of the pipeline workflow.  Basically a\npipelined version of bin/pineCLI.py in the Pine project. \n lib/rmse.py -> Determines RMSE (root mean square error) of the model.\nlib/stats.py is probably better to use now for classification projects. \n", 'Personal Blog:  mikedusenberry.com \n Built on top of the  Jekyll  project.', 'SICP: Notes & Worked Examples for the Wizard Book,  Structure and Interpretation of Computer Programs \n Goal \n This project will contain notes and solutions for the example problems as I work through  SICP  to gain deeper CS knowledge and explore the Lisp way of thinking. \n Links of interest \n \n SICP Online Text \n SICP Lecture Videos \n \n Current Lisp dev setup \n \n Vim 7.4 (with Python) \n Slimv Plugin \n MIT Scheme \n', 'Eigenfaces Demo \n What \n This is an  eigenfaces  demo written in the Octave/Matlab language to go along with  a recent blog post . \n Features \n \n Given a dataset of faces (the  cropped version  of the  Labeled Faces in the Wild (LFW)  dataset), this demo computes and visualizes the  eigenfaces , and shows the effect of projecting and recovering the original faces using these eigenfaces. \n \n Related Blog Post \n \n On Eigenfaces \n \n Dev notes \n \n Octave 3.8.1 \n displayImages.m  is based on a utility function (non-exercise) from the  Coursera ML course . \n', 'SystemML \n SystemML is a flexible, scalable machine learning (ML) language written in Java.\nSystemML\'s distinguishing characteristics are: (1) algorithm customizability,\n(2) multiple execution modes, including Standalone, Hadoop Batch, and Spark Batch,\nand (3) automatic optimization. \n Algorithm Customizability \n ML algorithms in SystemML are specified in a high-level, declarative machine learning (DML) language.\nAlgorithms can be expressed in either an R-like syntax or a Python-like syntax. DML includes\nlinear algebra primitives, statistical functions, and additional constructs. \n This high-level language significantly increases the productivity of\ndata scientists as it provides (1) full flexibility in expressing custom\nanalytics and (2) data independence from the underlying input formats and\nphysical data representations. \n Multiple Execution Modes \n SystemML computations can be executed in a variety of different modes. To begin with, SystemML\ncan be operated in Standalone mode on a single machine, allowing data scientists to develop\nalgorithms locally without need of a distributed cluster. Algorithms can be distributed across Hadoop or Spark.\nThis flexibility allows the utilization of an organization\'s existing resources and expertise. In addition, SystemML\ncan be operated via Java, Scala, and Python. SystemML also features an embedded API for scoring models. \n Automatic Optimization \n Algorithms specified in DML are dynamically compiled and optimized based on data and cluster characteristics\nusing rule-based and cost-based optimization techniques. The optimizer automatically generates hybrid runtime\nexecution plans ranging from in-memory single-node execution to distributed computations on Spark or Hadoop.\nThis ensures both efficiency and scalability. Automatic optimization reduces or eliminates the need to hand-tune\ndistributed runtime execution plans and system configurations. \n \n Building SystemML \n SystemML is built using  Apache Maven .\nSystemML will build on Windows, Linux, or MacOS and requires Maven 3 and Java 7 (or higher).\nTo build SystemML, run: \n mvn clean package\n \n \n Testing SystemML \n SystemML features a comprehensive set of integration tests. To perform these tests, run: \n cd system-ml\nmvn verify\n \n Note: these that these tests requires  R  to be installed and available as part of the PATH variable on the machine you are running these tests.  \n To install required packages for running integration tests, execute following command in R: \n install.packages(c("batch", "bitops", "boot", "caTools", "data.table", "doMC", "doSNOW", "ggplot2", "glmnet", "lda", "Matrix", "matrixStats", "moments", "plotrix", "psych", "reshape", "topicmodels", "wordcloud", "methods"), dependencies=TRUE)\n \n Known issue: package \'methods\' is not available for R version 3.2.1. In which case, please downgrade R to version 3.1.1.  \n \n Algorithms \n SystemML features a suite of algorithms that can be grouped into five broad categories:\nDescriptive Statistics, Classification, Clustering, Regression, and Matrix Factorization. Detailed descriptions of\nthese algorithms can be found in the Algorithm Reference packaged with SystemML. \n \n Linear Regression Example \n As an example of the capabilities and power of SystemML and DML, let\'s consider the Linear Regression algorithm.\nWe require sets of data to train and test our model. To obtain this data, we can either use real data or\ngenerate data for our algorithm. The  UCI Machine Learning Repository Datasets \nis one location for real data. Use of real data typically involves some degree of data wrangling. In the following\nexample, we will use SystemML to generate random data to train and test our model. \n This example consists of the following parts: \n \n Create DML Script to Generate Random Data \n Run DML Script to Generate Random Data \n Divide Generated Data into Two Sample Groups \n Split Label Column from First Sample \n Split Label Column from Second Sample \n Train Model on First Sample \n Test Model on Second Sample \n \n SystemML is distributed in several packages, including a standalone package.\nWe\'ll operate in Standalone mode in this example.\nIf you unpack the  .tar.gz  file, Standalone mode\ncan be executed either on Mac/Unix using the  runStandaloneSystemML.sh  script or on Windows using the\n runStandaloneSystemML.bat  file. \n \n Create DML Script to Generate Random Data \n Below, we have a  genLinearRegressionData.dml  script. In takes several named input parameters.\nIt generates a matrix of random data with a label column appended to this data matrix. \n #\n# This script generates random data for linear regression. A matrix is generated\n# consisting of a data matrix with a label column appended to it.\n#\n# INPUT PARAMETERS:\n# --------------------------------------------------------------------------------------------\n# NAME            TYPE    DEFAULT  MEANING\n# --------------------------------------------------------------------------------------------\n# numSamples      Int     ---      Number of samples\n# numFeatures     Int     ---      Number of features (independent variables)\n# maxFeatureValue Int     ---      Maximum feature value (absolute value)\n# maxWeight       Int     ---      Maximum weight (absolute value)\n# addNoise        Boolean ---      Determines whether noise should be added to Y\n# b               Double  ---      Intercept\n# sparsity        Double  ---      Controls the sparsity in the generated data (a value between 0 and 1)\n# output          String  ---      Location to write the generated data/label matrix\n# format          String  ---      Matrix output format\n# --------------------------------------------------------------------------------------------\n# OUTPUT: Matrix of random data with appended label column\n# ---------------------------------------------------------------------------------------------\n#\n# Example\n# ./runStandaloneSystemML.sh algorithms/datagen/genLinearRegressionData.dml -nvargs numSamples=1000 numFeatures=50 maxFeatureValue=5 maxWeight=5 addNoise=FALSE b=0 sparsity=0.7 output=linRegData.csv format=csv\n#\n\nX = Rand(cols=$numFeatures, max=1, min=-1, pdf="uniform", rows=$numSamples, seed=0, sparsity=$sparsity)\nX = X * $maxFeatureValue\n\nw = Rand(cols=1, max=1, min=-1, pdf="uniform", rows=$numFeatures, seed=0)\nw = w * $maxWeight\n\nY = X %*% w\nY = Y + $b\n\nif ($addNoise == TRUE) {\n    noise = Rand(cols=1, pdf="normal", rows=$numSamples, seed=0)\n    Y = Y + noise\n}\n\nZ = append(X,Y)\nwrite(Z, $output, format=$format)\n \n \n Run DML Script to Generate Random Data \n We can execute the  genLinearRegressionData.dml  script in Standalone mode using either the  runStandaloneSystemML.sh  or  runStandaloneSystemML.bat  file.\nIn this eample, we\'ll generate a matrix of 1000 rows of 50 columns of test data, with sparsity 0.7. In addition to this, a 51 st  column consisting of labels will\nbe appended to the matrix. \n ./runStandaloneSystemML.sh algorithms/datagen/genLinearRegressionData.dml -nvargs numSamples=1000 numFeatures=50 maxFeatureValue=5 maxWeight=5 addNoise=FALSE b=0 sparsity=0.7 output=linRegData.csv format=csv\n \n This generates the following files: \n linRegData.csv      # 1000 rows of 51 columns of doubles (50 data columns and 1 label column), csv format\nlinRegData.csv.mtd  # metadata file\n \n \n Divide Generated Data into Two Sample Groups \n Next, we\'ll create two subsets of the generated data, each of size ~50%. We can accomplish this using the  sample.dml  script.\nThis script will randomly sample rows from the  linRegData.csv  file and place them into 2 files. \n To do this, we need to create a csv file for the  sv  named argument (see  sample.dml  for more details),\nwhich I called  perc.csv . Based on the information in\n sample.dml , I created a  perc.csv  file containing the following: \n 0.5\n0.5\n \n This will create two sample groups of roughly 50 percent each. In addition, we need a  perc.csv.mtd  file, so I\ncopied the contents of another metadata file and modified the contents so that we have 2 rows and 1 column. \n { \n    "data_type": "matrix"\n    ,"value_type": "double"\n    ,"rows": 2\n    ,"cols": 1\n    ,"format": "csv"\n    ,"header": false\n    ,"sep": ","\n    ,"description": { "author": "SystemML" } \n}\n \n Now, the  sample.dml  script can be run. \n ./runStandaloneSystemML.sh algorithms/utils/sample.dml -nvargs X=linRegData.csv sv=perc.csv O=linRegDataParts ofmt=csv\n \n This script creates two partitions of the original data and places them in a  linRegDataParts  folder. The files created are\nas follows: \n linRegDataParts/1       # first partition of data, ~50% of rows of linRegData.csv, csv format\nlinRegDataParts/1.mtd   # metadata\nlinRegDataParts/2       # second partition of data, ~50% of rows of linRegData.csv, csv format\nlinRegDataParts/2.mtd   # metadata\n \n The  1  file contains the first partition of data, and the  2  file contains the second partition of data.\nAn associated metadata file describes\nthe nature of each partition of data. If we open  1  and  2  and look at the number of rows, we can see that typically\nthe partitions are not exactly 50% but instead are close to 50%. However, we find that the total number of rows in the\noriginal data file equals the sum of the number of rows in  1  and  2 . \n \n Split Label Column from First Sample \n The next task is to split the label column from the first sample. We can do this using the  splitXY.dml  script. \n ./runStandaloneSystemML.sh algorithms/utils/splitXY.dml -nvargs X=linRegDataParts/1 y=51 OX=linRegData.train.data.csv OY=linRegData.train.labels.csv ofmt=csv\n \n This splits column 51, the label column, off from the data. When done, the following files have been created. \n linRegData.train.data.csv        # training data of 50 columns, csv format\nlinRegData.train.data.csv.mtd    # metadata\nlinRegData.train.labels.csv      # training labels of 1 column, csv format\nlinRegData.train.labels.csv.mtd  # metadata\n \n \n Split Label Column from Second Sample \n We also need to split the label column from the second sample. \n ./runStandaloneSystemML.sh algorithms/utils/splitXY.dml -nvargs X=linRegDataParts/2 y=51 OX=linRegData.test.data.csv OY=linRegData.test.labels.csv ofmt=csv\n \n This splits column 51 off the data, resulting in the following files: \n linRegData.test.data.csv        # test data of 50 columns, csv format\nlinRegData.test.data.csv.mtd    # metadata\nlinRegData.test.labels.csv      # test labels of 1 column, csv format\nlinRegData.test.labels.csv.mtd  # metadata\n \n \n Train Model on First Sample \n Now, we can train our model based on the first sample. To do this, we utilize the  LinearRegDS.dml  (Linear Regression\nDirect Solve) script. Note that SystemML also includes a  LinearRegCG.dml  (Linear Regression Conjugate Gradient) algorithm \nfor situations where the number of features is large. \n ./runStandaloneSystemML.sh algorithms/LinearRegDS.dml -nvargs X=linRegData.train.data.csv Y=linRegData.train.labels.csv B=betas.csv fmt=csv\n \n This will generate the following files: \n betas.csv      # betas, 50 rows of 1 column, csv format\nbetas.csv.mtd  # metadata\n \n The LinearRegDS.dml script generates statistics to standard output similar to the following. \n BEGIN LINEAR REGRESSION SCRIPT\nReading X and Y...\nCalling the Direct Solver...\nComputing the statistics...\nAVG_TOT_Y,-2.160284487670675\nSTDEV_TOT_Y,66.86434576808432\nAVG_RES_Y,-3.3127468704080085E-10\nSTDEV_RES_Y,1.7231785003947183E-8\nDISPERSION,2.963950542926297E-16\nPLAIN_R2,1.0\nADJUSTED_R2,1.0\nPLAIN_R2_NOBIAS,1.0\nADJUSTED_R2_NOBIAS,1.0\nPLAIN_R2_VS_0,1.0\nADJUSTED_R2_VS_0,1.0\nWriting the output matrix...\nEND LINEAR REGRESSION SCRIPT\n \n Now that we have our  betas.csv , we can test our model with our second set of data. \n \n Test Model on Second Sample \n To test our model on the second sample, we can use the  GLM-predict.dml  script. This script can be used for both\nprediction and scoring. Here, we\'re using it for scoring since we include the  Y  named argument. Our  betas.csv \nfile is specified as the  B  named argument.   \n ./runStandaloneSystemML.sh algorithms/GLM-predict.dml -nvargs X=linRegData.test.data.csv Y=linRegData.test.labels.csv B=betas.csv fmt=csv\n \n This generates the following statistics to standard output. \n LOGLHOOD_Z,,FALSE,NaN\nLOGLHOOD_Z_PVAL,,FALSE,NaN\nPEARSON_X2,,FALSE,1.895530994504798E-13\nPEARSON_X2_BY_DF,,FALSE,4.202951207327712E-16\nPEARSON_X2_PVAL,,FALSE,1.0\nDEVIANCE_G2,,FALSE,0.0\nDEVIANCE_G2_BY_DF,,FALSE,0.0\nDEVIANCE_G2_PVAL,,FALSE,1.0\nLOGLHOOD_Z,,TRUE,NaN\nLOGLHOOD_Z_PVAL,,TRUE,NaN\nPEARSON_X2,,TRUE,1.895530994504798E-13\nPEARSON_X2_BY_DF,,TRUE,4.202951207327712E-16\nPEARSON_X2_PVAL,,TRUE,1.0\nDEVIANCE_G2,,TRUE,0.0\nDEVIANCE_G2_BY_DF,,TRUE,0.0\nDEVIANCE_G2_PVAL,,TRUE,1.0\nAVG_TOT_Y,1,,1.0069397725436522\nSTDEV_TOT_Y,1,,68.29092137526905\nAVG_RES_Y,1,,-4.1450397073455047E-10\nSTDEV_RES_Y,1,,2.0519206226041048E-8\nPRED_STDEV_RES,1,TRUE,1.0\nPLAIN_R2,1,,1.0\nADJUSTED_R2,1,,1.0\nPLAIN_R2_NOBIAS,1,,1.0\nADJUSTED_R2_NOBIAS,1,,1.0\n \n We see that the STDEV_RES_Y value of the testing phase is of similar magnitude\nto the value obtained from the model training phase. \n For convenience, we can encapsulate our DML invocations in a single script: \n #!/bin/bash\n\n./runStandaloneSystemML.sh algorithms/datagen/genLinearRegressionData.dml -nvargs numSamples=1000 numFeatures=50 maxFeatureValue=5 maxWeight=5 addNoise=FALSE b=0 sparsity=0.7 output=linRegData.csv format=csv\n\n./runStandaloneSystemML.sh algorithms/utils/sample.dml -nvargs X=linRegData.csv sv=perc.csv O=linRegDataParts ofmt=csv\n\n./runStandaloneSystemML.sh algorithms/utils/splitXY.dml -nvargs X=linRegDataParts/1 y=51 OX=linRegData.train.data.csv OY=linRegData.train.labels.csv ofmt=csv\n\n./runStandaloneSystemML.sh algorithms/utils/splitXY.dml -nvargs X=linRegDataParts/2 y=51 OX=linRegData.test.data.csv OY=linRegData.test.labels.csv ofmt=csv\n\n./runStandaloneSystemML.sh algorithms/LinearRegDS.dml -nvargs X=linRegData.train.data.csv Y=linRegData.train.labels.csv B=betas.csv fmt=csv\n\n./runStandaloneSystemML.sh algorithms/GLM-predict.dml -nvargs X=linRegData.test.data.csv Y=linRegData.test.labels.csv B=betas.csv fmt=csv\n \n In this example, we\'ve seen a small part of the capabilities of SystemML. For more detailed information,\nplease consult the SystemML Algorithm Reference and SystemML Language Reference.', 'SystemML-NN \n A deep learning library for  Apache SystemML . \n **Update**: The SystemML-NN library has been merged into the Apache SystemML project, and can be found at https://github.com/apache/systemml/tree/master/scripts/nn. \n Examples: \n Please see the  examples  folder for more detailed examples, or view the following two quick examples. \n Neural net for regression with vanilla SGD: \n ```python \n Imports \n source("nn/layers/affine.dml") as affine\nsource("nn/layers/l2_loss.dml") as l2_loss\nsource("nn/layers/relu.dml") as relu\nsource("nn/optim/sgd.dml") as sgd \n Generate input data \n N = 1024 # num examples\nD = 100 # num features\nt = 1 # num targets\nX = rand(rows=N, cols=D, pdf="normal")\ny = rand(rows=N, cols=t) \n Create 2-layer network: \n affine1 -> relu1 -> affine2 \n M = 64 # number of neurons\n[W1, b1] = affine::init(D, M)\n[W2, b2] = affine::init(M, t) \n Initialize optimizer \n lr = 0.05  # learning rate\nmu = 0.9  # momentum\ndecay = 0.99  # learning rate decay constant \n Optimize \n print("Starting optimization")\nbatch_size = 32\nepochs = 5\niters = 1024 / batch_size\nfor (e in 1:epochs) {\n  for(i in 1:iters) {\n    # Get next batch\n    X_batch = X[i:i+batch_size-1,]\n    y_batch = y[i:i+batch_size-1,] \n # Compute forward pass\nout1 = affine::forward(X_batch, W1, b1)\noutr1 = relu::forward(out1)\nout2 = affine::forward(outr1, W2, b2)\n\n# Compute loss\nloss = l2_loss::forward(out2, y_batch)\nprint("L2 loss: " + loss)\n\n# Compute backward pass\ndout2 = l2_loss::backward(out2, y_batch)\n[doutr1, dW2, db2] = affine::backward(dout2, outr1, W2, b2)\ndout1 = relu::backward(doutr1, out1)\n[dX_batch, dW1, db1] = affine::backward(dout1, X_batch, W1, b1)\n\n# Optimize with vanilla SGD\nW1 = sgd::update(W1, dW1, lr)\nb1 = sgd::update(b1, db1, lr)\nW2 = sgd::update(W2, dW2, lr)\nb2 = sgd::update(b2, db2, lr)\n \n }\n  # Decay learning rate\n  lr = lr * decay\n}\n``` \n Neural net for multi-class classification with dropout and SGD w/ Nesterov momentum: \n ```python \n Imports \n source("nn/layers/affine.dml") as affine\nsource("nn/layers/cross_entropy_loss.dml") as cross_entropy_loss\nsource("nn/layers/dropout.dml") as dropout\nsource("nn/layers/relu.dml") as relu\nsource("nn/layers/softmax.dml") as softmax\nsource("nn/optim/sgd_nesterov.dml") as sgd_nesterov \n Generate input data \n N = 1024 # num examples\nD = 100 # num features\nt = 5 # num targets\nX = rand(rows=N, cols=D, pdf="normal")\nclasses = round(rand(rows=N, cols=1, min=1, max=t, pdf="uniform"))\ny = matrix(0, rows=N, cols=t)\nparfor (i in 1:N) {\n  y[i, as.scalar(classes[i,1])] = 1  # one-hot encoding\n} \n Create network: \n affine1 -> relu1 -> dropout1 -> affine2 -> relu2 -> dropout2 -> affine3 -> softmax \n H1 = 64 # number of neurons in 1st hidden layer\nH2 = 64 # number of neurons in 2nd hidden layer\np = 0.5  # dropout probability\n[W1, b1] = affine::init(D, H1)\n[W2, b2] = affine::init(H1, H2)\n[W3, b3] = affine::init(H2, t) \n Initialize SGD w/ Nesterov momentum optimizer \n lr = 0.05  # learning rate\nmu = 0.5  # momentum\ndecay = 0.99  # learning rate decay constant\nvW1 = sgd_nesterov::init(W1); vb1 = sgd_nesterov::init(b1)\nvW2 = sgd_nesterov::init(W2); vb2 = sgd_nesterov::init(b2)\nvW3 = sgd_nesterov::init(W3); vb3 = sgd_nesterov::init(b3) \n Optimize \n print("Starting optimization")\nbatch_size = 64\nepochs = 10\niters = 1024 / batch_size\nfor (e in 1:epochs) {\n  for(i in 1:iters) {\n    # Get next batch\n    X_batch = X[i:i+batch_size-1,]\n    y_batch = y[i:i+batch_size-1,] \n # Compute forward pass\n## layer 1:\nout1 = affine::forward(X_batch, W1, b1)\noutr1 = relu::forward(out1)\n[outd1, maskd1] = dropout::forward(outr1, p, -1)\n## layer 2:\nout2 = affine::forward(outd1, W2, b2)\noutr2 = relu::forward(out2)\n[outd2, maskd2] = dropout::forward(outr2, p, -1)\n## layer 3:\nout3 = affine::forward(outd2, W3, b3)\nprobs = softmax::forward(out3)\n\n# Compute loss\nloss = cross_entropy_loss::forward(probs, y_batch)\nprint("Cross entropy loss: " + loss)\n\n# Compute backward pass\n## loss:\ndprobs = cross_entropy_loss::backward(probs, y_batch)\n## layer 3:\ndout3 = softmax::backward(dprobs, out3)\n[doutd2, dW3, db3] = affine::backward(dout3, outd2, W3, b3)\n## layer 2:\ndoutr2 = dropout::backward(doutd2, outr2, p, maskd2)\ndout2 = relu::backward(doutr2, out2)\n[doutd1, dW2, db2] = affine::backward(dout2, outd1, W2, b2)\n## layer 1:\ndoutr1 = dropout::backward(doutd1, outr1, p, maskd1)\ndout1 = relu::backward(doutr1, out1)\n[dX_batch, dW1, db1] = affine::backward(dout1, X_batch, W1, b1)\n\n# Optimize with SGD w/ Nesterov momentum\n[W1, vW1] = sgd_nesterov::update(W1, dW1, lr, mu, vW1)\n[b1, vb1] = sgd_nesterov::update(b1, db1, lr, mu, vb1)\n[W2, vW2] = sgd_nesterov::update(W2, dW2, lr, mu, vW2)\n[b2, vb2] = sgd_nesterov::update(b2, db2, lr, mu, vb2)\n[W3, vW3] = sgd_nesterov::update(W3, dW3, lr, mu, vW3)\n[b3, vb3] = sgd_nesterov::update(b3, db3, lr, mu, vb3)\n \n }\n  # Anneal momentum towards 0.999\n  mu = mu + (0.999 - mu)/(1+epochs-e)\n  # Decay learning rate\n  lr = lr * decay\n}\n```', 'Dotfiles \n A set of various dotfiles. \n \n Install with  ./install.sh . \n', '\n{% comment %}\nLicensed to the Apache Software Foundation (ASF) under one or more\ncontributor license agreements.  See the NOTICE file distributed with\nthis work for additional information regarding copyright ownership.\nThe ASF licenses this file to you under the Apache License, Version 2.0\n(the "License"); you may not use this file except in compliance with\nthe License.  You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an "AS IS" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n{% endcomment %}\n Deep Learning for Breast Cancer Mitosis Detection and Tumor Proliferation Score Prediction \n Overview \n The  Tumor Proliferation Assessment Challenge 2016 (TUPAC16)  is a "Grand Challenge" that was created for the  2016 Medical Image Computing and Computer Assisted Intervention (MICCAI 2016)  conference.  In this challenge, the goal is to develop state-of-the-art algorithms for automatic prediction of tumor proliferation scores from whole-slide histopathology images of breast tumors. \n Background \n Breast cancer is the leading cause of cancerous death in women in less-developed countries, and is the second leading cause of cancerous deaths in developed countries, accounting for 29% of all cancers in women within the U.S. [1]. Survival rates increase as early detection increases, giving incentive for pathologists and the medical world at large to develop improved methods for even earlier detection [2].  There are many forms of breast cancer including Ductal Carcinoma in Situ (DCIS), Invasive Ductal Carcinoma (IDC), Tubular Carcinoma of the Breast, Medullary Carcinoma of the Breast, Invasive Lobular Carcinoma, Inflammatory Breast Cancer and several others [3]. Within all of these forms of breast cancer, the rate in which breast cancer cells grow (proliferation), is a strong indicator of a patient’s prognosis. Although there are many means of determining the presence of breast cancer, tumor proliferation speed has been proven to help pathologists determine the best treatment for the patient. The most common technique for determining the proliferation speed is through mitotic count (mitotic index) estimates, in which a pathologist counts the dividing cell nuclei in hematoxylin and eosin (H&E) stained slide preparations to determine the number of mitotic bodies.  Given this, the pathologist produces a proliferation score of either 1, 2, or 3, ranging from better to worse prognosis [4]. Unfortunately, this approach is known to have reproducibility problems due to the variability in counting, as well as the difficulty in distinguishing between different grades. \n References:  \n[1] http://emedicine.medscape.com/article/1947145-overview#a3  \n[2] http://emedicine.medscape.com/article/1947145-overview#a7  \n[3] http://emedicine.medscape.com/article/1954658-overview  \n[4] http://emedicine.medscape.com/article/1947145-workup#c12  \n Goal & Approach \n Mitosis Detection \n At a high level, as shown in Figure 1, our approach begins by preprocessing a dataset of regions of tissue from whole slide images (WSIs) of breast tumors into a dataset of mitotic and non-mitotic patches. We then train a convolutional neural network (CNN) model to predict the presence of a mitotic figure in a given patch. Given an initial trained model, we preprocess the raw dataset again with model-based false-positive (FP) oversampling to generate a more difficult training dataset. We then train a new model on this second dataset. To make predictions on a new image region, we apply the model to the image in a sliding window fashion with noise marginalization, yielding a prediction at each location. A clustering algorithm is then used to smooth the potentially noisy set of predictions into a set of final predictions for mitosis locations. \n Tumor Proliferation Score Prediction (previous approach) \n In an effort to automate the process of classification, this project aims to develop a large-scale deep learning approach for predicting tumor scores directly from the pixels of whole-slide histopathology images (WSI).  Our proposed approach is based on a recent research paper from Stanford [1].  Starting with 500 extremely high-resolution tumor slide images [2] with accompanying score labels, we aim to make use of Apache Spark in a preprocessing step to cut and filter the images into smaller square samples, generating 4.7 million samples for a total of ~7TB of data [3].  We then utilize TensorFlow and Keras to train a deep convolutional neural network on these samples, making use of transfer learning by fine-tuning a modified ResNet50 model [4].  Our model takes as input the pixel values of the individual samples, and is trained to predict the correct tumor score classification for each one.  We also explore an alternative approach of first training a mitosis detection model [5] on an auxiliary mitosis dataset, and then applying it to the WSIs, based on an approach from Paeng et al. [6].  Ultimately, we aim to develop a model that is sufficiently stronger than existing approaches for the task of breast cancer tumor proliferation score classification. \n References:  \n[1] https://web.stanford.edu/group/rubinlab/pubs/2243353.pdf  \n[2] http://tupac.tue-image.nl/node/3  \n[3]  preprocess.py ,  breastcancer/preprocessing.py   \n[4]  MachineLearning-Keras-ResNet50.ipynb   \n[5]  preprocess_mitoses.py ,  train_mitoses.py   \n[6] https://arxiv.org/abs/1612.07180 \n \n Steps for Mitosis Detection \n Packages \n \n pip3 install -U numpy keras tensorflow pillow pandas dask scikit-learn pytest \n \n Help \n \n all script files have a  --help  option that will output the command-line options \n \n Raw data (on a shared filesystem on the ram machines): \n \n images:  data/mitoses/mitoses_train_image_data  (the preprocessing code expects, by default, the  data/mitoses/mitoses_train_image_data  path to be available from the base directory of the project) \n labels:  data/mitoses/mitoses_train_ground_truth , (same as above for preprocessing code) \n \n Preprocessing \n \n basic:  python3 preprocess_mitoses.py --save_path=data/mitoses/patches --rotations_train=0 --translations_train=0 --p_train=0.0001 --p_val=0.0001 --seed 0 \n \n Training \n \n basic logreg:  python3 train_mitoses.py --patches_path=data/mitoses/patches --model=logreg \n basic logreg on GPU 0 of a GPU server:  CUDA_VISIBLE_DEVICES=0 python3 train_mitoses.py --patches_path=data/mitoses/patches --model=logreg \n \n Evaluation (optional) \n \n eval above model:  python3 eval_mitoses.py --model_name=logreg --model_path=PATH_TO_EXPERIMENT_FOLDER_FROM_TRAINING/checkpoints/BEST_MODEL_IN_THIS_FOLDER.hdf5 --patches_path=data/mitoses/patches/val \n eval above model on GPU 0 of a GPU server:  CUDA_VISIBLE_DEVICES=0 python3 eval_mitoses.py --model_name=logreg --model_path=PATH_TO_EXPERIMENT_FOLDER_FROM_TRAINING/checkpoints/BEST_MODEL_IN_THIS_FOLDER.hdf5 --patches_path=data/mitoses/patches/val \n \n Hyperparameter Tuning \n \n basic logreg:  python3 hyperparam_tune_mitoses.py --patches_path=data/mitoses/patches --models logreg --log_interval 1000 \n basic logreg on GPU 0 of a GPU server:  CUDA_VISIBLE_DEVICES=0 python3 hyperparam_tune_mitoses.py --patches_path=data/mitoses/patches --models logreg --log_interval 1000 \n \n Testing \n \n pytest file.py \n \n Prediction template \n ```\nimport numpy as np\nfrom tf.keras.models import load_model \n from train_mitoses import normalize \n model_file = ...  # hdf5 file, no sigmoid at the end\nmodel_name = ....  # "vgg", "resnet", or "logreg" currently\nthreshold = ...  # scalar value\npatch_batch = ...  # numpy array of shape (N, 64, 64, 3) containing N images \n load the model and add the sigmoid layer if we want to use easier-to-interpret threshold values \n base_model = load_model(model_file, compile=False)\nprobs = keras.layers.Activation(\'sigmoid\', name="sigmoid")(base_model.output)\nmodel = keras.models.Model(inputs=base_model.input, outputs=probs) \n prediction \n norm_patch_batch = normalize((np.array(patch_batch) / 255).astype(np.float32), model_name)  # shape (N, 64, 64, 3)\nout_batch = model.predict_on_batch(norm_patch_batch)  # shape (N, 1) with probs\npred = out_batch > threshold  # shape (N, 1) with binary predictions\n``` \n Setup for Tumor Proliferation Score Prediction ( All nodes  unless other specified): \n \n System Packages: \n openslide \n Python packages: \n Basics \n pip3 install -U matplotlib numpy pandas scipy jupyter ipython scikit-learn scikit-image openslide-python \n \n \n TensorFlow (only on driver): \n pip3 install tensorflow-gpu  (or  pip3 install tensorflow  for CPU-only) \n \n \n Keras (bleeding-edge; only on driver): \n pip3 install git+https://github.com/fchollet/keras.git \n \n \n Spark 2.x (ideally bleeding-edge) \n Add the following to the  data  folder (same location on  all  nodes): \n training_image_data  folder with the training slides. \n testing_image_data  folder with the testing slides. \n training_ground_truth.csv  file containing the tumor & molecular scores for each slide. \n mitoses  folder with the following from the mitosis detection auxiliary dataset: \n mitoses_test_image_data  folder with the folders of testing images \n mitoses_train_image_data  folder with the folders of training images \n mitoses_train_ground_truth  folder with the folders of training csv files \n \n \n Layout:\n  ``` \n MachineLearning-Keras-ResNet50.ipynb \n breastcancer/ \n preprocessing.py \n visualization.py \n \n \n ... \n data/ \n mitoses \n mitoses_test_image_data \n 01 \n 01.tif \n 02 \n 01.tif\n... \n \n \n mitoses_train_ground_truth \n 01 \n 01.csv \n 02.csv\n  ... \n 02 \n 01.csv \n 02.csv\n  ...\n... \n \n \n mitoses_train_image_data \n 01 \n 01.tif \n 02.tif\n  ... \n 02 \n 01.tif \n 02.tif\n  ...\n... \n \n \n training_ground_truth.csv \n training_image_data \n TUPAC-TR-001.svs \n TUPAC-TR-002.svs \n ... \n testing_image_data \n TUPAC-TE-001.svs \n TUPAC-TE-002.svs \n ... \n \n \n preprocess.py \n preprocess_mitoses.py \n \n train_mitoses.py\n  ``` \n \n \n Adjust the Spark settings in  $SPARK_HOME/conf/spark-defaults.conf  using the following examples, depending on the job being executed: \n \n \n All jobs:\n     # Use most of the driver memory.\n    spark.driver.memory 70g\n    # Remove the max result size constraint.\n    spark.driver.maxResultSize 0\n    # Increase the message size.\n    spark.rpc.message.maxSize 128\n    # Extend the network timeout threshold.\n    spark.network.timeout 1000s\n    # Setup some extra Java options for performance.\n    spark.driver.extraJavaOptions -server -Xmn12G\n    spark.executor.extraJavaOptions -server -Xmn12G\n    # Setup local directories on separate disks for intermediate read/write performance, if running\n    # on Spark Standalone clusters.\n    spark.local.dirs /disk2/local,/disk3/local,/disk4/local,/disk5/local,/disk6/local,/disk7/local,/disk8/local,/disk9/local,/disk10/local,/disk11/local,/disk12/local \n \n \n Preprocessing:\n     # Save 1/2 executor memory for Python processes\n    spark.executor.memory 50g \n \n \n To execute the WSI preprocessing script, use  spark-submit  as follows (could also use Yarn in client mode with  --master yarn --deploy-mode client ):\n   PYSPARK_PYTHON=python3 spark-submit --master spark://MASTER_URL:7077 preprocess.py \n \n \n To execute the mitoses preprocessing script, use the following:\n   python3 preprocess_mitoses.py --help \n \n \n To execute the mitoses training script, use the following:\n   python3 training_mitoses.py --help \n \n \n To use the Jupyter notebooks, start up Jupyter like normal with  jupyter notebook  and run the desired notebook. \n \n \n Create a Histopath slide “lab” to view the slides (just driver): \n \n git clone https://github.com/openslide/openslide-python.git \n Host locally: \n python3 path/to/openslide-python/examples/deepzoom/deepzoom_multiserver.py -Q 100 path/to/data/ \n \n \n Host on server: \n python3 path/to/openslide-python/examples/deepzoom/deepzoom_multiserver.py -Q 100 -l HOSTING_URL_HERE path/to/data/ \n Open local browser to  HOSTING_URL_HERE:5000 . \n \n \n', "Mixture Density Networks \n An exploration of mixture density networks. \n Background \n A  mixture density network  (MDN) is an interesting model formalism built within the general framework of neural networks and probability theory for working on supervised learning problems in which the target variable cannot be easily approximated by a single standard probability distribution.  Essentially, an MDN allows one to model a conditional probability distribution p(y|x) as a mixture of distributions, in which the individual distributions and the corresponding mixture coefficients are parameterized by functions of the inputs x. \n Notebook \n \n mixture_density_networks.ipynb \n \n Blog post \n \n mikedusenberry.com/mixture-density-networks \n \n Resources \n \n Pattern Recognition and Machine Learning (PRML) , pp. 272-277, pp. 430-435 \n @hardmaru's  MDN blogs:  http://blog.otoro.net/2015/06/14/mixture-density-networks ,  http://blog.otoro.net/2015/11/24/mixture-density-networks-with-tensorflow \n http://edwardlib.org/tutorials/mixture-density-network \n"]
manrajgrover,['Memory-Game \n Memory Game based on jQuery,CSS and HTML \n Live Demo can be seen at http://manraj.collegespace.in/Experiments/Memory/', 'Sentimental Analysis \n Sentimental analysis of tweets related to airlines. \n What is it? \n This project uses Random Forest Classifier for finding sentiment of a tweet. There are three possible output: \n \n Positive \n Neutral \n Negative \n \n On the given dataset, it achieves 0.75 accuracy score. \n Installation \n Dataset used \n To download dataset, click  here \n How to use \n To be added \n Dependencies \n To be added \n License \n MIT  © Manraj Singh', 'Treasure-Hunt-Platform \n Platform developed on Php, MySQL, jQuery, CSS and HTML with Facebook Login implemented. \n Developed by Manraj Singh & Divjot Singh during their college festival season for Online Competitions. \n Successfully used for more than 10 college events. ', 'Live-Web-Document-Edit \n A live web document edit using PeerJS', 'Live-Search \n A live search with tags implementation created using Php,jQuery,CSS and Html.You can see it live here http://manraj.collegespace.in/Experiments/livesearch/', 'Python-Keylogger \n A keylogger developed using Python.Bind it with any software to start and log keystrokes.', 'Python-Socket-Chat \n Socket chat developed using Python.Run server first and then launch client to chat.', "\n geekiT \n     \n Google Chrome extension to mark problems solved on GeeksForGeeks.org so as to focus on unsolved problems. Also view your analysis. \n Installation \n     \n Or manually: \n \n Download or clone it using  git clone https://github.com/ManrajGrover/geekiT.git . \n Click on Settings and open Extension's tab. \n Click on the Load unpacked extension button and select the downloaded directory. \n \n License \n MIT  © Manraj Singh", 'Codes \n Contains solutions of questions of various judges and new concepts I encounter.', 'PageRank \n PageRank Implementation in Python', 'WebCrawler \n WebCrawler implementation in Python ', 'Ideone-Solutions-Downloader \n Judged in top 10 best under 20 lines JavaScript code hack on Challengepost(Now DevPost), its a code downloader to be used to download personal submissions on Ideone for backup purpose. \n Why this? \n Because Ideone APIs don\'t allow us to download all the codes at once. \n How to use? \n \n Copy the code from ConsoleCode.js and paste it in your Browser\'s Console. You can open it by pressing CTRL+Shift+J. Press Enter. \n After running the script and closing the newly opened browser windows you might have to go to your downloads area in your browser and press allow to download if it is a python file. \n \n You can watch video tutorial  here \n FAQs \n Chrome doesn\'t automatically download Python Codes.What to do? \n You can disable the enable malware protection option in Chrome while downloading and enable it back. But I won\'t recommend it. \n License \n ``` \n The MIT License (MIT) \n Copyright (c) 2014 Manraj Singh \n Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: \n The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. \n THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. \n ```', 'PCifi \n Convert PC/Laptop Into Wifi Hotspot \n How to use it? \n \n Download the file. \n Edit it using NotePad.Change YourSSID(Wifi Name) and YourPassword(Your Password) to what you require. \n Save it. \n Run the file in Admin mode. \n Set Up the Wifi if running for first time. \n Choose option 2 to start Wifi. \n Enjoy. :) \n', 'RAKE-Algorithm \n Implementation Of RAKE Algorithm using Python \n Can be extended to summarize text based on Phrase Score.(To be done)', 'Hindi Ontology \n A project on improving techniques of Ontology', "shortiT ( currently under work ) \n Personal URL shortening service. \n To Do's \n \n Do security checks \n Improve UI \n Better display messages \n Front end error checks \n Backend error checks \n", 'chat[ currently under development ] \n Web and Android Chat Application', 'manrajsingh.in \n \n Code powering my current  website . For getting code for blog, please see the  blog  repo. \n \n Build instructions \n To install all gems, run: \n sh\n$ bundle install \n To build the website locally, run: \n sh\n$ JEKYLL_ENV=production bundle exec jekyll build \n To serve the website locally, run: \n sh\n$ bundle exec jekyll serve --config _config.yml,_config-dev.yml \n To push the changes, run: \n sh\n$ git push origin master \n License \n MIT  ©  Manraj Singh', 'MathLab \n A repository to implement and maintain commonly used math related functions needed during coding competitions.', 'Bulk-URL-Shortener \n A 45 minute project to create a service for shortening URLs in bulk. \n  ', 'NGO-MAN \n Angel Hack Hackathon - This is an app we made during Angel Hack Hackathon held on June 20, 2015 - June 21, 2015. \n It is an app made for the low-cost Mozilla firefox OS devices. \n Our application revolves around the idea of simplifying various tasks of management and control; an employee of an NGO has to carry out. It helps in keeping a track of donations of various things like food, clothes, money and books. \n Contributors \n \n Manraj Singh Grover (@ManrajGrover) \n Sanyam Khurana (@CuriousLearner) \n Vikram Jaswal (@vikramj74) \n Nikhil Handa (@slayslot) \n Amit Tripathi (@amitt001) \n', "Puzzles \n A repository for collecting all interesting puzzles found in interviews and internet \n Hard Interview Puzzles \n \n There is a village of wizards and a village of dwarves. Once a year, the wizards go over to the village of dwarves and line all the dwarves up in increasing height order, such that each dwarf can only see the dwarves smaller than himself. The wizards have an infinite supply of white and black hats. They place either a white or black hat on the head of each dwarf. Then, starting with the tallest dwarf (in the back of the line), they ask each what color hat he is wearing. If the dwarf answers incorrectly, the wizards kill him (the other dwarves can hear his answer, but can't tell if he was killed or not). What strategy can the dwarves use to minimize the number of dwarves that are killed? What is the most number of dwarves that will be killed using this optimal strategy? ( Solution ) \n Consider a two-player game played on a circular table of unspecified diameter. Each player has an infinite supply of quarters, and take turns placing a quarter on the table such that it is completely on the table and does not overlap with any other quarters already played. A player wins if he makes the last legal move. Which player (if any) has a strategy that will guarantee a win, and what is that strategy? ( Solution ) \n How do you reverse the order of the words (not the characters) in a string of length n in with constant extra space in linear time? ( Solution ) \n How do you rotate a string of length n by m characters with constant extra space in linear time (wrt n)? ( Solution ) \n Consider a rectangular cake with a rectangular section (of any size or orientation) removed from it. How do you divide the cake exactly in half with only one cut? ( Solution ) \n You have a bar of chocolate that consists of n x m square blocks. If you can only break one piece at a time, how many breaks are necessary to break the original n x m piece into n*m 1 x 1 pieces? How many are sufficient? ( Solution ) \n How do you quickly count the number of set bits in a 32-bit integer in linear time (with respect to the number of set bits)? In constant time? ( Solution ) \n Given an array of size N that contains values between 1 and N-1, find the duplicate element (assuming there is only one). If it contains values between 1 and N+1, how would you find the missing element (again assuming there is only one missing)? Do each in O(N). ( Solution ) \n Give a one-line C expression to test whether an unsigned int is a power of two.( Solution ) \n How many points are there on the globe where by walking one mile south, one mile east and one mile north you reach the place where you started? ( Solution ) \n Given a singly linked list, determine whether it contains a loop or not. ( Solution ) \n Every day, Joe arrives at the train station from work at 6pm. His wife leaves home in her car to meet him there at exactly 6pm, and drives him home. One day, Joe gets to the station an hour early, and starts walking home, until his wife meets him on the road. They get home 20 minutes earlier than usual. How long was he walking? Distances are unspecified. Speeds are unspecified, but constant. ( Solution ) \n How do you divide a cake among n people, maximizing fairness? ( Solution ) \n In your cellar there are three light switches in the OFF position. Each switch controls one of three light bulbs on floor above. You may move any of the switches but you may only go upstairs to inspect the bulbs one time. How can you determine the switch for each bulb with one inspection? ( Solution ) \n Alice and Bob are on separate islands. Bob is sick, and Alice has the medicine. Eve has a boat and a chest that can be locked. She is willing to transport objects between Alice and Bob, but only in the chest, and if the chest is unlocked, she will steal whatever is inside. If both Alice and Bob have a padlock and a key such that their own key only opens their own lock, how can Alice send Bob the medicine so that Eve won't steal it? ( Solution ) \n Write some code to convert a positive integer into base minus 2. That is, whereas base 2 has a 1's place, a 2's place, a 4's place, etc., base minus 2 has a 1's place, a minus 2's place, a 4's place, a minus 8's place, ... (-2)^n. ( Solution ) \n A couple invites n-1 other couples to dinner. Once everyone arrives, each person shakes hands with everyone he doesn't know. Then, the host asks everyone how many hands they shook, and each person replies with a different number. Assuming that everyone knows his or her own spouse, how many hands did the hostess shake? ( Solution ) \n Two robots start at different places on the same linear track. What one program can you give to both robots to guarantee that they meet? The the program may consist only of the instructions move_left n, move_right n (where n is the number of spaces to move), if statements while loops, and the boolean values at_own_start and at_other_robots_start (note that you can't use other variables or counters). ( Solution ) \n Which offer is better and why? \n You are to make a statement. If the statement is true, you get exactly $10. If the statement is false, you get either less than or more than $10 but not exactly $10. \n You are to make a statement. Regardless of whether the statement is true or false, you get more than $10.\n( Solution ) \n You have two ropes and a box of matches. Each rope takes exactly one hour to burn, but they may not necessarily burn evenly -- i.e., the first half might burn in the first 10 minutes and the second in the remaining 50). How can you measure out 45 minutes by just using these two ropes? ( Solution ) \n Consider three identical airplanes starting at the same airport. Each plane has a fuel tank that holds just enough fuel to allow the plane to travel half the distance around the world. These airplanes possess the special ability to transfer fuel between their tanks in mid-flight. Devise a scheme that will allow one airplane to travel all the way around the world, landing only at the original airport. ( Solution ) \n You are at the bottom of the elevator shaft of a 100 story building. You see 21 wires labelled 1...21. The wires go up to the 100th floor where the ends are labelled A...U, but you don't know how they correspond to the ends at the bottom. You have a battery, a light bulb, and many small wires. How can you determine the pairing between the numbers and letters by only making one trip to the 100th floor and back down? ( Solution ) \n A woman starts paddling upstream in a canoe, and after one mile, encounters a log floating with the current. She continues to paddle upstream for onehour, then turns around and paddles downstream, until she returns to the dock where she started. If the woman and the log reach the dock at exactly the same time, how fast was the current flowing? Assume all speeds are constant. ( Solution ) \n Consider a centrifuge with 12 slots for test tubes. When you use a centrifuge, the tubes must be placed in the slots so that they are radially balanced (we can assume all tubes have the same mass). For example, for 3 tubes, you would place them in slots 4, 8 and 12. How can you place exactly 5 tubes in the centrifuge so that they are radially balanced? ( Solution ) \n You are on a strict medical regimen that requires you to take two types of pills each day. You must take exactly one A pill and exactly one B pill at the same time. The pills are very expensive, and you don't want to waste any. So you open the bottle of A pills, and tap one out into your hand. Then you open the bottle of B pills and do the same thing -- but you make a mistake, and two B pills come out into your hand with the A pill. But the pills are all exactly identical. There is no way to tell A pills apart from B pills. How can you satisfy your regimen and take exactly one of each pill at the same time, without wasting any pills? ( Solution ) \n Write an algorithm to find a given element in an n by n matrix where the rows and columns are monotonically increasing. ( Solution ) \n General Alice and General Bob, commanders of the allied armies A and B, respectively, are camped in the mountains on either side of a valley. Alice and Bob would like to attack enemy army C, camped in the valley below. Army A by itself is unable to defeat army C, as is army B, but a coordinated attack by A and B at the same time will secure a victory for Alice and Bob. However, the only way Alice and Bob can communicate is by sending messengers through the valley, who may or may not get captured en route by the enemy army C. Is there an algorithm by which Alice and Bob can coordinate an attack on army C so as to secure their victory? ( Solution ) \n Consider a circular race track with n gas stations spaced along it, each containing a fixed amount of gas. You are given an array containing the distances between consecutive gas stations and an array containing the amount of gas at each. Suppose the total amount of gas at all the gas stations is the same as the number of miles around the race track. Your car gets one mile to the gallon, but its gas tank has an unlimited capacity. Where do you start your car along the race track to guarantee that you get all the way around without running out of gas? Do this in O(n) time. ( Solution ) \n Given an array of n integers, find all Pythagorean triples in the array, that is, three elements such that a^2 + b^2 = c^2. Do this in O(n^2) time. ( Solution ) \n You are on a spaceship that has a computer with n processors. Suddenly, the spaceship gets hit with an alien laser beam, and some of the processors are damaged. However, you know that more than half of the processors are still good. You can ask one processor whether it thinks another processor is good or bad. A good processor will always tell the truth, but a bad one will always lie. A 'step' consists of asking one processor if it thinks another processor is good or bad. Find one good processor, only using n-2 steps. ( Solution ) \n Given an array of n integers, where one element appears more than n/2 times, find that element in linear time and constant extra space. ( Solution ) \n A spinning disc is painted black on one half and white on the other (i.e., the line forming the border between the black and white regions of the disc is a diameter of the disc). The disk is spinning on a turntable in an unknown direction at an unknown speed. You have special video cameras that can observe the color of a single point on the disc. How many cameras do you need to determine the direction the disc is spinning? ( Solution ) \n Create an equilateral triangle using three toothpicks. Now add three more equilateral triangles of the same size as the original using only three more toothpicks. ( Solution ) \n Write a method to generate a random number between 1 and 7, given a method that generates a random number between 1 and 5. The distribution between each of the numbers must be uniform.( Solution ) \n You have two identical eggs. Standing in front of a 100 floor building, you wonder what is the maximum number of floors from which the egg can be dropped without breaking it. What is the minimum number of tries needed to find out the solution?( Solution ) \n Four people need to cross a rickety bridge at night. Unfortunately, they have only one torch and the bridge is too dangerous to cross without one. The bridge is only strong enough to support two people at a time. Not all people take the same time to cross the bridge. Times for each person:  1 min, 2 mins, 7 mins and 10 mins. What is the shortest time needed for all four of them to cross the bridge?( Solution ) \n The probability of a car passing a certain intersection in a 20 minute windows is 0.9. What is the probability of a car passing the intersection in a 5 minute window? (Assuming a constant probability throughout)( Solution ) \n How would you cut a rectangular cake into two equal pieces when a rectangular piece has already been cut out of it? The cut piece can be of any size and orientation. You are only allowed to make one straight cut.( Solution ) \n \n Two old friends, Jack and Bill, meet after a long time. \n Jack: Hey, how are you man? \n Bill: Not bad, got married and I have three kids now. \n Jack: That’s awesome. How old are they? \n Bill: The product of their ages is 72 and the sum of their ages is the same as your birth date. \n Jack: Cool… But I still don’t know. \n Bill: My eldest kid just started taking piano lessons. \n Jack: Oh now I get it. \n How old are Bill’s kids?( Solution ) \n \n", '  ██████  ▄████▄  ▓█████  ██▓███  ▄▄▄█████▓ ██▀███  ▓█████ \n▒██    ▒ ▒██▀ ▀█  ▓█   ▀ ▓██░  ██▒▓  ██▒ ▓▒▓██ ▒ ██▒▓█   ▀ \n░ ▓██▄   ▒▓█    ▄ ▒███   ▓██░ ██▓▒▒ ▓██░ ▒░▓██ ░▄█ ▒▒███   \n  ▒   ██▒▒▓▓▄ ▄██▒▒▓█  ▄ ▒██▄█▓▒ ▒░ ▓██▓ ░ ▒██▀▀█▄  ▒▓█  ▄ \n▒██████▒▒▒ ▓███▀ ░░▒████▒▒██▒ ░  ░  ▒██▒ ░ ░██▓ ▒██▒░▒████▒\n▒ ▒▓▒ ▒ ░░ ░▒ ▒  ░░░ ▒░ ░▒▓▒░ ░  ░  ▒ ░░   ░ ▒▓ ░▒▓░░░ ▒░ ░\n░ ░▒  ░ ░  ░  ▒    ░ ░  ░░▒ ░         ░      ░▒ ░ ▒░ ░ ░  ░\n░  ░  ░  ░           ░   ░░         ░        ░░   ░    ░   \n      ░  ░ ░         ░  ░                     ░        ░  ░\n         ░\n', "Hack Kuku-Kube \n \n JavaScript Hacks for  Kuku Kube - Eye Test Game  and  Kuku Kube 2 . \n \n How to use it? \n \n Copy the code for the game you're going to play, you can find them below: \n Kuku Kube \n Kuku Kube 2 \n Go to game's web page and open JavaScript Console. \n Paste the copied code in the console \n Start the game and go back to console and hit enter to run the code. \n Sit back and get highest score. \n \n License \n MIT  © Manraj Singh", 'battery_status \n A Python CLI for Windows to notify when battery is below certain set percentage \n Dependencies \n Windows Management Instrumentation module(WMI) \n How to install: \n You can install it using pip. Use command  pip install battery_status  in terminal.\nAlternatively you can use  easy_install battery_status  to install it. \n How to use: \n Since this is a background application, you can run it in Bash using commands  check_battery_status  for checking battery percentage and  start_battery_notifier  for getting notification on 20 percentage or less. You will hear sound as notification.', 'Exploring-Stuff \n Random projects while learning new things.', 'Tutorials-PDF \n Download tutorial pdfs from http://www.tutorialspoint.com/ \n License \n ```\nThe MIT License (MIT) \n Copyright (c) 2015 Manraj Singh \n Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the "Software"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions: \n The above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software. \n THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n```', 'Mastering Android \n Here I will upload all my basic Android Apps', "Github Summary \n Web and Android App for knowing more about a Github user \n \n License & Credits \n Inspired by  Dan Foreman-Mackey 's  Open Source Report Card \n Github Summary was created by  Manraj Singh  and is made available under  the MIT license", 'Mastering Meteor \n A curated list of delightful Meteor packages and resources.\nThis repository also includes  boilerplate  for MeteorJS. \n Documentations \n \n Meteor \n Famous   - Repository is now deprecated \n Polymer \n Materialize \n \n Package Manager \n \n AtmosphereJS \n \n Tutorials \n \n First tutorial - Meteortips \n Second tutorial - Meteortips \n Meteor and Fam.ous tutorial - tutas-labs \n Meteor and Security - Discover Meteor \n Quora answer on Security - Quora \n Good tutorial on basics in Meteor \n Slack clone using MeteorJS \n \n Libraries and Recommendations \n \n Bootstrap Material Design \n Shake plugin \n Iron Routes \n Check out Localmarket source code \n \n Video Tutorials and Workshops \n \n Meteor tutorial \n Intermediate Meteor \n Differential: Meteor + Polymer -- December Devshop SF \n Michael Karliner: Meteor and the Internet of Things -- Devshop London Jan 2015 \n Scaling Codefights from 0 to 200k users - Tigran Sloyan \n Gadi Cohen: Meteor Famous Views -- Devshop London June 2015 \n Meteor + Famo.us: Made for each other -- February Devshop SF \n \n About Meteor \n \n Twitter \n Website \n', 'Safe-Journey \n Android App that tweets your location on shake (Handy in case of emergency) \n \n License \n ```\nThe MIT License (MIT) \n Copyright (c) 2015 Manraj Singh \n Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the "Software"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions: \n The above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software. \n THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n```', 'Twitterati[ currently under development ] \n Developing Twitter clone using Twitter API \n To Do \n \n [x] Decide framework to be used \n [ ] Set up Oauth \n [ ] Spend time to get this started \n \n License \n Apache', "MiniGithub \n A firefox addon facilitating client to stay connected to his account without logging into the github website. \n About the addon \n \n Compact \n Impressive UI for an addon ( :P ) \n Powered by Mozilla's Addon SDK \n \n Screenshots \n \n \n \n \n \n Contributions \n Contributions are very much welcome. Please create an issue regarding how you'd like to improve or implement something in the Addon. I'll stay involved with your suggestions and merge viable requests. \n Cheers.", 'Cricket-Player-Info-API \n A service for Cricket Player\'s information \n \n API Documentation \n Currently there is only one end point. There are plans to add more specific endpoints. ( See  this  ) \n /api/<cricketer> \nGet career statistics, domestic team information, international information and personal information of the cricketer. \n License \n ```\nThe MIT License (MIT) \n Copyright (c) 2015 Manraj Singh \n Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the "Software"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions: \n The above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software. \n THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n```', 'Social-Network \n A mini Social Network using Meteor[ Currently under Development ] \n Technologies used \n \n Meteor \n MaterializeCSS \n MongoDB \n \n To Do \n Version 1.0b \n \n [ ] Implement Newsfeed feature \n [ ] Add post feature \n [ ] Complete Profile feature \n [ ] Add Friend feature \n [ ] Forgot Password \n [x] Signup and Login \n [x] Decide on framework and get project started \n', 'organize-cli \n   \n     \n \n Organize files based on file types, formally known as  organizeit \n \n \n Installation \n $ npm install -g organize-cli \n Usage \n ```\nUsage: organize [options] \n Options:\n  -o, --output  Output directory - Creates one if doesn\'t exist         [string]\n  -d, --date    Organize files by dates                                [boolean]\n  -s, --source  Source directory to organize                 [string] [required]\n  -t, --type    Specific types to organize - strings of file extensions  [array]\n  -f, --folder  Specific folder to move specific files to               [string]\n  -h, --help    Show help                                              [boolean] \n Examples:\n  organize -s ~/Downloads -o . -t mp3 wav -f "Songs"\n``` \n Development \n Run: \n sh\n$ git clone https://github.com/manrajgrover/organize-cli.git\n$ cd organize-cli\n$ npm link \n This will setup a symbolic link to the CLI. Any changes in source files will now be reflected when running the  organize  command. \n To lint your code, run \n sh\n$ npm run lint \n Like it? \n :star2: this repo to show support. You can also tweet about this project by clicking  here . \n Related \n \n classifier \n \n License \n MIT  © Manraj Singh', 'Moksha Website \n [Deprecated] This repository contains two websites. One is basic static placeholder website with CountDown. And another is the main website. \n Moksha Static Website \n This directory will contain the static website. We plan to use Parallax theme for it. It will be one page website with Sponsors listed and previous year pictures. \n Current Volunteers: \n \n Anshul Khanna \n Manav \n Manraj Singh \n Nithila \n Prabhakar Gupta \n \n Moksha Main Website \n This directory will contain the dynamic website with Login, Registration and Event ID details. \n \n Divyanshu Kalra \n Kamakshi Suri \n Manraj Singh \n Mayank Badola \n Prabhakar Gupta \n Shubham Kumaram \n \n Contribution Guidelines \n No further contributions will be accepted. This repository is not maintained anymore.', 'change[ currently under development ]', 'SingleDivProject \n   \n \n ☝️ One  <div> . Many possibilities. \n \n What is this? \n This project focuses on exploring all the possibilities that can be done with a single  <div>  element using CSS only. \n Why? \n \n Great way to explore CSS properties. \n Restrictions forces you to learn new properties and different ways to do particular styling. \n Because we can. \n \n How to contribute? \n Please go through the  Contributions  guidelines before sending a pull request. \n Reddit threads \n \n What all can you do with a single  <div>  element using pure CSS? Check this out!  on  /r/programming/ \n One  <div> . Many possibilities.  on  /r/web_design/ \n One  <div> . Many possibilities.  on  /r/coolgithubprojects/ \n \n Demos \n Designs \n | Project Name | Your Name / Github Handle | Demo/Gif |\n| :---: | :---: | :---: |\n|  Battery  |  manrajgrover  |   |\n|  Hamburger Menu  |  manrajgrover  |   |\n|  Magnifying Glass   |  kamito98  |   |\n|  New Instagram Logo  |  manrajgrover  |   | \n Loaders \n | Project Name | Your Name / Github Handle | Demo/Gif |\n| :---: | :---: | :---: |\n|  Awesome Dots  |   manrajgrover  |   |\n|  Bouncing Ball  |   mtacchino  |   |\n|  Box Loader  |   AnubrataDS  |   |\n|  Clock  |   richardj  |   |\n|  Dot Loader  |   aaroniker  |   |\n|  Fill up  |   manrajgrover  |   |\n|  Iron Factory  |  NorthernTwig  |   |\n|  Revolving Circles  |   dcrousso  |   |\n|  Revolving Dots  |   dcrousso  |   |\n|  Ripple Effect  |   manrajgrover  |   |\n|  Play Loader  |  manrajgrover  |   |\n|  Simple Spinner  |   manrajgrover  |   |\n|  Simple Text Loader  |   manrajgrover  |   |\n|  Melting Ice Cream Loader  |   TaranVohra  |   | \n Flags \n | Project Name | Your Name / Github Handle | Demo |\n| :---: | :---: | :---: |\n|  Armenia  |  manrajgrover  |   |\n|  Austria  |  manrajgrover  |   |\n|  Bahamas  |  manrajgrover  |   |\n|  Bangladesh  |  manrajgrover  |   |\n|  Belgium  |  AndyMathys  |   |\n|  Brazil  |  esganzerla  |   |\n|  Bulgaria  |  manrajgrover  |   |\n|  Chile  |  adavis46  |   |\n|  Canada  |  Scott Kaye  |   |\n|  Cameroon  |  1forh  |   |\n|  Colombia  |  DHernandex  |   |\n|  Czech Republic  |  fadilf  |   |\n|  Denmark  |  jacksarick  |   |\n|  England  |  TheWebartist  |   |\n|  Finland  |  jacksarick  |   |\n|  France  |  manrajgrover  |   |\n|  Germany  |  manrajgrover  |   |\n|  Greece  |  jdhoek  |   |\n|  Hungary  |  jabbalaci  |   |\n|  India  |  fadilf  |   |\n|  Indonesia  |  karuna  |   |\n|  Ireland  |  skeevey  |   |\n|  Israel  |  hillai  |   |\n|  Italy  |  manrajgrover  |   |\n|  Ivory Coast  |  skeevey  |   |\n|  Japan  |  manrajgrover  |   |\n|  Laos  |  jamcgrath  |   |\n|  Latvia  |  vasiljevs  |   | \n|  Lithuania  |  fosron  |   |\n|  Nigeria  |  skeevey  |   |\n|  Pakistan  |  ObaidAshraf  |   |\n|  Poland  |  mschweichler  |   |\n|  Puerto Rico  |  aallfredo  and  jorluiseptor  |   |\n|  Russia  |  ad222kr  |   |\n|  Senegal  |  Death259  |   |\n|  South Africa  |  DHernandex  |   |\n|  Sierra Leone  |  ts96  |   |\n|  Suriname  |  adavis46  |   |\n|  Sweden  |  jacksarick  |   |\n|  Switzerland  |  thijswerrij  |   |\n|  Syria  |  iSWORD  |   |\n|  Taiwan  |  amowu  |   |\n|  Texas  |  domspad  |   |\n|  Thailand  |  jamcgrath  |   |\n|  The Netherlands  |  jdhoek  |   |\n|  Turkey  |  mehmetuken  |   |\n|  United Kingdom  |  jdhoek  |   |\n|  United States of America  |  jdhoek  |   |\n|  Yemen  |  Death259  |   | \n Tools \n | Project Name | Your Name / Github Handle | Demo/Gif |\n| :---: | :---: | :---: |\n|  Box Breathing  |   gnclmorais  |   | \n License \n MIT  ©  Manraj Singh', "microbar \n   \n     \n \n 🎉 A zero dependency, 1.2kb g-zipped, lightweight JavaScript Library for slim progress bars for Ajax'y applications. Inspired by YouTube and Github. \n \n \n Installation \n Download and extract  latest release  or install using package manager: \n npm \n sh\n$ npm install microbar \n bower \n sh\n$ bower install microbar \n Or use  unpkg : \n ```html \n \n ``` \n Or use  jsdeliver : \n ```html \n \n ``` \n Usage \n How to Load? \n Link  microbar.js  in your HTML \n ```html \n \n ``` \n or require it: \n js\nconst microbar = require('path/to/microbar'); \n Generate microbar \n js\nconst microbar = new microbar( settings ); \n settings \n settings  is a JSON Object in which you can specify various options. \n \n percentage   <integer> : (Optional) Initial starting position of  progress  bar ( between  0  and  100  ). By default, it will be 0. \n color   <string> : (Optional) Color of your  progress  bar. By default, it will be  black . \n speed   <integer> : (Optional) Speed of your  progress  bar ( between  1  and  10  ). By default, it will be  10 . \n target   <id> : (Optional) ID of your targetted DOM element. By default, it will be fixed to  top  of document. \n \n Methods available \n \n moveTo( percentage ) : Set percentage width of  progress  bar. \n getColor( ) : Get color of that  progress  bar. \n setColor( color ) : Set color of  progress  bar. \n getSpeed( ) : Get speed of  progress  bar. \n setSpeed( speed ) : Set speed of  progress  bar. \n \n If you would like to add more methods/customizations, please open an  issue . \n Example \n ```js \n const settings = {\n    percentage: 50,\n    color: 'red',\n    speed: 2,\n    target: 'lolCat'\n}; \n const bar = new microbar(settings); \n // Move bar forward\nbar.moveTo(80); \n // Get color of progress bar\nbar.getColor(); \n // Set color of progress bar\nbar.setColor('#141414'); \n // Get speed of progress bar\nbar.getSpeed(); \n // Set speed of progress bar\nbar.setSpeed(3);\n``` \n Development \n Run: \n sh\n$ git clone https://github.com/manrajgrover/microbar.git\n$ cd microbar\n$ npm install \n This will setup the library dependencies for you. \n To lint your code, run \n sh\n$ npm run lint \n To build for development purpose, run \n sh\n$ npm run dev \n To build for production purpose, run \n sh\n$ npm run prod \n License \n MIT  ©  manrajgrover", 'iConnect \n Project Description \n Link to Google Slides Presentation - \nhttps://docs.google.com/presentation/d/11idG6D5QvYInQcYoDa5E9CRCB5b_i05O_E-cICluJ00/edit?usp=sharing \n iConnect, a Social Reporting App, provides an easy-to-use yet powerful way to report crimes in REAL-TIME by the citizens. \n The app has many interactive features like Pinning on Google Maps, Emergency Video Broadcast by local authorities, and one-to-one Video Chat with concerned authorities for quick resolution or crime reporting. These video snippets can be used as video evidences in case of crime reporting. \n Objective \n To connect people in distress, in case of emergency and enable people nearby to help those in need. Also, we enable social services like police and medical services to reach those in need in real time. Also education lectures can be given through broadcast. \n The app can be divided into various modules : \n \n \n Login Form - Create a new user using Twitter + Facebook integration. Also takes in Mobile Number for verification of user. \n \n \n Home Page with Live Feed and SOS - This page shows all the nearby happenings like Robbery in a nearby bank. Helps the local citizens to stay alert. Each post in the local feed has the address of the event in it. \n There is also an SOS Button which can be used in emergency situations. It can be used to post an emergency tweet with     #iConnect #emergency, along with location co-ordinates of the user. \n \n \n Broadcast Mode - Broadcast mode is designed for officials to provide localized information to the crowd in case of High Alerts and other emergency. It uses P2P technology(WebRTC) for high scalability. \n This broadcast system can also be used for education purposes where Police can take Self Defense classes and immediate     health aid techniques.  \n \n \n Map - Shows all the nearby occurrences on Google Maps along with the description of the event and the user who posted it. Has a Button to add new event, which gives options for the type of crime/emergency so that concerned authority can be called. \n \n \n There is an Admin Panel to monitor all the events. This can be used by authorities to take action and resolve issues in real time. \n This module also has a “Broadcast to Emergency Centers” checkbox in case of emergency. For example, a person witnessing an armed robbery can secretly shoot a live videos to assist the authorities and for video evidence. \n While reporting an event, the user can also choose to provide a live video feed from a Phone(Android and iOS app/Web App) and Desktop and it is streamed to the concerned officials. \n Software platform used \n Meteor, Cordova and MongoDB\n \n Project Structure \n \n /client - Contains all the client side code. \n /lib - Vendor files are contained in this folder. \n /templates - Contains all Client Side templates - HTML, CSS and JS. \n \n \n /lib - Contains all the Routing, Collections and Publications. \n /packages - Contains all the reusable contents of the app. \n /server - Contains all the server side code and configurations. \n \n User flows \n \n A user enters the Web App. \n He/She is shown the Live Stream of Newsfeed as well as Live Broadcasts. \n In Live Broadcast, user can create a broadcast to reach large masses. \n Live Broadcast can also be used by authorities for Self Defence Training and Immediate Health Aid techniques. \n When user clicks on "View On Map", he/she is shown all the alerts that are within 5 mile radius of that user. \n If user wants to report an event/crime, he/she can add the event and same will reflect live on the newsfeed as well as map. \n User can choose to broadcast the present situation to the concerned SOS authorities. \n There is an SOS button, which user can press to alert all his/her emergency contacts as well as concerned authorities. \n There is also an Admin Panel, which shows Real Time updates to concerned authorities so as to act on them soon as they get information. \n This app can be used as a Web App as well as Android app. \n \n How to run? \n To run the project: \n \n Download the Zip file and extract it. \n Change directory to the extracted folder and run  $ meteor  command. \n It will create a local server and you can view app at  localhost:3000 , 3000 being default port. \n \n The app has been ported on Android as well as iOS using Cordova plugins. \n Next level of API designs \n \n We plan to extend Broadcast API. It currently connects user streams(video/audio) using unique sessions. In case of Broadcast, each peer may act as a relay which  helps in stability. \n We also plan to extend this in case of Natural Disasters by creating heat maps. \n We plan to track SOS being provided in real time. \n We can provide analytics for crime events, based on Male/Female, locations and type of events and time of day. \n', 'react-native-component-explorer \n Personal React Components Explorer App (For Android). \n Components Explored \n \n React Native Maps \n React Native ViewPager \n React Native Camera \n React Native Scrollable Tab View \n React Native Image Picker \n', 'react-native-box-loaders \n React Native Box loaders \n Demo \n \n License \n ```\nThe MIT License (MIT) \n Copyright (c) 2016 Manraj Singh \n Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the "Software"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions: \n The above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software. \n THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n```', 'HackerEath-CLI \n       \n \n CLI for compiling and running code using  HackerEarth API . \n \n Demo \n \n Install \n Run the following command \n shell\n$ npm install -g hackerearth-cli \n API Key \n You can get your HackerEarth API Key by visiting  HackerEath API  page. \n Usage \n Commands available \n ```shell\n$ hackerearth  \n Commands:\n  run      Run code on HackerEarth server\n  compile  Compile code on HackerEarth server\n  config   Change config file \n Options:\n  -h, --help  Show help                                      [boolean] \n ``` \n Command  run \n ```shell\n$ hackerearth run  \n Options:\n  -h, --help      Show help                                  [boolean]\n  -s, --source    Source Code file path                     [required]\n  -i, --input     Input file path                           [required]\n  -l, --language  Language. Change  config  for default.\n  -o, --output    Output file path                          [required] \n Examples:\n  hackerearth run -s A.cpp -i Input00.in -o Output.txt -l CPP11 \n ``` \n Command  compile \n ```shell\n$ hackerearth compile  \n Options:\n  -h, --help      Show help                                  [boolean]\n  -s, --source    Source Code file path                     [required]\n  -l, --language  Language. Change  config  for default. \n Examples:\n  hackerearth compile -s A.cpp -l CPP11\n``` \n Command  config \n Run  $ sudo hackerearth config  to change configuration of your installation. This includes default language and API Key. \n ```shell\n$ sudo hackerearth config [options] \n Options:\n  -h, --help  Show help                                      [boolean]\n  -l, --list  List language and their code                   [boolean] \n Examples:\n  sudo hackerearth config -l\n``` \n Development \n Run: \n sh\n$ git clone https://github.com/manrajgrover/HackerEarth-CLI.git\n$ cd HackerEarth-CLI\n$ npm link \n This will setup a symbolic link to the CLI. Any changes in source files will now be reflected when running the  hackerearth  command. \n To lint your code, run \n sh\n$ npm run lint \n License \n MIT  ©  Manraj Singh', 'HackerRank-CLI \n       \n \n CLI for running code using  HackerRank API . \n \n Demo \n \n Install \n Run the following command \n sh\n$ npm install -g hackerrank-cli \n API Key \n You can get your HackerRank API Key by visiting  HackerRank API  page. \n Usage \n Commands available \n ```sh\n$ hackerrank  \n Commands:\n  run     Run code on HackerRank server\n  config  Change config file \n Options:\n  -h, --help  Show help                                     [boolean]\n``` \n Command  run \n ```sh\n$ hackerrank run  \n Options:\n  -h, --help      Show help                                 [boolean]\n  -s, --source    Source Code file path                    [required]\n  -i, --input     Input file path                          [required]\n  -l, --language  Language. Change  config  for default.\n  -o, --output    Output file path                         [required] \n Examples:\n  hackerrank run -s A.cpp -i Input00.in -o Output.txt -l 2\n``` \n Command  config \n Run  $ sudo hackerrank config  to change configuration of your installation. This includes default language and API Key. \n ```sh\n$ sudo hackerrank config  \n Options:\n  -h, --help  Show help                                     [boolean]\n  -l, --list  List language and their code                  [boolean] \n Examples:\n  sudo hackerrank config -l\n``` \n Development \n Run: \n sh\n$ git clone https://github.com/manrajgrover/HackerRank-CLI.git\n$ cd HackerRank-CLI\n$ npm link \n This will setup a symbolic link to the CLI. Any changes in source files will now be reflected when running the  hackerrank  command. \n To lint your code, run \n sh\n$ npm run lint \n License \n MIT  ©  Manraj Singh', 'HackerRank API Node Wrapper \n       \n \n Node wrapper for HackerRank API \n \n This library enables compiling and running code and also fetching languages available using  HackerRank API . \n Install \n $ npm install hackerrank-node-wrapper \n API Key \n You can get your HackerRank API Key by visiting  HackerRank API  page. \n Usage \n Methods available \n \n runURL  : Get Run URL \n langURL  : Get Language URL \n apiKey  : Get API Key \n getLanguages( callback )  : Returns languages as a response from HackerRank \n run ( config, callback )  : Returns response as a callback from HackerRank after running the code on testcases provided \n \n Config \n Config should be an object containing following properties: \n \n source  : This is your source code to be executed \n lang  : This should be a number corresponding to language to be set for source code \n testcases : This should be a JSON list of strings, each string being a test case \n \n How to use \n Getting Languages \n ```javascript\nimport HackerRank from \'hackerrank-node-wrapper\'; \n const hr = new HackerRank(\'yourApiKey\'); \n hr.getLanguages((error, response) => {\n  if (error) {\n    console.log( Error: ${error} );\n  } else {\n    console.log(response.body);\n  }\n});\n``` \n Running Code \n ```javascript\nimport HackerRank from \'hackerrank-node-wrapper\'; \n const hr = new HackerRank(\'yourApiKey\'); \n let data = {\n  source: \'print "Hello World"\',\n  lang: 5,\n  testcases: \'["1"]\'\n}; \n hr.run(data, (error, response) => {\n  if(error) {\n    console.log( Error: ${error} );\n  } else {\n    console.log(response.body);\n  }\n});\n``` \n License \n MIT  ©  Manraj Singh', 'ReactNativeWithReduxExample [learning project, please ignore] \n Learning React Native with Redux', 'hbg (Hacker Boilerplate Generator) \n         \n \n CLI for generating boilerplate for coding competitions \n \n Install \n sh\n$ npm install -g hbg \n Usage \n Commands available \n ```sh\nsudo hbg  \n Commands:\n  gen     Generate boilerplate\n  add     Add default template\n  config  Change config file \n Options:\n  -h, --help  Show help                                         [boolean] \n ``` \n Command  gen \n ```sh\nUsage: hbg gen  \n Options:\n  -h, --help  Show help                                         [boolean]\n  -l, --lang  Language. Change  config  for default\n  -q, --ques  Number of questions. Change  config  for default \n Examples:\n  $ sudo hbg gen -l cpp -q 4 \n ``` \n Command  add \n ```sh\nUsage: hbg add  \n Options:\n  -h, --help      Show help                                     [boolean]\n  -t, --template  Path to the template file                    [required]\n  -l, --lang      Language chosen                              [required] \n Examples:\n  $ sudo hbg add -t test/template.cpp -l cpp \n ``` \n Command  config \n Run  $ sudo hbg config  to change configuration of your installation. This includes default language and default questions. \n ```sh\nUsage: hbg config \n Options:\n  -h, --help  Show help                                        [boolean]\n  -l, --list  List language and their extension                [boolean] \n Examples:\n  $ sudo hbg config -l \n ``` \n License \n MIT  ©  Manraj Singh', 'codechefer [ currently under work ]', 'football-cli \n       \n \n Command line interface for Hackers who love football ⚽ \n \n \n Installation \n Run \n shell\n$ npm install -g footballcli \n How to get API Key? \n Please register on  football-data.org  to get your API Key. Then run  $ football config  to add your API Key (use  sudo  if required). Requests made using API key increases your rate limit from 50 requests per day to 50 requests per minute. \n Usage \n Commands available \n ```shell\nfootball  \n Commands:\n  scores     Get scores of past and live fixtures\n  fixtures   Get upcoming and past fixtures of a league and team\n  standings  Get standings of particular league\n  lists      List of codes of various competitions\n  config     Change configuration and defaults \n Options:\n  -h, --help  Show help                                          [boolean] \n ``` \n Command  scores \n Get scores of past and live fixtures \n \n ```shell\nUsage: football scores [options] \n Options:\n  -h, --help  Show help                                          [boolean]\n  -l, --live  Live scores                                        [boolean]\n  -t, --team  Select team                                        [string] \n Examples:\n  football scores -t "Manchester United" -l \n ``` \n Command  fixtures \n Get upcoming and past fixtures of a league and team \n ```shell\nUsage: football fixtures [options] \n Options:\n  -h, --help    Show help                                         [boolean]\n  -d, --days    Number of days from today                         [number]\n  -l, --league  League                                            [string]\n  -t, --team    Team name or substring of it                      [string]\n  -n, --next    Next or upcoming matches                          [boolean] \n Examples:\n  football fixtures -l PL -d 5 -t "Manchester United" -n \n ``` \n \n Command  standings \n Get standings of particular league \n ```shell\nUsage: football standings [options] \n Options:\n  -h, --help    Show help                                         [boolean]\n  -l, --league  League to be searched                             [required] \n Examples:\n  football standings -l PL \n ``` \n Command  lists \n List of codes of various competitions \n ```shell\nUsage: football lists [options] \n Options:\n  -h, --help     Show help                                        [boolean]\n  -r, --refresh  Refresh league ids                               [boolean] \n Examples:\n  football lists -r \n ``` \n Command  config \n Change configuration and defaults \n ```shell\nUsage: football config \n Options:\n  -h, --help  Show help                                           [boolean] \n Examples:\n  football config \n ``` \n Note \n For World Cup fixtures, you need to specify  league  flag with value as  WC . API does not provide world cup information in consolidated response and is a known bug. \n Development \n Run: \n sh\n$ git clone https://github.com/manrajgrover/football-cli.git\n$ cd football-cli\n$ npm link \n This will setup a symbolic link to the CLI. Any changes in source files will now be reflected when running the  football  command. \n To lint your code, run \n sh\n$ npm run lint \n Get in touch \n Say hi on  twitter  and share this project by clicking  here \n Related \n You may find similar packages  here \n License \n MIT  © manrajgrover', 'qa.js [Currently under work] \n Framework for a better portfolio \n License \n MIT  © ManrajGrover', "trailerify \n \n Chrome extension to add IMDB ratings of a particular movie on Trailers Addict \n \n \n Installation \n \n Download the  zip file  or clone the  repository . \n Go to  chrome://extensions  in your Chrome Browser and click on  Load unpacked extensions... . \n Choose the unzipped folder/cloned repository and click  Select . \n \n Developer's Guide \n \n Clone the repository and change directory to the cloned folder. \n Run  npm install  to install all dependencies. \n Run  gulp  command to build the project and  gulp watch  for building project automatically. \n \n License \n MIT  © Manraj Singh", 'pb-002 \n This repository contains Phone Number Validator and Wiki Search Logger \n Contents \n \n Phone Number Validator \n WikiTerm \n NodeJS \n PHP \n Python \n', 'BasicMVC \n This is a basic MVC for OPT-IW003. \n Installation \n On MAC \n \n Clone the repository by running  git clone https://github.com/ManrajGrover/BasicMVC.git  in your MAMP htdocs. \n Start MAMP server. \n Go to  http://localhost:8888/BasicMVC/  to view rendered page. \n \n What it does? \n This MVC takes  DataSource/data.json  as data source and renders the template with data. On visiting any other route, it throws a rendered 404 template with error. \n Dependencies \n None', 'BasicORM \n This is a basic ORM for OPT-IW004 \n Installation \n \n Clone the repository by running  git clone https://github.com/ManrajGrover/BasicORM.git . \n Import database  contacts.db  using PhpMyAdmin or MySQL console. \n Check the demo by running  php demo.php \n \n What it does? \n This ORM implements only  SELECT  command of MySQL that too for specific columns as per given requirements.', 'Doctors REST API \n This project allows users to fetch all doctors, all clinics and their relations in a RESTful manner. \n Installation \n \n Download the Zip file and extract it. \n Import the database using MySQL console or phpMyAdmin. \n Make sure you change  AllowOverride None  to  AllowOverride All  in your  httpd.conf  file for URL rewriting. \n Run  composer install  to install all dependencies. \n Run the server. \n \n What it does? \n The project provides 4 end points as per given assignment. \n \n /doctors?page=4  - For getting details of all doctors in paginated manner \n /clinics?page=5  - For getting details of all clinics in paginated manner \n /doctors/id  - For getting doctor details and places where he/she practices. Here id is an integer \n /clinics/id  - For getting clinic details and all doctors who practice there. \n \n Dependencies \n \n slim/slim: ^3.0 \n', "Basic CRUD API \n This is a basic CRUD application \n Installation \n \n Download the Zip file and extract it. \n Import the database using MySQL console or phpMyAdmin. \n Make sure you change  AllowOverride None  to  AllowOverride All  in your  httpd.conf  file for URL rewriting. \n Run  composer install  to install all dependencies. \n Run the server. \n \n What it does? \n The project provides 4 end points as per given assignment. \n \n /employee/new  - For creating new employee. Method to be used  POST . Fields to be provided:  name ,  email ,  contact ,  designation . \n /employee/{id}  - For getting employee details. Method to be used  GET . \n /employee/{id}  - For updating employee's details. Method to be used  PUT . Fields to be provided:  name ,  email ,  contact ,  designation . \n /employee/{id}  - For deleting an employee. Method to be used  DELETE . \n \n Dependencies \n \n slim/slim: ^3.0 \n", 'Practo Tech Inventory Management System \n \n Inventory Management System provide easy and efficient management of technical inventory at Practo. \n Features \n \n Login using Google Accounts \n Easy request and approvals of items \n View current inventory owned \n Easy issue and return of items \n Full dashboard for managing inventory \n \n Technologies Used \n \n Java 8 \n Spring Boot v1.4.1 \n Spring OAuth2 (Google login) \n WS Spring Exceptions v1.6 \n Log4j v1.2.17 \n Maven \n Git \n MySQL v5.6 \n HSQL DB (Testing) v2.2.9 \n Node Package Manager \n Eclipse \n JavaScript (ES6) \n AngularJS v1.5.8 \n Angular Route v1.5.8 \n jQuery v3.1.1 \n Selectize v0.12.4 \n Gulp v3.9.1 \n HTML \n CSS \n Bootstrap v3.3.7 \n Hibernate (ORM) \n Checkstyle Plugin v2.17 \n \n Documentations \n \n Database Schema \n Requirements Documentation \n Plan of Action \n Apiary Documentation \n \n Integrations \n \n Travis Build \n Sentry \n New Relic \n \n Page Speed \n PageSpeed evaluation can be checked  here . \n Demo \n Project has been hosted on Amazon EC2. You may find the link  here . \n Development \n To build the project: \n \n Clone the project using git \n \n $ git clone https://github.com/ManrajGrover/InventoryManagement.git \n \n Change directory to the folder created \n \n $ cd InventoryManagement \n \n Run clean install \n \n $ mvn clean install \n To run tests: \n $ mvn test \n To generate test coverage: \n $ mvn verify \n Project uses Gulp for Front End Workflow Management. To run Gulp, change directory to to  src/main/resources/static  and run following command to install Web App dependencies. \n $ npm install \n Then run \n $ gulp', 'BasicCSSFramework [ currently under development ] \n This is a basic CSS Frameowork currently under development. Basic Setup for SCSS using Gulp is completed. Work is still under progress for the features required for the page. \n Installation \n \n Download the zip file. \n Copy  framework.css  in dist folder into your project. \n \n Follow progress on Github \n You can follow progress on Github by clicking  here \n License \n MIT  © Manraj Singh', 'Git-assignment \n Collaboration on git', 'Evento \n \n \n \n \n View your Github events in a better way \n \n Installation \n \n Download and extract zip file \n Change directory to the folder created and go to  api  folder \n Run  pip install -r requirements.txt \n Go to root folder of app and change directory to  webapp \n Run  npm install  to install all dependencies \n Visit  https://github.com/settings/applications/new  and create your own Github Application \n Get your client id and secret key and fill it in  api/application.yml \n Go to  webapp/src/js/angular-scripts.js  and change  this  line with client id \n Build the client side by going to  webapp  and running gulp \n Finally, go back to  api  folder and run  python app.py \n App will run on  http://localhost:5000 \n \n Technologies used \n \n Flask v0.11.1 \n Node v4.5.0 \n Angular v1.5.8 \n Angular Route v1.5.8 \n Bootstrap v3.3.7 \n jQuery v3.1.1 \n ngInfiniteScroll v1.3.0 \n Gulp v3.9.1 \n \n License \n MIT  © Manraj Singh', "img2tab \n       \n \n Convert images to HTML tables, for dynamic QR Codes, barcodes and more \n \n Install \n $ npm install img2tab \n Usage \n Method available \n \n getTable( 'path/to/image/or/url', width, height ) : Get HTML table for image given, returns a Promise \n \n How to use \n Getting HTML Table for given image \n ```javascript\nconst Img2Tab = require('img2tab'); \n const instance = new Img2Tab('path/to/image/or/url'); \n // Returns a Promise\nconst imageTable = instance.getTable(); \n imageTable.then(data => console.log(data));\n// => String containing HTML table\n``` \n License \n MIT  © Manraj Singh", 'img2tab-cli \n       \n \n CLI for converting images to html tables, for dynamic QR Codes, barcodes and more \n \n Install \n $ npm install -g img2tab-cli \n Usage \n How to use \n Getting HTML Table for given image \n ```javascript\n$ img2tab --help\nimg2tab [options] \n Options:\n  -s, --source     Source path/url                                    [required]\n  -o, --output     Path to output file\n  -c, --clipboard  Copy to clipboard                                   [boolean]\n  -w, --width      Width per pixel\n  --hi, --height   Height per pixel\n  -h, --help       Show help                                           [boolean]\n``` \n Related \n \n img2tab  - API for this CLI \n \n License \n MIT  © Manraj Singh', "algorithms-js          \n \n Consumable Data Structures and Algorithms library in JavaScript \n \n Note \n I'm looking for maintainers for this project. Please email me if you are interested in maintaining the project. \n Installation \n Run \n shell\n$ npm install algorithms-js \n Or use  unpkg : \n ```html \n \n ``` \n Or use  jsdeliver : \n ```html \n \n ``` \n Usage \n Library contains both algorithms as well as data structures: \n Data Structures \n Currently, library supports following data structures: \n \n Doubly Linked List \n Fenwick Tree \n Graph \n Heap \n Linked List \n Queue \n Stack \n Trie \n \n Algorithms \n Currently library supports following algorithms: \n Search \n Various Searching algorithms: \n \n Binary Search \n Breadth First Search \n Depth First Search \n Exponential Search \n Interpolation Search \n Jump Search \n Linear Search \n Ternary Search \n \n Sort \n Various Sorting algorithms: \n \n Bubble Sort \n Count Sort \n Heap Sort \n Insertion Sort \n Merge Sort \n Quick Sort \n Selection Sort \n \n Math \n Various Math algorithms: \n \n Extended Euclidean \n Fast Exponentiation \n GCD \n LCM \n \n String \n Various String algorithms: \n \n Levenshtein Distance \n \n Geometry \n Various Geometry algorithms: \n \n Tangent between circles \n \n Development \n Run: \n sh\n$ git clone https://github.com/manrajgrover/algorithms-js.git\n$ cd algorithms-js\n$ npm install \n This will setup the library dependencies for you. \n To run tests, run \n sh\n$ npm run test \n To lint your code, run \n sh\n$ npm run lint \n To generate test coverage, run \n sh\n$ npm run report \n To build docs, run \n sh\n$ npm run docs \n To build for browser, run \n sh\n$ npm run build-dev \n Get in touch \n Say hi on  twitter \n License \n MIT  © Manraj Singh", 'dotfiles \n 🔧 Personal .files, shell, git config, zsh and more. Also, makes setting up new machine a breeze. \n License \n MIT  ©  Manraj Singh', 'notebooks \n Notebooks from practices, challenges, data exploration, competitions and more \n Competitions \n \n India Hacks Qualifier 2017  - Rank:  55 / 6900 \nContains code for the first round of qualification of IndiaHacks in which 2 problems were given. \n Predict the Road Sign \n Predict the Segment \n \n \n \n Notebooks for Round 2 and Finals could not be shared. Rankings: \n \n India Hacks Round 2 2017 - Rank  5 / 60 \n India Hacks Finals 2017 - Rank  6 / 15 \n \n Tutorials \n Soon! \n Practices \n Soon! \n License \n MIT  ©  Manraj Singh', "\n \n \n  halo\n \n     \n       \n \n Beautiful spinners for terminal, IPython and Jupyter \n \n \n Install \n shell\n$ pip install halo \n Usage \n ```py\nfrom halo import Halo \n spinner = Halo(text='Loading', spinner='dots')\nspinner.start() \n Run time consuming work here \n You can also change properties for spinner as and when you want \n spinner.stop()\n``` \n Alternatively, you can use halo with Python's  with  statement: \n ```py\nfrom halo import Halo \n with Halo(text='Loading', spinner='dots'):\n    # Run time consuming work here\n``` \n Finally, you can use halo as a decorator: \n ```py\nfrom halo import Halo \n @Halo(text='Loading', spinner='dots')\ndef long_running_function():\n    # Run time consuming work here\n    pass \n long_running_function()\n``` \n API \n Halo([text|text_color|spinner|animation|placement|color|interval|stream|enabled]) \n text \n Type :  str \n Text shown along with spinner. \n text_color \n Type :  str \n Values :  grey ,  red ,  green ,  yellow ,  blue ,  magenta ,  cyan ,  white \n Color of the spinner text. Defaults to  None . \n spinner \n Type :  str|dict \n If string, it should be one of the spinners listed in the given  json  file. If a dict is passed, it should define  interval  and  frames . Something like: \n py\n{\n    'interval': 100,\n    'frames': ['-', '+', '*', '+', '-']\n} \n Defaults to  dots  spinner. For Windows users, it defaults to  line  spinner. \n animation \n Type :  str \n Values :  bounce ,  marquee \n Animation to apply to the text if it's too large and doesn't fit in the terminal. If no animation is defined, the text will be ellipsed. \n placement \n Type :  str \n Values :  left ,  right \n Which side of the text the spinner should be displayed. Defaults to  left \n color \n Type :  str \n Values :  grey ,  red ,  green ,  yellow ,  blue ,  magenta ,  cyan ,  white \n Color of the spinner. Defaults to  cyan . \n interval \n Type :  float \n Interval between each frame. Defaults to spinner interval (recommended). \n stream \n Type :  file \n Stream to write the output. Defaults to  sys.stdout . \n enabled \n Type :  bool \n Enable or disable the spinner. Defaults to  True . \n Methods \n Following are the methods available: \n spinner.start([text]) \n Starts the spinner. If  text  is passed, it is set as spinner text. Returns the instance. \n spinner.stop() \n Stops and clears the spinner. Returns the instance. \n spinner.clear() \n Clears the spinner. Returns the instance. \n spinner.render() \n Manually renders a new frame. Returns the instance. \n spinner.frame() \n Returns next frame to be rendered. \n spinner.succeed([text]) \n text :  Type :  str \n Stops the spinner and changes symbol to  ✔ . If text is provided, it is persisted else current text is persisted. Returns the instance. \n spinner.fail([text]) \n text :  Type :  str \n Stops the spinner and changes symbol to  ✖ . If text is provided, it is persisted else current text is persisted. Returns the instance. \n spinner.warn([text]) \n text :  Type :  str \n Stops the spinner and changes symbol to  ⚠ . If text is provided, it is persisted else current text is persisted. Returns the instance. \n spinner.info([text]) \n text :  Type :  str \n Stops the spinner and changes symbol to  ℹ . If text is provided, it is persisted else current text is persisted. Returns the instance. \n spinner.stop_and_persist([symbol|text]) \n Stops the spinner and changes symbol and text. Returns the instance. \n symbol \n Type :  str \n Symbol to replace the spinner with. Defaults to  ' ' . \n text \n Type :  str \n Text to be persisted. Defaults to instance text. \n \n spinner.text \n Change the text of spinner. \n spinner.color \n Change the color of spinner \n spinner.spinner \n Change the spinner itself. \n spinner.enabled \n Enable or disable the spinner. \n How to contribute? \n Please see  Contributing guidelines  for more information. \n Like it? \n 🌟 this repo to show support. Let me know you liked it on  Twitter .\nAlso, share the  project . \n Related \n \n py-spinners  - Spinners in Python \n py-log-symbols  - Log Symbols in Python \n ora  - Elegant terminal spinners in JavaScript (inspiration behind this project)  \n \n License \n MIT  © Manraj Singh", '\n  log-symbols\n \n \n \n Colored symbols for various log levels for Python \n \n       \n Install \n shell\n$ pip install log_symbols \n Usage \n ```py\nfrom log_symbols import LogSymbols #Enum \n print LogSymbols.SUCCESS.value # ✔\nprint LogSymbols.INFO.value # ℹ\nprint LogSymbols.WARNING.value # ⚠\nprint LogSymbols.ERROR.value # ✖\n``` \n Like it? \n :star2: this repo to show support. Let me know you liked it on  Twitter .\nAlso, share the  project . \n Related \n \n halo \n log-symbols \n \n License \n MIT  © Manraj Singh', "\n  spinners\n \n \n 🔄 More than 60 spinners for terminal, python wrapper for amazing node library  cli-spinners \n \n       \n \n The list of spinners is just a  JSON file . \n If you're looking to use them, consider using  halo  module for Python, or  ora  for node. \n Install \n shell\n$ pip install spinners \n Usage \n ```py\nfrom spinners import Spinners #Enum \n print Spinners.line.value \n {u'frames': [u'-', u'\\', u'|', u'/'], u'interval': 130} \n ``` \n Like it? \n :star2: this repo to show support. Let me know you liked it on  Twitter .\nAlso, share the  project . \n Related \n \n halo \n cli-spinners \n \n License \n MIT  © Manraj Singh", 'imputex', "docker-ds \n Personal docker environment for data science \n Installation \n Currently the images are not available on Docker Hub but soon they will be. \n Usage \n There are two images available: \n CPU \n To build CPU image \n sh\n$ docker-compose --file docker-compose.cpu.yml build \n To run container with CPU image \n sh\n$ docker-compose --file docker-compose.cpu.yml run --service-ports datascience \n GPU \n To build GPU image \n sh\n$ docker-compose --file docker-compose.gpu.yml build \n To run container with GPU image \n Currently  docker-compose  does not support  runtime (for GPU) in file version  3 . Hence docker-compose file version used is  2.3 . \n sh\n$ docker-compose --file docker-compose.gpu.yml run --service-ports datascience \n Note: \n If you're using  docker-compose  from this project, please do not forget to create  .env  file using  .env.sample  file. \n License \n MIT  © Manraj Singh", 'httyd \n \n License \n MIT ©  Manraj Singh', "blog \n \n Yeah, let's call it a  blog ! \n \n Source code powering my personal blog, developed over awesome  gatsbyjs  and  @greglobinski 's  personal blog starter   \n License \n MIT ©  gatsby   greglobinski   manrajgrover", "papers \n \n Notes and summaries of papers I've read \n \n Notes and Summaries \n Distributed Training \n \n Large Scale Distributed Deep Networks [ notes ][ paper ] \n Deep Gradient Compression: Reducing the communication bandwidth for distributed training [ notes ][ paper ] \n \n License \n MIT  © Manraj Singh", 'no_dues \n \n Clone the Github repository. \n Navigate to the cloned directory. \n Install and Configure MongoDB in your system \n Install Node.js and npm in your system. \n Run the command "npm install". \n Run the command "npm start". \n Go to "localhost:8000" in your search engine. \n']
bjdeng,[]
mxmeier,['Code example(s) used in the lecture Statistical Methods in Data Analysis \n \n Softmax Classifier \n', 'simulation_scripts \n Usage \n $ python simulation_scripts.py -s "step" "path_to_config_yaml" \n or if installed with pip:\n $ simulation_scripts_write -s "step" "path_to_config_yaml" \ncheck --help for options. \n Example \n simulation_scripts_write -s 0 ~/simulation_scripts/configs/11300.yaml', 'feldman_cousins \n Calculates Feldman Cousins confidence intervals for poissonian PDFs']
xingdi-eric-yuan,['multi-layer-convnet \n \n \n A multi-layer (single-channel image) convnet \n \n \n This is altered from the 3-channel version.  \n \n \n For more info about the 3-ch version (config and so on), check this:\nhttps://github.com/xingdi-eric-yuan/conv-net-version-3 \n \n \n To run this code, you should have \n* a MNIST( you can get it from  HERE );\n* OpenCV. \n Compile & Run \n \n \n Compile: "cmake ." and then "make"  \n \n \n Run: "./conv"  \n \n \n Structure and Algorithm \n See my several posts about CNNs at  my tech-blog . \n The MIT License (MIT) \n Copyright (c) 2014 Xingdi (Eric) Yuan \n Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the "Software"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions: \n The above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software. \n THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.', 'single-layer-convnet \n A single-layer Convolutional Neural Network\nSee more details here:\nhttp://eric-yuan.me/cnn/', 'Optical Flow \n ============ \n optical flow algorithms \n More details see here:\nhttp://eric-yuan.me/coarse-to-fine-optical-flow/ \n Files \n \n HSof  -- HORN & SCHUNCK METHOD  \n LKof  -- LUCAS & KANADE METHOD \n ctfLKof  -- LK method with COARSE-TO-FINE \n', 'single-layer-rbm \n A C++ implementation of single layer RBM \n More details see here:\nhttp://eric-yuan.me/rbm/', 'sparse coding \n C++ implementation of sparse coding algorithms \n More details see here:\nhttp://eric-yuan.me/sc/ \n Files \n \n sc  -- sparse coding code \n decoder  -- decoder for reconstructing image \n', 'conv-net-version-3.1.0 \n ===================== \n Deep neural network frame (C++ / OpenCV). \n To run this code, you should have \n* a cifar-10 dataset( put "cifar-10-batches-bin" where this .md file is, you can get it from  HERE , make sure to download the binary version which suitable for C programs);\n* OpenCV 3.0.\n* cmake \n Compile & Run \n \n Compile: \n cmake .\nmake   \n Run: \n ./cnn3   \n \n Updates \n \n 3-channels images supported. \n Add Dropout; \n Local Response Normalization supported. \n Use log files dig deeper. \n Use second order derivative back-prop to alter learning rate. \n Jul 1: version 3.1.0 released \n NEW : Jul.13 OpenCV 3.0 supported, use dft to accelerate convolution \n \n Layer Config Description \n \n For each layer, there is a  layer_name , a  layer_type , and a  output_format . \n There are currently 2 output formats:  matrix  (cv::Mat, CV_64FC1), and  image  (vector of cv::Mat, CV_64FC3). \n \n Input Layer \n \n batch size : the training process is using mini-batch stochastic gradient descent. \n \n Convolutional Layer \n \n kernel size : size of kernels for convolution calculation. \n kernel amount : amount of kernels for convolution calculation. \n combine map : amount of combine feature map, details can be found in  Notes on Convolutional Neural Networks . \n weight decay : weight decay for convolutional kernels. \n padding : padding before doing convolution. \n stride : stride when doing convolution (For "VALID" type of convolution,  result size = (image_size + 2 * padding - kernel_size) / stride + 1) . \n \n Fully Connected Layer \n \n num hidden neurons : size of fully connected layer. \n weight decay : weight decay for fully connected layer. \n \n Softmax Layer \n \n num classes : output size of softmax layer. \n weight decay : weight decay for softmax layer. \n \n Non-linearity Layer \n \n method : sigmoid/tanh/relu/leaky_relu. \n \n Pooling Layer \n \n method : max/mean/stochastic. \n overlap : if use overlap pooling. \n window size : window size when using overlap pooling. \n stride : pooling stride. \n \n Local Response Normalization Layer \n \n alpha ,  beta ,  k ,  n : see  ImageNet Classification with Deep Convolutional Neural Networks . \n \n Dropout Layer \n \n dropout rate : percentage of zeros when generating Bernoulli matrix. \n \n Combine Layer \n \n for implementing GoogLeNet, TODO... \n \n Branch Layer \n \n for implementing GoogLeNet, TODO... \n \n Structure and Algorithm \n See my several posts about CNNs at  my tech-blog . \n TODO \n combine layer\n branch layer \n GPU Version \n There\'s also a GPU version of this code which I used nVidia CUDA. . \n The MIT License (MIT) \n Copyright (c) 2014 Xingdi (Eric) Yuan \n Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the "Software"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions: \n The above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software. \n THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.', 'Hanafuda \n ===================== \n An exercise of using AI algorithm / c++.\nI\'m learning to play hanafuda now...\nJust simple Koi-Koi (2-player version).\n NOT FINISHED \n Rules \n See the  official wiki  for rules. \n Battle Platform \n Compile & Run \n \n \n Compile: "cmake CMakeLists.txt" and then "make"  \n \n \n Run: "./hanafuda"  \n \n \n You should compile the AI first, then put  header files  and  lib files  into the "/ai", as shown in demo. \n \n \n AI \n Compile \n \n \n Compile: "cmake CMakeLists.txt" and then "make"  \n \n \n It will generate a  .dylib  file by compiling, put this file (and all header files you use) into the certain path in battle platform. \n \n \n The MIT License (MIT) \n Copyright (c) 2014 Xingdi (Eric) Yuan \n Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the "Software"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions: \n The above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software. \n THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.', 'Basic Algorithms \n ===================== \n An exercise of implementing basic tree/graph/search algs using c++. \n Compile & Run \n \n \n Compile: "cmake CMakeLists.txt" and then "make"  \n \n \n Run: "./alg"  \n \n \n Currently including \n \n \n Binary Tree \n \n \n Min Heap \n \n \n DFS/BFS Traverse \n \n \n Dijkstra\'s alg \n \n \n A* Search \n \n \n Quick/Merge Sort \n \n \n Tree Rotation \n \n \n TODO \n \n \n Kruskal\'s Plan \n \n \n Prim\'s Plan \n \n \n The MIT License (MIT) \n Copyright (c) 2014 Xingdi (Eric) Yuan \n Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the "Software"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions: \n The above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software. \n THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.', 'Generic Stack \n ============ \n A C++ implemented generic stack class.', 'texture synthesis \n A C++ / OpenCV implementation of simple texture synthesis \n compile and run \n \n cmake . \n make \n ./ts \n \n More details see here:\nhttp://eric-yuan.me/texture-synthesis/', 'QR Decoder \n ============ \n Unfinished, just the QR code detecting part. \n Compile and run \n \n cmake . \n make \n ./qrdecoder \n', 'Max-flow Min-cut Graph Cut \n =====================\nImplementing Boykov\'s Max-Flow/Min-Cut segmentation algorithm\nDetails see http://www.csd.uwo.ca/~yuri/Papers/pami04.pdf \n \n NOT FINISHED \n Nov.11 : Currently using Push–relabel maximum flow algorithm. \n Nov.12 : Add Mouse Draw. \n Nov.12 : Combine max-flow/min-cut with image segmentation. \n \n Compile & Run \n \n \n Compile: "cmake ." and then "make"  \n \n \n Run: "./gcut"  \n \n \n The MIT License (MIT) \n Copyright (c) 2014 Xingdi (Eric) Yuan \n Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the "Software"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions: \n The above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software. \n THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.', 'named-entity-recognition \n ===================== \n Using both MITIE library and Convnet. \n To run this code, you should have \n* OpenCV\n* MITIE lib & dlib (new version can be found in  MITIE github ) \n Compile & Run \n \n Download MITIE and install mitie models by running:\n make MITIE-models \n put  mitielib  folder and  dlib  into  named-entity-recognition  folder \n \n put  MITIE-models/english/total_word_feature_extractor.dat  into  ./network  folder \n \n \n Compile by running:\n cmake .\nmake \n \n Run: \n ./NER x \n where x is run mode: \n 1: Train mitie \n 2: Train convolutional neural networks \n 3: Do named entity recognition using mitie \n 4: Do named entity recognition using convolutional neural networks \n \n Structure and Algorithm \n See  my tech-blog . \n TODO \n ... \n MITIE LICENSE \n MITIE is licensed under the Boost Software License. \n The MIT License (MIT) \n Copyright (c) 2014 Xingdi (Eric) Yuan \n Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the "Software"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions: \n The above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software. \n THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.', 'nerpp --- Named Entity Recognition Using CRF++ and C++ \n ===================== \n Named Entity Recognition Using CRF++ and C++ \n To run this code, you should have \n* CRF++ (latest version can be found in  CRF++ github ) \n Compile & Run \n \n \n Download CRF++  \n \n \n Put  CRF++-0.58 folder into  `nerpp  folder \n \n \n Install CRF++ by running:\n cd CRF++-0.58\n./configure \nmake\nsudo make install \n \n \n Compile by running:\n cmake .\nmake \n \n Run: \n ./nerpp x \n where x is run mode: \n 1: Train model (read file) \n 2: Batch test (read file) \n 3: Single sentence test (stdin) \n \n 4: Split data file into 2 files, for training and testing \n \n \n Example:\nWith the given data in  ./dataset/ , first split data into 2 parts by:\n ./nerpp 4\ndataset/data 0.8 \nThen train a model using the 80% of data:\n ./nerpp 1\ndataset/data_TR \nTest model using the 20% of data (result will be automatically saved in  data_TS_info.txt :\n ./nerpp 2\ndataset/data_TS \nTest single sentence from stdin:\n ./nerpp 3\ni want to see sport news on newspaper \n \n \n Structure and Algorithm \n See  An Introduction to Conditional Random Fields for Relational Learning .\nSee  my tech-blog . \n TODO \n ... \n CRF++ LICENSE \n CRF++ is licensed under  LGPL  /  BSD  Licenses. \n The MIT License (MIT) \n Copyright (c) 2014 Xingdi (Eric) Yuan \n Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the "Software"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions: \n The above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software. \n THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.', 'recurrent-net-version-1.1 \n ===================== \n Simplest bi-directional recurrent neural network (C++ / OpenCV). \n To run this code, you should have \n* OpenCV. \n Compile & Run \n \n Compile by running:\n cmake .\nmake \n Run: \n ./rnn \n \n Structure and Algorithm \n See  my tech-blog . \n TODO \n New post for bi-directional rnn... \n Config Files \n General Parameters Config \n \n gradient checking (automatically disable dropout, and use tiny dataset) \n store parameters into log file (for debugging, should be faster if not using it) \n batch size \n non-linearity method (sigmoid, tanh, ReLU) \n training epochs \n iteration per epoch \n learning rate \n ngram \n training percent (for cross validation) \n \n Layers Config \n \n layer type \n amount of hidden neurons \n dropout rate \n weight decay \n amount of output classes (in  Softmax  Layer, initialized to be 0, because it depends on dataset) \n \n Multi-Layer \n This network supports multiple hidden layers. \n The MIT License (MIT) \n Copyright (c) 2015 Xingdi (Eric) Yuan \n Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the "Software"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions: \n The above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software. \n THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.', 'long short-term memory recurrent net \n ===================== \n Bi-directional LSTM recurrent neural networks (C++ / OpenCV). \n To run this code, you should have \n* OpenCV. \n Compile & Run \n \n Compile by running:\n cmake .\nmake \n Run: \n ./lstm \n \n Structure and Algorithm \n See  my tech-blog . \n UPDATES \n \n word2vec supported. \n bi-directional LSTM. \n CoNLL04 dataset support. \n \n TODO \n \n bug fixes... \n \n Config Files \n Multi-Layer \n This network supports multiple hidden layers. \n The MIT License (MIT) \n Copyright (c) 2015 Xingdi (Eric) Yuan \n Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the "Software"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions: \n The above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software. \n THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.', 'cuda-deep-neural-net-0.1.1 \n ===================== \n UNFINISHED \n Deep neural network framework (C/C++/CUDA). \n To run this code, you should have \n* a cifar-10 dataset( put "cifar-10-batches-bin" where this .md file is, you can get it from  HERE , make sure to download the binary version which suitable for C programs);\n* nVidia graphic card which supports nVidia CUDA\n* for running network with pre-trained network, you should put pre-trained files into "config" folder, there is a demo config folder which is named "pre-trained-conf", you can rename it to "config" and replace the current "config" folder. \n Compile & Run \n add this project into nVidia nsight, add  curand  and  cufft  into path.  \n Updates \n \n 0.1.0: Aug.5, the first version released. \n 0.1.1: Aug.10, remove hostData in Mat, only use device memory, for speed up. \n 0.1.1: Aug.11, add functions that save matrices and configs into .txt files. \n 0.1.1: Aug.12, add functions that read network from .txt files. \n \n Data Structures \n Mat \n \n Similar with Mat in OpenCV, has memory in both CPU and GPU, use it to do most of the calculations. \n \n cpuMat \n \n Has memory only in CPU, use it to read dataset, and do pre-processing (unless your GPU memory is huge...). \n \n vector3f \n \n Similar with Scalar in OpenCV, has 3 float space, which corresponses 3 channels. For example, the sum of a 3-channal Mat is a vector3f. \n \n vector2i \n \n Similar with vector3f, but has 2 int space, use it to represent 2-d position, or size.  \n \n Layer Config Description \n \n For each layer, there is a  layer_name , a  layer_type , and a  output_format . \n There are currently 2 output formats:  matrix  single channel Mat, and  image  (vector of 3-channel Mat). \n \n Input Layer \n \n batch size : the training process is using mini-batch stochastic gradient descent. \n \n Convolutional Layer \n \n kernel size : size of kernels for convolution calculation. \n kernel amount : amount of kernels for convolution calculation. \n combine map : amount of combine feature map, details can be found in  Notes on Convolutional Neural Networks . \n weight decay : weight decay for convolutional kernels. \n padding : padding before doing convolution. \n stride : stride when doing convolution (For "VALID" type of convolution,  result size = (image_size + 2 * padding - kernel_size) / stride + 1) . \n \n Fully Connected Layer \n \n num hidden neurons : size of fully connected layer. \n weight decay : weight decay for fully connected layer. \n \n Softmax Layer \n \n num classes : output size of softmax layer. \n weight decay : weight decay for softmax layer. \n \n Non-linearity Layer \n \n method : sigmoid/tanh/relu/leaky_relu. \n \n Pooling Layer \n \n method : max/mean/stochastic. \n overlap : if use overlap pooling. \n window size : window size when using overlap pooling. \n stride : pooling stride. \n \n Local Response Normalization Layer \n \n alpha ,  beta ,  k ,  n : see  ImageNet Classification with Deep Convolutional Neural Networks . \n \n Dropout Layer \n \n dropout rate : percentage of zeros when generating Bernoulli matrix. \n \n Combine Layer \n \n for implementing GoogLeNet, TODO... \n \n Branch Layer \n \n for implementing GoogLeNet, TODO... \n \n Structure and Algorithm \n See my several posts about CNNs at  my tech-blog . \n CPU Version \n There\'s also a CPU version of this code which I used OpenCV as the linear algebra library. . \n TODO \n \n combine layer \n branch layer \n stochastic pooling \n \n The MIT License (MIT) \n Copyright (c) 2015 Xingdi (Eric) Yuan \n Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the "Software"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions: \n The above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software. \n THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.', 'A Pytorch Implementation of MatchLSTM for SQuAD \n \n A simple pytorch implementation of MatchLSTM (Wang and Jiang, 2016) model for SQuAD (Rajpurkar et al., 2016) question answering. Note that the model config used in this implementation might be different with what in the paper.  \n Tokenized SQuAD \n An NLTK tokenized version of SQuAD dataset is included. Because the original test set is hidden, I split a subset from training split as "valid" set, and use the original dev set as test set. the splitting is on Wikipedia article level. \n Requirements \n \n Python 2.7 \n Install Pytorch, follow  this . \n Download pretrained GloVe embeddings from  here . \n Run  pip install --requirement requirements.txt \n \n First Time Running \n \n Run  python helpers/embedding_2_h5.py  to generate glove embedding h5 file. \n It will take some time when first time running because it will generate an h5 file for SQuAD dataset \n \n Results \n \n In the following experiments, 300d GloVe embeddings pretrained on 840B tokens are used. \n The default using LSTM is not the pytorch built-in version, it is slower version but with layernorm (Ba et al., 2016) and dropout (Kingma et al., 2015) support. Enable  fast_rnn  to use the pytorch built-in LSTM, instead. \n Character level embedding doesn\'t seem to help in this specific model, and makes it slower, to use char level embeddings, enable  char_level . \n The following results are got from single Nvidia P40 GPUs. If you have less GPU memory, use smaller batch size. \n Other things implemented but haven\'t done ablation test yet, like highway connection (Srivastava et al., 2015) between each layers. (TODO) \n \n \n | config | valid f1 | valid em | dev f1 | dev em | param amount |\n| --- | --- | --- | --- | --- | --- |\n| default config |  0.68714  |  0.54033  |  0.73007  |  0.62091  | 1950002 |\n| enable fast_rnn | 0.66019 | 0.51396 | 0.71294 | 0.59972 | 1945802 |\n| enable char_level | 0.66670 | 0.51764 | 0.71956 | 0.61107 | 2143922 |\n| enable char + fast | 0.66481 | 0.51900 | 0.71524 | 0.60416 | 2138826 | \n References \n \n SQuAD Paper \n MatchLSTM Paper \n Layernorm Paper \n Variational Dropout Paper \n Highway Networks Paper \n \n Tips \n \n Make the pretrained embedding h5 file to only contain the overlap words between GloVe and SQuAD, makes it much faster to load embeddings before training.  \n \n LICENSE \n MIT', "asciiko (アス子) \n \n A deep ascii art generator. Work done during MSR Montreal hackathon 2018. \n Requirements \n \n Python 2/3 \n Install Pytorch, follow  this . \n Install OpenCV2,  conda install opencv  should work. \n Ask for permission of using ETL (a Japanese handwritten character recognition dataset) from  here , then unzip and put ETL1 or ETL6 inside  asciiko/  (we recommend using ETL6, which contains more punctuation marks, they might be helpful for generating ascii arts). \n \n First Time Running \n It will take some time if you run the first time, asciiko will preprocess and parse the ETL datasets, save the useful part into .npy files. \n To Run \n \n In  config/config.yaml , enable CUDA if you have access to it, also specify which split of ETL data are you using. \n To train a model, run  python train.py -c config/ . \n To generate ascii strings from an image, modify and run  python img2charid.py , it will generate a  .json  file; \n To generate ascii strings from a video, run  python utils/get_video_frames.py , to get all frames of a video into a folder, then modify and run  python img2charid.py , it will generate a  .json  file. \n To render the ascii arts, run  python char_classifier/renderer.py , with your  .json  file specified inside. \n a pretrained model (on ETL6) is provided in  saved_models/ . \n \n Image Processing \n Image processing is an important step before generating ascii arts, and the performance of the latter steps are highly relying on how good the images are processed, i.e., how good the edges are extracted. To our knowledge there's no 'panacea' for all input images/videos, so depending on which specific image or video you are using, sometimes you need to massage your edge detector carefully. Thanks to OpenCV, one can do some standard image processing tricks in one line of python code. We provide code to help tweaking your edge detector in  utils/ . \n Demo \n \n \n example outputs  corresponding to  DeepAA  sample images \n nichijou op, pic in pic \n nichijou op, up and down \n \n Authors \n eric yuan ,  @oya0306 ,  @penzant \n LICENSE \n GLWTPL \n References \n DeepAA", 'TextWorld-Coin-Collector \n \n PyTorch implementation of papar  Counting to Explore and Generalize in Text-based Games \n Coin Collector \n \n \n Coin collector is a set of games, each game is a randomly connected chain of rooms, the agent\'s goal is to navigate through the path and pick up the coin. \n Modes: easy / medium / hard, amount of off-chain rooms. \n Levels: Length of optimal trajectory. \n Action space:  {go, take} × {north, south, east, west, coin}\u200b \n Environment ID:  twcc_[mode]_level[level]_gamesize[#game]_step[max step]_seed[random seed]_[split] , please check  here  for more details. \n \n Requirements \n \n Python 3 \n PyTorch 0.4 \n TextWorld : install the  coin_collector  branch \n pip install https://github.com/microsoft/TextWorld/archive/refs/heads/coin_collector.zip \n Install gym_textworld by  pip install gym_textworld/ . \n tensorboardX \n pip install tensorboardX \n nltk + the punkt package: \n pip install nltk pytest \n python -c "import nltk; nltk.download(\'punkt\')" \n \n Game Generation \n \n Run  tw-make.py <env_id>  to generate games corresponding to games defined in config files. \n E.g.,  tw-make.py twcc_easy_level10_gamesize100_step50_seed9_train . \n You can use  scripts/check_for_duplicates.py  to check duplicates between training and /test sets. \n \n To Run \n \n LSTM-DQN: run  python lstm_dqn_baseline/train_single_generate_agent.py -c lstm_dqn_baseline/config/ . \n LSTM-DRQN: run  python lstm_drqn_baseline/train_single_generate_agent.py -c lstm_drqn_baseline/config/ . \n Configurations can be modified in the above two config files. \n \n LICENSE \n MIT \n References \n \n TextWorld: A Learning Environment for Text-based Games \n Counting to Explore and Generalize in Text-based Games \n', 'Interactive Language Learning by Question Answering \n \n Code for EMNLP 2019 paper "Interactive Language Learning by Question Answering". \n To install dependencies \n sudo apt update\nconda create -p ~/venvs/qait python=3.6\nsource activate ~/venvs/qait\npip install --upgrade pip\npip install numpy==1.16.4\npip install https://github.com/Microsoft/TextWorld/archive/rebased-interactive-qa.zip\npip install -U spacy\npython -m spacy download en\npip install tqdm h5py visdom pyyaml\nconda install pytorch torchvision cudatoolkit=9.2 -c pytorch \n Test Set \n Download the test set from  https://aka.ms/qait-testset . Unzip it. \n Pretrained Word Embeddings \n Before first time running it, download fasttext crawl-300d-2M.vec.zip from  HERE , unzip, and run  embedding2h5.py  for fast embedding loading in the future. \n To Train \n python train.py ./ \n Citation \n Please use the following bibtex entry:\n @article{yuan2019qait,\n  title={Interactive Language Learning by Question Answering},\n  author={Yuan, Xingdi and C\\^ot\\\'{e}, Marc-Alexandre and Fu, Jie and Lin, Zhouhan and Pal, Christopher and Bengio, Yoshua and Trischler, Adam},\n  booktitle={EMNLP},\n  year={2019}\n} \n License \n MIT', 'Interactive Machine Comprehension with Information Seeking Agents (iMRC) \n \n Code for paper "Interactive Machine Comprehension with Information Seeking Agents". \n To Install Dependencies \n sudo apt update\nconda create -p ~/venvs/imrc python=3.6 numpy scipy ipython matplotlib cython nltk pillow\nsource activate ~/venvs/imrc\npip install --upgrade pip\npip install numpy==1.16.4\npip install tqdm h5py pyyaml gym visdom\nconda install pytorch torchvision cudatoolkit=9.2 -c pytorch \n Pretrained Word Embeddings \n Before first time running it, download fasttext crawl-300d-2M.vec.zip from  HERE , unzip, and run  embedding2h5.py  for fast embedding loading in the future. \n Datasets \n \n We provide a split of the SQuAD dataset, because the official squad test data is hidden, so we split the training data to get an extra validation set, we use the official dev set as test set; \n Download NewsQA dataset and run their script to split and tokenize it. \n \n To Play \n We provide a simple interactive demo, one can feel what interactive MRC looks like. \n python play.py \n To Train \n python main.py \n Citation \n Please use the following bibtex entry:\n @article{yuan2019imrc,\n  title={Interactive Machine Comprehension with Information Seeking Agents},\n  author={Yuan, Xingdi and Fu, Jie and C\\^ot\\\'{e}, Marc-Alexandre and Tay, Yi and Pal, Christopher and Trischler, Adam},\n  journal={CoRR},\n  volume={abs/1908.10449},\n  year= {2019},\n  archivePrefix={arXiv},\n  eprint={1908.10449}\n} \n License \n MIT', 'GATA: Graph Aided Transformer Agent \n \n Code for NeurIPS 2020 paper  Learning Dynamic Belief Graphs to Generalize on Text-Based Games . \n ``` \n Dependencies \n conda create -p /tmp/gata python=3.6 numpy scipy ipython matplotlib cython nltk pillow\nsource activate /tmp/gata\npip install --upgrade pip\npip install numpy==1.16.2\npip install gym==0.15.4\npip install textworld\npip install -U "spacy<3"\npython -m spacy download en\npip install tqdm pipreqs h5py pyyaml visdom\nconda install pytorch torchvision cudatoolkit=9.2 -c pytorch \n Download FastText Word Embeddings \n curl -L -o crawl-300d-2M.vec.h5 "https://bit.ly/2U3Mde2"\n``` \n GATA \n Pre-training Graph Updater by Observation Generation \n ``` \n Download data for observation generation / contrastive observation classification \n cd obs_gen.0.1 ; wget https://aka.ms/twkg/obs_gen.0.1.zip ; unzip obs_gen.0.1.zip ; cd .. \n Train \n python train_obs_generation.py configs/pretrain_observation_generation.yaml \n ``` \n Pre-training Graph Updater by Contrastive Observation Classification \n ``` \n Download data for observation generation / contrastive observation classification \n cd obs_gen.0.1 ; wget https://aka.ms/twkg/obs_gen.0.1.zip ; unzip obs_gen.0.1.zip ; cd .. \n Train \n python train_obs_infomax.py configs/pretrain_contrastive_observation_classification.yaml \n ``` \n Train Action Scorer with RL \n ``` \n Download games \n cd rl.0.2 ; wget https://aka.ms/twkg/rl.0.2.zip ; unzip rl.0.2.zip ; cd .. \n Modify configs/train_gata_rl.yaml \n L30: True to load pre-trained graph encoder, False to randomly initialize. \n L31:  \'gata_pretrain_obs_gen_model\', \'gata_pretrain_obs_infomax_model\'. When L30 is True. \n L33:  \'gata_pretrain_obs_gen_model\' or \'gata_pretrain_obs_infomax_model\' \n L84:  3/7/5/9 correspond to the 1/2/3/4 in paper \n L85:  1/20/100 \n L125: False/True \n To train \n python train_rl_with_continuous_belief.py configs/train_gata_rl.yaml \n ``` \n GATA-GTF \n Pre-training Graph Encoder by Action Prediction \n ``` \n Download data \n cd ap.0.2 ; wget https://aka.ms/twkg/ap.0.2.zip ; unzip ap.0.2.zip ; cd .. \n Train \n python train_action_prediction.py configs/pretrain_action_prediction_full.yaml \n ``` \n Pre-training Graph Encoder by State Prediction \n ``` \n Download data \n cd sp.0.2 ; wget https://aka.ms/twkg/sp.0.2.zip ; unzip sp.0.2.zip ; cd .. \n Train \n python train_state_prediction.py configs/pretrain_state_prediction_full.yaml \n ``` \n Pre-training Graph Encoder by Deep Graph Infomax \n ``` \n Download data \n cd dgi.0.2 ; wget https://aka.ms/twkg/dgi.0.2.zip ; unzip dgi.0.2.zip ; cd .. \n Train \n python train_deep_graph_infomax.py configs/pretrain_deep_graph_infomax_full.yaml \n ``` \n Train Action Scorer with RL \n ``` \n Download games \n cd rl.0.2 ; wget https://aka.ms/twkg/rl.0.2.zip ; unzip rl.0.2.zip ; cd .. \n Modify configs/train_gata_gtf_rl.yaml \n L30: True to load pre-trained graph encoder, False to randomly initialize. \n L31:  \'gata_gtf_pretrain_ap_full_model\', \'gata_gtf_pretrain_sp_full_model\', or \'gata_gtf_pretrain_dgi_full_model\'. When L30 is True. \n L84:  3/7/5/9 correspond to the 1/2/3/4 in paper \n L85:  1/20/100 \n L125: False/True \n To train \n python train_rl_with_ground_truth_discrete_belief.py configs/train_gata_gtf_rl.yaml \n ``` \n GATA-GTP \n Pre-training Graph Encoder by Action Prediction \n ``` \n Download data \n cd ap.0.2 ; wget https://aka.ms/twkg/ap.0.2.zip ; unzip ap.0.2.zip ; cd .. \n Train \n python train_action_prediction.py configs/pretrain_action_prediction_seen.yaml \n ``` \n Pre-training Graph Encoder by State Prediction \n ``` \n Download data \n cd sp.0.2 ; wget https://aka.ms/twkg/sp.0.2.zip ; unzip sp.0.2.zip ; cd .. \n Train \n python train_state_prediction.py configs/pretrain_state_prediction_seen.yaml \n ``` \n Pre-training Graph Encoder by Deep Graph Infomax \n ``` \n Download data \n cd dgi.0.2 ; wget https://aka.ms/twkg/dgi.0.2.zip ; unzip dgi.0.2.zip ; cd .. \n Train \n python train_deep_graph_infomax.py configs/pretrain_deep_graph_infomax_seen.yaml \n ``` \n Pre-training Graph Updater by Command Generation \n ``` \n Download data for command generation \n cd cmd_gen.0.2 ; wget https://aka.ms/twkg/cmd_gen.0.2.zip ; unzip cmd_gen.0.2.zip ; cd .. \n Train \n python train_command_generation.py configs/pretrain_command_generation.yaml \n ``` \n Train Action Scorer with RL \n ``` \n Download games \n cd rl.0.2 ; wget https://aka.ms/twkg/rl.0.2.zip ; unzip rl.0.2.zip ; cd .. \n Modify configs/train_gata_gtp_rl.yaml \n L30: True to load pre-trained graph encoder, False to randomly initialize. \n L31:  \'gata_gtp_pretrain_ap_seen_model\', \'gata_gtp_pretrain_sp_seen_model\', or \'gata_gtp_pretrain_dgi_seen_model\'. When L30 is True. \n L84:  3/7/5/9 correspond to the 1/2/3/4 in paper \n L85:  1/20/100 \n L125: False/True \n To train \n python train_rl_with_discrete_belief.py configs/train_gata_gtp_rl.yaml \n ``` \n Monitoring training progress \n To monitor training progress: set " visdom: True " in  config_***.yaml  under the  general  section, and start  Visdom  in another terminal using the  visdom  command line. Then, open the link displayed by Visdom in your browser. \n Citation \n Please use the following bibtex entry:\n @article{adhikari2020gata,\n  title={Learning Dynamic Belief Graphs to Generalize on Text-Based Games},\n  author={Adhikari, Ashutosh and Yuan, Xingdi and C\\^ot\\\'{e}, Marc-Alexandre and Zelinka, Mikul\\\'{a}\\v{s} and Rondeau, Marc-Antoine and Laroche, Romain and Poupart, Pascal and Tang, Jian and Trischler, Adam and Hamilton, William L.},\n  journal={CoRR},\n  volume={abs/2002.09127},\n  year= {2020},\n  archivePrefix={arXiv},\n  eprint={2002.09127}\n} \n License \n MIT', 'Interactive Machine Comprehension with Dynamic Knowledge Graphs \n \n Implementation for the EMNLP 2021 paper. \n Dependencies \n sh\napt-get -y update\napt-get install -y unzip zip parallel\nconda create -p /tmp/imrc python=3.6 numpy scipy cython nltk\nconda activate /tmp/imrc\npip install --upgrade pip\npip install numpy==1.16.2\npip install gym==0.15.4\npip install tqdm pipreqs pyyaml pytz visdom\nconda install pytorch torchvision cudatoolkit=9.2 -c pytorch\npip install transformers\npip install allennlp \n Data Preparation \n Split SQuAD 1.1 and preprocess \n The original SQuAD dataset does not provide its test set, we take  23  wiki articles from its training set as our validation set. We then use the SQuAD dev set as our test set. \n ```sh \n download SQuAD from official website, then \n python utils/split_original_squad.py\n``` \n To speed up training, we parse (tokenization and SRL) the dataset in advance.  \n sh\npython utils/preproc_squad.py \nThis will result  squad_split/processed_squad.1.1.split.[train/valid/test].json , which are used in iMRC tasks. \n Preprocess Wikipedia data for self-supervised learning \n sh\npython utils/get_wiki_filter_squad.py\npython utils/split_wiki_data.py \n This will result  wiki_without_squad/wiki_without_squad_[train/valid/test].json , which are used to pre-train the continuous belief graph generator. \n Training \n To train the agent equipped with different types of graphs, run: \n ```sh \n without graph \n python main.py configs/imrc_none.yaml \n co-occurrence graph \n python main.py configs/imrc_cooccur.yaml \n relative position graph \n python main.py configs/imrc_rel_pos.yaml \n SRL graph \n python main.py configs/imrc_srl.yaml \n continuous belief graph \n in this setting, we need a pre-trained graph generator. \n we provide our pre-trained graph generator at \n https://drive.google.com/drive/folders/1zZ7C_-xaYsfg2Ms7_BO5n3Qzx69UqMKD?usp=sharing \n one can choose to train their own version by: \n python pretrain_observation_infomax.py configs/pretrain_cont_bnelief.yaml \n then using the downloaded/saved model checkpoint \n python main.py configs/imrc_cont_belief.yaml\n``` \n To change the task settings/configurations:\n```yaml\ngeneral:\n  naozi_capacity: 1  # capacity of agent\'s external memory queue (1, 3, 5)\n  generate_or_point: "point"  # "qmpoint": q+o_t, "point": q, "generate": vocab\n  disable_prev_next: False  # False: Easy Mode, True: Hard Mode \n model:\n  recurrent: True  # recurrent component described in Section 3.3 and Section 4.Additional Results\n``` \n Citation \n bibtex\n@inproceedings{Yuan2021imrc_graph,\n  title={Interactive Machine Comprehension with Dynamic Knowledge Graphs},\n  author={Xingdi Yuan},\n  year={2021},\n  booktitle="EMNLP",\n}', 'prompt_based_quesiton_selection \n Code and data for paper  Selecting Better Samples from Pre-trained LLMs: A Case Study on Question Generation \n We will update soon. ']
lucasmenendez,['\n \n Selection Area \n   \n \n Simple JavaScript mouse & touch seletion area to any DOM container element. \n \n How to use  |  API Reference  |  Demo \n \n \n How to use \n Download \n Install with  npm \n sh\nnpm install selection-area \n Install with  yarn \n sh\nyarn add selection-area \n From Source Code \n Clone or download zipped source code into  node_modules  project folder. \n sh\ngit clone https://github.com/lucasmenendez/selection-area.git <project>/node_modules/selection-area \n Import \n Local dist version \n Import using  script  html tags with vendor path: \n ```html \n \n ``` \n Or import using ES6  import : \n javascript\nimport { SelectionArea } from \'selection-area\'; \n CDN version \n Importing with  unpkg.com : \n ```html \n \n ``` \n Example \n Define container for selection area and selectable childs. \n ```html \n \n 0 \n 1 \n 2 \n 3 \n 4 \n 5 \n 6 \n 7 \n 8 \n 9 \n \n ``` \n Define  SelectionArea  behaviour with  configuration  object, check available parameters  here .  \n ``js\nlet config = {\n    container: document.querySelector(\'.parent\'),\n    area: {\n        class: \'custom-area\'\n    },\n    targets: \'.child\',\n    touchable: true,\n    autostart: true,\n    callback: selection => {\n        if (selection.length == 0) alert("empty selection");\n        else alert( ${ selection.length } items selected`);\n    }\n} \n let selectable = new SelectionArea(config);\n``` \n Also you can stylize the selection area element adding style yo defined class. \n ```css\n.custom-area {\n    background: rgba(52, 152, 219, 0.2);\n    border: 2px dotted #2980b9;\n} \n ```', '\n \n \n \n SHGF: Simple HTTP golang framework \n S imple  H TTP  G olang  F ramework. Provides simple API to create an HTTP server and routes with dynamic paths, registered by HTTP method. \n Main features \n \n Handle URL by path and method \n Register dynamic paths with typed params \n Parse forms easily \n TLS & HTTP/2  \n Static files and folders \n \n Reference \n Read all the reference documents into  GoDoc  article. \n Documentation \n All docs are in our  Wiki ;', 'stupiddb \n Ridiculously easy. \n Documentation in progress.. \n Benchmarks \n Two scenarios to measure size, and insert/read speed:\n* Insertion of large amount of records.\n* Read randomly by multiple keys.', '\n \n gopio \n Simple Golang GPIO API. Read more about  GPIO Sysfs Interface .', 'pylemanager \n Simple python GUI filemanager \n Demo \n', '\n \n \n Gobstract \n Gobstract package make extraction summaries from text provided. The algorithm measures sentence relations (measuring relevant token similarity), position and length to pick the text highlights. \n Installation \n PoS Tagging \n For more information check instructions  here . \n Abstracts \n ```bash\nexport MODELS=" " \n go get github.com/lucasmenendez/gobstact\n``` \n Use it \n ```go\npackage main \n import (\n    "fmt"\n    "io/ioutil"\n    "github.com/lucasmenendez/gobstract"\n) \n func main() {\n    var input string\n    if raw, err := ioutil.ReadFile("input"); err != nil {\n        fmt.Println(err)\n        return\n    } else {\n        input = string(raw)\n    } \n if t, err := gobstract.NewText(input, "es"); err != nil {\n    fmt.Println(err)\n} else {\n    var summary []string = t.Summarize()\n    for _, sentence := range summary {\n        fmt.Println(sentence)\n    }\n}\n \n }\n```', '\n \n \n Gopostager \n HMM applied to Part-Of-Speech Tagging in Go. Implementation of  Part-of-Speech Tagging with Hidden Markov Models - Graham Neubig \n Installation \n go get github.com/lucasmenendez/gopostagger \n Tested corpus \n Name | Language | Size | Link corpus\n----- | ----- | ------ | ----\nBrown | en | 11.6 Mb |  Link \nAnCora | es | 0.54 Mb |  Link \n Examples \n Tag sentence \n ```go\n    package main \n import (\n    "github.com/lucasmenendez/gotokenizer"\n    "github.com/lucasmenendez/gopostagger"\n    "fmt"\n)\n\nfunc main() {\n    var s string = "El mundo del tatuaje es la forma de representación artística más expresiva que puede existir para un artista, puesto que su obra permanece inalterable de por vida."\n\n    if m, e := gopostagger.LoadModel("./models/es"); e != nil {\n        fmt.Println(e)\n    } else {\n        var tagger *gopostagger.Tagger = gopostagger.NewTagger(m)\n        var tokens []string = gotokenizer.Words(s)\n        var tagged [][]string = tagger.Tag(tokens)\n\n        for _, token := range tagged {\n            fmt.Printf("%q ", token)\n        }\n    }\n}\n \n ``` \n Train corpus \n IMPORTANT: All datasets must have the following format:  raw_word/tag_propossed \n ```go\n    package main \n import (\n    "github.com/lucasmenendez/gopostagger"\n    "fmt"\n)\n\nfunc main() {\n    if m, e := gopostagger.Train("./es"); e != nil {\n        fmt.Println(e)\n    } else if e = m.Store("./models/es"); e != nil {\n        fmt.Println(e)\n    } else {\n        fmt.Println("Trained!")\n    }\n}\n \n ```', '\n \n \n Gotokenizer \n Simple rule-based word/sentence tokenizer. \n Installation \n bash\ngo install github.com/lucasmenendez/gotokenizer \n Demo \n ````go\npackage main \n import (\n    "fmt"\n    "github.com/lucasmenendez/gotokenizer"\n) \n func main() {\n    var input string =  Go (often referred to as golang) is a programming language created at Google[12] in 2.009 by Robert Griesemer, Rob Pike, and Ken Thompson[10]. It is a compiled, statically typed language in the tradition of Algol and C, with garbage collection, limited structural typing[3], memory safety features and CSP-style concurrent programming features added. \n var sentences []string = gotokenizer.Sentences(input)\nfor _, s := range sentences {\n    fmt.Printf("%q\\n", gotokenizer.Words(s))\n}\n \n }\n````', '\n \n \n Gotagger \n Simple keyword extraction \n Installation \n bash\ngo install github.com/lucasmenendez/gotagger \n Stopwords \n If you want to extend stopword list, create a file, named as language code, into a any folder (for example:  en  file will contain English stopwords). Then, set env var  STOPWORDS  to that folder path.\nExtended stopword lists can be found in  Stopwords ISO profile . \n Demo \n ```go\npackage main \n import (\n    "fmt"\n    "github.com/lucasmenendez/gotokenizer"\n    "github.com/lucasmenendez/gotagger"\n) \n func main() {\n    var limit int = 15\n    var lang string = " "\n    var text string = " " \n var words [][]string\nfor _, s := range gotokenizer.Sentences(text) {\n    words = append(words, gotokenizer.Words(s))\n}\n\nif tags, err := gotagger.GetTags(words, lang, limit); err != nil {\n    fmt.Println(err)\n} else {\n    fmt.Printf("%q\\n", tags)\n}\n \n }\n```', '  \n gop2p \n Simple  Peer-to-Peer  protocol implementation in pure Go. \n Download \n bash\ngo get github.com/lucasmenendez/gop2p \n Docs & example \n Checkout  GoDoc Documentation .', '\n \n \n Golangdetector \n Package golangdetector measures and suggest language according to input text based on languages dictionary occurrences. \n The languages dictionaries are based on  bitcoin/bips/bip-0039-wordlists . \n Installation \n bash\ngo install github.com/lucasmenendez/golangdetector \n Demo \n ```go\npackage main \n import (\n    "fmt"\n    "github.com/lucasmenendez/golangdetector"\n) \n func main() {\n    i :=  Go (often referred to as golang) is a programming language created at Google in 2009 by Robert Griesemer, Rob Pike, and Ken Thompson. It is a compiled, statically typed language in the tradition of Algol and C, with garbage collection, limited structural typing, memory safety features and CSP-style concurrent programming features added. The compiler and other language tools originally developed by Google are all free and open source. \n p := golangdetector.Detect(i)\nfmt.Println(p) // map[en:1]\n\nr := golangdetector.Suggest(i)\nfmt.Println(r) // en\n \n }\n```', '\n \n \n WebProletarian \n \n Simple JavaScript framework to execute inline function into a WebWorker easily. \n \n Installation \n Install with  npm \n sh\nnpm install webproletarian \n Install with  yarn \n sh\nyarn add webproletarian \n From Source Code \n Clone or download zipped source code into  node_modules  project folder.\n sh\ngit clone https://github.com/lucasmenendez/webproletarian.git <project>/node_modules/webproletarian \n Documentation \n Checkout the  documentation . \n Using WebProletarian \n Import dist version into the  index.html  file: \n ```html \n \n ``` \n Or import with ES6  import :\n javascript\nimport { WebProletarian } from \'webproletarian\' \n Example \n ```javascript\nimport { WebProletarian } from \'webproletarian\' \n const labourUnion = new WebProletarian(function() {\n    proletarian.read("event2", console.log); \n setInterval(() => {\n    proletarian.fire("event1", "thread: ping");\n}, 1000);\n \n }); \n labourUnion.read("event1", data => {\n    console.log(data);\n    labourUnion.fire("event2", "main: pong");\n});\n```', "\n MyKeys \n MyKeys is a simple web app to  manage collections of passwords as bookmarks . It transforms your  encrypted passwords into a long url  that you can add to bookmarks to open it later or share with your friends. \n It works with  Vue.js (v2)  and  Web Crypto API . \n Features \n Encrypt and share or save up to 10 passwords. \n The  mykeys.live  webapp allows to manage up to 10 credentials items, that contains the following information:\n -  Alias : Human tag to identify the credential. Up to 20 characters.  Ex.: 'Spotify for Artist' .\n -  Username : The account identifier of the credential into its service. Up to 25 characters:  Ex.: 'cruz.cafune@mecen-ent.com' .\n -  Password : The account password of the credential. Up to 25 characters.  Ex.: 'M4r4cuch0' .\n -  Descrption : Extra field to attach some information to the credential. Up to 50 characters.  Ex.: 'First, remember change the language to Canarian.' .  \n Securized with a master password \n When the user completes the credentials, the webapp requests a  master password  to encrypt them with the following requirements:\n - At least one letter.\n - At least one number.\n - At least eight characters.\nThe data is encrypted with that master password, without it, the data is unusable. \n Credentials data as URL \n Then, the webapp encodes the encrypted credentials into a long URL that the user can share with others or save as bookmark to see the credentials later. Ex.:\n https://mykeys.live/#/d/cf164f5164836bd4c4be0013eLuTdPZ05doNN6YqXgWvglI2bRDcuB3SDVwC2Iz7sW1O6WhQSK9cqIWSwHSkE3UQoTrBL10oWnWVUsO4VLJwyTSJjiw8AT1nsZ8V6abKVcVMi23YDYMx5OdVC0MBUmJ1xI9d1PLfJ6Jq72BezQSkA+UJQrqdetw105Xlj+ZM4bOblSULB%2FSVGoomgRluYQsKBCReqoLDnWgFdX8%2F3tHSEeOYjvmdKGh%2FaOkKH4N7r9gY%2FdCY30XR+GRo83652bfGSTSslROcZ4RUX2VhrrPvuMtcbQFZQ+xf9zuqzQ5ZmIyyWhWpHx6ULG9HwX9n8vFgiS+FDMaOOCCG7e03I4w1FfyZaU1Yhjj3I6y6lbYYf5ger%2FWQzWjVpwyZcGOZKiNs68DKwGNnVAV459rB6C5PhDO9W1W+97X5Ea9vbvKILGyLUXpl289YbZ821ZRWGhNqGTPWlcUdaiViZ%2FfchDuEvWKWvhsSfISbgVHGPTEu+hg5kREBAOWDen5lbHivfV%2FoL9+ZqfjrLYEHjx%2FV38bLc1hPtsenHS9%2Fg6GTuBIArDie8iW7B17Ng9F0CoR1lOp1hobT6GobDh5bITw5ldWC%2FRnPZrNVHSiT1KS8zmbyvxTW8fHSwrpzDcW1MJZl+oes9NfuynO4nBIBDePmzw%2FTfk%2FlVrUEq2FHuLIMGb82ldTtlCfMfYsGhpNQiwNdPvbrQi82VE9w4oc+v0oe8F69gphoU2tCuo5T9VVNN9TrUuY+5uDKT7MSvSQQx4chQyvzfCrIKGF%2FNuFjc2hcFgcyE0rQ1vaqvCyKVJhZ9kVR1a2a+RAyaRy8yvRrsd88TMSAb76IKoCYuBYfdNKFErQb0WjhY3JiNT2VqyZh8fBvfZnEItPwG4r5bbdEPyliGkT7DxhnDfR%2FVB%2Ff7NCW0nubQpH7kZdDuvT08JHZOH%2FAhQwIvlBmTnFEHTGsowuL6KUIcKtWx19INBIdWYEULwbh95m9HIM8w6KssAjFbC9k4KeDkazZI5E8yWIEJhqo7S4kG1EjVkfslj904MF8v2L9RL9Mm26Ez2CR5mRgTOmwwF8MVP0J+WNp%2FCIUIXGqy+ck8XKEGWqRPtNutvPLxmKx%2FHGTON8zXaZdO%2Ff6hlKuIUEyRlEWVJVCKRJD4lf7PQAN%2FzKBGsR%2FOCG44ICSISbVCRg+wz6Bn8sX7rg6HLTaJ5AFqTTe11v1E9AJGDVA7whaz5uMKh4mF9BB3eSAC1pSe2t6biLXk9uMnp7BWJClR5tqmhWImlc49sFala8OP6Dj1BfEMol8KaAu2WcJAW15P%2F2Z7yNeH17XDH8fTO+a%2Fmd2ux4zK5ZoC68IoPihNpFYeQjblREY0ZXdwkwRr5ZqrjYBG57opT7Y5nttIE9mSgm%2FsNlMF%2FZ35ObxB4ZhFsqnQELC2ReRHCDMsyxij5XrhcilaVT4MNFNooyaFopOjnP5MHlVa1Mw%2FgNRscIWG3He1spf1AFYrFbkgWm4Rogc28nd146zjb8hOZCIQT3F5nYquLoulcn+jW4DQrO%2FlRRbQ9F0F2R0fWv4x9xwf4ytzCD74rFUuEkw6vhRYsaO53kHQZP9ElxDrXor6hAPYtCWpPrYpEKjUXJm57eVVqeWjfWO4VHFB8WpwYF%2Fv7FuGD0ffcMDYees%2FkFIFA+UETen%2FW%2FczLEBK5TjqVevN8NxUUYCF4jljjuUoX8el9CX1amQztTDPLj0u52DERdfkIcaunQTOGz2Tnr944Lwkt+t8YE1uxUk0KOKS9RIPdjbiX+lnU1WA%2FHE5bBh8A6OXxij%2FlY%2FMK5%2FiE5JaNPtcQ== \nThe data is in the URL, and not in any server or device.  Remember , if the url or password is lost, data cannot be recovered. \n Credits \n Thanks to much to my CSS Grid teacher Sandra Laguna and to my professional beta testers  Pablo Duque ,  Mario Rodriguez ,  Marcos Stival ,  Alberto Rojas  and  Manuel Méndez .", 'MVVM-iOS \n MVVM iOS Boilerplate App using SwiftUI, Combine and CoreData. \n Demo app \n \n Core features \n \n MVVM architecture. \n SwiftUI for views. \n Combine for efficient reactive changes. \n CoreData for data persistance. \n \n Boilerplate utils \n CoreData DataSource \n Implements CoreData connection and publish it via singleton. \n | Source code | Example |\n|:---:|:---:|\n|  MVVM Boilerplate/DataSource/DataSource.swift  |  None  | \n CoreData Entity \n Provide a protocol to implement other CoreData entities.  \n | Source code | Example |\n|:---:|:---:|\n|  MVVM Boilerplate/Entities/Entity.swift  |  MVVM Boilerplate/Entities/TaskEntity.swift  | \n When a CoreData entity is managed by a developer defined class: \n *  Class > Name  entity configuration of data model must be equal to class name, ex.:  TaskEntity .\n *  Class > Codegen  entity configuration of data model must be  Manual/None . \n Entities Model \n Extendable class that implements some functions to provide reactive CoreData entities changes. \n | Source code | Example |\n|:---:|:---:|\n|  MVVM Boilerplate/Models/Model.swift  |  MVVM Boilerplate/Models/TasksModel.swift  |', '\n \n \n \n GoPSI - Private Set Intersection in Golang \n Simple Private Set Intersection implemented in pure Go. It uses SRA algorithm  [1]  as encryption scheme and Bloom Filters  [2]  to perform set intersection. \n Examples and Docs \n Two full examples are already implemented:\n- Simple SRA encryption:  code \n- PSI algorithm example:  docs   code . \n Checkout  GoDoc Documentation \n References \n \n Adi Shamir, Ronald L. Rivest and Leonard M. Adleman,  "Mental Poker" , April 1979. https://people.csail.mit.edu/rivest/pubs/SRA81.pdf \n Wikipedia,  "Bloom filter" , July 2005. https://en.wikipedia.org/wiki/Bloom_filter \n', "\n \n \n \n GoPaillier \n Extended version of a Paillier cryptosystem implementation in Go.  \n Features \n \n Extended Paillier cryptosystem implementation with negative number support (read more  here ). \n Uses Standard Form notation to encode numbers allowing to use Paillier encryption scheme over integer and floating points numbers (read more about  number package here ). \n Allows four different operations: \n Addition between encrypted and plain numbers:  A' + B . \n subtraction between encrypted and plain numbers:  A' + (-B) . \n Multiplication between encrypted and plain numbers:  A' * B . \n Division between encrypted and plain numbers:  A' * 1/B . \n \n Installation \n sh\ngo get github.com/lucasmenendez/gopaillier@latest \n Examples \n There are three basic examples ready to help starting with the library:\n- Basic Paillier example:  Source code .\n- Median example:  Source code .\n- SDK example:  Source code .", 'Last node standing \n The objective of the challenge is to implement a simple distributed concensus system based on libp2p and the Vocdoni SDK voting library.  The result of the challenge is a node agent written in go that supports the following functions: \n Phase 1) Discovery \n The program takes a libp2p groupKey as an argument. This key is used to find and connect nodes within the libp2p network. \n Using this p2p network the nodes exchange their public keys. \n Phase 2) Random leader election \n The group decides a master node, based on some quick consensus approach (for example based on the node public key modulus). \n Phase 3) Weakest node voting \n Using the provided SDK, the master node creates a vote census based on the fetched public keys. \n Next, the master node creates an election with the all the nodes as candidates.  \n Once the election is created, the nodes vote one of their group peers according to their local criteria concerning the performance of the connection of the node. That is, they vote the worst connected node (network latency, IP geo or any other metric).  \n Once all votes are submited, as can be checked through the SDK, the most-voted node is kicked out the libp2p group key by all the rest of the group peers. \n Alternative:  Instead of connection information, use a proof-of-work mechanism. Each node needs to solve as many PoW puzzles as possible, the node that generated less puzzles is kicked. \n Phase 4) Last node standing \n This process is repeated until there is only one standing node. \n Tecnical Info \n For the voting implmentation, you can use the following libraries: \n \n Vocdoni API Golang SDK for creating the census, election and submit votes \n \n https://github.com/vocdoni/vocdoni-node/tree/master/apiclient \n You can use the development network, the API can consule an existing gateway, such as: https://gw1.dev.vocdoni.net/v2 \n Use the Master branch \n \n As transport layer use our libp2p customization \n \n https://github.com/vocdoni/vocdoni-node/tree/master/ipfssync/subpub \n You can take as much time as you find necessary, and implement to the extent that you find meaningful. \n Feel free to join our Discord server to ask questions: \n https://discord.gg/xzz9BgHx']
phonkee,["Bruno Server \n This project is WIP and school project. Just before release on GitHub, I successfully made\ntranseferred live audio from peer to peer over network (not lan). In lack of\nworking hardware on another end, we weren't able to make a real call. So feel\nfree to test this, client can be found  here . \n Running the server \n Clone this repo and \n python brunod.py \n For all the options use the  --help  switch. Default TCP port is  9090 \nand UDP  31500 . Default ip is  localhost . \n Cotributing \n Open issue if you come across any bug or if you know python and have free time,\nclone the repo, fix the bug and make pull request! \n There is stuff to do with SSL connection which is fairly easy on server\nside but this work on client side before we can apply it. Protocol\nbetween peers (UDP connections) needs to be designed but server <-> client protocol\nis pretty much locked down. \n Some todos:\n  * SSL\n  * Text messaging\n  * Groups chats\n  * Friends\n  * Password hashing\n  * Video calls\n  * File transfer \n If you are really bored, you could write tests :)", "Bruno Client \n This is the client for  Bruno VoIP system  and is even more WIP than the server. \n With this client, you'll only be able to make voice call. Server support text messages but this client doesnt (atm). \n Running it \n Change the IPs in the source code to your liking (if not running server on localhost) and just  go run client.go . \n When the client is running\n``` \n \n \n \n /register    \n/login    \n/udp_init\n/call  \n``` \n \n \n \n To answer\n``` \n \n \n \n /answer\n``` \n \n \n \n You'll need portaudio to make the calls. \n Contribuing \n If you have ideas on peer-to-peer protocol, open up an issue and don't go working on it by your self without me (or us) knowing about it. Will make stuff easier. \n At the moment (GitHub release) there is one or two unnecessary gorutines which can be removed (fixed) and interface is\nshit. I'm new to Go so there might be some Go-idiom stuff too so feel free to fix those or alteast let me know about it.", "patrol \n Warning: this project is in initial state. It's nowhere near to complete!! \n Patrol is implementation of sentry server in go language.\nIt uses sentry protocol so you can use raven clients (currently protocol version 4 is supported).\nFrontend is written in angularjs. \n For demo you can try:\n    http://demo.patrol.name \n username: demo\npassword: demo\n \n It's still very limited, under heavy development. \n As database patrol uses exclusively postgres (for its advanced field types).\nFor queue you can currently use either redis or rabbitmq. \n Goal of this project is not to replace sentry with all its features, but\ncreate simple, portable, easily deployable solution for logging. \n In first stable version all the static data will be embedded to binary, so there\nwill be ne dependencies.\nAll configuration is provided with command line arguments. \n Development of frontend is located at https://github.com/phonkee/patrol-frontend/ \n Authors:\nphonkee \n Contributions:\nAny ideas are welcome \n test", 'README \n \n Warning !! \n This library is heavily developed and the api can be changed !! \nI hope that in couple of days the api will stabilize and I will continue\nto develop other drivers. \n What is this repository for? \n Ergoq package is small and lightweight message queue abstraction.\nCurrently redis implementation and amqp is done.\nIn the future more implementations will be done \n Usage \n All snippets of code assume import of library \n go\nimport "github.com/phonkee/ergoq" \n Ergoq supports drivers system as seen in sql package. Every driver uses it\'s own connection(for redis it\'s redis.Pool).\nTo open ergoq message queue you can use Open function and provide DSN. \nEvery driver can have slightly different implementation but usually you will see \n <driverName>://<host>:<port>/<database>?params\n \n Example: \n go\ndsn := "redis://localhost:6379/0?max_idle=100&max_active=100&idle_timeout=200"\ndsnAmqp := "amqp://guest:guest@localhost:5672/test?auto_ack=true&prefix=queues" \n Each driver can support it\'s params.  \n Drivers \n Redis \n connection: &redis.Pool \n DSN params: \n \n max_idle - default is 10 \n max_active - default is 10 \n idle_timeout - default is 300 \n \n Local \n local memory queue and publish/subscribe \n DSN params: \n \n size - max size of queues \n \n Open message queue \n You can open message two ways.  \n a. You provide DSN string to ergoq.Open and let ergoq make connections for you \n go\nmq, err := ergoq.Open("redis://localhost:6379/0")\nif err != nil {\n    panic(err)\n} \n b. You provide connection to OpenConnection \n go\npool := redis.Pool{\n    Dial: func() (redis.Conn, error) {\n        return redis.Dial("tcp", ":6379")\n    },\n}\nmq, err := ergoq.OpenConnection("redis", &pool, "auto_ack=true")\nif err != nil {\n    panic(err)\n} \n API \n MessageQueuer interface says it all. \n ```go\ntype MessageQueuer interface {\n    // Pushes message to queue\n    Push(topic string, messages ...[]byte) error \n // Pops message from queue\nPop(topic string) (QueueMessage, error)\n\n// Publishes message to queue (all subscribers)\n// Fanout\nPublish(topic string, message []byte) error\n\n// Subscribes to queue(s)\nSubscribe(quit <-chan struct{}, topics ...string) (chan SubscriberMessage, chan error)\n \n }\n``` \n Examples: \n ```go\n// Error checking is omitted, but please you make your checks!\nmq, _ := ergoq.Open("redis://localhost:6379/0") \n // If we want to push to queue (direct) only first who pops this value will\n// process it\n_ := mq.Push("queue", []byte("message")) \n // pop data from queue\n// second argument is blocking\n// third optional parameter is timeout for blocking\ndata, _ := mq.Pop("queue") \n // If we want to publish message to all subscribers of given queue\n// we need to call Publish method \n errPub := mq.Publish("user:1", "logged_in")\nif errPub != nil {\n    panic(errPub)\n} \n // subscribe to channels can be donw following way.\n// You need to provide "quit" channel when subscription will be stopped.\n// Subscribe returns 2 channels, result and errors.\nquit := make(chan struct{})\nresults, error := mq.Subscribe(quit, "user:1", "admins") \n go func() {\n    for {\n        select {\n            r <- results:\n                fmt.Println("result %+v", r)\n            e <- errors:\n                panic(e)\n        }\n    }\n}()\n``` \n Contribute \n Welcome!', 'godsn \n Parsing of DSN urls.  \n \n Used in  ergoq (queue abstraction)!', 'Gocacher \n \n \n Gocacher is cache abstraction. It\'s intended to use in web applications with\npossibility to choose cache implementation directly from configuration (dsn). \n Currently redis implementation is written and tested, but in the future there will\nbe more implementations (memcached, and other..). \n All cache implementations satisfy this interface\n```go\ntype Cacher interface {\n    // returns cache value by key\n    Get(key string) ([]byte, error) \n // Sets key to value, if expiration is not given it\'s used from settings\nSet(key string, value []byte, expiration ...time.Duration) error\n\n// Deletes key in cache\nDelete(key string) error\n\n// Increments key by 1, if num is given by that amout will be incremented\nIncr(key string, num ...int) (int, error)\n\n// Decrements key by 1, if num is given it decrements by given number\nDecr(key string, num ...int) (int, error)\n\n// Return cache to cache pool\nClose() error\n \n }\n``` \n Before we dive into examples all use this import\n go\nimport (\n    "time"\n    "github.com/phonkee/gocacher"\n) \n Examples: \n ```go\n// Open Cache with expiration of 60 seconds and prefix "cache"\ncache, _ := gocacher.Open("redis://localhost:5379/1?expiration=60&prefix=cache") \n // this key will be set with expiration set in url "expiration=60"\ncache.Set("key", []byte("message")) \n // value will be []byte("message")\nvalue, _ := cache.Get("key") \n // this key will expire in 5 seconds\ncache.Set("key-with-expiration", []byte("message"), time.Seconds*5) \n // deletes the key from cache\ncache.Delete("key") \n // increments key and returns incremented value\ni, _ := cache.Incr("increment-key") // i is now 1\ni, _ := cache.Incr("increment-key", 10) // i is now 11 \n i, _ := cache.Decr("increment-key", 5) // i is now 6\ni, _ := cache.Decr("increment-key") // i is now 5 \n ``` \n Open Cache connection \n You can open connection in two ways. \n go\ncache, _ := gocacher.Open("redis://localhost:5379/1") \n or you can provide connection to gocacher.OpenConnection and provide additional settings\nas url query \n ```go\npool := &redis.Pool{\n    Dial: func() (redis.Conn, error) {\n        return redis.Dial("tcp", "localhost:5379")\n    },\n} \n cache, _ := gocacher.OpenConnection("redis", pool) \n // With additional cache settings\ncache, _ := gocacher.OpenConnection("redis", pool, "expiration=60&prefix=cache")\n``` \n Parameters \n You can pass multiple parameters to  Open  or  OpenConnection  as query part of dsn. \n e.g.\n go\ncache, _ := gocacher.Open("locmem:///0?expiration=10s")\ncache, _ := gocacher.Open("redis://localhost:5379/1?expiration=60&prefix=cache") \n Cache implementations \n Currently two imlpementations are available:\n* redis - redis storage\n* locmem - local memory storage \n Locmem \n Local memory cache. Supports multiple databases. Currently there is no garbage collect or limiting of items in database.\nThis will be updated in near future. \n go\ncache, _ := gocacher.Open("locmem:///0") \n parameters: \n \n expiration - string parsed with time parse duration. (default expiration is 0s - neverending) \n \n Redis \n Redis cache support. Supports multiple databases and all commands. \n parameters: \n \n pool_max_active - redis pool settings \n pool_max_idle - redis pool settings \n pool_idle_timeout - redis pool settings \n expiration - default expiration \n prefix - prefix for cache keys \n \n Contribute \n Welcome!', 'patrol-frontend \n Frontend app for patrol \n Frontend application for patrol logging platform. \n author:\nphonkee', 'Goexpose \n Goexpose is lightweight json api server that maps url path to various tasks.\nGoexpose can be used in various scenarios: either make call commands on your servers (or \nfarm of servers), or you can use it as monitoring tool.\nBuiltin tasks are currently: \n \n shell task - list of shell commands \n http task - call external http request \n info task - information about server \n postgres task - run queries on postgres database \n redis task - run commands on redis \n cassandra task - run cassandra queries \n mysql task - task to run mysql queries \n multi task - run multiple tasks \n filesystem task - serving file(s) from filesystem \n \n I have a plan to implement other tasks with support for: memcache, mongodb, sqlite, file.. \n All these commands can accepts variables from route (gorilla mux is used).\nGOexpose has system for authorization, currently basic (username password) is implemented.\nIn the future more types of authorization will be implemented. \n Lets see example configuration file: \n json\n{\n    "host": "127.0.0.1",\n    "port": 9900,\n    "ssl": {\n        "cert": "./cert.pem",\n        "key": "./key.pem"\n    },\n    "reload_env": true,\n    "endpoints": [{\n        "path": "/info",\n        "authorizers": ["basic"],\n        "methods": {\n            "GET": {\n                "type": "info",\n                "description": "Info task"\n            }\n        }\n    }],\n    "authorizers": {\n        "basic": {\n            "type": "basic",\n            "config": {\n                "username": "hello",\n                "password": "world"\n            }\n        }\n    }\n} \n This means that Goexpose will listen on https://127.0.0.1:9900 \n"endpoints" is a list of defined endpoints that goexpose responds to. \n You can also write your configuration in yaml format(command line arg  -format ) \n Configuration: \n \n host - host that we will listen on \n port - port number \n ssl - ssl settings  \n cert - cert file \n key - key file \n \n \n reload_env - reload env variables on every request \n endpoints - list of endpoints, config for endpoint:     \n path - url path \n authorizers - list of authorizers applied to this endpoint (see Authorizers) \n methods - dictionary that maps http method to task \n \n \n \n Installation: \n Run go install  \n go install github.com/phonkee/goexpose\n \n Interpolation: \n Goexpose provides various variables from url, query, request.\nThis data is available in commands to interpolate various strings.\ntext/template is used and available data is in this structure: \n json\n{\n    "url": {},\n    "query": {},\n    "request": {\n        "method": "",\n        "body": ""\n    },\n    "env": {}\n} \n* env - environment variables\n* url - variables from url regular expressions\n* query - query values from "query_params"\n* request - request vars from goexpose request\n    * method - http method from request\n    * body - body passed to request \n Query Params: \n Goexpose has support to use query parameters. But you have to configure all\nparams that you want to use. Goexpose gives you possibility to validate params\nvalues with regular expressions and provide default value.\nYou can provide "query_params" on two levels. You can specify them on endpoint level\nand also on task level. \n Configuration: \n json\n{\n    "query_params": {\n        "return_params": true,\n        "params": [{\n            "name": "page",\n            "regexp": "^[0-9]+$",\n            "default": "0"\n        }, {\n            "name": "limit",\n            "regexp": "^[0-9]+$",\n            "default": "10"\n        }]\n    }\n} \n Formats: \n http task and shell task have possibility to set format of response.\nCurrently available formats are: "json", "jsonlines", "lines", "text".\nFormat can be combination of multiple formats. e.g. \n "format": "json|jsonlines"\n \n First format that returns result without error will be used.\nIf "text" is not found in format, it is automatically inserted to the end. \n Tasks: \n Tasks can be configured in config["methods"] which is a map[string]TaskConfig - \nhttp method to task.\nEvery task config has common part and configuration for given task.\nCommon configuration is: \n json\n{\n    "type": "http",\n    "authorizers": [],\n    "config": {},\n    "query_params": {\n        "params": [{\n            "name": "id",\n            "regexp": "^[0-9]+$",\n            "default": "0"\n        }],\n        "return_params": true\n    }\n} \n \n type - type of task. \n authorizers - list of authorizers for given endpoint (see Authorizers) \n config - configuration for given task type (will describe later in each task) \n query_params - query params (see Query Params) \n return_params - whether goexpose should return those params in response \n \n HttpTask: \n Http task is task that can do external request. Task configuration is following: \n json\n{\n    "type": "http",\n    "config": {\n        "single_result": 0,\n        "urls": [{\n            "url": "http://127.0.0.1:8000/{{.url.id}}",\n            "post_body": false,\n            "format": "json",\n            "return_headers": false\n        }, {\n            "url": "http://127.0.0.1:8000/{{.url.id}}",\n            "method": "PUT",\n            "post_body": false,\n            "format": "json",\n            "return_headers": false,\n            "post_body": true,\n        }]\n    }\n} \n Configuration: \n \n urls - list of url configurations \n url - url to send request to, url is interpolated (see Interpolation) \n method - request to url will not have the same method as request to goexpose, given method value\n    will be used \n format - format of response, if no format is given goexpose will try to read Content-Type, if application/json\n    (see Formats) \n return_headers - whether to return response headers from url response to goexpose response \n post_body - if goexpose should post body of goexpose request to url \n \n \n single_result - only that result will be returned (unwrapped from array) \n \n ShellTask: \n ShellTask is task that is able to run shell commands on target server. Every command\nis interpolated (see Interpolation) \n Warning!!!  Use appropriate regular expressions for inputs so you don\'t expose your server\nto shell injection.\nExample: \n json\n{\n    "type": "shell",\n    "config": {\n        "env": {\n            "key": "value"\n        },\n        "shell": "/bin/bash",\n        "commands": [{\n            "command": "echo \\"{{.url.id}}\\"",\n            "chdir": "/tmp",\n            "format": "json",\n            "return_command": true\n        }]\n    }\n} \n Configuration: \n \n env - custom environment variables \n shell - shell to run command with \n commands - list of commands to be called: \n command - shell command to be run, interpolated (see Interpolation) \n chdir - change directory before run command \n format - format of the response (see Formats) \n return_command - whether to return command in response \n \n \n single_result - index which command will be "unwrapped" from result array \n \n InfoTask: \n Info task returns information about goexpose. In result you can find version of goexpose and also\nall registered tasks with info. Task info has no configuration. \n PostgresTask: \n Run queries on postgres database. Configuration for postgres task: \n json\n{\n    "type": "postgres",\n    "config": {\n        "return_queries": true,\n        "queries": [{\n            "url": "postgres://username:password@localhost/database",\n            "query": "SELECT * FROM product WHERE id = $1",\n            "args": [\n                "{{.url.id}}"\n            ]\n        }]\n    }\n} \n Configuration: \n \n return_queries - whether queries with args should be added  \n queries - list of queries \n url - postgres url (passed to sql.Open, refer to https://github.com/lib/pq), interpolated (see Interpolation) \n methods - allowed methods, if not specified all methods are allowed \n query - sql query with placeholders $1, $2 ... (query is not interpolated!!!) \n args - list of arguments to query - all queries are interpolated (see Interpolation). \n \n \n single_result - index which query will be "unwrapped" from result array \n \n RedisTask: \n Task that can run multiple commands on redis. Example: \n json\n{\n    "type": "redis",\n    "config": {\n        "address": "127.0.0.1:6379",\n        "network": "tcp",\n        "database": 1,\n        "return_queries": true,\n        "queries": [{\n            "command": "GET",\n            "args": [\n                "product:{{.url.id}}"\n            ],\n            "type": "string"\n        }]\n    }\n} \n Configuration: \n \n address - address to connect to (see http://godoc.org/github.com/garyburd/redigo/redis#Dial)\n    Default: ":6379", interpolated (see Interpolation) \n network - network (see http://godoc.org/github.com/garyburd/redigo/redis#Dial)\n    Default: "tcp" \n database - database number\n    Default: 1 \n return_queries - whether to return queries in response \n queries - list of queries settings \n command - redis command \n args - arguments passed to redis command, all arguments are interpolated (see Interpolation) \n type - type of return value. Possible values are: \n bool \n float64 \n int \n int64 \n ints - list of integers \n string - default \n strings - list of strings \n uint64 \n values \n stringmap - map[string]string \n \n \n \n \n single_result - index which query will be "unwrapped" from result array \n \n CassandraTask: \n Run cassandra queries task. Example: \n json\n{\n    "type": "cassandra",\n    "config": {\n        "return_queries": true,\n        "queries": [{\n            "query": "SELECT * from user WHERE id = ?",\n            "args": [\n                "{{.url.id}}"\n            ],\n            "cluster": [\n                "192.168.1.1",\n                "192.168.1.2"\n            ],\n            "keyspace": "keyspace"\n        }]\n    }\n} \n Configuration: \n \n return_queries - whether to return query, args in response \n queries - list of queries configurations, query configuration: \n query - query with placeholders \n args - arguments to query which are interpolated (See interpolation) \n cluster - list of hosts in cluster, all args are interpolated (see Interpolation) \n keyspace - keyspace to use, interpolated (see Interpolation) \n \n \n single_result - index which query will be "unwrapped" from result array \n \n MySQLTask: \n Run mysql queries. Example: \n json\n{\n    "type": "mysql",\n    "config": {\n        "return_queries": true,\n        "queries": [{\n            "url": "user:password@localhost/dbname",\n            "query": "SELECT * FROM auth_user WHERE id = ?",\n            "args": [\n                "{{.url.id}}"\n            ]\n        }]\n    }\n} \n Configuration: \n \n return_queries - whether to return query with args to response \n queries - list of queries, query config: \n url - url to connect to (refer to https://github.com/go-sql-driver/mysql), interpolated (see Interpolation) \n query - query with placeholders \n args - list of arguments, every argument will be interpolated (see Interpolation) \n \n \n single_result - index which query will be "unwrapped" from result array \n \n MultiTask: \n Multi task gives possibility to run multiple tasks in one task. These task can be any tasks (except of embedded multi task). \n json\n{\n    "type": "multi",\n    "config": {\n        "single_result": 0,\n        "tasks": [{\n            "type": "http",\n            "config": {\n            "single_result": 0,\n            "urls": [{\n                "url": "http://www.google.com"\n            }]\n        }\n    }]        \n} \n Configuration: \n \n single_result - index which task will be "unwrapped" from result array \n tasks - list of tasks (these embedded tasks does not support authorizers) \n \n FilesystemTask: \n Filesystem task is simple yet powerful task. It can be configured to serve single file or serve all files in directory\nwith optional index page for directories. \n In following example we serve only one file on url /file/some. The output will be json with base encoded file content. \n json\n{\n    "path": "/file/some",\n    "methods": {\n        "GET": {\n            "type": "filesystem",\n            "config": {\n                "file": "/tmp/file"\n            }\n        }\n    }\n} \n In next example we will serve files in directory and provide index page for directories and also give possibility to \nreturn raw file as response. \n json\n{\n    "path": "/static/{file:.+}",\n    "methods": {\n        "GET": {\n            "query_params": {\n                "params": [{\n                    "name": "output",\n                    "regexp": "^raw$",\n                    "default": ""\n                }],\n            },                \n            "config": {\n                "file": "{{.url.file}}",\n                "output": "{{.query.output}}",\n                "directory": "/tmp",\n                "index": true\n            }\n        }\n    }\n} \n Configuration: \n \n file - file to serve (interpolated) \n directory - base directory (interpolated) \n output - type of the output (interpolated) \n "raw" - returns raw file contents, otherwise it\'s wrapped to json \n \n \n index - whether to serve index endpoint for directory \n \n Authorizers: \n Types of authentication ( I know it\'s silly name..)\nFirst you have to define your authorizers in top level "authorizers" and then you can use\nthem in your tasks defined by name. e.g.: \n json\n{\n    "endpoints": [{\n        "path": "/info",\n        "authorizers": ["username_pass"],\n        "methods": {\n            "GET": {\n                "type": "info",\n            }\n        }\n    }],\n    "authorizers": {\n        "username_pass": {\n            "type": "basic",\n            "config": {\n                "username": "hello",\n                "password": "world"\n            }\n        }\n    }\n} \n You can set your authorizers in endpoint configuration, or you can set in every task for fine tuned\nconfiguration. \n Basic \n Support for basic authentication. \n json\n{\n    "type": "basic",\n    "config": {\n        "username": "hello",\n        "password": "world"\n    }\n} \n LDAP \n Support for LDAP authentication. \n json\n{\n    "type": "ldap",\n    "config": {\n        "host": "localhost",\n        "port": 1234,\n        "network": "tls"\n    }\n} \n Configuration:\n* host - host of ldap server (default  localhost )\n* port - port of ldap server (default  389 ) \n* network - one of  tcp ,  tls  (default  tcp )\n* whitelist - list of usernames that can access goexpose endpoint\n* blacklist - list of usernames that are blacklisted to access endpoint \n @TODO - add tls support for certificates \n http \n Support to call external web services for authentication (such as rest).\nRequest to goexpose is allowed when the http call returns status code  200 . Otherwise Unauthorized is raised. \n json\n{\n    "type": "http",\n    "config": {\n        "url": "http://localhost:8080/api/users/login",\n        "data": "{\\"username\\": \\"{{.username}}\\", \\"password\\": \\"{{.password}}\\"}",\n        "method": "post"\n    }\n} \n Configuration:\n* url - url to which goexpose make request. Interpolated (username, password)\n* data - post data (such as json, url values). Interpolated (username, password)\n* method - http method. Interpolated (username, password) \n Example: \n in folder example/ there is complete example for couple of tasks.\nYou can find example  here!  or  yaml! \n @TODO:\nAdd tasks for: sqlite, memcached, mongodb \n Author: \n phonkee', 'Treasure ORM \n  This project is replaced with  Buildix  - query builder for sqlx  \n !!!! Treasure ORM is in phase of experimenting !!! \n ORM library for rust (or rather proof of concept, with following heavy development) inspired by awesome django framework. \n This is still experiment, and I started probably from the other side of ORM that should be started by its development, \nbut I think that ease of definition of models and its columns is the "sale argument" of every ORM.\nRight after that it\'s query language. \n Treasure ORM will be developed first on just Postgres, but with mind of multiple available dialects. \n All ORMs use some kind of reflection to have information about all columns, which is easy to implement in dynamic languages,\nbut in statically compiled languages it should be done another way.\nRust has for this its healthy macro system from which Treasure uses syntax expressions to generate needed code.\nI know you say "that\'s not idiomatic rust", but it helps to avoid a lot of things.\nLet me show my idea how should model definition look like. I will omit for now ForeignKey, ManyToMany, OneToOne which\nI am still doing design decisions for now. \n ```rust \n ![feature(custom_attribute,plugin)] \n ![plugin(treasure)] \n extern crate treasure; \n use treasure::models::model::Model; \n [model(db_name="custom_user",primary_key="id",unique(email,test),unique(some,other),index(some,other)] \n struct User { \n #[column(db_name="ID",primary_key)]\npub id: i32,\n\n#[column(unique)]\npub username: String,\n\n#[column]\npub password: Option<String>,\n\n#[column]\npub email: String,\n\n#[column]\npub some: String,\n\n#[column(db_name="custom_other")]\npub other: String,\n \n }\n``` \n Treasure will generate Model trait impl methods such as:\n rust\nfn model_options(&self) -> ModelOptions; \nwhich returns inspect information about model (store as static? in the future for speed improvements) \n rust\nfn init_new() -> Self; \nwhich is constructor method to create new model instance. \n Right now you can try and see what currently Treasure orm does by runnin:\n bash\n    cargo run --example simple --verbose \n For debugging/implementation purposes Treasure dumps generated implementations to stdout. \n Design \n Every struct that will be persistable should implement trait Model (call that struct model).\nThis struct must be annotated with "model" attribute that tells Treasure to inspect this model and generate\nneeded Model trait method. \nEvery field in model that is annotated with "column" attribute will be accepted as database column. All other not\nannotated will be in options list but marked as "unused". \n Model attrs \n For now model has following possible annotations\n* db_name - name of the database table, if not given snake case of struct name will be used\n* primary_key - name of the column that is primary_key, if incorrect compiler must raise sane error.\n    primary key can be also set as column attribute "primary_key"\n* unique - list of columns that are unique. Multiple occurences can happen. This will not be used much, but will be used\n    in database migrations (which we will support in the future)\n* index - list of fields that should belong to index. Multiple occurences can happen also. (For migrations)\n* managed - whether Treasure should handle creation of model in db (in future)\nMain goal is to write exhaustive compiler errors in case of error. We must have a lot of validations! We can do more! \n Decision needed: add annotation inline_options that inlines model_options() method \n Column attrs \n Every struct field that you want to persiste to database table must be annotated with "column"\nFor column there are following possigle annotations:\n* db_name - database table column name, if not given struct field name will be used.\n* primary_key - information that column is primary_key\n* unique - treat column as unique (will be used in db migrations)\n* index - attach to column index (in db migration)\n* not_persist - do not persist this field to database \n These are implemented and added to ColumnInfo which holds all informations about column.\nYou can see that we have trait Column which all POD types must implement. Also future Option  (which stands for nullable column),\nForeignKey, ManyToMany, OneToOne wil implement.\nThis gives us interesting way of defining new fields in the future (postgres array??) \n Column attributes will be extendable, so every Column implementor can have its own set of additional annotation attributes,\ne.g. for number types we can implement min, max, default.. The sky is the limit. \n Column validations \n Under design decisions!\nEvery model will have its own possibility to provide validation_fn in model annotations that will be called.\nAlso Column trait will have validate method that treasure will call with arguments: columnvalue, ColumnOptions instance\nfor given column. \n Code generation \n Treasure is doing quite a lot of code generation to be easily usable without code repetition. Also for future query language\nwe will need inspected information about model and its fields. That\'s why Treasure generates supporting methods for every model,\nthat gives all model information.\nTreasure also generates init_new function for every model where it calls Column::init with ColumnOptions parameter so it\ncan return appropriate value (default?) \n Query \n Treasure ORM provides set of macros to make querying of models easier. \nThis part is still in the making, some small part of select macros are already written, however now they need\nto be connected to real Builder. Treasure will provide two builders:\nBuilder - this builder works upon tables, columns\nModelBuilder - this builder will be tightly coupled with models and will have method to return Builder that\n                    will be populated from data from given model. \n Builder will also have ability to "map" results to object, probably it will be function that accepts closure with \nargument rows (in single mode row). This rows will not be direct rows from database engine, but abstraction over them\nbecause we have also possibility to have "aliases" for columns defined in model (db_name). \n Query macros: \n Query macros have their names by sql counterparts.\n* select - macro to perform select queries\n* update - macro to perform update queries (single instances or multiple rows)\n* delete - macro to perform delete queries (single instances or multiple rows)\n* insert - macro to insert model instances to database \n Every macro has first argument sort of identification followed by "[" where are all parts of query are specified, ending with "]". \nselect query has 2 possibilities:\n* many:  - this is for selecting multiple objects \n rust \nselect![many:User[<query_parts>]] \n \n one:  - this selects just one object from database. (TODO: exceptions DoesNotExist, MultipleObjectsReturned)\n rust \nselect![one:User[<query_parts>]]   \n \n The   part shown in example is where all modifiers are set. these modifiers are defined following way:\n [ ].\nYou can see that modifiers are not separated by "," it\'s from the nature of macros, rather their values are\nsurrounded by [] which makes them quite readable. In following example you can see that. \n Example:\n rust\nselect![many:User[\n    filter[\n        ["age__lt" => 10]    \n    ]\n    limit[1, 10]\n]] \n In next parts I will try to explain every query_part of queries \n filter: \n Filter applies to following queries: select, update(mass), delete(mass) \n In filter you can specify separate clauses such as:\n rust\n["username" => "phonkee"] \n First is name of model column following by => and value. Column name can have field lookups (such as in django)\nLookups take the form  ["field__lookuptype" => value] . If lookup type is not specified "__exact" is used.\nThe plan is to have support for following lookup types;\n* exact\n* iexact\n* contains\n* icontains\n* in\n* gt\n* gte\n* lt\n* lte\n* startswith\n* istartswith\n* endswith\n* iendswith\n* range\n* year\n* month\n* day\n* week_day\n* isnull\n* search\n* regex\n* iregex \n and probably other... \n @TODO: add not[....] modifier. \n Filtering supports also AND and OR conditions. They both have this format and [...], or [...].\nYou can stack them anyway you want. If you don\'t provide single "and" or "or" in filter, \nthey will be default wrapped in AND clause. \n rust\nselect!(many:User[\n    filter[\n        ["name__icontains" => "Peter"]\n        ["age__gte" => 30]\n    ]\n]) \n will be automatically wrapped to and clause and will equal to this:\n rust\nselect!(many:User[\n    filter[\n        and [\n            ["name__icontains" => "Peter"]\n            ["age__gte" => 30]\n        ]\n    ]\n]) \n AND and OR clauses can be stacked, so you can create really complex clauses: \n rust\nselect!(many:User[\n    filter[\n        or [\n            ["something__icontains" => "ehm"]            \n        ]\n        and [\n            ["name__icontains" => "Peter"]\n            ["age__gte" => 30]\n        ]\n        ["one__in" => ["one", "two", "three"] \n    ]\n]) \n will be equal as:\n rust\nselect!(many:User[\n    filter[\n        and [\n            or [\n                ["something__icontains" => "ehm"]            \n            ]\n            and [\n                ["name__icontains" => "Peter"]\n                ["age__gte" => 30]\n            ]\n            ["one__in" => ["one", "two", "three"] \n        ]\n    ]\n]) \n nice example is also  Example \n Isn\'t that pretty?\nI hope you like this query language as I like, more updates will come later, there are a ton of things more to implement,\nand yes I mean a ton just in these querying macros. Like update already instantiated query builder with additional filters, limits... \n TODO: \ndefine other macros \n select \n select macro instantiates select query builder and prepopulates it with model options. \n rust\n// selecting data from database\nselect!(many:User[\n    filter[\n        ["username" => "phonkee"]\n        ["age__gte" => 30]\n    ]\n]).collect(db) \n update \n update macro has two possibilities to call:\n* update!(model_instance[...]) - this updates model instance\n* update!(many:User[...]) - this makes bulk update to database (TODO: not implemented) \n examples: \n rust\n// example of update of single model instance\nlet _user = User::init_new();\nlet _qb = update!(user[\n    columns[\n        "count_logins",\n        "last_logged"\n    ]\n]); \n rust\n// example of update of single model instance\n// @TODO: implement, find out how to do additions...\nlet _qb = update!(many:User[\n    set[\n        ["quote" => "something"]\n    ]\n]); \n Signals \n add support for signals that will be probably in annotation, e.g.: \n ```rust \n [model(pre_insert="pre_insert")] \n struct User {\n}\n``` \n Signals to support:\n* post_load - after object has been load from db\n* pre_insert - before insert to db\n* post_insert - after insert to db\n* pre_update - before update of model instance to db\n* post_update - after update to database \n This will generate more code .... \n Database connection \n Design decision needed!\nWrite wrapper around results (all dialects can implement). \n Contributions: \n If you want to contribute with ideas and/or code, I will be very happy! \n Author: Peter Vrba (phonkee)\nLicense: MIT', "Context \n vcontext  package provides single object  Context . \nContext is dictionary like datastructure with custom access to items.\nYou can access your data by dot access. \nContext does not wrap data in itself, rather just access them. Context has  data  attribute where data is stored. \n Install \n vcontext is on  pypi  so you can simply: \n pip install vcontext \n __getitem__  vs  __getattribute__ \n For  Context  I have decided to use  __getitem__  approach, since I wanted to have consistent dot access also to lists/tuples.\nThis would be impossible using  __getattr__ . \n Item \n Item is dot-separated path to value. This item is splitted and values have following rules\n* string - access to dictionary item or object attribute\n* integer - access to list item\n* first part must be string (since Context is a dictionary) \n Example: \n python\ncontext = Context({\n    'status': 200,\n    'message': 'OK',\n    'result': [\n        {\n            'user': {\n                'username': 'phonkee',\n                'name': 'Peter Vrba'\n            }\n        }\n    ]\n})\nassert context['result.0.user.username'] == 'phonkee'\nassert context['status'] == 200 \n If the data is not found,  KeyError  is raised on part of item that was not found. Context provides  get  method where \nyou can specify default value if value is not found and that will never raise exception. \n Build items: \n Context would be useless if it only supported get of values. Context has support also to create/delete values on \nunderlying datas by given  item . \nLets build a structure from previous example: \n ```python\ncontext = Context()\ncontext['status'] = 200\ncontext['message'] = 'OK'\ncontext['result.0.user.username'] = 'phonkee'\ncontext['result.0.user.name'] = 'Peter Vrba' \n assert context.data == {\n    'status': 200,\n    'message': 'OK',\n    'result': [\n        {\n            'user': {\n                'username': 'phonkee',\n                'name': 'Peter Vrba'\n            }\n        }\n    ]\n}\n``` \n Now we try to delete item \n python\ndel context['result.0']\nassert len(context['result']) == 0 \n Isn't that cute little helper? \n .keys(item=None): \n Context also supports  keys  method. By calling this method context traverses recursively object. It has support for\ndict/list, for custom object it returns just the object key. \n python\ncontext = Context()\ncontext['hello.world'] = 'yay'\nassert context.keys() == ['hello.world'] \n api: \n Context provides following methods: \n \n .copy()  - deepcopies data and returns new context \n .dumps(item=None)  - dump to json, attributes: \n item - item to be dumped to json \n \n \n .items(**kwargs)  - list of key value items (tuple key, value), **kwargs passed to  keys  method \n .iteritems(**kwargs)  - generator version of items, **kwargs passed to  keys  method \n keys(item=None)  - returns list of all keys, attributes: \n item - item to be dumped to json \n \n \n \n Contribute: \n Contributions are welcome, there are still a lot of parts to be enhanced. \n TODO: \n \n add support for special list key  append  so we can append to list. e.g: \n python\ncontext['result.usernames.__append__'] = 'phonkee' \n \n Author \n Peter Vrba (phonkee)", 'response \n Response is simple helper for rest api responses. The naming of api functions is: for setter  Value  and for getter\n GetValue  (Only exception is  String  so we satisfy  Stringer  interface).\nLet\'s see some examples: \n ```go\nresponse.New(http.StatusOK).Write(w, r) \n // or with default OK status \n response.New().Write(w, r) \n ``` \n This writes following response to ResponseWriter (w). \n json\n{\n  "status": 200,\n  "message": "OK"\n} \n Response adds ability to add various data to response object. You can use method  Data  to set data\nor there are some shorthand for usual data keys. \n ```go \n response.New().Data("result", result).Write(w, r) \n // or with a shorthand \n response.New().Result(result).Write(w, r)\n``` \n There is also shorthand to provide slice result that automatiaclly adds  result_size . \n go\nresponse.New().SliceResult(result).Write(w, r) \n Shortcuts \n response serves following shortcuts to create responses \n* Data\n* Error\n* HTML\n* Result\n* SliceResult\n* Write \n Example: \n ```go\nresult := map[string]string{}\nresponse.Result(result)\n```` \n Is the same as doing \n```go\nresult := map[string]string{}\nresponse.New(http.StatusOK).Result(result)\n```` \n or even\n```go\nresult := map[string]string{}\nresponse.New().Result(result)\n```` \n Contributions \n You are welcome to send PR.', 'Metadata \n Metadata can inspect types and return JSON response for OPTIONS requests.\nAll api is chainable, but warning it changes data inplace. \n Example: \n ``go\n    type Product struct {\n        Name string json:"name"`\n    } \n metadata := New("Product detail").Description("Basic information about product")\nmetadata.Action(ACTION_CREATE).From(ProductNew{})\nmetadata.Action(ACTION_DELETE)\nmetadata.Action(ACTION_RETRIEVE).Field("result").From(Product{})\n \n ``` \n This will yield to this when json marshalled \n json\n    {\n        "name": "test endpoint",\n        "description": "description",\n        "actions": {\n            "POST": {\n                "type": "struct",\n                "fields": {\n                    "name": {\n                        "type": "string",\n                    }\n                }\n            }\n        }\n    } \n You can even describe more complicated structures \n ``go\n    type User struct {\n        Username string json:"username"`\n    } \n md := New("Product detail").Description("Basic information about product")\nmd.Action(ACTION_RETRIEVE).Field("result", "user").From(Product{})\n \n ``` \n Which then accepts structure like this: \n json\n{\n  "result": {\n    "user": {\n      "username": "phonkee"\n    }\n  }\n} \n Fields also support choices so you can do this \n go\n    md := New()\n    md.Action(ACTION_CREATE).Field("status").Choices().Add(1, "new").Add(2, "active").Add(3, "closed") \n TODO: \n add support for custom validators such as min max value and other. \n Contribute: \n Your contribution is welcome, feel free to send PR.', 'go-xmlrpc \n Warning! This project is under heavy development!! \n go-xml rpc takes another path to provide parsing of xml. Instead of reflect for binding xml => go, go-xmlrpc generates code for xml parsing.\nIt uses etree implementation and generates code directly for service methods.\nIsn\'t that nice?\nIt\'s safe, fast, awesome! \n Ok let\'s have a look at example \n ```go\n//go:generate xmlrpcgen --file $GOFILE HelloService\npackage example \n / \nSearchService xml rpc service for searching in database\n /\ntype SearchService struct {\n    Config Config\n} \n / \nSearch service method\n  /\nfunc (h *SearchService) Search(query string, page int, isit bool) ([]string, error) {\n    return []string{}, nil\n}\n``` \n That\'s pretty simple service with search method.\nNow run  go generate  and go-xmlrpc parses your service methods and generates xml parsing code directly to your methods. \n Current version generates this code: \n ```go\n// This file is autogenerated by xmlrpcgen\n// do not change it directly! \n package core \n import (\n    "github.com/beevik/etree"\n    "github.com/phonkee/go-xmlrpc"\n    "strconv"\n) \n var (\n    availableMethodsForSearchService = map[string]bool{\n        "Search": true,\n    }\n) \n / \n    MethodExists returns whether rpc method is available on service\n /\nfunc (s *SearchService) MethodExists(method string) (ok bool) {\n    _, ok = availableMethodsForSearchService[method]\n    return\n} \n / \n    ListMethods returns list of all available methods for given service\n /\nfunc (s *SearchService) ListMethods() []string {\n    result := make([]string, 0, len(availableMethodsForSearchService))\n    for key := range availableMethodsForSearchService {\n        result = append(result, key)\n    }\n    return result\n} \n / \n    Dispatch dispatches method on service, do not use this method directly.\n    root is params  etree.Element (actually "methodCall/params"\n /\nfunc (s  SearchService) Dispatch(method string, root  etree.Element) (doc  etree.Document, err error) { \n // call appropriate methods\nswitch method {\ncase "Search":\n    // Get parameters from xmlrpc request\n\n    v_2 := root.FindElement("param[1]/value")\n    if v_2 == nil {\n        err = xmlrpc.Errorf(400, "could not find query")\n        return\n    }\n\n    var query string\n    if query, err = xmlrpc.XPathValueGetString(v_2, "query"); err != nil {\n        return\n    }\n\n    v_3 := root.FindElement("param[2]/value")\n    if v_3 == nil {\n        err = xmlrpc.Errorf(400, "could not find page")\n        return\n    }\n\n    var page int\n    if page, err = xmlrpc.XPathValueGetInt(v_3, "page"); err != nil {\n        return\n    }\n\n    v_4 := root.FindElement("param[3]/value")\n    if v_4 == nil {\n        err = xmlrpc.Errorf(400, "could not find isit")\n        return\n    }\n\n    var isit bool\n    if isit, err = xmlrpc.XPathValueGetBool(v_4, "isit"); err != nil {\n        return\n    }\n\n    // If following method call fails there are 2 possible reasons:\n    // 1. you have either changed method signature or you deleted method. Please re-run "go generate"\n    // 2. you have probably found a bug and you should file issue on github.\n    // @TODO: add panic recovery that returns error with 500 code\n\n    var result_1 []string\n\n    result_1, err = s.Search(query, page, isit)\n\n    // create *etree.Document\n    doc = etree.NewDocument()\n    doc.CreateProcInst("xml", "version=\\"1.0\\" encoding=\\"UTF-8\\"")\n    methodResponse_5 := doc.CreateElement("methodResponse")\n    if err != nil {\n\n        // move this code to error.\n        fault_10 := methodResponse_5.CreateElement("fault")\n\n        code_9 := 500\n\n        // Try to cast error to xmlrpc.Error (with code added)\n        if code_6, ok_8 := err.(xmlrpc.Error); ok_8 {\n            code_9 = code_6.Code()\n        }\n\n        struct_7 := fault_10.CreateElement("value").CreateElement("struct")\n\n        member_11 := struct_7.CreateElement("member")\n        member_11.CreateElement("name").SetText("faultCode")\n        member_11.CreateElement("value").CreateElement("int").SetText(strconv.Itoa(code_9))\n\n        member_12 := struct_7.CreateElement("member")\n        member_12.CreateElement("name").SetText("faultString")\n        member_12.CreateElement("value").CreateElement("string").SetText(err.Error())\n\n    } else {\n        // here is place where we need to hydrate results\n        v_13 := methodResponse_5.CreateElement("params").CreateElement("param").CreateElement("value")\n        array_data_14 := v_13.CreateElement("array").CreateElement("data")\n        for _, item_15 := range result_1 {\n            value_16 := array_data_14.CreateElement("value")\n            value_16.CreateElement("string").SetText(item_15)\n\n        }\n\n    }\ndefault:\n    // method not found, this should not happened since we check whether method exists\n    err = xmlrpc.ErrMethodNotFound\n    return\n}\nreturn\n \n }\n```` \n go-xmlrpc has handler api so you can register your service instance (pointer) to handler and directly pass as http.Handler \n ```go\nhandler := xmlrpc.Handler() \n // We pass Config as example so you see that you can provide database connection or any other resource to service. \nif err := handler.AddService(&HelloService{Config:Config}, "hello"); err != nil {\n    panic(err)\n}\n``` \n You can then call methods  hello.Search  with your favorite xmlrpc client.\nYou can use then handler directly in your favorite mux router since it is Handler. \n Return values: \n Your service methods must return either:\n* error - simple error or xmlrpc.Error with code ( xmlrpc.Errorf(400, "this %s", "error) )\n* result and error \n This is because xml rpc should return at least error. \n Error: \n If you return xmlrpc error with added code it will be addedded to  result.fault .\nOtherwise error code will be 500. \n Features: \n \n since go-xmlrpc generates code in your package, you can use also unexported methods (yay) \n you can register your services with instantiated database connections, or other variables \n Automatically adds  system.listMethods  with all available methods \n inspect service method arguments and return values recursively (yay nice!) \n \n Limitations: \n \n Registered services must be pointers (just to be sure all your methods are usable) \n arguments currently don\'t support pointers (will be used in the future for non required arguments) \n \n Gotchas: \n Don\'t forget to rerun  go generate  when you either: \n \n change definition of service methods \n remove service methods \n add service methods \n \n TODO: \n \n Add support for missing types (unsigned integer family) \n Add proper error messages to parse errors (with whole path).  \n Cleanup code generation with proper documentation \n Possibly remove temporary variables in parsing code. \n \n Contributions: \n Your PRs are welcome \n Author: \n phonkee', "gopypi \n Gopypi is private pypi repository implemented in go language. Gopypi can store your private packages and support\n pip install .\nGopypi supports distutils/setuptools setup.py uploading of packages.\nAnd it does more thant this.\nGopypi supports management of active users, maintainers of packages and lot more. \n This project is under development and should NOT be used in production for now. I am working to \nimplement all features and bring alpha version in short time. Stay tuned! \n Features \n If you install gopypi you will get out of the box following features: \n \n auth module - maintain users with access permissions, maintainers access to packages \n packages - uploading of packages, versions, files (directly from setup.py) \n Licenses - list of licenses for uploaded packages \n Clean admin interface - Simple but powerful modern SPA admin interface (written in vue.js) \n Embedded static data - all static files are embedded directly into binary \n \n Gopypi has some optional features that can be enabled in administration, such as: \n \n download stats - stats about downloaded packages \n automatically assign maintainers - reads information from python packages and assigns users \n \n Admin \n Gopypi has modern SPA admin interface where you can browse information about packages, maintain user credentials etc.\nCurrently only admin can login into this administration, but that will change in near future, so all active users\ncan log into admin and see information about packages they maintain. \n Command line interface \n gopypi is single binary which has multiple subcommands, you can see them by typing  gopypi -h \n Installation \n The easiest way is to download precompiled binaries from github repository releases.\nCurrent version ccan be found here: \n https://github.com/phonkee/gopypi/releases/tag/0.5.3 \n Otherwise you can type \n go get github.com/phonkee/gopypi/...\n \n After you have installed gopypi you need to create config file. You can use gopypi command  makeconfig  that helps\nyou with generating new configuration interactively. \n ./gopypi makeconfig\n \n Database \n Gopypi currently supports postgres, mysql databases. To correctly setup database dsn refer to \n \n postgres - https://godoc.org/github.com/lib/pq \n mysql - https://github.com/go-sql-driver/mysql#dsn-data-source-name \n \n Now it's time to run database migrations: \n ./gopypi migrate --config gopypi.conf\n \n This will apply all database migrations automaticaly. For every new gopypi release migrate command is needed to be run. \n After you have applied all database migrations it's time to create new admin user, so you can access admin interface: \n ./gopypi createadmin --config gopypi.conf\n \n After you have succesfully created admin user you can now run server. \n ./gopypi runserver --config gopypi.conf\n \n Future features \n Gopypi has following features planned: \n \n list of package classifiers with stats \n only admin can login into admin interface \n enable registration from  python setup.py register \n email notifications about package changes \n classifiers - create page in admin groupping packages by classifier \n \n Upload package \n For instructions how to write  setup.py  you can find more information in admin interface  HOWTO .  \n Screenshots \n Here are some screenshots from gopypi admin \n Admin dashboard \n \n Admin user list \n \n Admin add user \n \n Admin package detail \n \n Admin download stats \n \n Contributors \n You are welcome! \n Author \n phonkee", 'go-rip \n \n \n Small library for making json rest calls.\ngo-rip provides fluent chainable api that makes doing rest calls reslly simple. \n name \n Why the name? Well  rest in peace  fits quite good as rest api client. \n Install \n You can install this library simply by typing: \n go get github.com/phonkee/go-rip\n \n Examples: \n Let\'s see this library in action.\nFirst let\'s create a client that we will use in all examples \n go\nclient := rip.New("http://127.0.0.1/api/v1") \n Client has even more methods to add: \n \n Headers - add default headers for all method calls \n AppendSlash - add trailing slash if not presented \n Base - set base url \n Client - set custom http.Client instead of default ones \n Data - add data to be sent as body, data can be: \n string - is converted to []byte \n byte[] - sent as is \n other - will be json Marshalled to []byte \n \n \n Path - add path prefix \n QueryValues - add query values (query parameters) \n UserAgent - set custom user agent instead of default one \n \n Now let\'s make a http GET request. \n ```go\ntype Product struct {} \n product = Product{}\nstatus := 0\nif err := client.Get("product", 1).Do(context.Background(), &product).Status(&status).Error(); err != nil {\n    panic("oops")\n}\n``` \n This makes a http GET call to "http://127.0.0.1/api/v1/product/1/" and unmarshals result\nto  product  variable. We are also handling errors and filling status variable with http status code in the same line.\nIsn\'t that nice? \n You can see that go-rip uses golangs context for http requests and must be passed as first argument to  Do  method. \n Let\'s make a POST request with some data. \n go\nproduct = Product{}\nresult = map[string]interface{}\nif err := client.Post("product", 1).Data(product).Do(context.Background(), &result).Error(); err != nil {\n    panic(err)\n} \n We just made http POST request to "http://127.0.0.1/api/v1/product/1/" and we sent body of the request\njson marshalled product. Ain\'t that really nice? \n author \n Peter Vrba  phonkee@phonkee.eu', 'go-pubsub \n Simple publish subscribe queue. This is available only inside application.\nThere is no intent to make this span across processes/machines. \n Examples \n ```go \n ps := pubsub.New()\nsubscriber := ps.Subscribe("user:1", "user:2").Do(func(message Message) {\n    println("We got meessage on topic", message.Topic())\n}) \n defer subscriber.Close() \n count := ps.Publish(NewMessage("user:1:username", "hello")) \n fmt.Printf("Published messages to %v subscribers", count) \n ``` \n go-pubsub has also default Hub, if you don\'t need to track your own Hub. \n ```go\nsubscriber := pubsub.Subscribe("user:1", "user:2").Do(func(message Message) {\n    println("We got meessage on topic", message.Topic())\n}) \n defer subscriber.Close() \n count := pubsub.Publish(NewMessage("user:1:username", "hello")) \n fmt.Printf("Published messages to %v subscribers", count) \n ``` \n tests \n If you want to run tests, you need goconvey to be installed. You can install it by typing: \n $ go get github.com/smartystreets/goconvey\n \n and then you can run \n go test\n \n or you can run goconvey for more info, navigate into go-pubsub directory and run: \n goconvey\n \n author \n Peter Vrba  phonkee@phonkee.eu', 'Eden Development \n We are developing Eden: World Builder, a block based building game created by Ari Ronen for iOS. \n Getting Started \n These instructions will get you a copy of the project up and running on your local machine for development and testing purposes. \n Setting up local repo \n Use the Terminal command  git clone https://github.com/ryankontos/EdenWorldBuilder.git  to download the existing repo to your home folder. You will also need to download the "Flurry" folder from this link: https://mega.nz/#!cDQ1SKKA!0G7USvZ3Yf79QOQ6u-Gjn8G7kTNyJ8sAMcqeRy9n3q0, unzip it, and place it inside /EdenWorldBuilder/iOS. \n Uploading your local repo \n After you are done making changes cd into ~/EdenWorldBuilder using  cd ~/EdenBuilder  then type  git add .  to add the changes to the resolve, then commit your changes and add a reason with  git commit -m "INSERT REASON HERE"  and finally upload with  git push -u origin master . You can also commit using Xcode\'s built in Source Control features. \n License \n This project is licensed under the GNU General Public License v3 - see the  LICENSE.md  file for details', 'www.Wusik.com \n Wusik ZR \n This product is not FREE. The source code provided here is only for educational means. To use this product you still need a valid license purchased from Wusik.com \n Also, to re-use this code you need a commercial license from Wusik.com, contact support for details. \n Do not share binaries of this code in any way, unless you have a license from Wusik.com that fits this need. \n Do not distribute this code in any way, but feel free to learn from it. \n In order to compile this code you need to download JUCE and understand how to use it. \n https://github.com/WeAreROLI/JUCE \n I have provided videos on how to do that on my Youtube Channel, starting with the video below. \n https://youtu.be/yqAt3Qo1Lv8 \n For the complete product information visit the following link \n http://wwww.wusik.com/w/index.php/products/synth-samplers/wusik-zr', 'swag \n Simple swagger for APIs. \n ```go \n // instantiate new swag\ns := swag.New(func(sw swag.Swagger) {\n    sw.Title("hello world")\n    sw.Description("Awesome api") \n // Define all available headers\nsw.Headers(\n    swag.Header("X-Rate-Limit-Limit", func(h swag.Headerer) {\n        h.Description("The number of allowed requests in the current period")\n        h.Type(swag.TypeInteger)    \n    })\n),\n \n }) \n // create api path prefix\napi := s.Path("/api/v1") \n // post api\npost := api.Path("/post") \n // define post endpoints\npost.Path("/").Get( \n // prepare all available responses\nswag.Responses(\n    swag.Response(http.StatusOk, func(r swag.Responder) {\n        r.Description("list of posts")\n        r.Returns([]Post{}) \n    }),\n    swag.Response(http.StatusNotFound, func(r swag.Responder) {\n        r.Description("no posts found") \n    }),\n)\n \n ).Post(\n    // now define what http.POST accepts\n    swag.Accepts(CreatePost{}, func(a swag.Acceptor) { \n     // if you use pointer, required would be set automatically to false\n    a.Field("title").Required(false)\n}),\n\n// Define all available headers\nswag.Headers(\n    swag.Header("X-Rate-Limit-Limit", func(h swag.Headerer) {\n        h.Description("The number of allowed requests in the current period")\n        h.Type(swag.Integer)    \n    })\n),\n\n// provide all available responses\nswag.Responses(\n    swag.Response(http.StatusOK, func(r swag.Responder) {\n        r.Returns(Post{})\n    }),\n    swag.Response(http.StatusBadRequest),\n),\n \n ) \n // new path, variable  id  should be automatically added\npost.Path("/{id}", func(p swag.Pather) {\n    // we can define for all actions that id is an integer (yay)\n    p.PathArgument("id").Type(swag.TypeInteger)\n}).Get( \n    // provide all available responses\n    swag.Responses(\n        swag.Response(http.StatusOK, func(r swag.Responder) {\n            r.Returns(Post{})\n        }),\n        swag.Response(http.StatusNotFound),\n    ),\n).Post(\n    // define what we accept (simple)\n    swag.Accepts(UpdatePost{}), \n // do not accept headers from top level\nswag.Headers(swag.NoHeaders())\n\n// define all responses\nswag.Responses(\n    swag.Response(http.StatusOK, func(r swag.Responder) {\n        r.Description("thank you")\n        r.Returns(Post{}, func(r swag.Returner) {\n            r.Inline()\n        })\n    }),\n    swag.Response(http.StatusUnauthorized, func(r swag.Responder) {\n        r.Description("Please login"),\n    }),\n    swag.Response(http.StatusBadRequest),\n),\n \n ).Delete(\n    // responses for delete\n    swag.Responses(\n        swag.Response(http.StatusAccepted),\n        swag.Response(http.StatusNotFound),\n    ),\n) \n // swagger  s  satisfies http.Handler so you can directly use it in http server\n// swagger  s  has Error method that returns error if something goes wrong \n // check for error (simplified)\nif err := s.Error(); err != nil {\n    panic(err)\n}\n``` \n Author \n Peter Vrba  phonkee@phonkee.eu', 'RenWave \n Simple renderer from audio to simple video with\n* background image\n* text labels\n* oscilloscope and other visualisations \n Language and libraries \n We use JUCE framework to build RenWave. \n Author \n Peter Vrba  phonkee@phonkee.eu', 'go-retry \n Simple package to retry function calls. \n usage \n This package provides simple interface that u can use to retry calls.\nLet\'s show example: \n ```go\n// Call retry function\nerr := retry.New(\n    retry.WithConstantBackoff(time.Second * 2), \n    retry.WithMaxRetries(10), \n).Retry(func(ctx context.Context) error { \n // just skip first call and retry this call \nif GetInfo(ctx).CurrentRetry == 0 {\n    return ErrRetryAgain\n}\n\n// we can get information about current retry\ninfo := retry.GetInfo(ctx)\nfmt.Printf("currently we are %v from %v", info.CurrentRetry, info.MaxRetries)\n\n// return error\nreturn fmt.Errorf("nope")\n \n }).Do(context.Background())\n``` \n Even in scenarios where you need to indefinitely retry calls: \n go\nerr := retry.New(\n    retry.WithExponentialBackoff(0),\n).Retry(func(ctx context.Context) error {\n    // Do some work\n    return nil\n}).Do(context.Background()) \n You can rely on context to be canceled \n ```go\nfunc() {\n    ctx, cf := context.WithTimeout(context.Background(), time.Second * 60)\n    defer cf \n // No need to set max retries, when context is canceled retry will end (assuming callback is returned when context is canceled)\nerr := retry.New(\n    retry.WithExponentialBackoff(0), \n).Retry(func(ctx context.Context) error {\n    // Do some work\n    return nil\n}).Do(ctx)\n \n }()\n``` \n backoff \n Go-retry provides constant and exponential retry. You can even implement your own\nretry if you need and pass it to  WithBackoff  option. \n Example: \n go\n// Custom backoff implementation\nerr := retry.New(\n    retry.WithBackoff(BackoffFunc(func(ctx context.Context) error {\n        timer := time.NewTimer(time.Second * (GetInfo(ctx).CurrentRetry % 2) + time.Second)\n        for {\n            select {\n            case <-timer.C:\n                return nil\n            case <-ctx.Done():\n                return ctx.Err()\n            }\n        }\n    })).Retry(func(ctx context.Context) error {\n        // Do some work\n        return nil\n    }).Do(context.Background())\n) \n author \n Peter Vrba  phonkee@phonkee.eu', 'Equalia \n This package helps you with implementation of Eq and PartialEq for structs.\nYou can provide which struct fields to compare and which not. \n \n Structs \n When we want to define which fields are omitted, or we want to provide\ncustom function to return value to be compared. \n ```rust \n [derive(Equalia)] \n [equalia(hash)] \n pub struct Entity { \n #[equalia(skip)]\nvalue1: u8,\n\n#[equalia(map = "map_func")]\nvalue2: u8,\n \n } \n // map function that changes value\nfn map_func(input: &u8) -> u8 {\n    input * 2\n} \n ``` \n When single field ins struct can identify equality. \n ```rust \n [derive(Equalia)] \n [equalia(hash)] \n pub struct Entity { \n #[equalia(only)]\nid: u8,\n\n// this value will be ignored\nvalue2: u8,\n \n }\n``` \n Hash \n When you provide  #[equalia(hash)]  for struct/enum equalia will automatically\nimplement  Hash  trait from given configuration. \n author \n Peter Vrba  phonkee@pm.me', 'Buildix \n Declarative query builder for sqlx.   \n Buildix takes different approach to query building. Instead of defining\nqueries inplace, buildix provides set of derive macros, that generate\neffective code for building and running sql queries.\nYou can easily define query builders with large possibilities.\nReal power comes in filter, where you can define multiple filtering capabilities\nand then only set some filters needed.\nBuildix is aware of Option values and can omit values that are not set.\nThis brings power to any apis that need to filter on multiple columns\nwith ease. \n Please refer to example select builder to see how buildix will work. \n Warning \n This project is work in progress and is heavily developed.\nCurrently buildix does not return any arguments, nor it\'s executing queries.\nI am working hard to bring all the functionalities, so please be kind to me.\nDesign is kinda done, but there will be definitely some changes.\nBuildix currently generates some form of general sql for select and delete.\nPlease stay tuned and star this repository. \n First implementation will implement "general" sql (POC), and then I will work on\nimplementations  for Postgres, MySQL, sqlite - databases that are supported in sqlx. \n This project is currently unusable in this phase and the api can change.\nDo not hack buildix with implementing its traits, because that\'s internal\nfunctionality and will be subject to change. \n Only use derive macros to work with it. \n The vision is to bring at least basic power in span of next weeks. \n Features: \n Features that I am working on (not in order) \n \n SelectBuilder \n [x] Base query \n [x] Join \n [x] Sort \n [x] Limit \n [x] Offset \n [x] Group \n [x] Filter (Implemented - testing) \n [ ] Count \n [ ] Having \n [x] Map - callback support \n [ ] Execute \n [ ] Stream support (low priority) \n [ ] support all dialects (Postgres, MySQL, SQLite, MS SQL) - (design) \n DeleteBuilder \n [x] Filter (shared with SelectBuilder) \n [x] Limit (shared with SelectBuilder) \n [ ] Count \n [x] Map - callback support \n [ ] Execute \n InsertBuilder \n [ ] Insert \n [ ] On duplicate key \n [ ] Returning auto_increment \n [ ] Map - callback support \n [ ] Execute w. Batch support \n UpdateBuilder \n [ ] Update \n [ ] Filter \n [ ] Count \n [ ] Map - callback support \n [ ] Execute w. Batch support \n \n The plan is to first be able to generate sql queries, then provide arguments\nand then work on executors. It\'s basically order from easiest to most tricky. \n Example select query builder \n ```rust \n [derive(Default, SelectBuilder)] \n struct SelectUserBuilder {\n    #[buildix(select)]\n    select: Vec , \n #[buildix(filter)]\nfilter: Filter,\n\n#[buildix(limit)]\nlimit: Option<i32>,\n\n#[buildix(offset)]\noffset: i32,\n\n#[buildix(sort = "name")]\nsort_name: Option<buildix::sort::Sort>,\n\n#[buildix(sort = "age")]\nsort_age: buildix::sort::Sort,\n\n#[buildix(count)]\ncount: i32,\n \n } \n [derive(Default, Select)] \n [buildix(from(table(name = "user", alias = "u")))] \n [buildix(from(join(name = "order", alias = "o", on = "o.user_id = u.id")))] \n [buildix(group = "name", group = "email")] \n struct SelectUser {\n    #[buildix(table = "u")]\n    name: String, \n #[buildix(table = "u")]\nemail: String,\n\n#[buildix(table = "u", column = "custom_age")]\nage: Option<i64>,\n\n#[buildix(expr = "IF(age > 18, true, false)")]\nis_adult: bool,\n\n#[buildix(expr = r#"COALESCE(other, "")"#)]\nother: String,\n \n } \n [derive(Default, Filter)] \n [buildix(operator = "OR")] \n struct Filter {\n    author_id: Option , \n #[buildix(expr = "last_updated < ?")]\nlast_updated: Option<i32>,\n\n// automatically provides filter = "priority = ?"\npriority: i32,\n\n#[buildix(expr = "age > ?", isnull)]\nage: Option<i32>,\n\n// inner filter will be sub clause in parentheses (if needed) \ninner: InnerFilter,\n\n// Vec automatically converts to IN(...), if no value is available\n// this filter will not be available in where clause\n// if you want to set expr. on this field, just provide single\n// ? and buildix will expand it automatically\n// if it\'s empty, it will not be shown in where clause\nid: Vec<i32>,\n \n } \n // even multiple filters supported \n [derive(Default, Filter)] \n [buildix(operator = "AND")] \n struct InnerFilter {\n    value: Option ,\n    value2: Option ,\n} \n ``` \n Filter \n Buildix filter is powerful. Buildix helps you create filters that will be\nwritten to sql only if they are set, it supports null and more.\nIt is very simple to filter by multiple filter values, and even embedded\nfilters which translate into sub clauses. This gives you powerful tool\nfor querying. After you define query once and verify query field names,\nyou can reuse it safely. \n Lets have a look at previous example how it actually translates to sql\nquery. \n Lets try to see what previous example looks like. You can see it in\ntests directory  test_readme_select.rs \n First lets create default querybuilder.\nThe columns do not correspond with database, it\'s just show off, what\npossibilities buildix has. \n rust\nlet mut qb = SelectUserBuilder::default();\nlet (q, _) = query.to_sql::<Postgres>().unwrap(); \n query is now \n sql\nSELECT u.name, u.email, u.custom_age AS age, IF(age > 18, true, false) AS is_adult, COALESCE(other, "") AS other FROM user AS u, INNER JOIN order o (o.user_id = u.id) WHERE (priority = ? OR age ISNULL) GROUP BY name, email ORDER BY age ASC \n if we set inner filter value \n rust\nlet mut qb = SelectUserBuilder::default();\nlet (q, _) = query.to_sql::<Postgres>().unwrap();\nqb.filter.inner.value = Some(42);\nqb.filter.inner.value2 = Some(314); \n now query is \n sql\nSELECT u.name, u.email, u.custom_age AS age, IF(age > 18, true, false) AS is_adult, COALESCE(other, "") AS other FROM user AS u, INNER JOIN order o (o.user_id = u.id) WHERE (priority = ? OR age ISNULL OR (value = ? AND value2 = ?)) GROUP BY name, email ORDER BY age ASC \n You can see how powerful this filtering is. Not to say that there is more\nfunctionality that helps you to build reliable query builders. \n Execute \n Buildix will provide method to execute given query builder, and it detects\nwhether you want to query single record or multiple records, and it will\nset results on query builder instance. If you provide  #[buildix(count)] \nit will also run count queries (for select queries) or in case of update/insert/delete\nqueries affected rows count. If you do not provide it, the code will not be generated\nhence it will be faster. \n In the future buildix will also support stream of records, but that\'s currently\nnot a priority. \n Delete query builder \n Partially designed. \n ```rust \n [derive(DeleteBuilder)] \n [buildix(table="user", map="map_delete")] \n struct UserDeleteBuilder {\n    #[buildix(filter)]\n    filter: Filter, \n #[buildix(limit)]\nlimit: Option<i32>,\n\n#[buildix(count)]\ncount: i32, // if configured, buildix will populate how many records has been deleted\n \n } \n // map_delete checks delete query (e.g. if limit is present)\npub fn map_delete(builder: &mut UserDeleteBuilder) -> buildix::Result<()> {\n    if builder.filter.user_id == 0 {\n        // return error or set user_id to Option \n    }\n    Ok(())\n} \n [derive(FilterQuery)] \n [buildix(operator = "OR")] \n struct Filter {\n    #[buildix(filter = "age > ?", isnull)]\n    age: Option , \n #[buildix(filter = "user_id = ?")]\nuser_id: i32,\n \n }\n``` \n Insert query builder \n Not fully designed yet. \n ```rust \n [derive(InsertBuilder)] \n struct UserInsertBuilder {\n    #[buildix(insert)]\n    insert: Vec , \n #[buildix(count)]\ncount: i32,\n \n } \n [derive(Insert)] \n [buildix(table = "user", unique_key = "id")] \n struct InsertUser {\n    #[buildix(update)]\n    name: String, \n // this field will not be updated when doing `ON DUPLICATE KEY`\nemail: String,\n\n#[buildix(update)]\nage: Option<i64>,\n\n// #[buildix(returning)]\n// id: i32,\n \n }\n``` \n Update query builder \n Not fully designed yet. \n ```rust \n [derive(UpdateBuilder)] \n struct UserUpdateBuilder {\n    #[buildix(update)]\n    update: Vec , \n #[buildix(count)]\ncount: i32,\n \n } \n [derive(Update)] \n [buildix(table = "user")] \n struct UpdateUser {\n    name: String,\n    email: String,\n    age: Option , \n #[buildix(filter)]\nfilter: UpdateFilter,\n \n } \n [derive(Default, Filter)] \n pub struct UpdateFilter {\n    id: i32,\n} \n ``` \n Author \n Peter Vrba  phonkee@pm.me', 'resume \n Here you can find my current resume \n author \n Peter Vrba  phonkee@pm.me', 'tracelog \n trace requests.', 'swag-go \n Simple swagger generator to be used for my purposes. \n Warning \n This library is intended to be used in  init  methods, so error handling is basically done with panic.\nIf you use it differently, please think about this.\nThe idea is to define all things in init, and then just serve swagger.json (or yaml) which is then cached. \n example \n ```go\n// package level Service (so we can access it) from handlers\n// usually put in domain (api) package so it\'s accessible everywhere\nvar Service swag.Swagger \n func init() {\n    // initialize swag Swagger\n    Service = swag.New("pet store")\n} \n type GetPetPathParams struct {\n    ID int  json:"id" swag_description:"primary key in pets database" \n} \n type Pet struct {\n    ID int  json:"id" swag_description:"unique identifier in database" \n    Name string  json:"name" \n    Born time.Time  json:"born" \n} \n type ErrorResponse struct {\n    Error string  json:"error" \n} \n type CreatePetSerializer struct {\n    Name string  json:"name" swag_description:"Name of your pet" \n} \n type FieldValidationError struct {\n    Fields map[string]string  json:"fields" \n} \n type FilterPetsQuery struct {\n    Dogs bool  json:"dogs" swag_description:"only dogs will be returned" \n} \n func init() {\n    // get single pet by id\n    Service.Path("/api/v1/pets/{id}", http.MethodGet).\n        // add path params\n        PathParams(GetPetPathParams{}).\n        Response(http.StatusOK, Pet{}).\n        // in this case 404 does not return any specific response\n        Response(http.StatusNotFound, nil).\n        Response(http.StatusInternalServerError, ErrorResponse{}) \n // create new pet endpoint\nService.Path("/api/v1/pets", http.MethodPost).\n    Body(CreatePetSerializer{}).\n    Response(http.StatusOK, Pet{}).\n    Response(http.StatusBadRequest, FieldValidationError{})\n\n// list pets endpoint\nService.Path("/api/v1/pets", http.MethodGet).\n    QueryParams(FilterPetsQuery{}).\n    Response(http.StatusOK, []Pet{})\n \n }\n``` \n We have also ability to have shared common properties: \n go\nApiV1 := Service.Prefix("/api/v1/").\n    Response(http.StatusNotFound, nil).\n    Response(http.StatusInternalServerError, ErrorResponse{}) \n And even this: \n ``go\ntype UserIdentifierPathQuery struct {\n    ID int json:"id"`\n} \n type Order struct {\n    ID int  json:"id" \n} \n type OrderCacheQueryParams struct {\n    NoCache bool  json:"no_cache" swag_description:"when true, orders will be fetched from database" \n} \n func init() {\n    // prepare prefix that identifies user by id\n    UsersOrdersApiV1 := Service.Prefix("/api/v1/users/{id}/orders").\n        // path params will be inherited in all paths derived from this prefix\n        PathParams(UserIdentifierPathQuery{}).\n        // query params will be inherited in all paths derived from this prefix \n        QueryParams(OrderCacheQueryParams{}).\n        // responses will be inherited in all paths derived from this prefix\n        Response(http.StatusNotFound, nil).\n        Response(http.StatusInternalServerError, ErrorResponse{}) \n // now get list of orders for user - path will be /api/v1/users/{id}/orders\nUsersOrdersApiV1.Path("", http.MethodGet).\n    Response(http.StatusOK, []Order{})\n\n// return single order by order_id\nUsersOrdersApiV1.Path("{order_id}", http.MethodGet).\n    Response(http.StatusOK, Order{})\n \n } \n ``` \n author \n Peter Vrba  phonkee@phonkee.eu', 'Crest \n Simple CRUD rest. This project uses generic to convert functions to http handlers.\nThis is POC and only create is currently  investigated . \n Example \n Let\'s try to create create handler from functions \n ```go \n type CreateUser struct {\n    Username string  json:"username" \n    Password string  json:"password" \n    Email string  json:"email" \n} \n type User struct {\n    ID string  json:"id" \n    Username string  json:"username" \n    Email string  json:"email" \n} \n // CreateUser knows how to create user\nfunc CreateUser(r  http.Request, s  CreateUser) (*User, error) {\n    // either return error\n    // return nil, crest.Error(http.status.BadRequest, nil)\n    // or return user\n    return &User{\n        ID: uuid.New().String,\n        Username: s.Username,\n        Email: s.Email,\n    }, nil\n} \n func init() {\n    // now let\'s register this handler to e.g. gorilla mux router\n    r := mux.NewRouter()\n    r.Path("/api/user/", create.Handler(CreateUser)).Method(http.MethodPut)\n}\n``` \n We have also ability to provide handler that does not return struct type, but generic  Responder  interface.\nSo let\'s try an example and let\'s suppose we have defined structures from previous example \n ```go\nfunc CreateUser( http.Request, s  CreateUser) (Responder, error) {\n    // create user and return back as custom response\n    return Response(http.StatusCreated, User{ID:uuid.New().String}), nil\n} \n ``` \n Author \n Peter Vrba  phonkee@phonkee.eu', 'redact \n Redact secrets from structs and convert to map[string]interface{} for safe logging via zap logger. \n example \n ``golang\ntype Config struct {\n    Username string\n    Password string redact:"protect" Tokens []string redact:"omit" URL string redact:"url"`\n} \n zap.L().Info("config", redact.Field("config", &Config{\n    Username: "shown",\n    Password: "not shown",\n    Tokens: []string{"not", "shown"},\n    URL: "http://username:password@localhost", // password will be removed\n})) \n ``` \n author \n Peter Vrba  phonkee@phonkee.eu', "collection \n Simple immutable collection generic type. That's all. \n author \n Peter Vrba  phonkee@phonkee.eu", 'tagstruct \n This package aims to provide a simple and efficient way to define struct tag parsers.\nCurrently it\'s very limited, but it\'s a start.\nWe can define parser that parses e.g. this: \n value="string", int=1, bool=true, float32=1.0, float64=2.0\n \n Example \n First we need to define available keywords \n go\ntype Tag struct {\n    DefaultFirst int `ts:"name=default"`\n} \n And then we can parse fields of struct \n go\ntagdef := New(Tag)\nresult, err := tagdef.Parse("default=42") \n TODO: \n We like to be able to parse objects (structs) in recursive manner and also arrays, such as: \n `some(id=1, value=2, span(from=1, to=2))`\n \n Even arrays \n `some[(id=1), (id=2)]`\n \n For this we will need to roll our own parser, but it\'s not that hard.', 'Attribs \n This package provides a simple way to parse attributes from a string.\nIt\'s more readable than json, and it was implemented to be used for parsing\nstruct field tags.\nNow you don\'t need to do special parsing for your awesome shiny struct tag, you can\njust define a structure and parse \'em.\nThis package will take care about rest. It provides simple error reporting, with position where\nerror occurred.\nParser and lexer is handwritten, so there is no overhead from using a parser generator.\nYou can find parser  here .\nAltough New function returns generic definition, mapping values to struct uses reflection, so please don\'t use it in performance critical code. \n Example \n Let\'s omit error handling in examples. First we need to define all attributes in single structure:\n go\ntype Tag struct {\n    DefaultFirst int `attr:"name=default"`\n    Readonly bool `attr:"name=readonly"`\n    Description string `attr:"description"`\n} \n Then we create a new instance of attribs. We use generics so parser will return everytime\ncorrect type. \n go\nparser, _ := New(Tag{}) \n And then we can parse fields of struct \n go\nresult, _ := parser.Parse("default=42, readonly, description = \'This is a description\'") \n Now result is: \n go\nTag{\n    DefaultFirst: 42,\n    Readonly: true,\n    Description: "This is a description",\n} \n Supported types \n Currently supported (tested) types are \n \n int, int8, int16, int32, int64 (and pointers) \n uint, uint8, uint16, uint32, uint64  (and pointers) \n string  (and pointers) \n bool  (and pointers) \n struct  (and pointers) \n array \n \n Todo: \n \n [ ] add support for escape sequences in string literals \n [ ] add support for squash? \n \n Author \n Peter Vrba  phonkee@phonkee.eu']
wetherc,['\n Quick links:  Our License  •  Making Contributions  •  Atlas Builds \n \n How We\'re Licensed \n Data + Design  is licensed under the terms of a  Creative Commons BY-NC-SA license . The full legal code is at the link, but here\'s a quick human-readable breakdown for you as well. (Note: This is only a human-readable summary of, and not a substitute for or supplement to, the license.) \n By Attribution \n If you use this book yourself, credit us! Basically, you just need to include a little snippet saying: \n \n Adapted from  Data + Design  by Trina Chiasson and Dyanna Gregory.\nLicensed under a Creative Commons BY-NC-SA 4.0 license. This work is not affiliated with or endorsed\nby the original authors. \n \n About all you\'re saying is, "Hey, this was originally made by these peeps, go check it out. Any edits to this were made by me, not them." Pretty reasonable, right? \n Non-Commercial \n If you fork and make edits to this book, you can\'t do it for monetary gain. The purpose of this book is to help people make better-informed decisions in collecting, analyzing, and presenting their data. And to do that free availability of this book is kind of a big deal. So don\'t sell it, mmkay? \n Share Alike \n Finally, your version has to be licensed under the same terms as this one is. No telling people they can sell their derivatives, no telling people they don\'t have to attribute back to the original. Basically, just keep everything free and open. \n Well, What If... \n Of course there are edge cases. There are times when you\'re just not going to be sure if doing  X  or  Y  will be kosher with the license or not. That\'s fine! Just  open an issue on GitHub  and let us know what\'s up. We\'ll talk it over and help you figure out if what you want to do is cool beans or if you should tweak your plans a little bit. It never hurts to ask! \n \n Quick links:  Our License  •  Making Contributions  •  Atlas Builds \n \n Can I Make Edits? \n Of course you can! That\'s part of the reason we\'re making this book totally openly sourced. We love when the community takes the time and effort to contribute to things like this. There are three different routes we recommend, depending on what you want to do. \n Fork Us! \n This is the best option if you want to go off in your own direction, or just want your own copy of the source for reference, or generally just don\'t think that you\'re in the mood to add a new chapter. For most people, this is the best option. \n What\'s even better? This is super easy to do. Just go  to the repository  and click  Fork  in the upper right corner. Voila! You\'re now the proud owner of a brand spanking new  Data + Design  repository. \n Please note that we are keeping the project\'s CSS in a separate repository ( link ). You\'ll need this if you want to generate your own builds of the book. Likewise, if you see HTML elements and CSS classes that don\'t look familiar, that\'s likely because the book is maintained according to  HTMLBook specifications . \n Fork Us! (Redux) \n Hey, wait a minute! What\'s up with this? It\'s just the same thing all over again! \n Whoops. You got us there. As it turns out, the first step is the same for both people who want to contribute back to our repo and those who don\'t: you just gotta fork it. \n But alright, let\'s say you\'ve got a great idea for a new chapter. What\'s the process for getting it accepted and published look like? Well, \n \n Fork the main repository and  keep it synced . \n Open an  issue with us  and label it as a question. Let everyone know what you\'re thinking. Even if you have a great chapter idea, we can\'t guarantee that there\'s a good spot in the book to put it and we don\'t want you spending hours of your time writing something that we\'ll end up rejecting. Get feedback on the idea and make sure that it\'ll be a chapter we can use. \n Start authoring! Here\'s where your fork comes in handy: you\'ll be making all chapter edits there. \n Get reviewers. Go back to your GitHub issue, check with the commenters there, and see who would be willing to review your chapter for content, grammar, and style. Add them as contributors on your fork, let them make edits, etc. \n Once you all think that the chapter\'s in a good place, create a pull request back to the main repository. That\'s when we\'ll go through and do an \'official\' edit and suggest any changes we\'d like to see before accepting your chapter. \n Publish! We\'ll accept your pull request and you\'ll offically have a chapter in  Data + Design  Congratulate yourself on a job well done and make sure that we didn\'t forget to add your information to our page of contributors. \n \n Use Google Docs \n We realize that not everyone knows how to use GitHub or wants to learn how to. And that\'s fine! If you don\'t want to worry about HTMLbook specifications and forks and pull requests and all that jazz, we\'ve got your back. The process is conceptually similar to Option #2: \n \n Open an  issue with us  and label it as a question. Let everyone know what you\'re thinking. Even if you have a great chapter idea, we can\'t guarantee that there\'s a good spot in the book to put it and we don\'t want you spending hours of your time writing something that we\'ll end up rejecting. Get feedback on the idea and make sure that it\'ll be a chapter we can use. \n Make a  Google Doc  and start authoring! If you don\'t have a Google account, we\'re sorry, but you\'ll just have to bite the bullet and create one. We\'d like to cover every major email provider in the world, but that\'s not terribly practical. \n Get reviewers. Share your document with  ebook@infoactive.co  and we\'ll get you in contact with some editors who are currently available. Make sure that you give us full editing rights so that we can add editors for you! \n Once you all think that the chapter\'s in a good place, drop us a line at  ebook@infoactive.co . We\'ll go through and do an \'official\' edit and suggest any changes we\'d like to see before accepting your chapter. \n Publish! We\'ll handle all the behind-the-scenes stuff to convert your chapter to HTML and add it to the published book. Congratulate yourself on a job well done and make sure that we didn\'t forget to add your information to our page of contributors. \n \n Rejecting Submissions \n Let\'s say you go through the whole process of proposing, authoring, editing, and submitting a new chapter and we reject it. What happens next? \n Well, that\'ll depend on your exact situation. It might be that we don\'t think the chapter is written to the standards of the rest of the book. If that\'s the case, revising and resubmitting is nearly always going to be an option. See if you can get input from someone whose chapter  has  been accepted; get another couple rounds of edits. And then edit it some more for good measure. Then go ahead and resubmit and see what happens! \n Alternately, it might end up that the chapter that was proposed and the chapter that was delivered are significantly different—to the point that your delivered chapter isn\'t a great fit for the book. If that\'s the case, don\'t despair: you\'ve still written a great chapter and can publish it on your forked repo. We\'re sorry that it didn\'t work out, but ask for feedback and see what needs to be changed for it to be accepted as an official addition. \n Or for some entirely other reason that we can\'t predict, we might reject your chapter without the chance to revise and resubmit. If and when this happens, we\'re sorry. (Legitimately, we don\'t get any sick joy out of this!) We try to put quality checks in place to prevent this from happening at all, but we can\'t catch everything. Regardless, you\'re always encouraged to make whatever edits you would like to your forked repository: after all, that sort of personalization and community involvement is why we open sourced this in the first place. \n That said, the best strategy is to just be proactive about your submission: get feedback early and get feedback often. This book was written by so many incredibly dedicated individuals, you shouldn\'t have any difficulty getting good, constructive feedback at every stage of the writing process. Take advantage of this. Even if we don\'t end up accepting your chapter, it\'ll still be something to be proud of having written. \n \n Quick links:  Our License  •  Making Contributions  •  Atlas Builds \n \n Building with Atlas \n [[In Progress]] \n Although, yes, we already have an HTML structure for the book, we still use an external platform to publish it. O\'Reilly\'s  Atlas  allows us to take our book and publish it to a finalized (and slightly prettied-up) HTML version as well as to PDF, MOBI, and EPUB formats. \n Now, if you\'re making an official contribution to our repository, this probably isn\'t too important to you since we\'ll be handling the actual releases. However, if you\'re striking it out on your own, you might want an easy way to publish your book to multiple print and web platforms. Once you\'ve finalized the edits to your forked repo, sign up for a free trial at the link above and you\'ll have the option to import your repository. As long as you\'ve kept everything to  HTMLbook spec , Atlas should be able to process the book without a hitch when you go to build a final release. \n \n Note: You can also author directly  in  Atlas using a nice, friendly GUI (much like authoring in Google Docs or using Microsoft Word). You\'ll still need to import the pre-existing book, but once that\'s done it\'s super easy to add your content using Atlas\'s visual editor. \n', "Applied Statistics \n ToDo: \n \n [ ] Convert this to Oreilly's HTMLBook specification? Maybeh? \n [X] Part 1 \n [X] Introduction \n [X] Why I'm writing this \n [X] Why you should care \n [X] Structure of the book (explanations, exercises, answers) \n [X] What you'll need (R, RStudio, etc.) \n [X] Statistical terminology \n [X] R Basics \n [X] Installing R, RStudio \n [X] Reading in data \n [X] Basic R syntax \n [X] Additional resources \n [X] Measuring Uncertainty \n [X] Measures of central tendency; skewness \n [X] SD; variance; IQR \n [X] Sampling distributions; CLT; SE \n [X] Visualizing uncertainty \n [X] Exercises \n [X] Additional resources \n [X] Research Design \n [X] Relating design and analysis \n [X] Designing sound experiments (garbage in, garbage out) \n [X] Intro to Statistical Inference \n [X] Hypotheses: directional and non-directional \n [X] Standardized test statistics \n [X] Significance: p-values; CIs; etc. \n [X] Degrees of Freedom \n [X] Drawing conclusions \n \n \n [ ] Part 2 \n [ ]  t -Tests \n [X] One-sample \n [X] Paired-samples \n [X] Independent-samples \n [X] Assumptions \n [X] Equality of variance \n [X] Robustness \n [X] Implementation in R \n [X] Case study \n [ ] Exercises \n [ ] Additional resources \n \n \n [ ] Alternatives to  t -Tools \n [X] What are nonparametric statistics? \n [X] Mann-Whitney \n [X] Wilcoxon signed rank \n [X] Concerns and assumptions \n [X] Implementation in R \n [X] Case study \n [ ] Exercises \n [ ] Additional resources \n \n \n [ ] Comparisons Among Multiple Samples: One-Way ANOVA \n [X] General use \n [X] Fixed/random effects \n [X] sum of squares \n [X] F-test \n [ ] post-hocs, planned comparisons \n [X] assumptions &c. \n [ ] robustness \n [ ] implementation in R \n [ ] case study \n [ ] exercises \n [ ] additional resources \n \n \n [ ] Multifactor Studies \n [ ] Multifactor Studies with Replication \n \n \n [ ] Part 3 \n [X] Correlation \n [X] Visualizing relationships \n [X] Measuring linear association \n [X] Multiple correlation \n [X] Partial correlation \n [X] Limitations (sensitivity to distribution; causality; linearity) \n [X] Case study \n [X] Exercises \n [X] Additional resources \n [ ] Simple Linear Regression \n [X] Uses of SLR \n [X] Least squares fitting \n [X] Coefficient of determination \n [X] Residuals \n [ ] Outliers \n [ ] Assumptions \n [ ] Case study \n [ ] Exercises \n [ ] Additional resources \n [ ] Multiple Regression \n [ ] Polynomial Regression \n [ ] Logistic Regression \n [ ] Model Checking and Refinement \n \n \n [ ] Part 4 \n [ ] Working with Time-Series Data \n [ ] Factor Analysis and PCA \n [ ] Cluster Analysis \n [ ] Working with Ranked and Other Nonparametric Data \n [ ] Counts, Proportions, and Odds \n \n \n [ ] Part 5 \n [ ] Choosing the Right Analysis \n [ ] The Role of Data Transformations \n [ ] Sharing Data with Others \n \n \n", "Graduate Work \n A hodgepodge of notes and other miscellaneous files for some (many? most?) of the courses I've taken in VT's Translational Biology, Medicine, and Health program. \n All non-encrypted or otherwise non-protected or privileged files included in this repository are licensed under a  Creative Commons Attribution 4.0 International License . This likewise applies to files that historically (i.e., at any prior point in the git commit history) have been publicly accessible, although only with respect to their revision at that point in time. Any subsequent or prior versions of such files may be protected and licensed under different terms than outlined above. (tl;dr: some files in the commit history may be unprotected. Feel free to use those. Earlier or later revisions might be encrypted. Don't try to use those.)", "Templates \n Included are commonly-used templates for papers, reports, presentations, and other documents. \n License \n All content is copyright of its respective owner. Copyright and licensing information, where available, are included in each template's directory or within the file itself as a comment.", 'rotavirus \n Rotavirus background, lab protocols, and other nifty tidbits', 'TODO: \n \n [ ] R basics \n [ ] Installation? \n [ ] Reading in data \n [ ] Loading packages \n [ ] Creating scripts \n [ ] Knitr and literate scripting \n [ ] Basic syntax \n [ ] Constants \n [ ] Vectors \n [ ] Matrices \n [ ] Data frames \n [ ] Factors \n [ ] Functions \n \n \n [ ] T-tests \n [ ] One-sample \n [ ] Two-sample \n [ ] Paired-samples (equal & unequal variances) \n [ ] Graphics \n [ ] ANOVAs \n [ ] One-way \n [ ] Two-way \n [ ] Two-way with interactions \n [ ] Two-way with repeated measures \n [ ] Graphics \n [ ] Correlations \n [ ] Bivariate \n [ ] Multiple \n [ ] Partial \n [ ] Graphics \n [ ] Regressions \n [ ] Simple linear \n [ ] Multiple \n [ ] Graphics \n', 'dotfiles', 'cast \n Execute commands on many cluster nodes at once. Wreck all your servers more efficiently!', 'Requirements \n \n Docker \n Python 3 \n wget \n unzip \n git \n Java \n \n To Run \n To run,\n $ cd path/to/aver-spark\n$ docker-compose up -d\n$ docker exec -it averspark_master_1 /bin/bash\n$ /code/init.sh\n$ python /code/csvToAvro.py \\\n    -f /tmp/data/Pitching.csv \\\n    -s /code/era.avsc \\\n    -o /output/era.avro\n$ /usr/spark-2.1.0/bin/spark-submit \\\n    --packages com.databricks:spark-avro_2.11:3.2.0 \\\n    /code/queryData.py \n This will start the Docker container, download all external dependencies, and run a couple of Spark jobs. This will output CSVs containing the result sets to two directories:  ./output/lowestERA/  and  ./output/mostInnings/ . \n Sources Consulted \n \n https://docs.python.org/3.5/ \n http://spark.apache.org/docs/latest/sql-programming-guide.html \n http://spark.apache.org/docs/latest/api/python/pyspark.sql.html \n https://docs.docker.com/machine/ \n https://github.com/databricks/spark-avro \n https://www.supergloo.com/fieldnotes/spark-sql-csv-examples-python/ \n https://google.github.io/styleguide/pyguide.html \n https://docs.python.org/2/howto/argparse.html \n https://www.cloudera.com/documentation/enterprise/5-6-x/topics/spark_avro.html \n http://avro.apache.org/docs/1.8.1/gettingstartedpython.html \n https://github.com/gstaubli/csv2avro/blob/master/src/csv2avro.py \n http://stackoverflow.com/questions/6141581/detect-python-version-in-shell-script \n http://stackoverflow.com/questions/2953646/how-to-declare-and-use-boolean-variables-in-shell-script \n http://stackoverflow.com/questions/17444679/reading-a-huge-csv-file \n http://stackoverflow.com/questions/35712601/avro-io-avrotypeexception-the-datum-avro-data-is-not-an-example-of-the-schema \n http://stackoverflow.com/questions/29759893/how-to-read-avro-file-in-pyspark \n http://stackoverflow.com/questions/19670061/bash-if-false-returns-true \n http://stackoverflow.com/questions/1614236/in-python-how-do-i-convert-all-of-the-items-in-a-list-to-floats \n', 'invino', "= Broadcast = \n Broadcast is inspired by the  NVIDIA Broadcast App  (creative project names are clearly my strong suit), and takes some practical hints from  Benjamin Elder ,  Fufu Fang , and  John Emmons . \n Basically, Nvidia released this awesome app to enable background blur or replacement on a live webcam in realtime, and not tied to whatever video conferencing platform your company happens to use (e.g., Zoom has its own built-in background replacement feature). It's really neat, but it was only released for Windows, and only works for folks with a 20 or 30-series Nvidia GPU. So my immediate goal with this is to enable the same functionality for folks running a GNU/Linux system, and to remove the explicit dependency on having a nice, fancy GPU so that laptop users, for example, can benefit from it as well. Eventually I might target a Windows- or Mac-compatible version (and actually, I don't really know for sure that this  isn't  compatible, it's just totally untested on anything but Ubuntu 20.04), but that's a ways off still. \n == Prerequisites == \n \n OS : Anything Linux-y. Built and tested on Ubuntu 20.04; YMMV on other distributions. \n Python 3.x \n Pip \n v4l2  and  v4l2loopback \n \n == Installation == \n git clone git@github.com:wetherc/broadcast.git\ncd broadcast\npython3 -m pip install . \n == Usage == \n python3 -m broadcast.server --help", 'Overview \n This provides a proof-of-concept ONNX runtime able to serve requests over gRPC. \n Usage \n All code blocks assume you are running from the repository root directory. \n \n Install any Python requirements (Python >=3.6)\n   bash\n  python3 -m pip install -r requirements.txt \n Train and export the model (using the iris dataset)\n   bash\n  python3 src/train.py \n Generate the gRPC code from our service definition:\n   bash\n  python3 -m grpc_tools.protoc \\\n    -I./src/protos \\\n    --python_out=./src \\\n    --grpc_python_out=./src \\\n    ./src/protos/onnxmodel.proto \n Start the gRPC server\n   bash\n  python3 src/model_server.py \n Serve a request. In a separate terminal window, run the client application\n   bash\n  python3 src/model_client.py \n', "Prerequisites \n This project uses Uber's  [[https://github.com/uber/h3 | H3]]  system for geospatial indexing. Installation instructions can be found in their repository. Broadly, you'll need  cc ,  make , and  cmake  available in your path. If you're on Mac, you can just  brew install h3  and you're good to go. Uber's documetion contains additional information for compiling from source if you're on a different platform or just enjoy the thrill of it. \n NOTE:  cmake   must  also be in your path to install the H3 Python bindings. You may need to  brew install  this as well if you're on a Mac. \n Installation and Usage \n Just\n cd /path/to/cogo\npip install -r requirements.txt\npip install -e .\njupyter lab \nand you're off to the races.", 'Kubernetes Resource Monitor \n This is a simple K8s service to monitor the system RAM and CPU of each node in\na Kubernetes cluster and that exposes a RESTful API endpoint to easily check\nwhich nodes, if any, meet specific resource requirements (e.g., "Are there\nany nodes with at least 8GB RAM") \n Prerequisites \n \n Docker . Please see the  Get Docker \n    page for installation instructions specific to your distribution. The Docker\n    daemon must be running before executing the  bootstrap.sh  script. Checking\n    the daemon\'s status and invoking it if dead is outside the scope of what\n    I\'ve built here. \n Minikube . If developing locally, Minikube probably is a good idea. The\n    bootstrap script provided in this repository\'s root assumes a Minikube\n    binary is available in your PATH. I developed this with Minikube 1.22.0,\n    but any reasonably modern release should also work OOTB. \n kubectl . Please reference the  installation docs \n    for your distribution. \n skaffold . Please reference the  installation docs \n    for your distribution \n \n Usage \n Assuming all prerequisites are met, you should be able to just\n ./bootstrap.sh \nto start the cluster. By default, we set the number of nodes to 3. \n After the cluster is provisioned and all services are deployed onto it,\nyou can access the REST API at  localhost:8080/v1/resources/ . This endpoint\naccepts 2 optional URL parameters,  cpu=<float>  and  ram=<int> , where\n cpu  is the minimum number of CPU units (in threads) a node must have and\n ram  is the minimum amount of system RAM in megabytes. \n For example, a GET request to  http://127.0.0.1:8080/v1/resources/?cpu=30&ram=4096 \nyields (on my local machine - YMMV):\n {\n  "last_updated": "Fri, 16 Jul 2021 00:47:21 GMT",\n  "nodes": {\n    "resmon": {\n      "cpu": 16,\n      "ram": "32890948Ki",\n      "status": "NOTOKAY"\n    },\n    "resmon-m02": {\n      "cpu": 16,\n      "ram": "32890948Ki",\n      "status": "NOTOKAY"\n    },\n    "resmon-m03": {\n      "cpu": 16,\n      "ram": "32890948Ki",\n      "status": "NOTOKAY"\n    }\n  }\n} \n minikube delete --profile resmon  will deprovision this cluster and destroy all\nassociated resources. \n Caveats \n Please note that GitHub is only hosting a mirror of this repository and is not\nthe authoritative source. Pull requests cannot be accepted.', 'Getting Started with Create React App \n This project was bootstrapped with  Create React App . \n Available Scripts \n In the project directory, you can run: \n npm start \n Runs the app in the development mode.\\\nOpen  http://localhost:3000  to view it in the browser. \n The page will reload if you make edits.\\\nYou will also see any lint errors in the console. \n npm test \n Launches the test runner in the interactive watch mode.\\\nSee the section about  running tests  for more information. \n npm run build \n Builds the app for production to the  build  folder.\\\nIt correctly bundles React in production mode and optimizes the build for the best performance. \n The build is minified and the filenames include the hashes.\\\nYour app is ready to be deployed! \n See the section about  deployment  for more information. \n npm run eject \n Note: this is a one-way operation. Once you  eject , you can’t go back! \n If you aren’t satisfied with the build tool and configuration choices, you can  eject  at any time. This command will remove the single build dependency from your project. \n Instead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except  eject  will still work, but they will point to the copied scripts so you can tweak them. At this point you’re on your own. \n You don’t have to ever use  eject . The curated feature set is suitable for small and middle deployments, and you shouldn’t feel obligated to use this feature. However we understand that this tool wouldn’t be useful if you couldn’t customize it when you are ready for it. \n Learn More \n You can learn more in the  Create React App documentation . \n To learn React, check out the  React documentation .']
mugizico,['Working through MIT OCW 6005 - 2011', 'cse385proj', 'visiGT \n a web application to vizualize comparisons between social media sharing counts ', 'Working through How I Start (Go) Tutorials.', 'Leonids Gatsby starter \n Leonids  is a clean, fixed sidebar Gatsby starter which is perfect for powering your GitHub hosted blog. \n Preview \n \n What is Leonids? \n A blazing fast static blog theme built on Gatsby.js \n \n \n Responsive templates. Looking good on any viewport. \n Fixed sidebar to provide a good navigation experience while reading. \n Light/Dark mode. \n And  the Leonids (/ˈliːənɪdz/ lee-ə-nidz) are a prolific meteor shower associated with the comet  Tempel-Tuttle . \n \n See a  demo  hosted on GitHub pages. \n Getting Started (Recommended) \n Vercel.com deploy button \n \n Netlify CMS deploy button \n \n Local development setup \n gatsby new my-blog https://github.com/renyuanz/leonids\ncd my-blog\ngatsby develop \n Check out your awesome blog at  http://localhost:8000  and Cheers! \n Legacy Jekyll theme \n This project was a Jekyll blog theme, which is archived and not maintained any more. You can still find the source code in  leonids-jekyll  branch.', 'Leonids Gatsby starter \n Leonids  is a clean, fixed sidebar Gatsby starter which is perfect for powering your GitHub hosted blog. \n Preview \n \n What is Leonids? \n A blazing fast static blog theme built on Gatsby.js \n \n \n Responsive templates. Looking good on any viewport. \n Fixed sidebar to provide a good navigation experience while reading. \n Light/Dark mode. \n And  the Leonids (/ˈliːənɪdz/ lee-ə-nidz) are a prolific meteor shower associated with the comet  Tempel-Tuttle . \n \n See a  demo  hosted on GitHub pages. \n Getting Started (Recommended) \n Vercel.com deploy button \n \n Netlify CMS deploy button \n \n Local development setup \n gatsby new my-blog https://github.com/renyuanz/leonids\ncd my-blog\ngatsby develop \n Check out your awesome blog at  http://localhost:8000  and Cheers! \n Legacy Jekyll theme \n This project was a Jekyll blog theme, which is archived and not maintained any more. You can still find the source code in  leonids-jekyll  branch.', 'advent_of_code_2021 \n https://adventofcode.com/2021 ', "data-infra \n This repo contains an end-to-end data infrastructure proof of concept. It is meant to show how a full-fledged data infrastructure can be built to allow sourcing, cleansing/munging, storing,\nand enabling long-term business analysis. It is not production-ready by any means, but could be a good starting point in building a more robust data platform. \n \n \n Start  here  for a background on what this project is(or isn't) \n \n \n Also checkout  my notes  detailing what I am doing and some troubleshooting. \n \n \n Structure \n this repo has 5 subdirectories as following: \n \n data_infra_templates  : contains  terraform  infrastructure declarations for the entire platform to allow quickly spinning up and down all components as needed \n workflow_orchestration  : contains  Airflow  DAGs/ data pipelines for running batch data processes. \n data_processing  : contains  Apache Beam  code for reading, writing, and processing data. \n data_notebooks  : contains some simple jupyter notebooks for exploring data \n docs  : technical decisions, R&D notes, specs, et al. \n \n TODO (Roadmap) \n General \n \n [ ] Stop pushing to  main  directly, start using feature branches for additional work. \n [ ] flesh out docs \n [ ] create comprehensive unittest suite whenever necessary \n [ ] enable creation of entire data stack with one top-level script ( script/setup ) \n [ ] enable automatic teardown of data stack with a script, because this is getting expensive.... \n [ ] enable monitoring and observability  \n \n INFRA templates (Terraform) \n \n [ ] Create Service Account for Contributors and other developers, and separate it from admin account \n [ ] create templates for Jupyter Notebooks (should they be Apache Beam backed?) \n \n Workflow Orchestration (Apache Airflow) \n \n [ ] When necessary, abstract away common libraries and modules :  dags/core  for general librairies,  dags/operators  for custom ones for e.g. \n [ ] set up CI/CD for Cloud Composer using Github Actions and  Google CSR \n \n Data Processing (Apache Beam) \n \n [ ] Expand aggregations and windowing of data. \n [ ] Allow for streaming capabilities with a combination of Pub/sub + GCS File Drop Notification if necessary \n \n Data Notebooks (Jupyter Notebooks) \n TBD"]
aymericbeaumet,['', 'This project is now unmaintained. You might be interested into this active fork: https://depot.vwa.re/ludofloria/myhordes-enhancer/ \n \n Die2Nite Enhancer \n       \n Die2Nite Enhancer is an extension allowing you to improve your game experience on the browser game  Die2Nite . \n The French ( Hordes ), Spanish ( Zombinoia ) and Dutch ( Die Verdammten ) versions are supported. \n Install as an extension (recommended) \n You can directly install this plugin as an extension for the following browsers:\n-  Google Chrome \n-  Mozilla Firefox \n-  Opera \n-  Safari \n Install as a script \n It is also possible to install this plugin as a script. Natively supported in Google Chrome, or with the GreaseMonkey extension under Mozilla Firefox. If you want to do it like so, I consider you know what you are doing: the script can be found  here . \n Feedback \n If you find any problem or have a suggestion, please give me a feedback  here . \n In case of a problem, please be sure all your softwares (browser and extensions) are up to date. Then post a new issue using the header below: \n ```\n-  Operating System:  OS + version\n-  Browser:  Browser + version (indicate GreaseMonkey version if relevant) \n Explain here how to reproduce the problem...\n``` \n License \n zlib/libpng ©  Aymeric Beaumet', 'npm install -g karma-cli\nnpm install\nkarma start', 'npm install -g karma-cli\nnpm install\nkarma start', ":wave: My name's Aymeric, I'm a Software Engineer living in Paris, France. \n 🔭 My day-to-day activities involve working on Infrastructure and Backend systems (Go, Rust, Kubernetes). \n ✍️ I sometime write on my  blog . \n 📫 You can reach out to me by  mail  or on  Twitter .", 'Comments \n aymericbeaumet.com  blog comments via  utteranc.es .']
bwang482,['emotionannotate \n For the  SMILE project \n Set up \n Python 2.7 with pip 7.1.2 \n Use \'pip install -r requirements.txt\' to install all relevant Python dependancies. \n Run web app \n cd src \n python app.py \n Run batch classification \n If you have a list of tweets that you want to run our emotion classifier on, you can: \n cd src \n python Classification.py \n Then follow the instructions. \n More info in src/readme.txt \n Input/output data format \n Input and output are both in json format.\nFor example, \n {"tweetid":"614912375288893440", "text":"@britishmuseum awesome museum"} \n as an input entry, \n {"tweetid":"614912375288893440", \n"text":"@britishmuseum awesome museum",\n"emotions":{"anger":"no","disgust":"no","happy":"yes","sad":"no","surprise":"no"}\n} \n as its output. \n Reference \n \n Bo Wang, Maria Liakata, Arkaitz Zubiaga, Rob Procter and Eric Jensen.  SMILE: Twitter Emotion Classification using Domain Adaptation . In 4th Workshop on Sentiment Analysis where AI meets Psychology (SAAIP), IJCAI 2016. \n', 'TDLSTM \n This Tensorflow implementation approximates the Target-dependent LSTMs proposed in (Tang et al. 2016). \n Updated \n \n Randomized search for hyperparameter optimization is now added. \n Hyperopt and Scikit-optimize (can choose from gp_minimize, forest_minimize and gbrt_minimize) for hyperparameter optimization are now added. \n Code updated to tf 1.2 \n', 'TDParse - Multi-target-specific sentiment recognition on Twitter \n From the EACL 2017 paper, TDParse utilises the syntactic information from parse-tree in conjunction with the left-right context of the target and achieves the state-of-the-art performance on both the benchmarking single-target corpus and new multi-target election data. \n Our approximated version of the LSTM models proposed in  COLING 2016 , can be found at  here . \n Dependencies \n \n Python 2.7 \n sklearn >= 0.18.1 \n gensim == 0.13.4 \n networkx == 1.11 \n ftfy  >= 4.1.1 \n TweeboParser  >= April 1, 2016 \n \n Data \n You can find our election corpus at  here . \n Usage \n Run  TDParse \n```bash \n e.g. using LibLinear with parameter tuning: \n ./run.sh lidong tdparse liblinear scale,tune,pred ../data/lidong/parses/lidong.train.conll ../data/lidong/parses/lidong.test.conll \n or without parameter tuning; adding your choice of C-parameter in the end: \n ./run.sh lidong tdparse liblinear scale,pred ../data/lidong/parses/lidong.train.conll ../data/lidong/parses/lidong.test.conll 0.01\n Run **Naive-seg** bash \n e.g. using scikit-learn implementation of Linear SVM \n ./run.sh election naiveseg sklearnSVM\n``` \n Reference \n "TDParse - Multi-target-specific sentiment recognition on Twitter" - Bo Wang, Maria Liakata, Arkaitz Zubiaga, Rob Procter, to be published in EACL 2017 \n Acknowledgement \n Thanks to Duy-Tin Vo and Yue Zhang, the authors of  "Target-dependent Twitter Sentiment Classification with Rich Automatic Features" , for sharing their code which I have built my implementation upon.', 'My seminar presentation slides for  CS918', 'Deep Signatures \n Using signatures as layers in a neural network. \n Learning the nonlinearity before a signature provides a compact way to select which terms in the signature (of the original path) are useful for the given dataset.', "Hugo Academic Theme \n \n The Hugo  Academic Resumé Template  empowers you to easily create your job-winning online resumé, showcase your academic publications, and create online courses or knowledge bases to grow your audience. \n \n \n \n ️ Trusted by 250,000+ researchers, educators, and students.  Highly customizable via the integrated  no-code, widget-based Wowchemy page builder , making every site truly personalized ⭐⭐⭐⭐⭐ \n Easily write technical content with plain text Markdown, LaTeX math, diagrams, RMarkdown, or Jupyter, and import publications from BibTeX. \n Check out the latest demo  of what you'll get in less than 10 minutes, or  get inspired by our academics and research groups . \n The integrated  Wowchemy  website builder and CMS makes it easy to create a beautiful website for free. Edit your site in the CMS (or your favorite editor), generate it with  Hugo , and deploy with GitHub or Netlify. Customize anything on your site with widgets, light/dark themes, and language packs. \n \n 👉  Get Started \n 📚  View the  documentation \n 💬  Chat with the  Wowchemy research community  or  Hugo community \n 🐦 Twitter:  @wowchemy   @GeorgeCushen   #MadeWithWowchemy \n ⬇️  Automatically import your publications from BibTeX  with the  Hugo Academic CLI   \n 💡  Suggest an improvement \n ⬆️  Updating?  View the  Update Guide  and  Release Notes \n \n We ask you, humbly, to support this open source movement \n Today we ask you to defend the open source independence of the Wowchemy website builder and themes 🐧 \n We're an open source movement that depends on your support to stay online and thriving, but 99.9% of our creators don't give; they simply look the other way. \n ❤️ Click here to become a GitHub Sponsor, unlocking awesome perks such as  exclusive academic templates and widgets \n \n Demo image credits \n \n Open book \n Course \n \n Latest news \n START_SECTION:news \n \n What's new in v5.2? \n What's new in v5.1? \n Version 5.0 (February 2021) \n Version 5.0 Beta 3 (February 2021) \n Version 5.0 Beta 2 (January 2021) \n \n END_SECTION:news"]
d-dozer,[]
mkolod,['scala_akka_r_server', 'README \n TODO', 'Word Cloud \n Work in progress - check back soon for documentation :)', 'JMX MBean for Jedis \n \n Setup \n First, import the dependencies: \n java\nimport java.lang.management.ManagementFactory;\nimport javax.management.MBeanServer;\nimport javax.management.ObjectName;\nimport us.marek.jedis.mbean.JedisMonitor;\nimport us.marek.jedis.mbean.JedisMonitorMBean; \n Next, hook up the MBean to your Jedis object. \n java\nfinal MBeanServer mbs = ManagementFactory.getPlatformMBeanServer(); \nfinal ObjectName name = new ObjectName("us.marek.jedis.mbean:type=JedisMonitor"); \nfinal JedisMonitorMBean mBean = new JedisMonitor(jedis);\nmbs.registerMBean(mBean, name); \n Finally, once your application is running:\n  1. Start VisualVM.\n  2. Enable the VisualVM JMX plugin.\n  3. Find your application on the left-hand side and double-click on it to open a detailed view.\n  4. Click on the MBeans tab.\n  5. Click on us.marek.jedis.mbean.JedisMonitor to open up the Attributes view.\n  6. Double-clicking on numeric values will open a self-updating time series graph. \n  7. Right-clicking on the time series graph will allow you to export the data to a CSV file and to change the time range of the graph. \n Also note that there exists a  JMeter JMX plugin , so you could monitor the Redis metrics of interest from JMeter while the other performance tests are running. This could be helpful in figuring out the bugs associated with the interplay between the load elsewhere in the application and the Java client\'s demands on the Redis connections. \n Example \n ```java\npackage us.marek.jedis.mbean; \n import java.io.File;\nimport java.lang.management.ManagementFactory;\nimport java.util.Scanner; \n import javax.management.InstanceAlreadyExistsException;\nimport javax.management.MBeanRegistrationException;\nimport javax.management.MBeanServer;\nimport javax.management.MalformedObjectNameException;\nimport javax.management.NotCompliantMBeanException;\nimport javax.management.ObjectName; \n import redis.clients.jedis.Client;\nimport redis.clients.jedis.Jedis;\nimport redis.clients.jedis.JedisPool;\nimport redis.clients.jedis.JedisPoolConfig; \n public class SampleApp { \n private static JedisPool pool = null;\nprivate static Jedis jedis = null;\n\npublic static void main(final String[] args) \n        throws InstanceAlreadyExistsException, MBeanRegistrationException,\n        NotCompliantMBeanException, MalformedObjectNameException {\n\n    Client client = null;\n\n    try {\n\n    pool = new JedisPool(new JedisPoolConfig(), "localhost");\n    jedis = pool.getResource();\n    client = jedis.getClient();\n    client.connect();\n\n    final MBeanServer mbs = ManagementFactory.getPlatformMBeanServer(); \n    final ObjectName name = new ObjectName("us.marek.jedis.mbean:type=JedisMonitor");\n    // update MBean information every 2 seconds\n    final JedisMonitorMBean mBean = new JedisMonitor(jedis, 2000L);\n    mbs.registerMBean(mBean, name);\n\n        /* Run forever until program is killed. This is just a test class\n           to see JMX in action via VisualVM/YourKit/JProfiler.\n         */\n        while (true) {\n\n            try {\n\n\n                Thread.sleep(100);\n\n            } catch (final InterruptedException ie) {\n\n                ie.printStackTrace();\n\n            }\n        }\n\n    } finally {\n\n        jedis.close();\n        pool.close();\n        client.close();\n\n    }\n\n}\n \n }\n```', 'NLP in Spark \n An open source project focused on bringing NLP tools to the Scala/Spark environment.', 'JNI Refresher \n Just run the "run.sh" shell script. Note that you may need to change it to pick the correct JAVA_HOME export, or delete it if you already have JAVA_HOME set up. Also, I\'m exporting a dylib on OS X, so feel free to change it to .so on Linux. \n Also, note that this assumes that the JNI methods will be applied as instance methods. To call static methods/methods on Scala singleton objects, you may need  scalah , which is a modified javah.  \n TODO: \n Explore  JNI SBT plugin  to conver this into an SBT build, with proper package conventions, without the shell script, etc. \n Notes: \n The full JNI spec can be found  here . \n If you\'d like to learn more about JNI, check out  this tutorial  (Java only, but you should get the idea for Scala). \n For comparisons between JNI and JNA, see  here . I agree with the author of the JNI vs. JNA blog post that JNI is actually easier for non-trivial use cases, and it\'s faster.  \n For better C++ (not just C) support, it may be worth trying out  JavaCpp  instead of writing JNI directly. \n Check out  this post  for JNI best practices (warning: it\'s a bit dated (2009), but JNI hasn\'t evolved much in the last few years, so it\'s probably OK). \n Specific info about Java and native exception handling in JNI code can be found  here .', 'Essential Play Code \n This repository contains exercises and solutions for\n Underscore\'s Essential Play \nbook and training course. \n If you want to discuss the content or exercises with the authors,\njoin us in our chat room on Gitter: \n \n Using the Source Code \n This repository contains two branches: one for  exercises  and one for  solutions .\nThe directory structure is the same in each branch,\nwith each exercise stored as a standalone Play project in its own directory. \n You will need to have Git and Java and an internet connection to run the exercises.\nAll other dependendencies are either included with the projects\nor downloaded on demand during compilation. \n See below for quick getting started instructions.\nFor more detailed instructions see Chapter 1 of the book. \n Getting Started on Linux or OS X \n Complete the following steps outlined in Chapter 1 in the section entitled\n"Setting up SBT for This Book": \n \n Clone this repository to a directory on your hard drive,\n   e.g.  C:\\essential-play-code : \n \n ~~~\n   bash$ git clone https://github.com/underscoreio/essential-play-code.git\n   ~~~ \n \n Change to the directory for the "hello world" exercise: \n \n ~~~\n   bash$ cd essential-play-code/chapter1-hello\n   ~~~ \n \n Run the  sbt.sh  script.\n   You may have to wait while SBT downloads various dependencies: \n \n ~~~\n   bash$ ./sbt.sh\n   # Lots of output here...\n   # The first run will take a while... \n [app] $\n   ~~~ \n \n Type  run  at the SBT prompt.\n   You may have to wait while SBT downlaods various dependencies: \n \n ~~~\n   [app] $ run\n   # Lots of output here...\n   # The first run will take a while... \n --- (Running the application from SBT, auto-reloading is enabled) --- \n [info] play - Listening for HTTP on /0:0:0:0:0:0:0:0:9000 \n (Server started, use Ctrl+D to stop and go back to the console...)\n   ~~~ \n \n Open  http://localhost:9000  in a web browser.\n   SBT will compile the application, which may take a while.\n   After this you should see the message  "Hello World!"  in your browser. \n \n ~~~\n   [app] $ run\n   # Lots of output here...\n   # The first run will take a while... \n --- (Running the application from SBT, auto-reloading is enabled) --- \n [info] play - Listening for HTTP on /0:0:0:0:0:0:0:0:9000 \n (Server started, use Ctrl+D to stop and go back to the console...)\n   ~~~ \n Getting Started on Windows \n You will need to have installed Git and Java (we recommend Oracle\'s Java 7 SDK).\nComplete the following steps outlined in Chapter 1 in the section entitled\n"Setting up SBT for This Book": \n \n Clone this repository to a directory on your hard drive, e.g.  C:\\essential-play-code : \n \n ~~~\n   C:> git clone https://github.com/underscoreio/essential-play-code.git ↩\n                  C:\\essential-play-code\n   ~~~ \n \n Change to the directory for the "hello world" exercise: \n \n ~~~\n   C:> cd\\essential-play-code\\chapter1-hello\n   ~~~ \n \n Run the  sbt.bat  script.\n   You may have to wait while SBT downloads various dependencies: \n \n ~~~\n   C:\\essential-play-code> sbt\n   # Lots of output here...\n   # The first run will take a while... \n [app] $\n   ~~~ \n \n Type  run  at the SBT prompt.\n   You may have to wait while SBT downlaods various dependencies: \n \n ~~~\n   [app] $ run\n   # Lots of output here...\n   # The first run will take a while... \n --- (Running the application from SBT, auto-reloading is enabled) --- \n [info] play - Listening for HTTP on /0:0:0:0:0:0:0:0:9000 \n (Server started, use Ctrl+D to stop and go back to the console...)\n   ~~~ \n \n Open  http://localhost:9000  in a web browser.\n   SBT will compile the application, which may take a while.\n   After this you should see the message  "Hello World!"  in your browser. \n \n ~~~\n   [app] $ run\n   # Lots of output here...\n   # The first run will take a while... \n --- (Running the application from SBT, auto-reloading is enabled) --- \n [info] play - Listening for HTTP on /0:0:0:0:0:0:0:0:9000 \n (Server started, use Ctrl+D to stop and go back to the console...)\n   ~~~', 'TODO \n \n Move native libs out of here :) \n Documentation as to how to set up CUDA and native libs \n Find examples that actually benefit from the GPU performance-wise (this vector mult actually runs slower on GPU than on the CPU) \n', 'Documentation Under Construction', 'Spellcheck.js \n This is a JS port of  Peter Norvig\'s spell checker , originally written in Python. My goal was to make it purely functional - you won\'t see any loops or other imperative constructs here. \n Getting Started \n \n \n This project comes with a Node.js template to serve static content. In Chrome (but not Firefox), this will be necessary since JS is accessing locally stored files, and that\'s against standard security policies. If you\'d like to use a different web server, go ahead. \n \n \n If you\'re planning to use Node, you\'ll need to install Express.js:\n     javascript   \n    $npm install express \n \n \n Start the web server\n     javascript\n    $node app.js \n \n \n The default port is set to 3000, so your application will show up on localhost:3000. \n \n \n Note that the page takes a long time to load the first time (about 30 seconds). This is because the spell checker algorithm is learning the word distributions from the attached document (big.txt). In order to prevent long initial loading times, you can persist the model associated with the reference "nWords" in spellcorrect.js, store its contents in a json file, remove the call to fetch data from big.txt and replace it with the call to fetch the json. This will skip the training step as you would have already done training ahead of time. \n \n \n Contributing \n Pull requests are most appreciated! \n License \n Licensed under the  Apache License, Version 2.0', 'Big Data Scala/Lambda Jam 2015 Talk Repo \n What\'s this? \n This repo was prepared specifically for  my Lambda Jam talk  (see the corresponding slide deck  here ). This is an example of using Scala "design patterns" such as as type classes to abstract away the machine learning execution platform (e.g. Scala collections, Apache Spark RDDs) from the algorithm itself. This will allow you to easily add in another execution engine (e.g. Apache Flink) without having to rewrite any of the machine learning algorithms with specific bindings. \n Also, I show how easy it is to simplify the optimization code by using simple functional programming features such as being able to pass functions as arguments to other functions or methods. For instance, the optimize() method may need to accept a weight updating scheme (SGD, Adagrad), which is a function. Similarly, weight initialization, cost and gradient calculation can be functions that are passed in. Other Scala features such as monads are a given in this design. \n What do the packages contain? \n The  optimization  package contains optimization algorithms (SGD, Adagrad), optimization-specific data types (Datum, VectorizedData, etc.), sampling code, etc. \n The  ml  package contains cost and gradient functions that can be provided to the optimizer in order to solve a particular machine learning problem. For now, it\'s just linear regression, but if you look at the tiny amount of code found in CostFunctions and Gradients, you\'ll realize how easy it would be to add logistic regression or any other model. \n The  plotting  package contains utilities to abstract away some of the  WISP  functionality, e.g. for doing multiple line plots on a single chart. \n The  demo  package contains an example with synthetically generated data for linear regression. We know what weights to expect from the model, and we train the model to check if it returns values close to the "ground truth" weights. The demo runs one example using stochastic gradient descent (SGD) against Scala collections, and another example using Adagrad against a Spark RDD. The results are plotted at the end. \n How to run the demo? \n Just start SBT and type "run" at the console. That\'s it! If you don\'t have SBT, install it from  here . \n Feedback is appreciated! \n If you have ideas for improving the design, adding new features, etc., please comment or issue a pull request. Thanks!', 'Word2Vec example for SF Advanced Spark Meetup (Jan. 12, 2016) \n To start sbt: \n 1) If you have sbt - simply type "sbt" \n 2) If you don\'t have sbt - type "./sbt" (assuming you\'re on a Linux/OS X/*nix OS). Note that you may need to "chmod" to enable the script to be executable, e.g. "chmod 744 ./sbt" \n Once sbt is running - type "run" \n Applications that can be executed: \n 1) To train model - select "us.marek.meetup.w2v.W2VTraining" \n 2) To generate synonyms - select "us.marek.meetup.w2v.W2VPrediction"', 'Add readme', 'Java-Python Interop \n Install Pyro\n $pip install pyro4 \n Start Pyro name server\n $pyro4-ns \nYou should get something like this \n Not starting broadcast server for localhost.\nNS running on localhost:9090 (127.0.0.1)\nWarning: HMAC key not set. Anyone can connect to this server!\nURI = PYRO:Pyro.NameServer@localhost:9090 \n Write some Python code to perform remote invocations on using Pyro. \n ```python\nimport Pyro4 \n @Pyro4.expose\nclass GreetingMaker(object):\n    def get_fortune(self, name):\n        return "Hello, {0}. Here is your fortune message:\\n" \\\n               "Tomorrow\'s lucky number is 12345678.".format(name) \n daemon = Pyro4.Daemon()                # make a Pyro daemon\nns = Pyro4.locateNS()                  # find the name server\nuri = daemon.register(GreetingMaker)   # register the greeting maker as a Pyro object\nns.register("example.greeting", uri)   # register the object with a name in the name server \n print("Ready.")\ndaemon.requestLoop()                   # start the event loop of the server to wait for calls\n``` \n Execute the above code \n python greeting_server.py \n You should get the message\n Ready. \n Write some Java code to use Pyro. Note that this project contains a Maven pom.xml in order to include Pyro dependencies and build an uber-jar with these dependencies. \n ```java\nimport net.razorvine.pyro.*; \n public class HelloPython { \n public static void main(final String[] args) throws Exception {\n\n    final NameServerProxy ns = NameServerProxy.locateNS(null);\n    final PyroProxy remoteobject = new PyroProxy(ns.lookup("example.greeting"));\n    final String result = (String) remoteobject.call("get_fortune", "Marek");\n    System.out.println(result);\n}\n \n }\n``` \n Build the Java code \n mvn clean package \n Test the Java code (after setting up Pyro, starting the name server and the Python server app). \n mvn test \n This will run the main method. You should see the following on the console \n Hello, Marek. Here is your fortune message:\nTomorrow\'s lucky number is 12345678. \n For more information about Pyro, see  here . For more information about Pyro in Java, see  here .', 'bazel_tutorial \n C++ example\n bazel build main:hello-world \nor\n bazel run main:hello-world \n C++ with Google test\n bazel test test:hello-test \n Java example\n bazel run :my-other-runner', 'Udacity_CarND_GPU_TF_Starter \n GPU-enabled starter kit (Term 1) \n This is a slight modification of the original  Starter Kit , with an emphasis on optimized NVIDIA CUDA-enabled GPU experience. \n This build includes optimized GPU code for TensorFlow for Kepler, Maxwell and Pascal architectures. This is different from the existing TF GPU Docker container, which does not include Pascal binaries and results in a JIT lag at application start time. This change and some others should enable short app start times and better runtime performance, especially on Pascal GPUs. \n The container also comes with additional features, such as text editors (vim, nano) and the ability to re-build TensorFlow (Bazel). \n Building the container \n \n Install the  nvidia-docker plugin . \n Test that nvidia-docker works: \n    nvidia-docker run --rm nvidia/cuda nvidia-smi \n Run the build:\n    nvidia-docker build -t udacity_carnd_gpu_tf_starter . \n Check that the image exists:\n    nvidia-docker images | grep udacity_carnd_gpu_tf_starter \n', 'Fast Bilinear Upsampling for PyTorch \n What is this? \n This implementation of bilinear upsampling is considerably faster than the native PyTorch one in half precision (fp16). It is also slightly faster for single precision (fp32). See the "Performance" section below. \n Requirements \n \n PyTorch 1.0.0+ \n CUDA 10.0+ \n GPU with compute capability 7.0+ (Tesla V100, Titan V, GeForce RTX 2070/2080/2080Ti, etc.) \n \n Caveats \n Currently this implementation is equivalent to PyTorch\'s bilinear upsampling with  align_corners=True . The case of  align_corners=False  hasn\'t been implemented yet. \n Installation \n $python setup.py install \n Sample execution \n The script  test.py  demonstrates use. The TL;DR is that the following calls are equivalent:\n* PyTorch: \n result = torch.nn.functional.interpolate(data, scale_factor=2, mode=\'bilinear\', align_corners=True) \n* This library: \n from bilinear_upsampling import Interpolation\ninterp = Interpolation()\nresult = interp(data, scale_factor=2) \n Performance \n Tensor dimensions: (128, 3, 224, 224) \n Test env: Titan V GPU, Intel Core i7-7800X CPU @ 3.50GHz \n fp16 \n | Direction | PyTorch  | This Implementation |\n|-----------|----------|---------------------|\n| forward   | 685 us   | 482 us              |\n| backward  | 15.11 ms | 4.17 ms             | \n fp32 \n | Direction | PyTorch  | This Implementation |\n|-----------|----------|---------------------|\n| forward   | 788 us   | 629 us              |\n| backward  | 1.92 ms  | 1.49 ms             |', 'dl_layers', 'PyProf2 - PyTorch Profiling tool \n What does this tool do? \n Analyzing the performance of deep neural networks is hard. Getting kernels out of  NVProf  or  NSight Compute  provides some generic kernel names and execution times, but not detailed information regarding the following: \n \n Which layer launched it: e.g. the association of  ComputeOffsetsKernel  with a concrete PyTorch layer or API is not obvious. \n What the tensor dimensions and precision were: without knowing the tensor dimensions and precision, it\'s impossible to reason about whether the actual (silicon) kernel time is close to maximum performance of such a kernel on the GPU. Knowing the tensor dimensions and precision, we can figure out the FLOPs and bandwidth required by a layer, and then determine how close to maximum performance the kernel is for that operation. \n Forward-backward correlation: currently it\'s very hard to determine what the forward pass step was that resulted in the particular weight and data gradients (wgrad, dgrad), which makes it difficult to determine the tensor dimensions required by these backprop steps to assess their performance. \n Did the kernel use  Tensor Cores ? \n Which line in the user\'s code resulted in launching this particular kernel (program trace)? \n \n PyProf addresses all of the issues above by: \n \n Instrumenting PyTorch operations to capture the tensor dimensions and precision using  NVTX . This information is recorded at profile capture time, e.g. using  NvProf . \n Querying the record produced by the profiler to correlate the kernel name and duration with PyTorch API/layer name, tensor dimensions, tensor precision, as well as calculating FLOPs and bandwidth for common operations. In addition, extra information from the profile is added for use by CUDA professionals, such as CUDA launch parameters (block/grid dimensions). \n \n Regarding FLOP and bandwidth implementations, these are usually quite straightforward. For example, for matrices A MxK  and B KxN , the FLOP count for a matrix multiplication is 2 * M * N * K, and bandwidth is M * K + N * K + M * N. Note that these numbers are based on the algorithm, not the actual performance of the specific kernel. For more details, see NVIDIA\'s  Deep Learning Performance Guide . \n Armed with such information, the user can determine various issues to help them tune the network. For instance, according to the  Tensor Core Performance Guide , the M, N and K dimensions that result in Tensor Core usage need to be divisible by 8. In fact, PyProf comes with a flag that lets the user obtain information regarding whether Tensor Cores were used by the kernel. Other useful information might include knowing that a particular kernel did not exploit much thread parallelism, as determined by the grid/block dimensions. Since many PyTorch kernels are open-source (or even custom written by the user, as in  CUDA Extensions ), this provides the user with information that helps root cause performance issues and prioritize optimization work. \n To install / uninstall \n bash\npip3 install . --user\npip3 uninstall pyprof2 \n How to get started? \n \n \n Add the following lines to your PyTorch network: \n python\nimport torch.cuda.profiler as profiler\nimport pyprof2\npyprof2.init() \n Run the training/inference loop with the  PyTorch\'s NVTX context manager \n with torch.autograd.profiler.emit_nvtx() . In addition,\nuse  profiler.start()  and  profiler.stop()  to pick an iteration(s)\n(say after warm-up) for which you would like to capture data.\nHere\'s an example: \n ```python\niters = 500\niter_to_capture = 100 \n Define network, loss function, optimizer etc. \n PyTorch NVTX context manager \n with torch.autograd.profiler.emit_nvtx(): \n for iter in range(iters):\n\n    if iter == iter_to_capture:\n        profiler.start()\n\n    output = net(images)\n    loss = criterion(output, labels)\n    loss.backward()\n    optimizer.step()\n\n    if iter == iter_to_capture:\n        profiler.stop()\n \n ``` \n \n \n Run NVprof to generate a SQL (NVVP) file. This file can be opened with NVVP, as usual. \n ```bash \n If you used profiler.start() and profiler.stop() \n nvprof -f -o net.sql --profile-from-start off -- python net.py \n If you did not use profiler.start() and profiler.stop() and want to profile everything \n nvprof -f -o net.sql -- python net.py\n``` \n \n \n Note:  if you\'re experiencing issues with hardware counters and you get a message such as  **_ERR_NVGPUCTRPERM The user running <tool_name/application_name> does not have permission to access NVIDIA GPU Performance Counters on the target device_** , please follow the steps described in  Hardware Counters . \n \n \n Run parser on the SQL file. The output is an ASCII file. Each line\nis a python dictionary which contains information about the kernel name,\nduration, parameters etc. This file can be used as input to other custom\nscripts as well. \n bash\npyprof2/parse/parse.py net.sql > net.dict \n \n \n Run the profiler. The input is the python dictionary created above. \n ```bash \n Columnated output of width 150 with some default columns. \n pyprof2/prof/prof.py -w 150 net.dict\n``` \n \n \n The next section describes the various command line options for  prof.py . \n Command line options for  prof.py \n prof.py  can produce a CSV output, a columnated output (similar to\n column -t  for terminal readability) and a space separated output\n(for post processing by AWK for instance).  The tool produces about 20\ncolumns of information for every GPU kernel. You can select a subset\nof columns and their order using the  -c  flag. Note that a few columns\nmight have the value  "na"  implying either its a work in progress or the\ntool was unable to extract that information. Assuming the directory is\n pyprof2/prof , here are a few examples of how to use  prof.py . \n ```bash \n Print usage and lists all available output columns. \n prof.py -h\n``` \n Here is a list of columns from calling  prof.py -h . \n idx:      Index\n    seq:      PyTorch Sequence Id\n    altseq:   PyTorch Alternate Sequence Id\n    tid:      Thread Id\n    layer:    User annotated NVTX string (can be nested)\n    trace:    Function Call Trace\n    dir:      Direction\n    sub:      Sub Sequence Id\n    mod:      Module\n    op:       Operattion\n    kernel:   Kernel Name\n    params:   Parameters\n    sil:      Silicon Time (in ns)\n    tc:       Tensor Core Usage\n    device:   GPU Device Id\n    stream:   Stream Id\n    grid:     Grid Dimensions\n    block:    Block Dimensions\n    flops:    Floating point ops (FMA = 2 FLOPs)\n    bytes:    Number of bytes in and out of DRAM \n ```bash \n Columnated output of width 150 with some default columns. \n prof.py -w 150 net.dict \n CSV output. \n prof.py --csv net.dict \n Space seperated output. \n prof.py net.dict \n Columnated output of width 130 with columns kernel,op,sil,tc,flops,bytes,device,stream,block,grid \n prof.py -w 130 -c kernel,op,sil,tc,flops,bytes,device,stream,block,grid net.dict\n``` \n Kernel              Op       Sil(ns) TC        FLOPs        Bytes Dev Str    Block          Grid\nelementwise_kernel  relu      381028 -      51380224    205520896   0   7  512,1,1    100352,1,1 \nvolta_fp16_s884cudn conv2d    160002 1    1644167168     51388416   0   7  256,1,1       784,1,1 \nelementwise_kernel  relu       96545 -      12845056     51380224   0   7  512,1,1     25088,1,1 \nvolta_fp16_s884cudn conv2d    346083 1    6576668672    128483328   0   7  256,1,1       784,2,1 \n ```bash \n CSV output with columns index,direction,kernel name,parameters,silicon time. \n prof.py --csv -c idx,dir,kernel,params,sil net.dict \n Space separated output with columns index,direction,kernel name,parameters,silicon time. \n prof.py -c idx,dir,kernel,params,sil net.dict \n Input redirection. \n prof.py < net.dict\n``` \n Example \n \n \n Run  nvprof  on the LeNet model in  examples/lenet.py . This will\noutput a SQL file called  net.sql . \n bash\nnvprof -f -o net.sql --profile-from-start off -- python examples/lenet.py \n (Optional) The SQL file can be opened with the NVIDIA Visual Profiler (NVVP)\nto view the timeline with detailed NVTX annotations inserted by PyProf2. \n bash\nnvvp net.sql \n \n \n Run the  parse.py  script on  net.sql  to extract kernel and runtime\ninformation and save it as  net.dict . \n bash\npyprof2/parse/parse.py net.sql > net.dict \n \n \n This is an intermediate file. It is a Python dictionary per line. We\nuse it for debugging but we don\'t expect you to do anything with it. \n \n \n Run  prof.py  on  net.dict  to get a CSV file. \n bash\npyprof2/prof/prof.py --csv net.dict > net.csv \n \n \n Profile-guided optimization \n If kernels that do matrix multiplication/GEMM or convolution use half\nprecision (fp16) data but do not use Tensor Cores (the TC column in the\nprofile analysis output doesn\'t show a "1"), one can follow some basic\nsteps to increase the likelihood that a Tensor Core-compatible kernel will\nbe chosen. For example, for GEMMs, M, N and K should be divisible by 8,\nand for convolutions, the number of input and output channels shuold be\ndivisible by 8. For more information, see detailed Tensor Core guides\nsuch as:\n- Blog Post:  Tips for Optimizing GPU Performance Using Tensor Cores \n- GTC Talk:  Tensor Core Deep Learning Performance Guide \n For both Tensor Core and non-Tensor Core Deep Learning performance\noptimization tips, see NVIDIA\'s  Deep Learning Performance\nGuide . \n Hardware Counters \n Profiling GPU workloads may require access to\n hardware performance counters .\nDue to a\n fix \nin recent NVIDIA drivers addressing\n CVE‑2018‑6260 ,\nthe hardware counters are disabled by default, and require elevated\nprivileges to be enabled again. If you\'re using a recent driver, you\nmay see the following message when trying to run nvprof: \n **_ERR_NVGPUCTRPERM The user running <tool_name/application_name> does not have permission to access NVIDIA GPU Performance Counters on the target device._** \n For details, see  here . \n Permanent solution \n Follow the steps  here . The current steps for Linux are:\n sudo systemctl isolate multi-user\nsudo modprobe -r nvidia_uvm nvidia_drm nvidia_modeset nvidia-vgpu-vfio nvidia\nsudo modprobe nvidia NVreg_RestrictProfilingToAdminUsers=0\nsudo systemctl isolate graphical \nThe above steps should result in a permanent change. \n Temporary solution \n When running on bare metal, you can run nvprof with  sudo . \n If you\'re running in a Docker image, you can temporarily elevate your privileges with one of the following (oldest to newest syntax): \n \nnvidia-docker run  --privileged \ndocker run --runtime nvidia  --privileged \ndocker run --gpus all  --privileged \n \n Notes \n \n While running NVprof, do not add  --analysis-metrics \nsince that will change which table NVprof writes the\nkernels to ( CUPTI_ACTIVITY_KIND_KERNEL  instead of the usual\n CUPTI_ACTIVITY_KIND_CONCURRENT_KERNEL ). Support for running with\nmetrics may be added in the future. \n \n TODOs \n \n The support for conv transpose is currently missing. \n PyProf currently works only with NvProf, but Nsight Compute support will be added in the future. \n', 'torch_custom_op - trivial saxpy example \n To test \n \n Make sure you have Docker and nvidia-docker installed. \n Start container at the root of the repo:\n docker run --rm -it --gpus all --ipc host nvcr.io/nvidia/pytorch:20.08-py3 \n When in the container:\n git clone https://github.com/mkolod/torch_custom_op.git\ncd torch_custom_op\npython setup.py install\ncd test\npython test.py \n']
shreyRobinhood,[]
deserat,["Front End Skill Assesment \n Thank you so much for your interest in a position on the Front End Development team at The Wonderful Company. We are very excited to continue our interview process with you! \n Below you will find images of a header that contains subnav items. On desktop, subnav items should drop down on hover of the parent nav item. On Mobile the sub-nav should expand and collapse on click events. \n We'd like you to build it. Sounds simple right? The layout is easy, heck you could pull up bootstrap and knock this out in under an hour. That is a valid approach but it won't get you very far on this assessment! \n The point of this skill assessment is for you to show us what you believe good development looks like. Are you into polish and visual detail? Refine the design, add some finesse. Are you a test fanatic? Show us your TDD process. Love data and multi-tier architecture? Let us see how you redux. Are you a build ninja? Let's see those muscles flexed. \n Here are some things we do want to see: How do you layout the project and what tools do you use? How does the project progress? What does the git history look like? Finally how do you handle loose requirements - what you bring to the table? Here are some things we value; performance, reproducibility, testing, accessibility, multi-channel deliverability (SEO, Social, Desktop, Mobile, etc...). You should assume the data is separate from the presentation. \n Fork this repo into your github or push it into your preferred git provider and make sure it is open. Fill this otherwise empty repo with goodness. You have 72 hours to submit your completed project. \n Please reach out if you have any questions or need further information. \n Good luck! We look forward to your response.\n. \n \n Assessment Images: \n Desktop - Default \n \n Desktop - Active Subnav \n \n Mobile - Default \n \n Mobile - Active Menu \n \n Mobile - Active Subnav \n \n \n Style Guide \n fonts \nfont-family: Arial \n mobile body \nfont-size: 14px \n mobile menu button \nfont-size: 12px \n desktop body \nfont-size: 12px \n Colors \nwhite: #ffffff \nblack: #000000 \ngold: #d1a04f \nbeige: #79766b \ndark grey background: #151515 \nextra dark grey: #202020 \ndark grey: #353535 \nmedium grey: #323232"]
soumith,["nnx: experimental 'nn' components \n The original neural network from Torch7,  nn , contains stable and widely\nused modules. 'nnx' contains more experimental, unproven modules, and\noptimizations. Modules that become stable and which are proven useful make \ntheir way into 'nn' (some already have). \n Library Documentation \n This section includes documentation for the following objects:\n *  Recurrent  : a generalized recurrent neural network container;\n *  SoftMaxTree  : a hierarchical log-softmax Module;\n *  TreeNLLCriterion  : a negative log-likelihood Criterion for the SoftMaxTree;\n *  PushTable (and PullTable)  : extracts a table element and inserts it later in the network;\n *  MultiSoftMax  : performs a softmax over the last dimension of a 2D or 3D input;\n *  SpatialReSampling  : performs bilinear resampling of a 3D or 4D input image; \n \n Recurrent \n References :\n * A.  Sutsekever Thesis Sec. 2.5 and 2.8 \n * B.  Mikolov Thesis Sec. 3.2 and 3.3 \n * C.  RNN and Backpropagation Guide \n A  composite Module  for implementing Recurrent Neural Networks (RNN), excluding the output layer.  \n The  nn.Recurrent(start, input, feedback, [transfer, rho, merge])  constructor takes 5 arguments:\n *  start  : the size of the output (excluding the batch dimension), or a Module that will be inserted between the  input  Module and  transfer  module during the first step of the propagation. When  start  is a size (a number or  torch.LongTensor ), then this  start  Module will be initialized as  nn.Add(start)  (see Ref. A).\n *  input  : a Module that processes input Tensors (or Tables). Output must be of same size as  start  (or its output in the case of a  start  Module), and same size as the output of the  feedback  Module.\n *  feedback  : a Module that feedbacks the previous output Tensor (or Tables) up to the  transfer  Module.\n *  transfer  : a non-linear Module used to process the element-wise sum of the  input  and  feedback  module outputs, or in the case of the first step, the output of the  start  Module.\n *  rho  : the maximum amount of backpropagation steps to take back in time. Limits the number of previous steps kept in memory. Due to the vanishing gradients effect, references A and B recommend  rho = 5  (or lower). Defaults to 5.\n *  merge  : a  table Module  that merges the outputs of the  input  and  feedback  Module before being forwarded through the  transfer  Module. \n An RNN is used to process a sequence of inputs. \nEach step in the sequence should be propagated by its own  forward  (and  backward ), \none  input  (and  gradOutput ) at a time. \nEach call to  forward  keeps a log of the intermediate states (the  input  and many  Module.outputs ) \nand increments the  step  attribute by 1. \nA call to  backward  doesn't result in a  gradInput . It only keeps a log of the current  gradOutput  and  scale .\nBack-Propagation Through Time (BPTT) is done when the  updateParameters  or  backwardThroughTime  method\nis called. The  step  attribute is only reset to 1 when a call to the  forget  method is made. \nIn which case, the Module is ready to process the next sequence (or batch thereof).\nNote that the longer the sequence, the more memory will be required to store all the \n output  and  gradInput  states (one for each time step).  \n To use this module with batches, we suggest using different \nsequences of the same size within a batch and calling  updateParameters  \nevery  rho  steps and  forget  at the end of the Sequence.  \n Note that calling the  evaluate  method turns off long-term memory; \nthe RNN will only remember the previous output. This allows the RNN \nto handle long sequences without allocating any additional memory. \n Example :\n```lua\nrequire 'nnx' \n batchSize = 8\nrho = 5\nhiddenSize = 10\nnIndex = 10000\n-- RNN\nr = nn.Recurrent(\n   hiddenSize, nn.LookupTable(nIndex, hiddenSize), \n   nn.Linear(hiddenSize, hiddenSize), nn.Sigmoid(), \n   rho\n) \n rnn = nn.Sequential()\nrnn:add(r)\nrnn:add(nn.Linear(hiddenSize, nIndex))\nrnn:add(nn.LogSoftMax()) \n criterion = nn.ClassNLLCriterion() \n -- dummy dataset (task is to predict next item, given previous)\nsequence = torch.randperm(nIndex) \n offsets = {}\nfor i=1,batchSize do\n   table.insert(offsets, math.ceil(math.random()*batchSize))\nend\noffsets = torch.LongTensor(offsets) \n lr = 0.1\nupdateInterval = 4\ni = 1\nwhile true do\n   -- a batch of inputs\n   local input = sequence:index(1, offsets)\n   local output = rnn:forward(input)\n   -- incement indices\n   offsets:add(1)\n   for j=1,batchSize do\n      if offsets[j] > nIndex then\n         offsets[j] = 1\n      end\n   end\n   local target = sequence:index(1, offsets)\n   local err = criterion:forward(output, target)\n   local gradOutput = criterion:backward(output, target)\n   -- the Recurrent layer is memorizing its gradOutputs (up to memSize)\n   rnn:backward(input, gradOutput) \n i = i + 1\n   -- note that updateInterval < rho\n   if i % updateInterval then\n      -- backpropagates through time (BPTT) :\n      -- 1. backward through feedback and input layers,\n      -- 2. updates parameters\n      r:updateParameters(lr)\n   end\nend\n``` \n Note that this won't work with  input  and  feedback  modules that use more than their\n output  attribute to keep track of their internal state between \ncalls to  forward  and  backward . \n \n SoftMaxTree \n A hierarchy of parameterized log-softmaxes. Used for computing the likelihood of a leaf class. \nThis Module should be used in conjunction with the  TreeNLLCriterion . \nUsing this for large vocabularies (100,000 and more) greatly accelerates training and evaluation \nof neural network language models (NNLM). \nA vocabulary hierarchy is provided via the  dp  package's\n BillionWords \n DataSource . \n The constructor takes 2 mandatory and 4 optional arguments : \n *  inputSize  : the number of units in the input embedding representation;\n *  hierarchy  : a Tensor mapping one  parent_id  to many  child_id  (a tree);\n *  rootId  : a number identifying the root node in the hierarchy. Defaults to  -1 ;\n *  accUpdate  : when the intent is to use  backwardUpdate  or  accUpdateGradParameters , set this to true to save memory. Defaults to false;\n *  static  : when true (the defualt), returns parameters with keys that don't change from batch to batch;\n *  verbose  : prints some additional information concerning the hierarchy during construction. \n The  forward  method returns an  output  Tensor of size 1D, while \n backward  returns a table  {gradInput, gradTarget} . The second \nvariable is just a Tensor of zeros , such that the  targets  can be \npropagated through  Containers  \nlike  ParallelTable . \n ```lua \n \n input = torch.randn(5,10)\ntarget = torch.IntTensor{20,24,27,10,12}\ngradOutput = torch.randn(5)\nroot_id = 29\ninput_size = 10  \nhierarchy = { \n \n [29]=torch.IntTensor{30,1,2}, [1]=torch.IntTensor{3,4,5}, \n   [2]=torch.IntTensor{6,7,8}, [3]=torch.IntTensor{9,10,11},\n   [4]=torch.IntTensor{12,13,14}, [5]=torch.IntTensor{15,16,17},\n   [6]=torch.IntTensor{18,19,20}, [7]=torch.IntTensor{21,22,23},\n   [8]=torch.IntTensor{24,25,26,27,28}\n}\nsmt = nn.SoftMaxTree(input_size, hierarchy, root_id)\nsmt:forward{input, target}\n-3.5186\n-3.8950\n-3.7433\n-3.3071\n-3.0522\n[torch.DoubleTensor of dimension 5]\nsmt:backward({input, target}, gradOutput)\n{\n  1 : DoubleTensor - size: 5x10\n  2 : IntTensor - size: 5\n} \n \n \n ``` \n \n TreeNLLCriterion \n Measures the Negative log-likelihood (NLL) for  SoftMaxTrees . \nUsed for maximizing the likelihood of SoftMaxTree outputs.\nThe SoftMaxTree Module outputs a column Tensor representing the log likelihood\nof each target in the batch. Thus SoftMaxTree requires the targets.\nSo this Criterion only computes the negative of those outputs, as \nwell as its corresponding gradients. \n \n \n PushTable (and PullTable) \n PushTable and PullTable work together. The first can be put earlier\nin a digraph of Modules such that it can communicate with a \nPullTable located later in the graph.  PushTable:forward(input)  \nfor an  input  table of Tensors to the output, excluding one, the index of which \nis specified by the  index  argument in the  PushTable(index)  constructor.\nThe Tensor identified by this  index  is communicated to one or many \nPullTables created via the  PushTable:pull(index)  factory method. \nThese can be inserted later in the digraph such that \na call to  PushTable:forward(input) , where  input  is a table or a Tensor, \nwill output a table with the previously  pushed  Tensor inserted \nat index  index . \n An example utilizing the above  SoftMaxTree  Module\nand a Linear Module demonstrates how the PushTable can be used to \nforward the  target  Tensor without any other \n Table Modules :\n```lua \n \n mlp = nn.Sequential()\nlinear = nn.Linear(50,100)\npush = nn.PushTable(2)\npull = push:pull(2)\nmlp:add(push)\nmlp:add(nn.SelectTable(1))\nmlp:add(linear)\nmlp:add(pull)\nmlp:add(smt) --smt is a SoftMaxTree instance\nmlp:forward{input, target} -- input and target are defined above\n-3.5186\n-3.8950\n-3.7433\n-3.3071\n-3.0522\n[torch.DoubleTensor of dimension 5]\nmlp:backward({input, target}, gradOutput) -- so is gradOutput\n{\n  1 : DoubleTensor - size: 5x10\n  2 : IntTensor - size: 5\n}\n The above code is equivalent to the following: lua\nmlp2 = nn.Sequential()\npara = nn.ParallelTable()\npara:add(linear)\npara:add(nn.Identity())\nmlp2:add(para)\nmlp2:add(smt)\nmlp2:forward{input, target}\n-3.5186\n-3.8950\n-3.7433\n-3.3071\n-3.0522\n[torch.DoubleTensor of dimension 5]\nmlp2:backward({input, target}, gradOutput)\n{\n  1 : DoubleTensor - size: 5x10\n  2 : IntTensor - size: 5\n}\n```\nIn some cases, this can simplify the digraph of Modules. Note that \na PushTable can be associated to many PullTables, but each PullTable \nis associated to only one PushTable. \n \n \n MultiSoftMax \n This Module takes 2D or 3D input and performs a softmax over the last dimension. \nIt uses the existing  SoftMax  \nCUDA/C code to do so such that the Module can be used on both GPU and CPU. \nThis can be useful for  keypoint detection . \n \n SpatialReSampling \n Applies a 2D re-sampling over an input image composed of\nseveral input planes (or channels, colors). The input tensor in  forward(input)  is \nexpected to be a 3D or 4D tensor of size :  [batchSize x] nInputPlane x width x height . \nThe number of output planes will be the same as the number of input\nplanes. \n The re-sampling is done using  bilinear interpolation . \nFor a simple nearest-neihbor upsampling, use  nn.SpatialUpSampling() ,\nand for a simple average-based down-sampling, use \n nn.SpatialDownSampling() . \n If the input image is a 3D tensor of size  nInputPlane x height x width ,\nthe output image size will be  nInputPlane x oheight x owidth  where\n owidth  and  oheight  are given to the constructor. \n Instead of  owidth  and  oheight , one can provide  rwidth  and  rheight , \nsuch that  owidth = iwidth*rwidth  and  oheight = iheight*rheight . \n As an example, we can run the following code on the famous Lenna image:\n lua\nrequire 'image'                                                           \nrequire 'nnx'\ninput = image.loadPNG('doc/image/Lenna.png')\nl = nn.SpatialReSampling{owidth=150,oheight=150}\noutput = l:forward(input)\nimage.save('doc/image/Lenna-150x150-bilinear.png', output) \n The input: \n   \n The re-sampled output: \n   \n Requirements \n \n Torch7 (www.torch.ch) \n \n Installation \n \n Install Torch7 (refer to its own documentation). \n clone this project into dev directory of Torch7. \n Rebuild torch, it will include new projects too. \n \n Use the library \n First run torch, and load nnx: \n sh\n$ torch   \n ``` lua \n \n require 'nnx'\n``` \n \n Once loaded, tab-completion will help you navigate through the\nlibrary (note that most function are added directly to nn): \n ``` lua \n \n nnx. + TAB\n...\nnn. + TAB\n``` \n \n In particular, it's good to verify that all modules provided pass their\ntests: \n ``` lua \n \n nnx.test_all()\nnnx.test_omp()\n``` \n", "Torch7 Library for iOS \n Torch7 provides a Matlab-like environment for state-of-the-art machine\nlearning algorithms. It is easy to use and provides a very efficient\nimplementation, thanks to an easy and fast scripting language (Lua) and a\nunderlying C implementation. \n This package has been modified (or just hacked) to fully compile\nTorch7 for iOS (iPad/iPhone) for all architectures (armv7, armv7a, arm64, i386 (simulator), x86_64 (simulator)) \n Requirements \n Torch7 needs to be installed prior to building the iOS\nversion. 'torch' needs to be available in the user's path. \n Installation \n Simply run:\n$ ./generate_ios_framework \n This will build all torch's libraries as static libs, and export them\nin a single dir: framework/. The dir is ready to be included in\nan iOS project: it includes an example class to load Torch from within\nyour Objective C project. \n Running \n In your XCode/iOS code (Objective C), simply import the class\nTorch.m/.h; include all the libs to the linker; and finally\nadd all the Lua files as resources. All you have left to do\nis to define a main.lua file to keep going...", 'Audio Library for Torch \n Audio library for Torch-7\n * Support audio I/O (Load files, save files)\n * Common audio operations (Short-time Fourier transforms, Spectrograms) \n Load the following formats into a torch Tensor\n * mp3, wav, aac, ogg, flac, avr, cdda, cvs/vms,\n * aiff, au, amr, mp2, mp4, ac3, avi, wmv,\n * mpeg, ircam and any other format supported by libsox. \n Calculate Short-time Fourier transforms with\n * window types - rectangular, hamming, hann, bartlett \n Generate spectrograms \n Dependencies \n \n libsox v14.3.2 or above \n libfftw3 \n \n Quick install on\nOSX (Homebrew):\n bash\n$ brew install sox\n$ brew install fftw \nLinux (Ubuntu):\n bash\n$ sudo apt-get install libfftw3-dev\n$ sudo apt-get install sox libsox-dev libsox-fmt-all \n Installation \n This project can be installed with  luarocks  like this: \n bash\n$ luarocks install https://raw.githubusercontent.com/soumith/lua---audio/master/audio-0.1-0.rockspec \n On Ubuntu 13.04 64-bit, I had to modify the command slightly because of new library directory structures not picked up by luarocks.\n bash\n$ sudo luarocks install https://raw.githubusercontent.com/soumith/lua---audio/master/audio-0.1-0.rockspec LIBSOX_LIBDIR=/usr/lib/x86_64-linux-gnu/ LIBFFTW3_LIBDIR=/usr/lib/x86_64-linux-gnu \n Or, if you have downloaded this repository on your machine, and\nyou are in its directory: \n bash\n$ luarocks make \n Usage \n audio.load\n```\n loads an audio file into a torch.Tensor\n usage:\n audio.load(\n     string                              -- path to file\n ) \n returns torch.Tensor of size NSamples x NChannels, sample_rate\n``` \n audio.save\n saves a tensor into an audio file. The extension of the given path is used as the saving format.\n usage:\n audio.save(\n     string                              -- path to file\n     tensor                              -- NSamples x NChannels 2D tensor\n     number                              -- sample_rate of the audio to be saved as\n ) \n audio.compress\n Compresses a tensor in-memory and returns a CharTensor. The extension of the given path is used as the saving format. This can be decompressed using the "decompress" method\n usage:\n audio.compress(__\n     tensor                              -- NSamples x NChannels 2D tensor\n     number                              -- sample_rate of the audio to be saved as\n     extension                           -- format of audio to compress in. Example: mp3, ogg, flac, sox etc.\n ) \n audio.decompress\n Decompresses a tensor in-memory and returns raw audio. The extension of the given path is used as the loading format.\n usage:\n audio.decompress(__\n     CharTensor                          -- 1D CharTensor that was returned by .compress\n     extension                           -- format of audio used to compress. Example: mp3, ogg, flac, sox etc.\n ) \n audio.stft\n calculate the stft of an audio. returns a 3D tensor, with number_of_windows x window_size/2+1 x 2(complex number with real and complex parts)\nusage:\naudio.stft(\n    torch.Tensor                        -- input single-channel audio\n    number                              -- window size\n    string                              -- window type: rect, hamming, hann, bartlett\n    number                              -- stride\n) \n audio.spectrogram\n generate the spectrogram of an audio. returns a 2D tensor, with number_of_windows x window_size/2+1, each value representing the magnitude of each frequency in dB\nusage:\naudio.spectrogram(\n    torch.Tensor                        -- input single-channel audio\n    number                              -- window size\n    string                              -- window type: rect, hamming, hann, bartlett\n    number                              -- stride\n) \n Example Usage \n Generate a spectrogram\n lua\nrequire \'audio\'\nrequire \'image\' -- to display the spectrogram\nvoice = audio.samplevoice()\nspect = audio.spectrogram(voice, 8192, \'hann\', 512)\nimage.display(spect)', 'eblearn-matlab \n eblearn matlab wrapper using libmatio (now included in eblearn trunk as matlab.h) \n Dependencies \n The wrapper uses libmatio-dev  \n URL: http://sourceforge.net/projects/matio/  \n Dont install libmatio from ubuntu repos (apt-get). They are out of date and you will get compile errors.\nDownload libmatio 1.5 from the matio website. \n If not, you can get the package and build it from the website. \n Usage \n Recompile eblearn with matio enabled \n Let eblearn know that you installed libmatio by opening  \n eblearn/tools/scripts/FindCustom.cmake \n and add the line \n \nSET(MATIO_FOUND TRUE)                                                                                                                                                                                                                       \n \n If libmatio is at a non-standard path (for example /home/sc3104/libmatio/), add the corresponding lines too \n \nSET(MATIO_INCLUDE_DIR "/home/sc3104/libmatio/include/")                                                                                                                                                                                        \nSET(MATIO_LIBRARIES_DIR "/home/sc3104/libmatio/lib/") \n \n Then recompile and install eblearn. \n Using the wrapper in your code \n In your program, add the line \n  #define __MATLAB__  \n before including eblearn headers \n Link your program with libmatio (compiler flag -lmatio) \n Check main.cpp for sample usage and CMakelists.txt for sample linking', "csvigo: a package to handle CSV files (read and write). \n Install: \n First install Torch7 (www.torch.ch) then simply install this package\nusing torch-rocks: \n luarocks install csvigo \n Use: \n The library provides 2 high-level functions: csvigo.load and csvigo.save. To get help\non these functions, simply do: \n ``` \n \n help(csvigo.save)\nhelp(csvigo.load)\n``` \n \n Loading a CSV file in 'query' mode gives you a convenient query function that\nyou can use to query subsets of your original CSV file. To get help on this query\nfunction, simply do: \n ``` \n \n query = csvigo.load{path='somefile.csv', mode='query'}\nquery('help')\n-- print some help\nall = query('all')\nsubset = query('union', {somevar=someval, someothervar={val1, val2}}) \n", '\n Torch-7 for Android \n \n Torch7 provides a Matlab-like environment for state-of-the-art machine\nlearning algorithms. It is easy to use and provides a very efficient\nimplementation, thanks to an easy and fast scripting language (Lua) and a\nunderlying C implementation. \n Modified to be compiled and used with Android \n Features \n \n Loading of lua packages from the apk directly. \n This is done by writing a custom package.loader\n  Reference: http://www.lua.org/manual/5.1/manual.html#pdf-package.loaders\n  The loader is in torchandroid.cpp as loader_android \n torchandroid.h and torchandroid.cpp give lots of helper functions to make life easier \n Print function overriden to redirect to logcat (only handles strings for now) \n Function to get apk assets as bytes (very useful) \n Full support for ffi and shared libraries \n \n torch.load  now takes three additional modes:  apkbinary32 ,  apkbinary64 ,  apkascii . One can store model files in the  assets  folder and use these modes to load them. If the model was saved on a 64-bit machine, use  apkbinary64 , if it was saved on a 32-bit machine, use  apkbinary32 . \n Requirements \n For CUDA-enabled version: NVIDIA CodeWorks for Android: https://developer.nvidia.com/codeworks-android. \n* NOTE: CodeWorks 1R5 does not have CUDA! You need to install 1R5 and then CUDA from 1R4. \n For CPU-only version : Android NDK (13b) and Android SDK  \n \n NOTE (Nov 2016): Android NDK v13b is required for NEON, even if building with CodeWorks and CUDA. \n This is due to some NDK bugs fixed in v13b - CodeWorks has 12b. NDK will only be used to build Lua JIT. \n Get it here: https://dl.google.com/android/repository/android-ndk-r13b-linux-x86_64.zip. \n Extract it under ~/NVPACK, next to 12b that comes with CodeWorks. \n Change NVPACK environvent to point to that NDK (see sample in ./.bashrc-android) \n \n Samples \n \n Three sample projects has been provided in demos/ \n demos/android-demo/jni/torchdemo.cpp is a simple use-case \n demos/android-demo/assets/main.lua is the file that is run \n demos/android-demo-cifar showcases classifying Camera inputs (or images from gallery) into one of 10 CIFAR-10 categories. \n Vinayak Ghokale from e-lab Purdue (https://github.com/e-lab) contributed a face detector demo, which showcases a fuller use-case (demos/facedetector_e-lab ). \n \n Building Torch Libraries and Java class. \n If on ubuntu, install the following packages:  sudo apt-get install libx32gcc-4.8-dev libc6-dev-i386 \nDefault is to build with CUDA - so make sure you installed NVIDIA CodeWorks for Android and its nvcc is in your PATH.\nOtherwise, set WITH_CUDA=OFF in build.sh \n \n git submodule update --init --recursive \n Optionally, open build.sh and modify ARCH (to match your device architecture) and WITH_CUDA variables. \n run build script:\n3 ./build.sh  \n \n You can use torch in your android apps. The relevant directories are\n* install/include - include directories\n* install/libs/$APP_ABI - static libs cross-compiled for your APP_ABI\n* install/share/lua - lua files \n Building Android Demo App \n \n Build Torch-Android atleast once using the steps above. \n [Optional] Connect your android phone in debugging mode,\n              to automatically install the apk. \n Change directory into demos/android-demo folder. \n Run build script.\n$ ./build.sh \n Run the app TorchDemo on your phone. \n', "fftw3-ffi \n A LuaJIT interface to FFTW3 \n Installation \n First, make sure FFTW3 is installed on your system. This package only requires the binary shared libraries (.so, .dylib, .dll).\nPlease see your package management system to install FFTW3. \nYou can also download and compile fftw3 from  FFTW3 web page \n sh\nluarocks install https://raw.github.com/soumith/fftw3-ffi/master/rocks/fftw3-scm-1.rockspec \n Usage \n lua\nlocal fftw = require 'fftw3'\n... \nFloat version\n lua\nlocal fftw = require 'fftw3'\nfftw = fftw.float\n... \n All FFTW C functions are available in the  fftw  namespace returned by require. The only difference is the naming, which is not prefixed\nby  fftw_  anymore. ", 'torch-signal \n A signal processing toolbox for Torch-7 \n \n Fourier Transforms (real & complex) (1D, 2D, 3D) \n Cosine Transforms (1D, 2D, 3D) \n Short-time Fourier Transforms \n Spectrogram \n Hilbert Transform \n Complex Cepstral Analysis, Real Cepstrums \n \n Quickstart \n Install fftw3 on your OS: \n OSX (Homebrew):\n bash\nbrew install fftw \n Ubuntu:\n bash\nsudo apt-get install libfftw3\nOR\nsudo apt-get install libfftw3-3 \n Install torch-signal:\n bash\nluarocks install https://raw.github.com/soumith/torch-signal/master/rocks/signal-scm-1.rockspec \n (add sudo for ubuntu) \n For documentation, go to:\nhttp://soumith.github.io/torch-signal/signal/ \n For examples, see tests/', "galaxyzoo \n Entry for GalaxyZoo Challenge:\nhttp://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge \n Our best score was 0.08246 placing us at 9th position, we trained the network for about 6 days, 4 models with different random seeds were trained on 2 NVidia Titan cards.\nThe model that was trained for entry is in 2_model.lua \n The winning model was Sander Dieleman's convnet appropach described here: http://benanne.github.io/2014/04/05/galaxy-zoo.html \n For install instructions, please look at INSTALL.md", 'torch-cheatsheet \n A quick page for everything Torch. \n https://github.com/torch/torch7/wiki/Cheatsheet', 'examplepackage.torch \n A hello-world for torch packages \n You can install the package by opening a terminal, changing directory into the folder and typing: \n luarocks make', "matio-ffi \n A LuaJIT interface to MATIO \n Installation \n First, make sure MATIO is installed on your system. This package only requires the binary shared libraries (.so, .dylib, .dll).\nPlease see your package management system to install MATIO. \nYou can also download and compile matio from  MATIO web page \nIf you are manually compiling matio from scratch (instead of using your package manager), please disable hdf5 support,  it seems to have a bug .  \n ```sh \n OSX \n brew install homebrew/science/libmatio \n Ubuntu <= 16.04 \n sudo apt-get install libmatio2 \n Ubuntu 18.04 \n sudo apt-get install libmatio4\nln -s /usr/lib/x86_64-linux-gnu/libmatio.so.4 /usr/lib/x86_64-linux-gnu/libmatio.so \n ``` \n sh\nluarocks install matio \n Usage \n Load a tensor from matlab array \n lua\nlocal matio = require 'matio'\n-- load a single array from file\ntensor = matio.load('test.mat', 'var_a')\n-- load multiple arrays from file\ntensors = matio.load('test.mat',{'var1','var2'})\n-- load all arrays from file\ntensors = matio.load('test.mat') \n Load structs, cell arrays and strings \n There is some basic support for more complex data structures defined by the MAT file format.\nStructs will be converted to lua tables, indexed by their field names. Cell arrays are converted to lua arrays (tables indexed by numerals). \n lua\nlocal matio = require 'matio'\n-- load a struct, which contains tensors, cell arrays and strings\nmy_complex_data = matio.load('test.mat') \n If you want to read sequences of characters as lua strings instead of Torch CharTensor, enable the following flag: \n lua\nmatio.use_lua_strings = true\n-- load data\ndata_with_strings = matio.load('test.mat') \n Save a tensor or a set of tensors to a .mat file \n lua\nlocal matio = require 'matio'\n-- save a single tensor to a .mat file\ndata = torch.rand(5,5)\nmatio.save('test1.mat',data)\n-- save a set of tensors to a .mat file\ndata1 = torch.rand(5,5)\ndata2 = torch.rand(2,3):float()\nmatio.save('test2.mat',{t1=data1,t2=data2})\n--save a mix of tensors, booleans, strings, and chars\ndata1 = torch.rand(2,3):float()\nmatio.save('test3.mat',{t1=data1,t2='hello',t3=9,t4=false})\n--save a table of string keys as a struct \nmatio.save('test4.mat',{ myStruct = {t1=9,t2=false} }) \nBy default, the matlab files are saved in MAT5 format, using ZLIB compression. To save without compression, change the following variable:\n lua\nmatio.compression = matio.ffi.COMPRESSION_NONE -- without compression\nmatio.compression = matio.ffi.COMPRESSION_ZLIB -- default compression \n Calling MATIO C functions \n All MATIO C functions are available in the  matio.ffi.  namespace returned by require. The only difference is the naming, which is not prefixed\nby  Mat_  anymore.  \n For example, look at matio.load in init.lua", "Calculus \n A torch7 package to do differentiation and integration over arbitrary Tensors, using a variety of methods. \n Some of the methods are:\n- First and Second order differentiation using finite-difference \n Soon to come:\n- Integration by Simpson's Rule\n- Integration using Monte-Carlo method \n This package was inspired by Julia's  Calculus package", 'convnet-benchmarks \n Easy benchmarking of all public open-source implementations of convnets.\nA summary is provided in the section below. \n Machine:  6-core Intel Core i7-5930K CPU @ 3.50GHz  +  NVIDIA Titan X  +  Ubuntu 14.04 x86_64 \n Imagenet Winners Benchmarking \n I pick some popular imagenet models, and I clock the time for a full forward + backward pass. I average my times over 10 runs. I ignored dropout and softmax layers. \n Notation \n Input is described as  {batch_size}x{num_filters}x{filter_width}x{filter_height} . Where  batch_size  is the number of images used in a minibatch,  num_filters  is the number of channels in an image,  filter_width  is the width of the image, and  filter_height  is the height of the image. \n One small note: \n The CuDNN benchmarks are done using Torch bindings. One can also do the same via Caffe bindings or bindings of any other library. This note is here to clarify that  Caffe (native)  and  Torch (native)  are the convolution kernels which are present as a default fallback. Some of the frameworks like TensorFlow and Chainer are benchmarked with CuDNN, but it is not explicitly mentioned, and  hence one might think that these frameworks as a whole are faster, than for example Caffe, which might not be the case . \n AlexNet (One Weird Trick paper)  - Input 128x3x224x224 \n | Library         | Class                                                                                                                | Time (ms)  | forward (ms) | backward (ms) |\n|:------------------------:|:-----------------------------------------------------------------------------------------------------------:| ----------:| ------------:| -------------:|\n| CuDNN[R4]-fp16 (Torch)     |  cudnn.SpatialConvolution      |  71    |  25      |   46      |\n| Nervana-neon-fp16    |  ConvLayer                         |      78    |  25          |    52         |\n| CuDNN[R4]-fp32 (Torch)      |  cudnn.SpatialConvolution     |      81    |  27          |   53          |\n| TensorFlow               |  conv2d                   |      81   |  26          |   55         |\n| Nervana-neon-fp32        |  ConvLayer                     |      87   |  28          |    58         |\n| fbfft   (Torch)                  |  fbnn.SpatialConvolution                    |      104   |  31          |    72         |\n| Chainer                 |   Convolution2D     |      177   |  40          |   136         |\n| cudaconvnet2*            |  ConvLayer         |      177   |  42          |   135         |\n| CuDNN[R2] *             |  cudnn.SpatialConvolution         |      231   |  70          |   161         |\n| Caffe (native)           |  ConvolutionLayer                 |      324   | 121          |   203         |\n| Torch-7 (native)         |  SpatialConvolutionMM                    |      342   | 132          |   210         |\n| CL-nn (Torch)            |  SpatialConvolutionMM              |      963   | 388          |   574         |\n| Caffe-CLGreenTea         |  ConvolutionLayer                                                         |      1442   | 210          |   1232         | \n Overfeat [fast]  - Input 128x3x231x231 \n | Library                  | Class                                                                                                                    | Time (ms)         | forward (ms)            | backward (ms)            |\n|:------------------------:|:------------------------------------------------------------------------------------------------------------------------:| -----------------:| -----------------------:| ------------------------:|\n| Nervana-neon-fp16          |  ConvLayer                                  |         176       |  58                    |   118                    |\n| Nervana-neon-fp32            |  ConvLayer                                |         211       |  69                    |   141                    |\n| CuDNN[R4]-fp16  (Torch)      |  cudnn.SpatialConvolution          |         242       |  86                    |  156             |\n| CuDNN[R4]-fp32  (Torch)      |  cudnn.SpatialConvolution              |         268       |  94                    |   174                    |\n| TensorFlow               |  conv2d                             |         279       |  90                    |   189                    |\n| fbfft  (Torch)                   |  SpatialConvolutionCuFFT                              |         342       |  114                    |   227                    |\n| Chainer                 |   Convolution2D               |         620       |  135                    |   484                    |\n| cudaconvnet2*            |  ConvLayer                   |         723       |  176                    |   547                    |\n| CuDNN[R2] *             |  cudnn.SpatialConvolution                   |         810       |  234                    |   576                    |\n| Caffe                    |  ConvolutionLayer                              |         823       |  355                    |   468                    |\n| Torch-7 (native)         |  SpatialConvolutionMM                                 |         878       |  379                    |   499                    |\n| CL-nn (Torch)            |  SpatialConvolutionMM                           |         963       |  388                    |   574                    |\n| Caffe-CLGreenTea         |  ConvolutionLayer                                                                      |      2857   | 616          |   2240         | \n OxfordNet [Model-A]  - Input 64x3x224x224 \n | Library                  | Class                                                                                                                    | Time (ms)         | forward (ms)            | backward (ms)            |\n|:------------------------:|:------------------------------------------------------------------------------------------------------------------------:| -----------------:| -----------------------:| ------------------------:|\n| Nervana-neon-fp16    |  ConvLayer                                  |    254        |  82                |   171                |\n| Nervana-neon-fp32        |  ConvLayer                                  |        320        |  103                    |   217                    |\n| CuDNN[R4]-fp16  (Torch)  |  cudnn.SpatialConvolution            |       471         |  140                    |   331                    |\n| CuDNN[R4]-fp32  (Torch)     |  cudnn.SpatialConvolution                     |       529         |  162                    |   366                    |\n| TensorFlow               |  conv2d                                |      540         |  158                    |   382                    |\n| Chainer                 |   Convolution2D                    |    885 | 251 | 632 |\n| fbfft    (Torch)                 |  SpatialConvolutionCuFFT                                         |       1092        |  355                    |   737                    |\n| cudaconvnet2*            |  ConvLayer                      |       1229        |  408                    |   821                    |\n| CuDNN[R2] *             |  cudnn.SpatialConvolution                     |       1099        |  342                    |   757                    |\n| Caffe                    |  ConvolutionLayer                              |       1068        |  323                    |   745                    |\n| Torch-7 (native)         |  SpatialConvolutionMM                                 |       1105        |  350                    |   755                    |\n| CL-nn (Torch)            |  SpatialConvolutionMM                           |       3437        |  875                    |   2562                   |\n| Caffe-CLGreenTea         |  ConvolutionLayer              |      5620   | 988          |   4632         | \n GoogleNet V1  - Input 128x3x224x224 \n | Library                  | Class                                                                                                                    | Time (ms)         | forward (ms)            | backward (ms)            |\n|:------------------------:|:------------------------------------------------------------------------------------------------------------------------:| -----------------:| -----------------------:| ------------------------:|\n| Nervana-neon-fp16    |  ConvLayer                                  |    230        |  72                 |   157                |\n| Nervana-neon-fp32        |  ConvLayer                                  |        270        |  84                     |   186                    |\n| TensorFlow               |  conv2d                                |      445         |  135                    |   310                    |\n| CuDNN[R4]-fp16   (Torch)     |  cudnn.SpatialConvolution                     |       462         |  112                    |   349                    |\n| CuDNN[R4]-fp32  (Torch)      |  cudnn.SpatialConvolution                     |       470         |  130                    |   340                    |\n| Chainer                 |   Convolution2D               |    687            |               189      |   497                       |\n| Caffe                    |  ConvolutionLayer                              |       1935        |  786                    |   1148                   |\n| CL-nn (Torch)            |  SpatialConvolutionMM                           |       7016        |  3027                   |   3988                   |\n| Caffe-CLGreenTea         |  ConvolutionLayer                                                                      |      9462   | 746          |   8716         | \n Layer-wise Benchmarking (Last Updated April 2015) \n Spatial Convolution layer (3D input 3D output, densely connected) \n forward + backprop (wrt input and weights) \n | Original Library         | Class/Function Benchmarked                                                                                               | Time (ms)         | forward (ms)            | backward (ms)            |\n|:------------------------:|:------------------------------------------------------------------------------------------------------------------------:| -----------------:| -----------------------:| ------------------------:|\n|  fbfft                 |  SpatialConvolutionCuFFT                                         |   256           |   101                 |  155                   |\n| cuda-convnet2 *          |  ConvLayer                      | 977               |  201                    | 776                      |\n| cuda-convnet            |  pylearn2.cuda_convnet    | 1077              |  312                    | 765                      |\n| CuDNN R2 *               |  cudnn.SpatialConvolution                     | 1019              |  269                    | 750                      |\n| Theano                   | CorrMM                                                                                                                   | 1225              |  407                    | 818                      |\n| Caffe                    |  ConvolutionLayer                              | 1231              |  396                    |   835                    |\n| Torch-7                  |  SpatialConvolutionMM                                 | 1265              |  418                    | 877                      |\n| DeepCL                   |  ConvolutionLayer                          |  6280             |  2648                   | 3632                     |\n|  cherry-picking **     |  best per layer                                                                                                          |  235              |   79                    |    155                   | \n This table is  NOT UPDATED For TITAN-X . These numbers below were on Titan Black and are here only for informational and legacy purposes. \n | Original Library         | Class/Function Benchmarked | Time (ms)         | forward (ms)            | backward (ms)            |\n|:------------------------:|:------------------------------------------------------------------------------------------------------------------------:| -----------------:| -----------------------:| ------------------------:|\n| Theano (experimental)  |  conv2d_fft                                 |  *1178           |   304                 |  874                   |\n| Torch-7                  |  nn.SpatialConvolutionBHWD                 | 1892              |  581                    | 1311                     |\n| ccv                      |  ccv_convnet_layer                                  | 809+bw            |  809                    |                          |\n| Theano (legacy)          |  conv2d                                   | 70774             |  3833                   | 66941                    | \n \n * indicates that the library was tested with Torch bindings of the specific kernels. \n ** indicates that the library was tested with Pylearn2 bindings. \n *** This is an experimental module which used FFT to calculate convolutions.  It uses a lot of memory according to @benanne \n **** The last row shows results obtainable when choosing the best-performing library for each layer. \n L1 - Input:  128x128  Batch-size  128 , Feature maps:     3->96 ,  Kernel Size:  11x11 ,  Stride:  1x1 \n L2 - Input:  64x64    Batch-size  128 , Feature maps:   64->128 ,  Kernel Size:    9x9 ,  Stride:  1x1 \n L3 - Input:  32x32    Batch-size  128 , Feature maps:  128->128 ,  Kernel Size:    9x9 ,  Stride:  1x1 \n L4 - Input:  16x16    Batch-size  128 , Feature maps:  128->128 ,  Kernel Size:    7x7 ,  Stride:  1x1 \n L5 - Input:  13x13    Batch-size  128 , Feature maps:  384->384 ,  Kernel Size:    3x3 ,  Stride:  1x1 \n The table is ranked according to the total time forward+backward calls for layers (L1 + L2 + L3 + L4 + L5) \n \n Breakdown \n forward \n Columns L1, L2, L3, L4, L5, Total are times in  milliseconds \n | Original Library         | Class/Function Benchmarked                                                                                                        |  L1 |   L2 |  L3 | L4 |  L5 | Total |\n|:------------------------:|:---------------------------------------------------------------------------------------------------------------------------------:| ---:| ----:| ---:| --:| ---:| -----:|\n| fbfft                    |  SpatialConvolutionCuFFT                                                    | 57 |  27 |   6 |  2 |  9 | 101 |\n| cuda-convnet2 *          |  ConvLayer                               | 36 | 113 |  40 |  4 |  8 | 201 |\n| cuda-convnet            |  pylearn2.cuda_convnet             | 38 | 183 |  68 |  7 | 16 | 312 |\n| CuDNN R2                 | cudnn.SpatialConvolution                               | 56 | 143 |  53 |  6 | 11 | 269 |\n| Theano                   | CorrMM                                                                                                                            | 91 | 143 | 121 | 24 | 28 | 407 |\n| Caffe                    |  ConvolutionLayer\\                               | 93 | 136 | 116 | 24 | 27 | 396 |\n| Torch-7                  | nn.SpatialConvolutionMM                                        | 94 | 149 | 123 | 24 | 28 | 418 |\n| DeepCL                   |  ConvolutionLayer                                   | 738| 1241 | 518| 47 |104 |2648 |\n|  cherry-picking **     |  best per layer                                                                                                                   | 36 | 27  |   6 |  2 |  8 |  79 | \n backward (gradInput + gradWeight) \n Columns L1, L2, L3, L4, L5, Total are times in  milliseconds \n | Original Library         | Class/Function Benchmarked                                                                                                        |  L1 | L2  |  L3 | L4 |  L5| Total |\n|:------------------------:|:---------------------------------------------------------------------------------------------------------------------------------:| ---:| ---:| ---:| --:| --:| -----:|\n| fbfft                    |  SpatialConvolutionCuFFT                                                    |  76 |  45 |  12 |  4 | 18 | 155   |\n| cuda-convnet2 *          |  ConvLayer                               | 103 | 467 | 162 | 15 | 29 | 776   |\n| cuda-convnet            |  pylearn2.cuda_convnet             | 136 | 433 | 147 | 15 | 34 | 765   |\n| CuDNN R2                 | cudnn.SpatialConvolution                               | 139 | 401 | 159 | 19 | 32 | 750   |\n| Theano                   | CorrMM                                                                                                                            | 179 | 405 | 174 | 29 | 31 | 818   |\n| Caffe                    |  ConvolutionLayer\\                               | 200 | 405 | 172 | 28 | 30 | 835   |\n| Torch-7                  | nn.SpatialConvolutionMM                                        | 206 | 432 | 178 | 29 | 32 | 877   |\n| DeepCL                   |  ConvolutionLayer                                   | 484 |2144 | 747 | 59 |198 |  3632 |\n|  cherry-picking **     |  best per layer                                                                                                                   |  76 |  45 |  12 | 4  | 18 | 155   |', "cuda-convnet2.torch \n Torch7 bindings for cuda-convnet2 kernels!\nKept as a separate repo because of the License, and because the codebase is not small. \n This is a Work IN PROGRESS!  \n DONT USE any modules which are not listed below \n Modules that are usable: \n ccn2.SpatialConvolution(nInputPlane, nOutputPlane, kH, [dH = 1], [padding = 0], [groups = 1], [partialSum = oH * oH])\nccn2.SpatialConvolutionLocal(nInputPlane, nOutputPlane, inputHeight, kH, [dH = 1], [padding = 0])\nccn2.SpatialMaxPooling(kW, [dW = kW])\nccn2.SpatialAvgPooling(kW, [dW = kW])\nccn2.SpatialCrossResponseNormalization(nCrossFeaturemaps, [addScale = 0.0001], [powScale = 0.75], [minDiv = 1]) \n What's left to do? \n All the modules from here: https://code.google.com/p/cuda-convnet/wiki/LayerParams \n How to do it? \n it is pretty simple, \n* Add the function signature from cudaconv3/include into ffi.lua\n* Call the function in your lua module \n For an example, look at SpatialConvolution.lua,  \n How to use them? \n Either send in an input of layout  Depth x Height x Width x Batch , or wrap around  nn.Transpose  modules \n Example\n lua\nfSize = {3, 96, 128, 128, 384}\nfeatures = nn.Sequential()\nfeatures:add(nn.Transpose({1,4},{1,3},{1,2}))\nfeatures:add(ccn2.SpatialConvolution(fSize[1], fSize[2], 9))\nfeatures:add(nn.ReLU())\nfeatures:add(ccn2.SpatialMaxPooling(2,2))\nfeatures:add(ccn2.SpatialConvolution(fSize[2], fSize[3], 5))\nfeatures:add(nn.ReLU())\nfeatures:add(ccn2.SpatialMaxPooling(2,2))\nfeatures:add(ccn2.SpatialConvolution(fSize[3], fSize[4], 4))\nfeatures:add(nn.ReLU())\nfeatures:add(ccn2.SpatialConvolution(fSize[4], fSize[5], 3))\nfeatures:add(nn.ReLU())\nfeatures:add(ccn2.SpatialMaxPooling(2,2))\nfeatures:add(nn.Transpose({4,1},{4,2},{4,3}))\nfeatures:add(nn.View(featuresOut)) \n NVMatrix to THTensor cheatsheet \n | NVMatrix            | THCudaTensor |\n| --------------------|:-------------:|\n| .getNumCols()       | .size[1]\n| .getNumRows()       | .size[0]\n| .getNumElements()   | THCudaTensor_nElement()\n| .getNumDataBytes()  | THCudaTensor_nElement() * 4\n| .getStride()        | .stride[0] \n| .isTrans()          | N/A\n| .getDevData()       | THCudaTensor_data()\n| .resize()           | THCudaTensor_resizeXd where X = dims\n| .getTextureObject() | THCudaTensor_getTextureObject\n| .isContiguous       | THCudaTensor_isContiguous\n| .isSameDims         | THCudaTensor_isSameSizeAs\n| .apply              | THCudaTensor_fill() \n \n check contiguity of all tensors, if not, make contiguous \n ignore/remove assertions (because you are doing contiguous checks anyways) \n harmonize getTextureObject. destroy all the texture objects after usage, treat them like pointers. NVMatrix does it in it's destructor, but since the object is not a member of the THCudaTensor structure, we have to destroy it manually after use. \n double-check places where strides are allowed (especially conv)\nAgg = ?, Agg.getBaseValue, Agg.output(.., ..) \n Remember that NVMatrix only supports 2D tensors! \n", 'So Chexy!', 'W.I.P.', "cudnn.torch \n Torch7 FFI bindings for NVIDIA cuDNN (R5) kernels! \n Modules are API compatible their  nn  equivalents. Fully unit-tested against  nn  implementations.\nConversion between  nn  and  cudnn  is available through  cudnn.convert  function. \n Installation \n \n Install cuDNN (version R5 EA) \n Have at least CUDA 7.0 \n Have  libcudnn.so  in your library path ($LD_LIBRARY_PATH) (Install cuDNN it from https://developer.nvidia.com/cuDNN ) \n Instead of the previous step, you can copy the library files into /usr/local/cuda/lib64/ or to the corresponding folders in CUDA directory \n \n Modules \n ```lua\n-- All inputs have to be 3D or 4D(batch-mode), except ReLU, Tanh, Sigmoid, and BatchNormalization\ncudnn.SpatialConvolution(nInputPlane, nOutputPlane, kW, kH, [dW = 1], [dH = 1], [padW = 0], [padH = 0], [groups = 1])\ncudnn.SpatialMaxPooling(kW, kH, dW, dH, padW, padH)\ncudnn.SpatialAveragePooling(kW, kH, dW, dH, padW, padH) \n -- the pointwise functions take an additional optional argument. if inplace=true then they do operations in-place without using any extra memory for themselves\ncudnn.ReLU(inplace[=false])\ncudnn.ClippedReLU(ceiling, inplace[=false])\ncudnn.Tanh(inplace[=false])\ncudnn.Sigmoid(inplace[=false]) \n -- SoftMax can be run in fast mode or accurate mode. Default is accurate mode.\ncudnn.SoftMax(fastMode [= false])          -- SoftMax across each image (just like nn.SoftMax)\ncudnn.LogSoftMax()                         -- LogSoftMax across each image (just like nn.LogSoftMax)\ncudnn.SpatialSoftMax(fastMode [= false])   -- SoftMax across feature-maps (per spatial location)\ncudnn.SpatialLogSoftMax()                  -- LogSoftMax across feature-maps (per spatial location)\ncudnn.VolumetricSoftMax(fastMode [= false])   -- SoftMax across feature-maps (per spatial location)\ncudnn.VolumetricLogSoftMax()                  -- LogSoftMax across feature-maps (per spatial location) \n cudnn.SpatialCrossEntropyCriterion()       -- A spatial version of LogSoftMax + ClassNLLCriterion in one shot\ncudnn.VolumetricCrossEntropyCriterion()       -- A volumetric version of LogSoftMax + ClassNLLCriterion in one shot \n -- Batch Normalization\ncudnn.BatchNormalization(nFeature, eps, momentum, affine) -- same arguments as https://github.com/torch/nn/blob/master/doc/simple.md#nn.BatchNormalization\ncudnn.SpatialBatchNormalization(nFeature, eps, momentum, affine)\ncudnn.VolumetricBatchNormalization(nFeature, eps, momentum, affine) \n -- Volumetric inputs (4D or 5D batched mode)\ncudnn.VolumetricConvolution(nInputPlane, nOutputPlane, kT, kW, kH, dT, dW, dH, padT, padW, padH)\ncudnn.VolumetricMaxPooling(kT, kW, kH, dT, dW, dH, padT, padW, padH)\ncudnn.VolumetricAveragePooling(kT, kW, kH, dT, dW, dH, padT, padW, padH) \n -- Recurrent Modules \n -- All inputs have to be 3D. Accepts input of seqLength x batch x inputDim, or batch x seqLength x inputDim if batchFirst set to true.\ncudnn.RNNReLU(inputDim, outputDim, numberOfLayers, [batchFirst = false])\ncudnn.RNNTanh(inputDim, outputDim, numberOfLayers, [batchFirst = false])\ncudnn.LSTM(inputDim, outputDim, numberOfLayers, [batchFirst = false])\ncudnn.GRU(inputDim, outputDim, numberOfLayers, [batchFirst = false])\ncudnn.BLSTM(inputDim, outputDim, numberOfLayers, [batchFirst = false])\n``` \n Modes \n There are two globally availabe modes useful for tuning performance:\n lua\nrequire 'cudnn'\ncudnn.benchmark = true -- uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms.\n                       -- If this is set to false, uses some in-built heuristics that might not always be fastest. \nby default  cudnn.benchmark  is set to  false .  Setting to  true  will improve performance, at the expense of using more\nmemory.  The input shape should be the same for each batch, otherwise autotune will re-run for each batch,\ncausing a huge slow-down. \n lua\ncudnn.fastest = true -- this is like the :fastest() mode for the Convolution modules,\n                     -- simply picks the fastest convolution algorithm, rather than tuning for workspace size \nby default,  cudnn.fastest  is set to  false .  You should set to  true  if memory is not an issue, and you\nwant the fastest performance \n lua\ncudnn.verbose = true -- this prints out some more verbose information useful for debugging \nby default,  cudnn.verbose  is set to  false . \n Conversion between  cudnn  and  nn \n Conversion is done by  cudnn.convert  function which takes a network and backend arguments and goes over\nnetwork modules recursively substituting equivalents. No memory copy is done, just metatables are swapped.\nIf you don't want to convert all modules you can pass a function as the third argument to  cudnn.convert .\nIt will be called at each step, with a module that is currently converted.  It is meant to exclude\nmodules i.e. if it returns  true , they will be left untouched, otherwise they will be subject to conversion. \n Note that you cannot do backward pass when using cuDNN and when your model has batch normalization layers and is in evaluate mode. \n ```lua\nnet = nn.Sequential()\nnet:add(nn.SpatialConvolution(3,96,11,11,3,3))\nnet:add(nn.ReLU())\ncudnn.convert(net, cudnn)\nprint(net) \n net = nn.Sequential()\nnet:add(nn.SpatialConvolution(3,96,11,11,3,3))\nnet:add(nn.ReLU())\ncudnn.convert(net, cudnn, function(module)\n   return torch.type(module):find('ReLU')\nend)\nprint(net)\n``` \n will result in:\n nn.Sequential {\n  [input -> (1) -> (2) -> output]\n  (1): cudnn.SpatialConvolution(3 -> 96, 11x11, 3,3)\n  (2): cudnn.ReLU\n}\nnn.Sequential {\n  [input -> (1) -> (2) -> output]\n  (1): cudnn.SpatialConvolution(3 -> 96, 11x11, 3,3)\n  (2): nn.ReLU\n} \n Older versions \n For version CuDNN R1, checkout the branch  R1 \nFor version CuDNN R2, checkout the branch  R2 \nFor version CuDNN R3, checkout the branch  R3 \nFor version CuDNN R4, checkout the branch  R4", 'imagenetloader.torch \n some old code that i wrote, might be useful to others. \n If your dataset is of the form: \n \n dogs/[image files of dogs] \n cats/[image files of cats] \n . \n . \n . \n \n then this loader loads the dataset nicely, and you have class-balanced sampling, a test iterator and other useful things. \n I also used it to load imagenet (both the 1.2million set and the 14 million full imagenet).', 'torch-ship-binaries \n A page describing how to ship torch binaries without sharing the source code of your scripts. \n 1) Create a self-contained torch install: \n bash\ngit clone https://github.com/torch/distro.git torch\ncd torch\n./install.sh \n(you can add and remove packages from install.sh, i\'ve added what I usually need all the time). \n 2) Convert your lua scripts into bytecode:\n luajit -b [script.lua] [script.out] \nMore doc here: http://luajit.org/running.html \n 3) Place the byte-code scripts in torch/myscripts \n 4) Create a shell script in the root of torch that sets the proper environment for your end-user, and passes the commandline options into your main lua script: \ntorch/run.sh\n```bash \n !/bin/bash \n currdir= dirname $0 \ncurrdir=$(cd "$currdir" && pwd)\nexport PATH=$currdir/install/bin/:$PATH\nexport LD_LIBRARY_PATH=$currdir/install/lib/:$LD_LIBRARY_PATH\nexport DYLD_LIBRARY_PATH=$currdir/install/lib/:$DYLD_LIBRARY_PATH\n$currdir/install/bin/luajit myscripts/main.out $*\n``` \n 5) Zip it and ship it, ask your thirdparty to unzip and start the program with "run.sh"\nAll the commandline options will be passed to the entry lua script, so you can have command-line options using torch.CmdLine or penlight etc.', 'Lua ZeroMQ bindings that just work irrespective of the machine or what is installed on it. \nZeroMQ is self-contained in this repository and is built and linked to the bindings statically. \n LZMQ bindings Copyright (C) 2013-2014 Alexey Melnichuk.\nØMQ is copyright (c) Copyright (c) 2007-2014 iMatix Corporation and Contributors. \nØMQ is free software licensed under the LGPL. \nØMQ and ZEROMQ are trademarks of iMatix Corporation. ', 'torch-docker \n Dockerfile to create an image for Torch7', "Downloads Cifar datasets directly from http://www.cs.toronto.edu/~kriz/cifar.html and converts them to Torch tables. \n Cifar-100 format \n Writes two files: cifar100-train.t7, cifar100-test.t7\nEach of them is a table of the form:\n lua\nth> c100 = torch.load('cifar100-train.t7')\nth> print(c100)\n{\n        labelCoarse : ByteTensor - size: 50000\n        data : ByteTensor - size: 50000x3x32x32\n        label : ByteTensor - size: 50000\n} \n Cifar-10 format \n Writes two files: cifar10-train.t7, cifar10-test.t7\nEach of them is a table of the form:\n lua\nth> c10 = torch.load('cifar10-train.t7')\nth> print(c10)\n{\n        data : ByteTensor - size: 50000x3x32x32\n        label : ByteTensor - size: 50000\n}", 'torch.js \n nodejs bindings for libTH (tensor library that powers torch). for fun! \n ``` \n download and install dependencies \n git clone https://github.com/soumith/torch.js.git\ncd torch.js && npm install \n for OSX \n export DYLD_LIBRARY_PATH=[path to libTH.dylib] \n for Linux \n export LD_LIBRARY_PATH=[path to libTH.so] \n run the test to see the magic happen \n node test.js\n```', 'NeuralNetworks \n Sequential\nConcat\nParallel \n SpatialConvolution\nSpatialMaxPooling\nReLU\nTanh\nSigmoid\nLinear\nSoftMax\nLogSoftMax \n ClassNLLCriterion\nMSECriterion', 'thnb \n iTorch notebooks. \n http://nbviewer.ipython.org/github/soumith/thnb/tree/master/', 'Training an Object Classifier in Torch-7 on multiple GPUs over  ImageNet \n In this concise example (1200 lines including a general-purpose and highly scalable data loader for images), we showcase:\n- train  AlexNet  or  Overfeat , VGG and Googlenet on ImageNet\n- showcase multiple backends: CuDNN, CuNN\n- use nn.DataParallelTable to speedup training over multiple GPUs\n- multithreaded data-loading from disk (showcases sending tensors from one thread to another without serialization) \n Requirements \n \n Install torch on a machine with CUDA GPU \n If on Mac OSX, run  brew install coreutils findutils  to get GNU versions of  wc ,  find , and  cut \n Download Imagenet-12 dataset from http://image-net.org/download-images . It has 1000 classes and 1.2 million images. \n \n Data processing \n The images dont need to be preprocessed or packaged in any database.  It is preferred to keep the dataset on an  SSD  but we have used the data loader comfortably over NFS without loss in speed.\nWe just use a simple convention: SubFolderName == ClassName.\nSo, for example: if you have classes {cat,dog}, cat images go into the folder dataset/cat and dog images go into dataset/dog \n The training images for imagenet are already in appropriate subfolders (like n07579787, n07880968).\nYou need to get the validation groundtruth and move the validation images into appropriate subfolders.\nTo do this, download ILSVRC2012_img_train.tar ILSVRC2012_img_val.tar and use the following commands:\n```bash \n extract train data \n mkdir train && mv ILSVRC2012_img_train.tar train/ && cd train\ntar -xvf ILSVRC2012_img_train.tar && rm -f ILSVRC2012_img_train.tar\nfind . -name "*.tar" | while read NAME ; do mkdir -p "${NAME%.tar}"; tar -xvf "${NAME}" -C "${NAME%.tar}"; rm -f "${NAME}"; done \n extract validation data \n cd ../ && mkdir val && mv ILSVRC2012_img_val.tar val/ && cd val && tar -xvf ILSVRC2012_img_val.tar\nwget -qO- https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh | bash\n``` \n Now you are all set! \n If your imagenet dataset is on HDD or a slow SSD, run this command to resize all the images such that the smaller dimension is 256 and the aspect ratio is intact.\nThis helps with loading the data from disk faster.\n bash\nfind . -name "*.JPEG" | xargs -I {} convert {} -resize "256^>" {} \n Running \n The training scripts come with several options which can be listed by running the script with the flag --help\n bash\nth main.lua --help \n To run the training, simply run main.lua\nBy default, the script runs 1-GPU AlexNet with the CuDNN backend and 2 data-loader threads.\n bash\nth main.lua -data [imagenet-folder with train and val folders] \n For 2-GPU model parallel AlexNet + CuDNN, you can run it this way:\n bash\nth main.lua -data [imagenet-folder with train and val folders] -nGPU 2 -backend cudnn -netType alexnet \nSimilarly, you can switch the backends to \'cunn\' to use a different set of CUDA kernels. \n You can also alternatively train OverFeat using this following command:\n```bash\nth main.lua -data [imagenet-folder with train and val folders] -netType overfeat \n multi-GPU overfeat (let\'s say 2-GPU) \n th main.lua -data [imagenet-folder with train and val folders] -netType overfeat -nGPU 2\n``` \n The training script prints the current Top-1 and Top-5 error as well as the objective loss at every mini-batch.\nWe hard-coded a learning rate schedule so that AlexNet converges to an error of 42.5% at the end of 53 epochs. \n At the end of every epoch, the model is saved to disk (as model_[xx].t7 where xx is the epoch number).\nYou can reload this model into torch at any time using torch.load\n lua\nmodel = torch.load(\'model_10.t7\') -- loading back a saved model \n Similarly, if you would like to test your model on a new image, you can use testHook from line 103 in donkey.lua to load your image, and send it through the model for predictions. For example:\n lua\ndofile(\'donkey.lua\')\nimg = testHook({loadSize}, \'test.jpg\')\nmodel = torch.load(\'model_10.t7\')\nif img:dim() == 3 then\n  img = img:view(1, img:size(1), img:size(2), img:size(3))\nend\npredictions = model:forward(img:cuda()) \n If you ever want to reuse this example, and debug your scripts, it is suggested to debug and develop in the single-threaded mode, so that stack traces are printed fully.\n lua\nth main.lua -nDonkeys 0 [...options...] \n Code Description \n \n main.lua  (~30 lines) - loads all other files, starts training. \n opts.lua  (~50 lines) - all the command-line options and description \n data.lua  (~60 lines) - contains the logic to create K threads for parallel data-loading. \n donkey.lua  (~200 lines) - contains the data-loading logic and details. It is run by each data-loader thread. random image cropping, generating 10-crops etc. are in here. \n model.lua  (~80 lines) - creates AlexNet model and criterion \n train.lua  (~190 lines) - logic for training the network. we hard-code a learning rate + weight decay schedule that produces good results. \n test.lua  (~120 lines) - logic for testing the network on validation set (including calculating top-1 and top-5 errors) \n dataset.lua  (~430 lines) - a general purpose data loader, mostly derived from  here: imagenetloader.torch . That repo has docs and more examples of using this loader. \n', "talks \n Slides of talks I've been giving, in one place.", "kaggle_retinopathy_starter.torch \n A starter kit in Torch for Kaggle Diabetic Retinopathy Detection.\nIt showcases:\n- Classification\n- Regression \n DID NOT FINISH IMPLEMENTING:\n- Metric Learning (Siamese and triplet networks)\n- Averaging model ensembles \n What else? \n \n 1-GPU or multi-GPU convolution neural networks \n multi-threaded data loading (data is loaded in compressed-form into memory and decompressed + jittered on the fly) \n test script to take your trained model (or models) and produce Kaggle-compatible CSV file ready for upload \n \n Getting started \n \n Install torch + dependencies, instructions are here:  INSTALL.md \n \n If you already have torch installed, update it and it's packages. These scripts use some recently added features. \n \n \n Run script with:\n th main.lua \n \n \n Ideas to try \n \n Maxout Networks: http://arxiv.org/abs/1302.4389 \n All Convolution Net: http://arxiv.org/abs/1412.6806 \n Separable Convolution filters \n Dropout/DropConnect \n PReLU: https://github.com/torch/nn/blob/master/doc/transfer.md#prelu \n Batch Normalization: https://github.com/torch/nn/blob/master/doc/simple.md#batchnormalization \n L2 Pooling: https://github.com/torch/nn/blob/master/SpatialLPPooling.lua \n Different data augmentation \n Polar/Log-polar images: https://github.com/torch/image#res-imagepolardst-src-interpolation-mode \n Different color spaces: https://github.com/torch/image#color-space-conversions \n All of Sander Deileman's tricks: \n http://benanne.github.io/2014/04/05/galaxy-zoo.html \n http://benanne.github.io/2015/03/17/plankton.html \n \n Broad Strategy \n \n Train several models \n Average the network predictions over many of these models \n For each image, average the prediction over several crops and orientations of the image (for example at test time, every image's prediction is the average of 4 rotations of the image) \n Win \n", 'Contains the depreceated SpatialConvolutionCUDA and SpatialMaxPoolingCUDA modules', "fakecuda \n A convenient package for the lazy torch programmer to leave all your :cuda() calls as-is when running on CPU. \n Usage: \n lua\nrequire('fakecuda').init(true) -- true is for using fakecuda, false or no-argument wont initialize fakecuda. \n Installation: \n bash\nluarocks install https://raw.githubusercontent.com/soumith/fakecuda/master/fakecuda-scm-1.rockspec \n Example: \n Let's say your code is sprinkled all over like this:\n```lua\nrequire 'torch'\nrequire 'nn' \n use_cpu = true\nrequire('fakecuda').init(use_cpu) \n a=torch.randn(10) \n ac = a:cuda() \n b = nn.SpatialConvolution(3,16,5,5)\nb:cuda() \n cutorch.synchronize()\n``` \n These are all nicely ignored with fakecuda. \n The only thing that has to be tread carefully is if you have calls like:\n lua\na=torch.CudaTensor(3,4) \n explicitly initializing a torch.CudaTensor is in murky territory where side-effects are not easy to ignore.", 'Applied Deep Learning for Computer Vision with Torch \n CVPR 2015, Boston, MA \n Slides and Notebooks \n \n Slides \n Amazon EC2 image with torch + itorch + Atari + notebooks can be launched  from this link  and the AMI ID is: ami-b36981d8 \n \n Notebooks:\n-  Deep Learning with Torch - A 60-minute blitz \n-  NNGraph - graph style neural networks \n-  Character-level Recurrent networks. An introduction to real-world nngraph RNN training . Original code by @karpathy\n-  Deep-Q Reinforcement learning to play Atari games', 'nn.js : high performance (not yet) neural networks for the Browser \n This package is in all respects inferior to  convnet.js \n This package is not meant to replace the excellent  convnet.js , \nbut provide a lower-level package. \n A fast low-level javascript package for multi-threaded neural net layers for the browser. \n For now, this package only implements the  forward  ops, and does not implement backward. \n It is useful when porting Torch models to the browser. \n For example, it powers the browser demo in:  http://soumith.ch/eyescream \n Layers: \n nn.SpatialConvolution(weight, bias, padH, padW) \nnn.SpatialMaxPooling(kH, kW, dH, dW)\nnn.ReLU()\nnn.Linear(weight, bias)\nnn.View(shape)\nnn.Sequential()\nnn.JoinTable(dim)\nnn.ParallelTable()\nnn.Identity() \n Uses: \n \n [TODO] Web workers for using multiple cores \n [TODO] SIMD optimizations if the browser supports  SIMD.js  or  WebAssembly \n \n Dependencies: \n \n ndarray \n ndarray-fill \n msgpack-js \n \n Unit tests: \n Unit tests can be run via nodejs.\n``` bash\n$ npm -g install mocha\n$ cd nnjs\n$ mocha  \n SpatialConvolution\n      ✓ Should compare against torch convolutions (126ms) \n SpatialMaxPooling\n      ✓ Should compare against torch SpatialMaxPooling \n Linear\n      ✓ Should compare against torch Linear layer \n Loader\n      ✓ Should load a full multi-layer model and compare against torch result (3051ms) \n 4 passing (3s)\n  ``` \n Building for the browser \n bash\nnpm install -g browserify\nbrowserify -r ./js/init.js:nn -r ndarray -r ndarray-fill -o static/js/nn.js', '\n WORK IN PROGRESS', 'The Eyescream Project \n Generating Natural Images using a pyramid of ConvNets \n To generate images, use: \n bash\nth generate.lua \n For training code, go here: https://github.com/facebook/eyescream', 'Install \n git clone https://github.com/soumith/badmm.torch\ncd badmm.torch\nluarocks make \n Run example / test \n th test.lua \n To debug in gdb if needed: \n gdb luajit  \nr test.lua', "Scripts to load the MS-COCO annotations into torch \n bash\nluarocks install dkjson \n Place instances_train-val2014.zip in the same folder. You can download it from http://mscoco.org/dataset/#download \n Then run the following commands: \n bash\nunzip instances_train-val2014.zip\nth load.lua    \nth load.lua ### yes, run it twice. \n Then you will have a file called  instances_train2014.tds.t7  that you can load via:\n lua\nannotations = torch.load('instances_train2014.tds.t7')\nfor k,v in pairs(annotations) do\n   print(k)\nend", 'Sunfish.lua \n \n tiny and basic chess engine for lua. \n Port of https://github.com/thomasahle/sunfish \n Pretty decent AI \n Game state readily available as a string \n inputs taken as a string \n Code is easily modifiable to plug into Torch to make a neural net play against it. \n Simply remove main function, and plug the board into your own scripts: https://github.com/soumith/sunfish.lua/blob/master/sunfish.lua#L541-L591 \n \n Usage \n ```bash\n$ lua sunfish.lua \n r  n  b  q  k  b  n  r\n   p  p  p  p  p  p  p  p\n   .  .  .  .  .  .  .  .\n   .  .  .  .  .  .  .  .\n   .  .  .  .  .  .  .  .\n   .  .  .  .  .  .  .  .\n   P  P  P  P  P  P  P  P\n   R  N  B  Q  K  B  N  R \n Your move: \ne2e4 \n r  n  b  q  k  b  n  r\n   p  p  p  p  p  p  p  p\n   .  .  .  .  .  .  .  .\n   .  .  .  .  .  .  .  .\n   .  .  .  .  P  .  .  .\n   .  .  .  .  .  .  .  .\n   P  P  P  P  .  P  P  P\n   R  N  B  Q  K  B  N  R \n My move:        g8f6 \n r  n  b  q  k  b  .  r\n   p  p  p  p  p  p  p  p\n   .  .  .  .  .  n  .  .\n   .  .  .  .  .  .  .  .\n   .  .  .  .  P  .  .  .\n   .  .  .  .  .  .  .  .\n   P  P  P  P  .  P  P  P\n   R  N  B  Q  K  B  N  R \n Your move: \n ```', 'neon.torch \n Nervana Neon kernels in Torch \n WORK IN PROGRESS', "Torch implementation of  Net2Net: Accelerating Learning via Knowledge Transfer by Chen, Goodfellow, Shlens \n \n Proof of concept with unit tests \n Handles batchnorm layers in conjunction with linear and convolutional layers \n \n ```lua\nn2n = require 'net2net' \n -- net  = network\n-- pos1 = position at which one has to widen the output\n-- pos2 = position at which the next weight layer is present\n-- newWidth   = new width of the layer\n-- batchnorm layer should be between pos1 and pos2\n-- batchnorm layer is modified to maintain identity-preserving mapping\nn2n.wider(net, pos1, pos2, newWidth) \n -- pos = position at which the layer has to be deepened\n-- nonlin = type of non-linearity to insert\n-- bnormFlag = boolean flag to insert batchnorm layer before the non-linearity\n-- inserted batchnorm layer maintains identity-preserving mapping\n-- make a forward pass through the model before calling n2n.deeper so that batch mean and variance can be computed\nn2n.deeper(net, pos, nonlin, bnormFlag)\n``` \n Example usage in test.lua", 'Inception.torch \n Newer version here: https://github.com/Moodstocks/inception-v3.torch \n ```\nth \n googlenet = dofile(\'googlenet.lua\')\nmodel = googlenet({cudnn.SpatialConvolution, cudnn.SpatialMaxPooling, cudnn.ReLU, cudnn.SpatialCrossMapLRN}) \n print(model)\n``` \n This repository contains a reference pre-trained network for the Inception\nmodel, complementing the Google publication. \n Going Deeper with Convolutions, CVPR 2015.\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\nDragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich. \n You can view "inception.ipynb" directly on GitHub, or clone the\nrepository, install dependencies listed in the notebook and play with code\nlocally. \n You may also be interested in the  Multibox \napproach that uses the Inception architecture for object detection, also\navailable on GitHub. \n Disclaimer: this is not an official Google product (experimental or otherwise).', 'DCGAN.torch: Train your own image generator \n \n Train your own network \n Train a face generator using the Celeb-A dataset \n Train Bedrooms, Bridges, Churches etc. using the LSUN dataset \n Train a generator on your own set of images. \n Train on the ImageNet dataset \n Use a pre-trained generator to generate images. \n Generate samples of 64x64 pixels \n Generate large artsy images (tried up to 4096 x 4096 pixels) \n Walk in the space of samples \n Vector Arithmetic of images in latent space \n \n Prerequisites \n \n Computer with Linux or OSX \n Torch-7 \n For training, an NVIDIA GPU is strongly recommended for speed. CPU is supported but training is very slow. \n \n Installing dependencies \n Without GPU \n \n Install Torch:  http://torch.ch/docs/getting-started.html#_ \n \n With NVIDIA GPU \n \n Install CUDA, and preferably CuDNN (optional). \n Instructions for Ubuntu are here:  INSTALL.md \n Install Torch:  http://torch.ch/docs/getting-started.html#_ \n Install optnet to reduce memory footprint for large images:  luarocks install optnet \n Optional, if you installed CuDNN, install cudnn bindings with  luarocks install cudnn \n \n Display UI \n Optionally, for displaying images during training and generation, we will use the  display package . \n \n Install it with:  luarocks install https://raw.githubusercontent.com/szym/display/master/display-scm-0.rockspec \n Then start the server with:  th -ldisplay.start \n Open this URL in your browser:  http://localhost:8000 \n \n You can see training progress in your browser window. It will look something like this:\n \n 1. Train your own network \n 1.1 Train a face generator using the Celeb-A dataset \n Preprocessing \n bash\nmkdir celebA; cd celebA \n Download img_align_celeba.zip from  http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html  under the link "Align&Cropped Images". \n bash\nunzip img_align_celeba.zip; cd ..\nDATA_ROOT=celebA th data/crop_celebA.lua \n Training \n bash\nDATA_ROOT=celebA dataset=folder th main.lua \n 1.2. Train Bedrooms, Bridges, Churches etc. using the LSUN dataset \n LSUN dataset is shipped as an LMDB database. First, install LMDB on your system. \n \n On OSX with Homebrew:   brew install lmdb \n On Ubuntu:  sudo apt-get install liblmdb-dev \n \n Then install a couple of Torch packages. \n bash\nluarocks install lmdb.torch\nluarocks install tds \n Preprocessing (with bedroom class as an example) \n Download  bedroom_train_lmdb  from the  LSUN website . \n Generate an index file:\n bash\nDATA_ROOT=[path_to_lmdb] th data/lsun_index_generator.lua \n Training \n bash\nDATA_ROOT=[path-to-lmdb] dataset=lsun th main.lua \n The code for the LSUN data loader is hardcoded for bedrooms. Change  this line  to another LSUN class to generate other classes. \n 1.3. Train a generator on your own set of images. \n Preprocessing \n \n Create a folder called  myimages . \n Inside that folder, create a folder called  images  and place all your images inside it. \n \n Training \n bash\nDATA_ROOT=myimages dataset=folder th main.lua \n 1.4. Train on the ImageNet dataset \n Preprocessing \n Follow instructions from this link. \n Training \n bash\nDATA_ROOT=[PATH_TO_IMAGENET]/train dataset=folder th main.lua \n All training options: \n lua\n   dataset = \'lsun\',       -- imagenet / lsun / folder\n   batchSize = 64,\n   loadSize = 96,\n   fineSize = 64,\n   nz = 100,               -- #  of dim for Z\n   ngf = 64,               -- #  of gen filters in first conv layer\n   ndf = 64,               -- #  of discrim filters in first conv layer\n   nThreads = 1,           -- #  of data loading threads to use\n   niter = 25,             -- #  of iter at starting learning rate\n   lr = 0.0002,            -- initial learning rate for adam\n   beta1 = 0.5,            -- momentum term of adam\n   ntrain = math.huge,     -- #  of examples per epoch. math.huge for full dataset\n   display = 1,            -- display samples while training. 0 = false\n   display_id = 10,        -- display window id.\n   gpu = 1,                -- gpu = 0 is CPU mode. gpu=X is GPU mode on GPU X\n   name = \'experiment1\',\n   noise = \'normal\',       -- uniform / normal\n   epoch_save_modulo = 1,  -- save checkpoint ever # of epoch \n 2. Use a pre-trained generator to generate images. \n The generate script can operate in CPU or GPU mode. \n to run it on the CPU, use:\n bash\ngpu=0 net=[checkpoint-path] th generate.lua \n for using a GPU, use:\n bash\ngpu=1 net=[checkpoint-path] th generate.lua \n Pre-trained network can be downloaded from here: \n \n for faces (celeb-A dataset):  celebA_25_net_G.t7 \n for bedrooms (LSUN dataset):  bedrooms_4_net_G.t7 \n \n 2.1. Generate samples of 64x64 pixels \n bash\ngpu=0 batchSize=64 net=celebA_25_net_G.t7 th generate.lua \n The batchSize parameter controls the number of images to generate. If you have  display  running,\nthe image will be shown there. The image is also saved to  generation1.png  in the same folder. \n \n 2.2. Generate large artsy images (tried up to 4096 x 4096 pixels) \n bash\ngpu=0 batchSize=1 imsize=10 noisemode=linefull net=bedrooms_4_net_G.t7 th generate.lua \n Controlling the  imsize  parameter will control the size of the output image.\nLarger the imsize, larger the output image. \n \n 2.3. Walk in the space of samples \n bash\ngpu=0 batchSize=16 noisemode=line net=bedrooms_4_net_G.t7 th generate.lua \n controlling the batchSize parameter changes how big of a step you take. \n \n Vector Arithmetic \n bash\nnet=[modelfile] gpu=0 qlua arithmetic.lua \n', 'test', "(this list is no longer maintained, and I am not sure how relevant it is in 2020) \n How to Train a GAN? Tips and tricks to make GANs work \n While research in Generative Adversarial Networks (GANs) continues to improve the\nfundamental stability of these models,\nwe use a bunch of tricks to train them and make them stable day to day. \n Here are a summary of some of the tricks. \n Here's a link to the authors of this document \n If you find a trick that is particularly useful in practice, please open a Pull Request to add it to the document.\nIf we find it to be reasonable and verified, we will merge it in. \n 1. Normalize the inputs \n \n normalize the images between -1 and 1 \n Tanh as the last layer of the generator output \n \n 2: A modified loss function \n In GAN papers, the loss function to optimize G is  min (log 1-D) , but in practice folks practically use  max log D \n  - because the first formulation has vanishing gradients early on\n  - Goodfellow et. al (2014) \n In practice, works well:\n  - Flip labels when training generator: real = fake, fake = real \n 3: Use a spherical Z \n \n Dont sample from a Uniform distribution \n \n \n \n Sample from a gaussian distribution \n \n \n \n When doing interpolations, do the interpolation via a great circle, rather than a straight line from point A to point B \n Tom White's  Sampling Generative Networks  ref code https://github.com/dribnet/plat has more details \n \n 4: BatchNorm \n \n Construct different mini-batches for real and fake, i.e. each mini-batch needs to contain only all real images or all generated images. \n when batchnorm is not an option use instance normalization (for each sample, subtract mean and divide by standard deviation). \n \n \n 5: Avoid Sparse Gradients: ReLU, MaxPool \n \n the stability of the GAN game suffers if you have sparse gradients \n LeakyReLU = good (in both G and D) \n For Downsampling, use: Average Pooling, Conv2d + stride \n For Upsampling, use: PixelShuffle, ConvTranspose2d + stride \n PixelShuffle: https://arxiv.org/abs/1609.05158 \n \n 6: Use Soft and Noisy Labels \n \n Label Smoothing, i.e. if you have two target labels: Real=1 and Fake=0, then for each incoming sample, if it is real, then replace the label with a random number between 0.7 and 1.2, and if it is a fake sample, replace it with 0.0 and 0.3 (for example). \n Salimans et. al. 2016 \n make the labels the noisy for the discriminator: occasionally flip the labels when training the discriminator \n \n 7: DCGAN / Hybrid Models \n \n Use DCGAN when you can. It works! \n if you cant use DCGANs and no model is stable, use a hybrid model :  KL + GAN or VAE + GAN \n \n 8: Use stability tricks from RL \n \n Experience Replay \n Keep a replay buffer of past generations and occassionally show them \n Keep checkpoints from the past of G and D and occassionaly swap them out for a few iterations \n All stability tricks that work for deep deterministic policy gradients \n See Pfau & Vinyals (2016) \n \n 9: Use the ADAM Optimizer \n \n optim.Adam rules! \n See Radford et. al. 2015 \n Use SGD for discriminator and ADAM for generator \n \n 10: Track failures early \n \n D loss goes to 0: failure mode \n check norms of gradients: if they are over 100 things are screwing up \n when things are working, D loss has low variance and goes down over time vs having huge variance and spiking \n if loss of generator steadily decreases, then it's fooling D with garbage (says martin) \n \n 11: Dont balance loss via statistics (unless you have a good reason to) \n \n Dont try to find a (number of G / number of D) schedule to uncollapse training \n It's hard and we've all tried it. \n If you do try it, have a principled approach to it, rather than intuition \n \n For example\n while lossD > A:\n  train D\nwhile lossG > B:\n  train G \n 12: If you have labels, use them \n \n if you have labels available, training the discriminator to also classify the samples: auxillary GANs \n \n 13: Add noise to inputs, decay over time \n \n Add some artificial noise to inputs to D (Arjovsky et. al., Huszar, 2016) \n http://www.inference.vc/instance-noise-a-trick-for-stabilising-gan-training/ \n https://openreview.net/forum?id=Hk4_qw5xe \n adding gaussian noise to every layer of generator (Zhao et. al. EBGAN) \n Improved GANs: OpenAI code also has it (commented out) \n \n 14: [notsure] Train discriminator more (sometimes) \n \n especially when you have noise \n hard to find a schedule of number of D iterations vs G iterations \n \n 15: [notsure] Batch Discrimination \n \n Mixed results \n \n 16: Discrete variables in Conditional GANs \n \n Use an Embedding layer \n Add as additional channels to images \n Keep embedding dimensionality low and upsample to match image channel size \n \n 17: Use Dropouts in G in both train and test phase \n \n Provide noise in the form of dropout (50%). \n Apply on several layers of our generator at both training and test time \n https://arxiv.org/pdf/1611.07004v1.pdf \n \n Authors \n \n Soumith Chintala \n Emily Denton \n Martin Arjovsky \n Michael Mathieu \n", 'A look at the distribution of development effort for various Deep Learning projects from Git logs \n A fork of https://gist.github.com/asmeurer/5843625 \n Disclaimer: I have no idea what this chart is supposed to tell.  \n', 'NYU-CV-Fall-2018 \n Assignment 2: Traffic sign competition \n Requirements \n \n \n Install PyTorch from http://pytorch.org \n \n \n Run the following command to install additional dependencies \n \n \n bash\npip install -r requirements.txt \n Training and validating your model \n Run the script  main.py  to train your model. \n Modify  main.py ,  model.py  and  data.py  for your assignment, with an aim to make the validation score better. \n \n By default the images are loaded and resized to 32x32 pixels and normalized to zero-mean and standard deviation of 1. See data.py for the  data_transforms . \n By default a validation set is split for you from the training set and put in  [datadir]/val_images . See data.py on how this is done. \n \n Evaluating your model on the test set \n As the model trains, model checkpoints are saved to files such as  model_x.pth  to the current working directory.\nYou can take one of the checkpoints and run: \n python evaluate.py --data [data_dir] --model [model_file] \n That generates a file  gtsrb_kaggle.csv  that you can upload to the private kaggle competition https://www.kaggle.com/c/nyu-cv-fall-2018/ to get onto the leaderboard.', 'Example to stream RGBD from one machine to another, including data-compression \n Uses lossy compression for RGB and lossless for Depth \n bash\npip install -r requirements.txt \n bash\n./name_server.sh &\npython server.py \n bash\npython client.py --ip [server_ip]']
gnardari,['Clojure Facebook Messenger Bot Example \n This is a simple example of how to get a bot running on a Facebook page using Clojure. \n Tutorial \n 1. Create a Facebook Page \n Click the link to  create a Facebook Page , choose a page type\nand click "Get Started". \n \n Click a bunch of "Skip"s. \n \n \n \n 2. Create a Facebook App and generate a page token \n Click the link to  create a Facebook App , then click\n"Skip and Create App", and fill the form to get an App ID and procede to the\ndashboard page: \n \n On the Dashboard page of your FB App, add the "Messenger" product: \n \n Click Settings, select the page and generate a page token \n \n 3. Setup environment variables \n ```shell\ncp .env_sample .env \n export PAGE_ACCESS_TOKEN=\'PASTE_YOUR_PAGE_TOKEN_HERE\'\nexport VALIDATION_TOKEN=\'TYPE_ANY_RANDOM_STRING_HERE\' \n source .env\n``` \n You can generate a random string in the command line with:\n shell\nopenssl rand -base64 36 \n \n Launch your server: \n shell\nlein ring server \n 4. Expose your machine to the world using ngrok \n If you dont have it,  download ngrok . \n On a different terminal launch ngrok to tunnel the port your bot is running to\na public HTTPS url: \n shell\nngrok http 3000 \n (your ngrok syntax may vary, this example uses version 2.0.25) \n This will give you an https URL, to be used in the next step. \n \n 5. Go back to your FB App Dashboard page and validate the webhook \n On the dashboard page of you facebook app click "Setup Webhooks": \n \n Use the https url from ngrok with the callback path and the random string\nverify token to fill the form. Check all checkboxes: \n \n 6. Say hello \n Open your page and send a message to it. If you follow all steps you should have an echo bot running. \n \n Tutorial adapted from https://github.com/fczuardi/fbbotexample', 'vision@home \n ROS + Tensorflow Object Detection API \n Installation \n Install OpenCV \n OpenCV Tutorial \n Python Tutorial \n Run tf object detection script inside your virtual env \n shell\nchmod +x install-tf-api.sh\n./install-tf-api.sh', 'treeseg', 'TODOs: \n \n Make inference more generic (command line/launch file configs) \n Add time benchmarks \n \n Dependencies \n \n Install nvidia drivers/cuda \n Install Tensorflow  pip install tensorflow or pip install tensorflow-gpu \n Install TensorRT  sudo aptitude install libnvinfer5 libnvinfer-dev \n \n Conversions \n (outputNodeName)+  means one or more names \n checkpoint to .pb \n cd conversions/scripts/\npython3 convert_to_pb.py path/to/model.chk.meta dir/of/checkpoint/ path/to/output.pb (outputNodeName)+ \n .pb to uff \n cd conversions/scripts/\npython3 convert_pb_to_uff.py path/to/model.pb path/to/output.uff (outputNodeName)+ \n .uff to .plan \n You need to do this conversion on the device that is going to use the model \n cd conversions\nmkdir build && cd build\ncmake ..\nmake -j4\n./uff_to_plan models/mymodel.uff models/mymodel.plan inputs/X 256 256 1 up23/BiasAdd 1 500000 float \n Inference \n ```\ncd inference \n fix paths and file names on src/run_plan.cu \n mkdir build && cd build\ncmake ..\nmake -j4\ncd src # inside build\n./run_plan\n``` \n Example Model \n We provide a simple model trained on  Kitti Road detection  that can be used to test the entire pipeline  here . All inputs were resized to (368x1200) for training. \n Input \n \n Output \n \n This project uses scripts from  NVIDIA trt_image_classification', "wasserstein \n C++ implementation of the Wasserstein distance (or earth mover's distance) \n Parameters \n ```\nA: Values observed in the distribution A \n AWeights: Weight for each value of A \n B: Values observed in the distribution B \n BWeights: Weight for each value of B\n``` \n Usage \n ```\nstd::vector  av = {3.4, 3.9, 7.5, 7.8};\nstd::vector  aw = {1.4, 0.9, 3.1, 7.2};\nstd::vector  bv = {4.5, 1.4};\nstd::vector  bw = {3.2, 3.5}; \n dist = wasserstein(av,aw,bv,bw);\n``` \n Examples \n \n The earth movers distance is: 0 \n \n The earth movers distance is: 0.582418 \n View testdist.cpp to see examples on how to use\n```\ng++ testdist.cpp -o test -I..\n./test \n The earth movers distance is: 0\nThe earth movers distance is: 0.582418\nThe earth movers distance is: 4.07813\nThe earth movers distance is: 0.25\nThe earth movers distance is: 5\n``` \n You can run the Python script that calls scipy's implementation for reference:\n```\npython dist.py \n The earth movers distance is: 0.0\nThe earth movers distance is: 0.5824175824175825\nThe earth movers distance is: 4.078133143804786\nThe earth movers distance is: 0.25\nThe earth movers distance is: 5.0\n```", 'Configs \n Things to setup: \n Packages \n ROS -  http://wiki.ros.org/ROS/Installation \nCUDA -  https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html \nOpenCV -  https://www.pyimagesearch.com/2018/05/28/ubuntu-18-04-how-to-install-opencv/ \n Problems \n When I had problems detecting a second display using NVIDIA drivers, this solved it:\n sudo dpkg-reconfigure gdm3', 'Urquhart Tesselations for 2D data association \n Overview \n Our method defines a set of polygons based on what we call "Urquhart Tessellations".\nIn summary, these polygons are used to define descriptors for sub-regions of a set of observed landmarks (observation).\nThis enables us to detect loops and compute landmark associations in 2D using only their position.  See the paper for more details . \n Dependencies \n The code was developed and tested using the following packages and versions\n QHull 2020.2 (Delaunay Triangulation)\nBoost 1.71.0 (Tree structure that stores the polygons)\nOpenCV 4.2.0 (Discrete fourier transform) \n You can run an example based on  example.cpp \n // add this package to my_workspace/src\ncatkin_make\nsource devel/setup.bash\nrosrun urquhart UrquhartExample PATH/TO/example/obsA.txt PATH/TO/example/obsB.txt \n ROS \n Since most of the use cases of this package will be with ROS, we use catkin to compile it. However, it should be simple to use with CMake only. \n Citing \n If you use this code, please cite \n @inproceedings{nardari2020,\ntitle={Place Recognition in Forest with Urquhart Tessellations},\nauthor={Nardari, Guilherme V. and Cohen, Avraham and Chen, Steven W.\nand Liu, Xu and Arcot, Vaibhav and Romero, Roseli A. F. and Kumar, Vijay},\nbooktitle={IEEE Robotics and Automation Letters (RA-L)},\nyear={2020}\n} \n This is an adaptation of the original Python code. We decided to reimplement it in C++ to provide a faster and more organized option for benchmarking. If you spot any bugs please create an issue or pull-request.', 'lio-sam-docker']
sameer2800,['lightoj-epic-solutions \n solution 1000 : \n Simple addition program. \n For given number of problems in each of the computers, we have to find the total number of problems \n solution 1001 : \n its just a beginners problem of dividing the given number of items into two seperate baskets which can contain maximum of 10 elements.\nso, just by using if (n > 10) we can divide them into two baskets. \n solution 1002 : \n The problem description is very straightforward. The solution is achieved\nwith a simple Dijkstra with a simple modification:  \n Instead of returning the value of the shortest path itself, the algorithm should \nreturn the maximum weight along the shortest path. \n The Dijkstra algorithm should be used only once every input case, because after doing so, \nthe output values are stored in the distance array. So it is not necessary to \ncall it for every vertex in the graph. \n solution 1006 : \n Basically in this qestion we have to optimize the code and make it efficient, We can clearly observer that the given problem has overlapping subproblems ie ( fn(n-1) + fn(n-2) + fn(n-3) + fn(n-4) + fn(n-5) + fn(n-6) ), so we can use dynamic programming for solving this.\nwe can use an array to store the already computed values and can be used for calculation of new values. \n ======= \n \n \n \n \n \n \n \n a2ee7631fcb08c189b7f22bcbeae5d82f6f53fb6 \n \n \n \n \n \n \n \n solution 1007 : \n based  on  Euler totient function.\nhere dp[i] represents number of relative primes of ith number\nfirst mark all prime numbers, now for each number \nif \'i\' is a prime number: dp[i] = i-1 (one can think)\nif its not,\n   find the smallest factor, consider it to be some k.\n    and i = k  (i/k) \n    if i/k % k  == 0\n        dp[i] = dp[i/k] k\n    else\n        dp[i] = dp[i/k]*(k-1)  \n (just think how can we got above 2 equations)\n \n solution 1020 : \n A simple problem based on number theory,if Alice takes first : \n game would be like : 1 2 3 4 5 6 7 8 9\n                     L W W L W W L W W \n if bob takes first : 1 2 3 4 5 6 7 8 9\n                     W W L W W L W W L \n W - stands for winning , L stands for loosing and number represents \nnumber of marbles present. \n solution 1053 : \n We just need to check the pythogras theroem ie a^2 + b^2 = c^2. \n solution 1053 : \n A simple problem based on Pythagorean theorem. Pythagoras\'s theorem, is a fundamental relation in Euclidean geometry among the three sides of a right triangle. It states that the square of the hypotenuse is equal to the sum of the squares of the other two sides. \n hypo ^ 2 =  side ^ 2 + side ^2 \n hypotenuse is larger than other two sides. In given 3 sides we can find maximum number and we can say that thoes are righttriangle sides or not by checking above theorm. \n solution 1069: \n A simple problem based on addition , substration , multiplication operations.  \n We\'ll be calculating whole traveling time of the lift after that adding to total operations time.  \n solution 1053 : \n A simple problem based on Pythagorean theorem. Pythagoras\'s theorem, is a fundamental relation in Euclidean geometry among the three sides of a right triangle. It states that the square of the hypotenuse is equal to the sum of the squares of the other two sides. \n hypo ^ 2 =  side ^ 2 + side ^2 \n hypotenuse is larger than other two sides. In given 3 sides we can find maximum number and we can say that thoes are righttriangle sides or not by checking above theorm. \n solution 1069: \n A simple problem based on addition , substration , multiplication operations.  \n We\'ll be calculating whole traveling time of the lift after that adding to total operations time.  \n solution 1078 : \n we need to find minimum length of a string that contains only \'X\' and that \nstring should be divisible by \'K\' and this is possible for every K such that\nk % 2 != 0 and k %5 != 0. \n we are given k and X.\nnow take temp =X , length = 1,mul = 10 \n while(temp % k != 0)\n    temp += X mul\n    length++;\n    mul  = 10 \n as value of temp can be greater than max value of int, we should think in a better way.\nnow instead of maintaining temp, maintain temp % k.\nfor example, k =111, X =3\n111 % 3 =( 100 % 3 + 11 % 3) % 3\ntake another variable namely mod = X % k \n while(mod != 0)\n    mod = (X mul % k + mod )% k\n    length++\n    mul = (mul  10) % k \n length gives us the result \n solution 1087 : \n size of each group atleast can be 3.\nrecursively take  groups of size greater than 2 (3,4,5....) for each curr\nif possible. \n dp[i] represents minimum number of groups can form starting from i to the last .\ndp[0] gives us solution. \n for(j = i+2 to n)\ndp[i] = min(dp[i],1+ dp[j]) \n check the solution for better understanding. \n solution 1107 : \n Just Check wheter the given input point satisfies the condition ((x3 > x1 && x3 < x2) && (y3 > y1 && y3 < y2)) \n solution 1113 : \n yo..use two stacks and do the operations as they are saying.   \nstl stack minimized my work. \n solution 1116: \n pretty simple problem  \n only even numbers can be written as product of an even number and other number.\nex :  6=  2  3 \nso for every even number, \nfirst take m =2, n = number/m \nif(n is not odd) then make m = m 2\nand continue the loop ,till we find n as odd number. \n solution 1119 : \n bitmask dynamic programming.\n\ngenerate all permutations of each subset of the given elements.\n\ndp[i] = write i in bit format ,find number of bits set to 1. and calculate\nsum of these respective elements.\n\nonce we reach the condition of all bits set ,  break out of the function.\n \n Solution 1117 : \n category : Set inclusive and exclusive principle.\n\ncount all the numbers which are present below \'n\' and divisible by given\n numbers.for doing this ,use set union formula.\n\ni used a recursive function which finds all subsets of given array , and \nin each subset count number of bits set and if it is odd ,add to our solution\n else substract it.\n\nfinally ans contains all numbers counted in this way and our final answer is n - ans.\n \n Solution 1123 : \n category : minimum spanning tree\n\nA good problem on this topic. I used prim\'s algorithm for constructing \nthe tree. if graph is not connected ,print -1 . Run a prims algorithm ,\nonce we can travel every node to every other node.\n\nAfter this, for each edge , construct the tree again on the previous built \n tree which contains n-1 edges exactly. instead of building from scatch again.\n In this way, compexity can be well reduced.\n \n Solution 1124 : \n category : Set inclusion and exclusion \n x1 + x2 +x3 + ....xr = n \n here each ai <=  xi <= bi . Distributing n identical objects among r groups with limits on each group. Every group must have a1 objects ,so fill all the groups with corresponding ai. Now,\n  we are left with  n - (a1 + a2 + a3+ ...ar). \n  So, our new update is 0 <= xi <= bi-ai. \n  Distribute these objects among R groups with formula \n  C(N1 + r -1,r-1 )  where N1 is new n.\n  So,from this we need to remove all the cases where each xi > bi-ai .\n  For doing this,generate all 2 ^r subsets . \n Solution 1125 : \n category : DP\n\ndp[i][j][k] represents "from i elements take j elements and sum of these elements modulo \'b\' is k and also ith element is last included".\n\nfor(temp is 0 to i-1)\nif(k > a[i])\n    dp[i][j][k] +=  dp[temp][j-1][k-a[i]] ; \nelse\n    dp[i][j][k]  += dp[temp][j-1][k-a[i] + b];\n\nto minimize complexity ,do some precomputations.\n \n Solution 1127 : \n category : bisection and binary search\n\nbasically the idea is we need to get the sums  of all the subsets of the array and count no of sums which are lesser than the given weight. \nmax size of array is 30 . to generate all subsets, complexity will be O(2^ 30) which cross limits.\n\nto minimize this, find sums of all subsets of first half numbers .\n(complexity is O(2^15)) .\nstore all these values inside a vector and sort them.\nnow, generate sums of second half numbers which is again O(2 ^ 15). \nthen ,for each subset sum of second generated numbers , find number of all elements \nin the first set such that total sum weight <= given weight .\nuse vector upper bound for this .\n\ntotal complexity : O((2 ^ n/2 )*log n )\n \n Solution 1129 : \n problem is to check presence of any string that is prefix to\nany other string of the given input.\nthe idea is to use a trie tree . whenever we insert a new \nstring in the trie , check whether is it a prefix ,or  this \ncontains  any other prefixes.\nTake a variable in each node which marks only leaf nodes.\nnow , start thinking from here !!!!!\n \n Solution 1131 : \n problem is to find f(n) and g(n) .\nif n < 10^6 , we could have simply solved by using DP. but n value ranges upto 2 ^31 \nwhich is around 10 ^10 , we used technique called matrix exponentation.\n\n    fn =     |       |   fn-1\n    fn-1 =   |       |   fn-2 \n    fn-2 =   |       |   fn-3       \n    gn =     |   M   | * gn-1\n    gn-1=    |       |   gn-2\n    gn-2 =   |       |   gn-3\n\n    M is the coefficient Matrix here. \n    To find fn in terms of known variables, expand rhs again and again.\n    and finally ,we obtain solution\n     fn =    |       |^(n-2)    f2\n    fn-1 =   |       |          f1 \n    fn-2 =   |       |          f0      \n    gn =     |   M   |      *   g2\n    gn-1=    |       |          g1\n    gn-2 =   |       |          g0\n\n    Matrix power can be done in log(n) time.\n    So complexity will be ((M.rows)^3 )*log(n).\n \n solution 1133 : \n we just need to use if else or switch case according to the given queries and compute the values. \n Solution 1134 : \n count no of substrings in an array which sum is a multiple of M.\ncreate another array where \n    arr[i] = (arr[i-1] + input[i] ) % m;\nwhich is basically adding the sum upto it modulo m. then count number of \npairs of equal numbers. bcoz we recieve the same number again in that array\nonly when the sum of in between elements are divisible by m.\ncomplexity is O(N).\n \n Solution 1135 : \n segment trees with lazy propagation . first u need to learn the implementation\nof lazy propagation with segment trees.this one is a good example of that.\n \n Solution 1137 : \n this is an interesting one. there are 2 variables to be find out to get the\nsolution .\n    r*theta = length of segment (which is known)\n\ntheta varies from 0 to PI/2 . So do binary search on theta and find out radius.\n\nchord length is given and compare this length with our new chord every time.\nnew chord = r*sin(theta)\n\nonce we find out theta, ans will be r -r*cos(theta).\n \n Solution 1138  : \n binary search. Just think in that way , u will get the solution.\nOnly see my solution,if u are unable to solve it.\n \n Solution 1141 : \n typical bfs solution.\n \n Solution 1142 : \n f(2x) =   A + A^2 + A^3+....+ A^x + ...+ A^2x\n      =>  A+ A^2+ A^3+ ....+ A^X + A^x(A+ A^2+.....A^x)     \n      => (A+ A^2+....A^x)(I + A^x)\nf(2x) = f(x)(I+A^(x/2));  \nf(2x+1) => f(2x) + A^x;\n \n Solution 1145 : \n     dynamic programming .\n    n dices,each range from 1 to k , sum should be S.\n    dp[n][S] => number of ways of arranging n dices which give sum S.so,for the\n    last dice possible values are 1 to k .\n    suppose , if nth dice has value 1,\n    dp[n][S] = dp[n-1][s-1] ,which is arranging n-1 dices with sum s-1.\n    similarly , if nth dics got value 2,    \n    dp[n][S] = dp[n-1][s-2] ,which is arranging n-1 dices with sum s-2.\n    .\n    .\n    .upto k for nth dice.\n\n    so, dp[n][S] = dp[n-1][s-1] + dp[n-1][s-2] + dp[n-1][s-3]+....+dp[n-1][s-k]\n\n    we have to calculate this sum in O(1) time ,so we store this sum in temp array.and for calculating nth index , we require only n-1th index values. so, we \n    are not required to store all the n-2 index values.\n\n    time complexity : O(N*S)\n    memory complexity : O(2*S)\n \n Solution 1148: \n Goal is to minimize the total population. \ncount all the similar numbers in an array .suppose N= 3, and \ntheir values are 2 2 2 , we could say that there are only 3 people\nall belongs to same team. and suppose when N = 5, and values are\n2 2 2 2 2 ,ans is 3+3 because 2 2 2 come under 1st set and the other\n 2 2 into another set.\n\n similarly count all values from 0 to 10^6.\n complexity : O(10^6).\n \n Solution 1149: \n find the  maximum bipartite matching of the graph after connecting\nto source and destination . i used ford fulkerson algorithm for implem\nenting the above said .\n \n Solution 1157: \n find the lcs array in following way :\nlcs[i][j] = longest common subsequence of two strings in which a starts\nwith \'i\' and b starts with \'j\'.\n\nlcs[0][0] gives the solution .\n\ndp[i][j] = number of distinct lcs , a starts with i and b with j.\n\nfind the first occurrence of all alphabets in both strings for each postion . // precompute them .\n\ndp[i][j] : \nall distinct subsequences having lcs[i][j] .\ncount all\n=> subsequences starts with \'a\'\n=> subsequences starts with \'b\'\n.....\n\nfill everything recursively .\ndp[0][0] gives us the solution.\n \n Solution 1158 : \n category : bitmasking with dp.\n \n Solution 1159 : \n dynamic programming .\n\nlcs[i][j][k] = lcs[i-1][j-1][k-1] + 1  , when a[i] == b[j] == c[k]\nelse\nlcs[i][j][k] = max(lcs[i-1][j][k],lcs[i][j-1][k],lcs[i][j][k-1]);\n \n Solution 1161 : \n inclusion , exclusion .\n\nmax numbers : 10000 \nwe need to find all the 4-tuples whose gcd is 1 .\nidea is to count 4tuples having gcd 2, gcd 3, gcd 4, .....\n\nso , gcd (1) = nC4 - {gcd2 + gcd 3 + gcd 4 + gcd 5 ......}\n \n Solution 1163 : \n  hint : pattern search by generating \'a-b\' for every a starts from\n 1 .one can easily notice a pattern.\n \n Solution 1164 : \n lazy propagation with segment tree\n \n Solution 1166 : \n greedy , dfs\n\nfor every misplaced number ,write dfs to find where the number is \nappearing and count this values.\n \n Solution 1168 : \n category : scc \nalgorithm : tarzan\'s algorithm .\n\nfind all the scc in the graph by using tarzans algorithm then for\nevery scc find number of outgoing edges other than internal nodes.\nif any scc contains 2 or more outgoing edges we cannot have a single\npath .\n\ncomplexity : tarzan algo is mainly dfs and which is O(V+E).\n \n Solution 1169 : \n category : dp\n\npretty simple when you are strong with dp.\n \n solution 1170 : \n catalan numbers,dynamic programming, upperbound ,lowerbound\n\ngenerate all perfect numbers below 1e10. start from 2 to 1e5 .easily \ndone in 1 sec.\n\nnow for each query,find out number of perfect numbers by using upper\nand lowerbounds.\n\ndp[i] =  number of bsts of size i .\n\ndp[i] = dp[j] * dp[j-i-1] for 1 < j< i. which is O(n^2) .\ncatalan numbers does the same , // google catalan numbers.\ndp[i] = C(i) = 1/2n+1 C(2n,n). which work in O(N).\n \n Solution 1173 : \n dynamic programming .\nalthough my solution dint get accepted , i m sure about my\napproach  :p. \nread this  :  http://lbv-pc.blogspot.in/2012/09/the-vindictive-coach.html\n \n Solution 1174 : \n category : floyd warshall / dfs-bfs /prim\n \n Solution 1175 : \n category : bfs\n\nrun the bfs twice, once from jane to every other location .\ninsert all the fire into the queue at once and then run the bfs again.\n \n Solution 1176 : \n m guys ,6*n tshirts.\nfirst m nodes are for m guys then next 6*n nodes  are for tshirts .\nsuppose 1st guy can wear XL ,M  ,there are n XL nodes and n M nodes.\nso join the first guy node with all these 2*n nodes .similarly\nbuild the graph for all the guys .\ntake a source and destination , connect source with m guys and\nconnect all 6*n nodes (tshirts) with destination and run bipartite\nmatching .\n \n solution 1179 : \n joshepus problem , \nwikepedia has good explanation .search !!!\n \n Solution 1182 : \n counted number of bits set by using __builtin_pop_count() function.\n \n Solution 1183 : \n segment trees, lazy propagation .we have done a lot . :p\n \n solution 1184 : \n maximum bipartite matching ,\nconstruct the graph by using the given conditions.then run mbm.\n \n Solution  1186 : \n category : nim\'s game .\n\nwe know the general solution for situation like  :\nsuppose there are n piles and 2 players are playing , each can remove any number\nof stones atleast 1 from only one pile at his chance , the last player who is\nunable to remove looses . the solution for this is :\n\nif xor of all piles are zero at any players chance, he looses .\n\nsimilarily ,we can relate our problem to piles problem.in each step a player can move to any position until his oppostion\'s place. we can assume this as our size of the pile .  and apply xor similarly.\n \n Solution 1187 : \n category : segment trees.\nsuppose given array , 0 0 1 3 0 5 . now the last guy says there are\nexactly 5 guys taller than him . so he must be 6 -5 = 1 .\ntake an another array 1 2 3 4 5 6 ,and remove 1 from this . \nnew array : 2 3 4 5 6\nnext ,the second last guy says there is no one greater than him ,so\nhe must be 6 , remove this from array .\nnew array : 2 3 4 5\nthen the next guy says there are 3 guys taller , so he must be 2.\nthen delete 2  , new array : 3 4 5 ...\nsimilarily ..continue further ...\n\ni used segment tree to construct the array and to delete each element\none by one.\n \n Solution 1189 : \n As the time limit is too low 0.5 sec,if we do bitmasking , it will \nbe timeout .\nif we observe carefully ,\nfact[n] > fact[n-1]  + fact[n-2] + ..... fact[1] + fact[0].\nwhich means , if , k > fact[n] , and if k is a valid solution , then\nk must contain fact[n] .\nso just we need a single recursive call instead of double call inside \nthe recursive function.\n \n Solution 1191 : \n category : dynamic programming .\n\nbuild the dp table yourself to get the recurrence.\ndp[n][k][m] = dp[n-1][k-1][m] + dp[n-2][k-1][m] +... dp[n-m][k-1][m]\n\nhere m is not varying, so we can safely remove third dimension m.\nand dp[n-1][k][m] = dp[n-2][k-1][m] + dp[n-3][k-1][m] +....\n\nso we can reformulize our equation ,\ndp[n][k] = dp[n-1][k] - dp[n-1-m][k-1] + dp[n-1][k-1] .\n \n Solution 1192 : \n category : nim game\nthis is almost similar to problem no : 1186.\n \n Solution 1193 : \n category : dynamic programming\n\ndp[i][j] = 1*dp[i-1][j-1] + 2*dp[i-1][j-2] ..... k*dp[i-1][j-k]\n \n dp[i-1][j] =                  1 dp[i-1][j-2] ......k-1 dp[i-1][j-k]+ \n                                 k *dp[i-1][j-k-1]; \n dp[i][j] = dp[i-1][j] + (dp[i-1][j-1] +   dp[i-1][j-2] ...)\n\nsee the solution for better understanding.\n \n Solution 1198 : \n greedy.\n \n Solution 1199 : \n sprague grundy numbers.\n\nfind grundy number for all the heap sizes.\n\ngrundy value of non movable states are 0.\ngenerally, g[0] = 0 .\n\ng[i] =  minimum number which is not present in the set of all the\ngrundy numbers where \'i\' can move into.\n\nanswer will be g[n1] ^ g[n2] ^ g[n3] ....\n \n Solution 1200 : \n category : Dynamic Programming\n\nfor a weight w, u can keep any object inside the sack whose weight\nis lesser than w.\nthe profit obtained is p[i] for ith object.\n\nfunc(weight) = maximum profit obtained when the sack size is w.\n\nprogrammatically,  \nfunc(weight) =   MAX  p[i] + func(weight- weig[i]);\n\nmemoize steps to avoid recalculations.\n \n Solution 1201 : \n category : dynamic programming .\n\nwhen i saw the problem first, i thought this is a simple coloring\nproblem . i coded in that manner , i got wrong answer . I opened\nthe forum and read some posts and realized this is not that simple as\njust color the nodes and count .\n\nI solved this by using DP and we can also solve through bipartite\nmatching.\n\nthe key idea is => if a node present in the solution , all of its\nneighbours shouldn\'t , and if a node is not present , all of its neighb\nours may or may present in the solution .\n\nfind out recurrence for the above idea and memoize it .that simple.\n \n Solution 1202 : \n category : greedy\n\nthere can be only 3 solutions possible ,\n1,2,impossible .\n \n Solution 1205 : \n category : dynamic programming\n\ninspired from this stackoverflow answer :p\nhttp://stackoverflow.com/questions/22394257/how-to-count-integers-between-large-a-and-b-with-a-certain-property/22394258#22394258\n\nthe solution in the above link , he  counts only number of palindromic\nintegers which are of same size of given Y and <= to Y .\n\nSo , I used almost same function and precomputed count of  palindromic\nintegers for all the sizes lesser than 22.\n\nand the answer is F(b) - F(a-1).\n\nnote : be careful with corner cases.\n \n <<<<<<< HEAD \n Solution 1210: \n category : scc\n \n ======= \n Solution 1207 : \n category : segment trees/lazy\n\nfor all the given posters, update the range of the poster with a new color .\n\nafter all posters done , query the color of each position of the wall ,\nand count the unique colors which is our answer.\n \n \n \n \n \n \n \n \n 1fd8274f824873a810f9e5dd5f9e7c966c9b9c61 \n \n \n \n \n \n \n \n Solution 1210 : \n category : scc\n \n Solution 1212 : \n category : data structures\n\nuse stl deque :p\n \n Solution 1213 : \n category : number theory\n\nwe could easily derive  the general formula if we write on a paper for each \nk = 1,2,3,4 (number of loops).\nspoiler ahead.\n\nf(n) = k * sum * n ^(k-1).\n \n Solution 1214 : \n category : division\n\nas the number is too large, take it as a string.\n \n Solution 1215 : \n category : number theory\n\nbrute force for all the factors of l and find the minimum factor which satisfies lcm of (a,b,factor) == l .\n \n Solution 1216 : \n category : basic geometry\n\nvol of truncated cone = 1/3 (pi*h*(r^2 + r2^2+ r*r2));\n\nthe only thing we have to do is  find out the \'r\' by using triangle equality.\n \n Solution 1217 : \n category : dp\n\naccording to the conditions given in the qsn , either an element can be taken\nor not. suppose , an element is taken , all the neighbours of it cannot be \ntaken . so with this , we can easily find out a recursive solution .\n\ndp(curr, taken , first) => curr is position , taken says whether it is present in the final answer, first is true if the first element is taken in the answer .\n\nif(taken == 0)\n    we can  either take the next element or not .so, 2 conditions ,\n     max ( dp(curr+1,1,first ) , dp(curr+1,0,first)\n\nif(taken == 1)\n    then we cannot take the next element.\n    dp(curr+1,0,first)\n\nAttribute \'first\' is used when we reach the last element .\n\nAfter this, use dp array for memoization  and return the answer if it is \nalready visited.\n \n Solution 1219: \n category : dfs and greedy\n \n Solution 1220: \n category : number theory\n\ncalculate all the non perfect numbers and insert them in an array. And for\neach non perfect number , raise the power until the value lesser than 2^32\nand store in map. similarly , go with negative numbers.\n \n Solution 1221 : \n category : bellman ford, floyd warshall\n\nRead the forum , it has very good explanation :D\n\nWe can also find negative cycles by using floyd warshall. Run the floyd warshall\nand then \nif edge[i][i] < 0\n    negative cycle exists.\n \n Solution 1224 : \n category : Trie tree\n\nWhen you insert the string into the trie tree , increment the counter of the node.And also store the height of each node .\n\nthen traverse the node  in a dfs manner, find the maximum value of heigh*countr.\n \n Solution 1225 : \n category : greedy\n \n Solution 1227 : \n category : greedy\n \n Solution 1229 : \n category : sprague grundy thereom\n\nexplanation : http://lbv-pc.blogspot.in/2012/07/treblecross.html\n \n Solution 1230 : \n category : dynamic programming\n\nThis problem is interesting as it is dp on a tree rather than normal array\ndps. lets go to explanation .\n\ndp[node][1] = number of nodes to be lightened in the subtree of \'node\' when \n            there is a lamppost in current node .\n\ndp[node][0] = this is same as above but there is no lampost in current node.\n\nNow, the value at dp[node][0] will be , as this node does not contain lampposts\nall its neighbours must contain lamps. which is\ndp[node][0]  +=   dp[neighbour][1].\n\nbut when we keep a lamp in current node , we may or may not be required to keep\nlamps in all of its neighbours. So , in this case we consider min of conditions.\nwhich is\n\ndp[node][1]  = min(dp[neighbour][0] , dp[neighbour][1] )\n\nAlong with this answer , we also need number of roads that recieve light from \nboth nodes . this is readers assignment. :p\n \n Solution 1231 : \n category : dynamic programming\n\nbasic level.\n \n Solution 1232 : \n category : dynamic programming\n\nAs time limits are very strict , we cannot write a recursive solution . \nInstead there is a iterative dp with time complexity O(N*k) and\nspace O(k)\n \n Solution 1233 : \n this one is a good dp problem.I  Built the solution from bottom up manner.\nthere are M places and count how many places can we obtain with the given\ncoins. \nwhat we do is start iterating the array with each coin. make the array \nvalue 1,if this value is obtainable. i.e dp[i] =1.\nAnd at each step ,count the number of same coins used to get this value in\ncount[i].\n    dp[0] = 1 // coz zero means no coins in hand ,which is possible\n    for coin j,\n\n        dp[i] = 1 , only when dp[i-j] = 1 and count[i-j]+1 < available_coins[j]\n\niterate the array in similar fashion for each coin.\n \n Solution 1234 : \n category: Number theory\n\nwe cannot have an double array of size 10^8 coz its memory is greater than\n32 mb . instead we can have array of size 10^6.\nIn every node of array , store sum of 100 harmonic numbers along with arr[n-1].\n\nso arr[1]  contains 1/1 + .... 1/100\nand arr[2] = arr[1] + 1/101 +1/102 + .... 1/200\n\nAnd queries also can be performed in similar manner .\n \n Solution 1235 : \n category : Bisection , binary search\n\nClearly we have 3^18 subsets which is order of 10 ^ 9. But we can solve\nthis through by the approach meet in the middle . \ndivide the array into 2 groups each can be maximum size of 9. Now generate\nall possible values of both groups which is 3^9 subsets. Now for every value\ngenerated in set2 , find it corresponding value in set1 i.e (K-val) , if we\nfound this val, then the answer is YES.\n \n Solution 1324 : \n category : Grammar, brute force\n\nThis problem asks you to test whether two boolean expressions are equivalent. There are only 10 different boolean variables, evaluate and compare the 2 expressions for each of the 2^10 possible assignments.\n \n Solution 1341 : \n category : Number theory\n\nThis problem is equivalent to counting the number of divisors of a that are strictly smaller than sqrt(a) and larger or equal than b.\nThe solution generates all the divisors of a that are smaller than sqrt(a) and counts how many are larger or equal than b. The divisors can be generated efficiently after first finding the prime factorization of a.\n', 'All useful codes \n 1d Segment tree with lazy propagation : \n my solution above calculates number of elements  in a given array which are multiples of 3.\nqsn link : lightoj.com/volume_showproblem.php?problem=1135.s\n \n 2d segment tree : \n As we know 1d segment tree for array queries such as update,range sums etc..\n2d segment tree is used for matrix queries.\n\nQueries are of \n1) update an element\n2) range query like : sum of elements from a to b row and  c to d column\n\ncomplexity of each query is log n * log m. where n= rows and m = columns.\n \n Bellman Ford : \n this algorithm is used to find the shortest distance from source (similar to\n    dijikstra) . but the complexity is O(VE) .\n\nBellman ford algo is mainly used to find negative cycles in the graph. but it \ndo not find the position of negative cycle.\n \n Floyd warshall : \n shortest distance between every two nodes in the entire\ngraph.\n \n Set inclusion and Exclusion : \n this solution is useful when we deal with all the subsets present in an array.\nthere are 2^n subsets.\n\nwe can generate subsets only when n is smaller . \nrange of n :  n < 20 .\n \n Maximum bipartite matching : \n variations : max flow min cut , ford fulkerson algo\n \n nCr with mod : \n calculates  nCr with modulo mod.\nnCr = n! / ((r!)*(n-r)!) \n ncr % mod = n! * inverse(r!) * inverse((n-r)!)  % mod\n\n inverse(a) = a ^ (mod -2) where mod must be prime.\n \n Prime sieve (segmented sieve) : \n Normal sieve upto 10^5 , and to generate prime numbers bigger than that \n   use sam_sieve which is basically segmented sieve. by this,we can generate\n   prime numbers upto range 10^9. \n Complexity : O(N) \n Prims Algorithm : \n This algorithm is used to build minimum spanning tree.I used priority\nqueue in stl which minimized my work :p.\n\ncomplexity : O(V + ElogE)\n \n Sprague - grundy numbers : \n category : Number theory.\n\nfind grundy number for all the heap sizes.\n\ngrundy value of non movable states are 0.\ngenerally, g[0] = 0 .\n\ng[i] =  minimum number which is not present in the set of all the\ngrundy numbers where \'i\' can move into.\n\nanswer will be g[n1] ^ g[n2] ^ g[n3] ....\n \n stl-comparator function : \n we are sorting array of structures here :\n\npriority : lesser value of gold comes first ,\nif 2 groups have same amount of gold, take the one with lesser silver ,\nsimilarly bronze.\n\nbut if all the three are equal ,\nthe string "name" with lexicographically bigger will come first.\n \n Tarzans algorithm : \n this is used to find out all the strongly connected components in a \ngraph. a scc is a part of a graph in which there exists a path from\nevery node to every other node.\n\nrun a dfs to find discovery time and also low time of all nodes . \nscc root is the one which has same discovery time  and low time .\npop all the elements when we find out the root and which gives us one scc.\n\ncomplexity : O(V+E).\n \n Trie tree : \n implemented by using linked lists . pretty simple to understand .\n \n Z-algorithm : \n A very good technique for pattern searching.\nZ- array contains      \nz[i] = maximum substring length from ith index  which is also equal to \nprefix of the array .\n\nif we append the pattern at the starting, answer will be equal to all\nindexes whose z[i] is equal to length of the pattern.\n\ncomplexity is O(N)\n', "Aparoksha'15-App \n Android app for technical fest of IIIT-Allhabad \n This application is a medium to get a step closer to Aparoksha where you can look for any kind of details you require. Here you'll find info about all the events catagorized according to their field and day of occurrence as well. You can also set reminders for various events as per your need. \n \n \n   \n The application has all essential features that keep you up to date with fest - listing of events by the days, listing of events by category, Registration/Log-in and setting of reminders for your favourite events.   \n Installation : \n Play Store  : https://play.google.com/store/apps/details?id=com.aparoksha.main&hl=en   \n To contribute to the project: \n 1) Download and install Android Studio. \n2) Fork the repo, and then download/pull the source code. \n3) Open Android Studio and import the project. \n4) Work on the project and commit changes to the forked repo.   \n5) Submit a pull request!  ", 'Description : \n This HTTP proxy server made in C++ language. It supports only HTTP protocol get method requests .\nIn  simple words proxy server acts as middleware between you and the server u request. \nIt hides the client  from the server as server can only track details of proxy server. \nWe can also filter the browser requests by allowing only those requests  to secure websites.\n \n College - proxy : \n Yes, this codes works also with any college - proxy (I tested it on my college proxy) \n. Just u need to comment line no 240 and uncomment   lines  244 to 247.In this case,we are making a \nproxy server which connects to another proxy server present in our college network and sends the \nrequest to that proxy server and receives data back  from that server.basically it means there \nare 2 proxy servers running in middle between client and server.\n \n How to Run : \n clone this project and go to the folder and run "make" through terminal and provide a port \nnumber too.\n\n$make\n$./proxy 6789\n\nBAAAM ..!! your proxy server starts running on ur local machine port 6789 . \n(U can have a port of max number around 65k other than first 1000 system registered ports )\n \n How to test : \n In the browser , go to Preferences -> Advanced -> Network -> Settings -> \nManual Proxy Configuration.\nNow, Enter 127.0.0.1 in HTTP proxy field and 6789  in port number field. and press ok .\n\n(enter the same port number that used in the command line  )\n\nWe can also run through telnet command.\n\nEX :\n$ telnet 127.0.0.1 6789\n$ GET http://www.google.com:80/index.html/ HTTP/1.0\\r\\nContent-Length:\n     80\\r\\nIf-Modified-Since: Sat, 29 Oct 1994 19:43:31 GMT\\r\\n\\r\\n\n \n How I  Made : \n First i created a socket which listens to http requests and used fork for handling \nmultiple requests at a time . Then after recieving request from the client socket , \nI used functions present in proxy_parse.h for parsing these requests . Then i created another\nsocket to the host present in that request, and i send parsed  request to that host.\nOnce i recieve data from the server socket , i am passing this data back to the client socket \nthen the browser shows us the page we want.\n\nWe will not have more than 100 requests. Error-checking is done at every step . Errors\nwill be displayed in terminal.\n', 'python-scripts \n applicational stuff', "blindwars \n Aparoksha'16", 'Rust \n Ford fulkerson Algorithm', "IIITA_HACKS \n Smart Facebook Birthday Assistant    \n How many times have we spent hours to think of a way to wish someone on his/her special day or reply to someone’s birthday wish on our wall! We do it based on how close they are to us in our life, right? What if i tell you based on your facebook chat with the person, there will be automated posts on the person’s wall and automated replies to people’s posts on your wall!\nYes our project is there to make it easy for you \n How we are doing this    \n1. We are reading the messages using the fb's api  \n2. We are applying machine learning to classify the messages  \n3. The machine learning API gives the measure of neutral, positive, negative and irrelevance in the message content.  \n4. We will use this probablity to classify the users as good friend, normal friend and just friend.  \n5. Based on above classification the posts and reply will be made on the friends wall.  \n hackathon ", "sam-s-club-auctions \n Members of sam's club can bid on a product listed in auctions site . The following automated scripts makes users work painless. All they need is a twitter account and guts to reply with their bid value on tweets posted by sam's club . Doesn't this sounds awesome ?  \n \n Scraper \n The scraper is written in python language.It scrapes the auctions from sam's club new auctions page. we can restrict number of auction pages to be scraped. As soon as scraping is completed , auctions images are stored in auction_images sub repo with auction_id as a file name , similarly auction details such as item name, auction end time , current bid value are stored in auction_details sub repo. \n Twitter \n The script reads all the auction images and its details from the corresponding auctions_images and auctions_details folders and posts it in sam's club twitter page. \n Installation \n python3 samsclubscraper.py \n python3 ../twitter/post_tweets.py \n dependencies \n beautifulSoup \n Urllib \n Tweepy", "Because some posts are too mainstream for my blog \n I plan to write posts that don't fit into my  blog  here through issues (See  #1 ). You can also use this place\nfor communiting with me (see  #2 , #3 ).   The oppurtunities are endless!!  :rocket: :rocket:  \n write your experience at inout here. :p", 'embark-dtwitter', 'HideIt \n An Augmented Reality project to hide a currency note realtime \n Description \n HideIt is an AR cross platform application built by using unity and vuforia to hide a currency note in realtime video/Camera. The application even hides a moving currency note.  \n Demo videos \n video link :: https://www.youtube.com/watch?v=BXsmsYBMGaE \n \n Note:: please watch in  youtube for full video \n Prerequisites \n Unity Engine - https://unity3d.com/get-unity/download \nVuforia Plugin for Unity - https://developer.vuforia.com/downloads/sdk \nAdd necessary build tools for android/ IOS while installing Unity   \n Installation \n Clone the repo and open it in Unity engine \n Import Vuforia plugin to workspace \n start running the application in desktop, point 100 rs Indian currency note to the camera and observe the mask on the screen \n Build the application and run in any cross platform os \n Notes \n There is an android APK file attached. Please download and experience the demo.', 'B-Audit \n TITLE   : INTELLIGENT BUILDING \n TEAM : Sri Sanketh Uppalapati and Sameer Killamsetty   \n Introduction \n A self-conscious home is one which can track its own lifecycle, automate its maintenance decision makings and even improve itself. Our service runs in the background and assists in making intelligent decisions i.e a balanced decision keeping cost and comfort in mind. \n Design \n \n Estimated target audience \n \n House owners/tenants/buyers: The maintenance costs of the house can be tracked and documented. Helps them made monetary assessments. \n Service contractors: They provide service to different building equipments. They can be a company or an individual. \n \n Problems that can be solved \n \n Reduction in maintenance costs: Implementing an auction marketplace helps reduce the repair costs as there is competitive bidding. \n Automating maintenance decisions: The building electrical equipment will be logged on purchase. Pre-determined maintenance routines will be used for automating maintenance decisions. Fault detection techniques will help detect which component of the building needs fixing/maintenance. \n Tracking building performance and lifecycle: Helps track building functionality cost and reduces hassle in transferring ownership \n \n Estimated MVP functionality \n \n Each actor has a wallet and can transfer tokens \n The owner can link as many house as possible and equipments(with relevant details like guarantee etc). \n Auction marketplace \n Automated maintenance decisions using pre determined routines \n Performance monitoring and tracking dashboard(for owner and service contractor) \n \n Final release functionality \n Including the functionalities of a MVP, the final release will have the following additional functionalities: \n \n Implemented fault detection \n Multi-wallet linking \n Tipping service providers(In case the service contractor is a company) \n \n The scope can be expanded as “Intelligent Buildings” is a vast ocean open for improvements. \n EXAMPLE \n Let’s take a geyser, when geyser gets damaged , the measure of the damage is stored on the blockchain. So the geyser company can take that data and can do analysis.\n1. When any household device is purchased, the purchase details will be stored on smart contract.\n2. Owner of the home/contract transfers  money to the dAPP.\n3. Contract daemon runs in the background, finds faults in the home and posts an auction in auction market place where contractors compete for the auction. our contract chooses the right contract based on his credibility and the amount.\n4. On successful repair, money gets transferred to the contractor. \n This is a simple case of 1 owner in 1 house. An owner can have multiple houses']
Wakeupbuddy,['OpenSurfaces Segmentation UI \n This repository contains the segmentation user interface from the\n OpenSurfaces  project, extracted as a\nlightweight tool.  A dummy server backend is included to run the demo. \n You can also view the demo\n online . \n \n To run the demo, there are two versions: one with django, and one with no\nframework.  The django version uses a dummy django server and compiles the\nwebsite live as necessary.  The non-django version is a flat html file\nextracted from the django version. \n If you find this tool helpful, please cite our\n project : \n \n@inproceedings{bell13opensurfaces,\n    author = "Sean Bell and Paul Upchurch and Noah Snavely and Kavita Bala",\n    title = "OpenSurfaces: A Richly Annotated Catalog of Surface Appearance",\n    booktitle = "SIGGRAPH Conf. Proc.",\n    volume = "32",\n    number = "4",\n    year = "2013",\n}\n \n and report any bugs using the  GitHub issue\ntracker .\nAlso, please "star" this project on GitHub; it\'s nice to see how many people\nare using our code. \n Version 1: Run with Django (Ubuntu Linux) \n \n Install dependencies ( coffee-script ,  django ,  django-compressor ,\n    ua-parser ,  BeautifulSoup ): \n \n Note:  this will change your django current installation if you are\n   not somewhere between  1.4.*  and  1.6.* .  I suggest looking into the\n    virtualenv  package if this is a problem for you. \n \n./django-setup-demo.sh\n \n \n Start the local webserver: \n \n \n./django-run-demo.sh\n \n \n Visit  localhost:8000  in a web browser \n \n To get the demo to work on Mac and Windows, you will have to look at the above\nscripts and run the equivalent commands for your system. \n After drawing 6 polygons, the submit button will show you the POST data\nthat would have been sent to the server. \n Version 2: Run without Django (Linux or Mac) \n \n Install  npm  and  node.js .  On Ubuntu, this is: \n \n \nsudo apt-get install npm nodejs\n \n \n Install  coffee-script : \n \n \nsudo npm install -g coffee-script\n \n \n Build static files (js, css, img) and then start a local python-based\n   webserver: \n \n \n./python-run-demo.sh\n \n \n Visit  localhost:8000  in a web browser \n \n To get the demo to work on Windows, you will have to look at the above scripts\nand run the equivalent commands for your system. \n Project Notes \n POST data \n When a user submits, the client will POST the data to the same URL.  On\nsuccess, the client expects the JSON response  {"message": "success", "result":\n"success"} .  The client will then notify the MTurk server that the task is\ncompleted.  For more details, see\n example_project/segmentation/views.py . \n When a user submits, the POST will contain these fields: \n \nresults: a dictionary mapping from the photo ID (which is just "1" in\n    this example) to a list of polygons.  Example:\n    {"1": [[x1,y1,x2,y2,x3,y3,...], [x1,y1,x2,y2,...]]}.\n    Coordinates are scaled with respect to the source photo dimensions, so both\n    x and y are in the range 0 to 1.\n\ntime_ms: amount of time the user spent (whether or not they were active)\n\ntime_active_ms: amount of time that the user was active in the current window\n\naction_log: a JSON-encoded log of user actions\n\nscreen_width: user screen width\n\nscreen_height: user screen height\n\nversion: always "1.0"\n\nfeedback: omitted if there is no feedback; JSON encoded dictionary of the form:\n{\n    \'thoughts\': user\'s response to "What did you think of this task?",\n    \'understand\': user\'s response to "What parts didn\'t you understand?",\n    \'other\': user\'s response to "Any other feedback, improvements, or suggestions?"\n}\n \n Feedback survey \n When the user finishes the task, a popup will ask for feedback.  In the django\nversion, disable this by setting  ask_for_feedback  to  \'false\'  in the file\n example_project/segmentation/vies.py .  In the non-django verfsion, update the\n window.ask_for_feedback  variable in  index.html . \n I recommend asking for feedback after the 2nd or 3rd time a user has submitted,\nnot the first time, and then not asking again (otherwise it gets annoying).\nUsers usually don\'t have feedback until they have been working for a little while. \n Compiling from coffeescript \n The javascript for the tool is automatically compiled from coffeescript files\nby  django-compressor  and accessed by the client at a url of the form\n /static/cache/js/*.js .  This is set up already if using django. \n If not using django, the  python-run-demo.sh  does this for you by manually\ncompiling coffeescript files and storing them in the  /static/  folder. \n Browser compatibility \n This UI works in Chrome and Firefox only.  The Django version includes a\nbrowser check that shows an error page if the user is not on Chrome or Firefox\nor is on a mobile device. \n Local  /static/  folder \n After you run the demo setup, the directory  /static/  will contain compiled css\nand javascript files. \n If you are usikng django and change any part of the static files (js, css,\nimages, coffeescript), you will need to repopulate the static folder with this\ncommand: \n \nexample_project/manage.py collectstatic --noinput\n \n If you are building on top of this repository: \n In  example_project/settings.py :\n  1. Change  SECRET_KEY  to some random string.\n  2. Fill in the rest of the values (admin name, database, etc). \n If you want to add this demo to your own (separate) Django project: \n In your  settings.py  file, make the following changes: \n \n \n Make sure  STATIC_ROOT  is set to an absolute writable path. \n \n \n Add this to the  STATICFILES_FINDERS  tuple: \n \n \n \n    \'compressor.finders.CompressorFinder\',\n \n \n Add this to the  INSTALLED_APPS  tuple: \n \n \n    \'django.contrib.humanize\',\n    \'compressor\',\n    \'segmentation\',\n \n \n Add this to  settings.py  (e.g. at the end): \n \n \n    # Django Compressor\n    COMPRESS_ENABLED = True\n    COMPRESS_OUTPUT_DIR = \'cache\'\n    COMPRESS_PRECOMPILERS = (\n        (\'text/coffeescript\', \'coffee --bare --compile --stdio\'),\n        (\'text/less\', \'lessc -x {infile} {outfile}\'),\n    )\n', "Semantic Amodal Segmentation dataset and API \n This is the Python API code for the amodal segmentation dataset proposed in  Semantic Amodal Segmentation  (CVPR 2017). This API code is built on  COCO API . \n setup \n \n git clone and compile: \n git clone https://github.com/wakeupbuddy/amodalAPI \n \n cd PythonAPI; python setup.py build_ext install; cd .. \n \n \n create soft link for coco/bsds images: \n \n ln -s /your/coco/images ./images \n ln -s /your/bsds/images ./bsds_images \n dowload  annotation files  and untar. \n \n notebook demo \n \n To see the annotation and some useful APIs, please run the  ipython notebook demo . \n \n evaluate \n \n \n dowload the  baseline amodalMask output  on coco val set and untar: \n \n \n run the segmentation evaluation.  \n \n bash eval.sh \n \n It measures amodal segment proposal quality using average recall. Please see details in table 3a and section 5.1 from  the paper . \n annotation tool \n We also release the web tool we used for annotation in another repo  here . It's modified based on  OpenSurface . \n citation \n If you find this dataset useful to your research, please consider citing:\n @inproceedings{zhu2017semantic,\n    Author = {Zhu, Yan and Tian, Yuandong and Mexatas, Dimitris and Doll{\\'a}r, Piotr},\n    Title = {Semantic Amodal Segmentation},\n    Booktitle = {Conference on Computer Vision and Pattern Recognition ({CVPR})},\n    Year = {2017}\n}", 'www \n personal page']
gitunit,[]
denisfitz57,['SRwT', 'srwtf', 'srwtf2', " AUTO-GENERATED-CONTENT:START (STARTER)  \n \n \n \n \n \n  Gatsby's hello-world starter\n \n Kick off your project with this hello-world boilerplate. This starter ships with the main Gatsby configuration files you might need to get up and running blazing fast with the blazing fast app generator for React. \n Have another more specific idea? You may want to check out our vibrant collection of  official and community-created starters . \n 🚀 Quick start \n \n \n Create a Gatsby site. \n Use the Gatsby CLI to create a new site, specifying the hello-world starter. \n ```shell \n create a new Gatsby site using the hello-world starter \n gatsby new my-hello-world-starter https://github.com/gatsbyjs/gatsby-starter-hello-world\n``` \n \n \n Start developing. \n Navigate into your new site’s directory and start it up. \n shell\ncd my-hello-world-starter/\ngatsby develop \n \n \n Open the source code and start editing! \n Your site is now running at  http://localhost:8000 ! \n Note: You'll also see a second link:  http://localhost:8000/___graphql . This is a tool you can use to experiment with querying your data. Learn more about using this tool in the  Gatsby tutorial . \n Open the  my-hello-world-starter  directory in your code editor of choice and edit  src/pages/index.js . Save your changes and the browser will update in real time! \n \n \n 🧐 What's inside? \n A quick look at the top-level files and directories you'll see in a Gatsby project. \n .\n├── node_modules\n├── src\n├── .gitignore\n├── .prettierrc\n├── gatsby-browser.js\n├── gatsby-config.js\n├── gatsby-node.js\n├── gatsby-ssr.js\n├── LICENSE\n├── package-lock.json\n├── package.json\n└── README.md\n \n \n \n /node_modules : This directory contains all of the modules of code that your project depends on (npm packages) are automatically installed. \n \n \n /src : This directory will contain all of the code related to what you will see on the front-end of your site (what you see in the browser) such as your site header or a page template.  src  is a convention for “source code”. \n \n \n .gitignore : This file tells git which files it should not track / not maintain a version history for. \n \n \n .prettierrc : This is a configuration file for  Prettier . Prettier is a tool to help keep the formatting of your code consistent. \n \n \n gatsby-browser.js : This file is where Gatsby expects to find any usage of the  Gatsby browser APIs  (if any). These allow customization/extension of default Gatsby settings affecting the browser. \n \n \n gatsby-config.js : This is the main configuration file for a Gatsby site. This is where you can specify information about your site (metadata) like the site title and description, which Gatsby plugins you’d like to include, etc. (Check out the  config docs  for more detail). \n \n \n gatsby-node.js : This file is where Gatsby expects to find any usage of the  Gatsby Node APIs  (if any). These allow customization/extension of default Gatsby settings affecting pieces of the site build process. \n \n \n gatsby-ssr.js : This file is where Gatsby expects to find any usage of the  Gatsby server-side rendering APIs  (if any). These allow customization of default Gatsby settings affecting server-side rendering. \n \n \n LICENSE : Gatsby is licensed under the MIT license. \n \n \n package-lock.json  (See  package.json  below, first). This is an automatically generated file based on the exact versions of your npm dependencies that were installed for your project.  (You won’t change this file directly). \n \n \n package.json : A manifest file for Node.js projects, which includes things like metadata (the project’s name, author, etc). This manifest is how npm knows which packages to install for your project. \n \n \n README.md : A text file containing useful reference information about your project. \n \n \n 🎓 Learning Gatsby \n Looking for more guidance? Full documentation for Gatsby lives  on the website . Here are some places to start: \n \n \n For most developers, we recommend starting with our  in-depth tutorial for creating a site with Gatsby .  It starts with zero assumptions about your level of ability and walks through every step of the process. \n \n \n To dive straight into code samples, head  to our documentation .  In particular, check out the  Guides ,  API Reference , and  Advanced Tutorials  sections in the sidebar. \n \n \n 💫 Deploy \n \n \n  AUTO-GENERATED-CONTENT:END ", 'Octocat Game \n Test your memory. \n All the images from https://octodex.github.com/. \n This was originally a Pen created at CodePen.io. You can find this one at https://codepen.io/JasonEtco/pen/yobKqg.', " AUTO-GENERATED-CONTENT:START (STARTER)  \n \n \n \n \n \n  Gatsby's hello-world starter\n \n Kick off your project with this hello-world boilerplate. This starter ships with the main Gatsby configuration files you might need to get up and running blazing fast with the blazing fast app generator for React. \n Have another more specific idea? You may want to check out our vibrant collection of  official and community-created starters . \n 🚀 Quick start \n \n \n Create a Gatsby site. \n Use the Gatsby CLI to create a new site, specifying the hello-world starter. \n ```shell \n create a new Gatsby site using the hello-world starter \n gatsby new my-hello-world-starter https://github.com/gatsbyjs/gatsby-starter-hello-world\n``` \n \n \n Start developing. \n Navigate into your new site’s directory and start it up. \n shell\ncd my-hello-world-starter/\ngatsby develop \n \n \n Open the source code and start editing! \n Your site is now running at  http://localhost:8000 ! \n Note: You'll also see a second link:  http://localhost:8000/___graphql . This is a tool you can use to experiment with querying your data. Learn more about using this tool in the  Gatsby tutorial . \n Open the  my-hello-world-starter  directory in your code editor of choice and edit  src/pages/index.js . Save your changes and the browser will update in real time! \n \n \n 🧐 What's inside? \n A quick look at the top-level files and directories you'll see in a Gatsby project. \n .\n├── node_modules\n├── src\n├── .gitignore\n├── .prettierrc\n├── gatsby-browser.js\n├── gatsby-config.js\n├── gatsby-node.js\n├── gatsby-ssr.js\n├── LICENSE\n├── package-lock.json\n├── package.json\n└── README.md\n \n \n \n /node_modules : This directory contains all of the modules of code that your project depends on (npm packages) are automatically installed. \n \n \n /src : This directory will contain all of the code related to what you will see on the front-end of your site (what you see in the browser) such as your site header or a page template.  src  is a convention for “source code”. \n \n \n .gitignore : This file tells git which files it should not track / not maintain a version history for. \n \n \n .prettierrc : This is a configuration file for  Prettier . Prettier is a tool to help keep the formatting of your code consistent. \n \n \n gatsby-browser.js : This file is where Gatsby expects to find any usage of the  Gatsby browser APIs  (if any). These allow customization/extension of default Gatsby settings affecting the browser. \n \n \n gatsby-config.js : This is the main configuration file for a Gatsby site. This is where you can specify information about your site (metadata) like the site title and description, which Gatsby plugins you’d like to include, etc. (Check out the  config docs  for more detail). \n \n \n gatsby-node.js : This file is where Gatsby expects to find any usage of the  Gatsby Node APIs  (if any). These allow customization/extension of default Gatsby settings affecting pieces of the site build process. \n \n \n gatsby-ssr.js : This file is where Gatsby expects to find any usage of the  Gatsby server-side rendering APIs  (if any). These allow customization of default Gatsby settings affecting server-side rendering. \n \n \n LICENSE : This Gatsby starter is licensed under the 0BSD license. This means that you can see this file as a placeholder and replace it with your own license. \n \n \n package-lock.json  (See  package.json  below, first). This is an automatically generated file based on the exact versions of your npm dependencies that were installed for your project.  (You won’t change this file directly). \n \n \n package.json : A manifest file for Node.js projects, which includes things like metadata (the project’s name, author, etc). This manifest is how npm knows which packages to install for your project. \n \n \n README.md : A text file containing useful reference information about your project. \n \n \n 🎓 Learning Gatsby \n Looking for more guidance? Full documentation for Gatsby lives  on the website . Here are some places to start: \n \n \n For most developers, we recommend starting with our  in-depth tutorial for creating a site with Gatsby .  It starts with zero assumptions about your level of ability and walks through every step of the process. \n \n \n To dive straight into code samples, head  to our documentation .  In particular, check out the  Guides ,  API Reference , and  Advanced Tutorials  sections in the sidebar. \n \n \n 💫 Deploy \n Build, Deploy, and Host On The Only Cloud Built For Gatsby \n Gatsby Cloud is an end-to-end cloud platform specifically built for the Gatsby framework that combines a modern developer experience with an optimized, global edge network. \n  AUTO-GENERATED-CONTENT:END "]
kaustubhn,["AmulMat \n Clustering of famous Amul cartoons. Applying CNN's for feature extraction and clustering based on 2048 features output from the model. \n Code \n Coming soon...", 'Supporting content for my blog post \n [(link)] https://medium.com/@kaustubhn/quick-dirty-sentiment-2fc2843ba6bd \n Data Source \n [(link)] https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews \n A few notes \n \n A good text preprocessing strategy ensures improved quality of models \n More data implies better models \n Vector dimension is something that can be tweaked depending on use case \n Experimentation of various machine learning classifiers is also a good direction to explore \n', 'Document Clustering an interesting approach \n We approach document clustering in a very interesting way in which we can define a process and one the process in setup\nit just need to be fed with data on regular intervals and rest the systems takes care of the tasks. \n Dependencies: \n \n Python 3 \n Gensim \n \n Note : This repository in under active development, use code in production at your own risk.', 'Collision Severity Prediction \n Predict collision severity based on several attributes \n Dependencies \n \n Python \n Scikit-Learn \n Pandas \n Matplotlib \n Seaborn \n', 'Large Flask App Template', "Your GitHub Learning Lab Repository for Managing Merge Conflicts \n Welcome to  your  repository for your GitHub Learning Lab course. This repository will be used during the different activities that I will be guiding you through. \n Oh! I haven't introduced myself... \n I'm the GitHub Learning Lab bot and I'm here to help guide you in your journey to learn and master the various topics covered in this course. I will be using Issue and Pull Request comments to communicate with you. In fact, I already added an issue for you to check out. \n \n I'll meet you over there, can't wait to get started! \n \n There are already some Pull Requests open, you will work on those later in the course. \n", 'My Tensorflow Voyage \n Learnings about tensorflow library', 'Hi there 👋 \n \n**kaustubhn/kaustubhn** is a ✨ _special_ ✨ repository because its `README.md` (this file) appears on your GitHub profile.\n\nHere are some ideas to get you started:\n\n- 🔭 I’m currently working on ...\n- 🌱 I’m currently learning ...\n- 👯 I’m looking to collaborate on ...\n- 🤔 I’m looking for help with ...\n- 💬 Ask me about ...\n- 📫 How to reach me: ...\n- 😄 Pronouns: ...\n- ⚡ Fun fact: ...\n', 'Simple Flask Application \n Application to demonstrate flask integration with redis via. API \n Steps to run the application \n Approach 1: Without Docker \n Prerequisite: To be installed on computer\n1. Python 3\n2. Redis  \n Step 1: Clone the repo \n$ git clone < repo-url > \n Step 2: Create Python Virutal Environment \n$ python3 -m venv venv   \n Step 3: Activate Virtual Environment\n$ source venv/bin/activate   \n Step 4: Cd to project dir \n$ cd < repo-name >   \n Step 5: Install Dependencies \n$ pip install -r requirements.txt   \n Step 6: Start Redis \nStart a new terminal to run redis server \n$ redis-server   \n Step 7: Start Application \n$ python app.py   \n Approach 2: With Docker \n Prerequisite: To be installed on computer\n1. Docker\n2. Docker Compose \n Step 1: Run docker compose commmand \n$ docker-compose up -d   \n To Run Tests \n Step 1: Run docker compose command \n$ docker-compose up -d \n Step 2: Ssh in docker container \n$ docker exec -it web bash \n Step 3: Run tests \n$ pytest tests.py']
killeent,['My simple configurations for BASH and VIM.', 'Some standard CS data structures and algorithms implemented in Go.', 'The goal of this project was to read in documents, build an inverted index and process queries on that data.  \n I abandoned this after I realized that the scope was bigger than I expected / I got tired of coding in Go. ', 'Goldsberry \n Goldsberry is a graph implementation written in C. \n Project Purpose \n \n To refresh my C programming abilities \n To refresh my code design abilities (e.g. modularity, encapsuation, design) \n To refresh my knowledge about graphs \n \n Building \n To build everything, type:  make .\nTo build only the CLI, type  make goldsberry .\nTo build only the test suite, type  make test .\n make clean  works as expected.  \n The only dependency is the C unit testing framework check: http://check.sourceforge.net/', 'Some various algorithms implemented in C & Python. This was inspired by an equivalent project in Go, but C & Python seemed like better choices since the goal is to learn the algorithms, not learn a language.  \n File            | Description\n----------------|--------------\nsubsets.py      | Simple script to generate the subsets of numbers 1 ... n, given input n.\npermutations.py | Simple script to generate the permutations of numbers 1 ... n, given input n.\ndynamic_fib.py  | Finds the nth fibonacci number using dynamic programming\nedit_distance.c | Calculates the edit distance between two strings using dynamic programming\npartition.py    | Partitions a sequence of n integers into k partitions such that the maximum sum of any partition is minimized.', 'Reggie \n Reggie is a web image scraper written in Java. Reggie both supports sequential and parallel scraping. \n Building \n \n To compile, type:  mvn compile \n To run unit tests, type:  mvn test \n \n Dependencies \n \n JUnit \n JSoup \n Apache Commons CLI \n', 'javalib \n javalib is a Java Library of Data Structures and Algorithms I have implemented for practice. \n Building \n \n To compile, type:  mvn compile \n To run unit tests, type:  mvn test \n \n Dependencies \n \n JUnit \n \n Data Structures \n \n Binary Heap \n Graph (undirected & directed) \n Queue \n Thread Pool \n Timer \n Bounded Buffer \n Future \n Hash Table \n Thread-safe LRU Cache \n Readers-Write Lock (w/ Fairness) \n \n Algorithms: \n Arrays:\n* quicksort\n* binary search\n* first occurrence\n* kth smallest element\n* count inversions\n* remove duplicates\n* merge sort\n* minimum cover\n* permute in-place\n* generate permutations\n* random subset\n* rotate\n* reverse \n Graphs:\n* shortest path (BFS)\n* shortest path (Djikstra)\n* shortest path (min cost & length)\n* all pairs shortest paths (Bellman-Ford)\n* cycle detection\n* find articulation vertices\n* topological sort (DAG) \n Math:\n* power\n* square root\n* binomial coefficient\n* enumerate primes \n Specialized:\n* minimize difference of two subsets of integer array \n Strings:\n* palindrome\n* Knuth-Morris-Pratt matching\n* multiply integer representations \n Trees:\n* pre-, in- and post-order traversal (recursive & iterative)\n* binary tree symmetry', 'Some basic things implemented in Node.js. \n Directory      | Description\n---------------|------------\ntcp_server    | A simple TCP server that responds to connections with the Date-Time.\narchive        | Convert directories to .tar.gz files and back again!\npostgres       | Run a query on a local postgres database. \ntwit           | Use the twit API to get a stream of tweets associated with a hashtag.', 'ExCiting Project \n ExCiting tries to make understanding research papers easier by summarizing the citations and descriptions that other authors have written about a paper. \nRead more about the project  here . \n Usage \n Dependencies \n This project has a number of dependencies to use. \n  - We are using the  The ACL Anthology Network Release 2013  dataset as our corpus of research papers because it came with a citation network.\n  - We are using  ParsCit  to "chunkify" the papers in our corpus. This tool extracts any intext citations and matches them up with the cited author and paper and outputs them in XML blocks.\n  - A number of ML and NLP tools including  nltk ,  algorithmia , and  natural .\n    - Installing NLTK\n      - Run  pip install nltk \n      - Run this from a python session:  nltk.download() \n      - Install all of the packages for the easiest time.\n    - Install any node packages using the  package.json  files in the directories.\n  - MongoDB. This project assumes that you have mongo installed and have a running mongod process. \n Data Preperation \n We\'ve included the latest database dump in this repo. Run  sh import-data.sh  to import it into your mongo instance. This will also download the aan dataset we used and extract it for you. That dataset is quite large, so if you don\'t need to use it, you should delete it from your disk. \n Contributing \n Before you push: \nIf you\'ve made any changes to the database, run  sh export-data.sh  so that others will get your updates.  \n After you pull: \nRun  sh import-data.sh  to get any database updates and to get the aan dataset.', 'deflate \n goal: file compression / decompression in C.', 'Elevator Challenge \n Challenge Performed from 1pm-4:30pm PST, on 2/4/15. This includes design, coding\nand writing this writeup.  \n Build Information \n \n gradle build  to build \n gradle run  to run main \n \n Top-level Interface \n First we define the goals of the system as related to the four interface\nfunctions that we need to provide. \n \n List((int, int, int)) status(): returns a list of results of the status of each\nelevator in the form (id, curr floor, dest floor). \n \n This function is unchanged. In general, because it is merely returning the state\nof the world, this function do not inform development, other than requiring us\nto provide an interface for getting information about the state of the world. \n \n update(int, int): sets the destination floor of the elevator specified by the\nfirst parameter (ID) to the floor specified by the second parameter. \n \n This is used by the elevator system to move elevators without requests. We\nremove the ability to change the current floor as this will be handled by step.\nIt didn\'t make sense to just randomly change the current floor (the laws of\nphysics will not allow it!). \n \n void pickup(int, int): requests a pickup at the floor specified by the first\nparameter, to travel to the floor at the second parameter. \n \n We extend the basic interface by allowing the client to specify their desired\nfloor when requesting elevator service, as opposed to just desiring to go up or\ndown. It is important to note that we are still not implementing the real-world\nfunctionality of requesting a new floor while in the elevator. \n In order to support this functionality, we will need to track the passengers in\nan elevator and their desired floors -> we will "open the doors" and let\npassengers out on their specific floor, while "keeping" the other passengers\ninside the elevator. \n Additionally, we will need to make sure that someone eventually picks up a\npassenger at the requested floor. To do so, we will need to make sure that a\npickup request either initiates the movement of an elevator to pick up that\npassenger or registers the request with an elevator already performing work \n(or as in our actual implementation, elevators are going all the time). \n \n void step(): Time-step our world. To do so, we will simply iterate through each\nof our elevators and have them move one floor towards their intended direction.\nThis is the simplest implementation. \n \n Synchronization Issues \n We make the following assumptions about our world to inform development. \n \n Requests (in the form of pickup(), update() API calls) are independent of \ncalls to step().  \n Calls to step should be executed atomically, i.e. no interleaving of pickup \nrequests. \n \n As a result, we decided to have a single lock guarding the state of our world. \n During a call to pickup() we: \n \n acquire the global lock  \n make our request  \n release the lock. \n \n During a call to update() we: \n \n acquire the global lock  \n update the\ndestination floor of the specified elevator \n release the lock \n \n During a call to step() we:  \n \n acquire the global lock  \n iterate through allelevators, performing their actions one by one  \n release the lock \n \n (we also acquire the lock to query the world) \n Though in the real world pickups, updates and the movement of elevators are\ncontinuous and occur in parallel, given the limited timespan necessary to\nimplement this project, we chose to have this simple implementation. \n Scheduling Issues \n I chose to implement a SCAN-like (Elevator) Algorithm used for disk scheduling\nto handle elevator scheduling. The SCAN algorithm is modeled after elevator\nservice so it has to be the optimal algorithm right? :) \n The SCAN algorithm means that the elevator will only move in one direction\n(either up or down) until reaching the top or bottom, before reversing course.\nThis constrains our behavior at each floor because the elevator will only pick\nup people who are intending to travel in the same direction we are headed. \n This algorithm is better in theory because it is more efficient -> we serve more\npassengers per floor traveled. For example, suppose under the FCFS algorithm we\nhave three passengers and one elevator on a 5 story building. The passenger\nrequests are (2, 4), (2, 1) and (2, 5) in that order. Under FCFS, we would\ntravel from 2->4->1->5 to drop off the passengers, requiring 9 total floors.\nUnder our algorithm, we would go from 2->5->1, requiring only 7 floors. \n There are a number of ways we could further optimize scheduling, that we\nrejected to simplify the implementation: \n \n \n Instead of having the elevator go all the way to the top floor when moving\nup, and all the way to the bottom floor when going down, we could simply stop at\nthe highest/lowest floor requested by any passenger in the elevator. However,\none could imagine where a person could starve if the elevator handled requests\nthat pinged between floors 2 and 5 while someone was waiting at floor 1. By\nguaranteeing that the elevator simply travels all the way up and all the way\ndown, we guarantee eventual satisifaction of a request at a cost of efficiency. \n \n \n If we have a lot of elevators, we could only have a subset of them working at\nlow request rates to improve efficiency (as measured by people serviced / total\nfloors moved). However, as with the above concern, this would greatly increase\nthe complexity of our system. \n \n \n Elevators are always moving if there is someone waiting for a request. This\nis terribly inefficient, and definitely should be improved upon, but was done so\nfor efficiency reasons. I thought about implementing some sort of global\nrequest queue to dispatch elevators to, but in order to implement this within a\nreasonable amount of time, I decided that the added complexity was too great. \n \n \n Class Design \n The main class we design is the ElevatorControlSystem class, which implements\nthe interface we designed above, and contains the state of our world. \n state: \n \n lock guarding the state of the (implicity this)  \n an array of lists of Requests, one for each floor of the building  \n an array of Elevators \n \n public functions: \n \n the four API functions described above  \n a constructor, which takes in a user-specified floor count and elevator count  \n getIDs(): returns the set of elevator IDs  \n floorRange(int elevatorID): returns the range of floors for an elevator \n \n private functions: \n \n isValidElevatorID(int elevatorID): check the validity of an elevatorID \n \n discussion: \n The API is discussed above, and the constructor is fairly self-explanatory.\nInstead, let\'s consider the class design. \n First, we have an array of lists of requests -> i.e. arr[0] contains the list\nof requests for the 0th floor. When an elevator arrives at a floor, it will\ncheck and  see if there are people waiitng. It will pick up all those whose\nrequests are in the direction that elevator is traveling. \n We also have an array of elevators. This provides the ElevatorControlSystem\naccess to each elevator in the system and allows it to query them for\ninformation. \n The next class we consider is the Request class. \n state: \n \n desired floor \n \n public functions: \n \n a constructor, which takes in the specified desired floor  \n getDesiredFloor(): returns the floor desired by this request \n \n discussion: \n The request class is fairly simple, but is designed for modularity and\nextensibility. We can think of the request as a person so we could imagine\nadding various extensions (e.g. if the person as a fireman, perhaps they have a\nkey that allows them exclusive access to the elevator, and their request could\nembody this greater priority). \n The last class we consider is the Elevator class. \n state: \n \n id  \n current, destination floor  \n min, max floor destination floor  \n a list of passengers (Requests) \n \n public functions: \n \n a constructor, which takes in a specified initial floor and min, max floors  \n a step() function, to be called whenever a global step occurs. This simply moves\nthe elevator one floor towards the desired floor.   \n a releasePassengers()function, which lets passengers out of the elevator that \nare at their requestedfloor  \n a serviceRequest() functions, which lets passengers into the elevator  \n a setDestinationFloor() function, which allows the ECS to set the destination\nfloor.  \n \n discussion: \n The challenge with the Elevator is that it is inherently coupled to the\nElevatorControlSystem, so we want to design it in a way so that the\ndependencies are one sided. Thus we made our Elevator simply keep track of who\nit holds andwhere it is headed, and made the ECS responsible for calling the\nstep, releasePassengers and serviceRequests functions at the appropriate times. \n Additionally, we like to add the ability to specify the min and max floors. Not\nonly is this necessary to implement our SCAN algorithm, it allows us to create\nelevators that only service certain ranges of the building. For example, many\nbuildings have elevators that only service a certain range of floors. \n The setDesiredFloor function is necessary for the ECS system to a move an\nelevator, without a request, and to decouple the SCAN functionality as best we\ncan. It will allow us to set up a different scheduling algorithm without having\nto modify the internal state of the Elevator. \n class organization: \n For now, we chose to implement Request and Elevator as inner classes within the\nElevatorControlSystem because they are fairly specific to the ECS\nimplementation. One could imagine eventually moving these classes out (and/or)\ndefining interfaces for them, but for now we will not. \n Implementation Notes \n A few examples of things I changed during implementation: \n \n added interface for an ElevatorControlSystem added floor range so that the client \ncan interact with update appropriately  \n add is valid elevator id  \n add exceptions for a lot of things \n \n In general, I stayed fairly true to the implementation specified above. Everything\nI changed was mostly to make the interfaces make more sense.  \n Final Thoughts \n I think it went okay -> but I would have liked to put more thought into the\nscheduling algorithm. The next thing I would have done with more time is to\nonly make elevators move if either: \n \n They have people inside them \n There is someone waiting somewhere \n \n This is a simple fix that could yield a lot of efficiency gains. ', 'suffix_tree \n Teaching myself suffix trees by implementing them in Python. \n instructions \n \n python setup.py install  to install \n py.test suffixtree  to test \n', 'gotcp \n A simple tcp server in Go. ', 'ATen: A TENsor library \n ATen is a simple tensor library thats exposes the Tensor operations in Torch\nand PyTorch directly in C++11. The wrapper respects the semantics of operators\nin PyTorch, except minor details due to differences between C++ in Python in\nthe way default arguments are handled. See the  documentation for tensors  in PyTorch for what these operations do.\nATen\'s API is auto-generated from the same declarations PyTorch uses so the\ntwo APIs will track each other over time. \n Tensor types are resolved dynamically, such that the API is generic and\ndoes not include templates. That is, there is one  Tensor  type. It can hold a\nCPU or CUDA Tensor, and the tensor may have Doubles, Float, Ints, etc. This design\nmakes it easy to write generic code without templating everything. \n See the  generated   Tensor.h  file  and  Functions.h  file  for the provided API. Excerpt:\n c++\nTensor atan2(const Tensor & other) const;\nTensor & atan2_(const Tensor & other);\nTensor pow(Scalar exponent) const;\nTensor pow(const Tensor & exponent) const;\nTensor & pow_(Scalar exponent);\nTensor & pow_(const Tensor & exponent);\nTensor lerp(const Tensor & end, Scalar weight) const;\nTensor & lerp_(const Tensor & end, Scalar weight);\nTensor histc() const;\nTensor histc(int64_t bins) const;\nTensor histc(int64_t bins, Scalar min) const;\nTensor histc(int64_t bins, Scalar min, Scalar max) const; \n Inplace operations are also provided, and always suffixed by  _  to indicate they will modify the Tensor. \n Installation \n TH/THC/THNN/THCUNN are provided (as git subtrees), so the repo is standalone. You will need a C++11 compiler, cmake, and the pyyaml python package.\n``` \n Install pyyaml used by python code generation to read API declarations \n OSX: if you don\'t have pip \n sudo easy_install pip \n Ubuntu: if you don\'t have pip \n apt-get -y install python-pip \n if you don\'t have pyyaml \n sudo pip install pyyaml \n mkdir build\ncd build\ncmake .. -DCMAKE_INSTALL_PREFIX=/where/you/want # specify your dest directory\nmake install\n``` \n Example usage \n Here is a simple example; again, the syntax follows Torch semantics. \n ```c++\nusing namespace at; // assumed in the following \n Tensor d = CPU(kFloat).ones({3, 4});\nTensor r = CPU(kFloat).zeros({3,4})\nfor(auto i = 0; i < 100000; i++) {\n  r = r.add(d);\n  // equivalently\n  r = r + d;\n  // or\n  r += d;\n}\n``` \n Want this running on the GPU?\n```c++\nusing namespace at; // assumed in the following \n Tensor d = CUDA(kFloat).ones({3, 4});\nTensor r = CUDA(kFloat).zeros({3,4})\nfor(auto i = 0; i < 100000; i++) {\n  r = r.add(d);\n  // equivalently\n  r = r + d;\n  // or\n  r += d;\n}\n``` \n Expressions like  CUDA(kFloat)  are first-class  at::Type  objects that represent\nthe type of a Tensor and are used to create Tensors when their type cannot be\ninferred. See the  generated   Type header  for its API. \n See more in  sample files . \n Creating your kernel \n It is easy to create new kernels, thanks to the  dispatch<>()  templated function. Example:\n```c++ \n // a simple sum kernel (for CPU only)\ntemplate \nstruct sum_op {\n  // dispatch handles variable arguments for you\n  Tensor CPU(const Type & t, Tensor & x_)\n  {\n    Tensor x = x_.contiguous();\n    auto x_p = x.data ();\n    int64_t size = x.numel();\n    T sum = 0;\n    for(int64_t i = 0; i < size; i++) {\n      sum += x_p[i];\n    }\n    return sum;\n  };\n  Tensor CUDA(Tensor& x) {\n    throw std::invalid_argument("device not supported");\n  };\n}; \n Tensor a = CPU(kFloat).rand({3, 7});\nstd::cout << a << std::endl;\nstd::cout << dispatch (a.type(),a) << " == " << a.sum() << std::endl;\n``` \n Efficient access to tensor elements \n When using Tensor-wide operations, the relative cost of dynamic dispatch is very small.\nHowever, there are cases, especially in your own kernels, where efficient element-wise access is needed,\nand the cost of dynamic dispatch inside the element-wise loop is very high.\nATen provides  accessors  that are created with a single dynamic check that a Tensor is the type and number of\ndimensions. Accessors then expose an API for accessing the Tensor elements efficiently: \n ```c++ \n Tensor foo = CPU(kFloat).rand({12,12}); \n // assert foo is 2-dimensional and holds floats.\nauto foo_a = foo.accessor ();\nfloat trace = 0; \n for(int i = 0; i < foo_a.size(0); i++) {\n  // use the accessor foo_a to get tensor data.\n  trace += foo_a[i][i];\n}\n``` \n Accessors are temporary views of a Tensor. They are only valid for the lifetime of the tensor that they\nview and hence should only be used locally in a function, like iterators. \n Using externally created data \n If you already have your tensor data allocated in memory (CPU or CUDA),\nyou can view that memory as a Tensor in ATen: \n c++\nfloat data[] = { 1, 2, 3,\n                 4, 5, 6};\nauto f = CPU(kFloat).tensorFromBlob(data, {2,3});\ncout << f << endl; \n These tensors cannot be resized because ATen does not own the memory, but otherwise\nbehave as normal tensors. \n Scalars and zero-dimensional tensors \n In addition to the  Tensor  objects, ATen also includes  Scalar s that represent a single number.\nLike a Tensor, Scalars are dynamically typed and can hold any one of ATen\'s  number types .\nScalars can be implicitly constructed from C++ number types. Scalars are needed because some functions like  addmm  take numbers along with Tensors and expect these\nnumbers to be the same dynamic type as the tensor. They are also used in the API to indicate places where\na function will  always  return a Scalar value, like  sum . \n ```c++\nTensor addmm(Scalar beta, const Tensor & self,\n             Scalar alpha, const Tensor & mat1,\n             const Tensor & mat2);\nScalar sum(const Tensor & self); \n //usage\nTensor a = ...\nTensor b = ...\nTensor c = ...\nTensor r = addmm(1.0, a, .5, b, c);\n``` \n In addition to Scalars, ATen also allows Tensor objects to be zero-dimensional. These Tensors hold\na single value and they can be references to a single element in a larger Tensor. They can be used anywhere a Tensor is expected. They are normally created by operators like  select  which reduce the dimensions of\na Tensor. \n c++\nTensor two = CPU(kFloat).rand({10,20});\ntwo[1][2] = 4;\n//~~~~~~~  zero-dimensional Tensor \n It is possible to convert between Scalar and zero-dim Tensors: \n c++\nTensor zero_dim = CPU(kFloat).scalarTensor(4);\nScalar from_tensor = Scalar(zero_dim); //only valid when zero_dim.dim() == 0; \n Avoiding unnecessary CUDA synchronization in your kernels when using Scalars \n Moving a single number from the GPU to the CPU introduces a synchronization point\nthat can add latency to your program. In certain cases the result of a GPU operator like  sum  which\nreturns a Scalar may be plugged into another GPU operator as an argument. If Scalars were always copied\nto the CPU, this would result in 2 copies. To avoid these synchronizations, Scalar objects can be\noptionally backed by a zero-dim Tensor, and are only copied to the CPU when requested. \n ```c++\nauto a = CUDA(kFloat).rand({3,4})\nScalar on_gpu = Scalar(a[1][1]); //backed by zero-dim Tensor\nassert(on_gpu.isBackedByTensor()); \n double value = on_gpu.toDouble(); // copied to CPU, if it was backed by GPU Tensor.\nScalar svalue = on_gpu.local(); // force the Scalar to become local to CPU. \n // get the scalar as a zero-dim tensor. If it was already backed\n// by a zero-dim Tensor then this op has no synchronization.\n// if the Scalar was local on CPU, it performs the copy\nTensor same_tensor = CUDA(kFloat).scalarTensor(on_gpu);\n``` \n Operators aware of the location of Scalars can arrange to do the minimal number of copies required.']
chyikweiyau,['web_channels']
ChrisCummins,["DEPRECATED REPOSITORY \n This repository contains the  old  source code for\nhttp://chriscummins.cc. Note that this has moved to a shiny  new\nrepository ,\nand this old code has been  merged\nin . This\nrepository is pending deletion. \n Any source code I've written is released under the MIT license (see\n COPYING ). Any third-party code is distributed under the terms of\ntheir license. \n Building the website \n The website can be built in a GNU/Linux environment using the standard GNU\nAutotools procedure: \n sh\n$ ./autogen.sh\n$ ./configure\n$ make all \n See  ./configure --help  for a list of configuration options for building the\nwebsite. \n Requirements \n \n Autoconf \n Automake \n Java \n Node.js \n Less \n \n Hosting the website \n The website can be deployed on a LAMP stack, by exporting the contents of the\n /build/www  directory. \n Requirements \n \n Apache \n MySQL \n PHP \n", "zsh \n DEPRECATED:  See  dotfile  for an\nup-to-date zsh configuration. \n Usage: \n cd\ngit clone --recursive https://github.com/ChrisCummins/zsh.git ~/.local/src/zsh\nln -sf .local/src/zsh/zshrc .zshrc \n The configuration can be extended by adding files to  local . E.g. \n echo 'export MY_WEBSITE=foobar:80` > ~/.local/src/zsh/local/web.zsh", "Particle engines \n This repository contains a few examples of particle systems, with demonstration applications of each. Each particle system consists of a two part frontend + backend relationship: \n \n \n The backend : this is a common backend which supplies a means for constructing and drawing a particle system using Cogl. This is reponsible for assembling a cogl primitive from the particle vertices and managing the memory associated with them. The interface is defined in  pe/particle-engine.h , and its implementation can be found in  pe/particle-engine.c . \n \n \n The frontend : this is responsible for modelling the behaviour, movement and lifespan of the particles. It is here that the particles are assigned characterstics which defines how they behave, such as swarming patterns, flocking or deterministic movement. There are several frontend implementations for different purposes: \n \n \n Particle Swarm  -  pe/particle-swarm.h \n \n Particle Emitter  -  pe/particle-emitter.h \n Particle System  -  pe/particle-system.h \n \n 1. Particle Swarm \n This is a fairly standard emulation of the 1986  Boids  program. It models each particle (or boid) as a simple entity whose behaviour is determiend by three rules: \n \n Cohesion  - a particle steers towards the center of mass of its surrounding particles. \n Alignment  - a particle attempts to mimic the movement of its surrounding particles. \n Separation  - a particle avoids other particles to prevent bumping into each other and to maintain a maximum swarm density. \n \n Additionally, there are extra influences affecting a particle's movement: \n \n Boundaries  - particles are contained within a bounding area, and will steer to avoid going out of these bounds. \n Global forces  - a global force can be applied uniformly to each of the particles, for example to model the effects of strong wind or a current in water. \n Speed limits  - the speed of a particle is determined by it's size, and has minimum and maximum speeds enforced. \n \n The implementation of these rules is contained within the  particle_apply_swarming_behaviour()  function in  pe/particle-swarm.c . Additionally, there is a JavaScript+HTML5 implementation of this which models the flocking behaviour of birds, and can be found in the web directory. \n Examples \n \n ./examples/ants \n ./examples/fish \n web/index.html \n \n 2. Particle Emitter \n Examples \n \n ./examples/catherine_wheel \n ./examples/fireworks \n ./examples/fountains \n ./examples/snow \n \n 3. Particle System \n A rough and ready model for simulating circular Kepler orbits of particles about a fixed center of mass. \n Examples \n \n ./examples/galaxy \n", "\n \n This repository contains the source code for the  Protein Isoelectric\nPoint Database , a website which houses data\naccumulated by members of Aston University's Life and Health Sciences\ndepartment. The purpose of the website is to provide a powerful and\neasy-to-use search engine for the mass of biological data which has\nbeen acquired, making the valuable research accessible to all. \n The project was developed by Chris Cummins (http://chriscummins.cc) as\npartial fulfillment of the requirements for the degree of MEng\nElectronic Engineering & Computer Science at Aston University.  Read\nthe submission report . \n Background \n Bioinformatics is a multidisciplinary field which uses computational methods to\naid in biological research by creating systems for storing, organising and\nanalysing complex biological data. Within this field there are many online\ndatabases categorising biological information at the molecular level, and one\nsuch purpose of these is for storing the functional and physical properties of\nproteins. Currently, no such database exists for one of the most widely-used,\nimportant, and useful properties of proteins: the isoelectric point (pI). An\nisoelectric point is the acidity (pH) at which a molecule carries no net charge;\nbelow the isoelectric point, proteins have a net positive charge, above it a net\nnegative charge. Additionally, proteins are at their lowest solubility at their\nisoelectric point, and this makes the isoelectric point a vitally important\nproperty when both characterising and purifying proteins. \n The dataset which has been compiled is a collection of entries stored as a\nnon-relational table, and for each entry it records the name of the protein, its\nidentity, origin, experimental conditions, its isoelectric point, and other\npertinent data. There are also links to a heterogeneous collection of databases\ncontaining associated data, such as amino acid sequence, function, etc. A\nweb-accessible database that warehouses this data and offers a robust and\nadaptable GUI for searching, viewing and downloading results would greatly\nincrease the accessibility of the dataset. For more detailed information about\npip-db, see  the documentation . \n Installation \n Execute: \n sh\n$ ./bin/build \n See  ./configure --help  for a list of configuration options for building the\nwebsite. \n Requirements \n \n Autoconf \n Automake \n Java \n Leiningen \n PostgreSQL \n Git   (optional, required only to build the extra tools) \n npm   (optional, required only to build the extra tools) \n node-optimist   (optional, required only to build the extra tools) \n Python   (optional, required only to build the extra tools) \n pdftex   (optional, required only to build the extra documentation) \n pandoc   (optional, required only to build the extra documentation) \n doctoc   (optional, required only to build the extra documentation) \n \n Running the website \n ```sh \n First, setup the required environment variables: \n $ ./scripts/env.sh\nStarting a development shell... \n Start a user database session: \n $ postgres -D pg >/dev/null & \n Run the website server: \n $ lein run\n``` \n License \n Copyright 2014 Chris Cummins. \n pip-db is free software: you can redistribute it and/or modify it\nunder the terms of the GNU General Public License as published by the\nFree Software Foundation, either version 3 of the License, or (at your\noption) any later version. \n pip-db is distributed in the hope that it will be useful, but WITHOUT\nANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\nFITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\nfor more details. \n You should have received a copy of the GNU General Public License\nalong with pip-db.  If not, see  http://www.gnu.org/licenses/ .", 'shit.js \n Secure Host Intrusion Technology \n \n There are only two hard things in Computer Science: cache invalidation and\nnaming things. \n -- Phil Karlton \n \n A host-based intrusion detection daemon for Linux platforms which performs\nsignature-based anomaly detection of the Linux filesystem.', 'liquid-src - Fluid links management \n Managing a large collection of links can be a pain. What if your super-awesome\nclient keeps sending you new YouTube promos to show-off on their websites? Do\nthey actually expect you to  sed  your way through each of their 150 HTML files?\nWhat if there was a way to keep just a single copy of your URLs which can be\nreferenced from anywhere? \n \n \n So that this: \n ```html\n \n \n ``` \n Looks like this: \n ```html\n \n \n ``` \n You create one single list of URLs with unique names for them, then you use\nthose names in your HTML. \n Getting Started \n To start with, we create our list of named URLs: \n js\n$.data(document.body, \'liquid-src\', {\n  "iframe": {\n    "gangnam-style":     "http://www.youtube.com/embed/9bZkp7q19f0",\n    "higgs-boson-blues": "http://www.youtube.com/embed/1GWsdqCYvgw"\n  },\n  "img": {\n    "google-logo":       "https://www.google.co.uk/images/srpr/logo11w.png"\n  },\n  "script": {\n    "jquery-ui":         "//ajax.googleapis.com/ajax/libs/jqueryui/1.10.3/jquery-ui.min.js"\n  }\n}); \n Let\'s call it  links.js . Now, we simply include the (extremely lightweight)\n JavaScript library \nand include it in your HTML files (note, requires jQuery), along with the list\nof links we just made: \n ```html \n \n \n ``` \n Now, any time you want to use one of your links, simple add the  liquid-src \nclass to the element, and instead of setting the  src  attribute to point to a\nURL, set a  data-src  attribute to the name which you gave the link. For\nexample, to include the Google logo from the above example: \n html\n<img class="liquid-src" data-src="google-logo"/> \n It works on any kind of tag with a  src=  attribute! \n ```html \n  Embedded YouTube videos  \n \n  JavaScript libraries  \n \n \n``` \n For further uses, see the\n examples section . \n License \n Fluid-src is licensed under the Apache License, Version 2.0.  View the license\nfile . \n Copyright 2013 Chris Cummins.', 'Computer Science Theory and Practise \n This is a personal repository containing notes and implementations of computer\nscience data structures and algorithms, created for my own benefit and personal\ngain. Anyone is free to fork and contribute.', 'This is the data for my blog \n It is automatically transformed by  Jekyll \ninto a static site whenever I push this repository to GitHub. \n Setup \n Install the dependencies. On Linux: \n sh\n$ sudo apt install ruby ruby-dev ffmpeg\n$ sudo gem install bundler\n$ bundle install \n On macOS: \n sh\n$ brew install ruby ffmpeg\n$ sudo "$(brew --prefix)/opt/ruby/bin/gem" install -n "$(brew --prefix)/bin" bundler:2.1.4\n$ "$(brew --prefix)/bin/bundle" install \n Running the site locally \n ```sh \n Generate site files \n $ make all \n Serve locally on http://127.0.0.1:4000 \n $ make run\n``` \n Pushing to staging site \n sh\n$ git remote add staging git@github.com:ChrisCummins/staging.git\n$ git push staging master:gh-pages \n Check the staging site at http://chriscummins.cc/staging \n License \n The website code is\n MIT Licensed ,\nbut the website  content  is not. The following directories and their contents\nare copyright Chris Cummins:\n*  _posts/ \n*  images/ ,\n*  img/ ,\n*  pub/ \n*  u/ . \n You may not reuse anything therein without my permission.', 'MSc Thesis \n To build the sources, run the following commands from this directory: \n ./autogen.sh; ./configure; make \n The configure script will attempt to adapt to your local build\nenvironment and determine what can and cannot be built with the tools\navailable. Take a look at the output of  ./configure  to see if\nanything useful is being missed.', 'A benchmarking timer \n \n \n The number of "moving parts" in a modern software stack means that\nprogram execution time is non-deterministic. As a result, performance\nevaluation should take a statistically rigorous approach, using the\nresults of multiple iterations to reduce the impact of outliers. \n This is a utility for adding statistical rigour to your program\nperformance evaluations. It repeatedly executes a program for a set\namount of time, then reports mean, lower and upper confidence values. \n Here\'s how you get started: \n $ srtime ./my_benchmark\n95% confidence values from 37 iterations:\n103.665 103.699 103.733 \n Features \n \n Millisecond precision timing of programs. \n User defined amount of time to collect results for (e.g. 60\n  seconds), or a minimum number of iterations to perform (e.g. 100). \n Results can be displayed graphically using the  -g  flag. \n User defined confidence intervals, output precision, and output\n  format. \n Can act as a filter for timing critical sections of a program based\n  on its output. \n Supports flushing the host system caches before every invocation of\n  the target program. \n \n For a list of all of the program features, see  srtime --help . \n Installation \n Install  python  >= 2.6, and the\n python-setuptools \npackage. Then, from this directory, run: \n sudo python setup.py install \n Contribute \n \n Source Code: http://github.com/ChrisCummins/srtime \n Issue Tracker: https://github.com/ChrisCummins/srtime/issues \n \n Patches welcome! \n Support \n If you are having issues, please get in touch: chrisc.101@gmail.com.', 'SublimeLinter-contrib-write-good \n \n This linter plugin for  SublimeLinter  provides an interface to  write-good . It will be used with files that have English prose. \n Installation \n SublimeLinter 3 must be installed in order to use this plugin. If SublimeLinter 3 is not installed, please follow the instructions  here . \n Linter installation \n Before using this plugin, you must ensure that  write-good  is installed on your system. To install  write-good , do the following: \n \n \n Install  Node.js  (and  npm  on Linux). \n \n \n Install  write-good  by typing the following in a terminal:\n    npm install -g write-good \n \n \n If you are using  nvm  and  zsh , ensure that the line to load  nvm  is in  .zshenv  and not  .zshrc . \n \n \n If a local version of write-good is available it will be used before trying to find the system version. \n \n \n Linter configuration \n In order for  write-good  to be executed by SublimeLinter, you must ensure that its path is available to SublimeLinter. Before going any further, please read and follow the steps in  “Finding a linter executable”  through “Validating your PATH” in the documentation. \n Once you have installed and configured  write-good , you can proceed to install the SublimeLinter-contrib-write-good plugin if it is not yet installed. \n Plugin installation \n Please use  Package Control  to install the linter plugin. This will ensure that the plugin will be updated when new versions are available. If you want to install from source so you can modify the source code, you probably know what you are doing so we won’t cover that here. \n To install via Package Control, do the following: \n \n \n Within Sublime Text, bring up the  Command Palette  and type  install . Among the commands you should see  Package Control: Install Package . If that command is not highlighted, use the keyboard or mouse to select it. There will be a pause of a few seconds while Package Control fetches the list of available plugins. \n \n \n When the plugin list appears, type  write-good . Among the entries you should see  SublimeLinter-contrib-write-good . If that entry is not highlighted, use the keyboard or mouse to select it. \n \n \n Settings \n For general information on how SublimeLinter works with settings, please see  Settings . For information on generic linter settings, please see  Linter Settings . \n Contributing \n If you would like to contribute enhancements or fixes, please do the following: \n \n Fork the plugin repository. \n Hack on a separate topic branch created from the latest  master . \n Commit and push the topic branch. \n Make a pull request. \n Be patient.  ;-) \n \n Please note that modifications should follow these coding guidelines: \n \n Indent is 4 spaces. \n Code should pass flake8 and pep257 linters. \n Vertical whitespace helps readability, don’t be afraid to use it. \n Please use descriptive variable names, no abbreviations unless they are very well known. \n \n Thank you for helping out!', 'Ray Tracer \n A fast parallelised ray tracer written in pure C++11, with support for\nsoft lighting, adaptive anti-aliasing, perspective and depth of field. \n Installation \n Build with  make . Requires a C++11 capable compiler\n(e.g.  g++ . \n Usage \n Include the  rt/rt.h  header and link against the compiled\n src/librt.so  library. For example programs, see\n examples/example1.cc  and  examples/example2.cc . \n Features \n \n Diffuse (Lambert) and specular (Phong) shading, and recursive\nreflections. \n Fast anti-aliasing using adaptive supersampling. \n Camera abstraction providing focal lengths and aperture. \n Automatic scene code generation using\n   mkscene . \n \n License \n Copyright 2015 Chris Cummins. \n Released under the terms of the\n GNU General Public License, Version 3 . \n This is free software: you can redistribute it and/or modify it under\nthe terms of the GNU General Public License as published by the Free\nSoftware Foundation, either version 3 of the License, or (at your\noption) any later version. \n This software is distributed in the hope that it will be useful, but\nWITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU\nGeneral Public License for more details.', '\n  downloads counter  \n \n \n \n  pypi version  \n \n \n \n  Better code  \n \n \n \n  Travis CI  \n \n \n \n  commit counter  \n \n \n \n  repo size  \n \n \n \n  license  \n \n \n \n sh\n$ python -m pip install -U labm8 \n Copyright 2014-2020 Chris Cummins  chrisc.101@gmail.com . \n Released under the terms of the Apache 2.0 license. See\n LICENSE  for details. \n Deployment Instructions \n \n \n Set the desired version number in  //:version.txt . \n \n \n Export the source tree to the public  github repository : \n \n \n sh\n$ bazel run //labm8/py:export \n \n Deploy a new versioned release of the python package to  pypi : \n \n sh\n$ bazel run //labm8/py:export_python_pip -- --release_type=release', 'OmniTune \n OmniTune is an extensible, distributed autotuner developed by Chris\nCummins as part of his MSc research project at the University of\nEdinburgh. For queries, contact chrisc.101@gmail.com.', '\n \n \n Update:  I have finished my PhD! Please click above for my dissertation,\nwhich I\'m delighted to say won the award for\n PhD Award for Best Disseration in Scotland 2019-2020 .\nThis repository is now in an archival state. \n My PhD \n  repo size  \n \n \n \n  commit counter  \n \n \n \n  Better code  \n \n \n \n  Travis CI  \n \n \n \n A monolithic repository for (almost) everything I have done while at the University of Edinburgh. Living an #open life. \n Publications \n \n Chris Cummins, Zacharias V. Fisches, Tal Ben-Nun, Torsten Hoefler, Hugh Leather. " ProGraML: Graph-based Deep Learning for Program Optimization and Analysis ".  arXiv:2003.10536 .\n    [source code] . \n Chris Cummins, " Deep Learning for Compilers ". PhD thesis.\n    [source code] .\n   Build command:  $ bazel build //docs/thesis . \n Chris Cummins, Pavlos Petoumenos, Alastair Murray, Hugh Leather.\n   " Compiler Fuzzing through Deep Learning ".\n   ISSTA \'18.\n    [source code] .\n    [pdf] .\n   Build command:  $ bazel build //docs/2018_07_issta . \n Chris Cummins, Pavlos Petoumenos, Alastair Murray, Hugh Leather.\n   " DeepSmith: Compiler Fuzzing through Deep Learning ".\n   ACACES \'18.\n    [source code] .\n    [pdf] .\n   Build command:  $ bazel build //docs/2018_07_acaces . \n Chris Cummins, Pavlos Petoumenos, Zheng Wang, Hugh Leather.\n   " End-to-end Deep Learning of Optimization Heuristics ".\n   PACT \'17.\n    [source code] .\n    [pdf] .\n   Build command:  $ bazel build //docs/2017_09_pact . \n Chris Cummins, Pavlos Petoumenos, Zheng Wang, Hugh Leather.\n   " Synthesizing Benchmarks for Predictive Modeling ".\n   CGO \'17.\n    [source code] .\n    [pdf] .\n    [acm] .\n   Build command:  $ bazel build //docs/2017_02_cgo . \n Chris Cummins, Pavlos Petoumenos, Michel Steuwer, Hugh Leather.\n   " Autotuning OpenCL Workgroup Sizes ". ACACES \'16.\n    [source code] .\n   Build command:  $ bazel build //docs/2016_07_acaces . \n Chris Cummins, Pavlos Petoumenos, Michel Steuwer, Hugh Leather.\n   " Towards Collaborative Performance Tuning of Algorithmic Skeletons ".\n   HLPGPU \'16, HiPEAC.\n    [source code] .\n    [pdf] .\n   Build command:  $ bazel build //docs/2016_01_hlpgpu . \n Chris Cummins, Pavlos Petoumenos, Michel Steuwer, Hugh Leather.\n   " Autotuning OpenCL Workgroup Size for Stencil Patterns ".\n   ADAPT \'16, HiPEAC.\n    [source code] .\n    [pdf] .\n    [arxiv] .\n   Build command:  $ bazel build //docs/2016_01_adapt . \n Chris Cummins. " Autotuning Stencils Codes with Algorithmic Skeletons ".\n   MSc Thesis, 2015. The University of Edinburgh.\n    [source code] .\n   Build command:  $ bazel build //docs/2015_08_msc_thesis . \n \n Talks \n \n Chris Cummins. " Compiler Fuzzing through Deep Learning ", 3rd August, 2018.\n   Codeplay, Edinburgh, Scotland.\n    [files] .\n    [slides] . \n Chris Cummins. " Machine Learning for Compilers ", 20th July, 2018.\n   Workshop on Introspective Systems for Automatically Generating Tests (ISAGT),\n   Amsterdam, Netherlands.\n    [files] .\n    [pdf] . \n Chris Cummins. " Compiler Fuzzing through Deep Learning ", 16th July, 2018.\n   ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA), Amsterdam, Netherlands.\n    [files] .\n    [slides] . \n Chris Cummins. " End-to-end Deep Learning of Optimization Heuristics ", 23rd March, 2018.\n   Facebook, Menlo Park.\n    [files] .\n    [slides] . \n Chris Cummins. " End-to-end Deep Learning of Optimization Heuristics ", 2nd Feb, 2018.\n   Google, Mountain View.\n    [files] .\n    [slides] . \n Chris Cummins. " Second Year Progression Review ", 18th Dec, 2017.\n   The University of Edinburgh, Scotland.\n    [files] .\n    [pdf] . \n Chris Cummins. " End-to-end Deep Learning of Optimization Heuristics ", 4th Oct, 2017.\n   The University of Edinburgh, Scotland.\n    [files] .\n    [slides] . \n Chris Cummins. " End-to-end Deep Learning of Optimization Heuristics ", 12th Sep, 2017.\n   International Conference on Parallel Architectures and Compilation Techniques (PACT), Portland, Oregon, USA.\n    [files] .\n    [slides] . \n Chris Cummins. " Deep Learning in Compilers ", 14th Jun, 2017.\n   The University of Edinburgh, Scotland.\n    [files] .\n    [pdf] .  [transcript] . \n Chris Cummins. " Using Deep Learning to Generate Human-like Code ", 22nd April, 2017.\n   Scottish Programming Languages Seminar, University of St.\n   Andrews, Scotland.\n    [files] .\n    [pdf] . \n Chris Cummins. " Synthesizing Benchmarks for Predictive Modeling ", 6th Febuary, 2017.\n   International Symposium on Code Generationand Optimization (CGO), Austin, Texas, USA.\n    [files] .\n    [slides] . \n Chris Cummins. " Machine Learning & Compilers ", 9th September, 2016.\n   Codeplay, Edinburgh, Scotland.\n    [files] .\n    [slides] . \n Chris Cummins. " Building an AI that Codes ", 22nd July, 2016.\n   Ocado Technology, Hatfield, England.\n    [files] .\n    [pdf] . \n Chris Cummins.\n   " All the OpenCL on GitHub: Teaching an AI to code, one character at a time ", 19th May, 2016.\n   Amazon Development Centre, Edinburgh, Scotland.\n    [files] .\n    [pdf] . \n Chris Cummins. " Autotuning and Algorithmic Skeletons ", Wed 10th Feb, 2016.\n   The University of Edinburgh, Scotland.\n    [files] .\n    [pdf] . \n Chris Cummins. " Towards Collaborative Performance Tuning of Algorithmic Skeletons ", Tues 19th Jan, 2016.\n   HLPGPU, HiPEAC, Prague.\n    [files] .\n    [pdf] . \n Chris Cummins. " Autotuning OpenCL Workgroup Size for Stencil Patterns ", Mon 18th Jan, 2016.\n   ADAPT, HiPEAC, Prague.\n    [files] .\n    [pdf] . \n Chris Cummins.\n   " Towards Collaborative Performance Tuning of Algorithmic Skeletons ", Thurs 14th Jan, 2016.\n   The University of Edinburgh, Scotland.\n    [files] .\n    [pdf] . \n \n Misc \n \n Curriculum Vitae .\n    [source code] .\n    [pdf] .\n    [html] .\n   Build command:  $ bazel build //docs/cv . \n Chris Cummins, Pavlos Petoumenos, Michel Steuwer, Hugh Leather.\n   " Collaborative Autotuning of Algorithmic Skeletons for GPUs and CPUs ".\n   Incomplete journal version of ADAPT and HLPGPU papers.\n    [source code] .\n   Build command:  $ bazel build //docs/2016_12_wip_taco . \n Chris Cummins. " Deep Learning for Compilers ". PhD First Year Review\n   Document, 2016.\n    [source code] .\n   Build command:  $ bazel build //docs/2016_11_first_year_review . \n Chris Cummins, Hugh Leather. " Autotuning OpenCL Workgroup Sizes ".\n   Rejected submission for PACT\'16 Student Research Competition.\n    [source code] .\n   Build command:  $ bazel build //docs/2016_07_pact . \n Chris Cummins, Pavlos Petoumenos, Michel Steuwer, Hugh Leather.\n   " Autotuning OpenCL Workgroup Sizes ".\n   Submission for PLDI\'16 Student Poster Session.\n    [source code] .\n   Build command:  $ bazel build //docs/2016_06_pldi . \n Chris Cummins. " Autotuning and Skeleton-aware Compilation ".\n   PhD Progression Review, 2015.\n    [source code] .\n   Build command:  $ bazel build //docs/2015_09_progression_review . \n \n Building the code \n See  INSTALL.md  and  CONTRIBUTING.md .', "torch-rnn \n torch-rnn provides high-performance, reusable RNN and LSTM modules for torch7, and uses these modules for character-level\nlanguage modeling similar to  char-rnn . \n You can find documentation for the RNN and LSTM modules  here ; they have no dependencies other than  torch \nand  nn , so they should be easy to integrate into existing projects. \n Compared to char-rnn, torch-rnn is up to  1.9x faster  and uses up to  7x less memory . For more details see \nthe  Benchmark  section below. \n Installation \n System setup \n You'll need to install the header files for Python 2.7 and the HDF5 library. On Ubuntu you should be able to install\nlike this: \n bash\nsudo apt-get -y install python2.7-dev\nsudo apt-get install libhdf5-dev \n Python setup \n The preprocessing script is written in Python 2.7; its dependencies are in the file  requirements.txt .\nYou can install these dependencies in a virtual environment like this: \n ```bash\nvirtualenv .env                  # Create the virtual environment\nsource .env/bin/activate         # Activate the virtual environment\npip install -r requirements.txt  # Install Python dependencies \n Work for a while ... \n deactivate                       # Exit the virtual environment\n``` \n Lua setup \n The main modeling code is written in Lua using  torch ; you can find installation instructions\n here . You'll need the following Lua packages: \n \n torch/torch7 \n torch/nn \n torch/optim \n lua-cjson \n torch-hdf5 \n \n After installing torch, you can install / update these packages by running the following: \n ```bash \n Install most things using luarocks \n luarocks install torch\nluarocks install nn\nluarocks install optim\nluarocks install lua-cjson \n We need to install torch-hdf5 from GitHub \n git clone https://github.com/deepmind/torch-hdf5\ncd torch-hdf5\nluarocks make hdf5-0-0.rockspec\n``` \n CUDA support (Optional) \n To enable GPU acceleration with CUDA, you'll need to install CUDA 6.5 or higher and the following Lua packages:\n-  torch/cutorch \n-  torch/cunn \n You can install / update them by running: \n bash\nluarocks install cutorch\nluarocks install cunn \n OpenCL support (Optional) \n To enable GPU acceleration with OpenCL, you'll need to install the following Lua packages:\n-  cltorch \n-  clnn \n You can install / update them by running: \n bash\nluarocks install cltorch\nluarocks install clnn \n OSX Installation \n Jeff Thompson has written a very detailed installation guide for OSX that you  can find here . \n Usage \n To train a model and use it to generate new text, you'll need to follow three simple steps: \n Step 1: Preprocess the data \n You can use any text file for training models. Before training, you'll need to preprocess the data using the script\n scripts/preprocess.py ; this will generate an HDF5 file and JSON file containing a preprocessed version of the data. \n If you have training data stored in  my_data.txt , you can run the script like this: \n bash\npython scripts/preprocess.py \\\n  --input_txt my_data.txt \\\n  --output_h5 my_data.h5 \\\n  --output_json my_data.json \n This will produce files  my_data.h5  and  my_data.json  that will be passed to the training script. \n There are a few more flags you can use to configure preprocessing;  read about them here \n Step 2: Train the model \n After preprocessing the data, you'll need to train the model using the  train.lua  script. This will be the slowest step.\nYou can run the training script like this: \n bash\nth train.lua -input_h5 my_data.h5 -input_json my_data.json \n This will read the data stored in  my_data.h5  and  my_data.json , run for a while, and save checkpoints to files with \nnames like  cv/checkpoint_1000.t7 . \n You can change the RNN model type, hidden state size, and number of RNN layers like this: \n bash\nth train.lua -input_h5 my_data.h5 -input_json my_data.json -model_type rnn -num_layers 3 -rnn_size 256 \n By default this will run in GPU mode using CUDA; to run in CPU-only mode, add the flag  -gpu -1 . \n To run with OpenCL, add the flag  -gpu_backend opencl . \n There are many more flags you can use to configure training;  read about them here . \n Step 3: Sample from the model \n After training a model, you can generate new text by sampling from it using the script  sample.lua . Run it like this: \n bash\nth sample.lua -checkpoint cv/checkpoint_10000.t7 -length 2000 \n This will load the trained checkpoint  cv/checkpoint_10000.t7  from the previous step, sample 2000 characters from it,\nand print the results to the console. \n By default the sampling script will run in GPU mode using CUDA; to run in CPU-only mode add the flag  -gpu -1  and\nto run in OpenCL mode add the flag  -gpu_backend opencl . \n There are more flags you can use to configure sampling;  read about them here . \n Benchmarks \n To benchmark  torch-rnn  against  char-rnn , we use each to train LSTM language models for the tiny-shakespeare dataset\nwith 1, 2 or 3 layers and with an RNN size of 64, 128, 256, or 512. For each we use a minibatch size of 50, a sequence \nlength of 50, and no dropout. For each model size and for both implementations, we record the forward/backward times and \nGPU memory usage over the first 100 training iterations, and use these measurements to compute the mean time and memory \nusage. \n All benchmarks were run on a machine with an Intel i7-4790k CPU, 32 GB main memory, and a Titan X GPU. \n Below we show the forward/backward times for both implementations, as well as the mean speedup of  torch-rnn  over \n char-rnn . We see that  torch-rnn  is faster than  char-rnn  at all model sizes, with smaller models giving a larger\nspeedup; for a single-layer LSTM with 128 hidden units, we achieve a  1.9x speedup ; for larger models we achieve about\na 1.4x speedup. \n \n Below we show the GPU memory usage for both implementations, as well as the mean memory saving of  torch-rnn  over\n char-rnn . Again  torch-rnn  outperforms  char-rnn  at all model sizes, but here the savings become more significant for\nlarger models: for models with 512 hidden units, we use  7x less memory  than  char-rnn . \n \n TODOs \n \n Get rid of Python / JSON / HDF5 dependencies? \n", 'Human or Robot?', '\n \n \n \n \n \n \n  Better code  \n \n \n \n  License  \n \n \n \n \n  Maintainer:  fivosts \n \n CLgen  is an open source application for generating runnable programs using\ndeep learning. CLgen  learns  to program using neural networks which model the\nsemantics and usage from large volumes of program fragments, generating\nmany-core OpenCL programs that are representative of, but  distinct  from, the\nprograms it learns from. \n \n Getting Started \n First check out  INSTALL.md  for instructions on getting the build\nenvironment set up . \n Then build CLgen using: \n sh\n$ bazel build -c opt //deeplearning/clgen \n Use our tiny example dataset to train and sample your first CLgen model: \n sh\n$ bazel-bin/deeplearning/clgen -- \\\n    --config $PWD/deeplearning/clgen/tests/data/tiny/config.pbtxt \n \n What next? \n CLgen is a tool for generating source code. How you use it will depend entirely\non what you want to do with the generated code. As a first port of call I\'d\nrecommend checking out how CLgen is configured. CLgen is configured through a\nhandful of\n protocol buffers  defined in\n //deeplearning/clgen/proto .\nThe  clgen.Instance  message type\ncombines a  clgen.Model  and\n clgen.Sampler  which define the\nway in which models are trained, and how new programs are generated,\nrespectively. You will probably want to assemble a large corpus of source code\nto train a new model on - I have  tools  which\nmay help with that. You may also want a means to execute arbitrary generated\ncode - as it happens I have  tools  for that too. :-) Thought of a\nnew use case? I\'d love to hear about it! \n Resources \n Presentation slides: \n \n \n \n Publication\n "Synthesizing Benchmarks for Predictive Modeling" \n(CGO\'17). \n Jupyter notebook \ncontaining experimental evaluation of an early version of CLgen. \n My documentation sucks. Don\'t be afraid to get stuck in and start\n reading the code! \n License \n Copyright 2016-2020 Chris Cummins  chrisc.101@gmail.com . \n Released under the terms of the GPLv3 license. See\n LICENSE  for details.', '```\n            ::::::::\n            ::::::::                          ::::\n            ::::::::                       :::::::\n            ::::::::                       :::::::\n            :::::::                        :::::::\n    :::::::::::::::    :::::::::::   :::::::::::::::::::\n  :::::::::::::::::  ::::::::::::::: :::::::::::::::::::\n :::::::::::::::::: ::::::::::::::::::::::::::::::::::::\n::::::::::::::::::: ::::::::::::::::::::::::::::::::::::     ::::::::::::::::\n::::::::    ::::::: ::::::     ::::::      :::::::           ::::::::::::::::\n:::::::     ::::::: ::::::     ::::::      :::::::           ::::::::::::::::\n:::::::     ::::::: ::::::     ::::::      :::::::\n:::::::     ::::::: ::::::     ::::::      :::::::    ::::::\n:::::::::::::::::::::::::::::::::::::      :::::::::::::::::\n :::::::::Chris:Cummins::::::::::::::      :::::::::::::::::\n  :::::::::::::::::: :::::::::::::::         :::::::::::::::\n   :::::::::   :::::   :::::::::::             ::::::::::: \n ::::::::::::::::    ::::  :::::::\n \n ::::::::::::::::::  :::::: :::::::\n  ::  Usage: ./run  ::  ::::  :::::::\n  ::::::::::::::::::::        :::::::\n  :::::::       :::::::::::::  ::::::     ::::::::::::        ::::::::::\n  :::::::             :::::::  ::::::   ::::::::::::::::    :::::::::::::\n ::::::::::::::        ::::::  ::::::  :::::::::::::::::::::::::::::::::::\n ::::::::::::::        ::::::  :::::: ::::::::     ::::::::::::::::::::::::\n ::::::::::::::        ::::::  :::::: :::::::::::::::::::: :::::::  ::::::\n ::::::::::::::        ::::::  :::::: ::::MIT:License::::    ::::::::\n  :::::::              ::::::  :::::: ::::::::::::::::::        ::::::::\n  :::::::              ::::::  :::::: :::::::::           ::::::   :::::::\n :::::::::            ::::::::::::::::::::::::::          :::::::::::::::::\n :::::::::            :::::::::::::::: :::::::::::::::::  ::::::::::::::::\n :::::::::            ::::::::::::::::  ::::::::::::::::   ::::::::::::::\n :::::::::            ::::::::::::::::    ::::::::::::::    :::::::::::\n```\n \n \n', 'Towards Collaborative Performance Tuning of Algorithmic Skeletons \n Chris Cummins ,\n Pavlos Petoumenos ,\n Michel Steuwer ,\n Hugh Leather . \n Abstract \n \n The physical limitations of microprocessor design have forced the industry\ntowards increasingly heterogeneous designs to extract performance. This trend\nhas not been matched with adequate software tools, leading to a growing\ndisparity between the availability of parallelism and the ability for\napplication developers to exploit it. \n Algorithmic skeletons simplify parallel programming by providing high-level,\nreusable patterns of computation. Achieving performant skeleton\nimplementations is a difficult task; skeleton authors must attempt to\nanticipate and tune for a wide range of architectures and use cases. This\nresults in implementations that target the general case and cannot provide the\nperformance advantages that are gained from tuning low level optimization\nparameters. Autotuning combined with machine learning offers promising\nperformance benefits in these situations, but the high cost of training and\nlack of available tools limits the practicality of autotuning for real world\nprogramming. We believe that performing autotuning at the level of the\nskeleton library can overcome these issues. \n In this work, we present OmniTune - an extensible and distributed framework\nfor dynamic autotuning of optimization parameters at runtime. OmniTune uses a\nclient-server model with a flexible API to support machine learning enabled\nautotuning. Training data is shared across a network of cooperating systems,\nusing a collective approach to performance tuning. \n We demonstrate the practicality of OmniTune in a case study using the\nalgorithmic skeleton library SkelCL. By automatically tuning the workgroup\nsize of OpenCL Stencil skeleton kernels, we show that that static tuning\nacross a range of GPUs and programs can achieve only 26% of the optimal\nperformance, while OmniTune achieves 92% of this maximum, equating to an\naverage 5.65x speedup. OmniTune achieves this without introducing a\nsignificant runtime overhead, and enables portable, cross-device and cross-\nprogram tuning. \n \n Presented  High-Level Programming for Heterogeneous and Hierarchical Parallel Systems.\nPrague, Czech Republic, Tuesday, Jan 19th 2016. Co-Located with HiPEAC 2016. \n @inproceedings{cummins2016b,\n    author    = "Cummins, Chris and Petoumenos, Pavlos and Steuwer, Michel and Leather, Hugh",\n    title     = "Towards Collaborative Performance Tuning of Algorithmic Skeletons",\n    booktitle = "High-Level Programming for Heterogeneous and Hierarchical Parallel Systems (HLPGPU)",\n    year      = "2016",\n}', 'Autotuning OpenCL Workgroup Size for Stencil Patterns \n Chris Cummins ,\n Pavlos Petoumenos ,\n Michel Steuwer ,\n Hugh Leather . \n Abstract \n \n Selecting an appropriate workgroup size is critical for the performance of\nOpenCL kernels, and requires knowledge of the underlying hardware, the data\nbeing operated on, and the implementation of the kernel. This makes portable\nperformance of OpenCL programs a challenging goal, since simple heuristics and\nstatically chosen values fail to exploit the available performance. To address\nthis, we propose the use of machine learning-enabled autotuning to\nautomatically predict workgroup sizes for stencil patterns on CPUs and multi-\nGPUs. \n We present three methodologies for predicting workgroup sizes. The first,\nusing classifiers to select the optimal workgroup size. The second and third\nproposed methodologies employ the novel use of regressors for performing\nclassification by predicting the runtime of kernels and the relative\nperformance of different workgroup sizes, respectively. We evaluate the\neffectiveness of each technique in an empirical study of 429 combinations of\narchitecture, kernel, and dataset, comparing an average of 629 different\nworkgroup sizes for each. We find that autotuning provides a median 3.79x\nspeedup over the best possible fixed workgroup size, achieving 94% of the\nmaximum performance. \n \n Presented  The 6th International Workshop on Adaptive Self-tuning Computing\nSystems. Prague, Czech Republic, Monday, Jan 18th 2016. Co-Located with HiPEAC\n2016. \n @inproceedings{cummins2016a,\n    author    = "Cummins, Chris and Petoumenos, Pavlos and Steuwer, Michel and Leather, Hugh",\n    title     = "Autotuning OpenCL Workgroup Size for Stencil Patterns",\n    booktitle = "The 6th International Workshop on Adaptive Self-tuning Computing Systems (ADAPT)",\n    arxivId   = "1511.02490v3",\n    year      = "2016",\n}', "Synthesizing Benchmarks for Predictive Modeling \n Chris Cummins ,\n Pavlos Petoumenos ,\n Zheng Wang ,\n Hugh Leather . \n Winner of Best Paper Award CGO'17 \n \n \n \n \n \n \n Abstract \n \n Predictive modeling using machine learning is an effective method for building\ncompiler heuristics, but there is a shortage of benchmarks. Typical machine\nlearning experiments outside of the compilation field train over thousands or\nmillions of examples. In machine learning for compilers, however, there are\ntypically only a few dozen common benchmarks available. This limits the\nquality of learned models, as they have very sparse training data for what are\noften high-dimensional feature spaces. What is needed is a way to generate an\nunbounded number of training programs that finely cover the feature space. At\nthe same time the generated programs must be similar to the types of programs\nthat human developers actually write, otherwise the learning will target the\nwrong parts of the feature space. \n We mine open source repositories for program fragments and apply deep learning\ntechniques to automatically construct models for how humans write programs. We\nthen sample the models to generate an unbounded number of runnable training\nprograms, covering the feature space ever more finely. The quality of the\nprograms is such that even human developers struggle to distinguish our\ngenerated programs from hand-written code. \n We use our generator for OpenCL programs, CLgen, to automatically synthesize\nthousands of programs and show that learning over these improves the\nperformance of a state of the art predictive model by 1.27x. In\naddition, the fine covering of the feature space automatically exposes\nweaknesses in the feature design which are invisible with the sparse training\nexamples from existing benchmark suites. Correcting these weaknesses further\nincreases performance by 4.30x. \n \n Keywords   Synthetic program generation, OpenCL, Benchmarking, Deep Learning, GPUs \n @inproceedings{cummins2017a,\n  title={Synthesizing Benchmarks for Predictive Modeling},\n  author={Cummins, Chris and Petoumenos, Pavlos and Wang, Zheng and Leather, Hugh},\n  booktitle={CGO},\n  year={2017},\n  organization={IEEE}\n} \n License \n The code for this paper (everything in the directory  code ) is released under\nthe terms of the GPLv3 license. See  LICENSE  for details. Everything\nelse (i.e. the LaTeX sources and data sets) are unlicensed, please contact\nChris Cummins  chrisc.101@gmail.com  before using. \n Acknowledgements \n \n @AdamHarries  for reviewing early drafts of\n  the paper. \n @jcjohnson  for guidance on hacking the LSTM. \n Volunteers at  @codeplaysoftware  and\n   University of Edinburgh  for particpating in the\n  qualitative evaluation. \n EPSRC grants\n  EP/L01503X/1 ( CDT in Pervasive Parallelism ),\n  EP/L000055/1 (ALEA),\n  EP/M01567X/1 (SANDeRs),\n  EP/M015823/1,\n  and EP/M015793/1 (DIVIDEND) \n", 'Shutterbug \n Shutterbug splits a large collection of photos into a series of DVD-sized folders for burning to disc, and provides a mechanism to go from these disc backups back to the original directory structure. \n Shutterbug is paranoid about data loss and corruption, and has some coping\nstrategies: \n \n It maintains a 1-1 mappings so that 1 input file = 1 file on a disc. This\n  means if a file becomes corrupted, you lose one image. Other approaches which\n  do not respect file boundaries lead to losing multiple images in one go. \n It randomizes the order of the files on the discs so that if a disc is lost,\n  you end up with lots of tiny gaps in your photo library, not one big one. \n It validates your files after restoring from backup, so you have a warning of\n  data corruption. Note this does not  prevent  data loss, only  discovers \n  it.  The best way to prevent irrecoverable data loss is to make more copies of\n  your data to begin with. \n \n Usage \n Requirements \n \n Python 3. \n \n Installation \n $ pip3 install shutterbug \n Archiving to disc \n To backup your photo library in  ~/Pictures/2016  to 4.7GB DVDs, split the folder into "chunks" using shutterbug: \n ```\n$ mkdir ~/chunks && cd ~/chunks\n$ shutterbug ~/Pictures/2016 --gzip\nchunk_001/ae3d47f87af176b74e1ec30599a7b31a.jpg.gz 4.93MB -> 4.90MB\nchunk_001/631600d1e11339794e81d75f104e9f19.jpg.gz 7.40MB -> 7.38MB\nchunk_001/130c52fe396237a59500a61b8101ff55.jpg.gz 6.79MB -> 6.77MB\nchunk_001/27fc10914e18b0e1b303c05a800c299d.jpg.gz 5.73MB -> 5.70MB\n...\nWrote chunk_001/MANIFEST.txt\nWrote chunk_001/README.txt\nchunk_001 has 723 files, size 4662.40 MB (99.2% of maximum size) \n chunk_002/5c9ce3b8071207ab702766ac2be76f10.jpg.gz 6.13MB -> 6.11MB\nchunk_002/bc17480a318e7ba9a3e4e2e57538917d.jpg.gz 9.63MB -> 9.60MB\n...\n``` \n Burn each of the resulting folders in  ~/chunks  to DVDs. \n Restoring from disc \n Copy each chunk from your DVDs back to disc, e.g.  ~/import/chunk_001 ,\n ~/import/chunk_002  etc. Restore the original file structure by running\nshutterbug from the output directory: \n $ mkdir ~/Pictures/2016 && cd ~/Pictures/2016\n$ shutterbug --unpack ~/import/chunk_*\n$ ~/Pictures/mkbackup.py ../compressed/* -u\n~/import/chunk_001/ae3d47f87af176b74e1ec30599a7b31a.jpg.gz -> ./2016-12 NYC (2434 of 5025).jpg\n~/import/chunk_001/631600d1e11339794e81d75f104e9f19.jpg.gz -> ./2016-12 NYC (4411 of 5025).jpg\n~/import/chunk_001/130c52fe396237a59500a61b8101ff55.jpg.gz -> ./2016-12 NYC (301 of 5025).jpg\n... \n Shutterbug will print warnings for files in case the size or contents have\nchanged.', "cldrive - Run arbitrary OpenCL kernels \n  Travis CI  \n \n \n \n  Better code  \n \n \n \n  License  \n \n \n \n cldrive  is a tool for running\narbitrary OpenCL kernels to record their runtimes and outputs. It reads OpenCL\nkernels from an input file, and for each, generates random inputs\n(parameterized by a given size), runs the kernel and records its execution time\nand outputs. It was developed as part of my work on\n Deep Learning benchmark synthesis , and\nhas been used in the following publications: \n \n Cummins, C., Petoumenos, P., Zang, W., & Leather, H. (2017). Synthesizing\n   Benchmarks for Predictive Modeling. CGO. IEEE. \n Cummins, C., Petoumenos, P., Wang, Z., & Leather, H. (2017). End-to-end\n   Deep Learning of Optimization Heuristics. PACT. IEEE. \n Ben-Nun, T., Jakobovits, A. S., & Hoefler, T. (2018). Neural Code\n   Comprehension: A Learnable Representation of Code Semantics. NeurIPS. \n Cummins, C., Petoumenos, P., Murray, A., & Leather, H. (2018). Compiler\n   Fuzzing through Deep Learning. ISSTA. \n Goens, A., Brauckmann, A., Ertel, S., Cummins, C., Leather, H., &\n   Castrillon, J. (2019). A Case Study on Machine Learning for Synthesizing\n   Benchmarks. MAPL. \n Cummins, C. (2020). Deep Learning for Compilers. University of Edinburgh. \n \n Build \n See  INSTALL.md  for instructions on setting up the build\nenvironment. \n Then build cldrive using: \n sh\n$ bazel build -c opt //gpu/cldrive \n This will build an optimized  cldrive  binary and print its path. \n Usage \n sh\n$ cldrive --srcs=<opencl_sources> --envs=<opencl_devices> \n Where  <opencl_sources>  if a comma separated list of absolute paths to OpenCL\nsource files, and  <opencl_devices>  is a comma separated list of\nfully-qualified OpenCL device names. To list the available device names use\n --clinfo . Use  --help  to see the full list of options. \n Example \n For example, given a file: \n sh\n$ cat kernel.cl\nkernel void my_kernel(global int* a, global int* b) {\n    int tid = get_global_id(0);\n    a[tid] += 1;\n    b[tid] = a[tid] * 2;\n} \n and available OpenCL devices: \n sh\n$ cldrive --clinfo\nGPU|NVIDIA|GeForce_GTX_1080|396.37|1.2\nCPU|Intel|Intel_Xeon_CPU_E5-2620_v4_@_2.10GHz|1.2.0.25|2.0 \n To run the kernel 5 times on both devices using 4096 work items divided into\nwork groups of size 1024: \n sh\n$ cldrive --srcs=$PWD/kernel.cl --num_runs=5 \\\n    --gsize=4096 --lsize=1024 \\\n    --envs='GPU|NVIDIA|GeForce_GTX_1080|396.37|1.2','CPU|Intel|Intel_Xeon_CPU_E5-2620_v4_@_2.10GHz|1.2.0.25|2.0'\nOpenCL Device, Kernel Name, Global Size, Local Size, Transferred Bytes, Runtime (ns)\nI 2019-02-26 09:54:10 [gpu/cldrive/libcldrive.cc:59] clBuildProgram() with options '-cl-kernel-arg-info' completed in 1851 ms\nGPU|NVIDIA|GeForce_GTX_1080|396.37|1.2, my_kernel, 4096, 1024, 65536, 113344\nGPU|NVIDIA|GeForce_GTX_1080|396.37|1.2, my_kernel, 4096, 1024, 65536, 57984\nGPU|NVIDIA|GeForce_GTX_1080|396.37|1.2, my_kernel, 4096, 1024, 65536, 64096\nGPU|NVIDIA|GeForce_GTX_1080|396.37|1.2, my_kernel, 4096, 1024, 65536, 73696\nGPU|NVIDIA|GeForce_GTX_1080|396.37|1.2, my_kernel, 4096, 1024, 65536, 73632\nI 2019-02-26 09:54:11 [gpu/cldrive/libcldrive.cc:59] clBuildProgram() with options '-cl-kernel-arg-info' completed in 76 ms\nCPU|Intel|Intel_Xeon_CPU_E5-2620_v4_@_2.10GHz|1.2.0.25|2.0, my_kernel, 4096, 1024, 65536, 105440\nCPU|Intel|Intel_Xeon_CPU_E5-2620_v4_@_2.10GHz|1.2.0.25|2.0, my_kernel, 4096, 1024, 65536, 55936\nCPU|Intel|Intel_Xeon_CPU_E5-2620_v4_@_2.10GHz|1.2.0.25|2.0, my_kernel, 4096, 1024, 65536, 63296\nCPU|Intel|Intel_Xeon_CPU_E5-2620_v4_@_2.10GHz|1.2.0.25|2.0, my_kernel, 4096, 1024, 65536, 56192\nCPU|Intel|Intel_Xeon_CPU_E5-2620_v4_@_2.10GHz|1.2.0.25|2.0, my_kernel, 4096, 1024, 65536, 55680 \n By default, cldrive prints a CSV summary of kernel stats and runtimes to\nstdout, and logging information to stderr. The raw information produced by\ncldrive is described in a set of protocol buffers\n //gpu/cldrive/proto:cldrive.proto . To print\n cldrive.Instances  protos to stdout, use argumet  --output_format=pbtxt \nto print text format protos, or  --output_format=pb  for binary format. \n License \n Copyright 2016-2020 Chris Cummins  chrisc.101@gmail.com . \n Released under the terms of the GPLv3 license. See  LICENSE  for details.", "\n  lmk - let me know\n   \n \n   \n \n \n \n Email notifications from the command line. \n Step 1  Wrap your long running job in  lmk : \n sh\n$ lmk 'bash ./experiments.sh'\n...  # command runs and outputs normally\n[lmk] chrisc.101@gmail.com notified \n Step 2  ☕ \n Step 3  Receive an email when it's done: \n \n Alternatively,  lmk -  reads passively from stdin: \n sh\n$ (./experiment1.sh; experiment2.py) 2>&1 | lmk -\n... # commands run and output normally\n[lmk] chrisc.101@gmail.com notified \n Installation \n sh\n$ pip install lmk \n License \n Made with ❤️ by  Chris Cummins . Released under  MIT License .", '\n  gh-archiver\n    \n \n \n \n Mirror a GitHub user\'s repos locally. \n This program fetches a GitHub user\'s repositories and mirrors them to a local\ndirectory. New repositories are cloned, existing repositories are fetched from\nremote. \n Setup \n Create a Github  personal access token .\nIf you intend to mirror your own private repositories, select "repo" from the\nlist of available scopes. To mirror only your public repositories or those\nanother user, no scopes are required. \n Create a ~/.github/access_tokens/gh_archiver.txt file containing your\nthe personal access token you just created: \n sh\n$ mkdir -p ~/.github/access_tokens\n$ cat <<EOF > ~/.github/access_tokens/gh_archiver.txt\nYourAccessToken\nEOF\n$ chmod 0600 ~/.github/access_tokens/gh_archiver.txt \n Then build and install the  gh_archiver  program using: \n sh\n$ basel run -c opt //datasets/github/gh_archiver:install \n Requires Python >= 3.6. \n Usage \n Mirror a Github user\'s repositories to a directory using: \n sh\n$ gh_archiver --user <github_username> --outdir <path> \n License \n Made with ❤️ by  Chris Cummins .\nReleased under  MIT License .', "End-to-end Deep Learning of Optimization Heuristics \n Chris Cummins ,\n Pavlos Petoumenos ,\n Zheng Wang ,\n Hugh Leather . \n Winner of Best Paper Award PACT'17 \n \n \n \n \n \n \n Abstract: \n \n Accurate automatic optimization heuristics are necessary for dealing with the\ncomplexity and diversity of modern hardware and software. Machine learning is\na proven technique for learning such heuristics, but its success is bound by\nthe quality of the features used. These features must be hand crafted by\ndevelopers through a combination of expert domain knowledge and trial and\nerror. This makes the quality of the final model directly dependent on the\nskill and available time of the system architect. \n Our work introduces a better way for building heuristics. We develop a deep\nneural network that learns heuristics over raw code, entirely without using\ncode features. The neural network simultaneously constructs appropriate\nrepresentations of the code and learns how best to optimize, removing the need\nfor manual feature creation. Further, we show that our neural nets can\ntransfer learning from one optimization problem to another, improving the\naccuracy of new models, without the help of human experts. \n We compare the effectiveness of our automatically generated heuristics against\nones with features hand-picked by experts. We examine two challenging tasks:\npredicting optimal mapping for heterogeneous parallelism and GPU thread\ncoarsening factors. In 89% of the cases, the quality of our fully automatic\nheuristics matches or surpasses that of state-of-the-art predictive models\nusing hand-crafted features, providing on average 14% and 12% more performance\nwith no human effort expended on designing features. \n \n @inproceedings{cummins2017b,\n  title={End-to-end Deep Learning of Optimization Heuristics},\n  author={Cummins, Chris and Petoumenos, Pavlos and Wang, Zheng and Leather, Hugh},\n  booktitle={PACT},\n  year={2017},\n  organization={ACM}\n} \n See  code/README.md  for instructions on re-producing the experiments. \n \n License \n The code for this paper (everything in the directory  code ) is released under\nthe terms of the GPLv3 license. See  LICENSE  for details. Everything\nelse (i.e. the LaTeX sources and data sets) are unlicensed, please contact\nChris Cummins  chrisc.101@gmail.com  before using. \n Acknowledgements \n \n EPSRC grants\n  EP/L01503X/1 ( CDT in Pervasive Parallelism ),\n  EP/M01567X/1 (SANDeRs),\n  EP/M015793/1 (DIVIDEND),\n  and EP/P003915/1 (SUMMER). \n", 'This is the data for my blog \n It is automatically transformed by  Jekyll \ninto a static site whenever I push this repository to GitHub. \n Setup \n Install the dependencies. On Linux: \n sh\n$ sudo apt install ruby ruby-dev ffmpeg\n$ sudo gem install bundler\n$ bundle install \n On macOS: \n sh\n$ brew install ruby ffmpeg\n$ sudo "$(brew --prefix)/opt/ruby/bin/gem" install -n "$(brew --prefix)/bin" bundler:2.1.4\n$ "$(brew --prefix)/bin/bundle" install \n Running the site locally \n ```sh \n Generate site files \n $ make all \n Serve locally on http://127.0.0.1:4000 \n $ make run\n``` \n Pushing to staging site \n sh\n$ git remote add staging git@github.com:ChrisCummins/staging.git\n$ git push staging master:gh-pages \n Check the staging site at http://chriscummins.cc/staging \n License \n The website code is\n MIT Licensed ,\nbut the website  content  is not. The following directories and their contents\nare copyright Chris Cummins:\n*  _posts/ \n*  images/ ,\n*  img/ ,\n*  pub/ \n*  u/ . \n You may not reuse anything therein without my permission.', 'Export bazel subtree to GitHub \n This package contains a utility for exporting a subset of my\n phd  repo as a standalone git repository.\nGive one or more bazel targets, it determines the required dependencies and\nfilters through the git history, exporting only the commits that are needed\nto reproduce the files, and rewriting the commits so that only the necessary\nfiles are modified. \n Usage \n To export the targets  //package/to/export/...  and  //another/package  to a\nGitHub repo called  github_export : \n sh\n$ bazel run //tools/source_tree:export_git_history -- \\\n        --targets=//package/to/export/...,//another/package \\\n        --mv_files=package/to/export/README.md:README.md \\\n        --github_repo=github_export \n Additionally, the  --mv_files  argument permits moving a file\'s location in\nthe exported repository, which is useful for exporting things like readme and\nlicense files to the package root. \n The utility can also be called from a python script. The script equivalent to\nthe previous command is: \n py\nfrom tools.source_tree import export_source_tree\nexport_source_tree.EXPORT(\n    github_repo=\'git_bazel_subtree_filter\',\n    targets=[\n        \'//package/to/export/...\',\n        \'//another/package\',\n    ],\n    move_file_mapping={\n        \'package/to/export/README.md\': \'README.md\',\n    },\n) \n And the  BUILD  target for it: \n py_binary(\n    name = "EXPORT",\n    srcs = ["EXPORT.py"],\n    deps = ["//tools/source_tree:export_source_tree"],\n)', 'rules_bats  -- Bazel build rules for Bats \n This adds a  bats_test  rule for  bats . \nTo use this rule, add the following to your WORKSPACE file: \n ```\nload("@bazel_tools//tools/build_defs/repo:http.bzl", "http_archive") \n http_archive(\n    name = "com_github_chriscummins_rules_bats",\n    strip_prefix = "rules_bats-c00ced5ccd21aadc651a941e1bca9cb5c143a850",\n    sha256="654818574a43119d3d2507fcfd5a623c3ff705a9a2add3d549d7cd4717326fc8",\n    urls = ["https://github.com/ChrisCummins/rules_bats/archive/c00ced5ccd21aadc651a941e1bca9cb5c143a850.zip"],\n) \n load("@com_github_chriscummins_rules_bats//:bats.bzl", "bats_deps")\nbats_deps()\n``` \n And in your BUILD file: \n ```\nload("@com_github_chriscummins_rules_bats//:bats.bzl", "bats_test") \n bats_test(\n    name = "bats_test",\n    srcs = [\n        "test_one.bats",\n        "test_two.bats",\n    ],\n    data = [\n        ":some_sh_library",\n        "//app/under/test",\n    ],\n)\n``` \n See  rules_bats_examples  for more details.', 'Example usage of  rules_bats \n See  rules_bats  for details.', 'Photolib \n A set of tools for managing my library of photographs. \n Installation \n $ brew install exempi python \n Usage \n sh\n$ bazel run //util/photolib:photolint -- $HOME/Photos/Photo\\ Library \n License \n Apache License Version 2.0. See  LICENSE.txt  for details.', 'ProGraML: Program Graphs for Machine Learning \n \n  PyPi Version  \n \n \n \n  Downloads counter  \n \n \n \n  license  \n \n \n \n  CI status  \n \n \n \n  Better code  \n \n \n \n  Commit counter  \n \n \n \n \n \n An expressive, language-independent representation of programs. \n \n \n \n    Check  the website \n    for more information.\n   \n \n Introduction \n ProGraML is a representation for programs as input to a machine learning model.\nThe key features are: \n \n \n Simple:  Everything is available through a  pip install , no compilation\n   required. Supports several programming languages ( C, C++, LLVM-IR, XLA ) and\n   several graph formats ( NetworkX, DGL, Graphviz, JSON ) out of the box. \n \n \n Expressive:  Captures every control, data, and call relation across entire\n   programs. The representation is independent of the source language. Features\n   and labels can be added at any granularity to support whole-program,\n   per-instruction, or per-relation reasoning tasks. \n \n \n Fast:  The core graph construction is implemented in C++ with a low\n   overhead interface to Python. Every API method supports simple and efficient\n   parallelization through an  executor  parameter. \n \n \n To get stuck in and play around with our graph representation, visit: \n \n \n \n Or if papers are more your ☕, have a read of ours: \n \n \n \n Supported Programming Languages \n The following programming languages and compiler IRs are supported\nout-of-the-box: \n \n \n Language \n API Calls \n Supported Versions \n \n \n C \n \n programl.from_cpp() ,\n       programl.from_clang() \n \n Up to ISO C 2017 \n \n \n C++ \n \n programl.from_cpp() ,\n       programl.from_clang() \n \n Up to ISO C++ 2020 DIS \n \n \n LLVM-IR \n \n programl.from_llvm_ir() \n \n 3.8.0, 6.0.0, 10.0.0 \n \n \n XLA \n \n programl.from_xla_hlo_proto() \n \n 2.0.0 \n \n \n Is your favorite language not supported here? Submit a  feature\nrequest ! \n Getting Started \n Install the latest release of the Python package using: \n pip install -U programl \n The API is very simple, comprising graph  creation  ops, graph  transform  ops,\nand graph  serialization  ops. Here is a quick demo of each: \n ```py \n \n \n \n import programl as pg \n \n \n \n Construct a program graph from C++: \n \n \n \n G = pg.from_cpp("""\n... #include  \n...\n... int main(int argc, char** argv) {\n...   std::cout << "Hello, world!" << std::endl;\n...   return 0;\n... }\n... """) \n \n \n \n A program graph is a protocol buffer: \n \n \n \n type(G). name \n\'ProgramGraph\' \n \n \n \n Convert the graph to NetworkX: \n \n \n \n pg.to_networkx(G)\n \n \n \n \n Save the graph for later: \n \n \n \n pg.save_graphs(\'file.data\', [G])\n``` \n \n \n \n For further details check out the  API\nreference . \n Contributing \n Patches, bug reports, feature requests are welcome! Please use the\n issue tracker  to file a\nbug report or question. If you would like to help out with the code, please\nread  this document . \n Citation \n If you use ProGraML in any of your work, please cite  this\npaper : \n @inproceedings{cummins2021a,\n  title={{ProGraML: A Graph-based Program Representation for Data Flow Analysis and Compiler Optimizations}},\n  author={Cummins, Chris and Fisches, Zacharias and Ben-Nun, Tal and Hoefler, Torsten and O\'Boyle, Michael and Leather, Hugh},\n  booktitle = {Thirty-eighth International Conference on Machine Learning (ICML)},\n  year={2021}\n}', 'Format: Automated Code Formatter \n  download  \n \n \n \n  Travis CI  \n \n \n \n  license  \n \n \n \n This projects implements an opinionated, non-configurable enforcer of code\nstyle. The aim is to take control of source formatting away from the developer,\nreducing cognitive load and allowing you to focus on what matters. \n Features \n ☑️  Consistent  code styling of C/C++, Python, Java, SQL, JavaScript, HTML,\n  Protocol Buffers, CSS, Go, Markdown, plain text, and JSON files. \n ☑️  Git-aware  pre-commit mode which formats changed files and signs off\n  commits. \n ☑️  Fast  incremental formats of large code bases using a "last modified"\n  time stamp cache. \n ☑️  Black-listing  of files from automated formatting using git-like ignore\n  files. \n ☑️  Safe  execution using inter-process locking to prevent multiple\n  formatters modifying files simultaneously. \n Install \n Download an archive from the\n release page  and extract it in\nyour  $PATH . \n Requires Python >= 3.7 and sqlite. Syntax-specific formatters may have\nadditional dependencies: \n \n Java  files require a host java. \n JSON  files require  jsonlint . \n \n Usage \n Format files in-place using: \n sh\n$ format <path ...> \n If a path is a directory, all files inside it are formatted. The type of\nformatting applied to a file is determined by its suffix. To print the files\nthat will be formatted without changing them, use: \n sh\n$ format --dry_run <path ...> \n Use the  --watch  flag on linux (sorry, no macOS support as of yet) to\nwatch files or directories and run the formatter on them on change: \n sh\n$ format --watch <path ...> \n For git repositories, you can install the formatter to execute as a pre-commit\nhook using: \n sh\n$ format --install_pre_commit_hook \n This installs hooks that run the formatter on changelists before commits, and\nadds a "Signed-off-by" footer to commit messages verifying that the commit\ncontents passed formatting checks. \n If you want to exclude files from formatting, create a  .formatignore  file. It\nhas similar rules to  .gitignore  files, e.g.: \n ```sh\n$ cat .formatignore \n Everything after \'#\' character is a comment \n hello.txt  # Name specific files to exclude from formatting\n* / .json  # Or glob them\n!package.json  # Use \'!\' character to create un-ignore patterns\n``` \n This program uses a filesystem cache to store various attributes such as a\ndatabase of file modified times. See  format --print_cache_path  to print the\npath of the cache. \n Development \n Development takes place in the  phd \nrepository. Grab a copy from here: \n sh\n$ git clone https://github.com/ChrisCummins/phd.git format\n$ cd format \n Run the test suite using: \n sh\n$ bazel test //tools/format/... \n Build and install the formatter binary from source using: \n sh\n$ ./tools/format/install.sh \n Update the  format  repository using: \n sh\n$ bazel run //tools/format:export \n Then manually create a new release and attach binary files. \n License \n Copyright 2020 Chris Cummins  chrisc.101@gmail.com . \n Released under the terms of the Apache 2.0 license. See  LICENSE  for details.', 'cec_exports_repo : Bazel subproject export \n \n Overview \n This project provides a bazel rule for exporting the git history for a subset of targets in a workspace as a separate repository. \n I made this tool to export sub-projects from a large git repository as separete git repositories. For example, let\'s say you have a monorepo in which there are five commits  c1 - c5 : \n \n Let\'s say there are two packages  //foo  and  //bar  which you would like to export as their own repositories. Commits  c1  and  c4  touch files in  //foo ,  c3  and  c5  touch  //bar , and  c4  touches files which are common to both. Using  exports_repo , you can create export actions, which, when executed create new git repositories containing only the history for the relevant files: \n \n Getting Started \n Add the following to your WORKSPACE file: \n ```py\nload("@bazel_tools//tools/build_defs/repo:http.bzl", "http_archive") \n http_archive(\n    name = "cec_exports_repo",\n    urls = ["https://github.com/ChrisCummins/exports_repo/archive/2020.05.06.tar.gz"],\n    sha256 = "338001b0e2e3cea978b72a456ac201b86042aecbbbdb779694ac4b131e949fc2",\n    strip_prefix = "exports_repo-2005.05.06",\n) \n load("@cec_exports_repo//tools/bzl:deps.bzl", "cec_exports_repo_deps")\ncec_exports_repo_deps()\n``` \n In your BUILD file, define an  exports_repo  target: \n ```py\nload("@cec_exports_repo//tools/bzl:exports_repo.bzl", "exports_repo") \n exports_repo(\n    name = "export",\n    remote = "git@github.com:ChrisCummins/exports_repo_example.git",\n    targets = ["//myapp/..."],\n    path_rename = {\n        "myapp/README.md": "README.md"\n    },\n)\n``` \n Run this new target to perform the export: \n sh\n$ bazel run //myapp:export --define=workspace=$(pwd) \n Note you must  --define  the path of the workspace.  This is becasue the repo export is non-hemetic and works by cloning the workspace repository. \n Usage \n exports_repo \n py\nexports_repo(name, remote, branch, targets, paths, path_remove, path_rename, tag_rename, always_export_path) \n An executable rule that exports the git history for a subset of targets and paths. \n This is a non-hemetic process which: \n \n Identifies the subset of files that will be exported by evaluating the source and build files of the listed  targets . \n Creates a temporary local clone of the current repository. \n Re-writes the history of this clone to include only the subset of required files. \n Force-pushes the rewritten history to  remote/branch . \n \n \n \n \n \n \n \n \n Attributes \n \n \n \n \n name \n \n Name, required \n A unique name for this rule. \n \n \n \n remote \n \n String, required \n Git remote to export to. \n \n          The URL of a git remote that the resulting repository is force pushed to.\n         \n \n \n \n branch \n \n String, default to "master" \n \n          The name of the remote branch that is pushed.\n         \n \n \n \n targets \n \n List of labels, optional \n The labels to export. \n \n          A list of bazel labels which are used to resolve the files to export. The source and build files for each target and its dependencies are used to resolve the subset of files to export.\n         \n \n \n \n paths \n \n List of files, optional \n Additional paths to export. \n \n          A list of additional files to export. All file paths are relative to the workspace root.\n         \n \n \n \n path_remove \n \n List of files, optional \n Files to exclude from export. \n \n          A list of files to exclude from export. All file paths are relative to the workspace root.\n         \n \n \n \n path_rename \n \n Dictionary, optional \n A mapping of paths to rename. \n \n          A dictionary of paths to rename during export. All file paths are relative to the workspace root.\n         \n \n \n \n tag_rename \n \n String, optional \n A tag renaming pattern. \n \n          A tag renaming pattern that is passed to git-filter-repo. The format for renaming patterns is <old:new>, and is used to subsitutute a prefix. For example, "foo:bar" will rename tag "foo-1.2.3" to "bar-1.2.3"; either <old> or <new> can be empty. For example, ":foo-" will rename tag "1.2.3" to "foo-1.2.3".\n         \n \n \n \n always_export_path \n \n File, default to "tools/always_export.txt" \n Path of a file containing additional paths to export. \n \n          A file which contains a list of additional paths which are implicitly added to the "paths" attribute. The file is ignored if it is not found. File path is relative to the workspace root.\n         \n \n \n \n \n Credits \n The heavy lifting is done by the excellent  git-filter-repo , I simply provide glue code which uses bazel query to feed it with commands. \n License \n Made with ❤️ by  Chris Cummins . Released under  MIT License .', 'bazel_llvm : LLVM libraries and binaries for bazel \n \n Overview \n Pre-built  LLVM releases  to use\nas dependencies in bazel projects: \n \n @llvm//<version>  provides a  cc_library  that can be used as\n    dep  for C++ apps in bazel that link against LLVM. \n @llvm//<vesion>:<bin>  provides pre-built LLVM binaries, e.g.\n    @llvm//10.0.0:clang  provides clang 10 binary. This can be used as\n   a  data  dep of scripts. \n \n Supported compiler / OS versions: \n \n Ubuntu Linux 18.04 with clang / gcc >= 7. \n macOS >= 10.15. \n \n Getting Started \n Add the following to your WORKSPACE file: \n ```py\nhttp_archive(\n    name = "llvm",\n    strip_prefix = "bazel_llvm- ",\n    urls = ["https://github.com/ChrisCummins/bazel_llvm/archive/ .tar.gz"],\n) \n load("@llvm//tools/bzl:deps.bzl", "llvm_deps")\nllvm_deps()\n``` \n In your BUILD files, reference  @llvm//<version>:<target> : \n ```py \n Link against LLVM 10.0.0 as a regular C++ dependency: \n cc_library(\n    name = "my_lib",\n    srcs = ["my_lib.cc"],\n    copts = ["-std=c++14", "-fno-rtti"],  # required\n    deps = ["@llvm//10.0.0"],\n) \n Use pre-compiled opt as a data dependency: \n sh_binary(\n    name = "my_script",\n    srcs = ["my_script.sh"],\n    data = ["@llvm//10.0.0:opt"],\n)\n``` \n See the  examples  directory for further usage. \n Known issues \n \n This uses LLVM releases archives, which are Release builds without\n  assertions enabled. To use a debug build, or one with assertions\n  enabled, you would need to provide your own build archive and update\n  the archives in  tools/bzl/deps.bzl . \n \n Work-in-progress: building from source \n I have tried a couple of times to get LLVM to build from source as a\n cmake_external()  target using  rules_foreign_cc . I have yet to get\nit to work. There seems to be conflicts between the bazel build\nenvironment and the build environment that LLVM usually expects. \n You can see my latest attempt\n here .\nI have a working Debug build on macOS, but now I get double-defined\nstatic variable errors when I link against it from a  cc_library() . I\ndon\'t have the patience to figure out enough about LLVM/Bazel build\ninternals to crack it. If you want to give it a go, build: \n sh\n$ bazel build @llvm//10.0.0:debug --sandbox_debug \n Patches welcome! ❤️']
xRomZak,[]
Kaixhin,["dotfiles \n This repo is a collection of some of my dotfiles. Configs have come from many places, but  Cătălin's dotfiles  are a prime source. \n Run  git submodule update --init  to download all the vim plugins first.", '\ufeff# zk2hk\nA simple Node.js script that converts ｚｅｎｋａｋｕ\u3000（ｆｕｌｌ－ｗｉｄｔｈ\u3000ｃｈａｒａｃｔｅｒｓ） command line arguments to hankaku (half-width characters).\nModified from scripts by  Yutaka Kondo  and  Philip Ronan .', 'mobile-first \n Presentation for IC SaaS on Mobile First web development', 'okabe \n A CoffeeScript data structure and algorithm library. \n \n To rule time is to rule the world ~ Amane Suzuha \n \n Hououin (鳳凰院) \n Data structures are ways of representing data for dealing with computational problems more efficiently. \n \n Stack \n Queue \n Dequeue \n Unordered List \n Ordered List \n Map \n Binary Tree \n Binary Heap \n Priority Queue \n Binary Search Tree \n AVL Tree / Balanced Binary Search Tree \n Graph \n \n Kyouma (凶真) \n Algorithms are lists of instructions for solving computational problems. \n \n Sequential Search \n Ordered Sequential Search \n Binary Search \n Breadth First Search \n Bubble Sort \n Short Bubble Sort \n Selection Sort \n Insertion Sort \n Shell Sort \n Merge Sort \n Quick Sort \n Pre-order Traversal \n Post-order Traversal \n In-order Traversal \n', 'git-concepts \n Presentation for BICV on Git', 'sybilsystem \n Construction on indefinite hiatus \n A MATLAB library for prototyping deep neural networks (that can be trained using backpropagation), named for the eponymous "supercomputer".\nDeveloped for clear code, not speed.\nTo get started run the initialize script. \n This work has been based on the  Unsupervised Feature Learning and Deep Learning Tutorial  and utilises some of the helper functions provided. \n Wishlist \n \n Classification results \n Batch accuracy \n Convolution and pooling layers (only FC now) \n Exporting networks and layers \n Automatic saving during training \n Visualisation of learnt features \n', '\n dockerfiles \n Compilation of Dockerfiles with automated builds enabled on the  Docker Hub .  Not suitable for production environments.  These images are under continuous development, so breaking changes may be introduced. \n Nearly all images are based on Ubuntu Core 14.04 LTS, built with minimising size/layers and  best practices  in mind. Dependencies are indicated left to right e.g. cuda-vnc is VNC built on top of CUDA. Explicit dependencies are excluded. \n \nUp-to-date builds\n-----------------\n\nSome builds based on certain software have builds that are triggered on schedule via a cron script to stay up to date on a weekly basis. These are:\n\n- [Brainstorm](https://github.com/IDSIA/brainstorm)\n- [Caffe](https://github.com/BVLC/caffe)\n- [DIGITS](https://github.com/NVIDIA/DIGITS)\n- [FGLab](https://github.com/Kaixhin/FGLab)/[FGMachine](https://github.com/Kaixhin/FGMachine)\n- [Keras](https://github.com/fchollet/keras)\n- [Lasagne](https://github.com/Lasagne/Lasagne)\n- [MXNet](https://github.com/dmlc/mxnet)\n- [neon](https://github.com/NervanaSystems/neon)\n- [Pylearn2](https://github.com/lisa-lab/pylearn2)\n- [Theano](https://github.com/Theano/Theano)\n- [Torch](https://github.com/torch/distro)\n \n Graphical applications \n Starting graphical (X11) applications is possible with the following commands: \n sh\ndocker run -it `# Running interactively, but can be replaced with -d for daemons` \\\n  -e DISPLAY `# Pass $DISPLAY` \\\n  -v=/tmp/.X11-unix:/tmp/.X11-unix `# Pass X11 socket` \\\n  --ipc=host `# Allows MIT-SHM` \\\n  <image> \n General information on running desktop applications with Docker can be found  in this blog post . You probably will also need to configure the X server host ( xhost ) to  give access . For hardware acceleration on Linux, it is possible to use  nvidia-docker  (with an image built for NVIDIA Docker), although OpenGL is  not fully supported . \n On Mac OS X, use XQuartz and  allow connections from network clients . Then the following can be used: \n sh\ndocker run -it \\\n  -e DISPLAY=`ifconfig en0 | grep inet | awk \'$1=="inet" {print $2}\'`:0 `# Use XQuartz network $DISPLAY` \\\n  --ipc=host \\\n  <image> \n Daemonising containers \n Most containers run as a foreground process. To daemonise (in Docker terminology, detach) such a container it is possible to use: \n docker run -d <image> sh -c "while true; do sleep 1; done" \n It is now possible to access the daemonised container, for example using bash: \n docker exec -it <id> bash \n Sibling containers \n To start containers on the host from within a docker container, the container requires  docker-engine  installed, with the same API version as the Docker daemon on the host. The Docker socket also needs to be mounted inside the container: \n -v /var/run/docker.sock:/var/run/docker.sock \n CUDA \n Many images rely on  CUDA . These images are versioned with the corresponding tags, e.g. "8.0" and "7.5", on the Docker Hub. \n These images need to be run on an Ubuntu host OS with  NVIDIA Docker  installed. The driver requirements can be found on the  NVIDIA Docker wiki . \n Deprecated images \n kaixhin/cuda  and  kaixhin/cudnn  have now been  deprecated  in favour of the official solution ( nvidia/cuda ). \n Migration \n In the future it will hopefully be possible to checkpoint and restore Docker containers easily using  CRIU . This would alleviate some issues, such as the inability to restart a VNC image successfully. \n Automated Builds \n Automated Builds  on the Docker Hub have several advantages, including reproducibility and security. However the build cluster has the following limits for Automated Builds: \n \n 2 hours \n 1 CPU \n 2 GB RAM \n 512 MB swap \n 30 GB disk space \n \n The main tip for keeping within the CPU and memory limits is to reduce parallelism/forking processes. Due to their logging system, redirecting stdout/stderr to /dev/null can potentially save a reasonable amount of memory. \n Acknowledgements \n Some Dockerfiles have been modified from the work of others. The source for these are: \n \n CUDA \n Samba \n VNC \n \n Citation \n If you find this useful in research please consider  citing this work .', 'Baka Punk \n A tool for finding similar songs in your music library. Similarity is based on BPM and key.\nThis is primarily a tool for DJs and requires: \n \n A MP3 music collection.  \n A structure of artist/album/songs. \n BPM and key tags. \n Keys tagged according to the Mixed In Key  Camelot Wheel . \n Musly . \n \n Setup \n Install Node.js and npm. Clone this repo and run  npm install  inside to download the dependencies.\nInstall Musly and run  musly -N  in this directory to initialise  collection.musly . \n Create a file called  music_dir.txt  which contains the path to your music library e.g. /Users/BakaPunk/Music.\nRun  node process.js  to scan your music and save it into  db.json  and  collection.musly . This can take a while to complete.\nFiles are identified by their path e.g. Baka Punk/Mashups/diSONICted.mp3.\nThe processing ignores files which have already been added, so can be run to add new music (but will also add files whose paths have changed).\nThe processing does not remove deleted files.\nTo clear the databases simply delete  db.json  and  collection.musly . \n Usage \n Run  node . . You will be prompted to enter part of a title to search (case-insensitive).\nIf there are any matches you will be prompted to enter the index corresponding to the correct song.\nThe "best" (max 150) matches will then be returned. \n Todo \n \n Page up and down of all results \n', 'End-to-End Training of Deep Visuomotor Policies \n Presentation on  End-to-End Training of Deep Visuomotor Policies .\n Supplementary videos . \n Setup \n Run  git submodule update --init  to fetch reveal.js.\nChange into its directory and use  npm install  to fetch its dependencies.\nRun  grunt  to build the reveal.js minimised files. \n Build \n Run  node watch.js  to build the presentation whenever it changes. \n Run  build.sh  to build the presentation manually.', 'CUDA Workshop \n Code from exercises for the July 2015 CUDA Workshop at Imperial College London organised by the  Deep Learning Network .', '\n \n \n \n \n \n \n \n Quickstart: https://kaixhin.github.io/FGLab/ \n FGLab is a machine learning dashboard, designed to make prototyping experiments easier. Experiment details and results are sent to a database, which allows analytics to be performed after their completion. The server is FGLab, and the clients are  FGMachines . \n Contents \n \n Installation \n Overview \n Examples \n API \n \n Installation \n FGLab tries to follow the  SemVer  standard whenever possible. Releases can be found  here . There are 3 ways to run FGLab: Installing  locally , via  Docker , or hosted on  Heroku . \n Option 1: Local \n \n Install  Node.js  from the website or your package manager. \n Install  MongoDB  from the website or your package manager. \n Make a database directory for MongoDB. For example,  mkdir -p <working directory>/db . \n Run the MongoDB daemon. From the previous example, run  mongod --dbpath <working directory>/db . \n Either clone this repository or download and extract a  zip / tar . \n Move inside the FGLab folder. \n Run  npm install .  npm install  also runs  bower install  to install additional required packages. \n FGLab requires a  .env  file in this directory. For most installations, it should be possible to copy  example.env  to  .env , but it may require customisation for non-standard MongoDB ports, or setting a different port for FGLab. An alternative is to set the following environment variables: \n MONGODB_URI (MongoDB database URI) \n FGLAB_PORT (port) \n \n Run  node lab  (or  npm start ) to start FGLab. You can now access the user interface from a browser on the current machine at  http://localhost:<FGLAB_PORT> , where  <FGLAB_PORT>  is 5080 by default. For  remote access , you need to be able to access the machine FGLab is running on from your remote machine via a local network or the internet. Given the default port, you would replace  http://localhost:5080  with  http://lan-hostname:5080  or  http://public-address.com:5080 , respectively. \n Please read the  overview  to understand how FGLab and FGMachine cooperate - both are needed in order to run experiments. Afterwards, you should set up instances of  FGMachine . \n To update, run  npm run update . \n Option 2: Docker \n Start a  MongoDB container  and link it to the  FGLab container : \n sh\nsudo docker run -d --name mongodb mongo\nsudo docker run -d --name fglab --link mongodb:mongo -p 5080:5080 kaixhin/fglab \n Although not recommended, it is possible to adjust  project schema  and other parts of the database. This can be accomplished either by connecting directly to MongoDB or via a GUI such as  mongo-express . \n sh\nsudo docker run -d --name mongo-express --link mongodb:mongo -p 8081:8081 mongo-express \n Option 3: Heroku \n The deploy button provisions a free dyno running FGLab on  Heroku , with a free 500MB MongoDB database from  MongoLab . \n \n Overview \n FGLab is based on several classes of object. One begins with a  project , which involves adjusting variables to achieve the desired results. In machine learning, these variables are  hyperparameters , which are set for the project. In a more general setting, the variables are simply options, which may therefore include implementation-dependent details. A project will then comprise of a set of  experiments  derived from adjusting options. \n Projects \n A project is created by uploading a  JSON  schema. JSON is a human-readable data-interchange format that is widely used and has mature libraries available for most programming languages. \n The JSON schema represents a map/associative array (without nesting), where the values are an object comprising of several fields: \n \n type : \n int \n float \n bool \n string \n enum \n default : Default value \n values : An array of strings comprising the  enum \n \n See  mnist.json  as an example schema for a project. Each schema should be uploaded with the filename corresponding to the desired name for the project e.g.  mnist.json . \n Often it is hard to specify some options in advance e.g. the type or structure of the machine learning model. Sometimes code may change, which would influence the results. The  string  type can be used to address changing options and versioning manually e.g.  cnn.v2 . \n This is stored by FGLab, and is used to construct a form which lets one choose options and submit an experiment to an available machine. The options are sent to your machine learning program via the FGMachine client. Your machine learning program then accepts the different fields via command-line options, the details of which are in the  FGMachine documentation .  Note that the  _id  field is reserved, as this will store the experiment ID as a  string . \n FGMachine will spawn your machine learning program, which should produce output files to be sent from FGMachine to FGLab. The details of this is available in the  FGMachine documentation . \n Grid and random search optimisers have also been implemented in FGLab, to allow searching over a range of hyperparameter space. Multiple string values are delimited by commas ( , ). \n Experiments \n An experiment is one complete training and testing run with a specific set of options. Depending on the experiment it may be impossible to control for every source of randomness, so experiments with the same set of options will still be assigned unique IDs. Experiments have a unique ID, in addition to a project ID, a machine ID, the chosen options, the current status (running/success/fail), timestamps, results, and custom data; this provides a comprehensive record of the experiment as a whole. \n The experiment page contains a "Logs" window, which uses WebSockets to display the experiment\'s  stdout  and  stderr  live. There is also an editable "Notes" text box that is automatically saved (at an interval of 0.5s), displaying on both the experiment page itself and the table of experiment results. \n The current format for results is documented with  FGMachine . \n Machines \n A  FGMachine client  registers itself with FGLab, providing hardware details as well as an address for interaction between FGLab and the machine. A machine (FGMachine) stores its own details, as well as a list of supported projects. Before a new experiment is chosen to be run, FGLab queries all machines in order to determine a machine with the capacity to run the experiment. \n Note that machines are implementation-independent, and may well store their own (large) data on experiments, for example learnt parameters and logs. As mentioned before, these can be uploaded to FGLab\'s database. \n Examples \n Examples utilising the range of abilities of FGLab/FGMachine can be found in the  examples folder . \n Password protection \n Just set up PASSWORD variable without and quotes in .env file to protect your FGLab with a password. Note: you should type in that password into password field, when prompted to.\nExample:\n PASSWORD=friend \n API \n The API is largely undocumented due to ongoing development introducing breaking changes. Ongoing documentation is available in  RAML :  api.raml . The following are noted for convenience: \n Submit a new experiment with a set of options \n```\nPOST /api/v1/projects/{projectId}/experiment \n e.g. curl -X POST -H "Content-Type: application/json" -d \'{projectOptions}\' http://{FGLab address}/api/v1/projects/{projectId}/experiment\n``` \n If the project does not exist, returns  400 {"error": "Project ID <projectId> does not exist"} . If the  projectOptions  are invalid, returns  400 {"error": "<validation message>"} . If no machines are available to run the job, returns  501 {"error": "No machine capacity available"} . If the machine fails to run the experiment for some reason, returns  500 {"error": "Experiment failed to run"} . If successful, returns  201 {"_id": "<experimentId>"} . \n Start a batch job with a list of option sets \n```\nPOST /api/v1/projects/{projectId}/batch?retry={retryTimeout (optional)} \n e.g. curl -X POST -H "Content-Type: application/json" -d \'[{projectOptions}]\' http://{FGLab address}/api/v1/projects/{projectId}/batch?retry={retryTimeout (optional)}\n``` \n The optional  retry  parameter specifies the maximum time in seconds to wait before trying to run a queued job again after capacity has been reached (the interval is randomly picked from a uniform distribution). If the project does not exist, returns  400 {"error": "Project ID <projectId> does not exist"} . If any of the  projectOptions  are invalid, returns  400 {"error": "<validation message>"}  for the first set of options that are wrong. If successful, returns  201 {"status": "Started"} . Future work aims to create a proper "optimiser" object that can be queried and have its work queue adjusted appropriately (hence differentiating it from a simple batch job queue). \n Register a webhook for an event \n```\nPOST /api/v1/webhooks \n e.g. curl -X POST -H "Content-Type: application/json" -d \'{webhookOptions}\' http://{FGLab address}/api/v1/webhooks\n``` \n webhookOptions  expects the following options \n json\n{\n  "url": "<URL to POST to>",\n  "objects": "<object collection to listen to (currently only \'experiments\')>",\n  "object_id": "<object ID>",\n  "event": "<event to listen to (currently only \'started\' or \'finished\')>"\n} \n If a valid URL is not provided, returns  400 {"error": "Invalid or empty URL"} . If a valid object collection is not provided, returns  400 {"error": "Object is not \'experiments\'"} . If a valid event is not provided, returns  400 {"error": "Event is not \'started\' or \'finished\'"} . If an object ID is not provided, returns  400 {"error": "No object ID provided"} . If successful, returns  201 {"status": "Registered", "options": <webhookOptions>"} . When the event occurs, the JSON data used to register the webhook is returned.', '\n \n \n \n \n \n \n \n Quickstart: https://kaixhin.github.io/FGLab/ \n FGLab is a machine learning dashboard, designed to make prototyping experiments easier. Experiment details and results are sent to a database, which allows analytics to be performed after their completion. The server is  FGLab , and the clients are FGMachines. \n Contents \n \n Installation \n Overview \n Examples \n \n Installation \n FGMachine tries to follow the  SemVer  standard whenever possible. Releases can be found  here . \n Option 1: Local \n \n Install  Node.js  from the website or your package manager. \n Either clone this repository or download and extract a  zip / tar . \n Move inside the FGMachine folder. \n Run  npm install . \n FGMachine requires a  .env  file in this directory. For most installations, it should be possible to copy  example.env  to  .env , but it may require customisation for non-standard FGLab or FGMachine ports. An alternative is to set the following environment variables: \n FGLAB_URL (FGLab URL, including port if necessary) \n FGMACHINE_URL (FGMachine URL, including port) \n \n Run  node machine  (or  npm start ) to start FGMachine. On the first run it will create  specs.json  and register itself with FGLab. Please read the  overview  to understand how FGMachine can interface with your machine learning code. \n Note: If you use a virtual environment, e.g.  virtualenv , activate the environment before running  node machine . \n Note: If you delete your machine in FGLab, delete  specs.json  before running FGMachine again to re-register. \n To update, run  npm run update . \n Option 2: Docker \n Start a  FGLab container  and link it to the  FGMachine container : \n sh\nsudo docker run -d --name fgmachine -h $(hostname) -v /var/run/docker.sock:/var/run/docker.sock -e FGLAB_URL=<FGLab URL> -e FGMACHINE_URL=<FGMachine URL> -p 5081:5081 kaixhin/fgmachine \n The  FGLab URL  will be the address of the host running FGLab, including the protocol ("http://") and port (:5080) - note that  localhost  will not work but the local network IP/hostname should. The  FGMachine URL  will be the address of the current host (as accessible by FGLab), including the protocol ("http://") and port (:5081). Docker\'s socket is passed to allow FGMachine to launch Docker containers itself. Note that as these are  sibling  containers, volume mounts ( -v ) are relative to the host, not the FGMachine container. \n To launch  NVIDIA Docker  containers, use the following: \n sh\nsudo docker run -d --name fgmachine -h $(hostname) -v /var/run/docker.sock:/var/run/docker.sock --net=host `curl -s localhost:3476/docker/cli` -e FGLAB_URL=<FGLab URL> -e FGMACHINE_URL=<FGMachine URL> -p 5081:5081 kaixhin/fgmachine \n Note that  --net=host  is passed to allow access to the NVIDIA Docker API. When launching a sibling container, you will need to run  `curl -s localhost:3476/docker/cli`  and manually add the arguments to the project implementation in the container, with  docker  as the command (do not use  nvidia-docker ). \n Overview \n Projects \n After a project has been created on FGLab, a corresponding  project implementation  must be specified in  projects.json . If this machine is available to run experiments for the project created on FGLab, then add the following field to  projects.json  (an example is available at  example.projects.json ). FGLab has an "Add to Machine" button which can automatically set up a template in  projects.json  for you (creating  projects.json  if it doesn\'t exist already).  Note that  <project_id>  links the created project on FGLab and FGMachine\'s project implemetations in  projects.json . \n json\n"<project_id>": {\n  "cwd": "<working directory (e.g. .)>",\n  "command": "<program (e.g. caffe)>",\n  "args": "<first command line options (e.g. train)>",\n  "options": "<command line options style for options (e.g. double-dash)>",\n  "boolean": "<optional: only pass flag if true, mandatory: pass flag and true/false argument>",\n  "capacity": "<machine capacity needed (as a fraction) (e.g. 0.5)>",\n  "results": "<absolute path to results directory (without experiment ID) (e.g. results)>"\n} \n cwd  is the working directory for the machine learning code.  cwd  can either be an absolute path, or a relative path, in which case it it relative to the FGMachine directory.  command  is the program/executable to be run.  args  is the first set of command line options to be sent to the program, prior to the experiment options.  options  processes the options in 4 different ways. For option settings:  {seed: 123, model: "cnn.v2", L2: true} , exemplar methods would be as such (with  boolean  as  "mandatory" ): \n |  options    | Program | Command Line [command] [args] [options]                                         |\n|-------------|---------|---------------------------------------------------------------------------------|\n| plain       | node    | node [args] seed 123 model cnn.v2 L2 true                                       |\n| single-dash | th      | th [args] -seed 123 -model cnn.v2 -L2 true                                      |\n| double-dash | caffe   | caffe [args] --seed=123 --model=cnn.v2 --L2=true                                |\n| function    | matlab  | matlab [args w/o final arg] [final arg](\'seed\',123,\'model\',\'cnn.v2\',\'L2\',true) | \n boolean  can be set to  "optional"  when boolean flags should be passed only when true, e.g.,  -L2 , or set to  "mandatory"  if the value should always be passed, e.g.,  -L2 true  and  -L2 false .  capacity  is a number between in the range 0-1 (inclusive) that represents (the inverse of) the amount of instances of the program the FGMachine host system can run in parallel (as a heuristic); for example a  capacity  of 0.5 indicates that the host is only capable of running 2 instances of the program at once.  results  is the directory in which the experiment results must be written into (see below for more details).  results  can either be an absolute path, or a relative path, in which case it it relative to the FGMachine directory. \n If you receive a "No machine capacity available" error message when submitting a new experiment, which can occur erroneously (for example, if experiments crash), then you can reset a machine\'s capacity on the machine\'s page in FGLab. \n FGMachine automatically reloads the  projects.json  file when it is changed. \n GPU capacity support \n In order to handle projects, which require GPUs to perform a task, you need to add two parameters for each project in  projects.json  file: \n json\n{\n  "gpu_capacity": "<gpu capacity needed (as a fraction of one GPU capacity, e.g. 0.5)>",\n  "gpu_command": "<option to pass to script to identify card number, including command line option style (e.g. -gpu)>",\n} \n Note that  gpu_capacity  represents (the inverse of) instances of the program the FGMachine host system can run on one GPU; for example a machine with 4 GPUs will be able to run 8 instances of the program with  capacity  0.1 and  gpu_capacity  0.5. However, if the  capacity  was 0.25 in the previous example, the machine would only be able to run 4 instances of the program. \n gpu_capacity  automatically assigns a GPU for experiments, which makes it easier to run batch experiments. Note that like  nvidia-smi , GPU IDs passed via  gpu-command  are  0-indexed . For manual control, it is recommended to use a GPU flag as part of the experiment hyperparameters in the project schema. \n Experiments \n Results and custom data must be saved as files into a subfolder in the specified results directory, where the name of the subfolder is the experiment ID, e.g.  /data/mnist/55e069f9cf4e1fe075b76b95 . For an example that uses the following features, see  rand.js .  \n Non-JSON files are uploaded to MongoDB  GridFS  via FGLab, which allows them to be downloaded later in their native format. Images and videos are automatically displayed on the experiment page, allowing plots to be created by the machine learning code. JSON files are automatically parsed, with fields being added to the experiment object. An example,  notes.json , may look like this: \n json\n{\n  "Framework": {\n    "Name": "Theano",\n    "Version Number": 0.7\n  },\n  "Notes": "Best parameters saved at epoch 55"\n} \n Multiple top-level fields can exist in the same file, but nested fields cannot be updated separately e.g.  Framework.Name .  Note that fields preceded with  _  are reserved for processing by FGLab . Currently supported fields are listed below: \n _scores \n The  _scores  field is a map that can be used to store multiple floats that represent the performance of the model. For example: \n json\n{\n  "_scores": {\n    "F1": "float",\n    "BLEU": "float",\n    "METEOR": "float"\n  }\n} \n _notes \n The  _notes  field is a free-form text field. Its primary use is via the experiment page on FGLab, where text written in the "Notes" text box is automatically saved (at an interval of 0.5s), displaying on both the experiment page itself and the table of experiment results. \n _charts \n The  _charts  field is a either an object or array of objects that can be used to store data that will be charted on FGLab using  C3.js , and hence mimics its  API . Given that FGLab renders uploaded images, this is to allow the interactivity afforded by C3.js. This means that it is possible to create different chart types and adjust plotting options, with a minor change in the API so that numeric arrays can be directly exported. Rather than prepending arrays in the  columns  array with the column names, the  columnNames  array is used to perform this on FGLab. \n Charts with lots of values are downsampled for performance reasons, using the  Largest-Triangle-Three-Buckets algorithm  for visualisation purposes. By default the following options are added to disable points and enable zoom, but these can be overriden: \n json\n{\n  "point": {"show": false},\n  "zoom": {"enabled": true}\n} \n An example  Multiple XY Line Chart  would be structured as such: \n json\n{\n  "_charts": {\n    "columnNames": [\n      "train",\n      "val",\n      "x1",\n      "x2"\n    ],\n    "data": {\n      "xs": {\n        "train": "x1",\n        "val": "x2"\n      },\n      "columns": [\n        [1.0, 0.8, 0.6, 0.4, 0.3, 0.2, 0.1, 0.1, 0.1, 0.1, 0.0],\n        [1.0, 0.9, 0.6, 0.4, 0.3],\n        [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n        [2, 6, 8, 9, 11]\n      ]\n    },\n    "axis": {\n      "x": {\n        "label": {\n          "text": "Iterations"\n        }\n      },\n      "y": {\n        "label": {\n          "text": "Losses"\n        }\n      }\n    }\n  }\n} \n The usage of  _charts  has an inherent tradeoff between storing numerical results in a more intuitive place in the experiment object and easily visualising data. The recommendation is to use  _charts  for visualising data where desired (which may not be necessary if plots are generated by the machine learning code), and extract the data given the  _charts \' structure. However, it is still possible to duplicate the numerical results in a separate array under a custom field in a JSON file. \n Examples \n Examples utilising the range of abilities of FGLab/FGMachine can be found in the  examples folder .', "nninit \n Parameter initialisation schemes for Torch7 neural network modules. Works with  nn , and therefore  nngraph . Allows arbitrary indexing of weights/biases/parameters. Supported modules: \n \n nn.Linear / nn.LinearNoBias \n nn.LookupTable \n nn.TemporalConvolution \n nn.SpatialConvolution / cudnn.SpatialConvolution \n nn.VolumetricConvolution / cudnn.VolumetricConvolution \n \n Readme contents: \n \n Installation \n Usage \n Example \n Development \n Acknowledgements \n \n Installation \n sh\nluarocks install nninit \n Usage \n nninit  adds an  init  method to  nn.Module , with the following API: \n lua\nmodule:init(accessor, initialiser, ...) \n The  accessor  argument is used to extract the tensor to be initialised from the module. The  initialiser  argument is a function that takes the module, tensor, and further options; it adjusts the tensor and returns the module, allowing  init  calls to be chained.  nninit  comes with several initialiser functions.  ...  represents additional arguments for the initialiser function. \n Accessors \n The  accessor  argument is used to extract the tensor to be initialised from the module. It can either be a string, table, or function.  \n string \n The tensor is accessed as a property of the module. For example: \n lua\nmodule:init('weight', nninit.constant, 1) \n table \n The tensor is first accessed as a property of the module from the first element, and a subtensor is then extracted using Torch's  indexing operator  applied to the second element. For example: \n lua\nmodule:init({'weight', {{1, 5}, {}}}, nninit.uniform, -1, 1) \n function \n The tensor must be returned as the result of the function applied to the module. For example: \n lua\nmodule:init(function(m) return m.weight:narrow(1, 1, 10) end, nninit.normal, 0, 0.01) \n Initialisers \n nninit.copy(module, tensor, init) \n Copies the  init  tensor to the tensor to be initialised. \n nninit.constant(module, tensor, val) \n Fills tensor with the constant  val . \n nninit.addConstant(module, tensor, val) \n Adds to current tensor with the constant  val . \n nninit.mulConstant(module, tensor, val) \n Multiplies current tensor by the constant  val . \n nninit.normal(module, tensor, mean, stdv) \n Fills tensor ~ N( mean ,  stdv ). \n nninit.addNormal(module, tensor, mean, stdv) \n Adds to current tensor with ~ N( mean ,  stdv ). \n nninit.uniform(module, tensor, a, b) \n Fills tensor ~ U( a ,  b ). \n nninit.addUniform(module, tensor, a, b) \n Adds to current tensor with ~ U( a ,  b ). \n nninit.eye(module, tensor) \n Only supports the module weights as the tensor. Relies on the module type to determine appropriate identity. \nFills weights with the identity matrix (for linear layers/lookup tables). \nFills filters with the Dirac delta function (for convolutional layers). Normalises by the number of input layers. \n nninit.xavier(module, tensor, [{[dist], [gain]}]) \n Fills tensor with  stdv = gain * sqrt(2 / (fanIn + fanOut)) . Uses the uniform distribution by default. \nOptional named parameters  dist  and  gain  can be passed in via a table. \nAlso known as Glorot initialisation. \n \n Glorot, X., & Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. In  International Conference on Artificial Intelligence and Statistics . \n \n nninit.kaiming(module, tensor, [{[dist], [gain]}]) \n Fills tensor with  stdv = gain * sqrt(1 / fanIn) . Uses the normal distribution by default. \nOptional named parameters  dist  and  gain  can be passed in via a table. The initialisation scheme typically includes the gain for ReLU units, which has to be manually specified in  nninit.kaiming  with the option  {gain = 'relu'} . \nAlso known as He initialisation. \n \n He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification.  arXiv preprint arXiv:1502.01852 . \n \n nninit.orthogonal(module, tensor, [{[gain]}]) \n Only supports tensors with at least 2 dimensions. \nFills tensor with a (normally distributed) random orthogonal matrix. \nOptional named parameter  gain  can be passed in via a table. \n \n Saxe, A. M., McClelland, J. L., & Ganguli, S. (2013). Exact solutions to the nonlinear dynamics of learning in deep linear neural networks.  arXiv preprint arXiv:1312.6120 . \n \n nninit.sparse(module, tensor, sparsity) \n Sets  (1 - sparsity)  percent of the tensor to 0, where  sparsity  is between 0 and 1. For example, a  sparsity  of 0.2 drops out 80% of the tensor. \n \n Martens, J. (2010). Deep learning via Hessian-free optimization. In  Proceedings of the 27th International Conference on Machine Learning (ICML-10) . \n \n nninit.convolutionAware(module, tensor, [{[gain], [std]}]) \n Only supports 2D convolutions with a symmetric filter size. \nFills convolution tensor with matrices that are orthogonal in the frequency space.\nThe initialisation scheme described in the paper includes the gain for ReLU units, which has to be manually specified with the option  {gain = 'relu'} .\nThe optional named parameter  std  can be passed in via a table. It specifies the noise to break symmetry in the inverse Fourier transform. \n \n Aghajanyan, A. (2017). Convolution Aware Initialization.  arXiv preprint arXiv:1702.06295 . \n \n Dists \n The 2 types of distribution supported are  'normal'  and  'uniform' . \n Gains \n Gains can be calculated depending on the succeeding nonlinearity. If  gain  is a number it is used directly; if  gain  is a string the following mapping is used. By default gains (where applicable) are set to 1. \n | Gain      | Parameters | Mapping                     |\n|-----------|------------|-----------------------------|\n| 'linear'  |            | 1                           |\n| 'sigmoid' |            | 1                           |\n| 'tanh'    |            | 5 / 3                       |\n| 'relu'    |            | sqrt(2)                     |\n| 'lrelu'   | leakiness  | sqrt(2 / (1 + leakiness^2)) | \n If the  gain  must be calculated from additional parameters,  gain  must be passed as table with the string as the first element as well as named parameters. For example: \n lua\nmodule:init('weight', nninit.kaiming, {gain = {'lrelu', leakiness = 0.3}}) \n Example \n ```lua\nlocal nn = require 'nn'\nrequire 'cunn'\nlocal cudnn = require 'cudnn'\nrequire 'rnn'\nlocal nninit = require 'nninit' \n local getBias = function(module)\n  return module.bias\nend \n local batchSize = 5\nlocal imgSize = 16\nlocal nChannels = 3\nlocal nFilters = 8\nlocal rho = 6\nlocal hiddenSize = 2 \n local cnn = nn.Sequential()\ncnn:add(cudnn.SpatialConvolution(nChannels, nFilters, 2, 2):init('weight', nninit.eye)\n                                                           :init('weight', nninit.mulConstant, 1/2)\n                                                           :init('weight', nninit.addNormal, 0, 0.01)\n                                                           :init(getBias, nninit.constant, 0))\ncnn:add(nn.View(nFilters 15 15))\ncnn:add(nn.Linear(nFilters 15 15, nFilters):init('weight', nninit.kaiming, {\n  dist = 'uniform',\n  gain = {'lrelu', leakiness = 0.3}\n}))\ncnn:add(nn.RReLU(1/3, 1/3))\ncnn:add(nn.Linear(nFilters, 6):init('weight', nninit.orthogonal, {gain = 'relu'}))\ncnn:add(cudnn.ReLU())\ncnn:add(nn.Linear(6, 4):init('weight', nninit.xavier, {dist = 'normal', gain = 1.1}))\ncnn:add(nn.Linear(4, hiddenSize):init('weight', nninit.sparse, 0.2)\n                                :init(getBias, nninit.constant, 0)) \n local model = nn.Sequential()\nmodel:add(nn.Sequencer(cnn))\nlocal lstm = nn.FastLSTM(hiddenSize, hiddenSize, rho)\n-- Note that chaining will pass through the module initialised, never parents\nlstm.i2g:init({'bias', {{2 hiddenSize+1, 3 hiddenSize}}}, nninit.constant, 1) -- High forget gate bias\nmodel:add(nn.Sequencer(lstm))\nmodel:cuda() \n local inputs = {}\nfor i = 1, rho do\n  table.insert(inputs, torch.ones(batchSize, nChannels, imgSize, imgSize):cuda())\nend\nprint(model:forward(inputs))\n``` \n Development \n To develop  nninit /use it to test new initialisation schemes,  git clone /download this repo and use  luarocks make rocks/nninit-scm-1.rockspec  to install  nninit  locally. \n Acknowledgements \n \n Lasagne \n Purdue e-Lab Torch Toolbox \n", "Easy21 \n Assignment from David Silver's  Reinforcement Learning course . Coded for clarity, not efficiency. \n Requires  Torch7  with the  Moses  package. \n Run  monte-carlo.lua  first to generate Q* and the plot of V (below), then  sarsa-lambda.lua  and  lin-fun-approx.lua  to generate their plots. \n Includes an additional method without value functions -  policy-gradient.lua  - that uses a simple neural network. \n", 'Atari  \n \n \n \n Work In Progress:  Crossed out items have been partially implemented. \n ~~Prioritised experience replay~~  [1]  persistent advantage learning  [2]  ~~bootstrapped~~  [3]  dueling  [4]  double  [5]  deep ~~recurrent~~  [6]  Q-network  [7]  for the Arcade Learning Environment  [8]  (and  custom environments ). Or PERPALB(triple-D)RQN for short... \n Additional asynchronous agents  [9] : \n \n One-step Sarsa \n One-step Q-learning \n N-step Q-learning \n Advantage actor-critic \n \n Run  th main.lua  to run headless, or  qlua main.lua  to display the game. The main options are  -game  to choose the ROM (see the  ROM directory  for more details) and  -mode  as either  train  or  eval . Can visualise saliency maps  [10] , optionally using guided  [11]  or "deconvnet"  [12]  backpropagation. Saliency map modes are applied at runtime so that they can be applied retrospectively to saved models. \n To run experiments based on hyperparameters specified in the individual papers, use  ./run.sh <paper> <game> <args> .  <args>  can be used to overwrite arguments specified earlier (in the script); for more details see the script itself. By default the code trains on a demo environment called Catch - use  ./run.sh demo  to run the demo with good default parameters. Note that this code uses CUDA if available, but the Catch network is small enough that it runs faster on CPU. If cuDNN is available, it can be enabled using  -cudnn true ; note that by default cuDNN is nondeterministic, and its deterministic modes are slower than cutorch. \n In training mode if you want to quit using  Ctrl+C  then this will be caught and you will be asked if you would like to save the agent. Note that for non-asynchronous agents the experience replay memory will be included, totalling ~7GB. The main script also automatically saves the last weights ( last.weights.t7 ) and the weights of the best performing DQN (according to the average validation score) ( best.weights.t7 ). \n In evaluation mode you can create recordings with  -record true  (requires FFmpeg); this does not require using  qlua . Recordings will be stored in the videos directory. \n Requirements \n Requires  Torch7 , and can use CUDA and cuDNN if available. Also requires the following extra luarocks packages: \n \n luaposix 33.4.0 \n luasocket \n moses \n logroll \n classic \n torchx \n rnn \n dpnn \n nninit \n tds \n xitari \n alewrap \n rlenvs \n \n xitari, alewrap and rlenvs can be installed using the following commands: \n sh\nluarocks install https://raw.githubusercontent.com/lake4790k/xitari/master/xitari-0-0.rockspec\nluarocks install https://raw.githubusercontent.com/Kaixhin/alewrap/master/alewrap-0-0.rockspec\nluarocks install https://raw.githubusercontent.com/Kaixhin/rlenvs/master/rocks/rlenvs-scm-1.rockspec \n Custom \n You can use a custom environment (as the path to a Lua file/ rlenvs -namespaced environment) using  -env , as long as the class returned respects the  rlenvs   API . One restriction is that the state must be represented as a single tensor (with arbitrary dimensionality), and only a single discrete action must be returned. To prevent massive memory consumption for agents that use experience replay memory, states are discretised to integers ∈ [0, 255], assuming the state is comprised of reals ∈ [0, 1] - this can be disabled with  -discretiseMem false . Visual environments can make use of explicit  -height ,  -width  and  -colorSpace  options to perform preprocessing for the network. \n If the environment has separate behaviour during training and testing it should also implement  training  and  evaluate  methods - otherwise these will be added as empty methods during runtime. The environment can also implement a  getDisplay  method (with a mandatory  getDisplaySpec  method for determining screen size) which will be used for displaying the screen/computing saliency maps, where  getDisplay  must return a RGB (3D) tensor; this can also be utilised even if the state is not an image (although saliency can only be computed for states that are images). This  must  be implemented to have a visual display/computing saliency maps. The  -zoom  factor can be used to increase the size of small displays. \n Environments are meant to be ephemeral, as an instance is created in order to first extract environment details (e.g. state representation), which will later be automatically garbage collected (not under the control of this code). \n You can also use a custom model (body) with  -modelBody , which replaces the usual DQN convolutional layers with a custom Torch neural network (as the path to a Lua file/ models -namespaced environment). The class must include a  createBody  method which returns the custom neural network. The model will receive a stack of the previous states (as determined by  -histLen ), and must reshape them manually if needed. The DQN "heads" will then be constructed as normal, with  -hiddenSize  used to change the size of the fully connected layer if needed. \n For an example on a GridWorld environment, run  ./run.sh demo-grid  - the demo also works with  qlua  and experience replay agents. The custom environment and network can be found in the  examples  folder. \n Results \n Single run results from various papers can be seen below. DQN-based agents use  ε = 0.001  for evaluation  [4, 5] .  \n DQN (Space Invaders)  [7] \n \n Double DQN (Space Invaders)  [5] \n \n Dueling DQN (Space Invaders)  [4] \n \n Persistent Advantage Learning DQN (Asterix)  [2] \n \n A3C (Beam Rider)  [9] \n \n Acknowledgements \n \n @GeorgOstrovski  for confirmation on network usage in advantage operators + note on interaction with Double DQN. \n @schaul  for clarifications on prioritised experience replay + dueling DQN hyperparameters. \n \n Citation \n If you find this library useful and would like to cite it, the following would be appropriate: \n @misc{Atari,\n  author = {Arulkumaran, Kai and Keri, Laszlo},\n  title = {Kaixhin/Atari},\n  url = {https://github.com/Kaixhin/Atari},\n  year = {2015}\n} \n References \n [1]  Prioritized Experience Replay \n[2]  Increasing the Action Gap: New Operators for Reinforcement Learning \n[3]  Deep Exploration via Bootstrapped DQN \n[4]  Dueling Network Architectures for Deep Reinforcement Learning \n[5]  Deep Reinforcement Learning with Double Q-learning \n[6]  Deep Recurrent Q-Learning for Partially Observable MDPs \n[7]  Playing Atari with Deep Reinforcement Learning \n[8]  The Arcade Learning Environment: An Evaluation Platform for General Agents \n[9]  Asynchronous Methods for Deep Reinforcement Learning \n[10]  Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps \n[11]  Striving for Simplicity: The All Convolutional Net \n[12]  Visualizing and Understanding Convolutional Networks  ', 'rlenvs \n Reinforcement learning environments for Torch7, inspired by  RL-Glue   [1]  and conforming to the  OpenAI Gym API   [2] . Supported environments: \n \n rlenvs.Acrobot  [3] \n rlenvs.Atari (Arcade Learning Environment)*  [4] \n rlenvs.Blackjack  [5] \n rlenvs.CartPole  [6] \n rlenvs.Catch  [7] \n rlenvs.CliffWalking  [8] \n rlenvs.DynaMaze  [9] \n rlenvs.GridWorld  [10] \n rlenvs.JacksCarRental  [8] \n rlenvs.Minecraft (Project Malmö)*  [11] \n rlenvs.MountainCar  [12] \n rlenvs.MultiArmedBandit  [13, 14] \n rlenvs.RandomWalk  [15] \n rlenvs.Taxi  [16] \n rlenvs.WindyWorld  [8] \n rlenvs.XOWorld  [17] \n \n Run  th experiment.lua  (or  qlua experiment.lua ) to run a demo of a random agent playing Catch. \n * Environments with other dependencies are installed only if those dependencies are available. \n Installation \n sh\nluarocks install https://raw.githubusercontent.com/Kaixhin/rlenvs/master/rocks/rlenvs-scm-2.rockspec \n The old API can be installed with the  v1  rockspec: \n sh\nluarocks install https://raw.githubusercontent.com/Kaixhin/rlenvs/master/rocks/rlenvs-scm-1.rockspec \n Atari Dependencies \n sh\nluarocks install https://raw.githubusercontent.com/lake4790k/xitari/master/xitari-0-0.rockspec\nluarocks install https://raw.githubusercontent.com/Kaixhin/alewrap/master/alewrap-0-0.rockspec \n Requires a  supported  Atari ROM to run. \n Minecraft Dependencies \n sh\nluarocks install luasocket \n Requires  Malmö  (includes Minecraft), extracted with directory name  MalmoPlatform .  libMalmoLua.so  should be added to  LUA_CPATH , and the level schemas should be exported to  MALMO_XSD_PATH . For example, if  MalmoPlatform  is in  /home/username , add the following to the end of your  ~/.bashrc : \n sh\nexport LUA_CPATH=\'/home/username/MalmoPlatform/Torch_Examples/?.so;\'$LUA_CPATH\nexport MALMO_XSD_PATH=/home/username/MalmoPlatform \n The Malmö client ( launchClient.sh ) must be operating to run. \n Usage \n To use an environment,  require  it and then create a new instance: \n lua\nlocal MountainCar = require \'rlenvs.MountainCar\'\nlocal env = MountainCar()\nlocal observation = env:start() \n API \n Note that the API is under development and may be subject to change \n rlenvs.envs \n A table of all environments available in  rlenvs . \n observation = env:start([opts]) \n Starts a new episode in the environment and returns the first  observation . May take  opts . \nNote that environments must actually implement this as  _start . \n reward, observation, terminal, [actionTaken] = env:step(action) \n Performs a step in the environment using  action  (which may be a list - see below), and returns the  reward , the  observation  of the state transitioned to, and a  terminal  flag. Optionally provides  actionTaken , if the environment provides supervision in the form of the actual action taken by the agent in spite of the provided action. \nNote that environments must actually implement this as  _step . \n stateSpace = env:getStateSpace() \n Returns a state specification as a list with 3 elements: \n | Type     | Dimensionality                                              | Range                                              |\n|----------|-------------------------------------------------------------|----------------------------------------------------|\n| \'int\'    | 1 for a single value, or a table of dimensions for a Tensor | 2-element list with min and max values (inclusive) |\n| \'real\'   | 1 for a single value, or a table of dimensions for a Tensor | 2-element list with min and max values (inclusive) |\n| \'string\' |  TODO                                                     | List of accepted strings                           | \n If several states are returned,  stateSpec  is itself a list of state specifications. Ranges may use  nil  if unknown. \n actionSpace = env:getActionSpace() \n Returns an action specification, with the same structure as used for state specifications. \n minReward, maxReward = env:getRewardSpace() \n Returns the minimum and maximum rewards produced by the environment. Values may be  nil  if unknown. \n \n The following are optional parts of the API. \n env:training() \n Changes settings for a "training mode", analogous to neural network modules. \n env:evaluate() \n Changes settings for an "evaluation mode", analogous to neural network modules. \n displaySpec = env:getDisplaySpec() \n Returns an RGB display specification, with the same structure as used for state specifications. Hence of the form  {<int/real>, {3, <height>, <width>}, {<range>}} . \n display = env:getDisplay() \n Returns a RGB display tensor for visualising the state of the environment. Note that this may not be the same as the state provided for the agent. \n env:render() \n Displays the environment using  image . Requires the code to be run with  qlua  (rather than  th ) and  getDisplay  to be implemented by the environment. \n Development \n Environments must inherit from  Env  and therefore implement the above methods (as well as a constructor).  experiment.lua  can be easily adapted for testing different environments. New environments should be added to  rlenvs/init.lua ,  rocks/rlenvs-scm-1.rockspec , and be listed in this readme with an appropriate reference. For an example of a more complex environment that will only be installed if its optional dependencies are satisfied, see  rlenvs/Atari.lua . \n References \n [1] Tanner, B., & White, A. (2009). RL-Glue: Language-independent software for reinforcement-learning experiments.  The Journal of Machine Learning Research, 10 , 2133-2136. \n[2] Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., & Zaremba, W. (2016). OpenAI Gym.  arXiv preprint arXiv:1606.01540 . \n[3] DeJong, G., & Spong, M. W. (1994, June). Swinging up the acrobot: An example of intelligent control. In  American Control Conference, 1994  (Vol. 2, pp. 2158-2162). IEEE. \n[4] Bellemare, M. G., Naddaf, Y., Veness, J., & Bowling, M. (2012). The arcade learning environment.  Journal of Artificial Intelligence Research, 47 , 253-279. \n[5] Pérez-Uribe, A., & Sanchez, E. (1998, May). Blackjack as a test bed for learning strategies in neural networks. In  Neural Networks Proceedings, 1998. IEEE World Congress on Computational Intelligence. The 1998 IEEE International Joint Conference on  (Vol. 3, pp. 2022-2027). IEEE. \n[6] Barto, A. G., Sutton, R. S., & Anderson, C. W. (1983). Neuronlike adaptive elements that can solve difficult learning control problems.  Systems, Man and Cybernetics, IEEE Transactions on , (5), 834-846. \n[7] Mnih, V., Heess, N., & Graves, A. (2014). Recurrent models of visual attention. In  Advances in Neural Information Processing Systems  (pp. 2204-2212). \n[8] Sutton, R. S., & Barto, A. G. (1998).  Reinforcement learning: An introduction  (Vol. 1, No. 1). Cambridge: MIT press. \n[9] Sutton, R. S. (1990). Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In  Proceedings of the Seventh International Conference on Machine Learning  (pp. 216-224). \n[10] Boyan, J., & Moore, A. W. (1995). Generalization in reinforcement learning: Safely approximating the value function.  Advances in Neural Information Processing Systems , 369-376. \n[11] Johnson, M., Hofmann, K., Hutton, T., & Bignell, D. (2016). The Malmo platform for artificial intelligence experimentation. In  International Joint Conference on Artificial Intelligence . \n[12] Singh, S. P., & Sutton, R. S. (1996). Reinforcement learning with replacing eligibility traces.  Machine Learning, 22 (1-3), 123-158. \n[13] Robbins, H. (1985). Some aspects of the sequential design of experiments. In  Herbert Robbins Selected Papers  (pp. 169-177). Springer New York. \n[14] Whittle, P. (1988). Restless bandits: Activity allocation in a changing world.  Journal of Applied probability , 287-298. \n[15] Sutton, R. S. (1988). Learning to predict by the methods of temporal differences.  Machine Learning, 3 (1), 9-44. \n[16] Dietterich, T. G. (2000). Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition. In  Journal of Artificial Intelligence Research . \n[17] Garnelo, M., Arulkumaran, K., & Shanahan, M. (2016). Towards Deep Symbolic Reinforcement Learning. In  Workshop on Deep Reinforcement Learning, NIPS 2016 .', 'Human-Level Control Through Deep Reinforcement Learning \n Presentation on  Human-Level Control Through Deep Reinforcement Learning . Also includes  Deep Reinforcement Learning with Double-Q Learning  and  Dueling Network Architectures for Deep Reinforcement Learning . Simply open  index.html  in a web browser (internet connection required). \n Personal work on deep reinforcement learning for the  Arcade Learning Environment : https://github.com/Kaixhin/Atari', "torch-pastalog \n A Torch interface for  pastalog  - simple, realtime visualization of neural network training performance. Requires a running pastalog server. \n Installation \n sh\nluarocks install https://raw.githubusercontent.com/Kaixhin/torch-pastalog/master/rocks/pastalog-scm-1.rockspec \n Usage \n See the  pastalog installation instructions  for details on how to set up and start the pastalog Node.js server. Once running: \n ```lua\nlocal pastalog = require 'pastalog' \n --[[\n      Machine learning code\n--]] \n -- API is pastalog(modelName, seriesName, value, step, [url])\npastalog('model1', 'validLoss', 2.5, 1)\npastalog('model1', 'validLoss', 2.5, 1, 'http://localhost:8120/data')\n```", '\n \n docker-torch-mega \n Available from the Docker Hub as  kaixhin/cuda-torch-mega . \n Ubuntu Core 14.04 +  CUDA  +  cuDNN v5  +  Torch7  (including iTorch). Includes the following additional packages: \n \n alewrap \n autograd \n classic \n dataset \n distlearn \n dpnn \n ffmpeg \n gvnn \n imagine-nn \n ipc \n loadcaffe \n logroll \n luaposix \n LuaSocket \n Moses \n nninit \n nnquery \n OptNet \n parallel \n rlenvs \n rnn \n signal \n tds \n torchnet \n torchx \n twrl \n xitari \n \n Requirements \n \n NVIDIA Docker  - see  requirements  for more details. \n \n Usage \n Use NVIDIA Docker:  nvidia-docker run -it kaixhin/cuda-torch-mega . \n For more information on CUDA on Docker, see  this readme . \n To use Jupyter/iTorch open up the appropriate port. For example, use  nvidia-docker run -it -p 8888:8888 kaixhin/cuda-torch-mega . Then run  jupyter notebook --ip="0.0.0.0" --no-browser  to open a notebook on  localhost:8888 .', 'IncSFA \n Incremental Slow Feature Analysis,  ported from MATLAB . \n Examples \n example_low_dim \n This is the toy example, first introduced by Wiskott in the SFA paper. It will show the feature outputs, slowness measured, and feature correlation during the training. IncSFA is driven to learn slow features that are as decorrelated as possible. One can watch this interplay between the two constraints during the learning. \n example_high_dim_images \n Requires  hdf5 . Warning: launches many gnuplot windows when  INCREMENTAL_VIZ = true . \n Here, IncSFA is applied to a sequence of high-dimensional images. It uses the iCubArm data in the Data folder (not included in this repo), stored already in matrix form in  robotdata.h5 . During training, it will show the feature outputs on a few selected episodes, as well as the slowness measured and the mutual correlation of the features. This takes over 100 episodes of training to really start showing good features, sometimes longer. It is currently set to stop at 200 episodes. Once it stops, it will run  view_result , which shows side-by-side the image sequences and the slow feature embedding. During training, it periodically stores the features in  feature_saved.t7 . \n view_result \n Used to view the embedding from  feature_saved.t7 . \n Functions \n | Function   | Description                                                                                            |\n|------------|--------------------------------------------------------------------------------------------------------|\n| amnesic    | Amnesic averaging, to set learning rates.                                                              |\n| quadexpand | Quadratic expansion of a column vector.                                                                |\n| CCIPCA     | Candid covariance free incremental principal component analysis.                                       |\n| CIMCA      | Covariance free incremental minor component analysis.                                                  |\n| IMCA       | Incremental minor component analysis. Doesn\'t normalize the features so there may be stability issues. | \n Citation \n \n V. R. Kompella, M. Luciw and J. Schmidhuber. "Incremental Slow Feature Analysis: Adaptive Low-Complexity Slow Feature Updating from High-Dimensional Input Streams", Neural Computation Journal, Vol. 24 (11), pp. 2994--3024, 2012. \n', 'Autoencoders \n This repository is a Torch version of  Building Autoencoders in Keras , but only containing code for reference - please refer to the original blog post for an explanation of autoencoders. Training hyperparameters have not been adjusted. The following models are implemented: \n \n AE : Fully-connected autoencoder \n SparseAE : Sparse autoencoder \n DeepAE : Deep (fully-connected) autoencoder \n ConvAE : Convolutional autoencoder \n UpconvAE : Upconvolutional autoencoder - also known by  several other names   (bonus) \n DenoisingAE : Denoising (convolutional) autoencoder  [1, 2] \n CAE : Contractive autoencoder  (bonus)   [3] \n Seq2SeqAE : Sequence-to-sequence autoencoder \n VAE : Variational autoencoder  [4, 5] \n CatVAE : Categorical variational autoencoder  (bonus)   [6, 7] \n AAE : Adversarial autoencoder  (bonus)   [8] \n WTA-AE : Winner-take-all autoencoder  (bonus)   [9] \n \n Different models can be chosen using  th main.lua -model <modelName> . \n The  denoising  criterion can be used to replace the standard (autoencoder)  reconstruction  criterion by using the denoising flag. For example, a denoising AAE (DAAE)  [10]  can be set up using  th main.lua -model AAE -denoising . The corruption process is additive Gaussian noise  ~ N(0, 0.5) . \n MCMC sampling  [10]  can be used for VAEs, CatVAEs and AAEs with  th main.lua -model <modelName> -mcmc <steps> . To see the effects of MCMC sampling with this simple setup it is best to choose a large standard deviation, e.g.  -sampleStd 5 , for the Gaussian distribution to draw the initial samples from. \n Requirements \n The following luarocks packages are required: \n \n mnist \n dpnn (for DenoisingAE) \n rnn (for Seq2SeqAE) \n \n Citation \n If you find this library useful and would like to cite it, the following would be appropriate: \n @misc{Autoencoders,\n  author = {Arulkumaran, Kai},\n  title = {Kaixhin/Autoencoders},\n  url = {https://github.com/Kaixhin/Autoencoders},\n  year = {2016}\n} \n References \n [1] Vincent, P., Larochelle, H., Bengio, Y., & Manzagol, P. A. (2008, July). Extracting and composing robust features with denoising autoencoders. In  Proceedings of the 25th international conference on Machine learning  (pp. 1096-1103). ACM. \n[2] Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., & Manzagol, P. A. (2010). Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.  Journal of Machine Learning Research, 11 (Dec), 3371-3408. \n[3] Rifai, S., Vincent, P., Muller, X., Glorot, X., & Bengio, Y. (2011). Contractive auto-encoders: Explicit invariance during feature extraction. In  Proceedings of the 28th international conference on machine learning (ICML-11)  (pp. 833-840). \n[4] Kingma, D. P., & Welling, M. (2013). Auto-encoding variational bayes.  arXiv preprint arXiv:1312.6114 . \n[5] Rezende, D. J., Mohamed, S., & Wierstra, D. (2014). Stochastic Backpropagation and Approximate Inference in Deep Generative Models. In  Proceedings of The 31st International Conference on Machine Learning  (pp. 1278-1286). \n[6] Jang, E., Gu, S., & Poole, B. (2016). Categorical Reparameterization with Gumbel-Softmax.  arXiv preprint arXiv:1611.01144 . \n[7] Maddison, C. J., Mnih, A., & Teh, Y. W. (2016). The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables.  arXiv preprint arXiv:1611.00712 . \n[8] Makhzani, A., Shlens, J., Jaitly, N., Goodfellow, I., & Frey, B. (2015). Adversarial autoencoders.  arXiv preprint arXiv:1511.05644 . \n[9] Makhzani, A., & Frey, B. J. (2015). Winner-take-all autoencoders. In  Advances in Neural Information Processing Systems  (pp. 2791-2799). \n[10] Arulkumaran, K., Creswell, A., & Bharath, A. A. (2016). Improving Sampling from Generative Autoencoders with Markov Chains.  arXiv preprint arXiv:1610.09296 .  ', "The Malmo Collaborative AI Challenge - Team Pig Catcher \n Approach \n The challenge involves 2 agents who can either  cooperate  or  defect . The optimal policy, based on stag hunt  [1] , depends on the policy of the other agent. Not knowing the other agent's policy, the optimal solution is then based on  modelling  the other agent's policy. Similarly, the challenge can be considered a  sequential social dilemma   [2] , as goals could change over time. \n By treating the other agent as part of the environment, we can use model-free RL, and simply aim to maximise the reward of our agent. As a baseline we take a DRL algorithm - ACER  [3]  - and train it against the evaluation agent (which randomly uses a focused or random strategy every episode). \n We chose to approach this challenge using  hierarchical RL . We assume there are 2 subpolicies, one for each type of partner agent. To do so, we use option heads  [4] , whereby the agent has shared features, but separate heads for different subpolicies. In this case, ACER with 2 subpolicies has 2 Q-value heads and 2 policy heads. To choose which subpolicy to use at any given time, the agent also has an additional classifier head that is trained (using an oracle) to distinguish which option to use. Therefore, we ask the following questions: \n \n Can the agent distinguish between the two possible behaviours of the evaluation agent? \n Does the agent learn qualitatively different subpolicies? \n \n Unfortunately, due to technical difficulties and time restrictions, we were unable to successfully train an agent. Full results and more details can be found in our video. \n Design Decisions \n For our baseline, we implemented ACER  [3]  in PyTorch based on reference code  [5, 6] . In addition, we augmented the state that the agent receives with the previous action, reward and a step counter  [7] . Our challenge entry augments the agent with option heads  [4] , and we aim to distinguish the different policies of the evaluation agent. \n We also introduce a novel contribution - a batch version of ACER - which increases stability. We sample a batch of off-policy trajectories, and then truncate them to match the smallest. \n Instructions \n Dependencies: \n \n Python 2 \n PyTorch \n Plotly \n Docker  +  docker-py \n \n Firstly,  build the Malmo Docker image . Secondly,  enable running Docker as a non-root user . \n Run ACER with  OMP_NUM_THREADS=1 python pc_main.py . The code automatically opens up Minecraft (Docker) instances. \n Discussion \n \n References \n [1]  Game Theory of Mind \n[2]  Multi-agent Reinforcement Learning in Sequential Social Dilemmas \n[3]  Sample Efficient Actor-Critic with Experience Replay \n[4]  Classifying Options for Deep Reinforcement Learning \n[5]  ikostrikov/pytorch-a3c \n[6]  pfnet/ChainerRL \n[7]  Learning to Navigate in Complex Environments   \n \n \n This repository contains the task definition and example code for the  Malmo Collaborative AI Challenge .\nThis challenge is organized to encourage research in collaborative AI - to work towards AI agents \nthat learn to collaborate to solve problems and achieve goals. \nYou can find additional details, including terms and conditions, prizes and information on how to participate at the  Challenge Homepage . \n \n \n \n Notes for challenge participants:  Once you and your team decide to participate in the challenge, please make sure to register your team at our  Registration Page . On the registration form, you need to provide a link to the GitHub repository that will \ncontain your solution. We recommend that you fork this repository ( learn how ), \nand provide address of the forked repo. You can then update your submission as you make progress on the challenge task. \nWe will consider the version of the code on branch master at the time of the submission deadline as your challenge submission. Your submission needs to contain code in working order, a 1-page description of your approach, and a 1-minute video that shows off your agent. Please see the  challenge terms and conditions  for further details. \n \n Jump to: \n \n Installation \n Prerequisites \n Minimal installation \n \n Optional extensions \n \n \n Getting started \n \n Play the challenge task \n \n Run your first experiment \n \n \n Next steps \n \n Run an experiment in Docker on Azure \n Resources \n \n Installation \n Prerequisites \n \n Python  2.7+ (recommended) or 3.5+ \n Project Malmo  - we recommend downloading the  Malmo-0.21.0 release  and installing dependencies for  Windows ,  Linux  or  MacOS . Test your Malmo installation by  launching Minecraft with Malmo  and  launching an agent . \n \n Minimal installation \n pip install -e git+https://github.com/Microsoft/malmo-challenge#egg=malmopy \n or  \n git clone https://github.com/Microsoft/malmo-challenge\ncd malmo-challenge\npip install -e . \n Optional extensions \n Some of the example code uses additional dependencies to provide 'extra' functionality. These can be installed using: \n pip install -e '.[extra1, extra2]' \nFor example to install gym and chainer: \n pip install -e '.[gym]' \n Or to install all extras: \n pip install -e '.[all]' \n The following extras are available:\n-  gym :  OpenAI Gym  is an interface to a wide range of reinforcement learning environments. Installing this extra enables the Atari example agents in  samples/atari  to train on the gym environments.  Note that OpenAI gym atari environments are currently not available on Windows. \n-  tensorflow :  TensorFlow  is a popular deep learning framework developed by Google. In our examples it enables visualizations through  TensorBoard . \n Getting started \n Play the challenge task \n The challenge task takes the form of a mini game, called Pig Chase. Learn about the game, and try playing it yourself on our  Pig Chase Challenge page . \n Run your first experiment \n See how to  run your first baseline experiment  on the  Pig Chase Challenge page . \n Next steps \n Run an experiment in Docker on Azure \n Docker is a virtualization platform that makes it easy to deploy software with all its dependencies. \nWe use docker to run experiments locally or in the cloud. Details on how to run an example experiment using docker are in the  docker README . \n Resources \n \n Malmo Platform Tutorial \n Azure Portal \n Docker Documentation \n Docker Machine on Azure \n CNTK Tutorials \n CNTK Documentation \n Chainer Documentation \n TensorBoard Documentation \n", 'ACER \n \n Actor-critic with experience replay (ACER)  [1] . Uses batch off-policy updates to improve stability. Trust region updates can be enabled with  --trust-region . Currently uses full trust region instead of "efficient" trust region (see  issue #1 ). \n Run with  python main.py <options> . To run asynchronous advantage actor-critic (A3C)  [2]  (but with a Q-value head), use the  --on-policy  option. \n Requirements \n \n OpenAI Gym \n Plotly \n PyTorch \n \n To install all dependencies with Anaconda run  conda env create -f environment.yml  and use  source activate acer  to activate the environment. \n Results \n \n Acknowledgements \n \n @ikostrikov  for  pytorch-a3c \n @apaszke  for  Reinforcement Learning (DQN) tutorial \n @pfnet  for  ChainerRL \n \n References \n [1]  Sample Efficient Actor-Critic with Experience Replay \n[2]  Asynchronous Methods for Deep Reinforcement Learning  ', "FCN-semantic-segmentation \n Simple end-to-end semantic segmentation using fully convolutional networks  [1] . Takes a pretrained 34-layer ResNet  [2] , removes the fully connected layers, and adds transposed convolution layers with skip connections from lower layers. Initialises upsampling convolutions with bilinear interpolation filters and zeros the final (classification) layer. \n Uses an independent cross-entropy loss per class. Trained with SGD with momentum, plus weight decay only on convolutional weights. Calculates and plots class-wise and mean intersection-over-union. Checkpoints the network every epoch. \n Note: This code does not achieve great results (achieves ~40 IoU fairly quickly, but converges there). Contributions to fix this are welcome! The goal of this repo is to provide strong, simple and efficient baselines for semantic segmentation using the FCN method, so this shouldn't be restricted to using ResNet 34 etc. \n Requirements \n \n CUDA \n PyTorch \n matplotlib \n Cityscapes Dataset \n \n Instructions \n \n Install all of the required software. To feasibly run the training, CUDA is needed. The crop size and batch size can be tailored to your GPU memory (the default crop and batch sizes use ~10GB of GPU RAM). \n Register on the Cityscapes website to  access the dataset . \n Download and extract the training/validation RGB data ( leftImg8bit_trainvaltest ) and ground truth data ( gtFine_trainvaltest ). \n Run  python main.py <options> . \n \n First a Dataset object is set up, returning the RGB inputs, one-hot targets (for independent classification) and label targets. During training, the images are randomly cropped and horizontally flipped. Testing calculates IoU scores and produces a subset of coloured predictions that match the coloured ground truth. \n References \n [1]  Fully convolutional networks for semantic segmentation \n[2]  Deep Residual Learning for Image Recognition  ", 'NoisyNet-A3C \n \n NoisyNet  [1]  (LSTM) asynchronous advantage actor-critic (A3C)  [2]  on the CartPole-v1 environment. This repo has a minimalistic design and a classic control environment to enable quick investigation of different hyperparameters. \n Run with  python main.py <options> . Entropy regularisation can still be added by setting  --entropy-weight <value> , but it is 0 by default. Run with  --no-noise  to run normal A3C (without noisy linear layers). \n Requirements \n \n OpenAI Gym \n Plotly \n PyTorch \n \n To install all dependencies with Anaconda run  conda env create -f environment.yml  and use  source activate noisynet  to activate the environment. \n Results \n NoisyNet-A3C \n On the whole, NoisyNet-A3C tends to be better than A3C (with or without entropy regularisation). There seems to be more variance, with both good and poor runs, probably due to "deep" exploration. \n \n \n NoisyNet-A3C is perhaps even more prone to performance collapses than normal A3C. Many deep reinforcement learning algorithms are still prone to this. \n \n A3C (no entropy regularisation) \n A3C without entropy regularisation usually performs poorly. \n \n A3C (entropy regularisation with β = 0.01) \n A3C with entropy regularisation usually performs a bit better than A3C without entropy regularisation, and also poor runs of NoisyNet-A3C. The performance tends to be significantly worse than the best NoisyNet-A3C runs. \n \n Note that due to the nondeterminism introduced by asynchronous agents, different runs on even the same seed can produce different results, and hence the results presented are only single samples of the performance of these algorithms. Interestingly, the general observations above seem to hold even when increasing the number of processes (experiments were repeated with 16 processes). These algorithms are still sensitive to the choice of hyperparameters, and will need to be tuned extensively to get good performance on other domains. \n Acknowledgements \n \n @ikostrikov  for  pytorch-a3c \n \n References \n [1]  Noisy Networks for Exploration \n[2]  Asynchronous Methods for Deep Reinforcement Learning  ', 'Rainbow \n \n Rainbow: Combining Improvements in Deep Reinforcement Learning  [1] . \n Results and pretrained models can be found in the  releases . \n \n [x] DQN  [2] \n [x] Double DQN  [3] \n [x] Prioritised Experience Replay  [4] \n [x] Dueling Network Architecture  [5] \n [x] Multi-step Returns  [6] \n [x] Distributional RL  [7] \n [x] Noisy Nets  [8] \n \n Run the original Rainbow with the default arguments: \n python main.py \n Data-efficient Rainbow  [9]  can be run using the following options (note that the "unbounded" memory is implemented here in practice by manually setting the memory capacity to be the same as the maximum number of timesteps): \n python main.py --target-update 2000 \\\n               --T-max 100000 \\\n               --learn-start 1600 \\\n               --memory-capacity 100000 \\\n               --replay-frequency 1 \\\n               --multi-step 20 \\\n               --architecture data-efficient \\\n               --hidden-size 256 \\\n               --learning-rate 0.0001 \\\n               --evaluation-interval 10000 \n Note that pretrained models from the  1.3  release used a (slightly) incorrect network architecture. To use these, change the padding in the first convolutional layer from 0 to 1 (DeepMind uses "valid" (no) padding). \n Requirements \n \n atari-py \n OpenCV Python \n Plotly \n PyTorch \n \n To install all dependencies with Anaconda run  conda env create -f environment.yml  and use  source activate rainbow  to activate the environment. \n Available Atari games can be found in the  atari-py  ROMs folder . \n Acknowledgements \n \n @floringogianu  for  categorical-dqn \n @jvmancuso  for  Noisy layer \n @jaara  for  AI-blog \n @openai  for  Baselines \n @mtthss  for  implementation details \n \n References \n [1]  Rainbow: Combining Improvements in Deep Reinforcement Learning \n[2]  Playing Atari with Deep Reinforcement Learning \n[3]  Deep Reinforcement Learning with Double Q-learning \n[4]  Prioritized Experience Replay \n[5]  Dueling Network Architectures for Deep Reinforcement Learning \n[6]  Reinforcement Learning: An Introduction \n[7]  A Distributional Perspective on Reinforcement Learning \n[8]  Noisy Networks for Exploration \n[9]  When to Use Parametric Models in Reinforcement Learning?  ', 'Dist-A3C \n \n TODO: Have server use mp - one thread for server, one for testing. Keep counter to know once finished. Also be able to send push notifications to kill running clients once counter done. \n Distributed asynchronous advantage actor-critic (A3C)  [1]  with generalised advantage estimation (GAE)  [2] . Run  python server.py <options>  to start the server and  python client.py <options>  for as many clients as wanted. \n Requirements \n \n OpenAI Gym \n MessagePack \n msgpack-numpy \n Plotly \n PyTorch \n PyZMQ \n \n To install all dependencies with Anaconda run  conda env create -f environment.yml  and use  source activate dista3c  to activate the environment. \n Acknowledgements \n \n @ikostrikov  for  pytorch-a3c \n \n References \n [1]  Asynchronous Methods for Deep Reinforcement Learning \n[2]  High-Dimensional Continuous Control Using Generalized Advantage Estimation  ', "Grokking PyTorch \n PyTorch  is a flexible deep learning framework that allows automatic differentiation through dynamic neural networks (i.e., networks that utilise dynamic control flow like if statements and while loops). It supports GPU acceleration,  distributed training ,  various optimisations , and plenty more neat features. These are some notes on how I think about using PyTorch, and don't encompass all parts of the library or every best practice, but may be helpful to others. \n Neural networks are a subclass of  computation graphs . Computation graphs receive input data, and data is routed to and possibly transformed by nodes which perform processing on the data. In deep learning, the neurons (nodes) in neural networks typically transform data with parameters and differentiable functions, such that the parameters can be optimised to minimise a loss via gradient descent. More broadly, the functions can be stochastic, and the structure of the graph can be dynamic. So while neural networks may be a good fit for  dataflow programming , PyTorch's API has instead centred around  imperative programming , which is a more common way for thinking about programs. This makes it easier to read code and reason about complex programs, without necessarily sacrificing much performance; PyTorch is actually pretty fast, with plenty of optimisations that you can safely forget about as an end user (but you can dig in if you really want to). \n The rest of this document, based on the  official MNIST example , is about  grokking  PyTorch, and should only be looked at after the  official beginner tutorials . For readability, the code is presented in chunks interspersed with comments, and hence not separated into different functions/files as it would normally be for clean, modular code. \n Imports \n py\nimport argparse\nimport os\nimport torch\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms \n These are pretty standard imports, with the exception of the  torchvision  modules that are used for computer vision problems in particular. \n Setup \n ```py\nparser = argparse.ArgumentParser(description='PyTorch MNIST Example')\nparser.add_argument('--batch-size', type=int, default=64, metavar='N',\n                    help='input batch size for training (default: 64)')\nparser.add_argument('--epochs', type=int, default=10, metavar='N',\n                    help='number of epochs to train (default: 10)')\nparser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n                    help='learning rate (default: 0.01)')\nparser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n                    help='SGD momentum (default: 0.5)')\nparser.add_argument('--no-cuda', action='store_true', default=False,\n                    help='disables CUDA training')\nparser.add_argument('--seed', type=int, default=1, metavar='S',\n                    help='random seed (default: 1)')\nparser.add_argument('--save-interval', type=int, default=10, metavar='N',\n                    help='how many batches to wait before checkpointing')\nparser.add_argument('--resume', action='store_true', default=False,\n                    help='resume training from checkpoint')\nargs = parser.parse_args() \n use_cuda = torch.cuda.is_available() and not args.no_cuda\ndevice = torch.device('cuda' if use_cuda else 'cpu')\ntorch.manual_seed(args.seed)\nif use_cuda:\n  torch.cuda.manual_seed(args.seed)\n``` \n argparse  is a standard way of dealing with command-line arguments in Python. \n A good way to write device-agnostic code (benefitting from GPU acceleration when available but falling back to CPU when not) is to pick and save the appropriate  torch.device , which can be used to determine where tensors should be stored. See the  official docs  for more tips on device-agnostic code. The PyTorch way is to put device placement under the control of the user, which may seem a nuisance for simple examples, but makes it much easier to work out where tensors are - which is useful for a) debugging and b) making efficient use of devices manually. \n For repeatable experiments, it is necessary to set random seeds for anything that uses random number generation (including  random  or  numpy  if those are used too). Note that cuDNN uses nondeterministic algorithms, and it can be disabled using  torch.backends.cudnn.enabled = False . \n Data \n ```py\ndata_path = os.path.join(os.path.expanduser('~'), '.torch', 'datasets', 'mnist')\ntrain_data = datasets.MNIST(data_path, train=True, download=True,\n                            transform=transforms.Compose([\n                              transforms.ToTensor(),\n                              transforms.Normalize((0.1307,), (0.3081,))]))\ntest_data = datasets.MNIST(data_path, train=False, transform=transforms.Compose([\n                             transforms.ToTensor(),\n                             transforms.Normalize((0.1307,), (0.3081,))])) \n train_loader = DataLoader(train_data, batch_size=args.batch_size,\n                          shuffle=True, num_workers=4, pin_memory=True)\ntest_loader = DataLoader(test_data, batch_size=args.batch_size,\n                         num_workers=4, pin_memory=True)\n``` \n Since  torchvision  models get stored under  ~/.torch/models/ , I like to store  torchvision  datasets under  ~/.torch/datasets . This is my own convention, but makes it easier if you have lots of projects that depend on MNIST, CIFAR-10 etc. In general it's worth keeping datasets separately to code if you end up reusing several datasets. \n torchvision.transforms  contains lots of handy transformations for single images, such as cropping and normalisation. \n DataLoader  contains many options, but beyond  batch_size  and  shuffle ,  num_workers  and  pin_memory  are worth knowing for efficiency.  num_workers  > 0 uses subprocesses to asynchronously load data, rather than making the main process block on this. The typical use-case is when loading data (e.g. images) from disk and maybe transforming them too - this can be done in parallel with the network processing the data. You will want to tune the amount to a) minimise the number of workers and hence CPU and RAM usage (each worker loads a separate batch, not individual samples within a batch) b) minimise the time the network is waiting for data.  pin_memory  uses  pinned memory  (as opposed to paged memory) to speed up any RAM to GPU transfers (and does nothing for CPU-only code). \n Model \n ```py\nclass Net(nn.Module):\n  def  init (self):\n    super(Net, self). init ()\n    self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n    self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n    self.conv2_drop = nn.Dropout2d()\n    self.fc1 = nn.Linear(320, 50)\n    self.fc2 = nn.Linear(50, 10) \n def forward(self, x):\n    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n    x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n    x = x.view(-1, 320)\n    x = F.relu(self.fc1(x))\n    x = self.fc2(x)\n    return F.log_softmax(x, dim=1) \n model = Net().to(device)\noptimiser = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum) \n if args.resume:\n  model.load_state_dict(torch.load('model.pth'))\n  optimiser.load_state_dict(torch.load('optimiser.pth'))\n``` \n Network initialisation typically includes member variables, layers which contain trainable parameters, and maybe separate trainable parameters and non-trainable buffers. The forward pass then uses these in conjunction with functions from  F  that are purely functional (don't contain parameters). Some people prefer to have completely functional networks (e.g., keeping parameters separately and using  F.conv2d  instead of  nn.Conv2d ) or networks completely made of layers (e.g.,  nn.ReLU  instead of  F.relu ). \n .to(device)  is a convenient way of sending the device parameters (and buffers) to GPU if  device  is set to GPU, doing nothing otherwise (when  device  is set to CPU). It's important to transfer the network parameters to the appropriate device before passing them to the optimiser, otherwise the optimiser will not be keeping track of the parameters properly! \n Both neural networks ( nn.Module ) and optimisers ( optim.Optimizer ) have the ability to save and load their internal state, and  .load_state_dict(state_dict)  is the recommended way to do so - you'll want to reload the state of both to resume training from previously saved state dictionaries. Saving the entire object can be  error prone . If you have saved tensors on GPU and want to load them on CPU or another GPU, the easiest way is to directly load them onto CPU using the  map_location  option , e.g.,  torch.load('model.pth', map_location='cpu') . \n Some points of note not shown here are that the forward pass can make use of control flow (e.g., a member variable or even the data itself can determine the execution of an if statement. It is also perfectly valid to  print  tensors in the middle, making debugging much easier. Finally, the forward pass can make use of multiple arguments. A short snippet (not tied to any sensible idea) to illustrate this is below: \n py\ndef forward(self, x, hx, drop=False):\n  hx2 = self.rnn(x, hx)\n  print(hx.mean().item(), hx.var().item())\n  if hx.max.item() > 10 or self.can_drop and drop:\n    return hx\n  else:\n    return hx2 \n Training \n ```py\nmodel.train()\ntrain_losses = [] \n for i, (data, target) in enumerate(train_loader):\n  data = data.to(device=device, non_blocking=True)\n  target = target.to(device=device, non_blocking=True)\n  optimiser.zero_grad()\n  output = model(data)\n  loss = F.nll_loss(output, target)\n  loss.backward()\n  train_losses.append(loss.item())\n  optimiser.step() \n if i % 10 == 0:\n    print(i, loss.item())\n    torch.save(model.state_dict(), 'model.pth')\n    torch.save(optimiser.state_dict(), 'optimiser.pth')\n    torch.save(train_losses, 'train_losses.pth')\n``` \n Network modules are by default set to training mode - which impacts the way some modules work, most noticeably dropout and batch normalisation. It's best to set this manually anyway with  .train() , which propagates the training flag down all children modules. \n Here the  .to()  method not only takes the device, but also sets  non_blocking=True , which enables asynchronous data copies to GPU from pinned memory, hence allowing the CPU to keep operating during the transfer;  non_blocking=True  is simply a no-op otherwise. \n Before collecting a new set of gradients with  loss.backward()  and doing backpropagation with  optimiser.step() , it's necessary to manually zero the gradients of the parameters being optimised with  optimiser.zero_grad() . By default, PyTorch  accumulates  gradients, which is very handy when you don't have enough resources to calculate all the gradients you need in one go. \n PyTorch uses a tape-based automatic gradient (autograd) system - it collects which operations were done on tensors in order, and then replays them backwards to do reverse-mode differentiation. This is why it is super flexible and allows arbitrary computation graphs. If none of the tensors require gradients (you'd have to set  requires_grad=True  when constructing a tensor for this - as by default new tensors  don't  require gradients) then no graph is stored! However, networks tend to have parameters that require gradients, so any computation done from the output of a network will be stored in the graph. So if you want to store data resulting from this, you'll need to manually disable gradients or, more commonly, store it as a Python number (via  .item()  on a PyTorch scalar) or numpy array. See the  official docs  for more on autograd. \n One way to cut the computation graph is to use  .detach() , which you may use when passing on a hidden state when training RNNs with truncated backpropagation-through-time. It's also handy when differentiating a loss where one component is the output of another network, but this other network shouldn't be optimised with respect to the loss - examples include training a discriminator from a generator's outputs in GAN training, or training the policy of an actor-critic algorithm using the value function as a baseline (e.g. A2C). Another technique for preventing gradient calculations that is efficient in GAN training (training the generator from the discriminator) and typical in fine-tuning is to loop through a networks parameters and set  param.requires_grad = False . \n Apart from logging results in the console/in a log file, it's important to checkpoint model parameters (and optimiser state) just in case. You can also use  torch.save()  to save normal Python objects, but other standard choices include the built-in  pickle . Note that here that if the parameters are on GPU they will be saved on GPU, but can be recovered on CPU using  map_location , as discussed previously; casting the model parameters to CPU before using  .state_dict()  would therefore require casting back to GPU before resuming the rest of your code. \n Testing \n ```py\nmodel.eval()\ntest_loss, correct = 0, 0 \n with torch.inference_mode():\n  for data, target in test_loader:\n    data = data.to(device=device, non_blocking=True)\n    target = target.to(device=device, non_blocking=True)\n    output = model(data)\n    test_loss += F.nll_loss(output, target, reduction='sum').item()\n    pred = output.argmax(1, keepdim=True)\n    correct += pred.eq(target.view_as(pred)).sum().item() \n test_loss /= len(test_data)\nacc = correct / len(test_data)\nprint(acc, test_loss)\n``` \n In response to  .train()  earlier, networks should explicitly be set to evaluation mode using  .eval() . \n As mentioned previously, the computation graph would normally be made when using a network. By using the  inference_mode  context manager via  with torch.inference_mode()  this is prevented from happening. Previous versions of PyTorch only had the  no_grad  context manager, which can be used to disable gradient computations within a context, but allow the variables from the context to be used as part of a computation graph. For full details of what this means and what it entails, see the  official docs  on locally disabling gradients. \n Extra \n This is an extra section just to add a few useful asides. \n Memory problems? Check the  official docs  for tips. \n CUDA errors? They are a pain to debug, and are usually a logic problem that would come up with a more intelligible error message on CPU. It's best to be able to easily switch between CPU and GPU if you are planning on using the GPU. A more general development tip is to be able to set up your code so that it's possible to run through all of the logic quickly to check it before launching a proper job - examples would be preparing a small/synthetic dataset, running one train + test epoch, etc. If it is a CUDA error, or you really can't switch to CPU, setting  CUDA_LAUNCH_BLOCKING=1  will make CUDA kernel launches synchronous and as a result provide better error messages. \n A note for  torch.multiprocessing , or even just running multiple PyTorch scripts at once. Because PyTorch uses multithreaded BLAS libraries to speed up linear algebra computations on CPU, it'll typically use several cores. If you want to run several things at once, with multiprocessing or several scripts, it may be useful to manually reduce these by setting the environment variable  OMP_NUM_THREADS  to 1 or another small number - this reduces the chance of CPU thrashing. The  official docs  have some other notes for multiprocessing in particular.", 'SARCOS \n Machine learning methods applied to the  SARCOS dataset , treated as a multivariate regression problem. \n Note: As discovered by  @rajshah4 , there is a large amount of leakage between the training and test sets. This code and results are currently left as-is for reference, but should not be considered representative of results with a proper training/test split. \n | Method                           | MSE      | # Params    | # Avg. Path Params |\n| -------------------------------- | -------- |-------------|--------------------|\n| Linear regression                | 10.69263 | 154         | N/A                |\n| Decision tree                    | 3.70763  | 319,591     | 24.6               |\n| Neural network (1 hidden layer)  | 2.83472  | 7,431       | N/A                |\n| Neural network (5 hidden layers) | 2.65670  | 270,599     | N/A                |\n| Random forest                    | 2.39401  | 141,540,436 | 16,771.0           |\n| Neural network (3 hidden layers) | 2.12862  | 139,015     | N/A                |\n| Gradient boosted trees           | 1.44412  | 988,256     | 6,807.7            |', 'spinning-up-basic \n Basic versions of agents from  Spinning Up in Deep RL  written in  PyTorch . Designed to run quickly on CPU on  Pendulum-v0  from  OpenAI Gym . \n To see differences between algorithms, try running  diff -y <file1> <file2> , e.g.,  diff -y ddpg.py td3.py . \n For MPI versions of on-policy algorithms, see the  mpi  branch . \n Algorithms \n \n Vanilla Policy Gradient /Advantage Actor-Critic ( vpg.py ) \n Trust Region Policy Gradient  ( trpo.py ) \n Proximal Policy Optimization  ( ppo.py ) \n Deep Deterministic Policy Gradient  ( ddpg.py ) \n Twin Delayed DDPG  ( td3.py ) \n Soft Actor-Critic  ( sac.py ) \n Deep Q-Network ( dqn.py ) \n \n Implementation Details \n Note that implementation details can have a significant effect on performance, as discussed in  What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study . This codebase attempts to be as simple as possible, but note that for instance on-policy algorithms use separate actor and critic networks, a state-independent policy standard deviation, per-minibatch advantage normalisation, and several critic updates per minibatch, while the deterministic off-policy algorithms use layer normalisation. Equally, soft actor-critic uses a transformed Normal distribution by default, but this can also help the on-policy algorithms. \n Results \n Vanilla Policy Gradient/Advantage Actor-Critic \n \n Trust Region Policy Gradient \n \n Proximal Policy Optimization \n \n Deep Deterministic Policy Gradient \n \n Twin Delayed DDPG \n \n Soft Actor-Critic \n \n Deep Q-Network \n \n Code Links \n \n Spinning Up in Deep RL  (TensorFlow) \n Fired Up in Deep RL  (PyTorch) \n', 'PlaNet \n \n PlaNet: A Deep Planning Network for Reinforcement Learning  [1] . Supports symbolic/visual observation spaces. Supports some Gym environments (including classic control/non-MuJoCo environments, so DeepMind Control Suite/MuJoCo are optional dependencies). Hyperparameters have been taken from the original work and are tuned for DeepMind Control Suite, so would need tuning for any other domains (such as the Gym environments). \n Run with  python.main.py . For best performance with DeepMind Control Suite, try setting environment variable  MUJOCO_GL=egl  (see instructions and details  here ). \n Results and pretrained models can be found in the  releases . \n Requirements \n \n Python 3 \n DeepMind Control Suite  (optional) \n Gym \n OpenCV Python \n Plotly \n PyTorch \n \n To install all dependencies with Anaconda run  conda env create -f environment.yml  and use  source activate planet  to activate the environment.  \n Links \n \n Introducing PlaNet: A Deep Planning Network for Reinforcement Learning \n google-research/planet \n \n Acknowledgements \n \n @danijar  for  google-research/planet  and  help reproducing results \n @sg2  for  running experiments \n \n References \n [1]  Learning Latent Dynamics for Planning from Pixels  ', "EC \n Episodic control algorithms: \n \n MFEC \n NEC \n \n Default options for MFEC: \n python main.py --kernel mean \n Default options for NEC: \n python main.py --algorithm NEC \\\n               --key-size 128 \\\n               --num-neighbours 50 \\\n               --dictionary-capacity 500000 \\\n               --episodic-multi-step 100 \\\n               --epsilon-final 0.001 \\\n               --discount 0.99 \\\n               --learn-start 50000 \n Used in  Memory-efficient episodic control reinforcement learning with dynamic online k-means  and  Sample-Efficient Reinforcement Learning with Maximum Entropy Mellowmax Episodic Control , both presented at the  Workshop on Biological and Artificial Reinforcement Learning, NeurIPS 2019 . The source code for these papers can be found in this repo  here , but is not as organised as this codebase. \n Notes \n Notes about this codebase based on conversations with Robert Kirk \n \n This uses brute-force kNN search, as opposed to the approximate kNN search used in the original works. Without knowing the full details of DeepMind's k-d tree, especially how they might update the tree with new data, I chose to stick to the exact version to be safe. \n As in the  original works , this uses a hash of the state in order to detect exact state matches. \n Key/value gradients for the DND are taken from the first instance of each updated key, but averaging would make more sense. \n The gradients for the keys and values in the DND do not get applied properly (effectively, the gradients are zero) - this is a bug! Despite the bug, this codebase can still reproduce the results from the NEC paper (hence those updates may not be important). \n \n Acknowledgements \n \n Alex Pritzel for implementation details \n @RobertKirk  for debugging \n", 'A Pragmatic Look at Deep Imitation Learning \n \n Imitation learning algorithms (with PPO  [1] ): \n \n AIRL  [2] \n BC  [3] \n DRIL  [4]  (without BC) \n FAIRL  [5] \n GAIL  [6] \n GMMIL  [7]  (including an optional self-similarity term  [8] ) \n nn-PUGAIL  [9] \n RED  [10] \n \n Options include: \n \n State-only imitation learning:  state-only: true/false \n R1 gradient regularisation  [11] :  r1-reg-coeff: 0.5 \n \n Requirements \n Requirements can be installed with:\n sh\npip install -r requirements.txt \nNotable required packages are  PyTorch ,  OpenAI Gym ,  D4RL-PyBullet  and  Hydra .  Ax  and the  Hydra Ax sweeper plugin  are required for hyperparameter optimisation; if unneeded they can be removed from  requirements.txt . \n Run \n The training of each imitation learning algorithm can be started with:\n sh\npython main.py algorithm=ALG/ENV \nwhere  ALG  is one of  [AIRL|BC|DRIL|FAIRL|GAIL|GMMIL|PUGAIL|RED]  and  ENV  is one of  [ant|halfcheetah|hopper|walker2d] . For example:\n sh\npython main.py algorithm=AIRL/hopper \n Hyperparameters can be found in  conf/config.yaml  and  conf/algorithm/ALG/ENV.yaml , with the latter containing algorithm- and environment-specific hyperparameters that were tuned with Ax. \n Results will be saved in  outputs/ENV_ALGO/m-d_H-M-S  with the last subfolder indicating the current datetime. \n Hyperparameter optimisation \n Hyperparameter optimisation can be run by adding  -m hydra/sweeper=ax hyperparam_opt=ALG , for example:\n sh\npython main.py -m algorithm=AIRL/hopper hydra/sweeper=ax hyperparam_opt=AIRL \n hyperparam_opt  specifies the hyperparameter search space. \n Seed sweep \n A seed sweep can be performed as follows:\n sh\npython main.py -m algorithm=AIRL/hopper seed=1,2,3,4,5 \nor via the existing bash script:\n sh\n./scripts/run_seed_experiments.sh ALG ENV \n The results will be available in  ./output/seed_sweeper_ENV_ALG  folder (note that running this code twice will overwrite the previous results). \n Results \n   \n Acknowledgements \n \n @ikostrikov  for  https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail \n \n Citation \n If you find this work useful and would like to cite it, the following would be appropriate: \n tex\n@article{arulkumaran2021pragmatic,\n  author = {Arulkumaran, Kai and Ogawa Lillrank, Dan},\n  title = {A Pragmatic Look at Deep Imitation Learning},\n  journal={arXiv preprint arXiv:2108.01867},\n  year = {2021}\n} \n References \n [1]  Proximal Policy Optimization Algorithms \n[2]  Learning Robust Rewards with Adversarial Inverse Reinforcement Learning \n[3]  Efficient Training of Artificial Neural Networks for Autonomous Navigation \n[4]  Disagreement-Regularized Imitation Learning \n[5]  A Divergence Minimization Perspective on Imitation Learning Methods \n[6]  Generative Adversarial Imitation Learning \n[7]  Imitation Learning via Kernel Mean Embedding \n[8]  A Pragmatic Look at Deep Imitation Learning \n[9]  Positive-Unlabeled Reward Learning \n[10]  Random Expert Distillation: Imitation Learning via Expert Policy Support Estimation \n[11]  Which Training Methods for GANs do actually Converge?  ', 'Minimal Criterion Artist Collective \n \n Usage \n Run  main.py  with a choice of generator  --generator StyleGAN|CPPN|FractalFlame . Requires  PyTorch . Details on the algorithm are available in the  paper . \n libmypaint  and  MIDI  generators are available under their corresponding branches (undocumented). \n Samples \n Image and audio samples can be found in  samples  folder. \n | | | | |\n|-|-|-|-|\n|   |   |   |   |\n|   |   |   |   |\n|   |   |   |   |\n|   |   |   |   | \n Citation \n tex\n@inproceedings{arulkumaran2022minimal,\n  author = {Arulkumaran, Kai and Nguyen-Phuoc, Thu},\n  title = {Minimal Criterion Artist Collective},\n  booktitle = {Genetic and Evolutionary Computation Conference Companion},\n  year = {2022}\n}', 'Generalised UDRL \n \n All You Need Is Supervised Learning: From Imitation Learning to Meta-RL With Upside Down RL \n Implementation of a generalised UDRL agent. Supports online RL, imitation learning, offline RL, goal-conditioned RL, and meta-RL:\n sh\npython main.py --mode [online|imitation|offline|goal|meta] \n Note that some implementation decisions were made for simplicity, and hence do not reflect an implementation that works across all modes simultaneously. \n Requirements \n Requirements can be installed with:\n sh\npip install -r requirements.txt \n Results \n Online                        | \xa0\xa0IL\xa0\xa0\xa0    | Offline                         | \xa0GCRL\xa0\xa0    | Meta-RL\n:----------------------------:|:-----------------------------------:|:-------------------------------:|:-------------------------:|:-------------------------:\n  |   |   |   |  \n Citation \n tex\n@inproceedings{arulkumaran2022all,\n  author = {Arulkumaran, Kai and Ashley, Dylan R. and Schmidhuber, Jürgen and Srivastava, Rupesh K.},\n  title = {All You Need is Supervised Learning: From Imitation Learning to Meta-RL with Upside Down RL},\n  booktitle = {Multi-disciplinary Conference on Reinforcement Learning and Decision Making},\n  year = {2022}\n}']
yoosan,['Algorithms \n DataStructure and Algorithms \n All the code which inclue  CPP ,  PY  or other langs in this repository are the basic abstract datastructure and algorithms\nIncluding LinkedList, binarytree, binary-search-tree, stack, queue,hash-table and Graph. All the algorithms are correct which can be applied in these datastructures. \n You can git these code and run them, every file is avaliable.If you have some question, please contract with me by the email that is cliviazhou@foxmail.com', "For fun \n I'm a freshman of web-front, so this project is just for my practice of  javascript  and  html+css. \n All of the code are open and free, so you can download and then use them. If you have some question, please contact me by the e-mail that you can find on my homepage. \n A loading animaition (Javascript) \n http://yoosan.me/demo.html \n A demo of transition-timing-function (CSS) \n http://yoosan.me/transition.html \n 2048 game imitation(Javascript) \n http://yoosan.me/2048.html", "LeetCode Algorithms solution \n This project maybe(or not) include all solutions of the Leetcode's algoritms exercise. For practicing code skills, I start solve these problems.For each problem i would like to give the C++ code and Python code as well as i can. \n It is not easy to finish all of problems, so it will take a period of time to complete. My plan is 2 month. For some diffcult problems, I will record the algorithms and make a explaination. \n All code are available and can be accepted, You can clone them and submit to the OJ, but my recommendation is that you should try you best to solve them firstly. \n If you have some more genious solution, please tell me.", 'The web spider for zhihu.com \n I use the scarpy to crawl the data from website. \nFor storage, using MySQL to store the basic information of users and question title and urls, using redis as a cache of cookies and users.\nFinally, I have used ElasticSearch for the visualization query.', 'README \n I used Ruby on Rails to build a web application.  The data of this project is crawled by the project  (spider-zh)[https://github.com/yoosan/spider-zh] . \n How to run \n First, you should install the requirements, including: \n \n Ruby >= 2 \n Rails >= 4.2.1 \n rvm (recommend) \n MySQL \n \n and the gems listed in the  Gemfile . Running this project also needs the data which is stored in MySQL. If all requirements satisfied, you should tap  rails s  and open  http://localhost:3000 \n Demo \n This projects looks like \n \n -- \n', 'INTRODUCTION \n Coursework of THE parallel and distributed system, implementing MPI for matrix multiplication. \n Install mpi(I use OpenMPI) on your machine, firstly. Then clone this repo and complie and run it by:  \n ```bash\nmake\nmpirun -np 6 ./nn \nor \nmpirun -np 6 ./nn > mpi.txt & \n ./serial \nor \n./serial > serial.txt & \n ```', 'Introduction \n A spark project for nlp', 'deepmind_qa \n The implementation of  Teaching Machines to Read and Comprehend ,  DeepMind, In NIPS 2015  by  torch7  and  lua . \n I attempt to implement it, but maybe take some time. \n There are three models:\n+ Deep_LSTM\n+ Attentive_Reader\n+ Imparient_Reader \n (In progress ... ) \n License \n MIT', 'Sentpair \n The implementation of our paper  Modeling Sentence Pairs with Tree-structured Attentive Encoder (pdf)  to appear at the  COLING2016 . It runs both training and evaluation. Note that we just test the projects on Mac OS X and Ubuntu 14. \n Preparing \n You can download the preprocessed data (recommend) from  here . Alternatively you can process them by yourself. The original links are: \n \n SICK dataset \n MSRP dataset \n AI2-8grade dataset \n \n Requirement \n The software can run on CPU or GPU, dependency requirements are following: \n \n torch7 \n python \n \n The package should be installed are the following. For example, you can install the  nn  package by  luarocks install nn .  \n shell\nluarocks install nn\nluarocks install nngraph\nluarocks install penlight\nluarocks install optim\nluarocks install xlua\nluarocks install sys\nluarocks install cutorch\nluarocks install cunn \n Running \n To run our models, you can tap the command  \n th main.lua -<opt_name> opt_value -<opt_name> opt_val ... \n For example, to run the model on the SICK dataset, you should tap \n th main.lua -task MSRP -structure atreelstm -lr 0.05 -n_epoches 10 \n more details in the file  main.lua . \n Results \n We achieve the results as  \n \n \n \n Bib \n @inproceedings{zhou2016sentpair,\n    title={Modelling Sentence Pairs with Tree-structured Attentive Encoder},\n    author={Yao Zhou, Cong Liu and Yan Pan},\n    booktitle={The International Conference on Computational Linguistics (COLING)},\n    year={2016}\n} \n License \n MIT', 'dataset \n TODO', 'DeepMind Atari Deep Q Learner \n This repository hosts the  original code  published along with  the article  in Nature and my experiments (if any) with it. \n Tested on Ubuntu 14.04 with nVidia GTX 970: \n \nMore videos on  YouTube Playlist: Deepmind DQN Playing \n DQN 3.0 \n This project contains the source code of DQN 3.0, a Lua-based deep reinforcement\nlearning architecture, necessary to reproduce the experiments\ndescribed in the paper "Human-level control through deep reinforcement\nlearning", Nature 518, 529–533 (26 February 2015) doi:10.1038/nature14236. \n To replicate the experiment results, a number of dependencies need to be\ninstalled, namely:\n* LuaJIT and Torch 7.0\n* nngraph\n* Xitari (fork of the Arcade Learning Environment (Bellemare et al., 2013))\n* AleWrap (a lua interface to Xitari)\nAn install script for these dependencies is provided. \n Two run scripts are provided: run_cpu and run_gpu. As the names imply,\nthe former trains the DQN network using regular CPUs, while the latter uses\nGPUs (CUDA), which typically results in a significant speed-up. \n Installation instructions \n The installation requires Linux with apt-get. \n Note: In order to run the GPU version of DQN, you should additionally have the\nNVIDIA® CUDA® (version 5.5 or later) toolkit installed prior to the Torch\ninstallation below.\nThis can be downloaded from https://developer.nvidia.com/cuda-toolkit\nand installation instructions can be found in\nhttp://docs.nvidia.com/cuda/cuda-getting-started-guide-for-linux \n To train DQN on Atari games, the following components must be installed:\n* LuaJIT and Torch 7.0\n* nngraph\n* Xitari\n* AleWrap \n To install all of the above in a subdirectory called \'torch\', it should be enough to run \n ./install_dependencies.sh\n \n from the base directory of the package. \n Note: The above install script will install the following packages via apt-get:\nbuild-essential, gcc, g++, cmake, curl, libreadline-dev, git-core, libjpeg-dev,\nlibpng-dev, ncurses-dev, imagemagick, unzip \n Training DQN on Atari games \n Prior to running DQN on a game, you should copy its ROM in the \'roms\' subdirectory.\nIt should then be sufficient to run the script \n ./run_cpu <game name>\n \n Or, if GPU support is enabled, \n ./run_gpu <game name>\n \n Note: On a system with more than one GPU, DQN training can be launched on a\nspecified GPU by setting the environment variable GPU_ID, e.g. by \n GPU_ID=2 ./run_gpu <game name>\n \n If GPU_ID is not specified, the first available GPU (ID 0) will be used by default. \n Storing a .gif for a trained network \n Once you have a snapshot of a network you can run \n ./test_gpu <game name> <snapshopt filename>\n \n to make it play one game and store the .gif under  gifs . For example \n ./test_gpu breakout DQN3_0_1_breakout_FULL_Y.t7\n \n Options \n Options to DQN are set within run_cpu (respectively, run_gpu). You may,\nfor example, want to change the frequency at which information is output \nto stdout by setting \'prog_freq\' to a different value.', "DeepRL \n This code implements the standard deep Q-learning and dueling network with experience replay (memory buffer) for playing simple games. \n DQN algorithm implemented in this code is from the Google DeepMind's paper  Playing Atari with Deep Reinforcement Learning [ link ]. \n Dueling network is from the paper  Dueling Network Architectures for Deep Reinforcement Learning  [ link ] \n Requirement \n DeepRL is implemented with  Torch  and the packages of its ecosystem. This code is well worked on my Mac Pro with CPU (I haven't tested it on Linux and GPU). Install  Torch7  firstly, then you should install the following packages by  luarocks \n bash\nluarocks install nn\nluarocks install image\nluarocks install qt\nluarocks install optim \n Running \n You can run this code by tapping the command in the project dir. \n lua\nqlua main.lua \n The result looks like \n \n \n DQN: I got the accuracy of  93.2%  (932 success of 1000 epochs). \n Dueling: I got the accuracy of  99.2%  (992 success of 1000 epochs). \n Code \n The  envir.lua  indicates the environment in reinforcement learning stage, which receives the action and produces the states and a reward for agent. \n The  agent.lua  is the implementation of agent which receives the states and reward to produce the action directed by the policy network. \n The  learner.lua  is the learning algorithm of DQN with experience replay as the following. \n \n MISC \n I completed this code when I was an intern at  Horizon Robotics . I will greatly thank the article of  Andrej Karpathy  and other implementations: SeanNaren's code  and  EderSantana's gist . \n LICENSE \n MIT", 'mxnet-seq2seq \n This project implements the sequence to sequence learning with mxnet for open-domain chatbot \n Sequence to Sequence learning with LSTM encoder-decoder \n The seq2seq encoder-decoder architecture is introduced by  Sequence to Sequence Learning with Neural Networks \n This implementation borrows idea from  lstm_bucketing , I slightly modified it and reconstructed the embedding layer. \n How to run \n Firstly, process the data by\n python datautils.py \nthen run the model by\n python main.py \n The architecture \n We know that  seq2seq encoder-decoder  architecture includes two RNNs (LSTMs), one for encoding source sequence and another for decoding target sequence. \n For NLP-related tasks, the sequence could be a natural language sentence. As a result, the encoder and decoder should  share the word embedding layer  . \n The bucketing is a grate solution adapting the arbitrariness of sequence length. I padding zero to a fixed length at the encoding sequence and make buckets at the decoding phrase.  \n The data is formatted as: \n ```\n0 0 ... 0 23 12 121 832 || 2 3432 898 7 323\n0 0 ... 0 43 98 233 323 || 7 4423 833 1 232\n0 0 ... 0 32 44 133 555 || 2 4534 545 6 767 \n \n 0 0 ... 0 23 12 121 832 || 2 3432 898 7\n0 0 ... 0 23 12 121 832 || 2 3432 898 7\n0 0 ... 0 23 12 121 832 || 2 3432 898 7 \n \n ```\nThe input shape for embedding layer is  (batch_size, seq_len) , the input shape for lstm encoder is  (batch_size, seq_len, embed_dim)  . \n More details coming soon \n For any question, please send me email.  \n yoosan.zhou at gmail dot com\n', 'Video-understanding-dataset \n Please feel free to pull a request. \n Note: ActivityNet v1.3, Kinetics-600, Moments in time, AVA will be used at  ActivityNet challenge 2018 \n Video Classification \n Dataset | Paper | Website | Category | #Examples |#Classes | Duration | Organizer | SOTA performance\n--------|-------|---------|----------|-----------|---------|----------|-----------|-----------------\nUCF101  |  PDF  |  Link  | human action | 13,320 | 101 | <10s | UCF | 98% (DeepMind I3D)\nHMDB51  |  PDF  |  Link  | human action | 6,766 | 51 | <10s | Brown | 80.7% (DeepMind I3D)\n ActivityNet v1.3 |  PDF  |  Link  | human activities | ~20,000 | 200 | - | ActivityNet | 8.83% err (iBUG)\nCharades |  PDF  |  Link  | daily human activities | 9,848 | 157 | - | AI2 | 0.3441 mAP (DeepMind I3D)\n Kinetics  |  PDF  |  Link  | human action | ~500,000 | 600 |  10s  | DeepMind  | -\nSports-1M |  PDF  |  Link  | sports | ~1 million | 478 | 5m36s | Google & Stanford | -\nYouTube-8M |  PDF  |  Link  | visual contents | ~7 million | 4716 | 120-500s | Google Cloud | 85% GAP (WILLOW)\nFCVID |  PDF  |  Link  | visual contents |  91,223 | 239 | 100s+ | Fudan-Columbia | - \nSomething-Something |  PDF  |  Link  | action with objects |  108,499 | 174 | ~4s | TwentyBN | - \n Moments in Time  |  PDF  |  Link  | action or activity | ~1 million | 339 | 3s | MIT-IBM Watson | -\nSLAC |  arXiv  |  Link  | recognition and localization | 520K | 200 | ~30.6s | MIT and Facebook | -  \n Temporal Action Detection \n Dataset | Paper | Website | #Examples | Organizer | SOTA performance\n--------|-------|---------|-----------|-----------|-----------------\nTHUMOS2014 |  PDF  |  Link  | 9.682 | UCF| -\nActivityNet(v1.3) |  PDF  |  Link  | ~20,000 | ActivityNet| 0.344(SJTU & Columbia )\nBroad Video Highlights | - |  Link  | 18000 | Baidu | - \n Spatio-temporally Localized Atomic Visual Actions \n Dataset | Paper | Website | #Examples |  #Classes | Organizer | SOTA performance\n--------|-------|---------|-----------|-----------|-----------|-----------------\n AVA  |  arXiv  |  Link  | 57.6k | 80 | Google & Berkeley| - \n Hand Gestures in Videos \n Dataset | Paper | Website | #Examples |  #Classes | Organizer | SOTA performance\n--------|-------|---------|-----------|-----------|-----------|-----------------\nJester | - |  Link   |  148,092  | 27 | TwentyBN | 95.34%(Ke Yang, NUDT_PDL) \n Video Captioning \n Dataset | Paper | Website | Context | #Examples | Organizer | SOTA performance\n--------|-------|---------|----------|-----------|-----------|-----------------\nMPII-MD | PDF |  Link  | movie | 68,337 clips with 68,375 sentences| MPII | -\nMSR-VTT | PDF |  Link  | 20 categories| 10,000 clips wth 200,000 sentences| MSR | -\nCharades | PDF |  Link  | human activity| 9,848 clips wth 27,847 sentences| AI2 | -\nDensevid | PDF |  Link  | event | 20k clips and 100k sentences | Stanford, ActivityNet | - \n Video Question Answering \n Dataset | Paper | Website | Task | #Examples | Organizer | SOTA performance\n--------|-------|---------|----------|-----------|-----------|-----------------\nMovieQA | PDF |  Link  | question-answering in movies | 408 movies & 14944 QAs| UToronto | -\nMarioQA | PDF |  Link  | reasoning events in game videos | 187,757 examples with 92,874 QAs| POSTECH | -', "I3D-TensorFlow \n This repo contains the inflated version of the recently popular ConvNets. \n Convert weights from the pretrained model \n DeepMind have provided the Inception-v1 inflated 3D model, building upon the  sonnet . I slightly modified their code and rewrited the i3d model using the protogenetic tensorflow op. The pretrained weights from kinetics-i3d can be easily migrated to the new model.\nTo botain the weights from kinetics-i3d, execute the following instructions.\n bash\n$ git clone https://github.com/yoosan/i3d-tensorflow\n$ cd i3d-tensorflow\n$ git clone https://github.com/deepmind/kinetics-i3d\n$ python convert_weights.py \n Training the I3D model on UCF101 \n Now I'm preparing the code for training model on the UCF101 dataset. Using the kinetics pretrained weights, we achive a result of  95.2%  top-1 accuracy on split 1 of UCF101 with RGB modality.", " Hi, this is yao.👏  \n   \n  About Me  \n \n Currently, I'm senior applied researcher at Tencent WeChat(微信) Group. \n Before that, I was a computer vision researcher at SenseTime for two years (from 2017 to 2019). \n My interest focuses on  \n Computer vision and multimodal understanding. \n Large-scale distributed system and training acceleration. \n Content-based recommendation system e.g. short videos. \n \n \n \n \n \n \n Last Updated on: 11/26/2021"]
pushingice,["scavenger-hunt \n This is a scavenger hunt to learn Linux commands. Our goal is to find all\nthe clues and learn how to use basic Linux commands in the process. \n Setup \n If you are using a new Linux install or Live CD, you may need to install\nGit first ( sudo apt-get install git  on Ubuntu). Open a terminal and type: \n git clone https://github.com/pushingice/scavenger-hunt.git\ncd scavenger-hunt\n \n First, choose a secret number with at least 4 digits to share with your team,\nor keep to yourself if you are working alone. Don't forget it! The secret\nnumber makes your clues unique, so other teams can't look over your shoulder.\nThen type: \n python generate_clues.py [secret number]\n \n NOTE: On some older systems 'python' may need to be typed as 'python3'. \n Any time we enclose something in square brackets, you need to replace it\nwith an actual value (called an argument). For example, to get started I\nmight type: \n python generate_clues.py 42\n \n This will create a subdirectory called  clues . Be sure to keep this file\n(called the README) open in a separate viewer. \n Dictionary Location \n This code is tested on Ubuntu 22.04. If you get an error about being unable\nto find your dictionary, try the following commands \n cd /\nfind . -name words\n \n Change the value of the 'conf' file to the location of your dictionary. \n Reference \n If you want to learn more about Linux when you are finished, or need a reference\nduring the hunt, go here: http://www.tldp.org/LDP/intro-linux/html/index.html. \n Clue 1: The Hunt Begins \n man \n The first command we are going to learn is  man , which is short for manual.\nTyping  man [command]  will give you a help page (usually called a manpage)\nfor most commands. \n ls \n The next command we need to learn is  ls  (list). Type  man ls  and read the\ndescription. Press  q  to exit. Then type  ls  and you should see something\nlike this: \n APPENDIX.md clues generate_clues.py LICENSE.md next_clue.py README.md\n \n Items which are blue are directories and everything else is a file. Any time\nyou need to know which files and directories are available, type  ls . \n cd \n We need a couple more tools before we can start clue hunting. To change to\nanother directory we use  cd  (change directory). You may notice that\n man cd  doesn't work. Sometimes there is no manpage for a command. In that\ncase google is your friend. Changing directories is pretty simple: \n cd clues\n \n This puts us in the clues directory. To go up a directory, we can do this: \n cd ..\n \n If you ever get lost, just do \n cd ~/scavenger-hunt\n \n to return home. If you  cd  to the  clues  directory and do an  ls , you\nwill notice that there are a lot of clue directories. Most of them contain\nfake clues. Throughout our hunt we will be looking for real clues. Using\n cd , navigate to  clues/12345  and type  ls . You should see a single\nfile named  clue . \n cat \n Finally we need to be able to look at our clues. First read the manpage for\n cat , then do: \n cat clue\n \n This should list the clue in your terminal. From now on, everything we need\nwill be contained in these clue files. It's a good idea to keep track of\nall the clue folders (like  123456 ) on a piece of paper. You can also do\nthings like copy all the clue files to your home folder, or cut and paste\nthe clue text into another file."]
andreaskoepf,["faster-rcnn \n This is an experimental Torch7 implementation of Faster RCNN - a convnet for object detection with a region proposal network.\nFor details about R-CNN please refer to the paper  Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks  by Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. \n Work in progress \n Status: Basic detection in my personal environment works.\nA 'small' network is used that can be trained on a 4 GB GPU with 800x450 images.\nBegan experimenting with ImageNet: create-imagenet-traindat.lua can be used to create a training data file for the ILSVRC2015 dataset. \n Todo: \n \n [!] regularly evaluate net during traning to compute test-set loss \n generate training graph with  gnuplot \n add final per class non-maximum suppression to generate final proposals (already included but eval code rewrite still pending) \n remove hard coded path, create full set of command line options \n add parameters to separately enable/disable training of bounding box proposal-network and fine-tuning + classification. \n \n Experiments to run: \n \n test smaller networks \n 6x6 vs. 7x7 classification ROI-pooling output size \n impact of RGB, YUV, Lab color space \n test relevance of local contrast normalization \n \n References / Review / Useful Links \n \n SPP Paper \n Fast R-CNN paper \n R-CNN paper \n vgg net, cifar.torch \n 55 epoche learn rate schedule \n", "matrixwalk \n I created a little random walk agent that moves over a 2D or 3D character lattice and tries to predict the next character for a selected action (e.g. N,E,S,W for 2D) with a recurrent neural network.  \n The agent can reach each grid position with a hugh number of different trace-histories .. nevertheless it would be sufficient to have one internal hidden-state for each unique grid position. \n Observation:\nI plotted the hidden state with t-SNE and found that no magic 'state compression' happens in a RNN - basically the RNN generates a lot of different hidden states for the same grid position depending on the history (it keeps as much context as is necessary to uniquely identify a position based on the last n observations, e.g. keeps a 'backlog' of 2 or 3 characters in most cases). \n It might be desirable for a neural system to identify redundant states and compress them to a single internal representation: e.g. multiple histories all leading to the same grid positions could be treated equally so that the next character prediction would not have to be learned for each context individually but experience could be shared. \n My impl is based on Andrej Karpathy's  char-rnn  but beside of the RNN models it was written from scratch to learn something about RNNs in  Torch . \n Next steps for this repo: Experiment with ideas for redundant state discovery and 'state compression'.", '\n \n PyTorch is a Python package that provides two high-level features:\n- Tensor computation (like NumPy) with strong GPU acceleration\n- Deep neural networks built on a tape-based autograd system \n You can reuse your favorite Python packages such as NumPy, SciPy and Cython to extend PyTorch when needed. \n \n More about PyTorch \n Installation \n Binaries \n From Source \n Docker Image \n Building the Documentation \n Previous Versions \n Getting Started \n Communication \n Releases and Contributing \n The Team \n \n | System | 2.7 | 3.5 | 3.6 |\n| :---: | :---: | :---: | :--: |\n| Linux CPU |   |   |  —  |\n| Linux GPU |   |   |  —  |\n| Windows CPU / GPU |  —  |   |   —  |\n| Linux (ppc64le) CPU |   | — |   |\n| Linux (ppc64le) GPU |   | — |   |\n See also the  ci.pytorch.org HUD . \n More About PyTorch \n At a granular level, PyTorch is a library that consists of the following components: \n | Component | Description |\n| ---- | --- |\n|  torch  | a Tensor library like NumPy, with strong GPU support |\n|  torch.autograd  | a tape-based automatic differentiation library that supports all differentiable Tensor operations in torch |\n|  torch.jit  | a compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code  |\n|  torch.nn  | a neural networks library deeply integrated with autograd designed for maximum flexibility |\n|  torch.multiprocessing  | Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training |\n|  torch.utils  | DataLoader and other utility functions for convenience | \n Usually one uses PyTorch either as: \n \n a replacement for NumPy to use the power of GPUs. \n a deep learning research platform that provides maximum flexibility and speed. \n \n Elaborating further: \n A GPU-Ready Tensor Library \n If you use NumPy, then you have used Tensors (a.k.a ndarray). \n \n PyTorch provides Tensors that can live either on the CPU or the GPU, and accelerates the\ncomputation by a huge amount. \n We provide a wide variety of tensor routines to accelerate and fit your scientific computation needs\nsuch as slicing, indexing, math operations, linear algebra, reductions.\nAnd they are fast! \n Dynamic Neural Networks: Tape-Based Autograd \n PyTorch has a unique way of building neural networks: using and replaying a tape recorder. \n Most frameworks such as TensorFlow, Theano, Caffe and CNTK have a static view of the world.\nOne has to build a neural network, and reuse the same structure again and again.\nChanging the way the network behaves means that one has to start from scratch. \n With PyTorch, we use a technique called reverse-mode auto-differentiation, which allows you to\nchange the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes\nfrom several research papers on this topic, as well as current and past work such as\n torch-autograd ,\n autograd ,\n Chainer , etc. \n While this technique is not unique to PyTorch, it\'s one of the fastest implementations of it to date.\nYou get the best of speed and flexibility for your crazy research. \n \n Python First \n PyTorch is not a Python binding into a monolithic C++ framework.\nIt is built to be deeply integrated into Python.\nYou can use it naturally like you would use  NumPy  /  SciPy  /  scikit-learn  etc.\nYou can write your new neural network layers in Python itself, using your favorite libraries\nand use packages such as Cython and Numba.\nOur goal is to not reinvent the wheel where appropriate. \n Imperative Experiences \n PyTorch is designed to be intuitive, linear in thought and easy to use.\nWhen you execute a line of code, it gets executed. There isn\'t an asynchronous view of the world.\nWhen you drop into a debugger, or receive error messages and stack traces, understanding them is straightforward.\nThe stack trace points to exactly where your code was defined.\nWe hope you never spend hours debugging your code because of bad stack traces or asynchronous and opaque execution engines. \n Fast and Lean \n PyTorch has minimal framework overhead. We integrate acceleration libraries\nsuch as  Intel MKL  and NVIDIA (cuDNN, NCCL) to maximize speed.\nAt the core, its CPU and GPU Tensor and neural network backends\n(TH, THC, THNN, THCUNN) are mature and have been tested for years. \n Hence, PyTorch is quite fast – whether you run small or large neural networks. \n The memory usage in PyTorch is extremely efficient compared to Torch or some of the alternatives.\nWe\'ve written custom memory allocators for the GPU to make sure that\nyour deep learning models are maximally memory efficient.\nThis enables you to train bigger deep learning models than before. \n Extensions Without Pain \n Writing new neural network modules, or interfacing with PyTorch\'s Tensor API was designed to be straightforward\nand with minimal abstractions. \n You can write new neural network layers in Python using the torch API\n or your favorite NumPy-based libraries such as SciPy . \n If you want to write your layers in C/C++, we provide a convenient extension API that is efficient and with minimal boilerplate.\nThere is no wrapper code that needs to be written. You can see  a tutorial here  and  an example here . \n Installation \n Binaries \n Commands to install from binaries via Conda or pip wheels are on our website:\n https://pytorch.org \n NVIDIA Jetson platforms \n Python wheels for NVIDIA\'s Jetson Nano, Jetson TX2, and Jetson AGX Xavier are available via the following URLs: \n \n Stable binaries: \n Python 2.7: https://nvidia.box.com/v/torch-stable-cp27-jetson-jp42 \n Python 3.6: https://nvidia.box.com/v/torch-stable-cp36-jetson-jp42 \n Rolling weekly binaries: \n Python 2.7: https://nvidia.box.com/v/torch-weekly-cp27-jetson-jp42 \n Python 3.6: https://nvidia.box.com/v/torch-weekly-cp36-jetson-jp42 \n \n They requires JetPack 4.2 and above and are maintained by @dusty-nv \n From Source \n If you are installing from source, we highly recommend installing an  Anaconda  environment.\nYou will get a high-quality BLAS library (MKL) and you get controlled dependency versions regardless of your Linux distro. \n Once you have  Anaconda  installed, here are the instructions. \n If you want to compile with CUDA support, install\n-  NVIDIA CUDA  9 or above\n-  NVIDIA cuDNN  v7 or above \n If you want to disable CUDA support, export environment variable  USE_CUDA=0 .\nOther potentially useful environment variables may be found in  setup.py . \n If you are building for NVIDIA\'s Jetson platforms (Jetson Nano, TX1, TX2, AGX Xavier), Instructions to  are available here \n Install Dependencies \n Common\n conda install numpy ninja pyyaml mkl mkl-include setuptools cmake cffi typing \n On Linux\n```bash \n Add LAPACK support for the GPU if needed \n conda install -c pytorch magma-cuda90 # or [magma-cuda92 | magma-cuda100 ] depending on your cuda version\n``` \n Get the PyTorch Source \n ```bash\ngit clone --recursive https://github.com/pytorch/pytorch\ncd pytorch \n if you are updating an existing checkout \n git submodule sync\ngit submodule update --init --recursive\n``` \n Install PyTorch \n On Linux\n bash\nexport CMAKE_PREFIX_PATH=${CONDA_PREFIX:-"$(dirname $(which conda))/../"}\npython setup.py install \n On macOS\n bash\nexport CMAKE_PREFIX_PATH=${CONDA_PREFIX:-"$(dirname $(which conda))/../"}\nMACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py install \n On Windows \n At least Visual Studio 2017 Update 3 (version 15.3.3 with the toolset 14.11) and  NVTX  are needed. \n If the version of Visual Studio 2017 is higher than 15.4.5, installing of "VC++ 2017 version 15.4 v14.11 toolset" is strongly recommended.\n  If the version of Visual Studio 2017 is lesser than 15.3.3, please update Visual Studio 2017 to the latest version along with installing "VC++ 2017 version 15.4 v14.11 toolset".\n  There is no guarantee of the correct building with VC++ 2017 toolsets, others than version 15.4 v14.11.\n  "VC++ 2017 version 15.4 v14.11 toolset" might be installed onto already installed Visual Studio 2017 by running its installation once again and checking the corresponding checkbox under "Individual components"/"Compilers, build tools, and runtimes". \n NVTX is a part of CUDA distributive, where it is called "Nsight Compute". For installing it onto already installed CUDA run CUDA installation once again and check the corresponding checkbox.\nBe sure that CUDA with Nsight Compute is installed after Visual Studio 2017. \n Currently VS 2017, VS 2019 and Ninja are supported as the generator of CMake. If  ninja.exe  is detected in  PATH , then Ninja will be used as the default generator, otherwise it will use VS 2017.\n  If Ninja is selected as the generator, the latest MSVC which is newer than VS 2015 (14.0) will get selected as the underlying toolchain if you have Python > 3.5, otherwise VS 2015 will be selected so you\'ll have to activate the environment. If you use CMake <= 3.14.2 and has VS 2019 installed, then even if you specify VS 2017 as the generator, VS 2019 will get selected as the generator. \n CUDA and MSVC has strong version dependencies, so even if you use VS 2017 / 2019, you will get build errors like  nvcc fatal : Host compiler targets unsupported OS . For this kind of problem, please install the corresponding VS toolchain in the table below and then you can either specify the toolset during activation (recommended) or set  CUDAHOSTCXX  to override the cuda host compiler (not recommended if there are big version differences). \n | CUDA version | Newest supported VS version                             |\n| ------------ | ------------------------------------------------------- |\n| 9.0 / 9.1    | Visual Studio 2017 Update 4 (15.4) ( _MSC_VER  <= 1911) |\n| 9.2          | Visual Studio 2017 Update 5 (15.5) ( _MSC_VER  <= 1912) |\n| 10.0         | Visual Studio 2017 (15.X) ( _MSC_VER  < 1920)           |\n| 10.1         | Visual Studio 2019 (16.X) ( _MSC_VER  < 1930)           | \n ```cmd\ncmd\n:: [Optional] Only add the next two lines if you need Python 2.7. If you use Python 3, ignore these two lines.\nset MSSdk=1\nset FORCE_PY27_BUILD=1 \n :: [Optional] If you want to build with VS 2019 generator, please change the value in the next line to  Visual Studio 16 2019 .\n:: Note: This value is useless if Ninja is detected. However, you can force that by using  set USE_NINJA=OFF .\nset CMAKE_GENERATOR=Visual Studio 15 2017 \n :: Read the content in the previous section carefully before you preceed.\n:: [Optional] If you want to override the underlying toolset used by Ninja and Visual Studio with CUDA, please run the following script block.\n:: "Visual Studio 2017 Developer Command Prompt" will be run automatically.\n:: Make sure you have CMake >= 3.12 before you do this when you use the Visual Studio generator.\n:: It\'s an essential step if you use Python 3.5.\nset CMAKE_GENERATOR_TOOLSET_VERSION=14.11\nset DISTUTILS_USE_SDK=1\nfor /f "usebackq tokens=*" %i in ( "%ProgramFiles(x86)%\\Microsoft Visual Studio\\Installer\\vswhere.exe" -version [15^,16^) -products * -latest -property installationPath ) do call "%i\\VC\\Auxiliary\\Build\\vcvarsall.bat" x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION% \n :: [Optional] If you want to override the cuda host compiler\nset CUDAHOSTCXX=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64\\cl.exe \n python setup.py install \n ``` \n Adjust Build Options (Optional) \n You can adjust the configuration of cmake variables optionally (without building first), by doing\nthe following. For example, adjusting the pre-detected directories for CuDNN or BLAS can be done\nwith such a step. \n On Linux\n bash\nexport CMAKE_PREFIX_PATH=${CONDA_PREFIX:-"$(dirname $(which conda))/../"}\npython setup.py build --cmake-only\nccmake build  # or cmake-gui build \n On macOS\n bash\nexport CMAKE_PREFIX_PATH=${CONDA_PREFIX:-"$(dirname $(which conda))/../"}\nMACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build --cmake-only\nccmake build  # or cmake-gui build \n Docker Image \n Dockerfile is supplied to build images with cuda support and cudnn v7. You can pass  -e PYTHON_VERSION=x.y  flag to specify which Python version is to be used by Miniconda, or leave it unset to use the default. Build from pytorch repo directory as docker needs to copy git repo into docker filesystem while building the image.\n docker build -t pytorch -f docker/pytorch/Dockerfile .  # [optional] --build-arg WITH_TORCHVISION=0 \n You can also pull a pre-built docker image from Docker Hub and run with nvidia-docker,\nbut this is not currently maintained and will pull PyTorch 0.2.\n nvidia-docker run --rm -ti --ipc=host pytorch/pytorch:latest \nPlease note that PyTorch uses shared memory to share data between processes, so if torch multiprocessing is used (e.g.\nfor multithreaded data loaders) the default shared memory segment size that container runs with is not enough, and you\nshould increase shared memory size either with  --ipc=host  or  --shm-size  command line options to  nvidia-docker run . \n Building the Documentation \n To build documentation in various formats, you will need  Sphinx  and the\nreadthedocs theme. \n cd docs/\npip install -r requirements.txt \nYou can then build the documentation by running  make <format>  from the\n docs/  folder. Run  make  to get a list of all available output formats. \n Previous Versions \n Installation instructions and binaries for previous PyTorch versions may be found\non  our website . \n Getting Started \n Three pointers to get you started:\n-  Tutorials: get you started with understanding and using PyTorch \n-  Examples: easy to understand pytorch code across all domains \n-  The API Reference \n Communication \n \n forums: discuss implementations, research, etc. https://discuss.pytorch.org \n GitHub issues: bug reports, feature requests, install issues, RFCs, thoughts, etc. \n Slack: The  PyTorch Slack  hosts a primary audience of moderate to experienced PyTorch users and developers for general chat, online discussions, collaboration etc. If you are a beginner looking for help, the primary medium is  PyTorch Forums . If you need a slack invite, please fill this form: https://goo.gl/forms/PP1AGvNHpSaJP8to1 \n newsletter: no-noise, one-way email newsletter with important announcements about pytorch. You can sign-up here: https://eepurl.com/cbG0rv \n \n Releases and Contributing \n PyTorch has a 90 day release cycle (major releases). Please let us know if you encounter a bug by  filing an issue . \n We appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion. \n If you plan to contribute new features, utility functions or extensions to the core, please first open an issue and discuss the feature with us.\nSending a PR without discussion might end up resulting in a rejected PR, because we might be taking the core in a different direction than you might be aware of. \n The Team \n PyTorch is a community driven project with several skillful engineers and researchers contributing to it. \n PyTorch is currently maintained by  Adam Paszke ,  Sam Gross ,  Soumith Chintala  and  Gregory Chanan  with major contributions coming from hundreds of talented individuals in various forms and means.\nA non-exhaustive but growing list needs to mention: Trevor Killeen, Sasank Chilamkurthy, Sergey Zagoruyko, Adam Lerer, Francisco Massa, Alykhan Tejani, Luca Antiga, Alban Desmaison, Andreas Kopf, James Bradbury, Zeming Lin, Yuandong Tian, Guillaume Lample, Marat Dukhan, Natalia Gimelshein, Christian Sarofeen, Martin Raison, Edward Yang, Zachary Devito. \n Note: this project is unrelated to  hughperkins/pytorch  with the same name. Hugh is a valuable contributor in the Torch community and has helped with many things Torch and PyTorch. \n License \n PyTorch is BSD-style licensed, as found in the LICENSE file.', 'Grasp-Toolkit \n Grasp Toolkit Code and full thesis document \n Disclaimer:\n- This repository exists mainly for archival purposes and contains the  work I did for my master thesis on robotic grasping.  I worked on this a while back and so much of it may not be relevant/useful given the current state of the art in the field of intelligent robotic grasping.  The  PDF Document  describes the details of this work at length.\n- The document is comprised of two parts\n  - Part 1 describes a method for grasp synthesis applicable to multifigered hands.  It also contains examples of its\n    applications on various hand models.\n  - Part 2 describes a Matlab toolkit for simulating grasping tasks. It uses bullet physics for collision and \ndynamics computations through Matlab Mex Files. \n \n All of the code for the experiments, demos, etc. described in the thesis document is contained in this repository.  \n Documentation for this code is minimal so getting it to a functional state is a non-trivial task.   \n', 'The Transformer - Attention Based Sequence Transduction \n Slides of a 25 min deep-dive into the transformer sequence-to-sequence architecture \n( Attention Is All You Need, Vaswani et al., 2017) . I gave this talk \nat the  Data Science Meetup Münster  \non September 19, 2019. \n Links to Resources \n Fun: \n \n https://talktotransformer.com/ \n https://transformer.huggingface.co/ (Write With Transformer) \n \n Blogs: \n \n http://jalammar.github.io/ \n http://nlp.seas.harvard.edu/2018/04/03/attention.html (The Annotated Transformer) \n https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html \n \n PyTorch: \n \n https://huggingface.co/pytorch-transformers/ \n https://github.com/pytorch/fairseq \n https://docs.fast.ai/text.models.html \n \n Tensorflow: \n \n https://www.tensorflow.org/beta/tutorials/text/transformer \n https://github.com/google-research/bert \n', "Experiment: Structured Latent Space \n Idea: Induce structure in the latent space to mirror the input transition-topology in the latent space. \n A simple action conditioned MLP auto-encoder is used. Actions are mapped (state-independent) through an MLP and added to the 2D encoder latent projection. Input is a grid start-position (one-hot encoded) + action (one-hot encoded). Target is the resulting position on the grid (also one-hot-encoded). The one-hot encoding of the inputs is used as example for arbitrary state-patterns. \n The action-space consists of 5 discrete actions: N, S, W, E and stay (invalid actions are replaced by stay, e.g. when agent would leave grid). \n ```\nExample: \n 6x6 grid \n Input:\n[0., 0., 0., 0., 0., 0.],\n[0., 0., 0., 0., 0., 0.],\n[0., 0., 0., 0., 0., 0.],\n[0., 1., 0., 0., 0., 0.],\n[0., 0., 0., 0., 0., 0.],\n[0., 0., 0., 0., 0., 0.] \n Action: \n[1, 0, 0, 0, 0] (North) \n Target:\n[0., 0., 0., 0., 0., 0.],\n[0., 0., 0., 0., 0., 0.],\n[0., 1., 0., 0., 0., 0.],\n[0., 0., 0., 0., 0., 0.],\n[0., 0., 0., 0., 0., 0.],\n[0., 0., 0., 0., 0., 0.]\n``` \n To directly visualize the latent space the bottleneck-layer of the auto-encoder is 2D. \n Initially the model has no information about the meaning of different actions or how input patterns relate to each other. \n For each action a fixed latent space offset is learned (independent of the source position) which is added to the 2D latent-encoding of the encoder to 'predict' the destination position. \n Results \n It works to some extend but was more brittle than I had expected. The code was tested for input grids up to 6x6. For larger inputs a different model would be required (it did not immediately work when grid is set to 8x8). \n Latent space during training \n The animated GIFs show plots of the 2D latents during training for all input grid positions. \n \n \n \n \n \n", 'MLP Hidden Layer Activation Visualization \n To gain some intuition about the internal representation of simple multi-layer perceptrons (MLPs) I trained a neural network with  PyTorch  using a range of different activation functions on a 2D -> 1D function. The training pairs consist of the u,v image coordinates [0, 1]^2 as inputs (first row) and the intensities from a 4x4 checkerboard pattern as targets (last row). The other rows show intensities of neurons in the hidden layers for all the u,v input coordinates (one box=one neuron). The animations show how the neuron responses change over the course of the first 4000 steps trained with Adam (lr=0.01, loss_fn=MSE). For further details please refer to the implementation in  main.py . \n Sigmoid \n \n \n tanh \n tanh \n \n \n ReLU \n ReLU \n \n \n LeakyReLU \n LeakyReLU \n \n \n ELU \n ELU \n \n \n Cosine \n \n \n CELU \n CELU \n \n \n GELU \n GELU \n \n \n Mish \n Mish \n \n \n SELU \n SELU \n \n \n SiLU \n SiLU \n', "NOTE: INTENDED FOR INTERNAL USE ONLY \n This is an internal repositroy of  LAION . Using this script outside the LAION cluster will fail. \n Dockerfile for Idle Captioning \n This script generates synthetic captions for images of the LAION text-image datasets to utilize GPUs during 'idle' periods. \n Setup & Run \n Installing and running the captioning script on a fresh machine: \n \n Run  git clone https://github.com/andreaskoepf/laion_idle_cap.git  to clone this repository on the new machine. \n Run  cd laion_idle_cap  to change to the newly created directory. \n Run  ./install_docker.sh  to install nvidia-docker. \n Run  ./pull.sh  to pull the captioning docker image which contains all dependencies. \n Run  ./start.sh --gpus 0-7 --workers 2  to start the captioning script (detached) in a new docker container. If the  --gpus  option is omitted all available GPUs are used. To select specific devices use comma separated device indices or indice-ranges (e.g.  1-3  or  0,2,4 ). The  --workers  option allows to launch more then one worker per GPU (recommended is 2 for full GPU utilization, default is 1). \n Optionally run  ./attach.sh  to attach your terminal to the running instance of the captioning script and see its output. \n \n Note : Some of the scripts (e.g.  start.sh  and  stop.sh ) fail when they are launched by a user who is is not member of the docker group (e.g. you might see an error like 'permission denied to connect to the docker deamon socket'). In this case please use  sudo  to run them as superuser. \n Stopping the Docker Container \n \n run  ./stop.sh  or  docker stop laion_cah \n \n Other Script Files \n \n start_bash.sh  starts the docker container and launches bash (start attached, source will be mounted to  /mnt/src ) \n start_dev.sh  maps the file  docker/c_h2.py  into a new docker container and starts the script attached (useful for testing changes made outside the docker container, e.g. during development). \n build.sh  builds the docker image (e.g. laion_idle_cah:v0) \n save_image.sh  writes the docker image into a tar file \n push.sh  push the docker container to docker hub \n"]
rushter,['Facebook Recruiting IV Human or Robot \n Predict if an online bid is made by a machine or a human \n placed 7th/1004', "Data science blogs \n A curated list of data science blogs \n \n A Blog From a Human-engineer-being http://www.erogol.com/  (RSS) \n Aakash Japi http://aakashjapi.com/  (RSS) \n Abhinav Sagar https://medium.com/@abhinav.sagar  (RSS) \n Adit Deshpande https://adeshpande3.github.io/  (RSS) \n Advanced Analytics & R http://advanceddataanalytics.net/  (RSS) \n Adventures in Data Land http://blog.smola.org  (RSS) \n Ahmed BESBES https://ahmedbesbes.com/  (RSS) \n Ahmed El Deeb https://medium.com/@D33B  (RSS) \n Airbnb Data blog https://medium.com/airbnb-engineering/tagged/data-science  (RSS) \n Alex Perrier http://alexisperrier.com/  (RSS) \n Algobeans | Data Analytics Tutorials & Experiments for the Layman https://algobeans.com  (RSS) \n Amazon AWS AI Blog https://aws.amazon.com/blogs/ai/  (RSS) \n Amit Chaudhary https://amitness.com  (RSS) \n Analytics Vidhya http://www.analyticsvidhya.com/blog/  (RSS) \n Analytics and Visualization in Big Data @ Sicara https://blog.sicara.com  (RSS) \n Andreas Müller http://peekaboo-vision.blogspot.com/  (RSS) \n Andrej Karpathy blog http://karpathy.github.io/  (RSS) \n Andrey Vasnetsov https://comprehension.ml/  (RSS) \n Andrew Brooks http://brooksandrew.github.io/simpleblog/  (RSS) \n Andrey Kurenkov http://www.andreykurenkov.com/writing/  (RSS) \n Andrii Polukhin https://polukhin.tech/  (RSS) \n Anton Lebedevich's Blog http://mabrek.github.io/  (RSS) \n Arthur Juliani https://medium.com/@awjuliani  (RSS) \n Audun M. Øygard http://www.auduno.com/  (RSS) \n Avi Singh https://avisingh599.github.io/  (RSS) \n Beautiful Data http://beautifuldata.net/  (RSS) \n Beckerfuffle http://mdbecker.github.io/  (RSS) \n Becoming A Data Scientist http://www.becomingadatascientist.com/  (RSS) \n Ben Bolte's Blog http://benjaminbolte.com/ml/  (RSS) \n Ben Frederickson http://www.benfrederickson.com/blog/  (RSS) \n Berkeley AI Research http://bair.berkeley.edu/blog/  (RSS) \n Big-Ish Data http://bigishdata.com/  (RSS) \n Blog on neural networks http://yerevann.github.io/  (RSS) \n Blogistic Regression https://wcbeard.github.io/blog/  (RSS) \n blogR | R tips and tricks from a scientist https://drsimonj.svbtle.com/  (RSS) \n Brain of mat kelcey http://matpalm.com/blog/  (RSS) \n Brilliantly wrong thoughts on science and programming https://arogozhnikov.github.io/  (RSS) \n Bugra Akyildiz http://bugra.github.io/  (RSS) \n Carl Shan http://carlshan.com/  (RSS) \n Casual Inference https://lmc2179.github.io/  (RSS) \n Chris Stucchio https://www.chrisstucchio.com/blog/index.html  (RSS) \n Christophe Bourguignat https://medium.com/@chris_bour  (RSS) \n Christopher Nguyen https://medium.com/@ctn  (RSS) \n cnvrg.io blog https://blog.cnvrg.io/  (RSS) \n colah's blog http://colah.github.io/archive.html  (RSS) \n Daniel Bourke https://www.mrdbourke.com  (RSS) \n Daniel Forsyth http://www.danielforsyth.me/  (RSS) \n Daniel Homola https://danielhomola.com/  (RSS) \n Data Blogger https://www.data-blogger.com/  (RSS) \n Data Double Confirm https://projectosyo.wixsite.com/datadoubleconfirm  (RSS) \n Data Miners Blog http://blog.data-miners.com/  (RSS) \n Data Mining Research http://www.dataminingblog.com/  (RSS) \n Data Mining: Text Mining, Visualization and Social Media http://datamining.typepad.com/data_mining/  (RSS) \n Data School http://www.dataschool.io/  (RSS) \n Data Science 101 http://101.datascience.community/  (RSS) \n Data Science @ Facebook https://research.fb.com/category/data-science/  (RSS) \n Data Science Dojo Blog https://datasciencedojo.com/blog/  (RSS) \n Data Science Insights http://www.datasciencebowl.com/data-science-insights/  (RSS) \n Data Science Tutorials https://codementor.io/data-science/tutorial  (RSS) \n Data Science Vademecum http://datasciencevademecum.wordpress.com/  (RSS) \n Data Science Notebook http://uconn.science/  (RSS) \n Dataaspirant http://dataaspirant.com/  (RSS) \n Dataclysm https://theblog.okcupid.com/tagged/data  (RSS) \n DataGenetics http://datagenetics.com/blog.html  (RSS) \n Dataiku https://blog.dataiku.com/  (RSS) \n DataKind http://www.datakind.org/blog  (RSS) \n Datanice https://datanice.wordpress.com/  (RSS) \n Dataquest Blog https://www.dataquest.io/blog/  (RSS) \n DataRobot http://www.datarobot.com/blog/  (RSS) \n Datascienceblog.net https://www.datascienceblog.net  (RSS) \n Datascope http://datascopeanalytics.com/blog  (RSS) \n DatasFrame http://tomaugspurger.github.io/  (RSS) \n David Mimno http://www.mimno.org/  (RSS) \n David Robinson http://varianceexplained.org/  (RSS) \n Dayne Batten http://daynebatten.com  (RSS) \n Deep and Shallow https://deep-and-shallow.com  (RSS) \n Deep Learning http://deeplearning.net/blog/  (RSS) \n Deepdish http://deepdish.io/  (RSS) \n Delip Rao http://deliprao.com/  (RSS) \n DENNY'S BLOG https://dennybritz.com/  (RSS) \n Dimensionless https://dimensionless.in/blog/  (RSS) \n Distill http://distill.pub/  (RSS) \n District Data Labs https://www.districtdatalabs.com/blog \n Diving into data https://blog.datadive.net/  (RSS) \n Domino Data Lab's blog http://blog.dominodatalab.com/  (RSS) \n Dr. Randal S. Olson http://www.randalolson.com/blog/  (RSS) \n Drew Conway https://medium.com/@drewconway  (RSS) \n Dustin Tran http://dustintran.com/blog/  (RSS) \n Eder Santana https://edersantana.github.io/blog.html  (RSS) \n Edwin Chen http://blog.echen.me  (RSS) \n EFavDB http://efavdb.com/  (RSS) \n Eigenfoo https://eigenfoo.xyz/  (RSS) \n Ethan Rosenthalh https://www.ethanrosenthal.com/#blog  (RSS) \n Emilio Ferrara, Ph.D.  http://www.emilio.ferrara.name/  (RSS) \n Entrepreneurial Geekiness http://ianozsvald.com/  (RSS) \n Eric Jonas http://ericjonas.com/archives.html  (RSS) \n Eric Siegel http://www.predictiveanalyticsworld.com/blog  (RSS) \n Erik Bern http://erikbern.com  (RSS) \n ERIN SHELLMAN http://www.erinshellman.com/  (RSS) \n Eugenio Culurciello http://culurciello.github.io/  (RSS) \n Fabian Pedregosa http://fa.bianp.net/  (RSS) \n Fast Forward Labs https://blog.fastforwardlabs.com/  (RSS) \n Florian Hartl http://florianhartl.com/  (RSS) \n FlowingData http://flowingdata.com/  (RSS) \n Full Stack ML http://fullstackml.com/  (RSS) \n GAB41 http://www.lab41.org/gab41/  (RSS) \n Garbled Notes http://www.chioka.in/  (RSS) \n Grate News Everyone http://gratenewseveryone.wordpress.com/  (RSS) \n Greg Reda http://www.gregreda.com/blog/  (RSS) \n i am trask http://iamtrask.github.io/  (RSS) \n I Quant NY http://iquantny.tumblr.com/  (RSS) \n inFERENCe http://www.inference.vc/  (RSS) \n Insight Data Science https://blog.insightdatascience.com/  (RSS) \n INSPIRATION INFORMATION http://myinspirationinformation.com/  (RSS) \n Ira Korshunova http://irakorshunova.github.io/  (RSS) \n I’m a bandit https://blogs.princeton.edu/imabandit/  (RSS) \n Java Machine Learning and DeepLearning http://ramok.tech/machine-learning/  (RSS) \n Jason Toy http://www.jtoy.net/  (RSS) \n jbencook https://jbencook.com/  (RSS) \n Jeremy D. Jackson, PhD http://www.jeremydjacksonphd.com/  (RSS) \n Jesse Steinweg-Woods https://jessesw.com/  (RSS) \n John Myles White http://www.johnmyleswhite.com/  (RSS) \n Jonas Degrave http://317070.github.io/  (RSS) \n Jovian https://blog.jovian.ai/  (RSS) \n Joy Of Data http://www.joyofdata.de/blog/  (RSS) \n Julia Evans http://jvns.ca/  (RSS) \n jWork.ORG.  https://jwork.org/  (RSS) \n Kavita Ganesan's NLP and Text Mining Blog http://kavita-ganesan.com/  (RSS) \n KDnuggets http://www.kdnuggets.com/  (RSS) \n Keeping Up With The Latest Techniques http://colinpriest.com/  (RSS) \n Kenny Bastani http://www.kennybastani.com/  (RSS) \n Kevin Davenport https://kldavenport.com/  (RSS) \n kevin frans http://kvfrans.com/  (RSS) \n korbonits | Math ∩ Data http://korbonits.github.io/  (RSS) \n Large Scale Machine Learning  http://bickson.blogspot.com/  (RSS) \n LATERAL BLOG https://blog.lateral.io/  (RSS) \n Lazy Programmer http://lazyprogrammer.me/  (RSS) \n Learn Analytics Here https://learnanalyticshere.wordpress.com/  (RSS) \n LearnDataSci http://www.learndatasci.com/  (RSS) \n Learning With Data https://learningwithdata.com/  (RSS) \n Life, Language, Learning http://daoudclarke.github.io/  (RSS) \n Locke Data https://itsalocke.com/blog/  (RSS) \n Loic Tetrel https://ltetrel.github.io/  (RSS) \n Louis Dorard http://www.louisdorard.com/blog/  (RSS) \n M.E.Driscoll http://medriscoll.com/  (RSS) \n Machine Learning (Theory) http://hunch.net/  (RSS) \n Machine Learning and Data Science http://alexhwoods.com/blog/  (RSS) \n Machine Learning https://charlesmartin14.wordpress.com/  (RSS) \n Machine Learning Mastery http://machinelearningmastery.com/blog/  (RSS) \n Machine Learning Blogs https://machinelearningblogs.com/  (RSS) \n Machine Learning, etc http://yaroslavvb.blogspot.com  (RSS) \n Machine Learning, Maths and Physics https://mlopezm.wordpress.com/  (RSS) \n Machined Learnings http://www.machinedlearnings.com/  (RSS) \n MAPPING BABEL https://jack-clark.net/  (RSS) \n MAPR Blog https://mapr.com/blog/ \n MAREK REI http://www.marekrei.com/blog/  (RSS) \n Mark White https://www.markhw.com/blog  (RSS) \n MARGINALLY INTERESTING http://blog.mikiobraun.de/  (RSS) \n Math ∩ Programming http://jeremykun.com/  (RSS) \n Matthew Rocklin http://matthewrocklin.com/blog/  (RSS) \n Mic Farris http://www.micfarris.com/  (RSS) \n Mike Tyka http://mtyka.github.io/  (RSS) \n Mirror Image https://mirror2image.wordpress.com/  (RSS) \n Mitch Crowe http://www.mitchcrowe.com/  (RSS) \n MLWave http://mlwave.com/  (RSS) \n MLWhiz http://mlwhiz.com/  (RSS) \n Models are illuminating and wrong https://peadarcoyle.wordpress.com/  (RSS) \n Moody Rd http://blog.mrtz.org/  (RSS) \n Moonshots http://jxieeducation.com/  (RSS) \n Mourad Mourafiq http://mourafiq.com/  (RSS) \n Natural language processing blog http://nlpers.blogspot.fr/  (RSS) \n Neil Lawrence http://inverseprobability.com/blog.html  (RSS) \n Neptune Blog: in-depth articles for machine learning practitioners https://neptune.ai/blog  (RSS) \n Nikolai Janakiev https://janakiev.com/  (RSS) \n NLP and Deep Learning enthusiast http://camron.xyz/  (RSS) \n no free hunch http://blog.kaggle.com/  (RSS) \n Nuit Blanche http://nuit-blanche.blogspot.com/  (RSS) \n Number 2147483647 https://no2147483647.wordpress.com/  (RSS) \n On Machine Intelligence https://aimatters.wordpress.com/  (RSS) \n Opiate for the masses Data is our religion. http://opiateforthemass.es/  (RSS) \n p-value.info http://www.p-value.info/  (RSS) \n Pete Warden's blog http://petewarden.com/  (RSS) \n Peter Laurinec - Time series data mining in R https://petolau.github.io/  (RSS) \n Plotly Blog http://blog.plot.ly/  (RSS) \n Probably Overthinking It http://allendowney.blogspot.ca/  (RSS) \n Prooffreader.com http://www.prooffreader.com  (RSS) \n ProoffreaderPlus http://prooffreaderplus.blogspot.ca/  (RSS) \n Publishable Stuff http://www.sumsar.net/  (RSS) \n PyImageSearch http://www.pyimagesearch.com/  (RSS) \n Pythonic Perambulations https://jakevdp.github.io/  (RSS) \n quintuitive http://quintuitive.com/  (RSS) \n R and Data Mining https://rdatamining.wordpress.com/  (RSS) \n R-bloggers http://www.r-bloggers.com/  (RSS) \n R2RT http://r2rt.com/  (RSS) \n Ramiro Gómez http://ramiro.org/notebooks/  (RSS) \n Randy Zwitch http://randyzwitch.com/  (RSS) \n RaRe Technologies http://rare-technologies.com/blog/  (RSS) \n Reinforcement Learning For Fun https://reinforcementlearning4.fun  (RSS) \n Revolutions http://blog.revolutionanalytics.com/  (RSS) \n Rinu Boney http://rinuboney.github.io/  (RSS) \n RNDuja Blog http://rnduja.github.io/  (RSS) \n Robert Chang https://medium.com/@rchang  (RSS) \n Rocket-Powered Data Science http://rocketdatascience.org  (RSS) \n Sachin Joglekar's blog https://codesachin.wordpress.com/  (RSS) \n samim https://medium.com/@samim  (RSS) \n Sebastian Raschka http://sebastianraschka.com/blog/index.html  (RSS) \n Sebastian Ruder http://sebastianruder.com/  (RSS) \n Sebastian's slow blog http://www.nowozin.net/sebastian/blog/  (RSS) \n Self Learn Data Science https://selflearndatascience.com  (RSS) \n Shakir's Machine Learning Blog http://blog.shakirm.com/  (RSS) \n Simply Statistics http://simplystatistics.org  (RSS) \n Springboard Blog http://springboard.com/blog \n Startup.ML Blog http://startup.ml/blog  (RSS) \n Stats and R https://www.statsandr.com/blog/  (RSS) \n Statistical Modeling, Causal Inference, and Social Science http://andrewgelman.com/  (RSS) \n Stigler Diet http://stiglerdiet.com/  (RSS) \n Stitch Fix Tech Blog http://multithreaded.stitchfix.com/blog/  (RSS) \n Stochastic R&D Notes http://arseny.info/  (RSS) \n Storytelling with Statistics on Quora http://datastories.quora.com/ \n StreamHacker http://streamhacker.com/  (RSS) \n Subconscious Musings http://blogs.sas.com/content/subconsciousmusings/  (RSS) \n Swan Intelligence http://swanintelligence.com/  (RSS) \n TechnoCalifornia http://technocalifornia.blogspot.se/  (RSS) \n TEXT ANALYSIS BLOG | AYLIEN http://blog.aylien.com/  (RSS) \n The Angry Statistician http://angrystatistician.blogspot.com/  (RSS) \n The Clever Machine https://theclevermachine.wordpress.com/  (RSS) \n The Data Camp Blog https://www.datacamp.com/community/blog  (RSS) \n The Data Incubator http://blog.thedataincubator.com/  (RSS) \n The Data Science Lab https://datasciencelab.wordpress.com/  (RSS) \n The Data Science Swiss Army Knife https://www.kamwithk.com/  (RSS) \n THE ETZ-FILES http://alexanderetz.com/  (RSS) \n The Science of Data http://www.martingoodson.com  (RSS) \n The Shape of Data https://shapeofdata.wordpress.com  (RSS) \n The unofficial Google data science Blog http://www.unofficialgoogledatascience.com/  (RSS) \n Tim Dettmers http://timdettmers.com/  (RSS) \n Tombone's Computer Vision Blog http://www.computervisionblog.com/  (RSS) \n Tommy Blanchard http://tommyblanchard.com/category/projects  (RSS) \n Towards Data Science https://towardsdatascience.com/  (RSS) \n Trevor Stephens http://trevorstephens.com/  (RSS) \n Trey Causey http://treycausey.com/  (RSS) \n UW Data Science Blog http://datasciencedegree.wisconsin.edu/blog/  (RSS) \n Victor Zhou https://victorzhou.com  (RSS) \n Wellecks http://wellecks.wordpress.com/  (RSS) \n Wes McKinney http://wesmckinney.com/archives.html  (RSS) \n While My MCMC Gently Samples http://twiecki.github.io/  (RSS) \n WildML http://www.wildml.com/  (RSS) \n Will do stuff for stuff http://rinzewind.org/blog-en  (RSS) \n Will wolf http://willwolf.io/  (RSS) \n WILL'S NOISE http://www.willmcginnis.com/  (RSS) \n William Lyon http://www.lyonwj.com/  (RSS) \n Win-Vector Blog http://www.win-vector.com/blog/  (RSS) \n Yanir Seroussi http://yanirseroussi.com/  (RSS) \n Zac Stewart http://zacstewart.com/  (RSS) \n ŷhat http://blog.yhat.com/  (RSS) \n ℚuantitative √ourney http://outlace.com/  (RSS) \n 大トロ http://blog.otoro.net/  (RSS) \n \n RSS \n You can import an  opml file  to your favorite RSS reader. \nAlso you can add  a feed  where the list is always up to date. \n Contributing \n Your contributions are always welcome!", 'Machine learning algorithms \n A collection of minimal and clean implementations of machine learning algorithms. \n Why? \n This project is targeting people who want to learn internals of ml algorithms or implement them from scratch. \nThe code is much easier to follow than the optimized libraries and easier to play with. \nAll algorithms are implemented in Python, using numpy, scipy and autograd.   \n Implemented: \n \n Deep learning (MLP, CNN, RNN, LSTM) \n Linear regression, logistic regression \n Random Forests \n Support vector machine (SVM) with kernels (Linear, Poly, RBF) \n K-Means \n Gaussian Mixture Model \n K-nearest neighbors \n Naive bayes \n Principal component analysis (PCA) \n Factorization machines \n Restricted Boltzmann machine (RBM) \n t-Distributed Stochastic Neighbor Embedding (t-SNE) \n Gradient Boosting trees (also known as GBDT, GBRT, GBM, XGBoost) \n Reinforcement learning (Deep Q learning) \n \n Installation \n sh\n        git clone https://github.com/rushter/MLAlgorithms\n        cd MLAlgorithms\n        pip install scipy numpy\n        python setup.py develop \n How to run examples without installation \n sh\n        cd MLAlgorithms\n        python -m examples.linear_models \n How to run examples within Docker \n sh\n        cd MLAlgorithms\n        docker build -t mlalgorithms .\n        docker run --rm -it mlalgorithms bash\n        python -m examples.linear_models \n Contributing \n Your contributions are always welcome! \nFeel free to improve existing code, documentation or implement new algorithm. \nPlease open an issue to propose your changes if they are big enough.  ', 'socks5 \n A toy socks 5 server written in Python \n https://rushter.com/blog/python-socks-server/', '\n Twitter:  @rushter \n Blog:  https://rushter.com/blog/ \n']
pdoherty,[]
jhjin,["OverFeat-Torch7 Wrapper \n OverFeat is a Convolutional Network-based image classifier and feature extractor from NYU.\nThe original can be found in the repository: https://github.com/sermanet/OverFeat\nThis application loads weights from OverFeat and construct a network for vanilla Torch7.\nTorch7 and extra packages (image, nn, torchffi) should be properly installed. \n Install \n Run the shell script to download weights and build this library. \n bash\nsh install.sh \n Run demo \n Run the command below.\nBy default, the script loads a small network,\nand categorizes the  bee.jpg  image using  nn  backend (on CPU). \n bash\nth run.lua \n For example, if you want to run a big model on GPU using  cudnn  library in a memory-efficient manner (inplace opertor), use the command instead. \n bash\nth run.lua --network big --backend cudnn --inplace \n Docker \n If you'd prefer a faster setup process and are familiar with Docker, you can build and run a container with all of the necessary dependencies with the following commands: \n bash\ndocker build -t overfeat-torch .\ndocker run overfeat-torch\ncd /root/overfeat/\nth run.lua", 'TME Motorway dataset preprocessing \n This repository contains a preprocessing script that converts raw images from TME Motor dataset to those in RGB format.\nThe dataset can be found in  here .\nOpenCV library is reqruied to run this script. \n Compile \n sh\ng++ preprocess.cpp -o preprocess -I/usr/local/include/opencv2 -L/usr/local/lib -lm -lopencv_core -lopencv_highgui -lopencv_imgproc \n Run \n sh\n./preprocess 1 left   # or right', 'K-means-Learning-Torch7 \n K-means feature learning demo on CIFAR translated to Lua/Torch7. This work was\npublished as "An Analysis of Single-Layer Networks in Unsupervised Feature\nLearning" by Adam Coates et al. 2011. The original MatLab code can be found in\nhttp://www.cs.stanford.edu/~acoates/ \n Dependencies \n It requires  unsup  and  optim  packages. \n sh\nluarocks install unsup optim \n Run demo \n Use the command below. \n sh\nth run.lua', "Convolutional neural networks with stochastic input \n Despite of the success of deep networks, they could be easily fooled by few pixels of noise so as to output incorrect answers.\nOur feedforward model utilizes uncertainty information and achieves high robustness against strong noise on a large-scale dataset.\nThis package contains implementations of stochastic feedforward operators that were mostly modified and derived from  nn  and  cunn  packages. \n The video summarizes our work http://youtube.com/watch?v=9cP06jFpxt0 .\nMore details are in the paper http://arxiv.org/abs/1511.06306 . \n Install \n Choose both or either of  nn / cunn  backend packages depending on your computing environment. \n bash\nluarocks install https://raw.githubusercontent.com/jhjin/stochastic-cnn/master/stnn-scm-1.rockspec    # cpu\nluarocks install https://raw.githubusercontent.com/jhjin/stochastic-cnn/master/stcunn-scm-1.rockspec  # cuda \n Available modules \n This is a list of available modules. \n lua\nnn.StochasticCAddTable()\nnn.StochasticConcatTable()\nnn.StochasticDropout()\nnn.StochasticIdentity()\nnn.StochasticLinear()\nnn.StochasticLogSoftMax()\nnn.StochasticReLU()\nnn.StochasticSoftMax()\nnn.StochasticSpatialAveragePooling()\nnn.StochasticSpatialBatchNormalization()\nnn.StochasticSpatialConvolution()\nnn.StochasticSpatialConvolutionMM()\nnn.StochasticSpatialMaxPooling()\nnn.StochasticSpatialSampling()\nnn.StochasticSpatialSoftMax()\nnn.StochasticThreshold()\nnn.StochasticView() \n Example \n Refer to the following code or check the  demo  directory. \n ```lua\nrequire('stnn') \n -- set dummy input and input variance\nlocal x = torch.randn(1,1,4,4)\nlocal x_var = x:clone():fill(0.1) \n -- standard feedforward\nlocal model = nn.Sequential()\nmodel:add(nn.SpatialConvolution(1,8,3,3))\nmodel:add(nn.SpatialMaxPooling(2,2,2,2))\nlocal y_standard = model:forward(x) \n -- stochastic feedforward\nlocal model_st = nn.toStochasticModel(model)\nlocal y_stochastic  = model_st:forward(x, x_var) \n -- compare results\nprint(y_standard:view(-1))\nprint(y_stochastic:view(-1))\n```", 'Flattened convolutional neural networks \n This package has 1D convolution modules (over channel, in vertical, in horizontal) used in\n[Flattened Convolutional Neural Networks for Feedforward Acceleration] (http://arxiv.org/abs/1412.5474)\nwhere we denote the flattened convolution layer as a sequence of one-dimensional filters across all 3D directions. \n Install \n Choose both or either of  nn / cunn  backend packages depending on your computing environment. \n bash\nluarocks install https://raw.githubusercontent.com/jhjin/flattened-cnn/master/nnconv1d-scm-1.rockspec    # cpu\nluarocks install https://raw.githubusercontent.com/jhjin/flattened-cnn/master/cunnconv1d-scm-1.rockspec  # cuda \n or use this command if you already cloned this repo. \n bash\ncd nn-conv1d\nluarocks make rocks/nnconv1d-scm-1.rockspec\ncd ../cunn-conv1d\nluarocks make rocks/cunnconv1d-scm-1.rockspec \n Available modules \n This is a list of available modules. \n lua\nnn.LateralConvolution(nInputPlane, nOutputPlane)        -- 1d conv over feature\nnn.HorizontalConvolution(nInputPlane, nOutputPlane, kL) -- 1d conv in horizontal\nnn.VerticalConvolution(nInputPlane, nOutputPlane, kL)   -- 1d conv in vertical \n Example \n Run the command below. \n bash\nth example.lua', "Low precision (8-bit) Torch nn library \n This experimental work uses  Google's low precision GEMM \nand only supports few modules. \n Install \n bash\ngit clone https://github.com/jhjin/nn8 --recursive\ncd nn8\nluarocks make rocks/nn8-scm-1.rockspec \n Test \n bash\nth test-precision.lua   # small model\nth test-speed.lua       # large model", 'Triplet Criterion On-the-fly \n A loss function based on the distances between anchor, positive and negative embeddings used in \n"FaceNet: A Unified Embedding for Face Recognition and Clustering" http://arxiv.org/abs/1503.03832 .\nThe module finds positive and negative embeddings within a current mini-batch on-the-fly,\nso it does not require additional space to save embeddings.\nThis is basically a simpler version of https://github.com/Atcold/torch-TripletEmbedding . \n Install \n Install this module via luarocks \n luarocks install https://raw.githubusercontent.com/jhjin/triplet-criterion/master/rocks/triplet-scm-1.rockspec \n Usage \n The loss function can be used in the same way as other criterions except few parameters as follows. \n lua\nrequire(\'triplet\')\nlocal loss = nn.TripletCriterion(samples, blocks, norm, margin) \n \n samples  : the number of faces sampled from each identity in a batch \n blocks  : the number of identities in a batch ( samples  x  blocks  <  batchSize ) \n norm  : Lp-norm for distances between embeddings (default 2) \n margin  : a hypersphere margin between anchor-positive and anchor-negative pairs (default 0.2) \n \n In a mini-batch, samples from the same identity should be prepared in a consecutive ordering by thier batch index.\nIn the case of 2  samples  and 3  blocks  with a  batchSize  of 8, for example, the batch should be prepared in \n | Batch index | Identity                |\n|-------------|-------------------------|\n| 1           | Person A                |\n| 2           | Person A                |\n| 3           | Person B                |\n| 4           | Person B                |\n| 5           | Person C                |\n| 6           | Person C                |\n| 7           | Person randomly sampled |\n| 8           | Person randomly sampled | \n From the example, anchor and positive embeddings are selected from\nthe first  samples  x  blocks  region (batch index 1~6)\nwhile negative embeddings are selected from\nthe rest ( batchSize  -  samples  x  blocks ) of the region (batch index 7,8). \n Training \n A large size of batch is preferred in order to let the training converge to a higher score/accuracy.', 'gSLICr-Torch wrapper \n A Torch wrapper for gSLICr super-pixel algorithm\nhttps://github.com/carlren/gSLICr . \n Install \n bash\ngit clone http://github.com/jhjin/gSLICr-torch.git --recursive\ncd gSLICr-torch\nluarocks make \n Demos \n bash\nqlua visual_demo.lua input.jpg\nth seg_mask_demo.lua input.jpg output.dat', 'Tensorflow C++ API example \n The repository provides a basic image classification example using Tensorflow shared library (.so).\nTested on the Ubuntu 16.04 machine. \n Dependencies \n Download  cudnn  library under the  lib  directory for CUDA. \n bash\nlib/cudnn.h\nlib/libcudnn.so.5  # with major version included in filename \n and  make dependency  to install dependent packages via apt-get.\nUpdate  CMakeLists.txt  according to your configuration if needed. \n Build and run \n Calling the Makefile target will build tensorflow library,\ndownload a pretrained model, and run the app. \n bash\nmake \n If you need python interface, try  pip install lib/tensorflow*.whl .', "Caffe2 C++ API example \n The repository provides a basic image classification and detection example using Caffe2 shared library (.so).\nTested on the Ubuntu 16.04 machine. \n Dependencies \n Install dependent packages via apt-get. \n bash\nmake dependency\nmake cudnn       # (optional) download publicly released cudnn v5 \n Download  cudnn  library and decompress under the  ./cuda  directory which will be used by Caffe2 library.\nFor some reasons if you do not have an access to the latest cudnn, just proceed with  make cudnn .\nIt will download publicly available cudnn (v5) - quite ancient but still ok to experiment with. \n bash\n./cuda/include/cudnn.h\n./cuda/lib64/libcudnn.so.* \n Before build Caffe2, update  CMakeLists.txt  according to your configuration if needed. \n Build and run \n The default make target will do all jobs for you - build caffe2 library, download a pretrained model (Squeeznet) and test images, compile and run the app.\nThe example app is heavily based on  Leo Vandriel's work . \n bash\nmake     # all at once \n Or if you want to move slowly step-by-step \n bash\nmake model    # download model\nmake build    # build caffe2\nmake app      # compile app\nmake cnn      # run classification app (inference)\nmake rcnn     # run detection app (inference) \n Desired outcome from the command is: \n text\n$ make cnn\n==> init network\n==> parse image list\n==> prepare batch\n==> feedforward\n==> retrieve results\nP( lemon | lemon.jpg ) = 0.949022\nP( daisy | flower.jpg ) = 0.960899 \n text\n$ make rcnn\n==> using CPU\n==> init network\n==> parse image list\n==> prepare batch (1 x 3 x 600 x 1116)\n==> feedforward\n==> retrieve results\nP( person | street.jpg [185,186,85,260] ) = 0.999836\nP( person | street.jpg [366,208,124,281] ) = 0.99977\nP( person | street.jpg [754,191,108,178] ) = 0.992861\nP( person | street.jpg [466,241,98,81] ) = 0.973806\nP( person | street.jpg [914,180,88,219] ) = 0.95949\nP( person | street.jpg [840,179,45,81] ) = 0.898059\nP( person | street.jpg [921,177,82,72] ) = 0.849532\nP( person | street.jpg [953,178,50,57] ) = 0.747317\nP( person | street.jpg [464,245,107,186] ) = 0.702847\nP( car | street.jpg [939,164,173,333] ) = 0.988054\nP( car | street.jpg [556,198,30,24] ) = 0.834219\nP( car | street.jpg [585,203,26,21] ) = 0.787161\nP( car | street.jpg [515,199,37,24] ) = 0.756677\nP( motorcycle | street.jpg [771,299,83,134] ) = 0.994598\nP( truck | street.jpg [828,153,187,230] ) = 0.909398\nP( truck | street.jpg [258,181,78,102] ) = 0.771324\nP( couch | street.jpg [442,310,175,186] ) = 0.985259"]
kumarkrishna,['WLogger \n Minimalistic command-line progress tracker. \n Wlogger  helps logging your progress as you work. Minimalistic interface \nfor procastinators who spend too much time planning. Add to it a reminder\nextension, and just the productivity manager you deserve, and probably\nneed as well. Let those  planned  projects finally see the light of day!! \n Installation \n pip install wlogger \n Usage \n Use of flags \n sh\n$ wlogger --section WLogger --add First Commit \nAdd tasks using  --add  across tabs maintained by  --section .\n sh\n$ wlogger --section Wlogger --remove First Commit \nRemove existing tasks from sections with  --remove .\n sh\n$ wlogger --section Wlogger --progress Second Commit \nTrack progress directly when not in ToDo List.\n Use of  --section  is optional, adding tasks  General  tab. \n Markdown support and Github Integration. \n sh\n$ wlogger --md \n ToDo List : \n \n [X]  wlogger  identifies adding, tracking tasks. \n [X]  wlogger  asks for configuration on first request. \n [X]  wlogger  stores the progrss with datetime stamp. \n [ ]  wlogger  allows reminders. \n [ ]  wlogger  uses natural language processing for recommendations. \n [X]  wlogger  supports markdown generation. \n [ ]  wlogger  supports tab-completion. \n [ ]  wlogger  integrated with Github. \n', 'Pixyll \n pixyll.com \n \n Pixyll is a simple, beautiful theme for Jekyll that emphasizes content rather than aesthetic fluff. It\'s mobile  first , fluidly responsive, and delightfully lightweight. \n It\'s pretty minimal, but leverages large type and drastic contrast to make a statement, on all devices. \n This Jekyll theme was crafted with <3 by  John Otander \n( @4lpine ). \n 中文版  https://github.com/ee0703/pixyll-zh-cn . \n Getting Started \n If you\'re completely new to Jekyll, I recommend checking out the documentation at  http://jekyllrb.com  or there\'s a tutorial by  Smashing Magazine . \n $ git clone git@github.com:johnotander/pixyll.git\n$ cd pixyll\n$ gem install bundler # If you don\'t have bundler installed\n$ bundle install \n Verify your Jekyll version \n It\'s important to also check your version of Jekyll since this project uses Native Sass which\nis  only supported by 2.0+ . \n Fork, then clone \n Fork the repo, and then clone it so you\'ve got the code locally. \n Modify the  _config.yml \n The  _config.yml  located in the root of the Pixyll directory contains all of the configuration details\nfor the Jekyll site. The defaults are: \n ```yml \n Site settings \n title: Pixyll\nemail: your_email@example.com\nauthor: John Otander\ndescription: "A simple, beautiful theme for Jekyll that emphasizes content rather than aesthetic fluff."\nbaseurl: ""\nurl: "http://pixyll.com" \n Build settings \n markdown: kramdown\npermalink: pretty\npaginate: 3\n``` \n Jekyll Serve \n Then, start the Jekyll Server. I always like to give the  --watch  option so it updates the generated HTML when I make changes. \n $ jekyll serve --watch \n Now you can navigate to  localhost:4000  in your browser to see the site. \n Using Github Pages \n You can host your Jekyll site for free with Github Pages.  Click here  for more information. \n A configuration tweak if you\'re using a gh-pages sub-folder \n In addition to your github-username.github.io repo that maps to the root url, you can serve up sites by using a gh-pages branch for other repos so they\'re available at github-username.github.io/repo-name. \n This will require you to modify the  _config.yml  like so: \n ```yml \n Site settings \n title: Repo Name\nemail: your_email@example.com\nauthor: John Otander\ndescription: "Repo description"\nbaseurl: "/repo-name"\nurl: "http://github-username.github.io" \n Build settings \n markdown: kramdown\npermalink: pretty\npaginate: 3\n``` \n This will ensure that the the correct relative path is constructed for your assets and posts. Also, in order to run the project locally, you will need to specify the blank string for the baseurl:  $ jekyll serve --baseurl \'\' . \n If you don\'t want the header to link back to the root url \n You will also need to tweak the header include  /{{ site.baseurl }} : \n ```html \n \n \n \n {{ site.title }} \n \n        {% include navigation.html %}\n       \n \n \n \n ``` \n A relevant Jekyll Github Issue:  https://github.com/jekyll/jekyll/issues/332 \n Contact Form \n The contact form uses  http://formspree.io . It will require you to fill the form out and submit it once, before going live, to confirm your email. \n More setup instructions and advanced options can be found at  http://formspree.io \n Disqus \n To configure Disqus, set up a  Disqus site  with the same name as your site. Then, in  _config.yml , edit the  disqus_shortname  value to enable Disqus. \n txtpen \n To configure txtpen, set up a  txtpen site  with the same name as your site. Then, in  _config.yml , edit the  txtpen_sitename  value to enable txtpen \n Customizing the CSS \n All variables can be found in the  _sass/_variables.scss  file, toggle these as you\'d like to change the look and feel of Pixyll. \n Page Animation \n If you would like to add a  fade-in-down effect , you can add  animated: true  to your  _config.yml . \n AnchorJS \n AnchorJS :  A JavaScript utility for adding deep anchor links to existing page content. AnchorJS is lightweight, accessible, and has no dependencies.  You can turn it on by toggling  enable_anchorjs . Because it offers many ways for customization, tweaks should be done in  _includes/footer.html . Default settings after turning AnchorJS on are: \n ```html \n \n    anchors.options.visible = \'always\';\n    anchors.add(\'article h2, article h3, article h4, article h5, article h6\');\n \n ``` \n See  documentation  for more options. \n Put in a Pixyll Plug \n If you want to give credit to the Pixyll theme with a link to  http://pixyll.com  or my personal website  http://johnotander.com  somewhere, that\'d be awesome. No worries if you don\'t. \n Web analytics and search engines \n You can measure visits to your website either by using  Google Analytics  tracking embed or the more advanced  Google Tag Manager  container.\n* For Google Analytics set up the value for  google_analytics , it should be something like  google_analytics: UA-XXXXXXXX-X .\n* For Google Tag Manager set up the value for  google_tag_manager , it should be something like:  google_tag_manager: GTM-XXXXX .\n*  Do not  set both of above methods because this will cause conflicts and skew your reporting data.\n* Remember that you need to properly configure the GTM container in its admin panel if you want it to work. More info is available in  GTM\'s docs . \n Your website is, by default, set to be allowed for crawling and indexing by search engines. (Unless you made yourself a custom robots.txt file). You can use front matter settings on each page to control how search engines will it. Sometimes you may want to exclude a particular page from indexing or forbid Google to store a copy of your page in its cache. It is up to you. Use the  meta_robots  frontmatter key and assign values based on  this table . Some examples: \n ```yaml \n exclude page from index \n meta_robots: noindex \n allow indexing, disallow caching \n meta_robots: noarchive \n allow indexing, disallow crawling links \n meta_robots: nofollow \n disallow indexing, follow links \n meta_robots: noindex,follow\n``` \n In order to get more information about your website\'s status in search engines, you can register it in  Google Search Console  and/or  Bing Webmaster Tools . Both these tools will ask you to authorize your website with them and there are couple of ways to do that. Pixyll supports verification via meta tags - just fill in values for  google_verification  and/or  bing_verification  in  _config.yml , the verification strings and meta tags will then be added automatically. \n If search engine optimization is your thing, you can also set up  meta_description  values for each page/post. By default Pixyll uses  summary  to populate the  <meta name="description" content="...">  tag and falls back to  description  from  _config.yml  if  summary  is not present in page/post\'s front matter. The  summary  is also used for generating Open Graph tags. Why would you want to use a dedicated variable for meta description? Because character limit to properly display this description in search results (as a snippet) is way smaller than in Open Graph. It is recommended to keep it at 155-160 characters, for more in-depth info read  this article . \n And lastly - if you happen to write in language other than English be sure to change  og_locale  in  _config.yml  to reflect it. \n Enjoy \n I hope you enjoy using Pixyll. If you encounter any issues, please feel free to let me know by creating an  issue . I\'d love to help. \n Upgrading Pixyll \n Pixyll is always being improved by its users, so sometimes one may need to upgrade. \n Ensure there\'s an upstream remote \n If  git remote -v  doesn\'t have an upstream listed, you can do the following to add it: \n git remote add upstream https://github.com/johnotander/pixyll.git \n Pull in the latest changes \n git pull upstream master \n There may be merge conflicts, so be sure to fix the files that git lists if they occur. That\'s it! \n Thanks to the following \n \n BASSCSS \n Jekyll \n Refills \n Solarized \n Animate.css \n \n Contributing \n \n Fork it \n Create your feature branch ( git checkout -b my-new-feature ) \n Commit your changes ( git commit -am \'Add some feature\' ) \n Push to the branch ( git push origin my-new-feature ) \n Create new Pull Request \n', 'paper-azzi \n Notes and review on interesting papers.', 'Paper-Spray \n This is a list of interesting research papers started by\n Kumar  and  Biswa  (currently being maintained only by Kumar),\nmainly in Machine Learning, but definitely not limited to it.\nThis is mainly an initiative to inculcate a reading habit among ourselves.\nSuggested reads are always welcome! \n We would try submit only links which are freely available, but we may also add\nfew links which can be accessed freely only from an university network. \n We have created a webpage for  Paper-Spray \ncontaining a searchable list of the papers in the json file. \n Entry format: \n \n \n Paper Title \n Date Added, Keywords \n Author, Conference, Year \n \n \n Abbreviations:\n* AI: Artificial Intelligence\n* CV : Computer Vision\n* DL: Deep Learning\n* ML : Machine Learning\n* NLP : Natural Language Processing\n* RL : Reinforcement Learning \n How it works? \n The papers are added to  paper-list.json . They can either be added\nmanually or by using the  add_papers.py  script. Thereafter the README is\ngenerated by using the  create_readme.py  script. This script appends the paper\nnames present in the json file to the contents of  readme.template ,\nto generate  README.md . \n Some scripts such as  add_papers.sh  and  add_papers_minimal.sh  have\nbeen created for convenience. \n The scripts give a warning when adding duplicate papers. In that case,\nenter \'n\' when asked to abort adding the paper. \n The webpage for paper-spray reads the json file and creates a table using js libraries.\nThere is no need of generating static html pages for any change in the json file. \n -\nCLI for adding papers :\n* Add ```$paperspraypath``` as environment variable for path to the github repository.\n```sh\nexport paperspraypath=/path/to/github/repository\n```\n* Add an alias to .bashrc / .bash_profile to directly add papers from any folder through terminal :D .\n```sh\nalias spray-papers="bash $paperspraypath/scripts/add_papers.sh"\n```\n * Use ```spray-papers``` as terminal command.\n \n Papers \n \n Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation \n 17/08/2017, RL, K-FAC \n Yuhuai Wu, Elman Mansimov, Shun Liao, Roger Grosse, Jimmy Ba, arXiv   [Review]   \n Function Optimization Using Connectionist Reinforcement Learning Algorithms \n 16/08/2017, RL \n Ronald J. Williams, Jing Peng, Connection Science   \n How to Escape Saddle Points Efficiently \n 16/08/2017, ML, Non-Convex \n Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, Michael I. Jordan, ICML 2017   [Review]   \n Emergence of Locomotion Behaviours in Rich Environments \n 16/07/2017, RL, Robotics, PPO \n Nicolas Heess, Dhruva TB, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa, Tom Erez, Ziyu Wang, S. M. Ali Eslami, Martin Riedmiller, David Silver, arXiv   \n Continuous control with deep reinforcement learning \n 16/07/2017, RL, DDPG \n Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra, ICLR 2016   \n Trust Region Policy Optimization \n 16/07/2017, RL, Robotics \n John Schulman, Sergey Levine, Philipp Moritz, Michael Jorda, Pieter Abbeel, ICML, 2015   \n The Reversible Residual Network: Backpropagation Without Storing Activations \n 14/06/2017, CV, RevNets \n Aidan N. Gomez, Mengye Ren, Raquel Urtasun, Roger B. Grosse, arXiv   \n Preconditioning Kernel Matrices \n 15/12/2016, kernel methods \n Kurt Cutajar, Michael Osborne, John Cunningham, Maurizio Filippone, ICML 2016   \n Image-to-Image Translation with Conditional Adversarial Networks \n 15/12/2016, CV, DL, GAN \n Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A.Efros, arxiv   \n Continous Control with Deep Reinforcement Learning \n 15/12/2016, DL, RL, DDPG \n Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra, ICLR 2016   \n Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space \n 04/12/2016, generative model, latent space \n Anh Nguyen, Jason Yosinski, Yoshua Bengio, Alexey Dosovitskiy, Jeff Clune, arXiv   \n Semantic Facial Expression Editing using Autoencoded Flow \n 04/12/2016, autoencoder, latent space, image manipulation \n Raymond Yeh, Ziwei Liu, Dan B Goldman, Aseem Agarwala, arXiv   \n Full-Capacity Unitary Recurrent Neural Networks \n 02/11/2016, DL \n Scott Wisdom, Thomas Powers, John R. Hershey, Jonathan Le Roux, Les Atlas, NIPS 2016   \n Conditional Image Synthesis With Auxiliary Classifier GANs \n 02/11/2016, CV, DL, GAN \n Augustus Odena, Christopher Olan, Jonatho Shlens, arxiv   \n Stochastic Variational Deep Kernel Learning \n 02/11/2016, DL, ML \n Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, Eric P. Xing, NIPS 2016   \n Multi-Scale Context Aggregation by Dilated Convolutions \n 01/11/2016, CV, DL \n Fisher Yu, Vladlen Klotun, ICLR 2016   \n Neural Machine Translation in Linear Time \n 01/11/2016, NMT, DL, dilated-convolutions \n Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, Koray Kavukcuoglu, arxiv   \n Recurrent Highway Networks \n 01/11/2016, DL, RNN \n Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutnik, Jurgen Schmidhuber, arxiv   \n Recurrent Switching Linear Dynamical Systems \n 31/10/2016, ML \n Scott Linderman, Andrew Miller, Ryan Adams, David Blei, Liam Paninski, Matthew Johnson, arxiv   \n Operator Variational Inference \n 31/10/2016, ML, variational \n Rajesh Ranganath, Jaan ALtosaar, Dustin Tran, David M. Blei, arxiv   \n Professor Forcing: A New Algorithm for Training Recurrent Networks \n 31/10/2016, DL, RNN \n Alex Lamb, Anirudh Goyal, Ying Zhang, Saizheng Zhang, Aaron Courville, Yoshua Bengio, NIPS 2016   \n Pointer Sentinel Mixture Models \n 30/10/2016, DL, NLP \n Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher, rxiv   \n Can Active Memory Replace Attention? \n 28/10/2016, DL \n Lukasz Kaiser, Samy Bengio, NIPS 2016   \n Analysis of Thompson Sampling for the Multi-armed Bandit Problem \n 25/10/2016, Sampling, Bandits \n Shipra Agrawal, Navin Goyal, JMLR 2012   \n Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models \n 25/10/2016, DL, RL \n Bradly Stadie, Sergey Levine, Pieter Abbeel, arxiv   \n Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks \n 22/10/2016, DL, optimization, bayesian \n José Miguel Hernández-Lobato, Ryan P. Adams, JMLR   \n Towards Deep Symbolic Reinforcement Learning \n 21/10/2016, DL, RL \n Marta Garnelo, Kai Arulkumaran, Murray Shanahan, arxiv   \n Layer Normalization \n 21/10/2016, DL, optimization \n Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton, arXiv   \n A Theory of Generative ConvNet \n 21/10/2016, ML, statistics, generative, cnn \n Jianwen Xie, Yang Lu, Song-Chun Zhu, Ying Nian Wu, ICML 2016   \n Modular Multitask Reinforcement Learning with Policy Sketches \n 06/10/2016, RL, policy sketch \n Jacob Andreas, Dan Klein, Sergey Levine, ICML 2017   \n A Tutorial on Energy-Based Learning \n 27/09/2016, ML, energy models \n Yann LeCun, Sumit Chopra, Raia Hadsell, Marc’Aurelio Ranzato, and Fu Jie Huang,   \n SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient \n 21/09/2016, GAN, sequence generation, policy gradient \n Lantao Yu, Weinan Zhang, Jun Wang, Yong Yu, arXiv   \n Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network \n 16/09/2016, CV, GAN, super resolution \n Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi, arXiv   \n Energy-based Generative Adversarial Network \n 16/09/2016, GAN, DL, generative model, energy function \n Junbo Zhao, Michael Mathieu, Yann LeCun, arXiv   \n Generating Videos with Scene Dynamics \n 15/09/2016, CV, DL, GAN \n Carl Vondrick, Hamed Pirsiavash, Antonio Torralba, NIPS 2016   \n Generative Visual Manipulation on the Natural Image Manifold \n 15/09/2016, CV, DL, GAN \n Jun-Yan Zhu, Philipp Krähenbühl, Eli Shechtman and Alexei A. Efros, ECCV 2016   \n Why does deep and cheap learning work so well? \n 10/09/2016, DL, ML, physics \n Henry W. Lin, Max Tegmark, arXiv   \n Reward Augmented Maximum Likelihood for Neural Structured Prediction \n 01/09/2016, DL, RL, MLE \n Mohammad Norouzi, Samy Bengio, Zhifeng Chen, Navdeep Jaitly, Mike Schuster, Yonghui Wu, Dale Schuurmans, NIPS 2016   \n Densely Connected Convolutional Networks \n 28/08/2016, DL, CNN \n Gao Huang, Zhuang Liu, Kilian Q. Weinberger, arXiv   \n Mollifying Networks \n 18/08/2016, ML, optimization \n Caglar Gulcehre, Marcin Moczulski, Francesco Visin, Yoshua Bengio, arXiv   \n Optimization Methods for Large-Scale Machine Learning \n 18/08/2016, ML, optimization \n Léon Bottou, Frank E. Curtis, Jorge Nocedal, arXiv   \n Deep FisherNet for Object Classification \n 02/08/2016, CV, DL, object classification \n Peng Tang, Xinggang Wang, Baoguang Shi, Xiang Bai, Wenyu Liu, Zhuowen Tu, arXiv   \n Attention-over-Attention Neural Networks for Reading Comprehension \n 26/07/2016, DL, NLP, Attention memory \n Yiming Cui, Zhipeng Chen, Si Wei, arXiv   \n BinaryConnect : Training Deep Neural Networks with binary weights during propagations \n 21/07/2016, DL, binary-connect \n Matthieu Courbariaux, Yoshua Bengio, Jean-Pierre David, NIPS 2015   \n Stochastic backpropagation and approximate inference in deep generative models \n 20/07/2016, generative-models \n Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra, ICML 2014   \n Markov Chain Monte Carlo and Variational Inference: Bridging the Gap \n 20/07/2016, MCMC, VAE \n Tim Salimans, Diederik P. Kingma, Max Welling, ICML 2015   \n Efficient approaches for escaping higher order saddle points in non-convex optimization \n 19/07/2016, ML, non-convex-optimization \n Anima Anandkumar, Rong Ge, COLT 2016   \n Gated-Attention Readers for Text Comprehension \n 19/07/2016, DL, NLP \n Bhuwan Dhingra, Hanxiao Liu, William W. Cohen, Ruslan Salakhutdinov, arXiv   \n Tensor decompositions for learning latent variable models \n 19/07/2016, ML, TF \n Anima Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, Matus Telgarsky, JMLR 2014   \n Pixel Recurrent Neural Networks \n 19/07/2016, DL, RNN \n Aaron van den Oord, Nal Kalchbrenner, Koray Kavukcuoglu, ICML 2016   \n NICE: Non-linear Independent Components Estimation \n 18/07/2016, DL \n Laurent Dinh, David Krueger, Yoshua Bengio, ICLR 2015   \n Higher Order Statistical Decorrelation without Information Loss \n 18/07/2016, DL, IT \n Gustavo Deco, Wilfried Brauer, NIPS 1995   \n Conditional Generative Aversarial Nets \n 14/07/2016, GAN, DL \n Mehdi Mirza, Simon Osindero, NIPS DL Workshop, 2014   \n Neural Generative Question Answering \n 14/07/2016, DL, NLP, QA \n Jun Yin, Xin Jiang, Zhengdong Lu, Lifeng Shang, Hang Li, Xiaoming Li, ICLR 2016   \n A Decomposable Attention Model for Natural Language Inference \n 09/07/2016, DL, NLP \n Ankur P. Parikh, Oscar Tackstrom, Dipanjan Das, Jakob Uszkoreit, arXiv   \n Sequence Level Training with Recurrent Neural Networks \n 06/07/2016, DL, RNN \n Marc\'Aurelio Ranzato, Sumit Chopra, Michael Auli, Wojciech Zaremba, ICLR 2016   \n Memorability of Image Regions \n 04/07/2016, CV, DL \n Aditya Khosla, Jianxiong Xiao, Antonio Torralba, Aude Oliva, NIPS 2012   \n Character-Aware Neural Language Models \n 04/07/2016, DL, NLP \n Yoon Kim, Yacine Jernite, David Sontag, Alexander M. Rush, AAAI 2016   \n Learning Language Games through Interaction \n 03/07/2016, DL, NLP \n Sida Wang, Percy Liang, Chris Manning, ACL 2016   \n Object Detectors emerge in Deep Scene CNNs \n 02/07/2016, CV, DL \n Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba, ICLR 2015   \n An Infinite Restricted Boltzmann Machine \n 01/07/2016, ML, RBM \n Marc-Alexandre Cote, Hugo Larochelle, Neural Computation   \n Learning to See by Moving \n 30/06/2016, CV, DL \n Pulkit Agrawal, Joao Carreira, Jitendra Malik, ICCV 2015   \n Distinguishing cause from effect using observational data: methods and benchmarks \n 29/06/2016, ML, cause-inference \n Joris M. Mooij, Jonas Peters, Dominik Janzing, Jakob Zscheischler, Bernhard Scholkopf, JMLR 2016   \n Neural Variational Inference for Text Processing \n 29/06/2016, DL, NLP \n Yishu Miao, Lei Yu, Phil Blunsom, arXiv   \n Learning to Transduce with Unbounded Memory \n 27/06/2016, DL, NTM, neural data structures \n Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, Phil Blunsom, NIPS 2015   \n Mean Shift, Mode Seeking, and Clustering \n 26/06/2016, ML, Clustering \n Yizong Cheng, IEEE, 1995   \n Adaptive Online Gradient Descent \n 25/06/2016, optimization, gradient descent \n Peter L. Bartlett, Elad Hazan, Alexander Rakhlin, NIPS 2007   \n Visual Genome \n 24/06/2016, vision, nlp multimodal dataset \n Ranjay Krishna et. al., Dataset   \n Learning Visual Predictive Models of Physics for Playing Billiards \n 23/06/2016, CV, DL \n Katerina Fragkiadaki, Pulkit Agrawal, Sergey Levine, Jitendra Malik, ICLR 2016   \n Tutorial on Variational Autoencoders \n 22/06/2016, DL, VAE \n Carl Doersch, arXiv   \n Towards Conceptual Compression \n 22/06/2016, DL \n Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, Daan Wierstra, arXiv   \n Dynamic Memory Networks for Visual and Textual Question Answering \n 22/06/2016, CV, DL, NLP, MemNets \n Caiming Xiong, Stephen Merity, Richard Socher, ICML 2016   \n Delving Deeper into Convolutional Networks for Learning Video Representations \n 22/06/2016, CV, DL , videos \n Nicolas Ballas, Li Yao, Chris Pal, Aaron Courville, ICLR 2016   \n Describing Videos by Exploiting Temporal Structure \n 22/06/2016, CV, DL, video \n Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal, Hugo Larochelle, Aaron Courville, ICCV 2015   \n Generative Adversarial Imitation Learning \n 21/06/2016, DL, generative \n Jonathan Ho, Stefano Ermon, arXiv   \n f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization \n 21/06/2016, DL, GAN, f-GAN \n Sebastian Nowozin, Botond Cseke, Ryota Tomioka, arXiv   \n Variational Inference with Normalizing Flows \n 21/06/2016, DL, VAE, inference \n Danilo Jimenez Rezende, Shakir Mohamed, ICML 2015   \n A Recurrent Latent Variable Model for Sequential Data \n 20/06/2016, DL, VRNN \n Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron Courville, Yoshua Bengio, NIPS 2015   \n Semi-Supervised Learning with Deep Generative Models \n 20/06/2016, DL, generative \n Diederik P. Kingma, Danilo J. Rezende, Shakir Mohamed, Max Welling, NIPS 2014   \n Human-level control through deep reinforcement learning \n 19/06/2016, RL, AI, DL \n Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Nature   \n Variational Dropout and the Local Reparameterization Trick \n 19/06/2016, DL, dropout \n Diederik P. Kingma, Tim Salimans, Max Welling, NIPS 2015   \n Ask Your Neurons: A Neural-based Approach to Answering Questions about Images \n 19/06/2016, CV, DL, NLP \n Mateusz Malinowski, Marcus Rohrbach, Mario Fritz, ICCV 2015   \n Generating Images from Captions with Attention \n 19/06/2016, CV, DL \n Elman Mansimov, Emilio Parisotto, Jimmy Lei Ba, Ruslan Salakhutdinov, ICLR 2016   \n LSTM: A Search Space Odyssey \n 19/06/2016, DL, NLP \n Klaus Greff, Rupesh Kumar Srivastava, Jan Koutnik, Bas R. Steunebrink, Jurgen Schmidhuber, arXiv   \n InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets \n 18/06/2016, DL, InfoGAN \n Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, Pieter Abbeel, arXiv   \n Improved Techniques for Training GANs \n 18/06/2016, DL, GAN \n Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, arXiv   \n  Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units \n 17/06/2016, DL, CV \n Wenling Shang, Kihyuk Sohn, Diogo Almeida, and Honglak Lee, ICML 2016   \n Fast dropout training \n 17/06/2016, DL, dropout \n Sida I. Wang, Christopher D. Manning, ICML 2013   \n Stating the Obvious: Extracting Visual Common Sense Knowledge \n 15/06/2016, DL, NLP \n Mark Yatskar, Vicente Ordonez, Ali Farhadi, NAACL 2016   \n Learning to Communicate with Deep Multi-Agent Reinforcement Learning \n 15/06/2016, DL, RL \n Jakob N. Foerster, Yannis M. Assael, Nando de Freitas, Shimon Whiteson, arXiv   \n Safely Interruptible Agents \n 15/06/2016, AI, RL, safety \n Laurent Orseau, Stuart Armstrong, UAI 2016   \n Deep Spatial Autoencoders for Visuomotor Learning \n 15/06/2016, CV, DL, RL, robotics \n Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, Pieter Abbeeel, ICRA 2016   \n Multi-Bias Non-linear Activation in Deep Neural Networks \n 15/06/2016, ML, DL, activation function \n Hongyang Li, Wanli Ouyang, Xiaogang Wang, ICML 2016   \n Learning Simple Algorithms from Examples \n 15/06/2016, ML, DL, AI \n Wojciech Zaremba, Tomas Mikolov, Armand Joulin, Rob Fergus, ICML 2016   \n Extracting and Composing Robust Features with Denoising Autoencoders \n 15/06/2016, DL, DAE \n Pascal Vincent, Hugo Larochelle, Yoshua Bengio, Pierre-Antoine Manzagol, ICML 2008   \n Sentence Similarity Learning by Lexical Decomposition and Composition \n 14/06/2016, DL, NLP \n Zhiguo Wang, Haitao Mi, Abraham Ittycheriah, arXiv   \n Learning visual groups from co-occurrences in space and time \n 14/06/2016, CV, DL \n Phillip Isola, Daniel Zoran, Dilip Krishnan, Edward H. Adelson, ICLR 2016   \n Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning \n 14/06/2016, CV, DL \n William Lotter, Gabriel Kreiman, David Cox, arxiv   \n Matching Networks for One Shot Learning \n 14/06/2016, DL, one-shot \n Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, Daan Wierstra, arxiv   \n Deep Reinforcement Learning in Large Discrete Action Spaces \n 14/06/2016, DL, RL \n Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sunehag, Timothy Lillicrap, Jonathan Hunt, Timothy Mann, Theophane Weber, Thomas Degris, Ben Coppin, arxiv   \n Composing graphical models with neural networks for structured representations and fast inference \n 14/06/2016, DL, graphical-models \n Matthew J. Johnson, David Duvenaud, Alexander B. Wiltschko, Sandeep R. Datta, Ryan P. Adams, arXiv   \n Skip-Thought Vectors \n 13/06/2016, DL, NLP \n Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard Zemel, Antonio Torralba, Raquel Urtasun, Sanja Fidler, NIPS 2015   \n Visually Indicated Sounds \n 12/06/2016, CV, DL \n Andrew Owens, Philip Isola, Josh McDermott, Antonio Torralba, Edward Adelson, William Freeman, CVPR 2016   \n DRAW: A Recurrent Neural Network for Image Generation \n 11/06/2016, CV, DL, DRAW \n Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, Daan Wierstra, JMLR 2015   \n Dynamic Capacity Networks \n 10/06/2016, DL \n Amjad Almahairi, Nicolas Ballas, Tim Cooijmans, Yin Zheng, Hugo Larochelle, Aaron Courville, JMLR, 2016   \n Denoising Autoencoder with Modulated Lateral Connections learns Invariant Representations of Natural Images \n 10/06/2016, CV, DL, ladder-networks \n Antii Rasmus, Tapani Raiko, Harri Valpola, ICLR 2015   \n Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks \n 10/06/2016, CV, DL, DCGAN \n Alec Radford, Luke Metz, Soumith Chintala, ICLR 2016   \n Improving sentence compression by learning to predict gaze \n 09/06/2016, DL, NLP \n Sigrid Klerke, Yoav Goldberg, Anders Sogaard, NAACL 2016, Best Short Paper   \n Match-SRNN: Modeling the Recursive Matching Structure with Spatial RNN \n 08/06/2016, DL, NLP \n Shengxian Wan, Yanyan Lan, Jun Xu, Jiafeng Guo, IJCAI 2016   \n Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks \n 08/06/2016, DL, CV, GAN, LAPGAN \n Emily Denton, Soumith Chintala, Arthur Szlam, Rob Fergus, NIPS 2015   \n Neural Module Networks \n 08/06/2016, DL, CV, visual QA \n Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein, arXiv   \n A Neural Probabilistic Language Model \n 07/06/2016, DL, NLP \n Yoshua Bengio, Rejean Ducharme, Pascal Vincent, Christian Jauvin, JMLR 2003   \n Adversarially Learned Inference \n 07/06/2016, ML, DL, inference, generative model \n Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mastropietro, Aaron Courville, Subt. NIPS 2016   \n Learning to Optimize \n 06/06/2016, DL, optimization \n Ke Li, Jitendar Malik, arxiv   \n Auto-Encoding Variational Bayes \n 06/06/2016, VAE \n Diederik P Kingma, Max Welling, ICLR 2014   \n Language Understanding for Text-based Games Using Deep Reinforcement Learning \n 06/06/2016, DL, NLP, RL \n Karthik Narasimhan, Tejas Kulkarni, Regina Barzilay, EMNLP 2015   \n Retrofitting Word Vectors to Semantic Lexicons \n 06/06/2016, NLP, word vectors \n Manaal Faruqui, Jesse Dodge, Sujay K. Jauhar, Chris Dyer, Eduard Hovy, Noah A. Smith, NAACL 2015   \n Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics \n 06/06/2016, ML, non-parametric estimation \n Michael U. Gutmann, Aapo Hyvarinen, JMLR 2012   \n Training Products of Experts by Minimizing Contrastive Divergence \n 06/06/2016, ML, contrastive divergence \n Geoffrey E. Hinton, Neural Computation 2002   \n Neural Word Embedding as Implicit Matrix Factorization \n 06/06/2016, DL, NLP, word vectors \n Omer Levy, Yoav Goldberg, NIPS 2014   \n Context Encoders: Feature Learning by Inpainting \n 05/06/2016, CV, DL, context-encoder \n Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, Alexei AEfros, CVPR 2016   \n Long Short-term Memory \n 04/06/2016, DL, RNN, LSTM \n Sepp Hochreiter, Jurgen Schmidhuber, Neural Computation, 1997   \n Maxout Networks \n 03/06/2016, DL, dropout, maxout \n Ian Goodfellow, David Farley, Mehdi Mirza, Aaron Courville, Yoshua Bengio, JMLR 2013   \n A Clockwork RNN \n 02/06/2016, DL, RNN, clock-work \n Jan Koutnik, Klaus Greff, Faustino Gomez, Jurgen Schmidhuber, JMLR 2014   \n Long Short-Term Memory-Networks for Machine Reading \n 02/06/2016, DL, NLP, machine understanding \n Jianpeng Cheng, Li Dong, Mirella Lapata,   \n Effective Approaches to Attention-based Neural Machine Translation \n 02/06/2016, DL, NLP, neural machine translation \n Minh-Thang Luong, Hieu Pham, Christopher D. Manning, EMNLP 2015   \n A Neural Attention Model for Sentence Summarization \n 02/06/2016, DL, NLP, summarization \n Alexander M. Rush, Sumit Chopra, Jason Weston, EMNLP 2015   \n To See or not to See : The need for attention to perrceive changes in scenes \n 01/06/2016, attention, vision \n Ronald Rensink, Kevin O\'Regan, James Clark, Psychological Science, 1997   \n Teaching Machines to Read and Comprehend \n 01/06/2016, DL, NLP, attention \n Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom, NIPS 2015   \n Recurrent Models of Visual Attention \n 01/06/2016, CV, DL, RL, attention \n Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu, NIPS 2014   \n Learning to compose neural networks for question answering \n 01/06/2016, DL, compose-NN, RL, QA \n Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein, NAACL 2016 (best-paper)   \n Asynchronous Methods for Deep Reinforcement Learning \n 01/06/2016, DL, RL \n Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, 2016   \n Density estimation using Real NVP \n 01/06/2016, DL, latent space, image generation \n Laurent Dinh, Jascha Sohl-Dickstein, Samy Bengio, Google Brain   \n Sparse Filtering \n 01/06/2016, ML, DL, sparse \n Jiquan Ngiam, Pang Wei Koh, Zhenghao Chen, Sonia Bhaskar, Andrew Y. Ng, NIPS 2011   \n Reinforcement Learning Neural Turing Machines \n 31/05/2016, DL, NTM, RL \n Wojciech Zaremba, ICLR 2016   \n Neural Networks with Few Multiplications \n 31/05/2016, DL, optimization \n Zhouhan Lin, Matthieu Courbariaux, Roland Memisevic, Yoshua Bengio, ICLR 2016   \n Recurrent neural network based language model \n 30/05/2016, DL, NLP, RNN, language-model \n Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan Cernocky, Sanjeev Khudanpur, INTERSPEECH 2010   \n Ask Me Anything: Dynamic Memory Networks for Natural Language Processing \n 30/05/2016, DL, DMN, NLP, dynamic-memory-networks \n Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, Richard Socher, ICML 2016   \n Actor-Mimic : Deep Multitask and Transfer Reinforcement Learning \n 30/05/2016, DL, RL, actor-mimic \n Emilio Parisotto, Jimmy Ba, Ruslan Salakhutdinov, ICLR 2016   \n Neural Programmer-Interpreters \n 29/05/2016, DL, NPI \n Scott Reed, Nando de Freitas, ICLR 2016   \n End-to-End Training of Deep Visuomotor Policies \n 29/05/2016, CV, DL, RL, robotics, control \n Sergey Levine, Chelsea Finn, Trevor Darrell, Pieter Abbeel, JMLR 2016   \n MovieQA : Understanding Stories in Movies through Question-Answering \n 28/05/2016, CV, DL, QA, movie-story \n Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, Sanja Fidler, CVPR 2016   \n Order-Embeddings of Images and Language \n 28/05/2016, CV, DL, image-caption, hierarchy \n Ivan Vendrov, Ryan Kiros, Sanja Fidler, Raquel Urtasun, ICLR 2016   \n Action Recognition using Visual Attention \n 28/05/2016, CV, DL, action-recognition, attention \n Shikhar Sharma, Ryan Kiros, Ruslan Salakhutdinov, ICLR 2016   \n End-To-End Memory Networks \n 28/05/2016, DL, memory-networks, end-to-end \n Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus, NIPS 2015   \n Dueling Network Architectures for Deep Reinforcement Learning \n 27/05/2016, DL, dueling-networks, RL \n Ziyu Whang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, Nando de Freitas, ICML 2016   \n Memory Networks \n 27/05/2016, DL, memory-networks \n Jason Weston, Sumit Chopra, Antoine Bordes, ICLR 2015   \n Playing Atari with Deep Reinforcement Learning \n 27/05/2016, DL, DQN, RL \n Volodymyr Mnih et al, NIPS DL workshop 2013   \n Deep Networks with Stochastic Depth \n 27/05/2016, DL, stochastic-depth \n Gao Huang, Yu Sun et al, 2016   \n An Introduction to Variational Methods for Graphical Models \n 26/05/2016, graphical-models, ML, variational-methods \n Michael Jordan et al, Machine Learning 1999   \n Deep Visual-Semantic Alignments for Generating Image Descriptions \n 26/05/2016, CV, DL, image-captioning, NLP \n Andrej Karpathy, Li Fei-Fei, CVPR 2015   \n VQA : Visual Question Answering \n 26/05/2016, CV, DL, QA \n Aishwarya Agarwal et al, ICCV 2015   \n Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks \n 26/05/2016, DL, RNN, scheduled-sampling \n Samy Bengio et al, NIPS 2015   \n Batch Normalization : Accelerating Deep Network Training by Reducing Covariate Shift \n 26/05/2016, batch-norm, DL \n Sergey Ioffe, Christian Szegedy, JMLR 2015   \n Show, Attend and Tell: Neural Image Caption Generation with Visual Attention \n 25/05/2016, CV, DL, attention, caption \n Kelvin Xu et al, JMLR 2015   \n Pointer Networks \n 24/05/2016, DL, Pointer-Nets \n Oriol Vinyals, Meire Fortunato, Navdeep Jaitly, NIPS 2015   \n Order Matters : Sequence to Sequence for sets \n 24/05/2016, DL, seq2seq, ordered, sorting \n Oriol Vinyals, Samy Bengio, Manjunath Kudlur, ICLR 2016   \n Neural Machine Translation by Jointly Learning to Align and Translate \n 23/05/2016, DL, NMT \n Bahdanau, Cho, Bengio, ICLR 2015   \n Visualizing Data using t-SNE \n 23/05/2016, ML, Embeddings \n Maaten, Hinton,, JMLR 2008   \n Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation \n 22/05/2016, DL, NLP, RNN-ED \n Kyunghyun Cho et al, ACL 2014   \n Deep Residual Learning for Image Recognition \n 22/05/2016, CV, DL, ResNets \n Kaiming He et al, 2015   \n Neural Turing Machines \n 22/05/2016, DL, NTM \n Alex Graves et al, 2014   \n Support Vector Machine Learning for Interdependent and Structured Output Spaces \n 21/05/2016, ML, StructSVM \n Ioannis Tsochantaridis et al, ICML 2004   \n Generative Adversarial Networks \n 21/05/2016, DL, GAN, generative \n Ian Goodfellow et al, NIPS 2014   \n Sequence to Sequence learning with neural networks \n 21/05/2016, DL, Seq2Seq \n Ilya Sutskever, Oriol Vinyals, and Quoc Le, NIPS 2014   \n Adam : A Method for Stochastic Optimization \n 20/05/2016, ML, Optimization, ADAM \n Diederik Kingma, Jimmy Ba, ICLR 2015   \n Neural GPUs Learn Algorithms \n 20/05/2016, DL \n Lukasz Kaiser, Ilya Sutskever, ICLR 2016   \n Generating Sequences With Recurrent Neural Networks \n 19/05/2016, DL \n Alex Graves, 2014   \n Generative Adversarial Text to Image Synthesis \n 19/05/2016, CV, DL \n Scott Reed et al, ICML 2016   \n Learning word embeddings efficiently with noise-contrastive estimation \n 18/05/2016, DL, NLP \n Andriy Mnih et al, NIPS 2013   \n Generating Sentences from a Continuous Space \n 18/05/2016, DL, NLP \n Samuel Bowman et al, 2015   \n Reinforcement Learning: A Survey \n 18/05/2016, AI, ML, RL \n Leslie Kaebling et al, JAIR 1996   \n Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning \n 18/05/2016, AI, ML, REINFORCE \n Ronald Williams, Machine Learning 1992   \n Training Neural Networks Without Gradients: A Scalable ADMM Approach \n 17/05/2016, DL \n Gavin Taylor et al, ICML 2016   \n k-means++: The Advantages of Careful Seeding \n 17/05/2016, Clustering, ML \n David Arthur et al, SODA 2007   \n Bridging the Gaps Betweeen Residual Learning, Recurrent Neural Networks and Visual Cortex \n 13/04/2016, CV, DL, cortex \n Quanli Liao, Tomas Poggio, arxiv   \n Eye movements in natural behaviour \n 01/06/2015, eye-movement, attention \n Mary Hayhoe, Dana Ballard, Trends in Cognitive Sciences, 2005   \n Optimizing Neural Networks with Kronecker-factored Approximate Curvature \n 19/03/2015, K-FAC \n James Martens, Roger Grosse, ICML 2015   \n Randomized Nonlinear Component Analysis \n 13/05/2014, RNCA, ML \n David Lopez-Paz, Suvrit Sra, Alex Smola, Zoubin Grahramani, Bernhard Scholkopf, ICML, 2014   \n On Information and Sufficiency \n 01/03/1951, classics, KL-divergence, information theory \n S. Kullback and R. A. Leibler, The Annals of Mathematical Statistics   [Review]   \n', 'Pycon-VQA-Theano \n PyCon Workshop on training VQA models with Theano/Keras \n Workshop outline : \n \n Recap : Basic Neural Networks with Theano \n Under the hood : Theano optimizations \n The VQA Problem \n Compile and train end-to-end networks  \n Observations and results with VQA \n Experiments and Visualizations on pre-trained model \n Introduction to Keras \n Training own models on MNIST for character recognition \n \n Prerequisites : \n While most of the topics are introduced in the talk from scratch, familiarity with the following would definitely help : \n \n Introductory Machine Learning \n Symbolic Algebra ( focus on Symbolic Differentiation ) \n Introductory Calculus, Linear Algebra \n Containers in Python \n Convolution Neural Networks ~ Recurrent Neural Networks \n \n For hands-on  : \n \n Installed  Theano ,  Keras ,  iPython \n \n Resources : \n Workshop Resources \n \n Outline   \n Worksop slides and iPython notebooks will be available  here \n \n Neural Networks  : \n \n Neural Networks in Machine Learning \n \n Theano  : \n \n Official Theano documentation \n Getting started with Theano \n Theano : A Python framework for fast computation of mathematical expressions \n \n Deep Learning References  : \n \n VQA  : A walkthrough blog by Avi Singh \n Recurrent Neural Networks  : Effectiveness of RNNs by Andrej Karpathy \n Convolution Neural Networks  : Introductory blog by Christopher Olah  \n', 'Datasets for Computer Vision \n This is a curated list of datasets and benchmarks for computer-vision tasks, (maintainted in reverse chronological order). \nSuggestions and pull requests are welcome. The goal is to make this a collaborative effort to maintain an updated list of quality datasets and benchmarks. For NLP tasks, check  nlp-datasets \n Areas \n \n Visual Question Answering   \n Face Recognition \n Odometry, Stereo \n \n Visual Question Answering \n \n (VQA)  VQA: Visual Question Answering, 2015  [paper]   [data] \n \n Face Recognition \n \n (MS-Celeb-1M)  MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition, 2016  [paper]   [data] \n \n Odometry \n \n (KITTI)  Vision meets Robotics: The KITTI Dataset, 2013  [paper]   [data] \n', "Awesome Neuroscience \n A curated list of awesome learning resources, research papers for neuroscience, with connections to learning (biological and artificial). Inspired by  awesome-computer-vision  and  awesome-biology . \n For contributing to this list (please do!),  send a pull request ,  open an issue  or contact through  email . While contributing, do consider adding a short description on how you found the resource  awesome  and personally read/benefitted. \n Table of Contents \n \n Papers \n Books \n \n Papers \n \n \n To see or not to see : The need for attention to perceive changes in scenes   [paper]   [blog-post] \n Ronald A. Rensink, J. Kevin O'Regan and James J. Clark   \n \n \n Receptive fields of single neurones in the cat's striate cortex \n [paper] \n DH Hubel and TN Wiesel, 1959   \n \n \n Receptive fields, binocular interaction and functional architecture in the cat's visual cortex \n [paper] \n DH Hubel and TN Wiesel, 1962   \n \n \n Receptive fields of cells in striate cortex of very young, visually inexperienced kittens \n [paper] \n DH Hubel and TN Wiesel, 1963 \n \n \n Receptive fields and functional architecture of monkey striate cortex \n [paper] \n DH Hubel and TN Wiesel, 1968   \n \n \n Books \n \n Principles of Neural Science by Kandel and Schwartz   [book] \n Theoretical Neuroscience by Dayan and Abbott   [book] \n Vision : A Computational Investigation into the Human Representation and Processing of Visual Information by David Marr   [book] \n", 'Today I Learnt \n A compilation of interesting things I learnt/discovered today.', 'theory-with-papers \n 2018-07 \n \n The Mirage of Action-Dependent Baselines in Reinforcement Learning [ paper ] [ notes ] \n \n 2018-06 \n \n Gradient Estimation Using Stochastic Computation Graphs [ paper ] [ notes ] \n Backpropagation through the Void: Optimizing control variates for black-box gradient estimation [ paper ] [ notes ] \n']
carwyn,['datatools \n Random Collection of Data Manipulation Tools', 'trello-converter \n Converts Trello JSON Export To Other Things', 'How to Run The Veillance Data Capture Simulator \n Install and Test Go \n \n Install Go from https://golang.org/ \n Set the GOPATH environment variable to $HOME/go or $HOME (up to you). \n Test Go using  go version  from the command line. \n \n Make Sure libpcap In Installed and pcap.h is in your INCLUDE path. \n On Linux this is in the libpcap/libpcap-devel packages.\nOn OS X I\'m not sure where this is but should be installed if XCode is. \n Fetch and Build Interceptor \n \n Use the command "go get" to fetch everything including dependencies: \n go get github.com/carwyn/veillance/interceptor \n This will pull down all the dependencies into the $GOPATH/src tree. \n Build by running the following in the veillance/interceptor directory: \n go build \n Run the interceptor from the veillance/interceptor directory: \n sudo ./interceptor -i eth0 "port 80 or port 53" \n NOTE: Change the network interface passed to  -i  to match your system. \n Test the interceptor via the test web page: \n http://localhost:8080/test.html \n This will print a live stream to the Javascript console. \n To stop the interceptor use the key combination  Ctrl+\\ \n \n Connecting to the Websocket Server Programatically \n \n Open a websocket to http://localhost:8080/entry \n Messages are sent to client as UTF-8 encoded JSON strings. \n Format is as described below. \n \n JSON Payload \n \n WARNING: This element will evolve over time! \n \n Fragment JSON:  {"Id":123,"Name":"Bob","Type":"HTTP","Text":"Hello World!"} \n Id: Fragment ID of se\nName: Identity of originating data source.\nType: Type of oringinating data source.\nText: The captured text. \n Selection JSON:  {"Fid":31,"Words":[1,3]} \n Fid: Fragment ID.\nWords: Selected words. \n Domain JSON:  {"Id":321,"Name":"Bill","Type":"DNS","Text":"twitter.com"} \n Id: Seen domain identifier.\nName: Identity of originating data source.\nType: Type of oringinating data source.\nText: The captured text. \n Example Veillance Messages \n ```\nid: 207\nevent: fragment\ndata: {"Id":16,"Name":"SimUser","Type":"HTTP","Text":"Well the barrel broke my shoulder as on to the ground it sped"} \n id: 206\nevent: selection\ndata: {"Fid":31,"Words":[1,3]} \n id: 208\nevent: domain\ndata: {"Id":12,"Name":"SimUser","Type":"DNS","Text":"twitter.com"}\n```', "Learning Python \n A shortlist of resources for learning Python. \n Computer Science  links moved. \n Links From Workshop \n \n BBC Bitesize - What is an algorithm? \n Richard Feynman Computer Heuristics Lecture \n Pancakes, Served With a Side of Science \n \n Python \n \n The Official Python Website \n Python Wiki \n Beginners Guide \n Programmers \n Non-Programmers \n \n \n \n Tutorials \n \n Official Python Tutorial \n http://introtopython.org/ \n A Byte of Python \n http://www.learnpython.org/ \n Google's Python Class \n \n In Depth Tutorials \n \n Learning to Program  (Alan Gauld) \n How to Think Like a Computer Scientist  ( Interactive Version  by Runestone) \n Alice in Python Projectland \n \n Books \n \n There are LOTS of Books \n Many Free Books \n \n Runestone Interactive Books \n These guys get a special mention for their excellent collection of interactive Computer Science and Python books. \n \n Runestone Interactive \n Full Runestone Library  (select titles below) \n How to Think Like a Computer Scientist: Interactive Edition \n Problem Solving with Algorithms and Data Structures using Python \n CS for All \n CS160 Course \n CS Principles: Big Ideas in Programming \n \n \n \n Online Books \n \n Learn Python the Hard Way  (Zed Shaw) 4th edition first draft (Python3) \n Automate the Boring Stuff with Python \n Think Python: How to Think Like a Computer Scientist \n \n Children's Books \n \n Codio Cyfrifiadur  /  Computer Coding for Kids \n \n Standout Resources \n \n The Hitchhiker’s Guide to Python  (Kenneth Reitz) \n Python Packaging User Guide \n Awesome Python \n Python 3 Module of the Week \n Full Stack Python \n Python Pedia \n \n Project and Dependency Management \n \n https://python-poetry.org/ \n https://dephell.org/ \n \n Practice Material \n \n https://www.practicepython.org/ \n \n Online Environments \n \n Try Python \n https://trinket.io/ \n https://repl.it/ \n https://www.pythonanywhere.com/ \n \n Editors and IDEs \n There are  many ! \n Learning Environments \n \n https://codewith.mu/ excellent for beginners. \n http://thonny.org/ a very impressive learning environment. \n \n Mobile Python \n Great for learning on the go. \n \n Pythonista 3  for iOS \n Pydroid 3 \n \n Advanced Environments \n \n https://atom.io/ with  python-tools  and  autocomplete-python \n Visual Studio Code  with  Python Extension \n PyCharm  and  PyCharm Edu \n https://ipython.org/ and https://jupyter.org/ \n \n Gamification \n \n https://checkio.org/ \n https://lenovogamestate.com/ \n \n Communities and Conferences \n \n http://stackoverflow.com/ \n http://pyconuk.org/ \n \n Language Popularity (Pinch of Salt Required) \n \n Tiobe Index \n PYPL PopularitY of Programming Language \n \n Miscellaneous \n \n https://hackr.io/tutorials/learn-python \n https://www.quora.com/What-are-the-best-resources-for-learning-Python \n http://www.whoishostingthis.com/resources/python/ \n https://python.zeef.com/alan.richmond \n \n Other Things \n Robots \n \n http://www.sphero.com/education \n https://www.lego.com/en-us/boost \n \n Visual Programming \n \n https://en.wikipedia.org/wiki/Visual_programming_language \n https://scratch.mit.edu/ and https://www.scratchjr.org/ \n http://snap.berkeley.edu/ \n https://csprinciples.cs.washington.edu/sevenbigideas.html \n https://www.gethopscotch.com/ \n", 'Official Micro:bit \n Official website http://microbit.org/ \n Javascript Blocks Editor: https://makecode.microbit.org/ \n Python Editor: http://python.microbit.org/ \n Technical Details: http://tech.microbit.org/ \n Microsoft Makecode \n https://makecode.com/ \n MicroPython \n http://micropython.org/ \n Mu Editor: https://codewith.mu/ \n Micro:bit Runtime \n http://lancaster-university.github.io/microbit-docs \n Arm MBED \n \n https://developer.mbed.org/platforms/Microbit/ \n https://developer.mbed.org/teams/microbit/ \n https://developer.mbed.org/teams/Lancaster-University/ \n \n Reset Original Program \n https://support.microbit.org/support/solutions/articles/19000021613-first-experience-demo-program \n Update DAPLink Firmware \n https://www.mbed.com/en/platform/hardware/prototyping-production/daplink/daplink-on-kl26z/ \n Micellaneous \n \n https://microworldtour.github.io/ \n https://fedoramagazine.org/learn-programming-using-micro-bit-fedora/ \n', 'MessTest \n This is a little project started to collect examples of bizarre UNC and NTFS\nfilesystem behaviour. It was started after finding a file with the following\nname on a UNC share: \n @GMT-2000.01.10-00.00.00.txt \n This file could not be deleted or renamed via the UNC path returning File Not\nFound errors. Neither can it be copied using robocopy or renamed using\nWindows Explorer via the UNC path. It can however be manipulated via the local\ndrive letter path. \n I\'ve opened a Microsoft Answers Question on this:  Bizarre Disallowed Filenames on UNC Shares . \n Setup For Testing \n To run the script in this repository or otherwise test the issue with other\ntools simply make the same directory available via both drive letter path and\nUNC (SMB) path, for example: \n | Drive Path | UNC Path |\n| -- | -- |\n|  C:\\Users\\bob\\test  |  \\\\localhost\\C$\\Users\\bob\\test  | \n The script is designed to be run from the drive path and works out the\ncorrect UNC path. Make sure the user the script runs as has access rights\nto create directories and files via both paths. \n To manually test this without using the script simply try writing some\ntext to a file via the two paths: \n ```\n"testing" | Out-File -FilePath C:\\Users\\bob\\test\\@GMT-2000.01.10-00.00.00.txt \n "testing" | Out-File -FilePath \\localhost\\C$\\Users\\bob\\test\\@GMT-2000.01.10-00.00.00.txt\n``` \n Discoveries So Far \n Upon further investigation I found other examples that produce errors or\nunexpected behaviour upon attempted file creation via UNC paths: \n | Attempted | Result | Note |\n| -- | -- | -- |\n|  @GMT-1000.00.00-00.00.00.txt  |  .txt  | Just extension. |\n|  @GMT-2000.00.00-00.00.00.txt  |  .txt  | Just extension. |\n|  @GMT-1000.01.10-00.00.00.txt  |  .txt  | Just extension. |\n|  @GMT-2000.01.10-00.00.00.txt  | File Not Found | Huh? |\n|  @GMT-1000.00.00-00.00.00  | Access Denied | What? |\n|  @EST-1000.00.00-00.00.00.txt  |  @EST-1000.00.00-00.00.00.txt  | Works |\n|  @EST-1000.01.10-00.00.00.txt  |  @EST-1000.01.10-00.00.00.txt  | Works |\n|  XGMT-1000.00.00-00.00.00.txt  |  XGMT-1000.00.00-00.00.00.txt  | Works |\n|  @GMT-1000.00-00.00.00.00.txt  |  @GMT-1000.00-00.00.00.00.txt  | Flipped hyphen. | \n A few months before I\'d seen  PENTESTER’S WINDOWS NTFS TRICKS COLLECTION \nwhich made me think it might be an alternate data streams issue but upon\nlooking back this doesn\'t match any of those cases. The filenames being\ndate based also seems too coincidental. But why then is the File Not Found\nexample different? \n Further Observations \n \n It seems that this issue also effects Samba shares when the file is created from\nPowerShell and other Windows tools. \n It\'s possible to create at least some of the filenames using the "New File"\nmenu in Windows Explorer, but then Windows Explorer can\'t then do anything with them. \n FUSE and libsmbclient are able to deal with the files on Windows share with no issue. \n', "Artificially Intelligent Art Workshop \n Neural Style Transfer \n The  painting  of the North Wales Tech logo presented here has been created by applying  A Neural Algorithm of Artistic Style  as presented by Leon A. Gatys, Alexander S. Ecker, Matthias Bethge in their 2015 paper available here: https://arxiv.org/abs/1508.06576 \n \n In this paper the authors present an Artificial Intelligence system based on a Deep Neural Network that creates artistic images by separating and recombining neural representations of content and style from arbitrary images. The authors note the following observation of interest: \n \n In fine art, especially painting, humans have mastered the skill to\ncreate unique visual experiences through composing a complex interplay\nbetween the content and style of an image. Thus far the algorithmic\nbasis of this process is unknown and there exists no artificial system\nwith similar capabilities. \n \n This technique is these days more commonly known as  Neural Style Transfer . \n Creating the Style Transfered NWT Logo \n Style Input: The Scream by Edvard Munch in 1893 \n Norwegian: Skrik\nGerman: Der Schrei der Natur (The Scream of Nature) \n \n \n I was walking along the road with two friends – the sun was setting –\nsuddenly the sky turned blood red – I paused, feeling exhausted, and leaned\non the fence – there was blood and tongues of fire above the blue-black\nfjord and the city – my friends walked on, and I stood there trembling with\nanxiety – and I sensed an infinite scream passing through nature. \n -- Edvard Munch \n \n Many commentators have suggested the painting represents the  the anxiety of modern man . \n Input Image: The North Wales Tech Logo \n \n Neural Networks \n https://playground.tensorflow.org/ \n Hands on Workshops \n Beginners \n https://tenso.rs/demos/fast-neural-style/ \n Intermediate \n There are many implementations of Neural Style Transfer out there, with an entire  Reddit thread  dedicated to discussing which is best. A selection of the most commonly mentioned is listed below. The PyTorch based one was used to render the North Wales Tech logo. \n Lua Torch based version:  \n \n https://github.com/jcjohnson/neural-style \n \n Python based PyTorch Versions:  \n \n https://github.com/ProGamerGov/neural-style-pt \n \n Python based TensorFlow versions: \n \n https://github.com/anishathalye/neural-style \n https://github.com/cysmith/neural-style-tf \n https://github.com/lengstrom/fast-style-transfer \n \n Advanced \n For those of you that would like to learn more about what's going on and to learm more about the Machine Learning frameworks themselves see the following links. \n \n Official PyTorch Neural Style Transfer Tutorial \n Porting Arbitrary Style Transfer to the Browser \n \n AI and Machine Learning in the Cloud \n Amazon Web Services \n Overview: https://aws.amazon.com/machine-learning/ \n Pre-built machine images: \n https://aws.amazon.com/machine-learning/amis/ \n Google Cloud \n Overview: https://cloud.google.com/products/ai/ \n Pre-built machine images: \n https://console.cloud.google.com/marketplace/details/click-to-deploy-images/deeplearning \n Microsoft Azure \n Overview: https://azure.microsoft.com/en-us/overview/ai-platform/ \n Machine Learning Studio: \n https://azure.microsoft.com/en-gb/services/machine-learning-studio/ \n Pre-built machine images: \n https://azure.microsoft.com/en-us/services/virtual-machines/data-science-virtual-machines/ \n Other bits: \n https://notebooks.azure.com/pytorch \n https://azure.microsoft.com/en-gb/blog/world-class-pytorch-support-on-azure/\nhttps://azure.microsoft.com/en-gb/blog/microsoft-extends-ai-support-to-pytorch-1-0-deep-learning-framework/ \n Other Links \n http://karpathy.github.io/neuralnets/ \n http://cs231n.github.io/ \n https://www.youtube.com/watch?reload=9&v=i94OvYb6noo \n https://arxiv.org/abs/1812.08434 \n https://www.quora.com/How-do-artificial-neural-networks-work \n https://github.com/chuanli11/CNNMRF\nhttps://deepai.org/machine-learning-model/CNNMRF \n https://www.reddit.com/r/MachineLearning/comments/8o846n/d_what_is_the_best_implementation_of_neural_style/ \n https://medium.com/tensorflow/neural-style-transfer-creating-art-with-deep-learning-using-tf-keras-and-eager-execution-7d541ac31398 \n https://www.pyimagesearch.com/2018/08/27/neural-style-transfer-with-opencv/ \n https://medium.com/artists-and-machine-intelligence/neural-artistic-style-transfer-a-comprehensive-look-f54d8649c199 \n Other AI Art Work \n https://ai.google/research/teams/brain/magenta/", 'building-systems \n A collection of articles about building systems.']
jgc128,['Serelex \n Name \n Serelex - a tool for semantic relation extraction. \n Synopsis \n serelex [ options ] \n Description \n A tool for semantic relation extraction. The program finds pairs of semantically related words based on the text definitions coming from the Wikipedia articles (other texts may be also used). The extraction method implemented in this system is based on three similarity measures (cosine, gloss overlap, and Karaulov\'s measure) between texts and two nearest-neighbor algorithms (KNN and Mutual KNN). The tool is a cross-platform console application. \n Licensed under  LGPLv3 . \n Options \n -c   file \nConcepts file, default concepts.csv. A text file containing a set of input words (one word per line). The program will try to find semantic relations between these input words. For instance, if words \'crocodile, alligator, house, and building\' were given as input the program will try to return pairs \'crocodile,alligator\' and \'house,building\' among all possible combinations. \n -d   file \nDefinitions file, default definitions.csv. A text file containing a set of definitions for words specified in the concepts.csv file. If this file contains definitions for more words that given in concepts.csv the program will skip the words which do not appear in the concepts.csv. The sample-data directory contains files with "definitions" derived from the introduction of Wikipedia articles. Other texts may be used as definitions (traditional dictionary glosses, WordNet, etc.) if provided in the same format. \n -s   file \nStop-words file, default stoplist.csv. A text file containing stop words (one word per line). Words from this list will not be used by a similarity measure. All occurrences of these words in definitons.csv will be removed. \n -o   file \nOutput file, default result.csv. A text file containing set of found semantic relations between words specified in the file concepts.csv. Each line of this file contains a pair of semantically related words, according to the specified method. \n -S   o|c|k \nSimilarity method, default o. Semantic similarity measure used for relation extraction. \n o - Gloss overlap measure equal to number of common words in the definitions of two words. \n c - Cosine between bag-of-word vectors build from definitions of respective of two words \n k - Karaulov\'s semantic similarity measure.  \n -M   1|2 \nComponent analysis method, default knn. An algorithm used to derive semantic relations from pairwise similarity scores between the words. \n 1 - knn. Standard nearest-neighbor algorithm (KNN). Here K most similar words are related to a target word. \n 2 - mknn. Mutual nearest-neighbor algorithm (MKNN). Here K mutually most similar words are related to a target word. \n -K   NUM \nNumber of nearest-neighbors, default K = 2. \n -T   T1 T2 T3 \nT1, T2, T3 - parameters of Karaulov\'s semantic similarity measure, default T1 = 2, T1 = 1, T3 = 6. \n Files and Catalogs \n bin  - contains binary excutable file: \n \n serelex_win32.exe - excutable file for 32-bit Windows \n serelex_win64.exe - excutable file for 64-bit Windows \n serelex_i686 - executable file for i686 Debian-based systems \n serelex_amd64 - executable file for amd64 Debian-based systems \n \n docs  - provides documentation in html format \n sample-data  - contains sample source data: \n \n concepts.csv - a text file containing a set of input words (one word per line) \n definitions.csv - a text file containing a set of definitions for words specified in the concepts.csv file \n stoplist.csv - a text file containing stop words (one word per line) \n \n src  - the source code \n windows  - provides project to Microsoft Visual Studio 2010 \n test.sh  and  test.bat   - run analysis for algorithm KNN, MutalKNN, with overlap and cosinus method and K = 1, 2, 5, 10 \n Build \n To build under Windows, use MS Visual Studio 2010 project in  windows  folder. To select configuration - x32 or x64 - use select list on tools panel. \nTo build under *nix use command  make . To build on amd64 use command  make ARCH=-m64', 'kScreen \n A small utility for capturing screenshots and instant upload to hosting klan-hub.ru \n Attention! This project is under construction!', 'Lexico-Semantic Search Engine \n Name \n Serelex - a lexico-semantic search engine.  \n This system is a kind of "lexico-semantic search engine". Given a text query it provides a list of related words.\nA traditional search engine provides as a results a list of related documents. The current version is based on two \nsemantic similarity measures -- Serelex and PatternSim. The first relies on definitions of words, while the second \nrelies on a text corpus. \n How to install \n \n Install Node.JS (Ubuntu -- https://github.com/joyent/node/wiki/Installing-Node.js-via-package-manager). \n Install MongoDB (Ubuntu -- http://docs.mongodb.org/manual/tutorial/install-mongodb-on-debian-or-ubuntu-linux/). \n Clone this repository (git clone ...). \n Go to the directory with lsse and type "npm install" to install all Node.JS dependencies of the system. \n Use mongorestore tool to restore databases: "mongorestore backup", where "backup" is a folder with directory serelex2, containing files system.indexes.bson, words.bson and relations.bson. Downloadable here -- http://cental.fltr.ucl.ac.be/team/~panchenko/data/serelex/mongodb.tgz.\nIf indexes were not generated automatically, please do  \n $mongo \n \n \n use serelex2 \n \n \n \n \n db.words.ensureIndex({word: 1}) \n \n \n \n \n db.words.ensureIndex({id: 1}) \n \n \n \n \n db.relations.ensureIndex({word: 1, model: 1}) \n \n \n \n \n db.lemms.ensureIndex({lemma: 1}, {unique:true}) \n \n \n \n \n db.lemms.ensureIndex({forms: 1}) \n \n \n Use PORT environment variable to set port (e.g. "export PORT=8080" for Linux, "set PORT=8080" for Windows). By default -- 80. \n Start the application: "node app". \n \n Additional:  \n \n Use "node import_v2" to import all CSV files with semantic relations, described in data_models.js to MongoDB. \n Use "node generate_access_log [count] [file name]" to generate access log for JMeter with random data. \n \n API \n \n execute request GET /find/<model>/<word> to obtain results \n for instance  http://serelex.cental.be/find/norm60-corpus-all/ubuntu \n the result is a This request should return a set of words related to \'ubuntu\' in JSON format \n', 'Serelex4Win \n Serelex4Win - client for the lexical-semantic search engine [LSSE] (https://github.com/PomanoB/lsse/). \nLSSE now running on [serelex.it-claim.ru] (http://serelex.it-claim.ru) \n Description \n This program is a simple and beautiful view shows the search results for a given query.\nSupports easily navigate to the next search item by simply clicking on it, and through a stack of transitions, you can always go back to the results of any previous searches! \n Screenshot \n Facebook\n![Facebook] (http://klan-hub.ru/host_new/images/50c3944b8eca4.png)   \n Snapped mode - Clinton\n![Clinton] (http://klan-hub.ru/host_new/images/50c3944b7e5aa.png)   \n Apple\n![Apple] (http://klan-hub.ru/host_new/images/50c3944bcc580.png)   \n Stanford\n![Stanford] (http://klan-hub.ru/host_new/images/50c3944bab59f.png)   \n Installation \n You can use Windows Store to install this application - [Serelex in Windows Store] (http://apps.microsoft.com/webpdp/app/lsse/48dc239a-e116-4234-87fd-ac90f030d72c)\nAnother variant - manual installation latest version from packages from folder Serelex/Serelex/AppPackages/Serelex_1.0.0.0_AnyCPU_Test. To install app, right-click on .ps1-file and enter Y on request. \n Building \n To build the project requires Windows 8 and Visual Studio 2012 Professional or higher (Visual Studio 2012 Express for Windows 8 does not fit, as in this project uses Portable Class Library) \n License \n This project is released under a license [LGPLv3] (http://www.gnu.org/licenses/lgpl.html)', 'Serelex \n The project was moved to  https://github.com/jgc128/DefVectors', 'This app allow you to watch battery status of your Windows Phone on tablet/personal computer with Windows 8', 'Advanced Line Follower Robot \n Please see issues section for details about current progress.', 'KONICA MINOLTA Pathological Image Segmentation Challenge \n http://crowdsourcing.topcoder.com/KonicaMinoltaChallenge', 'PyTorch helpers \n Models \n \n TODO \n \n Utils \n \n TODO \n', 'Common Utilities \n Zzz', "MedNLI - Natural Language Inference in Clinical Texts \n Information \n This repository contains the code to fully reproduce experiments in the paper. \nAs such, it has quite a few dependencies and not trivial to install.\nIf you want just a simple ready-to-use baseline with pre-trained models,\nplease have a look at our baselines repository:\nhttps://github.com/jgc128/mednli_baseline \n Installation \n \n Clone this repo:  git clone ... \n Install NumPy:  pip install numpy==1.13.3 \n Install PyTorch v0.2.0:  pip install http://download.pytorch.org/whl/cu80/torch-0.2.0.post3-cp36-cp36m-manylinux1_x86_64.whl  (see https://github.com/pytorch/pytorch#installation for details) \n Install requirements:  pip install -r requirements.txt \n Install MetaMap: https://metamap.nlm.nih.gov/Installation.shtml \n Make sure to set  METAMAP_BINARY_PATH  in the  config.py  to your MetaMap binary installation \n Install PyMetaMap: https://github.com/AnthonyMRios/pymetamap \n Install UMLS Metathesaurus: https://www.nlm.nih.gov/research/umls/ \n Make sure to set  UMLS_INSTALLATION_DIR  in the  config.py  pointing to your UMLS installation  \n \n Downloading the datasets \n \n Download SNLI: https://nlp.stanford.edu/projects/snli/ \n Download MultiNLI: http://www.nyu.edu/projects/bowman/multinli/ (we experimented with MultiNLI v0.9) \n Download MedNLI: https://jgc128.github.io/mednli/ \n \n Put all of the data inside the  ./data/  dir so is has the following structure:\n $ ls data/\nmednli_1.0  multinli_0.9  snli_1.0   \n $ ls data/snli_1.0/\nREADME.txt  snli_1.0_dev.jsonl  snli_1.0_dev.txt  snli_1.0_test.jsonl  snli_1.0_test.txt  snli_1.0_train.jsonl  snli_1.0_train.txt \n Downloading the word embeddings \n | Word Embedding  | Link |\n| ------------- | ------------- |\n|glove |   glove.840B.300d.pickled  |\n|mimic |   mimic.fastText.no_clean.300d.pickled  |\n|bio_asq |  bio_asq.no_clean.300d.pickled  |\n|wiki_en |  wiki_en.fastText.300d.pickled  |\n|wiki_en_mimic |   wiki_en_mimic.fastText.no_clean.300d.pickled  |\n|glove_bio_asq |   glove_bio_asq.no_clean.300d.pickled  |\n|glove_bio_asq_mimic | glove_bio_asq_mimic.no_clean.300d.pickled  | \n Put all embeddings inside the  ./data/word_embeddings/  dir so is has the following structure: \n $ ls data/word_embeddings/\nglove.840B.300d.pickled     glove_bio_asq_mimic.no_clean.300d.pickled   mimic.fastText.no_clean.300d.pickled \n Running the code \n Code tested on Python 3.4 and Python 3.6.3 \n \n Configuration:  config.py \n Preprocess the data:  python preprocess.py \n This script will create files  genre_*.pkl  in the  ./data/nli_processed/  directory \n Preprocess the test data:  python preprocess.py process_test \n Extract concepts:  python metamap_extract_concepts.py \n Make sure to run MetaMap servers first before executing this script  \n The script above works only for the MedNLI dataset. Rename the files  genre_*.pkl  to  genre_concepts_*.pkl  for SNLI and all MultiNLI domains. \n Call  main_data_test  as the main function to process the test data \n Create word embeddings cache:  python pickle_word_vectors.py <path_to_glove/word2vec file> ./data/word_embeddings/<name> \n See  WORD_VECTORS_FILENAME  in the  config.py  for file namings \n Create UMLS graph cache:  python parse_umls_create_concepts_graph.py \n Optional: to create input data for the  official retrofitting script  run  python create_retorfitting_data.py \n Train the model:  python train_model.py \n You can change the parameters in the  config  function or in the command line:  python train_model.py with use_umls_attention=True use_token_level_attention=True  (see the  Sacred documentation  for details) \n \n Using a pre-trained model \n \n Download model weights, and the the model-specific tokenizer and embeddings (see the table below). \n Put the model weights into the  ./data/saved_models/  dir. \n Put the tokenizer and the embeddings into the  ./data/  dir. \n Create an input file that contains premises and hypotheses, delimited by the  \\t  character (see  example ). \n Run the  predict.py  script and provide the input data in STDIN:  python predict.py < data/input.txt . The resulting probabilities of the  contradiction ,  neutral , and  entailment  classes correspondingly wll be printed to STDOUT. If you do not want to see the logging and wish to save the results to a file, redirect STDERR to /dev/null and STDOUT to a file:  python predict.py < data/test_input.txt 2>/dev/null > data/test_input_probabilities.txt \n \n You can configure the model weights, tokenizer, and the embeddings filename using the command line arguments:\n python train_model.py with model_class=PyTorchInferSentModel model_weights_filename=PyTorchInferSentModel_50_glove_bio_asq_mimic_clinical__.slysamwq.h5 tokenizer_filename=tokenizer_clinical_.pickled embeddings_filename=embeddings_clinical_.pickled   \n | Model description | Model files and parameters |\n|  ------------- |  ------------- | \n|InferSent model, trained on MedNLI only using the glove_bio_asq_mimic word vectors |  model_class:  PyTorchInferSentModel     model weights     tokenizer     embeddings  | \n More models coming soon! \n Configuration options \n ``python\nmodel_class = 'PyTorchInferSentModel' # class name of the model to run. See the create_model function for the available models\nmax_len = 50 # max sentence length\nlowercase = False # lowercase input data or nor\nclean = False # remove punctuation etc or not\nstem = False # do stemming to not\nword_vectors_type = 'glove'  # word vectors - see the WORD_VECTORS_FILENAME in config.py` for details\nword_vectors_replace_cui = ''  # filename with retorifitted embeddings for CUIs, eg cui.glove.cbow_most_common.CHD-PAR.SNOMEDCT_US.retrofitted.pkl\ndownsample_source = 0 # down sample the source domain data to the size of the MedNLI \n transfer learning settings \n genre_source = 'clinical' # source domain for transfer learning. target='' and tune='' - no transfer\ngenre_target = '' # target domain - always MedNLI in case of experiemnts in the paper\ngenre_tune = '' # fine-tuning domain\nlambda_multi_task = -1 # whether to use dynamically sampled batches from different domains or not.\nuniform_batches = True # a batch will contain samples from just one domain \n rnn_size = 300 # size of LSTM\nrnn_cell = 'LSTM' # LSTM is used in the experiments in the paper\nregularization = 0.000001 # regularization strength\ndropout = 0.5 # dropout\nhidden_size = 300 # size of the hidden fully-connected layers\ntrainable_embeddings = False # train embeddings or not \n knowledge-based attention \n set both to true to reproduce the token-level UMLS attention used in the paper \n use_umls_attention = False # whether to use the knowledge-based attention or not\nuse_token_level_attention = False # use CUIs or separate tokens for attention \n batch_size = 512 # batch size\nepochs = 40 # number of epochs for training\nlearning_rate = 0.001 # learning rate for the Adam optimizer\ntraining_loop_mode = 'best_loss'  # best_loss or best_acc - the model will be saved on the base loss or accuracy on the validation set correspondingly \n ``` \n Experiments in the paper \n Baselines \n To run the BOW, InferSent, and ESIM models with default settings, use the following commands accordingly: \n python train_model.py with model_class=PyTorchSimpleModel\npython train_model.py with model_class=PyTorchInferSentModel\npython train_model.py with model_class=PyTorchESIMModel \n Transfer learning \n To pre-train the model on the  Slate  domain, fine-tune on the MedNLI and test on the dev set of MedNLI (Sequential transfer in the paper), run the following command: \n python train_model.py with genre_source=slate genre_tune=clinical genre_target=clinical \n To run the Multi-target transfer learning, specify the genres and use the corresponding versions of the models:  PyTorchMultiTargetSimpleModel ,  PyTorchMultiTargetInferSentModel , and  PyTorchMultiTargetESIMModel . \n Word embeddings \n All word embeddings have to be pickled first - see the  pickle_word_embeddings.py  script.\nTo run the model with a specific embeddings, use the  word_vectors_type  parameter: \n python train_model.py with word_vectors_type=wiki_en_mimic \n Retorfitting \n \n First, create the input data for retrofitting with the  create_retrofitting_data.py  script.  \n Second, run the official script from GitHub. (https://github.com/mfaruqui/retrofitting). \n Next, pickle the resulting word vectors with the  pickle_word_vectors.py  script. \n Finally, set the  word_vectors_replace_cui  parameter to the pickled retrofitted vectors: \n python train_model.py with word_vectors_replace_cui=cui.glove.cbow_most_common.CHD-PAR.SNOMEDCT_US.retrofitted.pkl \n \n Knowledge-directed attention \n Set both  use_umls_attention  and  use_token_level_attention  to  True  to reproduce the token-level UMLS attention experiments: \n python train_model.py with use_umls_attention=True use_token_level_attention=True \n Reference \n The paper was accepted to EMNLP 2018! Meanwhile, here is an extended arXiv version: \n Romanov, A., & Shivade, C. (2018). Lessons from Natural Language Inference in the Clinical Domain. arXiv preprint arXiv:1808.06752. \nhttps://arxiv.org/abs/1808.06752 \n @article{romanov2018lessons,\n    title = {Lessons from Natural Language Inference in the Clinical Domain},\n    url = {http://arxiv.org/abs/1808.06752},\n    abstract = {State of the art models using deep neural networks have become very good in learning an accurate mapping from inputs to outputs. However, they still lack generalization capabilities in conditions that differ from the ones encountered during training. This is even more challenging in specialized, and knowledge intensive domains, where training data is limited. To address this gap, we introduce {MedNLI} - a dataset annotated by doctors, performing a natural language inference task ({NLI}), grounded in the medical history of patients. We present strategies to: 1) leverage transfer learning using datasets from the open domain, (e.g. {SNLI}) and 2) incorporate domain knowledge from external data and lexical sources (e.g. medical terminologies). Our results demonstrate performance gains using both strategies.},\n    journaltitle = {{arXiv}:1808.06752 [cs]},\n    author = {Romanov, Alexey and Shivade, Chaitanya},\n    urldate = {2018-08-27},\n    date = {2018-08-21},\n    eprinttype = {arxiv},\n    eprint = {1808.06752},\n}", 'MedNLI Baseline \n A simple baseline for Natural Language Inference in clinical domain using the MedNLI dataset.\nIncludes simplified CBOW and InferSent models from the corresponding paper.  \n Installation \n \n Clone this repo:  git clone https://github.com/jgc128/mednli_baseline.git \n Install NumPy:  pip install numpy==1.15.2 \n Install PyTorch v0.4.1:  pip install http://download.pytorch.org/whl/cu92/torch-0.4.1-cp36-cp36m-linux_x86_64.whl  (see https://pytorch.org/ for details) \n Install requirements:  pip install -r requirements.txt \n \n Downloading the dataset, word embeddings, and pre-trained models \n \n Create the  ./data  directory inside the cloned repository \n Create the  ./data/cache  directory  \n \n \n Download MedNLI: https://jgc128.github.io/mednli/ \n Extract the content of the  mednli_data.zip  archive into the  ./data/mednli  dir ( unzip -d data/mednli mednli_data.zip ) \n \n \n Download word embeddings (see the table below) and put the  *.pickled  files into the  ./data/word_embeddings/  dir ( wget -P data/word_embeddings/ https://mednli.blob.core.windows.net/shared/word_embeddings/https://mednli.blob.core.windows.net/shared/word_embeddings/mimic.fastText.no_clean.300d.pickled ) \n Download pre-trained models (see below) and put the  *.pkl  and the  *.pt  files into the  ./data/models/  dir \n \n Word embeddings \n | Word Embedding  | Link |\n| ------------- | ------------- |\n|glove |   glove.840B.300d.pickled  |\n|mimic |   mimic.fastText.no_clean.300d.pickled  |\n|bio_asq |  bio_asq.no_clean.300d.pickled  |\n|wiki_en |  wiki_en.fastText.300d.pickled  |\n|wiki_en_mimic |   wiki_en_mimic.fastText.no_clean.300d.pickled  |\n|glove_bio_asq |   glove_bio_asq.no_clean.300d.pickled  |\n|glove_bio_asq_mimic | glove_bio_asq_mimic.no_clean.300d.pickled  | \n Models \n | Model     | Embeddings          | MedNLI Dev accuracy | Files |\n|-----------|---------------------|---------------------|-------|\n| CBOW      | mimic               | 0.670               |  model spec  /  model weights  |\n| InferSent | glove               | 0.743               |  model spec  /  model weights  |\n| InferSent | mimic               | 0.783               |  model spec  /  model weights  |\n| InferSent | wiki_en             | 0.763               |  model spec  /  model weights  |\n| InferSent | wiki_en_mimic       | 0.774               |  model spec  /  model weights  |\n| InferSent | glove_bio_asq_mimic | 0.770               |  model spec  /  model weights  | \n Using a pre-training model \n Run the  predict.py  file with three arguments:\n1. Path to the model specification file ( *.pkl )\n1. Input file in the  jsonl  format (see  mli_dev_v1.jsonl ) or the  \\t -separated premise and hypothesis (see  test_input.txt ) \n1. Output file  .csv  to save predicted probabilities of each of the three classes (contradiction, entailment, and neutral) \n Notes:\n1. The model weights file ( *.pt ) should be located in the same dir as the model specification file ( *.pkl )\n1. In case of the  jsonl  format the sentences are taken from the  sentence1_binary_parse  and  sentence2_binary_parse  fields,\n where the  sentence1  is the premise and  sentence2  is the hypothesis. All other fields are optional \n Example command to run the prediction:\n python predict.py data/models/mednli.infersent.mimic.128.saek2t5q.pkl data/input_test.txt data/predictions_test.csv \n Training the model \n Run the  train.py  file. The options are set in the  config.py  file. Command-line interface is coming soon!\nBy default, the model specification and the model weights are saved in the  ./data/models  dir. \n Training the feature based system \n To run a traditional feature based system, run the  train_feature_based.py  file. \nThis system achieves 0.523 accuracy on the dev set using a gradient boosting classifier \nwith features based on word overlaps, tf-idf similarities, word embeddings similarities, and blue scores. \n Reference \n Romanov, A., & Shivade, C. (2018). Lessons from Natural Language Inference in the Clinical Domain. arXiv preprint arXiv:1808.06752. \nhttps://arxiv.org/abs/1808.06752 \n ```\n@article{romanov2018lessons,\n    title = {Lessons from Natural Language Inference in the Clinical Domain},\n    url = {http://arxiv.org/abs/1808.06752},\n    abstract = {State of the art models using deep neural networks have become very good in learning an accurate mapping from inputs to outputs. However, they still lack generalization capabilities in conditions that differ from the ones encountered during training. This is even more challenging in specialized, and knowledge intensive domains, where training data is limited. To address this gap, we introduce {MedNLI} - a dataset annotated by doctors, performing a natural language inference task ({NLI}), grounded in the medical history of patients. We present strategies to: 1) leverage transfer learning using datasets from the open domain, (e.g. {SNLI}) and 2) incorporate domain knowledge from external data and lexical sources (e.g. medical terminologies). Our results demonstrate performance gains using both strategies.},\n    journaltitle = {{arXiv}:1808.06752 [cs]},\n    author = {Romanov, Alexey and Shivade, Chaitanya},\n    urldate = {2018-08-27},\n    date = {2018-08-21},\n    eprinttype = {arxiv},\n    eprint = {1808.06752},\n}', 'CISS Materials \n Lectures:\n *  lectures/ \n Tutorials:\n *  tutorials/ \n Materials:\n *  Project Ideas', 'Common Utils v2', 'Misha Movie \n Face processing', 'Puzzles Playground']
shivenmian,['Bookmarks \n A list of interesting links I bookmarked on my Chrome at a time when I was more into Android Development. Most of them are on Android, and they are really random of sorts, but were pretty useful for me.', 'treasure-hunt \n A web application written in bare PHP to run online treasure hunts. Hacky code originally written for Chakravyuha (a hunt I co-organised) \n I am currently looking for improvements in the system so that the application can be used in further versions of Chakravyuha as well as for similar competitions.', "AskSusi \n A small Slack bot I wrote in around an hour. Uses the Susi API on which I'm currently working on for another project, and the RTM API for using with Slack. I'm also working on a Node version for the same. \n More about Susi: http://asksusi.com \n On your  virtualenv , install the requirements using  pip install -r requirements.txt . Set up a bot on Slack and add the token in the config file, and set the  botname  in  asksusi.py . \n Run the app using  python asksusi.py . \n TODO: Write this in Flask and deploy it.", 'IntelliCode \n IntelliCode is a slackbot I wrote in under an hour, which gives you instant coding answers! It returns answers taken up from StackOverflow (accepted answers / the most voted ones) for your queries.  \n Built this for Byldathon v10, IIIT Delhi (came 2nd overall). \n \n TODO: \n1. Integrate Flask into it, and add it to the Slack App Directory.\n2. Try doing sentiment analysis of replies to see if the answer (if not accepted) was satisfactory.', 'load-balancer-SA \n Sample boilerplate used:  Hackathon Starter', 'dshare \n DShare: Decentralised Platform for Storage Sharing']
warmlogic,['LabLackey readme \n Purpose \n LabLackey is an iOS-based framework for running psychology experiments. Currently it has one experimental paradigm implemented, which tests recognition memory. The framework was designed to accommodate multiple tasks, and there are plans to implement both other stimulus--response paradigms (e.g., the Stroop task) as well as experiments to collect behavioral measures in other ways (e.g., probe with a question every so often and log the response, such as, "Were you just daydreaming?"). Experiments will be designed with end-user configuration in mind. \n How to use LabLackey \n \n The app currently runs in Xcode\'s iOS simulator (a developer account is needed to load it onto an iOS device). \n It has been tested with Xcode 4.5.2 as an iPhone app under iOS 6. \n Once the repository is cloned to your local computer, you will see that the actual project itself is currently named  Exp . \n A JSON-formatted configuration file is needed to run an experiment. A basic default configuration file is located at  LabLackey/Exp/config.json . \n Upon running, the app expects to find this configuration file in the iTunes File Sharing "Documents" directory for this app ( ~/Library/Application Support/iPhone Simulator/6.0/Applications/[GUID]/Documents/config.json ). However, we\'re not actually using iTunes File Sharing yet (i.e., we\'re not running on an iOS device), and this directory won\'t exist until after the initial compilation. Thus, if it doesn\'t find  Documents/config.json  upon running, it will instead use the basic default configuration file. \n NB: JSON files do not seem to be able to contain comments. \n To run the app, open  Exp.proj  in Xcode and click the "Run" button at the top left. \n Then choose an experiment to participate in. \n Logged experiment data currently gets saved to a CSV file in  ~/Library/Application Support/iPhone Simulator/6.0/Applications/[GUID]/Documents/[Experiment name]/ \n \n Current features: \n \n Experiment configuration \n Initial support for having multiple types of experiments. \n \n Full support for reading experiment parameters from a JSON file. \n \n NB: The config file goes in the app\'s iTunes File Sharing  Documents  directory (see above). It must be named  config.json . See  LabLackey/Exp/config.json  for a basic default configuration. \n JSON-related resources: \n http://json.org \n http://developer.apple.com/library/ios/#documentation/Foundation/Reference/NSJSONSerialization_Class/Reference/Reference.html \n \n \n \n Stimuli \n \n Construct stimulus pool from images in a folder (hard coded) \n Parameters for study and test phase trials (set in  config.json ) \n Stimulus array randomization \n \n Sub-selection for study and test phases. \n \n The number of old/studied stimuli to show during test is hard coded at the time of compilation as the variable  _numberToTransferFromStudyToTest  in  EXExperiment.m . \n This is because I haven\'t completely decided how I want to handle this "transfer". \n \n \n \n Trial presentation \n \n \n Random jitter for time durations (fixation, ISI, and stimulus presentation) \n \n Uses  arc4random_uniform(upper_bound) \n e.g., http://stackoverflow.com/questions/160890/generating-random-numbers-in-objective-c \n \n \n \n Log data \n \n response \n reaction time \n write to a CSV file \n \n Features to implement: \n \n Experiment configuration \n Use  type   key:value  in  config.json  to configure specific types of experiments. \n \n Handle additional configuration files for when an experimenter wants to provide participants with a separate  config.json  file. \n \n \n Stimuli \n \n Create stimulus pool from words in a text file \n \n Stimulus metadata \n \n e.g., set particular presentation parameters, or know whether a stimulus is old or new (specific to recognition memory experiment) \n \n \n \n Log data \n \n Log other types of data \n subject information (need some kind of unique identifier) \n experiment parameters (maybe write the experiment array from  config.json  to its own file) \n stimulus presentations for study and test \n trial/event type (e.g.,  STUDY_PRESENTATION ,  TEST_RESPONSE ) \n \n \n \n Support for exporting the log file via email or uploading to a server \n \n \n GUI/Visual presentation \n \n Initial experiment description/introduction screen. \n This would have the "Back" button in case you didn\'t want to start that experiment. \n Then the "Back" button should be removed from any phase instruction screen. \n \n \n \n Other experimental paradigm ideas: \n \n Probe for data every now and then via local notifications \n http://developer.apple.com/library/mac/#documentation/NetworkingInternet/Conceptual/RemoteNotificationsPG/IPhoneOSClientImp/IPhoneOSClientImp.html \n \n Links: \n Project page: https://github.com/warmlogic/LabLackey', 'expertTrain README \n Purpose \n expertTrain is a visual category expertise training environment written in Matlab using Psychtoolbox. \n About \n \n The environment supports running experiments with multiple sessions, where each session is divided into phases. Configuration files for the experiments called  EBIRD ,  EBUG ,  SPACE , and  COMP  are included. \n There are four potential main phases for each session of  EBIRD  and  EBUG : \n Old/New recognition ( recog ): study a list of targets, tested on recognizing targets and lures. \n Subordinate matching ( match ): decide whether two stimuli are from the same species. \n Naming ( name ): must press corresponding species key, and the species number is not displayed on screen. \n Viewing ( view ): must press corresponding species key, displayed on screen with each stimulus.  This phase is not being used. \n There are two additional augmented introductory training phases (typically to be used on Training Day 1): \n Nametrain ( nametrain ): Just like the  name  phase, but species are introduced a one or two at a time (as defined in  config_EXPNAME.m ) and subjects have to name the species even if no exposure has occurred. The idea is that this will force subjects to learn the species labels quickly. \n Viewname ( viewname ): intermixed viewing and naming blocks (described above) for introducing the subject to different species.  This phase is not being used. \n Another phase is included that could be used for stimulus similarity normalization ( compare ). A configuration file for a separate experiment called  COMP  is included, though the comparison task could easily be implemented in a training experiment with the correct configuration setup. \n Another experiment with its own set of phases is called  SPACE . This is a spacing effect experiment. There are four phases: \n Exposure ( expo ): expose subject to stimuli and have them provide ratings. These stimuli will be shown in the  multistudy  phase. \n Paired associate study ( multistudy ): view paired associate stimuli (words and images). \n Math distractor ( distract_math ): solve math problems as a distractor task. \n Cued recall ( cued_recall ): cued recall for stimuli in the  multistudy  phase, with a typed response for word stimuli. \n expertTrain  has been developed and tested under Matlab 2013a and Psychtoolbox 3.0.11 (Flavor: beta) on Mac OS X 10.8.3, as well as 10.6.8 using Matlab 2012b. It has been used extensively on Windows XP with Matlab 2013a, and to a lesser extent on Debian 7. \n You must use a USB keyboard with this experiment. \n \n Installation \n \n Download and install Psychtoolbox (PTB) version 3 \n http://psychtoolbox.org/download/ \n Make sure to add it to your Matlab path \n Download  expertTrain  to a reasonable location on your computer (e.g.,  ~/Documents/experiments/ ) \n You can clone with the GitHub app or regular git in Terminal, or download the zip. \n https://github.com/warmlogic/expertTrain \n It is  not  recommended that you add it to your Matlab path \n Acquire a stimulus image set (e.g., creatures/sheinbugs or birds) \n Name all stimulus images using this pattern:  FamilySpeciesExemplar.extension \n e.g.,  ab1.bmp  (family a, species b, exemplar 1);  sc2.bmp  (family s, species c, exemplar 2) \n You can name them with single letters for family and species followed by exemplar number (as above), or: \n the experiment supports multiple character family names, which will is useful for particular paradigms (e.g., when stimulus images are manipulated). \n Family names can contain digits (e.g.,  fam1 ), but species names cannot contain digits. Exemplar numbers can only consist of digits (because any numbers in the  species+exemplarNumber  string will be read as part of the exemplar number). \n \n \n \n \n All species exemplar images should be stored flat in a single family directory, within  expertTrain/images/STIM_SET_NAME/FAMILY_NAME/ \n e.g.,  expertTrain/images/Creatures/a/  (for family "a" images, where all images in this folder start with "a") \n \n \n There is a creature set located on curran-lab: \n /Volumes/curranlab/ExperimentDesign/Experiment Stimuli/Creatures/sorted_in_selected_not_selected.zip \n NB: If you use this stimulus set and the provided config files (see "Preparing the experiment", below), you must rename the family 1 directory to "a" and the family 2 directory to "s". \n \n \n There is a bird set located on curran-lab:  /Volumes/curranlab/ExperimentDesign/Experiment Stimuli/Birds/Birds_matt/Final Bird Stimuli  (email me or  tclab@colorado.edu  if you need help) \n \n Preparing the experiment \n \n Set up a config function file for your experiment ( config_EXPNAME.m ), as well as any supporting functions or files. \n Supporting files: the config function runs the functions  et_saveStimList()  and  et_processStims() \n See  expertTrain/config_EBUG.m  for an example. \n Note how it runs  et_saveStimList()  and  et_processStims() \n Apologies for being such a long/extensive config file, but it is well organized. \n \n \n For the config structures in  config_EBUG.m , each entry in  expParam.sesTypes  is a separate session (e.g., different days of the experiment). The phases for each session are configured below that in the  expParam.session  field. The requirement is that  expParam.session  has a field for each  expParam.sesTypes  entry. \n For Net Station integration: \n Set up your Net Station acquisition template to have a Multi-Port ECI device between the Source device and the Recorder device, and connect the blue STIM tube through them all the way to the Display device. \n Connect the behavioral testing computer and the Net Station computer together with an ethernet cable. \n Find the IP address of the Net Station computer ( System Prefs > Network > Ethernet , or it\'s listed in the Multi-Port ECI panel in Net Station) and put the IP address in the top of the config file as the variable  expParam.NSHost  as a string. \n When you run the experiment, be sure to include the proper argument in the experiment command or popup window to use Net Station. (See "Running the experiment" below for more information.) \n When Net Station is open and the experiment runs, the experiment will automatically start and stop recording EEG. \n Less well described/organized features (see examples in  config_EBUG.m  for now): \n et_calcExpDuration()  is a function to determine how long your experiment will be. \n Instructions are read from external text files in  expertTrain/text/instructions/ . \n Press the  g  key (might need to hold it for a second) to end the impedance check to continue when there is a message to the experimenter, to dismiss the final screen, etc. \n There are practice modes for matching, naming, and recognition. Hopefully the provided config is clear enough on how to set them up. Use  expParam.runPractice=true;  to run the practice. \n Practice stimuli can either be chosen from a separate directory of images (in the  expertTrain/images/STIM_SET_NAME/FAMILY_NAME/  directory structure, as with experiment stimuli), or they can be randomly selected from the experimental families/species. Set  cfg.stim.useSeparatePracStims  to either  true  or  false . \n \n \n Image manipulation conditions are supported. Use different family names for each condition. Species orders can be yoked together across families if there is something common about conditions and exemplars. \n See  config_EBIRD.m  for an example of adding stimulus manipulation conditions. \n \n \n Impedance breaks (every X trials [phases: matching, name] or Y blocks [phases: recognition, nametrain, viewname]). \n Blink breaks (every X seconds) \n Test using a previous phase\'s stimuli in a current phase (see the example field  usePrevPhase  in  config_EBUG.m , e.g.,  usePrevPhase = {\'sesName\', \'phaseName\', phaseNum}; . \n Also use the field  reshuffleStims , which must be  true  or  false . \n \n \n Resize image stimuli using the field  cfg.stim.stimScale  in in  config_EBUG.m . Set equal to the proportion of image; e.g., 1.0 = full-size image. Instruction images can be scaled as well. \n There are multiple versions of the recognition and naming/viewing response key images (in  expertTrain/images/resources/ ). \n Net Station support (sending tags to NS) is fully implemented (http://docs.psychtoolbox.org/NetStation). Use the  NetStation.m  function. \n If your computer doesn\'t have much memory, you can choose to not preload all stimulus images by setting cfg.stim.preloadImages to  false  in the config file. \n You can do a photocell test to determine accuracy of timing of Psychtoolbox presenting stimuli and talking to Net Station. This is done with an argument in the initial  expertTrain  command. See  help expertTrain  for details. Email  tclab@colorado.edu  if you need help with this. \n \n Running the experiment \n \n In Matlab,  cd  into the  expertTrain  directory. \n Run the experiment:  expertTrain(\'EXPNAME\',subNum,useNS);  (where  \'EXPNAME\'  is a string,  subNum  is an integer, and  useNS  is 1 or 0 (for either using Net Station to record EEG or not)) \n e.g.,  expertTrain(\'EBUG\',1,1);  runs EBUG subject 1 (called EBUG001 in the data directory) and recording with Net Station. \n You can also run the experiment by just running the command  expertTrain; . You are then required to enter the experiment details in a dialogue box. \n NB: You must have  config_EXPNAME.m  set up ahead of time. See "Preparing the experiment" above. \n Run each successive session using the same command. The experiment will pick up at the next session. \n If you just want to try out different sessions or phases of the EBUG experiment without running through the entire thing, you can edit the top of  config_EBUG.m  so that one of the debug code chunks is uncommented. \n NB: Need to delete the subject folder every time you change  config_EBUG.m  in order to apply the changes. \n If you need to break out of the experiment while it\'s running, press  control-c  (might need to press it twice). \n IMPORTANT : If you break out of a session, the experiment has the capability of resuming where you left off. However, it is not advisable to break out unless it can\'t be avoided. There is a bug with the recognition portion where the response key image gets messed up. \n To get back to the Matlab command window: \n If you\'re on Mac OS X, type  control-c  again and enter the command  sca  (blindly if you have to) to clear any remaining PTB windows. \n If you\'re on Windows, first alt-tab to the Matlab application, then type  control-c  again and enter the command  sca  (blindly if you have to) to clear any remaining PTB windows. \n \n \n Debugging \n PTB seems bad at showing actual error messages, so using multiple monitors is a good way to debug. \n If you\'re running multiple monitors and you have turned on  dbstop if error , if the experiment encounters an error you can type  dbup  and then  ME  to see the error stack trace. \n A mat file with this same error information gets saved to the session directory in case you have not turned on  dbstop if error . Load it and examine the  ME  variable to find your bug. \n To get back to the Matlab command window, (Windows users: first alt-tab to Matlab) type  control-c  again and enter the command  sca  (blindly if you have to) to clear any remaining PTB windows. \n Resuming \n The experiment can be resumed from (approximately) where it left off if it crashes. This happens automatically. \n \n Important notes \n \n I think this has been resolved (meaning it is no longer an issue), but you\'re better safe than sorry. If you\'re running on Windows XP, it seems that you should not allow participants to push other keys along with the response key. \n For example, do not let participants rest their hand(s) on the Control key, as the double key press may crash the experiment. This is probably too extreme, but you may want to physically remove the Control key and other modifier keys (e.g., Alt and Windows keys) from the participant keyboard. \n \n Convenient functions \n Windows run-experiment batch file \n \n On Windows (only tested on XP), you can make a batch file for easily running the experiment in Matlab from, e.g., the desktop. \n Make a file in Notepad called  RunExpertTrain.bat  with this inside (but modify the path as appropriate for your setup): \n matlab -sd "C:\\Documents and Settings\\curranlab\\My Documents\\My Experiments\\expertTrain" -r "expertTrain" \n \n \n Save it in the  expertTrain  directory. \n Create a shortcut, move to somewhere convenient (e.g., the desktop), double-click to run. \n \n Windows backup-data rsync function \n \n It is easy to use rsync on a Mac to backup local data to a remote server. However, this is not the case on Windows. Here\'s how to do it (only tested on XP): \n Install  cwRsync  (and maybe  Cygwin ??) \n http://www.rsync.net/resources/howto/windows_rsync.html (second link down, not Windows Backup Agent) \n \n \n Make a file in Notepad called  BackupEXPNAME.cmd  with this inside, but change EXPNAME to your experiment name (e.g., EBUG or EBIRD) and modify the paths below as appropriate: \n SET CWRSYNCHOME=%PROGRAMFILES%\\CWRSYNC\n  SET CYGWIN=nontsec\n  SET CWOLDPATH=%PATH%\n  SET PATH=%CWRSYNCHOME%\\BIN\n  rsync -avzP --include="EBIRD* " --exclude=" " --perms --update --max-delete=0 --verbose \'/cygdrive/c/Documents and Settings/curranlab/My Documents/My Experiments/expertTrain/data/\' /cygdrive/z/Data/EBIRD/Behavioral/Sessions/\n  cd c:\\WINDOWS\\system32\n  attrib -h /s z:\\Data\\EBIRD\\Behavioral\\Sessions \\ * . *\n   \n \n \n Note that the last line (starts with "attrib") should end with: "Sessions" followed by a backslash and then asterisk-dot-asterisk, with  no spaces between these items . \n Replace "EBIRD" with your experiment name. \n Save it in  c:\\Program Files\\cwRsync \n Create a shortcut, move to somewhere convenient (e.g., the desktop), double-click to run. \n \n Known Issues \n \n Resuming a partially run recognition phase ( et_recognition.m ) may cause a squashed version of the stimulus image to be presented where the response key image should be. I have no idea why this happens. \n \n TODO \n \n Initial Eyelink eye tracking support: http://docs.psychtoolbox.org/EyelinkToolbox \n \n Links \n \n Project page: https://github.com/warmlogic/expertTrain \n Psychtoolbox: http://psychtoolbox.org/download/ \n My page: http://psych.colorado.edu/~mollison/ \n', "Update Nov 12, 2013 : You can now escape the evil (Carbon-based) clutches of Net Station and do everything within Matlab/FieldTrip, including but not limited to: filtering, event segmentation, artifact detection/rejection, ICA component rejection, and analysis. All that is required is writing your own trial segmentation function  as described here . This obviates some of what is described below, including using the ERP PCA toolkit for ICA. Contact me for more information if you are interested, as it may take me a while to update this documentation. \n High-Level Summary \n This toolkit contains  MATLAB  functions and scripts that I've written for importing and analyzing EEG data with  FieldTrip . The main goal is to provide wrapper functions for quickly and easily running analyses and making plots in the way that I've determined has been fruitful for using FieldTrip. \n I've set up the functions to import and process  Net Station  data; they don't currently deal with other formats, but it should relatively easy to modify mat-mvm to import anything that FieldTrip can import. Also included are scripts for using  PyEPL -based experimental data in the context of Net Station and FieldTrip, dealing with Net Station files, interfacing with the  ERP PCA Toolkit , running statistics, running ICA blink correction, ANOVAs, and more. There are other scripts as well, many of which are specific to my experiments \n The toolkit has been tuned for my workflow, and so I can't guarantee how well they will work for others, but I wanted to put them out there so I can keep track of them in a version control system and so that others can use them. I would enjoy hearing feedback that anyone might have. \n Getting Started and Documentation \n See the StartingOut page in the  Wiki  tab to get started. \n Some functions may not be documented particularly well, and that's because I haven't gotten around to doing that yet. Hopefully the more important functions are documented in the function header, accessible with the 'help' command in MATLAB. \n The Functions page has a brief overview of some of the most useful functions to use after you get your data in FieldTrip format. \n If you have any questions, please email me. You can find my email address on the project homepage under the members section. \n Licensing \n Some functions, like a few in the mat-mvm/stat/rmaov directory, have been downloaded from the MATLAB File Exchange and modified, and they are included because I use them. The original download URL and copyright information is contained in those files, which are covered under the Simplified BSD License. The Simplified BSD License is GPL-compatible, and thus these files can be included here. \n About me \n I am a  graduate student  in  Tim Curran's   lab  in the  Department of Psychology and Neuroscience  at the  University of Colorado Boulder . \n How to start using the scripts \n NB : As noted on the main page, I now do all my processing and analysis within Matlab/FieldTrip, so many of the instructions below are out of date. See mat-mvm/space/space_ft_seg_tla.m and the accompanying mat-mvm/space/space_trialfun.m for examples of the new workflow. \n In order to generalize to other experiments, things may need to change, but may not. To get trial metadata during segmentation (which gets put in the trialinfo field by FieldTrip), you will need to either have .evt files exported from your Net Station recordings, events.mat files created from other behavioral data, and/or other ways of reading behavioral data that might need to get built in to seg2ft or ideally would get put in your trialfun function. An example of event file creation can be found at another one of my projects, [https://github.com/warmlogic/expertTrain expertTrain], an experiment framework that started as an expertise training experiment but has grown. See expertTrain/analysis/space_createEvents.m for an example of event creation. \n Introduction \n These tools are essentially wrapper functions that I've written for using the  FieldTrip  toolbox with data recorded in  Net Station . You'll need to download  FieldTrip , and possibly the  ERP PCA Toolkit  and  EEGLAB . For initial behavioral data processing, I also use some functions contained in the Kahana lab's [http://memory.psych.upenn.edu/Software eeg_toolbox]; this will likely only apply to you if you use  PyEPL  for your experiments. \n A few subfolders in mat-mvm (excluding data, eeg, eptoolkit, ft_general, netstation, stat, utilities, etc.) hold experiment-specific scripts. Given the lack of anything resembling a tutorial, it would behoove you to explore (one of) these directories. Read the details below before starting. \n Initial Setup Information \n \n The scripts are mostly set up to deal with Net Station (NS) files and may not work well with other formats, though they might. \n I've never tried anything besides NS and EEGLAB format. \n For an example of EEGLAB files, look at the  kahn2  scripts, though note that these are set up for a particular file/directory setup that I did not create. \n If you're using another format, note that you will need to modify parts of some of the main scripts that check on the file extension. \n \n \n For the format of NS files you should use either  EGIS  or  raw  (a.k.a.  sbin , for simple binary).  EGIS  loads faster. \n When using NS files, you will need to know the names given to each segment type in the Segmentation Tool. Do not split the segment types into separate files; instead, you should have all of the segment types in a single  EGIS  or  sbin / raw  file, with the subject number at the beginning of each filename. \n Take note that the files need to be stored in a particular path on your computer or the network. This is defined in your analysis script, the important functions mentioned below, and  mm_ft_setSaveDirs.m . \n The scripts expect NS files with individual trials, but you can import average files if you'd like. Note that if you use average files, Net Station will do some funky re-naming of your segments. I believe it adds  Sub001  to the beginning of the segment name, but you should check on this. Error checking code in  seg2ft.m  will tell you the available segment names if you have input the wrong ones. \n \n \n I would eventually like to split the scripts into separate data processing and data visualization/analysis scripts, but I haven't gotten around to that yet. \n I have separated the data processing steps in my scripts for running on a computer cluster; these scripts are in the experiment folders and are (not intuitively) named with  *_ftprocess_*.m . \n \n \n \n Actually getting your data in MATLAB \n To get started in analyzing your data: \n \n The most important functions that get your data in FieldTrip format are  create_ft_struct.m ,  seg2ft.m , and  process_ft_data.m . They have extensive help sections that you can view with the 'help' command in MATLAB. The help sections may not explain all of the options (yet), so feel free to peruse the top of the code for some of these. \n You will need to file your such that the tools know where to find it. There are some assumptions made regarding file locations. \n Take a look at some of the current experiment scripts such as  mat-mvm/soco/soco_ft_seg_tla.m  or  mat-mvm/tnt/tnt_ft_seg_pow.m , but note that they may not be tuned for your experiment and all options may not be present. \n I will give a detailed initial setup here in the future. For now, try to glean them from the example experiment scripts. \n \n \n \n Preprocessing \n Here's how I preprocessed my data at one point (it has since changed a bit): \n \n Net Station: filter, segment/epoch, eye artifact detection, bad channel replacement/interpolation \n ERP PCA Toolkit: ICA blink removal, baseline correction \n Net Station: artifact detection, bad channel replace, average rereference \n FieldTrip: use these tools to read in NS files and get the data in FieldTrip format \n \n Analyzing and Plotting \n See the Functions section. \n Tips \n To automatically add mat-mvm to your path, put this in ~/Documents/MATLAB/startup.m: \n myMatlabDir = fullfile(getenv('HOME'),'Documents','MATLAB');\naddpath(genpath(fullfile(myMatlabDir,'mat-mvm')));\n \n To remove version control (.git, .svn, and CVS) directories from your path (I don't know if they would cause any conflicts, but I do this anyway), put this in ~/Documents/MATLAB/startup.m: \n %% remove version control directories from path (.git, .svn, CVS)\nentries = regexp(path, ['[^',pathsep,']*',pathsep], 'match');\n% find the version control entries\nvc_entries = cell2mat(cellfun(@(x) ~isempty(strfind(x,'.git')) | ~isempty(strfind(x,'.svn')) | ~isempty(strfind(x,'CVS')), entries, 'UniformOutput', false));\n% remove them\nrmpath(sprintf(repmat('%s',1,sum(vc_entries)),entries{vc_entries}));\n \n Brief overview of some of the more important functions \n Be sure to read the StartingOut section above before jumping in too deep. \n NB: Functions are usually suffixed with ER for ERP data (Event Related) and TFR for time–frequency data (TF Representation). This is adopted from the typical FieldTrip function naming convention. \n Plotting \n mm_ft_simpleplotER: An extremely simple function for plotting ERP data \n mm_ft_plotER and mm_ft_plotTFR: The most basic grand average plotting functions \n mm_ft_subjplotER and mm_ft_subjplotTFR: Plot individual subjects \n mm_ft_contrastER and mm_ft_contrastTFR: Plot condition contrasts \n mm_ft_clusterplotER and mm_ft_clusterplotTFR: Plot non-parametric statistical clustering results \n Statistics \n mm_ft_ttestER and mm_ft_ttestTFR: Run t-tests, comparing two conditions \n mm_ft_rmaov2ER, mm_ft_rmaov2TFR, mm_ft_rmaov33ER, and mm_ft_rmaov33TFR: Run ANOVAs comparing multiple conditions, ROIs, and time windows. \n mm_ft_clusterstatER and mm_ft_clusterstatTFR: Run non-parametric statistical clustering \n mm_ft_corr_dprimeER: Run correlations between ERP voltage and d'. You could probably input other data besides d'. \n Other \n There are many other functions, mostly for bookkeeping and getting your data ready to analyze. Check out some of the experiment scripts for examples. \n For example, check out send_gmail, which will send you an email (via your gmail account) when processing is done. !ProTip: use Cmd-Enter or Cmd-Shift-Enter to run sections of a script that has been divided up using cell notation (%%) to start your processing, and then follow that up with an email call to let you know when it's done.", 'SOSI \n Description \n Experiment 1 from Mollison & Curran (2012): http://dx.doi.org/10.1016/j.neuropsychologia.2012.06.027 \n Requirements \n This experiment runs using PyEPL http://pyepl.sourceforge.net. See their help forum for installation issues https://sourceforge.net/p/pyepl/discussion/548620/. \n Instructions \n See SOSI/doc/SOSI_Instructions_EEG.doc for instructions on how to run the experiment.', 'SOCO \n Description \n Experiment 2 from Mollison & Curran (2012): http://dx.doi.org/10.1016/j.neuropsychologia.2012.06.027 \n Requirements \n This experiment runs using PyEPL http://pyepl.sourceforge.net. See their help forum for installation issues https://sourceforge.net/p/pyepl/discussion/548620/. \n Instructions \n See SOCO/doc/SOCO_Instructions_EEG.doc for instructions on how to run the experiment.', 'COSI2 \n Description \n Experiment 3 from Mollison & Curran (2012): http://dx.doi.org/10.1016/j.neuropsychologia.2012.06.027 \n Requirements \n This experiment runs using PyEPL http://pyepl.sourceforge.net. See their help forum for installation issues https://sourceforge.net/p/pyepl/discussion/548620/. \n Instructions \n See COSI2/doc/COSI2_Instructions_EEG.doc for instructions on how to run the experiment.', 'sourceExperimentImages \n Description \n Images used in the source memory experiments in Mollison & Curran (2012): http://dx.doi.org/10.1016/j.neuropsychologia.2012.06.027 \n Instructions \n Unzip the two  .zip  files in images/object_stims  \n Attribution \n Images were collected from http://www.clipart.com, from the stimuli set provided by Brady, Konkle, Alvarez, & Oliva (2008), and through image searching online. If using the images provided by Brady et al. (2008), please see http://cvcl.mit.edu/MM/download.html. \n Brady, T. F., Konkle, T., Alvarez, G. A., & Oliva, A. (2008). Visual long-term memory has a massive storage capacity for object details. Proceedings of the National Academy of Sciences of the United States of America, 105, 14325--14329.', 'My dissertation', 'My bibliography file', "happening README \n Description \n This Is Happening  was a web application developed over three weeks during the fall of 2014 when I was a fellow at  Insight Data Science . See  this PDF  for a quick overview. \n Purpose:\n- Aggregate streaming social activity to find events that you might not otherwise stumble upon!\n- Not just a real-time social media mapper, find anomalous events that stand out above typical background activity!\n- That is to say: Locate trending social activity in meatspace! \n Getting Started \n Dependencies \n \n \n twitter: A twitter account, an application  dev.twitter.com/apps , and its authentication keys. \n \n \n tweepy:  pip install tweepy \n \n \n Keys \n Make a file named  twitauth.cfg  in the project's root directory your application's keys from  dev.twitter.com . \n apikey=\napikeysecret=\naccesstoken=\naccesstokensecret= \n Stream data from a geographic region \n python streamFromGeo_twitter.py \n Currently writes data to disk. \n Search for data within a geographic region \n python searchForGeo_twitter.py \n Currently does not write data to disk. \n Running the app \n \n Recommended:  Install virtualenv and fire up a virtual environment. \n \n ```\n  # Install virtualenv\n  sudo pip install virtualenv \n # Create virtualenv folder  venv \n  virtualenv venv \n # Activate the virtual environment\n  source venv/bin/activate\n  ``` \n \n Install Python project dependencies. \n \n pip install -r requirements.txt \n \n To test your application, run the manage.py file:  python manage.py runserver , and open your web browser to\n localhost:5000 . \n", 'fast_jet_lag \n Beat jet lag by fasting, as described here: http://harpers.org/blog/2012/03/the-empty-stomach-fasting-to-beat-jet-lag/ \n The main idea is to fast for about 12 or 16 hours and then eat a meal at the time that breakfast will be in your destination time zone.', 'wedding_program \n This is a repository of the wedding-related documents I made for our wedding.  All documents are in LaTeX.  PDFs have been provided for your viewing convenience. \n Contents \n \n wedding_program.tex : The program that our guests received. \n ceremony_script.tex : The script that we wrote together and with the help of our family and friends. \n wedding_table_numbers.tex : Numbers for each dinner table. \n wedding_table_names.tex : Numbers and names for each dinner table. \n charity_tokens.tex : A sign with instructions about the charity tokens. \n \n Et cetera \n \n The recipe for the beer we brewed and served before the ceremony: https://www.brewtoad.com/recipes/going-to-the-chapel \n', 'crypto-social-analysis \n Analysis of cryptocurrency-related activity, including subreddit traffic and price.', 'Contents \n This repo contains the following items: \n \n functions : Convenient functions to go in your  ~/.bash_profile  (or similar) to download template Jupyter Notebooks. \n exports : a definition of your personal initials to be included in the template notebook filename upon download. \n notebooks : Contains the actual template Jupyter notebooks downloaded by the functions mentioned above. \n jupyter_userdata.sh : A script to configure a Jupyter Notebook server with a password. \n This script essentially automates  these  instructions. \n \n \n \n Jupyter Notebook server setup \n The server configuration script requires a couple manual steps. After installing  jupyter  and  notebook  via your favorite method (e.g.,  conda ): \n \n Run the  jupyter_userdata.sh  script to set a password and copy the appropriate configuration to your server config file. \n Run your notebook server, browse to its URL, and enter the password when prompted. You can start the server in a  tmux  (or  screen ) session if you want to run it in the background. \n Your browser may produce a safety warning about accessing the server page. \n Chrome: Click "Advanced" and proceed to the notebook server page. \n Safari: Follow  these  instructions. \n If you want to have a fully trusted server, you can use  these  instructions. \n \n \n', 'Calculate the number of hours of propane in a propane tank. I use this for homebrewing beer.', 'Matt\'s dotfiles \n This repo borrows heavily from Mathias Bynens\'s  dotfiles  and Dries Vints\'s  dotfiles . It\'s the Ubuntu Linux version of my  dotfiles \n This also includes  tmux  files from  this repo . \n Installation \n Warning:  If you want to give these dotfiles a try, you should first fork this repository, review the code, and remove things you don\'t want or need. Don\'t blindly use my settings unless you know what that entails. Use at your own risk! \n Quick instructions overview \n \n If needed, copy public and private ssh keys from previous computer to  ~/.ssh/  and  chmod  to  600 \n Clone this repo \n mkdir -p ~/github/warmlogic \n cd ~/github/warmlogic/ \n git clone git@github.com:warmlogic/dotfiles.git \n cd ~/github/warmlogic/dotfiles/ \n \n \n Run  01-bootstrap.sh  to copy necessary files (hidden and otherwise) \n cp .extra ~/.extra  and edit, if desired (explained below) \n Run  03-ubuntu.sh  to set up Ubuntu packages preferences \n Restart your computer \n Run  04-python.sh  to set up a Python 3 conda environment named  py3 \n \n More details below. \n Using Git and the bootstrap script \n Clone the repository wherever you want (I keep it in  ~/src/dotfiles ). The bootstrapper script ( 01-bootstrap.sh ) will pull in the latest version and copy the files to your home folder. \n cd  into your local  dotfiles  repository, and start the installation: \n bash\nsource 01-bootstrap.sh \n Specify the  $PATH \n If  ~/.path  exists, it will be sourced along with the other files before any feature testing (such as  detecting which version of  ls  is being used ) takes place. \n Here\'s an example  ~/.path  file that adds  /usr/local/bin  to the  $PATH : \n bash\nexport PATH="/usr/local/bin:$PATH" \n Add custom commands without creating a new fork \n If  ~/.extra  exists, it will be sourced along with the other files. You can use this to add a few custom commands without the need to fork this entire repository, or to add commands you don\'t want to commit to a public repository. \n NB:  ~/.extra  is included in the repo, but it is not automatically copied over by  01-bootstrap.sh . Therefore, you\'ll want to run the following command and edit the new file\'s contents: \n bash\ncp .extra ~/.extra \n You can also use  ~/.extra  to override settings, functions, and aliases. It\'s probably better to  fork this repository  instead, though. \n Set up Ubuntu \n Install some packages: \n bash\n./03-ubuntu.sh \n Python/Anaconda setup \n You may also want Python 3 and a number of useful packages related to data analysis (via  miniconda ). This installs everything listed in  init/environment-py3.yml . \n bash\n./04-python.sh \n Additional setup \n Visual Studio Code Extensions \n \n Brewfile \n LaTeX Workshop \n Markdown All in One \n markdownlint \n Python \n Remote VSCode \n Spell Right \n SQL Server \n Visual Studio IntelliCode - Preview \n \n SublimeText Packages \n \n Package Control \n Anaconda \n WordCount \n Pretty JSON \n MarkdownEditing \n Change color scheme to ArcDark \n \n \n INI \n rsub  ( see instructions ) \n \n Could use these for additional Python linting setup: \n \n SublimeLinter \n SublimeLinter-contrib-mypy \n \n Jupyter Notebook Extensions \n These should be automatically turned on, via  .jupyter/nbconfig/notebook.js \n \n ExecuteTime \n spellchecker \n Table of Contents (2) \n Collapsible headings \n Highlight selected word \n Scroll down \n \n TODO \n \n Consider including an  IPython startup script . \n \n Feedback \n Suggestions/improvements  welcome !', 'aws_setup_teardown  README \n This is repo is inspired by the  setup  directory from the original  fast.ai course . I\'ve modified it significantly. \n Process to provision and set up new instance \n On your local machine \n \n Install  awscli :  pip install -U awscli \n Configure  awscli :  aws configure --profile my-profile-name \n export  the appropriate environment variable:  export AWS_PROFILE=my-profile-name \n Run  setup_ec2_instance.sh  with the appropriate arguments \n Add newly provisioned instance to your  ~/.ssh/config  (includes  RemoteForward  for remote editing with Sublime or VS Code): \n \n txt\nHost aws-my-instance\n    Hostname ec2-ip.us-west-2.compute.amazonaws.com\n    User ubuntu\n    IdentityFile ~/.ssh/aws-key-my-instance.pem\n    Port 22\n    RemoteForward 52698 127.0.0.1:52698 \n Set up the remote instance \n This includes installing my dotfiles, appropriate Linux packages, a conda Python environment for data science tasks, and configuring for AWS access. \n \n SSH into the instance:  ssh aws-my-instance \n Generate new ssh key :  ssh-keygen -t rsa -b 4096 -C "your_email@example.com" \n Add public key to GitHub account settings   here :  less ~/.ssh/id_rsa.pub \n Clone setup repos: \n mkdir -p ~/github/warmlogic; cd ~/github/warmlogic \n git clone git@github.com:warmlogic/dotfiles_linux.git \n git clone git@github.com:warmlogic/jupyter_setup.git \n \n \n Run scripts from  dotfiles_linux \n cd ~/github/warmlogic/dotfiles_linux \n ./01-bootstrap.sh \n ./03-ubuntu.sh \n ./04-python.sh \n \n \n If desired,  cp .extra ~/.extra  and edit \n Edit  ~/.exports  as necessary \n source ~/.bashrc \n Configure  awscli  (for S3, etc. access):  aws configure --profile my-profile-name \n \n Set up a Jupyter Notebook server \n \n Configure a Jupyter Notebook server \n cd ~/github/warmlogic/jupyter_setup \n ./jupyter_userdata.sh \n Enter desired password \n \n \n Go to the home directory:  cd \n Start a  tmux  session:  tmux new -s jupyter \n Start a Jupyter Notebook server:  nb-server \n Detach from the  tmux  session:  C-b d \n Reattach to the  tmux  session:  tmux a -t jupyter \n \n Access the Jupyter Notebook server on your local machine \n \n Browse to your Jupyter Notebook server\'s URL \n', 'Haikuincidence \n Find tweets that contain coincidental haikus, and  tweet these beautiful poems . \n text\nYou\'re a poet and\nyou didn\'t even know it.\nHey, that\'s a haiku! ✌️ \n Setup \n App data \n \n Add phrases to  data/track.txt  to only search for tweets that contain any of the exact strings, one string per line (see  the documentation about  track  for more info) \n If file does not exist or is empty, gets tweets from the  sample stream \n \n \n Add phrases to  data/ignore_tweet.txt  to ignore tweets that contain tokens from any of these strings, one string per line. Uses  AND  and  OR  logic like the track list, but tokens for  AND  (a single line) can match anywhere. Also matches basic plural versions of words (e.g.,  dog  in  data/ignore_tweet.txt  will match  dogs  in a tweet\'s text). See the function  text_contains_ignore_list_plural  (in  utils/text_utils.py ) for more info. \n Whether or not this file exists, by default this program ignores tweets with words read in the  get_ignore_tweet_list  function (in  utils/data_utils.py ) \n \n \n Add phrases to  data/ignore_profile.txt  to ignore tweets from accounts whose descriptions contain any of these strings.  OR  logic only; matches substrings. \n Add pre-defined syllable counts to  data/syllables.json \n \n Local Python environment \n \n Install poetry as described  here \n Install requirements:  poetry install \n \n Credentials and other variables \n If running the app on Heroku (see below),  .env  is not needed but it may still be convenient to fill in the environment variables. \n \n Copy the (hidden)  .env_template.ini  file to  .env \n Edit  .env  to include your credentials (don\'t commit this file) \n \n Database \n \n If running the app on Heroku, you can easily provision a database for your app by installing the Postgres add-on (see below). \n Your database credentials will automatically be added to your app\'s Config Vars. \n If not running the app on Heroku, you\'ll need to set up your own database. \n Add your database credentials to  .env \n \n Run the application \n Locally \n \n Run the application:  poetry run python haikuincidence/app.py \n Alternatively, activate the virtual environment that poetry created by running  poetry shell  and then run the script:  python haikuincidence/app.py \n Deactivate the virtual environment by running  deactivate \n \n As a Heroku app \n These instructions use the Heroku CLI \n \n Fork this repo on GitHub and ensure you have a branch called  main \n Create a new app on Heroku:  heroku create my-app-name \n Install add-ons for: \n Papertrail \n heroku addons:create papertrail -a my-app-name \n \n \n Postgres \n heroku addons:create heroku-postgres -a my-app-name \n \n \n Create a new token:  heroku authorizations:create -d "my cool token description" \n Add the token to your GitHub repo\'s Secrets under the name  HEROKU_API_KEY \n Add your Heroku app\'s name to the GitHub repo\'s Secrets under the name  HEROKU_APP_NAME \n Configure the application by adding environment variables as  Config Vars \n Commit and push to your GitHub repo\'s  main  branch \n This can be through committing a change, merging a PR, or just running  git commit -m "empty commit" --allow-empty \n This will use GitHub Actions to build the app using Docker and deploy to Heroku \n \n Heroku logs \n \n View the logs via the  Heroku CLI  or on Papertrail \n \n Attribution \n I borrowed and adapted code from these nice resources. Thank you! \n \n Default oppressive word filter from  this repo \n Syllable counting originally from  this script \n Haiku checker inspired by  this script \n URL regex from  this gist \n \n License \n Copyright (c) 2018 Matt Mollison Licensed under the MIT license.', 'seamless-scraper \n Scrape restaurant names and addresses from Seamless', "Wine Scorecard \n A printable scorecard for keeping track of wine ratings at a party. \n Usage \n \n Print the scorecards double sided \n Have a bunch of friends over, asking each to bring a bottle of wine \n Cover up each bottle to hide its details (e.g., put in a paper bag) \n Give each bottle a number \n Taste and rate the wines \n Collect ratings in a spreadsheet like  this one \n Learn about your group's preferences! \n \n Notes \n Wine stain graphic downloaded from  here .", 'This Is Happening \n Locate, summarize, and visualize pockets of social activity in meatspace. \n This app detects geotagged Twitter activity that stands out above typical background levels and tweets about it. \n Example account:  https://twitter.com/happening_sf \n Setup \n Local Python environment \n \n Install poetry as described  here \n Install requirements:  poetry install \n \n Credentials and other variables \n If running the app on Heroku (see below),  .env  is not needed but it may still be convenient to fill in the environment variables. \n \n Copy the (hidden)  .env_template.ini  file to  .env \n Edit  .env  to include your credentials (don\'t commit this file) \n \n Database \n \n If running the app on Heroku, you can easily provision a database for your app by installing the Postgres add-on (see below). \n Your database credentials will automatically be added to your app\'s Config Vars. \n If not running the app on Heroku, you\'ll need to set up your own database. \n Add your database credentials to  .env \n \n Run the application \n Locally \n \n Run the application:  poetry run python thisishappening/app.py \n Alternatively, activate the virtual environment that poetry created by running  poetry shell  and then run the script:  python thisishappening/app.py \n Deactivate the virtual environment by running  deactivate \n \n As a Heroku app \n These instructions use the Heroku CLI \n \n Fork this repo on GitHub and ensure you have a branch called  main \n Create a new app on Heroku:  heroku create my-app-name \n Install add-ons for: \n Papertrail \n heroku addons:create papertrail -a my-app-name \n \n \n Postgres \n heroku addons:create heroku-postgres -a my-app-name \n \n \n Create a new token:  heroku authorizations:create -d "my cool token description" \n Add the token to your GitHub repo\'s Secrets under the name  HEROKU_API_KEY \n Add your Heroku app\'s name to the GitHub repo\'s Secrets under the name  HEROKU_APP_NAME  (or however it is configured in  .github/workflows/deploy.yaml ) \n Configure the application by adding environment variables as  Config Vars \n Commit and push to your GitHub repo\'s  main  branch \n This can be through committing a change, merging a PR, or just running  git commit -m "empty commit" --allow-empty \n This will use GitHub Actions to build the app using Docker and deploy to Heroku \n \n Heroku logs \n \n View the logs via the  Heroku CLI  or on Papertrail \n \n TODO \n App \n \n Try using  tweepy  or  twint  instead of  twython \n Try running on fly.io instead of Heroku \n \n Data \n \n Tweets \n [x] Use a density-based metric for event detection, rather than aggregating within pre-defined boundaries (map tiles) and tracking statistics for each tile \n [ ] Define activity thresholds using intuitive, human readable values \n \n \n [x] Prevent a single prolific user from easily triggering an event by decreasing the weight of their tweets \n [x] Deduplicate tokens within each tweet \n [x] Reduce weight for tweets with specific longitude and latitude (e.g., "canonical" city locations that get assigned to Instagram photo posts) \n [ ] Detect and ignore spam tweets, e.g., job postings, apartment listings \n Queries \n [x] Provide access to the tweets associated with each event \n [x] Write query to keep most recent N days of data and run in main loop \n [x] Maintain maximum recent_tweets table row count and run in main loop \n \n ML for event finding \n \n [x] Clustering \n [x] When an event is found, run a clustering algorithm (DBSCAN) on all recent tweets to determine the full set of event tweets \n [x] Define cluster neighborhood limits using intuitive, human readable values (e.g., kilometers) \n \n Publishing / Analytics \n \n [x] When an event is found, tweet an image of a map with the location/heat map \n [x] Set my tweet\'s location to the event latitude and longitude \n [x] Exclude my own tweets from the search (put myself in the list of users to ignore) \n [ ] Plot the pulse of a neighborhood over time: count of tweets by hour \n \n Realizations \n \n Many tweets show up at the canonical city location, especially due to Swarm\'s "posted a photo" feature \n Using tiles / artificial region boundaries required more workarounds and convoluted solutions than originally expected. Some downsides: \n Potentially splits events across regions \n Keeping the running statistics requires storing many rows in database table; wouldn\'t be an issue if I wasn\'t trying to operate on a shoestring budget because I could run my own database \n It\'s not uncommon to get a false alarm due to one user posting many tweets in a short time period \n To add a location to the bot tweets, need to enable in: Settings and privacy -> Privacy and safety -> Location information -> Add location information to your Tweets \n \n License \n Copyright (c) 2020 Matt Mollison Licensed under the MIT license.', 'dalle-mini-tools \n A (soon-to-be) collection of tools for generating  dalle-mini  images \n Purpose \n This is repository is a collection of tools for doing inference against dalle-mini and dalle-mega. \n Installation & Usage \n Install the dependencies, then try out the CLI. Once installed, try  python generate.py --help  for more. \n Important: Dependencies are specified in  pyproject.toml . If you update the dependencies, run  poetry update  to update the lockfile  poetry.lock  and commit the changes to these two files. This is how poetry maintains the project\'s virtual environment. \n There are two ways to install: \n \n Using pip \n Using poetry \n \n Once installed, if everything runs OK, you should get images in an  output  directory. Like: \n \n pip install \n ```sh \n create a virtual environment with Python 3.10, and activate it \n install the project using the state of the code in the  main  branch \n you can replace  main  with any branch name or commit sha \n python -m pip install git+ssh://git@github.com/xeb/dalle-mini-tools.git@main \n install the project using the current state of your local code \n python -m pip install /path/to/dalle-mini-tools \n Generate an image from text \n python generate.py "a man at a computer trying to generate images"\n``` \n poetry install \n ```sh \n # If you installed poetry 1.1.x before, uninstall first \n curl -sSL https://install.python-poetry.org | python3 - --uninstall \n Install poetry 1.2.x preview \n curl -sSL https://install.python-poetry.org | python3 - --preview \n Create virtual env for this project, install requirements \n poetry install \n Enter virtual environment \n poetry shell \n Generate an image from text \n python generate.py "a man at a computer trying to generate images" \n Alternatively, rather than enter the venv with  poetry shell , run directly: \n poetry run python generate.py --help\n``` \n What is in this project? \n To date, the project contains: \n \n generate.py  is a command-line interface for generating images. This has no dependencies. \n sitegen.py  is a static website generator that uses  templates/template.html  to create index pages per the specified  output_dir  so you can upload results to a webserver \n server.py  is a Flask web server to host requests. This depends the  request.py  library and on having a  worker.py  running since requests are queued into an SQS queue. \n worker.py  is a worker process that listens to a SQS queue and then runs the model (via  generate.py ) \n request.py  is a command-line tool (and library used by  server.py ) for sending requests to the SQS queue. The server depends on this. \n \n Development notes \n \n Autoformat python files with  poetry run make lint \n Installs  ipywidgets  to avoid a tqdm error:  AttributeError: \'tqdm_notebook\' object has no attribute \'disp\' \n Installs  tokenizers  0.11.6 to support running on Apple silicon (0.12.x isn\'t working) \n You may need to  pip install --upgrade "jax[cuda]" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html  to get NVIDIA GPU support; will see how to do this in Poetry later \n \n More to come...', 'slack-export-parser \n Convert a Slack export into a prompt-and-completion pandas DataFrame. Purpose is to create a dataset to  fine tune a model like GTP-3 . \n ```sh \n create a virtual environment \n install the package \n python -m pip install /path/to/slack-export-parser \n get help \n python slack_export_parser/parse_prompt_completion.py --help \n python slack_export_parser/parse_prompt_completion.py --export_dir="My Export Folder" --display_name=matt --n_prior=2 --prepend_channel=True --prepend_sender=True --sender_type=display_name\n``` \n Args: \n \n export_dir : path to the uncompressed export directory \n display-name : username to use for the completion / response \n n_prior : number of messages prior to the completion to include \n prepend_channel : whether to prepend the channel name to the prompt \n prepend_sender : whether to prepend the sender to the prompt \n sender_type :  display_name  or  user \n']
jjrob13,['', 'EvilHangman \n iOS app that cheats at hangman \n Boot it up and see if you can win!', 'MyHealth \n MyHealth is a java-based health client-server application that allows users to track all of their vital health statistics. \n See the BitBucket repo used during development at https://bitbucket.org/CSE360group17/myhealth', 'ArticleReader \n Some building blocks to build a system that when passed a url will read the article aloud. \n As of right now, capable of taking an article url and returning a url of the mp3 file with the article being read.', 'missionaries_and_cannibals \n An assignment for my CSE 471, Intro to AI, class.  The classic problem of having n missionaries and n cannibals on one side of a river with a goal to transport all of them to the other side safely.  The caveat being that the boat can only carry one or two people at a time, and the number of cannibals on one side of the river can never outnumber the number of missionaries.  This puzzle will be solved using the problem formulation method from the Russell/Norvig AI book.  \n Guide: \nRequirements: \ngcc 4.8 (for C++11) \ncmake 2.8  \n Command line Compilation Steps: \n1. Go into "build" directory \n2. Enter command "cmake .." \n3. Enter command "make" \n4. Run executable "missionaries_and_cannibals"', 'EEE333Lab3 \n All part one files are in the Part 1 folder under "src". \n To download all current files, click the "Download Zip" button on the right hand side.', '471FinalProject \n Project for introduction to AI that includes Searching, Learning, and Game Theory sections.', '333FinalProject \n Souped-up vending machine for VHDL course.  Designed for Basys 2.', 'sklearn-cotraining \n An implementation of the Co-Training semi-supervised learning technique from (Blue, Mitchell 1998) that is meant to work well with scikit-learn.', 'deep_ultimate_tic_tac_toe \n Keras-based neural net architecture to play ultimate tic tac toe. Rules: https://mathwithbaddrawings.com/2013/06/16/ultimate-tic-tac-toe/', "```\nUsage: make_index.py [OPTIONS] \n Options:\n  --page_offset INTEGER   Page of pdf from which to start the index.  Past all\n                          of the preamble.\n  --end_page TEXT         final page on which to index contents.\n                          Absolute page\n                          number.\n                          Determined from counting from the very\n                          beginning of the pdf\n                          (i.e. including TOC, preamble,\n                          etc.)\n  --tokenizer TEXT        ['alphanum', or 'alpha']\n  --ngram_range TEXT      how many words to be used when analyzing phrases.\n  --n_entries INTEGER     how many index entries to create\n  --input_file TEXT       input pdf\n  --output_file TEXT      output text file\n  --max_pf FLOAT          maximum page frequency for phrases.  phrases that\n                          appear more frequently are discarded\n  --min_pf INTEGER        min page frequency for phrases.\n  --min_word_len INTEGER\n  --help                  Show this message and exit.\n```"]
SumonMSelim,['ADayWithPHP \n A day with PHP awesomeness to share my knowledge of PHP to newbie learners. \n \n Getting Started with PHP \n PHP Variables, Data Types and Operators \n PHP Conditions \n PHP Loops (while/do-while, for, foreach) \n Arrays in PHP \n Functions in PHP \n Forms and Databases \n \n Errors Handling \n \n \n OOP in PHP \n \n Class \n Extensibility and Inheritance \n \n Useful Links:\n* http://www.php.net/\n* http://www.php.net/manual/en/language.types.php\n* http://www.php.net/manual/en/language.operators.arithmetic.php\n* http://www.php.net/manual/en/language.operators.string.php\n* http://www.php.net/manual/en/language.oop.php\n* http://www.php.net/manual/en/ref.errorfunc.php#errorfunc.constants', 'my-settings \n All my setting files of IDEs, Servers, Shells in one place. \n IDEs \n Shell', 'Learning WebRTC \n Source code of my works while I learn WebRTC.', 'programming-logic-bootcamp \n C++ codes for Programming Logic Bootcamp workshop', '30-seconds-of-laravel', 'Refactor your legacy code using SOLID principles. \n This is a Laravel application used only for demo purpose - mainly as a pseudo application. \n N.B.: Pardon me if there is any mistake.   \n Resources \n Video: https://www.youtube.com/watch?v=TWY9Bin9TY0 \n Slide: https://docs.google.com/presentation/d/1uBOPaLyZ1SSN-u8usc8HiOFVWikL7YO3b27I0WdD8V8/', '💻Software Engineer. 🗺️Digital Nomad. 🐱Cat Lover. \n', 'Laravel Twitter Clone \n Prerequisites \n \n Docker \n \n Installation \n Build and start Docker services \n bash\ndocker-compose up --build -d \n Connect to php-fpm (app) container \n bash\ndocker-compose exec app sh \n Copy .env file \n bash\ncp .env.example .env \n Install the dependencies \n bash\ncomposer install \n Run the database migration and seeder (if needed) \n bash\nphp artisan migrate --seed \n Run the tests \n bash\nbin/phpunit']
abakan-zz,['EURoute.me \n Make the most out of your Europe trip! \n Route Me is a Flask app for recommending customized travel routes in Europe.']
martinraison,['\n NOTE: This project needs testing, use at your own risk \nScalaWit\n======== \n Scala client for  Wit.AI \n What this client can do for you: \n \n Handle HTTP asynchronous requests to Wit.AI \n Parse the JSON response and give you nice Scala objects to play with \n \n Setup \n No packaged  *.jar  for this project yet. In the meantime:\n*  git clone https://github.com/martinraison/ScalaWit.git; cd ScalaWit; sbt assembly \n* Add the resulting  core/target/scala-2.10/core-assembly-0.1.jar  to your project\'s classpath. \n Usage \n Documentation is on its way :) To get started, you should have a look at:\n* the examples below\n* the client methods in  Wit.scala  - they mirror the official API pretty closely\n* the data structures in  Models.scala  -  WitMessage ,  WitIntent ,  WitExpression ,  WitEntity , etc. Note that most API methods ( getMessage ,  getIntents ,  getEntities , etc) also have a  ***Raw  version that you can use if you just want the JSON string returned by the server. \n Examples \n Creating the client (+ imports): \n bash\nscala> import com.scalawit.Wit._, scala.concurrent._, scala.concurrent.duration._, javax.sound.sampled._, java.io._\nscala> val client = new Wit("YOUR_TOKEN") \n Sending text input \n bash\nscala> val message = Await.result(client.getMessage("hello"), 5 seconds)\nscala> message.right.get.pretty\nMessage:\n    msgId = dd206aea-eff9-441b-bcc4-19e42186912f\n    msgBody = hello\n    outcome = Outcome:\n        intent = greetings\n        entities = \n        confidence = 0.52 \n Sending audio input \n ```bash\nscala> val audio = AudioSystem.getAudioInputStream(new File("resources/hello.wav"))\nscala> val message = Await.result(client.postSpeech(audio), 5 seconds)\nscala> message.right.get.pretty\nMessage:\n    msgId = 68c68ce4-225a-43ad-978b-b7a8086e18fd\n    msgBody = hello\n    outcome = Outcome:\n        intent = greetings\n        entities = \n        confidence = 0.525 \n ``` \n Other calls: \n bash\nscala> val intents = Await.result(client.getIntents, 5 seconds)\nscala> intents.right.get.pretty\n* Intent:\n    name = grab_beer\n    doc = \n    id = 234f2dea-e7c2-4fc5-9636-a7f09c56de8f\n    metadata = {"key": 34}\n* Intent:\n    name = greetings\n    doc = \n    id = 1bc55974-d242-46e5-a605-de934d3eb0f1 \n The  Wit.AI API  will likely be updated faster than this client.\nSuggestions and pull requests welcome! \n You\'re also welcome to submit an issue/pull request if you see JSON responses not being parsed correctly (failure or missing fields). Remember you can always use the  ***Raw  versions of the client methods to bypass parsing and just return a JSON string. \n License \n The MIT License (MIT) \n Copyright (c) 2014 Martin Raison \n Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the "Software"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions: \n The above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software. \n THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.', 'hello', 'AI Masterclass Labs \n Bienvenue dans l\'AI Masterclass Labs. Le but de cette séance est d\'aider l\'association "Agir pour l\'Ecole" à développer de nouveaux outils d\'apprentissage de l\'écriture pour les enfants. \n Nous disposons de données contenant des caractères écrits par des enfants. Le but est de créer un modèle permettant de détecter automatiquement quel caractère a été écrit (ce qui peut ensuite être utilisé dans des applications pour tablette/smartphone/etc) \n Ces données sont présentes en quantité très limitée pour l\'instant (quelques centaines de caractères), c\'est pourquoi il est préférable d\'entraîner d\'abord sur un plus gros dataset (EMNIST), et d\'appliquer ensuite ce modèle sur les données de l\'association. \n Prérequis \n Pytorch et torchvision doivent être installés. Suivre les instructions d\'installation ici http://pytorch.org/ \n Pour entraîner un modèle de reconnaissance de caractères sur EMNIST: \n Utilisation \n python main.py \n Pour évaluer le modèle sur des données fournies par l\'association: \n python eval.py \n Instructions \n \n forker le repo \n ajouter l\'URL du fork sur  ce document \n coder! Le but est d\'améliorer le score d\'évaluation. Nous exécuterons un script d\'évaluation similaire à  eval.py , votre code doit donc contenir un fichier  predictor.py  avec une classe  Predictor \n lorsque vous mettez à jour le repo, nous lançons automatiquement l\'évaluation et mettons à jour le  leaderboard \n \n Quelques pistes (non exhaustives) \n \n Le modèle de base n\'utilise pas de convolutions... ça peut être utile :) \n Vous pouvez aussi utiliser des techniques de "data augmentation" (rotations/translations aléatoires des images de training, etc) \n Les données d\'entraînement (EMNIST) et d\'évaluation (Agir pour l\'Ecole) proviennent de sources différentes! Il peut être utile d\'utiliser les données de validation fournies pour "fine-tuner" le modèle (mais pas n\'importe comment...) \n \n Bon courage!', 'Tiny "Hello, world!" binary in Rust \n This shows how to create a tiny "Hello, world!" binary in Rust. On my system (macOS), the resulting binary size is 8432 bytes, which is the same as when compiling the following C program with  gcc -Os -o main main.c : \n ```c \n include  \n int main() {\n  printf("Hello, World!\\n");\n}\n``` \n The goal here is to stay as close as possible to the C "Hello, world!" program above. However it\'s possible to get even smaller binaries, as documented in  this post . \n Look into the  official Rust documentation  for additional information about creating Rust executables without the standard library. \n Get started \n $ cargo +nightly build --release\n$ ./target/release/tiny-hello-rs\nHello, world!\n$ wc -c ./target/release/tiny-hello-rs\n    8432 ./target/release/tiny-hello-rs', 'ASCII TV \n Stream ASCII movies over HTTP \n Usage \n Start watching Star Wars Episode IV from a shell like this: \n curl https://asciitv.fr \n \n Credits \n \n Original art work : Simon Jansen ( http://www.asciimation.co.nz/ ) \n Inspiration: Martin W. Kirst ( ascii-telnet-server ) \n \n About \n I post about things I make on Twitter  @braizh', 'kmm-repro-ios-subscription-crash \n This is a repro for https://github.com/apollographql/apollo-android/issues/3127', 'kmm-repro-ios-subscription-crash \n This is a repro for https://github.com/apollographql/apollo-android/issues/3127', 'kmm-repro-graphql-codegen-crash \n This is a repro for https://github.com/apollographql/apollo-android/issues/3137', 'kmm-repro-graphql-codegen-crash \n Repro for https://github.com/apollographql/apollo-android/issues/3144']
astanway,[]
Jokeren,['ACM-ICPC-Template \n My ICPC algorithm code templates', "\n \n \n \n Documentation  |\n------------------- |\n \n Triton \n This is the development repository of Triton, a language and compiler for writing highly efficient custom Deep-Learning primitives. The aim of Triton is to provide an open-source environment to write fast code at higher productivity than CUDA, but also with higher flexibility than other existing DSLs. \n The foundations of this project are described in the following MAPL2019 publication:  Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations . Please consider citing this work if you use Triton! \n The  official documentation  contains installation instructions and tutorials. \n Quick Installation \n You can install the latest stable release of Triton from pip: \n bash\npip install triton \nBinary wheels are available for CPython 3.6-3.9 and PyPy 3.6-3.7. \n And the latest nightly release: \n bash\npip install -U --pre triton \n Changelog \n Version 1.1 is out! New features include:\n- Many, many bugfixes\n- More documentation\n- Automatic on-disk caching of compiled binary objects\n- Random Number Generation\n- Faster (up to 2x on A100), cleaner blocksparse ops \n Contributing \n Community contributions are more than welcome, whether it be to fix bugs or to add new features. Feel free to open GitHub issues about your contribution ideas, and we will review them. A contributor's guide containing general guidelines is coming soon! \n If you’re interested in joining our team and working on Triton & GPU kernels,  we’re hiring ! \n Compatibility \n Supported Platforms:\n  * Linux \n Supported Hardware:\n  * NVIDIA GPUs (Compute Capability 7.0+)\n  * Under development: AMD GPUs, CPUs \n Disclaimer \n Triton is a fairly recent project, and it is under active development. We expect it to be pretty useful in a wide variety of cases, but don't be surprised if it's a bit rough around the edges :)", 'gBolt \n \n \n \n gBolt \n Features \n Install \n Usage \n Input Specification \n Reference \n Citation \n Extension \n \n \n \n gBolt --a fast, memory efficient, and light-weight implementation for gSpan algorithm in data mining \n Features \n gBolt  is up to 100x faster (see detailed  experiments ) than  Yan\'s  original implementation with multi-threading on a single machine.  gBolt  also reduces more than 200 folds memory usage, running efficiently on personal computers.   \n gBolt  is  fast  because it: \n \n Adopts  OpenMP  task-based parallel programming; \n Incorporates  C++11  hastable and hashset; \n Uses contiguous memory storage; \n Uses partial pruning. \n \n gBolt  is  memory efficient  because it: \n \n Incorporates  C++11  emplace_back method; \n Reconstructs a graph with frequent edges and nodes before mining; \n Uses a customized  Path  data structure to reuse memory in recursive procedures. \n \n gBolt  is  light-weight  because it: \n \n Can be installed without any heavy third-party library; \n Delivers cross-platform performance without utilizing architectural features. \n \n gBolt  is  correct  because: \n \n We have ran  experiments  for  extern/data/Compound_422  and  extern/data/Chemical_340  with minimal support from 0.1 to 0.9. The results generated by  gBolt  are exactly the same as Yan\'s gSpan-64.  \n \n Install \n Required : \n \n cmake :  sudo apt-get install cmake  or install from  source . \n gcc >= 4.9 \n \n Optional : \n \n jemalloc : install from  source . \n OpenMP Environment : enable multi-threading \n \n Steps : \n git clone --recursive https://github.com/Jokeren/gBolt.git\ncd gBolt\nmkdir build && cd build\ncmake ..\nmake\n \n Build Options : \n \n -DGBOLT_SERIAL=ON : serial execution without  OpenMP \n -DGBOLT_PERFORMANCE=ON : display simple performance information and use hash map \n -DJEMALLOC_DIR=/path/to/dir : use jemalloc for memory management \n -DCMAKE_BUILD_TYPE=<Release> or <RelWithDebInfo> : config build type \n \n Usage \n Run an example : \n ./build/gbolt -i extern/data/Chemical_340 -s 0.2\n \n Arguments help : \n ./build/gbolt -h\n \n Multi-threading config : \n export OMP_NUM_THREADS=<hardware core num for recommendation>\n \n Input Specification \n Examples : \n ./extern/data\n \n Format : \n t # <graph-id>\nv <vertex-id> <vertex-label>\n...\ne <vertex-id> <vertex-id> <edge-label>\n...\n \n \n \n <graph-id>  must be contiguous without gaps, which means  gBolt  only supports  t # <id>  followed by  t # <id + 1> . \n \n \n <vertex-id>  must be contiguous without gaps, which means  gBolt  only supports  v # <id>  followed by  v # <id + 1> . \n \n \n All the  <id>  fields must be non-negative integers starting from 0.  \n \n \n All the  <label>  fields must be non-negative integers.  \n \n \n Reference \n Yan, Xifeng, and Jiawei Han. "gspan: Graph-based substructure pattern mining." Data Mining, 2002. ICDM 2003. Proceedings. 2002 IEEE International Conference on. IEEE, 2002. \n Citation \n @misc{zhou_2019, title={Jokeren/gBolt}, url={https://github.com/Jokeren/gBolt}, journal={GitHub}, author={Zhou, Keren}, year={2019}, month={March}}\n \n Extension \n gBolt  is designed for efficiency, so we have not developed utilities for it. Your are welcome to implement  Python ,  C , or graphical interfaces.', 'DataMining-Ullman \n Ullman isomorphism algorithm \n Features: \n \n Basic isomorphism detection \n Refinement pruning \n \n Usage: \n ./ullman -database [file_name] -query [file_name] &> log\n \n Progress: \n \n Upload the final report \n \n 12/21/2014 \n \n Document \n Refine codes \n Correctness test \n', "Cousera \n Keren Zhou's homeworks for Cousera online courses. \n Currently including: \n \n Mining Massive Datasets. \n", "Notes \n Have fun reading papers and books! \n Concurrency \n \n Read-Log-Update A Lightweight Synchronization Mechanism for Concurrent Programming \n A Pragmatic Implementation of Non-Blocking Linked-Lists \n A Wait-free Queue as Fast as Fetch-and-Add \n Non-blocking Patricia Tries with Replace Operations \n The Java Memory Model \n Foundations of the C++ Concurrency Memory Model \n The Foundations for Scalable Multi-core Software in Intel® Threading Building Blocks \n SALSA: Scalable and Low Synchronization NUMA-aware Algorithm for Producer-Consumer Pools \n [ ] NUMASK: High Performance Scalable Skip List for NUMA \n BRAVO–Biased Locking for Reader-WriterLocks \n Eraser: A Dynamic Data Race Detector for Multithreaded Programs \n ThreadSanitizer – data race detection in practice \n Scheduling Multithreaded Computations by Work Stealing \n Wait-Free Synchronization \n CDSCHECKER Checking Concurrent Data Structures Written with CC++ Atomics \n Algorithms for Scalable Synchronization on SharedMemory Multiprocessors \n Everything You Always Wanted to Know About Synchronization but Were Afraid to Ask \n Nonblocking Concurrent Data Structures with Condition Synchronization \n Software Transactional Memory for Dynamic-sized Data Structures \n Transactional Memory \n Transactional Data Structure Libraries \n \n Parallel Computing \n \n A programming system for future proofing performance critical libraries \n BLIS A Framework for Rapidly Instantiating BLAS Functionality \n Barrier Elision for Production Parallel Programs \n Implementing Strassen's Algorithm with BLIS \n Lightweight Dynamic Selection for Kernel-based Data-parallel Programming Model \n The Big Data Challenges of Connectomics \n Experimenting with Low-Overhead OpenMP Runtime on BG/Q \n Reducers and Other Cilk++ Hyperobjects \n \n Deep Learning \n \n Deep Learning with Limited Numerical Precision \n One weird trick for parallelizing convolutional neural networks \n Overcoming Challenges in Fixed Point Training of Deep Convolutional Networks \n Scaling Distributed Machine Learning with the Parameter Server \n Optimizing Memory Efficiency for Deep Convolutional Neural Networks on GPUs \n \n GPUs \n \n Efficient Synchronization Primitives for GPUs \n Enterprise Breadth-First Graph Traversal on GPUs \n GPU Multisplit \n iBFS Concurrent Breadth-First Search on GPUs. \n Analyzing CUDA Workloads Using a Detailed GPU Simulator \n Demystifying GPU Microarchitecture through Microbenchmarking \n NVIDIA TESLA V100 GPU ARCHITECTURE \n Visualizing Complex Dynamics in Many-Core Accelerator Architectures \n \n CPUs \n \n Simultaneous Multithreading Maximizing On-Chip Parallelism \n A Single-Chip Multiprocessor \n Evaluating the Potential of Multithreaded Platforms for Irregular Scientific Computations \n Victim Replication: Maximizing Capacity while Hiding Wire Delay in Tiled Chip Multiprocessors \n The future of microprocessors \n Elastic cooperative caching: an autonomous dynamically adaptive memory hierarchy for chip multiprocessors \n IBM POWER7 multicore server processor \n A Primer on Memory Consistency and Cache Coherence \n IBM Blue Gene/Q memory subsystem with speculative execution and transactional memory \n Speculative Lock Elision: Enabling Highly Concurrent Multithreaded Execution \n \n Machine Learning \n \n A Few Useful Things to Know about Machine Learning \n \n Compiler \n \n Graspan: A Single-machine Disk-based Graph System for Interprocedural Static Analyses of Large-scale Systems Code \n Program Locality Analysis Using Reuse Distance \n", "Homework \n Keren Zhou's Homework", "Tutorial \n Keren Zhou's sample codes", 'jokeren.github.io \n Personal Website', 'DataMining-GSP \n Hash^2 GSP algorithm in data mining. This work is done by Keren Zhou, Qiang Li, Gang zeng, and Wuzhao Zhang. You can contact us by sending an email to  kerenzhou@outlook.com . \n If you want to learn the whole structure of our work, please read  report.pdf . \n Features: \n General  : \n \n Support max gap and min gap. \n Hash-tree, with  state pruning  and  adaptive adjust hash  method. \n Time-list matching algorithm. \n \n Specific  : \n \n \n There are two types of input data, you can switch by specifying the  -file_type  argument, where  0  represents the common input data, and  1  represents input data the same as the spmf project. We provide some of the test data in the  data  directory. \n common input data : \n Examples are  100.txt ,  seq.txt  and  gen.data , the format is as follow: \n sequence_id  number_of_items  item_id  item_id ... \n sequence_id  number_of_items  item_id  item_id ... \n sequence_id  number_of_items  item_id  item_id ... \n Where each line represents an itemset, consist of distinct items. \n spmf input data : \n Exapmles are  BMS1_spmf.txt ,  kosarak10k.txt ,  kosarak25k.txt , and  small.txt , the format is as follow: \n item_id -1 item_id item_id -1 ... -1 item_id -2 \n item_id -1 item_id item_id -1 ... -1 item_id -2 \n item_id -1 item_id item_id -1 ... -1 item_id -2 \n Where each item_set is seperated by  -1 , and  -2  indicates the end of a sequence. \n \n \n There is an experimental  data_provider  in the source code, but the features are not fully developed. Currently it supports the  Gaussian distribution  and  Even distribution . You must modify the parameters in the file if you want to use it, and recompiliation is also required. \n \n \n Usage: \n ./gsp -i [file_name] -t [support: float] -sequNUM [unsigned int32] -min [unsigned int32] -max [unsigned int32] -eventNUM [unsigned int32] -file_type [0:common, 1:spmf]\n \n For instance, if you want to use  100.txt  in  ../data/  directory, you should use the following command: \n ./gsp -i ../data/100.txt -t 0.5 -sequNUM 100 -min 2 -max 4 -eventNUM 100 -file_type 0\n \n Advantages: \n \n Up to 10 times faster than the original GSP algorithm encountering large data. Faster than Prefixspan algorithm. The experiment result is shown in the following table. \n \n kosarak10k.txt  10000 sequences \n algorithm | dataset | support | time(s)\n--------- | ------- | ------- | -------\nhash^2 gsp | kosarak10k | 0.05 | 0.133\ngsp | kosarak10k | 0.05 | 0.235\nprefix | kosarak10k | 0.05 | 0.232\nhash^2 gsp | kosarak10k | 0.04 | 0.099\ngsp | kosarak10k | 0.04 | 0.382\nprefix | kosarak10k | 0.04 | 0.23\nhash^2 gsp | kosarak10k | 0.03 | 0.15\ngsp | kosarak10k | 0.03 | 0.454\nprefix | kosarak10k | 0.03 | 0.24\nhash^2 gsp | kosarak10k | 0.02 | 0.207\ngsp | kosarak10k | 0.02 | 1.217\nprefix | kosarak10k | 0.02 | 0.272\nhash^2 gsp | kosarak10k | 0.01 | 0.484\ngsp | kosarak10k | 0.01 | 4.373\nprefix | kosarak10k | 0.01 | 0.372 \n kosarak25k.txt  25000 sequences \n algorithm | dataset | support | time(s)\n--------- | ------- | ------- | -------\nhash^2 gsp | kosarak25k | 0.05 | 0.299\ngsp | kosarak25k | 0.05 | 0.436\nprefix | kosarak25k | 0.05 | 0.345\nhash^2 gsp | kosarak25k | 0.04 | 0.233\ngsp | kosarak25k | 0.04 | 0.631\nprefix | kosarak25k | 0.04 | 0.42\nhash^2 gsp | kosarak25k | 0.03 | 0.323\ngsp | kosarak25k | 0.03 | 0.921\nprefix | kosarak25k | 0.03 | 0.425\nhash^2 gsp | kosarak25k | 0.02 | 0.339\ngsp | kosarak25k | 0.02 | 2.22\nprefix | kosarak25k | 0.02 | 0.591\nhash^2 gsp | kosarak25k | 0.01 | 0.611\ngsp | kosarak25k | 0.01 | 8.95\nprefix | kosarak25k | 0.01 | 0.7 \n BMS1_spmf60k.txt  60000 sequences \n algorithm | dataset | support | time(s)\n--------- | ------- | ------- | -------\nhash^2 gsp | BMS1_spmf60k | 0.05 | 0.093\ngsp | BMS1_spmf60k | 0.05 | 0.188\nprefix | BMS1_spmf60k | 0.05 | 0.188\nhash^2 gsp | BMS1_spmf60k | 0.04 | 0.078\ngsp | BMS1_spmf60k | 0.04 | 0.796\nprefix | BMS1_spmf60k | 0.04 | 0.191\nhash^2 gsp | BMS1_spmf60k | 0.03 | 0.109\ngsp | BMS1_spmf60k | 0.03 | 0.797\nprefix | BMS1_spmf60k | 0.03 | 0.235\nhash^2 gsp | BMS1_spmf60k | 0.02 | 0.124\ngsp | BMS1_spmf60k | 0.02 | 2.58\nprefix | BMS1_spmf60k | 0.02 | 0.312\nhash^2 gsp | BMS1_spmf60k | 0.01 | 0.453\ngsp | BMS1_spmf60k | 0.01 | 22.9\nprefix | BMS1_spmf60k | 0.01 | 0.485 \n Progress: \n 12/9/2014: \n \n Adaptive hash-tree branches functions. \n Multi-thread. \n Fix data generator. \n', 'KeplerGEMM \n Fast GEMM for deep learning on Kepler', 'Torch Tensor Benchmarks \n Environment \n \n E5-2680 v3 @ 2.50GHz (AVX2, FMA) \n 12 threads \n \n Functions \n TH_TENSOR_APPLY \n- fill \n TH_TENSOR_APPLY2 \n- add\n- mul\n- div\n- copy \n TH_TENSOR_APPLY3 \n- cadd\n- cmul\n- cdiv', "Awesome-GPU \n \n Architecture \n Resources Management \n Parallelism \n Cache \n Memory \n White Papers \n \n \n Algorithms \n BLAS \n Stencils \n Scans \n \n \n Applications \n Deep Learning \n \n \n Tools \n Benchmarks \n Models \n Simulators \n Profilers \n \n \n Runtime \n Scheduling \n \n \n Code Generation \n Compilers \n Programming Models \n Profile Guided Optimization \n Binaries \n \n \n \n Architecture \n Resources Management \n \n TECS'21 - Reducing Energy in GPGPUs through Approximate Trivial Bypassing \n ASPLOS'17 - Locality-Aware CTA Clustering for Modern GPUs \n ASPLOS'17 - Dynamic Resource Management for Efficient Utilization of Multitasking GPUs \n HPCA'17 - Dynamic GPGPU Power Management Using Adaptive Model Predictive Control \n ISCA'16 - Transparent Offloading and Mapping (TOM): Enabling Programmer-Transparent Near-Data Processing in GPU Systems \n \n Parallelism \n \n HPCA'18 - Accelerate GPU Concurrent Kernel Execution by Mitigating Memory Pipeline Stalls \n HPCA'17 - Controlled Kernel Launch for Dynamic Parallelism in GPUs \n GTC'17 - COOPERATIVE GROUPS \n ISCA'16 - LaPerm: Locality Aware Scheduler for Dynamic Parallelism on GPUs \n ISCA'16 - Virtual Thread Maximizing Thread-Level Parallelism beyond GPU Scheduling Limit \n Berkeley TechRpts'16 - Understanding Latency Hiding on GPUs \n \n Cache \n \n ISCA'16 - APRES: Improving Cache Efficiency by Exploiting Load Characteristics on GPUs \n SC'15 - Adaptive and Transparent Cache Bypassing for GPUs \n \n Memory \n \n ICCAD'21 - Improving Inter-kernel Data Reuse With CTA-Page Coordination in GPGPU \n SC'21 - In-Depth Analyses of Unified Virtual Memory System for GPU Accelerated Computing \n IBM'20 - Umpire: Application-Focused Management and Coordination of Complex Hierarchical Memory \n HPCA'13 - Reducing GPU Offload Latency via Fine-Grained CPU-GPU Synchronization \n \n White Papers \n \n NVIDIA Hopper - NVIDIA H100 Tensor Core GPU Architecture \n NVIDIA Ampere - NVIDIA A100 Tensor Core GPU Architecture \n NVIDIA Turing - NVIDIA TURING GPU ARCHITECTURE \n NVIDIA Volta - NVIDIA TESLA V100 \n NVIDIA Pascal - NVIDIA TESLA P100 \n NVIDIA Kepler - NVIDIA’s Next Generation CUDA Compute Architecture: Kepler \n NVIDIA Fermi - NVIDIA’s Next Generation CUDA Compute Architecture: Fermi \n AMD CDNA 2 - INTRODUCING AMD CDNA 2 ARCHITECTURE \n AMD CDNA - INTRODUCING AMD CDNA ARCHITECTURE \n \n Algorithms \n BLAS \n \n GTC'20 - DEVELOPING CUDA KERNELS TO PUSH TENSOR CORES TO THE ABSOLUTE LIMIT ON NVIDIA A100 \n IPDPS'20 - Demystifying Tensor Cores to Optimize Half-Precision Matrix Multiply \n PPoPP'19 - A Coordinated Tiling and Batching Framework for Efficient GEMM on GPU \n GTC'18 - CUTLASS: CUDA TEMPLATE LIBRARY FOR DENSE LINEAR ALGEBRA AT ALL LEVELS AND SCALES \n \n Stencils \n \n CGO'20 - AN5D: Automated Stencil Framework for High-Degree Temporal Blocking on GPUs \n IPDPS'20 - On Optimizing Complex Stencils on GPUs \n PPoPP'18 - Register Optimizations for Stencils on GPUs \n \n Scans \n \n NVResearch TechRpts'16 - Single-pass Parallel Prefix Scan with Decoupled Look-back \n \n Applications \n Deep Learning \n \n PPoPP'21 - Understanding and bridging the gaps in current GNN performance optimizations \n SC'21 - E.T.: re-thinking self-attention for transformer models on GPUs \n OSDI'21 - GNNAdvisor: An Adaptive and Efficient Runtime System for GNN Acceleration on GPUs \n SC'20 - Sparse GPU Kernels for Deep Learning \n PPoPP'18 - SuperNeurons: Dynamic GPU Memory Management for Training Deep Neural Networks \n HPCA'17 - Towards Pervasive and User Satisfactory CNN across GPU Microarchitectures \n \n Tools \n Benchmarking \n \n GTC'18 - Dissecting the NVIDIA Volta GPU Architecture via Microbenchmarking \n ISPASS'10 - Demystifying GPU Microarchitecture through Microbenchmarking \n \n Models \n \n PMBS'19 - Instruction Roofline An insightful visual performance model for GPUs \n ECP'19 - Performance Tuning of Scientific Codes with the Roofline Model \n GTC'18 - VOLTA Architecture and performance optimization \n Synthesis Lectures on Computer Architecture'12 - Performance Analysis and Tuning for General Purpose Graphics Processing Units (GPGPU) \n SC'10 - Fundamental_Optimizations \n \n Simulators \n \n ISPASS'10 - Visualizing Complex Dynamics in Many-Core Accelerator Architectures \n ISPASS'09 - Analyzing CUDA Workloads Using a Detailed GPU Simulator \n \n Profilers \n \n PLDI'18 - GPU Code Optimization using Abstract Kernel Emulation and Sensitivity Analysis \n CGO'18 - CUDAAdvisor: LLVM-based runtime profiling for modern GPUs \n CCGRID'18 - Exposing Hidden Performance Opportunities in High Performance GPU Applications  \n THPC'16 - Monitoring Heterogeneous Applications with the OpenMP Tools Interface \n Euro-Par'15 - Identifying Optimization Opportunities Within Kernel Execution in GPU Codes \n SC'13 - Effective sampling-driven performance tools for GPU-accelerated supercomputers \n ISPASS'12 - Lynx: A dynamic instrumentation system for data-parallel applications on GPGPU architectures  \n ICPP'11 - Parallel Performance Measurement of Heterogeneous Parallel Systems with GPUs \n Vampir|Score-P \n TAU \n PAPI \n Allinea MAP \n Open|SpeedShop \n HPCToolkit \n NVIDIA Nsight Systems \n NVIDIA Nsight Compute \n SASSI \n NVBit \n \n Runtime \n Scheduling \n \n PPoPP'22 - CASE: A Compiler-Assisted SchEduling Framework for Multi-GPU Systems \n TPDS'20 - cCUDA: Effective Co-Scheduling of Concurrent Kernels on GPUs \n \n Code Generation \n Compilers \n \n AMD'21 - Generating GPU Compiler Heuristics using Reinforcement Learning \n TACO'21 - Domain-Specific Multi-Level IR Rewriting for GPU: The Open Earth Compiler for GPU-accelerated Climate Simulation \n LLVM'17 - Implementing implicit OpenMP data sharing on GPUs \n CGO'16 - gpucc: An Open-Source GPGPU Compiler \n LLVM'16 - Offloading Support for OpenMP in Clang and LLVM \n PMBS'15 - Performance Analysis of OpenMP on a GPU using a CORAL Proxy Application \n LLVM'15 - Integrating GPU Support for OpenMP Ofﬂoading Directives into Clang \n LLVM'14 - Coordinating GPU Threads for OpenMP 4.0 in LLVM \n \n Programming Models \n \n CGO'21 - C-for-metal: high performance SIMD programming on intel GPUs \n ECRTS'19 - Novel Methodologies for Predictable CPU-To-GPU Command Offloading \n ASPLOS'14 - Paraprox: Pattern-Based Approximation for Data Parallel Applications \n \n Profile Guided Optimization \n \n Geometry and Optimization'21 - Cooperative Profile Guided Optimizations \n IPDPS'13 - Kernel Specialization for Improved Adaptability and Performance on Graphics Processing Units (GPUs) \n \n Binaries \n \n CGO'19 - Decoding CUDA binary \n ISCA'15 - Flexible software profiling of GPU architectures \n", 'Checkee-helper \n Analyze tool for www.checkee.info \n Usage \n Step 1: Copy data from www.checkee.info, and save it as a  csv  file \n Step 2: Execute main.py \n     python main.py [function] [input_file] [location]\n \n Functions: \n \n days: Number of checked people each day \n rate: Rate of passed people \n \n Locations: \n \n ShangHai \n BeiJing \n GuangZhou \n Europe \n ChengDu \n ShenYang \n \n Sample \n \n', 'Concurrent-Data-Structures \n A Set of Concurrent Data Structures in Java \n Quadboost: A Scalable Concurrent Quadtree: \n http://ieeexplore.ieee.org/document/8066340/', 'CUPTI-Examples', 'hpctoolkit-gpu-samples \n Test cases to validate the correctness of hpctoolkit for GPU-accelerated applications. \n Usage \n Clone \n git clone --recursive https://github.com/Jokeren/hpctoolkit-gpu-samples\n \n Setup \n export OMP_NUM_THREADS=<#threads>\nexport HPCTOOLKIT_GPU_TEST_REP=<#repeat times>\n \n Run \n CUDA Programs \n cd <sample path>\nmake ARCH=<GPU arch>\n./<application name> <device id (default 0)>\n \n OpenMP Programs \n cd <sample path>\nmake SHOWFLAGS="-L<OpenMP path> -lomp"\nLD_LIBRARY_PATH=<OpenMP path>:$LD_LIBRARY_PATH ./<application name> <device id (default 0)>\n \n Runtime Behaviors \n | Case                          | Purpose                       |\n|---------------------------------|---------------------------------|\n| cu_multi_contexts_multi_streams  | Within each context, multiple CPU threads launch kernels to streams concurrently | \n Launch Patterns \n | Case                          | Purpose                       |\n|-----------------------------|-----------------------------|\n| cuda_vec_add  |  cudaLaunchKernel |\n| cuda_cooperative_group |  cudaLaunchCooperativeKernel  |\n| cu_vec_add  |  cuLaunchKernel |\n| cu_multi_entries  |  cuLaunchKernel  for difference kernels with the same calling context |\n| cu_cooperative_group |  cuLaunchCooperativeKernel  (ERROR) |\n| target_vec_add  |  omp target  | \n Call Trees \n | Case                          | Purpose                       |\n|-----------------------------|-----------------------------|\n| cu_call_path  | acyclic call graph |\n| cu_call_path_recursive  | recursive device function calls |\n| cu_call_path_recursive_mutual  | mutual recursive device function calls |\n| cu_call_path_long  | long call path with missing samples in the middle |\n| cu_call_path_thread_aware  | different CPU threads pass different parameters |\n| cuda_call_path_dynamic  | dynamic parallelism |\n| cuda_call_path_dynamic_recursive  | recursive call with dynamic parallelism | \n Bug Reports \n | Case                          | Purpose                       |\n|-----------------------------|-----------------------------|\n| nvdisasm  | nvdisasm correctness check samples |\n| cuobjdump  | cuobjdump correctness check samples |\n| cupti_test  | cupti_test correctness check samples | \n Verification \n | Case                          | Purpose                       |\n|-----------------------------|-----------------------------|\n| cuda_pc_sampling_tuning  | pc sampling is performed on all SMs independently |\n| cuda_shared_memory_stall  | no stall reason indicates shared memory latency | \n Applications \n | Case                          | Purpose                       |  URL  |\n|-----------------------------|-----------------------------|----|\n| Laghos | large-scale application;  RAJA  and  CUDA  programming model comparison |https://github.com/CEED/Laghos|\n| target_lulesh  |  OMP Target  performance |https://computation.llnl.gov/projects/co-design/lulesh|\n| RAJAPerf  |  CUDA  and  RAJA  performance test suite |https://github.com/LLNL/RAJAPerf|\n| sw4  | 3-D seismic modeling |https://github.com/geodynamics/sw4|\n| cuda_tensor_contraction | nekbone | https://nek5000.mcs.anl.gov/|\n| cuda_tensor_transpose | ExaTENSOR | https://iadac.github.io/projects/|\n| target_tensor_transpose |  OMP Target  version of ExaTENSOR | https://iadac.github.io/projects/|', "Academic Kickstart \n Academic  is a framework to help you create a beautiful website quickly. Perfect for personal, student, or academic websites.  Check out the latest demo  of what you'll get in less than 10 minutes or  view the documentation . \n Academic Kickstart  provides a minimal template to kickstart your new website by following the simple steps below. \n \n Getting Started \n The following two methods describe how to install in the cloud using your web browser and how to install on your PC using the Command Prompt/Terminal app. \n Quick install using your web browser \n \n Install Academic with Netlify \n Netlify will provide you with a customizable URL to access your new site \n \n \n On GitHub, go to your newly created  academic-kickstart  repository and edit  config.toml  to personalize your site. Shortly after saving the file, your site will automatically update \n Read the  Quick Start Guide  to learn how to add Markdown content. For inspiration, refer to the  Markdown content  which powers the  Demo \n \n Install on your PC \n Prerequisites: \n \n Download and install Git \n \n Download and install Hugo \n \n \n Clone (or  Fork  or  download ) the  Academic Kickstart  repository with Git:  \n git clone https://github.com/sourcethemes/academic-kickstart.git My_Website \n Note that if you forked Academic Kickstart, the above command should be edited to clone your fork. \n \n \n Initialize the theme: \n cd My_Website\n   git submodule update --init --recursive \n \n \n View your new website: \n hugo server \n Now you can go to  localhost:1313  and your new Academic powered website should appear. \n \n \n Read the  Quick Start Guide  to learn how to add Markdown content, customize your site, and deploy it. \n \n \n License \n Copyright 2017  George Cushen . \n Released under the  MIT  license. \n", 'vim-config', 'cuda-gdb-samples', 'tvm-samples', 'COMP533 Final Project \n Dev Info \n Data preparation \n https://datasets.imdbws.com/ \n First put the downloaded data with .tsv suffixes into a directory such as  ./data . \n Code organization \n ./python # python files for data sampling and cleaning\n./sql # sql files for relation definitions and analyses\n \n Data Sample \n python ./python/subsample.py [data_dir] [sample_ratio (0, 1]]\n \n Data Clean \n python ./python/clean.py [sample_data_dir]\n', 'tls_test \n _dl_allocate_tls function test on lassen', "Academic Kickstart \n Academic  is a framework to help you create a beautiful website quickly. Perfect for personal, student, or academic websites.  Check out the latest demo  of what you'll get in less than 10 minutes or  view the documentation . \n Academic Kickstart  provides a minimal template to kickstart your new website by following the simple steps below. \n \n Getting Started \n The following two methods describe how to install in the cloud using your web browser and how to install on your PC using the Command Prompt/Terminal app. \n Quick install using your web browser \n \n Install Academic with Netlify \n Netlify will provide you with a customizable URL to access your new site \n \n \n On GitHub, go to your newly created  academic-kickstart  repository and edit  config.toml  to personalize your site. Shortly after saving the file, your site will automatically update \n Read the  Quick Start Guide  to learn how to add Markdown content. For inspiration, refer to the  Markdown content  which powers the  Demo \n \n Install on your PC \n Prerequisites: \n \n Download and install Git \n \n Download and install Hugo \n \n \n Clone (or  Fork  or  download ) the  Academic Kickstart  repository with Git:  \n git clone https://github.com/sourcethemes/academic-kickstart.git My_Website \n Note that if you forked Academic Kickstart, the above command should be edited to clone your fork. \n \n \n Initialize the theme: \n cd My_Website\n   git submodule update --init --recursive \n \n \n View your new website: \n hugo server \n Now you can go to  localhost:1313  and your new Academic powered website should appear. \n \n \n Read the  Quick Start Guide  to learn how to add Markdown content, customize your site, and deploy it. \n \n \n License \n Copyright 2017  George Cushen . \n Released under the  MIT  license. \n", '', "GVProf \n \n \n \n GVProf is a value profiler for NVIDIA GPUs to explore value-related inefficiencies in GPU-accelerated applications. \n Quick Start \n ```bash\ngit clone --recursive git@github.com:Jokeren/GVProf.git && cd GVProf \n Install gvprof \n ./bin/install \n Setup environment variables \n export GVProfInstall=$(pwd)/gvprof\nexport PATH=${GVProfInstall}/bin:$PATH\nexport PATH=${GVProfInstall}/hpctoolkit/bin:$PATH\nexport PATH=${GVProfInstall}/redshow/bin:$PATH \n If your GPU driver is greater than 470.57, this environment variable is necessary to set. \n export SANITIZER_DISABLE_PARALLEL_LAUNCHES=1 \n Test a sample \n cd samples/vectorAdd.f32\nmake\ngvprof -e redundancy ./vectorAdd\n``` \n Documentation \n \n Installation Guide \n User's Guide \n Developer's Guide \n \n Papers \n \n Keren Zhou, Yueming Hao, John Mellor-Crummey, Xiaozhu Meng, and Xu Liu.  GVProf: A Value Profiler for GPU-based Clusters . In:  The International Conference for High Performance Computing, Networking, Storage, and Analysis  (SC), 2020 \n Keren Zhou, Yueming Hao , John Mellor-Crummey, Xiaozhu Meng, and Xu Liu.  ValueExpert: Exploring Value Patterns in GPU-accelerated Applications . In:  Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems*(ASPLOS), 2022 (Keren and Yueming are co-first authors) \n", 'cori-gpu-report \n An example to show CUPTI API does not work on cori-gpu nodes. \n Reproduce steps \n module load esslurm\nsalloc -C gpu -N 1 -t 30 -c 10 --gres=gpu:1 -A m1759\nmake\nulimit -c unlimited\nsrun ./pc_sampling\n', "sanitizer-bug-reproducer \n Use empty instrumentors to show the bugs of NVIDIA's compute sanitizer \n Usage \n git clone https://github.com/Jokeren/sanitizer-bug-reproducer\nconfig cuda path in *.sh\nmake\n./run_blackscholes.sh\n./run_srad.sh\n", 'TinySAT', 'nvprof-overhead \n Prerequisites \n \n nvcc ,  gcc ,  mpirun ,  cmake  commands are on your path \n gcc>=6.4.0 ,  cmake>=3.3.0 ,  nvcc>=10.0 \n \n Build \n \n export CUDA_HOME=/path/to/cuda , e.g.  export CUDA_HOME=/usr/local/cuda-10.1 \n export MPI_HOME=/path/to/mpi , e.g.  export MPI_HOME=/usr/local/mpi \n ./build.sh \n \n Tests \n ./run_cupti.sh\n./run_nvprof.sh\n', 'nvbit-call-stack', 'CUDA-CFG-10.1 \n We used CUDA-10.1. \n We used the following command to dump CFGs: \n nvdisasm -cfg -poff <cubin_name>\n \n Stream_COPY.cfg  is the CFG of the unoptimized (-O0 -G)  Stream_COPY  kernel in RAJAPerf Suite. \n tensor_contraction.cfg  is the CFG of the optimized (-O3 -lineinfo)  tensor_contraction  kernel in ExaTensor. \n Problem 1 \n There are some dangling blocks not connected with other blocks. \n We are okay with the current representation because the problem is really with boost APIs.  \n In  tensor_contraction.cfg ,  .L22  is a dangling block. The block is supposed to be linked by  .L_11 . It makes sense to separate this block from other blocks because the instruction before it is an EXIT instruction, meaning that the block is never executed. \n It is not only the last block has the chance to become a dangling block. In  Stream_COPY.cfg ,  .L_15  and  .L_14  are dangling blocks. While  .L_15  is the last block,  .L_14  is an internal block. \n Our control flow analyzer uses boost graphviz API to parse a dot graph. The API returns a set of nodes in the dot graph without identifying which subgraph each node belongs to. Therefore, if there are several dangling blocks with the same instruction address, we cannot associate them with the corresponding subgraph, resulting in instruction "gaps." Currently, we just fill in NOP instructions for these gaps. \n If nvdisasm links these dangling blocks in the output CFGs, we can construct accurate control flow graphs. If not, it is still fine because instructions in these blocks are mostly not important. \n Problem 2 \n Blocks in the CFGs are not " basic blocks ." \n In compiler construction, a basic block is a straight-line code sequence with no branches in except to the entry and no branches out except at the exit.\n \n In  tensor_contraction.cfg , however, blocks are not divided by branch instructions. \n In comparison, in  Stream_COPY.cfg , most blocks end with branch instructions. \n I suppose it is the NVCC compiler that merges basic blocks in the optimized version so that nvdisasm outputs "super" blocks in the CFGs. \n We need the real basic block representation since our control flow analyzer identifies loop nests in CFGs. The loop analysis algorithm requires each basic block is a sequential code.', 'GPA \n G PU  P erformance  A dvisor \n \n GPA is a performance advisor for NVIDIA GPUs that suggests potential code optimization opportunities at a hierarchy of levels, including individual lines, loops, and functions. GPA uses data flow analysis to approximately attribute measured instruction stalls to their root causes and uses information about a program\'s structure and the GPU to match inefficiency patterns with suggestions for optimization. GPA estimates each optimization\'s speedup based on a PC sampling-based performance model. \n Quick Start \n bash\ngit clone --recursive https://github.com/Jokeren/GPA.git && cd GPA\n./bin/install.sh\n./bin/bench.sh rodinia/bfs \n Documentation \n \n Installation Guide \n User\'s Guide \n [Developer\'s Guide] \n \n Papers \n \n K. Zhou, X. Meng, R. Sai, D. Grubisic and J. Mellor-Crummey, "An Automated Tool for Analysis and Tuning of GPU-accelerated Code in HPC Applications."  IEEE Transactions on Parallel and Distributed Systems  (TPDS) (2021). \n K. Zhou, X. Meng, R. Sai and J. Mellor-Crummey, "GPA: A GPU Performance Advisor Based on Instruction Sampling,"  2021 IEEE/ACM International Symposium on Code Generation and Optimization  (CGO), Seoul, Korea (South), 2021, pp. 115-125, doi: 10.1109/CGO51591.2021.9370339. \n', 'GPA-Benchmark \n Benchmark applications for GPU Performance Advisor \n Experimental Result \n Platform:  Volta V100 \n |       Application         |           Kernel              |         Optimization          | Original  | Optimized     | Speedup   | Estimate Speedup  | Error     |\n|:----------------------:   |:--------------------------:   |:----------------------------: |:--------: |:---------:    |:-------:  |:----------------: |:-----:    |\n|    rodinia/backprop       |   bpnn_layerforward_CUDA      |         Warp Balance          |  18.10us  |  15.36us      |  1.18x    |       1.21x       |   2%      |\n|    rodinia/backprop       |   bpnn_layerforward_CUDA      |      Strength Reduction       |  15.32us  |  12.63us      |  1.21x    |       1.13x       |   7%      |\n|       rodinia/bfs         |           Kernel              |        Loop Unrolling         | 578.28us  |  508.54us     |  1.14x    |       1.59x       |  28%      |\n|     rodinia/b+tree        |         findRangeK            |         Code Reorder          |  53.29us  |  46.40us      |  1.15x    |       1.28x       |  10%      |\n|       rodinia/cfd         |      cuda_compute_flux        |           Fast Math           | 187.53ms  |  128.37ms     |  1.46x    |       1.54x       |   5%      |\n|    rodinia/gaussian       |            Fan2               |        Thread Increase        | 116.76ms  |  30.21ms      |  3.86x    |       3.33x       |  16%      |\n|    rodinia/heartwall      |           kernel              |        Loop Unrolling         |  49.03ms  |  42.35ms      |  1.16x    |       1.15x       |   1%      |\n|     rodinia/hotspot       |       calculate_temp          |      Strength Reduction       |  15.45us  |  13.40us      |  1.15x    |       1.10x       |   5%      |\n|     rodinia/huffman       | vlc_encode_kernel_sm64huff    |         Warp Balance          | 133.24us  |  121.59us     |  1.10x    |       1.17x       |   6%      |\n|     rodinia/kmeans        |         kmeansPoint           |        Loop Unrolling         | 787.14us  |  700.73us     |  1.12x    |       1.21x       |   7%      |\n|     rodinia/lavaMD        |       kernel_gpu_cuda         |        Loop Unrolling         |  4.07ms   |   3.61ms      |  1.11x    |       1.12x       |   1%      |\n|       rodinia/lud         |        lud_diagonal           |         Code Reorder          | 221.81us  |  162.96us     |  1.36x    |       1.48x       |   8%      |\n|     rodinia/myocyte       |          solver_2             |           Fast Math           | 308.55ms  |  259.63ms     |  1.19x    |       1.13x       |   5%      |\n|     rodinia/myocyte       |          solver_2             |       Function Spliting       | 259.69ms  |  254.47ms     |  1.02x    |       1.03x       |   1%      |\n|       rodinia/nw          |    needle_cuda_shared_1       |         Warp Balance          | 840.70us  |  762.70us     |  1.10x    |       1.09x       |   1%      |\n| rodinia/particlefilter    |      likelihood_kernel        |        Block Increase         |  2.34ms   |   1.21ms      |  1.92x    |       1.93x       |   1%      |\n|  rodinia/streamcluster    |     kernel_compute_cost       |        Block Increase         |  21.51ms  |  14.17ms      |  1.52x    |       1.46x       |   4%      |\n|     rodinia/sradv1        |           reduce              |         Warp Balance          |  2.01ms   |   1.95ms      |  1.03x    |       1.16x       |  11%      |\n|   rodinia/pathfinder      |       dynproc_kernel          |         Code Reorder          |  93.48us  |  88.67us      |  1.05x    |       1.23x       |  15%      |\n|       Quicksilver         |     CycleTrackingKernel       |       Function Inlining       |   1.18s   |   1.05s       |  1.12x    |       1.18x       |   5%      |\n|       Quicksilver         |     CycleTrackingKernel       |        Register Reuse         |   1.05s   |   1.02s       |  1.03x    |       1.04x       |   1%      |\n|        ExaTENSOR          |      tensor_transpose         |      Strength Reduction       |  5.46ms   |   5.08ms      |  1.07x    |       1.06x       |   1%      |\n|        ExaTENSOR          |      tensor_transpose         | Memory Transaction Reduction  |  5.08ms   |   4.91ms      |  1.03x    |       1.05x       |   2%      |\n|        PeleC            | pc_expl_reactions           |         Block Increase        |  440.12ms |   370.34ms |  1.19x   |       1.23x       |   3%   |\n|        Minimod          | target_pml_3d               |         Fast Math             |  89.12ms  |   86.31ms |  1.03x   |       1.09x       |   6%   |\n|        Minimod          | target_pml_3d               |         Code Reorder          |  86.31ms  |   82.07ms |  1.05x   |       1.10x       |   5%   |', 'pytorch-fixes \n Platform \n \n CPU: 2 x Intel(R) Xeon(R) CPU E5-2695 v4 \n GPU: NVIDIA Tesla V100 16GB \n \n Fixes \n \n SpatialConvolutionMM \n \n Tests:  1-spatial-convolution-model.py  (1.05x),  1-spatial-convolution-unit.py  (1.16x) \n https://github.com/pytorch/pytorch/issues/48539 \n \n ReplicationPad \n \n Tests:  2-replication-pad3d.py  (1.15x) \n https://github.com/pytorch/pytorch/issues/48889', 'triton-samples', 'Fast Normalized Cut for Image Segmentation \n Packages \n The following packages are required to run the pipeline: \n bash\npip install opencv-python scikit-image tqdm seaborn \n Preprocess \n bash\npython preprocess.py --data-dir ../data/train --output-dir ../data/train/processed --stride 32 --size 128 --denoise\npython preprocess.py --data-dir ../data/test --output-dir ../data/test/processed --stride 32 --size 128 --denoise\npython preprocess.py --data-dir ../data/valid --output-dir ../data/valid/processed --stride 32 --size 128 --denoise \n U-Net \n Train \n bash\npython run.py --mode train --experiment supervised --data-dir ../data/ --model-dir ../data/ --epochs 10 --partial <ratio>\npython run.py --mode test --experiment supervised --data-dir ../data/ --model-dir ../data/ --epochs 10 --partial <ratio> \n W-Net \n bash\npython unsupervised.py --mode test --experiment unsupervised --data-dir ../data/ --model-dir ../data/ \npython unsupervised.py --mode train --experiment unsupervised --data-dir ../data/ --model-dir ../data/ \n SU-Net \n bash\npython run.py --mode train --experiment semi-supervised --data-dir ../data/ --model-dir ../data/ --epochs 5 --partial <rate> --mu <mu>\npython run.py --mode test --experiment semi-supervised --data-dir ../data/ --model-dir ../data/ --epochs 5 --partial <ratio> --mu <mu>']
euler16,['Geeks-Scraping \n Script to scrap Geeks for Geeks.', 'CollegeSpace-downloader \n Downloads course mateial and past papers ', 'Competitive_Programming \n Algorithms, Data Structures and problems', 'Face-Recognizer \n A Near "from scratch" implementation of face recognition system \n TO-DO \n A few things were left to be done in future\n1.improve the interface\n    Apart from header files everything else is scattered(apologies)\n2.Bring in GUI\n3.Use a better classification algorithm \n Dataset obtained from:- \n Olivetti Research Laboratory in Cambridge, UK\n \n Future motivation:- \n implement a robust and Intelligent visualisation system. \n Prerequisites:- \n COE-115 - Quantum Mechanics :D\nCOE-112 - Linear Algebra (Matrices and Determinants) \n Features not implemented from scratch:- \n 1.Mat datatype (required for fast matrix calculation)\n2.PCA class (required for finding eigenvectors of covariance matrix of the training set)\n3.few opencv classes not involved in the Algorithm\n4.Haar Cascade classifier (in facedetect.h), used for finding faces in image', 'Udacity-Deep-Learning \n my IPy notebooks and other files made during the course ', 'Implementation of "A Neural Algorithm of Artistic Style" \n Results: \n \n \n Using the StarryNight: \n \n How to run \n Dependencies: \n \n TensorFlow \n Scipy \n Numpy \n \n You will need to download the  VGG-19 model . \n References:\n-  A Neural Algorithm of Artistic Style \n-  https://github.com/jcjohnson/neural-style', 'Principle-of-Communication-Engineering-Lab \n Matlab code for the Lab \n Contribution guidelines :- \n \n Files should be arranged according to classes. \n Word files should be separate from matlab files. \n Same files should not be added again . \n', 'Personal blog \n euler16.github.io \n \nIf you have any doubts regarding the blog posts, file an issue here.', 'Char RNN \n This repository contains PyTorch code for training and visualising (through heatmaps) Recurrent Neural Networks. \n Recurrent Neural Networks \n Recurrent Nets are Neural Network architecure for modelling sequences. The code in the repository is about the 2 main variants of RNN :-  LSTM and GRU .  \nThe working of RNN can be summarized in the following gif\n \n The input to the RNN in our case is vectorized representation of characters. \n Installation \n Create and activate a virtualenv. \n \n virtualenv charrnn \n source charrnn/bin/activate \n \n Install    PyTorch   \n(see requirements.txt for the version, listed there  as torch)\n \nClone the repository \n \n git clone https://github.com/euler16/CharRNN.git \n cd CharRNN \n \n Install other dependencies \n \n pip install -r requirements.txt \n \n Note:- not all dependencies mentioned in the requirements.txt file are required. \n Running the code \n The model contains 3 main folders:- efficient, simple, visualisation.\nThe simple folder contains code for implementing an RNN from scratch (without using the LSTM/GRU module from Pytorch). The efficient folder contains code using the modules from PyTorch and also uses embedding table. The visualisation folder contains code for generating heatmaps.\n \nIn all three repositories run  \n \n python train.py --path/to/data \n \n Check the argument parser code to know more about the arguments available \n Visualization \n \nIn the visualisation folder, train a model using \n \n python vis_generate.py \n \n Then run \n \n python heatmap_plot.py \n \n Please see the argument parsers as well as the files that are being loaded and saved!! (if you are still facing problems file an issue here (code and installation related issues only)). \n This code has been used for    this    blog post.', 'PyData2017 \n Code and Slides for PyData Conference (India 2017)', "WeatherApp \n \n Purpose \n This is a prototype weather application to be used as a reference for the students at CB. \n Anatomy \n This application makes use of a few technologies: \n \n Google Maps API \n Used for the map and allowing the user to locate themselves or select a location the view the weather of \n VueJS \n The MVC framework to build the application \n ChartJS \n The charting library to create a graph to show the maximum and minimum temperature over the days \n DarkSky API \n Weather API to pull weather data from \n Materialize \n Front-end framework following Google's Material Design specifications \n \n Note : The DarkSky API key currently present in the code has been reset so you will need to request your own individual key to be able to retrieve the weather data. This can be done by creating an account at  their website", 'Tweeper \n Desktop app based on Twitter API for instant tweeting!!. \n The app is my first attempt at building anything with  Electron.js . \nElectron is an open source library developed by GitHub for building cross-platform desktop applications with HTML, CSS, and JavaScript. Electron accomplishes this by combining Chromium and Node.js into a single runtime and apps can be packaged for Mac, Windows, and Linux. \n Setup \n \n Login to apps.twitter.com . \n Create new app and go to keys tabs. Save the key to your  Environment Variables  as CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN and ACCESS_TOKEN_SECRET. \n \n Installation \n ```bash \n Clone this repository \n git clone https://github.com/euler16/Tweeper \n Go into the repository \n cd Tweeper \n Install dependencies \n npm install \n Run the app \n npm start\n``` \n A Note on Environment Variables \n If you are having problems setting up Environment Variables, check out these guides: \n \n Linux \n Mac  (For a GUI, check out  EnvPane ) \n Windows \n', 'GraphicsLab \n This repository contains programmes implemented in our graphics course at NSIT.', 'Deep-RL \n Practising Reinforcement Learning', 'Torched.js \n Yet another Deep Learning Javascript Library :)', "Node-Tutorials \n This repository contains code snippets that I used/saved/wrote while learning JavaScript and Node.js. Placed in this repository for archival purpose. \nI haven't pushed the node_modules folders since they are quite large so remember to npm install all the dependences before working/tinkering.", 'PyData-2018 \n Slides and Code for my talk on Quantum Computation at PyData India (August 12).', '\n \n \n \n Quantum Computing for Humans! \n ... \n Qu.js is a JavaScript first Quantum Computing framework . \n Motivation \n Currently the Quantum Computing environment, though in its nascent stage, is dominated by 2 Python based Quantum Computing frameworks  Qiskit  and  PyQuil  developed by  IBM  and  Rigetti  respectively. These libraries are in turn high-level versions of Assembly like languages QASM (Qiskit) and Quil (PyQuil) which enable users to run their Quantum programs on actual Quantum Computers being developed by these companies. \nQu.js aims to be a common JavaScript frontend that allows developers to  write  backend agnostic Quantum Programs .  In this sense, the motive behind Qu.js is similar to  Keras  in Machine Learning .   \n \n Moreover I developed this library in order to understand the basics of Quantum Computing properly :). \n \n Functionality \n Currently Qu.js provides (or aims to provide) the following functionalities: \n \n [x] A local Quantum Simulator that can run on browser as well as Node.js \n [x] Compilation of JS code into QASM and Quil \n [ ] Circuit Visualization \n [ ] Saving and Loading Circuit \n [ ] QASM to Quil \n [ ] A Twitter based interface (working on it :) ) \n [ ] Support  Cirq  backend (another Quantum Computing Library) \n', 'Playing-with-Qunatum-Computing \n Trying out different libraries, notes, codes', 'Distinguishing-Unitary-Gates-on-IBM-Quantum-Processor \n Qiskit 0.6 implementation of https://arxiv.org/abs/1807.00429   \n Background \n The paper is about distinguishing phase change gate and identity gate using a Quantum Circuit. The author discusses two schemes - \n \n Parallel - using two qubits and applying the gate on both qubits  \n Sequential - using a single qubit  \n \n The circuit is then tested on IBM Quantum Processor. \nTo run the Jupyter Notebook you will need to install Python 3.5+, qiskit 0.6 and an account on [ IBM Q ].', 'Image-Captioning \n To run, do the following \n \n clone the repo locally \n execute\n    > python -m SimpleHTTPServer \n', 'SocialViz \n Visualizer', 'NewBlogWork \n Coming soon!', 'Demo', "Lipreading  Chainer \n \nThis is the chainer code for the paper  Combining Residual Networks with LSTMs for Lipeading . You can find the paper  here . \nThe authors present a word level lipreading model based on Resnets. The input to the model is a silent video and the model then outputs the word it thinks was spoken. In the paper this task of visual speech recognition has been modelled as video classification. \n The code is based on PyTorch implementation of the same work which can be found  here . \n Dataset \n The model has been trained on Oxford-BBC  Lip Reading in the Wild (LRW)  dataset. The dataset consists of short video clips of news anchor speaking a single word. The words dictionary size is 500. The dataset contains about 1000 utterances of each of the 500 words. Dataset size is around 70GB. \n How to Run \n \n Download the LRW dataset from  this website \n Preprocess the dataset as given in the PyTorch counterpart of this repo (available  here ) \n Write the appropriate dataset path in config.json \n Run the following command :-\n python main.py --config config.json \n after the training is over, change the mode variable in config.json to 'backendGRU' and run the above command. \n Finally fine tune the model by switching the mode to 'finetuneGRU'. \n \n Make sure you change the path variable to saved model location after step 4. \n TODOs \n \n [x] Chainer code, tested \n [x] Tested on CPU \n [ ] Making it work on GPU  \n", 'SketchRNN-Chainer \n Implementation of A Neural Representation of Sketch Drawings', 'udacity-clouddevops-proj1 \n static site hosting project', 'Exploration', 'pdfzee', 'Starter kit for  Alembic  with Forestry \n This is a very simple starting point if you wish to use Alembic  as a Jekyll theme gem  with Forestry. \n \n', 'Starter kit for  Alembic  with Forestry \n This is a very simple starting point if you wish to use Alembic  as a Jekyll theme gem  with Forestry. \n \n', 'equivarient-poset-representation']
pjeide,[]
Netherdrake,['Supported Workflows \n \n Ubuntu local: i3 + fish + neovim   \n Ubuntu remote: tmux + fish + neovim   \n Ubuntu remote minimal: Regular vim only, no plugins \n \n Documentation  for OS related setups.\nInstall scripts are available for Ubuntu only. \n Vim Language Support \n Rust  (Great) \nR, Python  (Good)   \n Notes \n Tmux + Vim Configs \n tmux.conf  goes into  ~/.tmux.conf \n nvim/  and  fish/  go into  ~/.config/(nvim|fish) \n Install Tmux Plugin Manager \n git clone https://github.com/tmux-plugins/tpm ~/.tmux/plugins/tpm \n Tmux && Linux \n Make sure you have  xsel  or  xclip  installed. \n Then, install this plugin: \nhttps://github.com/tmux-plugins/tmux-yank \n Powerline fonts \n Install this: \nhttps://github.com/powerline/fonts \n Currently using  14pt Roboto Mono for Powerline. \n Re-compiling YCM (YouCompleteMe) \n cd ~/.vim/plugged/YouCompleteMe\n./install.py  \\\n    --clang-completer \\\n    --rust-completer', 'Dockerfile that takes latest Ubuntu as base and installs latest version of Go, Node.js, npm and\nbower.', 'Dockerfile that takes latest Ubuntu as base and installs latest version of Go, Node.js, npm and\nbower.', 'First Meteor project.', 'mongodb-backup \n \n This image extends the excellent [tutumcloud/monogodb-backup](https://github.com/tutumcloud/mongodb-backup) project.\n \n This image syncs data between two running mongodb databases by running mongodump on the backup database and importing it to the restore mongodb database. Intermediate database dumps are saved to  /backup  and can optionally be backed up to Amazon S3. \n This image may also be used for individual backup and/or restore functionality. \n Usage: \n docker run -d \\\n    --env MONGODB_BACKUP_HOST=mongodb.backup.host \\\n    --env MONGODB_BACKUP_PORT=27017 \\\n    --env MONGODB_BACKUP_USER=admin \\\n    --env MONGODB_BACKUP_PASS=password \\\n    --env MONGODB_RESTORE_HOST=mongodb.restore.host \\\n    --env MONGODB_RESTORE_PORT=27017 \\\n    --env MONGODB_RESTORE_USER=admin \\\n    --env MONGODB_RESTORE_PASS=password \\\n    --env AWS_ACCESS_KEY_ID=changeme \\\n    --env AWS_SECRET_ACCESS_KEY=changeme \\\n    --env AWS_DEFAULT_REGION=us-east-1 \\\n    --env S3_BUCKET=changeme \\\n    --env S3_PATH=mongodb \\\n    --env S3_BACKUP=yes \\\n    --volume host.folder:/backup \\\n    --name mongodb-sync \\\n    agaveapi/mongodb-sync \n Moreover, if you link  agaveapi/mongodb-sync  to a mongodb container(e.g.  tutum/mongodb ) with an alias named mongodb-backup, this image will try to auto load the source  host ,  port ,  user ,  pass  if possible. The same is true if you link  agaveapi/mongodb-sync  to a mongodb container(e.g.  tutum/mongodb ) with an alias named mongodb-restore, this image will try to auto load the destination  host ,  port ,  user ,  pass  if possible. \n docker run -d -p 27017:27017 -p 28017:28017 -e MONGODB_PASS="mypass" --name mongodb-backup tutum/mongodb\ndocker run -d -p 37017:27017 -p 38017:28017 -e MONGODB_PASS="mypass" --name mongodb-restore tutum/mongodb\ndocker run -d --link mongodb-backup:mongodb-backup --link mongodb-restore:mongodb-restore -v host.folder:/backup agaveapi/mongodb-sync \n Parameters \n MONGODB_BACKUP_HOST    the host/ip of the mongodb database you wish to backup\nMONGODB_BACKUP_PORT    the port number of the mongodb database you wish to backup\nMONGODB_BACKUP_USER    the username of the mongodb database you wish to backup. If MONGODB_BACKUP_USER is empty while MONGODB_BACKUP_PASS is not, the image will use admin as the default backup database username\nMONGODB_BACKUP_PASS    the password of the mongodb database you wish to backup\nMONGODB_BACKUP_DB      the database name to dump. If not specified, it will dump all the databases\nEXTRA_BACKUP_OPTS      the extra options to pass to mongodump command\n\nMONGODB_RESTORE_HOST    the host/ip of the mongodb database you wish to backup\nMONGODB_RESTORE_PORT    the port number of the mongodb database you wish to backup\nMONGODB_RESTORE_USER    the username of the mongodb database you wish to backup. If MONGODB_RESTORE_USER is empty while MONGODB_RESTORE_PASS is not, the image will use admin as the default backup database username\nMONGODB_RESTORE_PASS    the password of the mongodb database you wish to backup\nMONGODB_RESTORE_DB      the database name to dump. If not specified, it will dump all the databases\nEXTRA_RESTORE_OPTS      the extra options to pass to mongodump command\n\nAWS_ACCESS_KEY_ID       The AWS access key for the account to which the backup will be made\nAWS_SECRET_ACCESS_KEY   The AWS secret key for the account to which the backup will be made\nAWS_DEFAULT_REGION      The default region for the backup bucket. Defaults to us-east-1\nS3_BUCKET               The name of the bucket where the backup will be copied.\nS3_PATH                 The path within the bucket where the database dump archive will be saved\nS3_BACKUP               If set, backups will be archived to S3.\nCRON_TIME               The interval of cron job to run mongodump. `0 0 * * *` by default, which is every day at 00:00\nMAX_BACKUPS             The number of backups to keep. When reaching the limit, the old backup will be discarded. No limit, by default. **Note: s3 backups will not be purged in this process. Select an expiration date in your bucket to enforce cloud backups.**\n\nINIT_BACKUP             If set, create a backup when the container launched\nINIT_RESTORE            If set, restore the most current backup when the container launched\nINIT_SYNC               If set, sync the two mongodb databases immediately when the container launched\n \n Run exclusively as a backup process \n To run this image only as a backup process: \n docker run -d \\\n    --env MONGODB_BACKUP_HOST=mongodb.backup.host \\\n    --env MONGODB_BACKUP_PORT=27017 \\\n    --env MONGODB_BACKUP_USER=admin \\\n    --env MONGODB_BACKUP_PASS=password \\\n    --volume host.folder:/backup \\\n    --name mongodb-sync \\\n    agaveapi/mongodb-sync backup \n To archive copies of the the backups to S3: \n docker run -d \\\n    --env MONGODB_BACKUP_HOST=mongodb.backup.host \\\n    --env MONGODB_BACKUP_PORT=27017 \\\n    --env MONGODB_BACKUP_USER=admin \\\n    --env MONGODB_BACKUP_PASS=password \\\n    --env AWS_ACCESS_KEY_ID=changeme \\\n    --env AWS_SECRET_ACCESS_KEY=changeme \\\n    --env AWS_DEFAULT_REGION=us-east-1 \\\n    --env S3_BUCKET=changeme \\\n    --env S3_PATH=mongodb \\\n    --env S3_BACKUP=yes \\\n    --volume host.folder:/backup \\\n    --name mongodb-sync \\\n    agaveapi/mongodb-sync backup \n Restore from a backup \n To see the list of backups in a running backup container, you can run: \n docker exec mongodb-sync ls /backup \n To restore a mongodb database from an existing backup on disk \n docker run -it --rm \\\n    --env MONGODB_RESTORE_HOST=mongodb.restore.host \\\n    --env MONGODB_RESTORE_PORT=27017 \\\n    --env MONGODB_RESTORE_USER=admin \\\n    --env MONGODB_RESTORE_PASS=password \\\n    --volume /existing/local/backup/folder:/backup \\\n    agaveapi/mongodb-sync /restore.sh /backup/2015.08.06.171901 \n Run as a one-off sync process \n If you have need to run one-off sync processes such as creating snapshots of your production db for testing in a QA environment, you can invoke this image as needed using the following command. \n docker run -d --rm \\\n    --env MONGODB_BACKUP_HOST=mongodb.backup.host \\\n    --env MONGODB_BACKUP_PORT=27017 \\\n    --env MONGODB_BACKUP_USER=admin \\\n    --env MONGODB_BACKUP_PASS=password \\\n    --env MONGODB_RESTORE_HOST=mongodb.restore.host \\\n    --env MONGODB_RESTORE_PORT=27017 \\\n    --env MONGODB_RESTORE_USER=admin \\\n    --env MONGODB_RESTORE_PASS=password \\\n    agaveapi/mongodb-sync /sync.sh \n If your existing mongo images are already running in containers, you can do the following: \n docker run -d --rm \\\n    --links mongodb-prod:mongodb-backup \\\n    --links mongodb-qa:mongodb-restore \\\n    agaveapi/mongodb-sync /sync.sh', "Where are Account, Post and other utilities? \n A big chunk of steemtools has been merged into  python-steem . ( documentation ) \n This library is a collection of things that don't quite fit into the main Python STEEM library. \n Installation \n pip install -U steemtools \n Documentation \n See the  source . \n ¯_(ツ)_/¯ \n There are also a few  examples .", 'Install Anaconda with Python 3.5 64bit \n https://www.continuum.io/downloads \n Clone the repo \n git clone https://github.com/cshrem/SteemPowerTrail\ncd SteemPowerTrail \n Install requirements \n pip install -r requirements.txt \n Add private posting keys to Piston wallet \n piston addkey 5..... \n Edit config.json. \n Example:\n {\n    "author_subscriptions": ["curie"],\n    "voter_subscriptions": ["curie"],\n    "reserve_voting_power": 90,\n    "sim_mode": false\n} \n Notes: \nIf  sim_mode  is true, the bot won\'t actually vote. It will just run as a simulation. \n Run the script \n UNLOCK=piston_wallet_password\npython autovote.py', 'How to replicate the issue in Meteor \n Run meteor:\n meteor \n and navigate to localhost:3000. \n Open developer console in browser. Refresh. \n See the error, generated as a failure to generate proper signature. \n How to run steem.js in browser \n Just open  imports/index.html  in browser. It uses steem.min.js from node_modules\nas installed by  meteor npm i .', 'Install:\n pip install -r requirements.txt \n Demo 1:\n python scraper.py \n Demo 2:\n python playground.py', 'Python Library for Steem \n Python 3 library for Steem! \n Stable \n \n \n \n \n \n \n Develop \n \n \n \n Installation \n Install with  pip : \n $ sudo apt-get install libffi-dev libssl-dev python-dev\n$ pip3 install steem\n \n Manual installation: \n $ git clone https://github.com/xeroc/python-steem/\n$ cd python-steem\n$ python3 setup.py install --user\n \n Upgrade \n $ pip install --user --upgrade\n \n Additional dependencies \n steemapi.steemasyncclient :\n *  asyncio==3.4.3 \n *  pyyaml==3.11 \n Documentation \n Thanks to readthedocs.io, the documentation can be viewed\n online \n Documentation is written with the help of sphinx and can be compile to\nhtml with: \n cd docs\nmake html\n', 'Charts \n Installation \n First, we need to install dependencies, by running:\n pip install -r requirements.txt \n Run as a notebook \n Start the Jupyter Notebook by running:\n jupyter notebook \n It will open the notebook in your browser. Start hacking on it :) \n Run as a script \n The notebook can also be executed from cli (useful for just refreshing plot.ly charts).\n runipy Charts.ipynb \n Caveats \n You can render the charts by staying in offline mode. Otherwise, you will need to sign up for\nplot.ly service and provide your API key.', 'A modern Piston fork \n steem-python  is a fork of the legendary  Piston  library by\n @xeroc . \n It features a refactored codebase, JSON-RPC support, new types and transactions,\nfull API coverage, and a handful of new features. \n Installation \n You can install  steem-python  with  pip : \n pip install -U git+git://github.com/Netherdrake/steem-python \n Warning: This is NOT the  official   steem-python  library.\nUse at own risk. \n Documentation \n Full documentation is available at  http://steem.readthedocs.io \n Example Uses \n Here are a few example scripts that utilize  steem-python . \n Syncing Blockchain to a Flat File \n Here is a relatively simple script built on top of  steem-python  that will let you sync STEEM blockchain into a simple file. You can run this script as many times as you like, and it will continue from the last block it synced. \n ```python \n import json\nimport os\nfrom contextlib import suppress\nfrom steem.blockchain import Blockchain \n def get_last_line(filename):\n    if os.path.isfile(filename):\n        with open(filename, \'rb\') as f:\n            f.seek(-2, 2)\n            while f.read(1) != b"\\n":\n                f.seek(-2, 1)\n            return f.readline() \n def get_previous_block_num(block):\n    if not block:\n        return -1 \n if type(block) == bytes:\n    block = block.decode(\'utf-8\')\n\nif type(block) == str:\n    block = json.loads(block)\n\nreturn int(block[\'previous\'][:8], base=16)\n \n def run(filename):\n    b = Blockchain()\n    # automatically resume from where we left off\n    # previous + last + 1\n    start_block = get_previous_block_num(get_last_line(filename)) + 2\n    with open(filename, \'a+\') as file:\n        for block in b.stream_from(start_block=start_block, full_blocks=True):\n            file.write(json.dumps(block, sort_keys=True) + \'\\n\') \n if  name  == \' main \':\n    output_file = \'/home/user/Downloads/steem.blockchain.json\'\n    with suppress(KeyboardInterrupt):\n        run(output_file)\n``` \n To see how many blocks we currently have, we can simply perform a line count. \n wc -l steem.blockchain.json \n We can also inspect an arbitrary block, and pretty-print it.  Replace 10000 with desired block_number + 1. \n sed \'10000q;d\' steem.blockchain.json | python -m json.tool \n Witness Killswitch \n Occasionally things go wrong: software crashes, servers go down... One of the main roles for STEEM witnesses is to reliably mint blocks. This script acts as a kill-switch to protect the network from missed blocks and prevents embarrassment when things go totally wrong. \n ```python\nimport time\nfrom steem import Steem \n steem = Steem() \n variables \n disable_after = 10  # disable witness after 10 blocks are missed\nwitness_name = \'furion\'\nwitness_url = "https://steemit.com/steemit/@furion/power-down-no-more"\nwitness_props = {\n    "account_creation_fee": "0.500 STEEM",\n    "maximum_block_size": 65536,\n    "sbd_interest_rate": 15,\n} \n def total_missed():\n    return steem.get_witness_by_account(witness_name)[\'total_missed\'] \n if  name  == \' main \':\n    treshold = total_missed() + disable_after\n    while True:\n        if total_missed() > treshold:\n            tx = steem.commit.witness_update(\n                signing_key=None,\n                url=witness_url,\n                props=witness_props,\n                account=witness_name) \n         print("Witness %s Disabled!" % witness_name)\n        quit(0)\n\n    time.sleep(60)\n \n ``` \n Batching Operations \n Most of the time each transaction contains only one operation (for example, an upvote, a transfer or a new post). We can however cram multiple operations in a single transaction, to achieve better efficiency and size reduction. \n This script will also teach us how to create and sign transactions ourselves. \n ```python\nfrom steem.transactionbuilder import TransactionBuilder\nfrom steembase import operations \n lets create 3 transfers, to 3 different people \n transfers = [\n    {\n        \'from\': \'richguy\',\n        \'to\': \'recipient1\',\n        \'amount\': \'0.001 STEEM\',\n        \'memo\': \'Test Transfer 1\'\n    },\n    {\n        \'from\': \'richguy\',\n        \'to\': \'recipient2\',\n        \'amount\': \'0.002 STEEM\',\n        \'memo\': \'Test Transfer 2\'\n    },\n    {\n        \'from\': \'richguy\',\n        \'to\': \'recipient3\',\n        \'amount\': \'0.003 STEEM\',\n        \'memo\': \'Test Transfer 3\'\n    } \n ] \n now we can construct the transaction \n we will set no_broadcast to True because \n we don\'t want to really send funds, just testing. \n tb = TransactionBuilder(no_broadcast=True) \n lets serialize our transfers into a format Steem can understand \n operations = [operations.Transfer(**x) for x in transfers] \n tell TransactionBuilder to use our serialized transfers \n tb.appendOps(operations) \n we need to tell TransactionBuilder about \n everyone who needs to sign the transaction. \n since all payments are made from  richguy , \n we just need to do this once \n tb.appendSigner(\'richguy\', \'active\') \n sign the transaction \n tb.sign() \n broadcast the transaction (publish to steem) \n since we specified no_broadcast=True earlier \n this method won\'t actually do anything \n tx = tb.broadcast()\n``` \n Simple Voting Bot \n Here is a simple bot that will reciprocate by upvoting all new posts that mention us. Make sure to set  whoami  to your Steem username before running. \n ```python\nfrom contextlib import suppress \n from steem.blockchain import Blockchain\nfrom steem.post import Post \n def run():\n    # upvote posts with 30% weight\n    upvote_pct = 30\n    whoami = \'my-steem-username\' \n # stream comments as they are published on the blockchain\n# turn them into convenient Post objects while we\'re at it\nb = Blockchain()\nstream = map(Post, b.stream(filter_by=[\'comment\']))\n\nfor post in stream:\n    if post.json_metadata:\n        mentions = post.json_metadata.get(\'users\', [])\n\n        # if post mentions more than 10 people its likely spam\n        if mentions and len(mentions) < 10:\n            post.upvote(weight=upvote_pct, voter=whoami)\n \n if  name  == \' main \':\n    with suppress(KeyboardInterrupt):\n        run()\n```', 'WIP \n This project is under  active  development. \n Tutorial \n https://steemdata.com/api', 'Mentions App \n This app allows you to search for posts from steemit.com \n Try it \n Preview \n', 'Hello world.', "Requirements \n Make sure you have docker installed, and that your user is in the docker group.\n usermod -aG docker $(whoami) \n Usage \n ```\n% ./run.sh\nUsage: ./run.sh COMMAND [DATA] \n Commands:\n    install - pulls latest docker image from server (no compiling)\n    build - only builds DECENT container (from docker file)\n    rebuild - builds DECENT container (from docker file), and then restarts it \n start - starts DECENT container\nstop - stops DECENT container\nrestart - restarts DECENT container\nreplay - starts DECENT container (in replay mode)\n\nwallet - open cli_wallet in the container\nenter - enter a bash session in the container\n\nstatus - show status of DECENT container\nlogs - show all logs inc. docker logs, and DECENT logs\n \n ``` \n Miner Setup \n First, lets build the container.\n ./run.sh build \n Add your miner-id and private-key to  config.ini .\n vim data/decentd/config.ini \n Start the container.\n ./run.sh start \n Accessing the cli_wallet \n After you've built your container, start it.\n ./run.sh start \n To attach to the running container, and access the wallet, simply run:\n ./run.sh wallet \n Thank you \n Pull Requests with fixes and improvements are welcome. \nIf you found this tool useful, please consider voting for miner  furion .\n vote_for_miner <your-account-name> furion true true \n Credits \n This project is based on someguy123's  peerplays-docker .", "Python EOS Base Library \n This is an unofficial Python EOS Base Library by  @furion . \nHeavily influenced by past works of Dr.-Ing. Fabian Schuh ( @xeroc ) \n Warning: This library is work-in-progress, and is not suitable for any use! \n See also \n \n py-eos-api  - Python wrapper for EOS API's \n \n Installation \n pip install -U git+https://github.com/Netherdrake/py-eos-base.git \n TODO: \n \n Base Types \n Operations \n Transaction Builder \n \n License \n MIT", 'python-bittrex \n Python bindings for bittrex.  I am Not associated -- use at your own risk, etc. \n Install \n pip install -U https://github.com/Netherdrake/python-bittrex', 'Python EOS Api Client \n This is an unofficial API wrapper by  @furion \n Installation \n pip install -U git+https://github.com/Netherdrake/py-eos-api \n Usage \n ```python \n \n \n \n from eosapi import Client\nc = Client(nodes=[\'http://localhost:8888\']) \n c.get_info() \n \n \n \n {\'head_block_id\': \'0000652e92c1f73e14503383ee18c28901dd301ff5be0b94c77d846d799d5050\',\n \'head_block_num\': 25902,\n \'head_block_producer\': \'initi\',\n \'head_block_time\': \'2017-09-16T04:25:18\',\n \'last_irreversible_block_num\': 25884,\n \'participation_rate\': \'1.00000000000000000\',\n \'recent_slots\': \'1111111111111111111111111111111111111111111111111111111111111111\'}\n \n \n \n \n c.get_account? \n \n \n \n Signature: c.get_account(name) -> dict\nDocstring: Fetch a blockchain account\nFile:      ~/GitHub/EOS/py-eos-api/eosapi/api.py\nType:      method\n \n \n \n \n c.get_account(\'inita\') \n \n \n \n {\'eos_balance\': \'1000000.0000 EOS\',\n \'last_unstaking_time\': \'1969-12-31T23:59:59\',\n \'name\': \'inita\',\n \'permissions\': [{\'name\': \'active\',\n   \'parent\': \'owner\',\n   \'required_auth\': {\'accounts\': [],\n    \'keys\': [{\'key\': \'EOS6MRyAjQq8ud7hVNYcfnVPJqcVpscN5So8BhtHuGYqET5GDW5CV\',\n      \'weight\': 1}],\n    \'threshold\': 1}},\n  {\'name\': \'owner\',\n   \'parent\': \'owner\',\n   \'required_auth\': {\'accounts\': [],\n    \'keys\': [{\'key\': \'EOS6MRyAjQq8ud7hVNYcfnVPJqcVpscN5So8BhtHuGYqET5GDW5CV\',\n      \'weight\': 1}],\n    \'threshold\': 1}}],\n \'staked_balance\': \'0.0000 EOS\',\n \'unstaking_balance\': \'0.0000 EOS\'}\n \n ``` \n You can also use a lower level  HttpClient  directly:\n```python\nfrom eosapi import HttpClient \n h = HttpClient(["http://localhost:8888"]) \n print(h.exec(\'chain\', \'get_block\', \'{"block_num_or_id": 5}\'))\nprint(h.exec(\'chain\', \'get_block\', {"block_num_or_id": 5}))\nprint(h.exec(\'chain\', \'get_info\'))\n``` \n You can also stream raw blocks (polling indefinitely):\n```python\nfrom eosapi import Client\nc = Client() \n for block in c.stream_blocks(start_block=100, mode=\'head\'):\n    print(block)\n``` \n TODO \n \n add support for type hints  (Union[NativeType, PythonType]) \n split api into submodules to avoid potential collisions \n apigen: load from json spec files once they are finalized \n \n License \n MIT', "Awesome EOS \n A curated list of awesome  EOS  frameworks, libraries, software and resources. \n EOS.io \n \n EOS Wiki  - High Level EOS Software Overview \n EOS Documentation  - Lower Level API Documentation \n EOS: An Introduction - Black Edition  - Ian Grigg's Whitepaper \n EOSIO Developer Portal  - Official EOSIO developer portal, with docs, APIs etc. \n \n Tools \n \n eos-wallet-app  - Web wallet for EOS \n eos-docker  - someguy123's EOS in a Box \n genesis  - Create an EOS Blockchain Genesis \n Cleos Auto Completion  - Command auto completion features for EOS Cleos. \n teos  - Alternative CLI to EOS, C++ API library \n eos-voter  - Desktop Light Wallet + Voting Application \n windshield  - EOS Nodes Dashboard: watch personal or public nodes and receive alerts if any node fails, chain fork etc. \n EOSDevHelper  - Cross-platform PC wallet. \n EOSBenchTool  - EOS pressure testing tool. \n PocketEOS-IOS  - Open source wallet for IOS. \n PocketEOS-Android  - Open source wallet for Android. \n EOS REACH Android  - Open source wallet for Android. \n \n Explorer \n \n EOSPark  - Powerful EOS Explorer \n EOSFlare  - EOS Block Explorer \n \n Language Support \n Go \n \n eosapi  - EOS' JSON API Wrapper for Golang \n \n JavaScript \n \n eosjs  - General purpose library for the EOS blockchain. \n eosjs-api  - EOS api wrapper for JS \n eosjs-ecc  - Elliptic curve cryptography functions \n eosjs-fcbuffer  - Serialization library for native data structures \n eosjs-json  - JSON schemas for EOS interfaces \n react-native-eos  - EOS for react-native with native cryptography \n eosjs-rn  - eosjs for react native \n eosjs-ecc-rn  - eosjs-ecc for react native \n demux-js  - Deterministic event-sourced state and side effect handling for blockchain applications \n \n Java / Kotlin \n \n eos-jvm  - General purpose library for the EOS blockchain, with a focus on building and pushing transactions. \n \n Python \n \n eosjs_python  - Python wrapper on eosjs \n eosapi  - EOS api wrapper for Python 3.6+ \n pyeos_client  - EOS RPC wrapper for Python \n django scatter auth  - Django Authentication using Scatter \n \n Scala \n \n eos-scala-rpc-api  - EOS api wrapper for Scala \n \n Elixir \n \n eosrpc-elixir-wrapper  - EOS RPC Api Wrapper for elixir \n \n Swift \n \n SwiftyEOS  - EOS account management and RPC wrapper in Swift. \n eos-swift  - General purpose library for the EOS blockchain, with a focus on building and pushing transactions. \n \n Smart Contracts Examples \n \n MonsterEOS  - Tamagotchi game, integrates  eosio.token  with account balances, randomization etc. \n DecenTwitter  - Decent(ralized) Twitter - ZERO!!! Ram Cost \n Everipedia  - IPFS integration, tokenization and governance \n \n \n Contributing to Awesome EOS \n Your contributions are always welcome! \n I will keep some pull requests open if I'm not sure whether those libraries are awesome, you could\n vote for them  by adding :+1: to them.\nPull requests will be merged when their votes reach 3.", '\n Viewly Alpha \n This is the second iteration of the Viewly Alpha (https://alpha.view.ly). \n Staging \n \n \n Running in Docker \n To run in Docker, make sure you have latest  docker  and  docker-compose  installed.\nYou will also need a  docker.dev.env  file, which is not part of this repo, because\nit contains API keys to AWS and other resources. \n Clone the repo \n git clone git@github.com:Viewly/alpha-2.git --recursive \nThen, add the  docker.dev.env  file into project root. \n Bring up the Docker stack \n docker-compose -f docker-compose.web.yml -f docker-compose.workers.yml up \nIf there are no errors, you should be able to open Alpha Web app at http://localhost:50001 \n Note: To avoid conflicts with locally installed Postgres/Redis/Flask, the\nthe Dockerized version of the app binds to ports that have  1  added at the end.\nFor example, the PostgreSQL port binds to host on  54321  rather than  5432 . The latter\nis available within the container only. \n Note: To persist the data, PostgreSQL container will mount its data volume into\n postgres_data  locally. Remove this folder if you wish to start from scratch. \n Re-building the stack from scratch \n If you\'ve added extra dependencies or applied changes that require\ncontainers be rebuilt, you can use this command:\n docker-compose -f docker-compose.web.yml -f docker-compose.workers.yml  build --no-cache \n Alternatively you can just delete images when bringing  down  the stack.\n docker-compose -f docker-compose.web.yml -f docker-compose.workers.yml  down --rmi all \n Running Locally \n Follow this guide if you wish to run the app bare-metal. \n Dependencies \n Package Dependencies:\n - Python 3.6 or higher\n - PostgreSQL 9.6 or 10.x\n - Redis\n - npm \n Install JS dependencies:\n cd src/static\nnpm i \n Install Python dependencies:\n pip install -r requirements.txt \n Environment Variables \n To run Flask in development, you need these environment variables set:\n export FLASK_APP=src/views.py\nexport FLASK_ENV=development \n Here is the default environment (you may want to set these yourself): \n | Variable      | Default                    |\n| ------------- | -------------------------- |\n| PRODUCTION    | False                      |\n| SECRET_KEY    | not_a_secret               |\n| POSTGRES_URI  | postgres://localhost/alpha |\n| MAIL_USERNAME | postmaster@mg.view.ly      |\n| MAIL_PASSWORD | \'\'                         | \n Note: This list does not include AWS related variables. Look for those in AWS section\nof the readme . \n Web App \n Run the Flask server:\n flask run --port 5000 \n Run the React app server:\n cd src/static\nnpm start \n Database Management \n In development, you can initialize your PostgreSQL database with:\n flask db-init \n If you\'ve messed up, you can nuke the database, and re-create the schemas with:\n flask db-reset \n Database Migrations (optional) \n To avoid having to delete and re-create the database in development, we can use migrations. \n First, change the schema in  models.py .\nThen, create a migrations file.\n flask db migrate -m "example migration message" \n Lastly, apply the migration:\n flask db upgrade \n Celery Workers \n Transcoding workers:\n celery worker -A src.tasks.transcoder -l info -c 1 -P solo \n Celery Cron Jobs \n Enable the beat service:\n celery -A src.tasks.cron beat \n Run the cron worker:\n celery worker -A src.tasks.cron -l info -c 1 -P solo \n Amazon Services \n S3 Setup \n Create S3 Upload and Video storage buckets trough  ElasticTranscoder.ipynb . \n Example config:\n config = {\n    \'region_name\': \'eu-central-1\',\n    \'pipeline_name\': \'viewly-pipeline-v1\',\n    \'s3_input_bucket\': \'viewly-uploads-eu1\',\n    \'s3_output_bucket\': \'viewly-videos-eu1\',\n} \n ACL and CORS will be applied automatically, however we need to perform some manual tasks\non fresh deployment. \n Uploader Bucket Configuration \n Upload Bucket CORS Policy \n```xml \n xml version="1.0" encoding="UTF-8"? \n \n \n * \n POST \n PUT \n DELETE \n 3000 \n ETag \n content-type \n origin \n x-amz-acl \n x-amz-meta-qqfilename \n x-amz-date \n x-amz-content-sha256 \n authorization \n \n \n``` \n Manual Setup \n Use Amazon S3 Console to:\n - Enable  Transfer Acceleration  on the Upload bucket\n - Add a lifecycle rule to  Clean up incomplete multipart uploads \n IAM Policy \nCreate a  s3-viewly-uploader  API user with the following S3 policy:\n json\n{\n    "Version": "2012-10-17",\n    "Statement": [\n        {\n            "Effect": "Allow",\n            "Action": "s3:PutObject",\n            "Resource": "arn:aws:s3:::viewly-uploads-us1/*"\n        }\n    ]\n} \n Replace  viewly-uploads-us1  with upload bucket name. \nUse this users API credentials as  S3_UPLOADER_PUBLIC_KEY  and  S3_UPLOADER_SECRET_KEY . \n Environment Variables \n | Variable               | Default |\n| ---------------------- | ------- |\n| S3_UPLOADS_BUCKET      |         |\n| S3_UPLOADS_REGION      |         |\n| S3_UPLOADER_PUBLIC_KEY |         |\n| S3_UPLOADER_SECRET_KEY |         | \n Videos Bucket Configuration \n To be able to stream files from this bucket via CloudFormation, the following\nbucket policy needs to be applied:\n json\n{\n    "Id": "Policy1517839392609",\n    "Version": "2012-10-17",\n    "Statement": [\n        {\n            "Sid": "Stmt1517839387764",\n            "Action": [\n                "s3:GetObject"\n            ],\n            "Effect": "Allow",\n            "Resource": "arn:aws:s3:::viewly-videos-us1/*",\n            "Principal": "*"\n        }\n    ]\n} \n We also need the following CORS config:\n```xml \n xml version="1.0" encoding="UTF-8"? \n \n \n \n GET \n 3000 \n \n \n \n``` \n Environment Variables \n | Variable         | Default |\n| ---------------- | ------- |\n| S3_VIDEOS_BUCKET |         |\n| S3_VIDEOS_REGION |         | \n Elastic Transcoder \n Use  ElasticTranscoder.ipynb  to create and configure the ET pipeline.\nOutput configuration saved  here . \n CloudFront \n Create a Cloudfront Distribution manually.\nUse the  viewly-videos-*  bucket, and set the default TTL to 1 day or more.\nCreate a CNAME (cdn.view.ly), and issue custom certificate trough ACM. \n Environment Variables \n | Variable    | Default                |\n| ----------- | ---------------------- |\n| CDN_URL     | https://cdn.view.ly    |\n| PLAYER_URL  | https://player.view.ly | \n IAM Manager Account \n The manager account has just the privileges necessary to:\n - Read, Write, Delete files in the uploads and videos buckets\n - Create, Cancel and Read ElasticTranscoder Jobs\n - Invalidate CloudFormation Caches (ie. on thumbnail change) \n Manager Policy \n json\n{\n    "Version": "2012-10-17",\n    "Statement": [\n        {\n            "Sid": "VisualEditor0",\n            "Effect": "Allow",\n            "Action": [\n                "s3:PutObject",\n                "s3:GetObjectAcl",\n                "s3:GetObject",\n                "s3:ListBucketMultipartUploads",\n                "s3:GetObjectTagging",\n                "s3:ListBucket",\n                "s3:PutObjectTagging",\n                "s3:DeleteObject",\n                "s3:GetBucketAcl",\n                "s3:GetBucketLocation",\n                "s3:PutObjectAcl",\n                "s3:GetObjectVersion"\n            ],\n            "Resource": [\n                "arn:aws:s3:::viewly-uploads-us1",\n                "arn:aws:s3:::viewly-videos-us1",\n                "arn:aws:s3:::viewly-uploads-us1/*",\n                "arn:aws:s3:::viewly-videos-us1/*"\n            ]\n        },\n        {\n            "Sid": "VisualEditor1",\n            "Effect": "Allow",\n            "Action": [\n                "rekognition:ListCollections",\n                "rekognition:DetectFaces",\n                "rekognition:DetectText",\n                "rekognition:DescribeStreamProcessor",\n                "elastictranscoder:ListPipelines",\n                "elastictranscoder:ReadJob",\n                "cloudfront:CreateInvalidation",\n                "elastictranscoder:ListJobsByStatus",\n                "s3:ListObjects",\n                "rekognition:GetLabelDetection",\n                "rekognition:GetContentModeration",\n                "rekognition:SearchFaces",\n                "rekognition:ListStreamProcessors",\n                "elastictranscoder:CancelJob",\n                "rekognition:SearchFacesByImage",\n                "elastictranscoder:CreateJob",\n                "s3:HeadBucket",\n                "rekognition:DetectLabels",\n                "rekognition:GetCelebrityRecognition",\n                "elastictranscoder:ListJobsByPipeline",\n                "rekognition:GetPersonTracking",\n                "rekognition:DetectModerationLabels",\n                "rekognition:GetFaceDetection",\n                "rekognition:RecognizeCelebrities",\n                "cloudfront:GetInvalidation",\n                "elastictranscoder:ReadPreset",\n                "rekognition:CompareFaces",\n                "rekognition:GetCelebrityInfo",\n                "rekognition:ListFaces",\n                "elastictranscoder:ReadPipeline",\n                "rekognition:GetFaceSearch",\n                "cloudfront:ListInvalidations",\n                "elastictranscoder:ListPresets"\n            ],\n            "Resource": "*"\n        }\n    ]\n} \n Manager Account \nCreate  viewly-alpha-manager  API account with the above policy. \n Environment Variables \n | Variable                | Default |\n| ----------------------- | ------- |\n| AWS_MANAGER_PUBLIC_KEY  |         |\n| AWS_MANAGER_PRIVATE_KEY |         | \n Refresh staging db from a local instance \n First, need to scale down staging kubernetes cluster to 0, since we are wiping the database.\n```\npg_dump -Fc alpha > dump.db \n pg_restore -U alpha -h alpha-staging.cvnyb565p4lg.eu-central-1.rds.amazonaws.com -d alpha --clean dump.db\n``` \n Testing \n Make sure the .env is loaded. Tests require \'Kovan\' chain.\n PYTHONPATH=$(pwd) py.test', 'DEMO: https://vimeo.com/242144832 \n TODO \n \n deploy as a SteemData service \n \n Requirements \n This guide assumes MacOS with HomeBrew. \n Dependencies \n Install JS dependencies:\n```\nnpm install -g browserify \n cd public\nnpm i \n browserify -r eosio > bundle.js\n``` \n Install Python 3.6. \n brew install python3 \n Install and Start Redis:\n brew install redis\nbrew services start redis \n Install Python dependencies:\n pip install -r requirements.txt \n Run Locally \n Run the Flask server:\n export FLASK_APP=src/views.py\nflask run --reload --debugger \n Environment Variables \n | Variable      | Default                          |\n| ------------- | -------------------------------- |\n| PRODUCTION    | False                            |\n| SECRET_KEY    | not_a_secret                     |', 'srand  is a simple, small and slow(er) sampled rand(om) generator.\nIt gets its entropy from  getrandom() . \n Usage \n Get 256bit entropy hash:\n ~/G/srand % srand\n0x741d4bb30d9466fae40310bdbc3bb72b41d21611e24d50fa16d884429a960047 \n Get random password:\n ~/G/srand % srand pw\n&U4Pg0@ZTceNzSt0wdGKakx$H?L)(E5W \n Get random password of custom length:\n ~/G/srand % srand pw 18\n9$W6B#SXfL)x&nbaHM \n Warranty \n None']
lukeyeager,["Flask-Autodoc \n Flask-Autodoc is a Flask extension that automatically creates documentation for your endpoints based on the routes, function arguments and docstrings. \n \n \n \n \n \n Requirements \n Flask-Autodoc is compatible with Python versions 2 and 3; and it depends only on Flask. \n Install \n To install Flask-Autodoc, run pip: \n pip install flask-autodoc\n \n or clone this directory and run setup: \n python setup.py install\n \n Usage \n Start using Flask-Autodoc by importing it and initializing it: \n from flask import Flask\nfrom flask.ext.autodoc import Autodoc\n\napp = Flask(__name__)\nauto = Autodoc(app)\n \n by default, Flask-Autodoc will only document the routes explicitly decorated with  doc : \n @app.route('/user/<int:id>')\n@auto.doc()\ndef show_user(id):\n    return user_from_database(id)\n \n to generate the documentation, use the  html()  method: \n @app.route('/documentation')\ndef documentation():\n    return auto.html()\n \n Custom documentation \n To access the documentation without rendering html: \n @app.route('/documentation')\ndef documentation():\n    return auto.generate()\n \n the documentation will be returned as a list of rules, where each rule is a dictionary containing: \n \n methods: the set of allowed methods (ie ['GET', 'POST']) \n rule: relative url (ie '/user/ ') \n endpoint: function name (ie 'show_user') \n doc: docstring of the function \n args: function arguments \n defaults: defaults values for the arguments \n \n Custom template \n To use a custom template for your documentation, give a  template  argument to the  html  method. This will use a template from the flask  templates  directory.  \n Additionnal arguments (other than  group ,  groups , and  template ) will be passed down to the template: \n auto.html(\n\n    template='custom_documentation.html'\n\n    title='My Documentation',\n    author='John Doe',\n)\n \n title  and  author  will be available in the template: \n <!-- templates/custom_documentation.html -->\n...\n{% if title is defined %}\n    {{title}}\n{% endif %}\n...\n \n Documentation sets \n Endpoints can be grouped together in different documentation sets. It is possible for instance to show some endpoints to third party developers and have full documentation for primary developers. \n To assign an endpoint to a group, pass the name of the group as argument of the  doc  decorator: \n @app.route('/user/<int:id>')\n@auto.doc('public')\ndef show_user(id):\n \n to assign an endpoint to multiple groups, pass a list of group names as the  groups  argument to  doc : \n @app.route('/user/<int:id>')\n@auto.doc(groups=['public','private'])\ndef show_user(id):\n \n to generate the documentation for a specific group, pass the name of the group to the  html  or  generate  methods: \n auto.html('public')\nauto.html(groups=['public','private'])\nauto.generate('public')\n \n Examples \n Apps in the  examples  directory are an api for a blog: \n \n simple  is a simple app \n factory  uses blueprints \n \n Run with \n python simple/blog.py\n \n and connect to  /doc/public  and  /doc/private  to see public and private documentations. \n Screenshots \n \n", 'dynipo-web \n Dynamic IP Otter', 'dynipo-reporter \n Dynamic IP Otter Reporter', 'dynipo-android \n Dynamic IP Otter - Anroid App', 'github-testing \n This is the README', 'flask-sqlalchemy-socketio-demo \n \n Demo for how to use Flask, SQLAlchemy and SocketIO together \n Setup \n pip install -r requirements.txt\n./manage.py db init\n./manage.py db migrate\n./manage.py db upgrade \n Asynchronous framework \n There are three options for the asynchronous backend framework: \n \n eventlet \n pip install eventlet \n gevent \n pip install gevent gevent-websocket \n threading \n built-in \n \n Communication method \n There are three options for the worker-server communication method: \n \n SocketIO \n pip install socketio-client \n Redis \n pip install redis \n ZeroMQ \n pip install pyzmq \n \n Run server \n ./manage.py runserver \n Test functionality \n ./manage.py add', 'dockerfiles', 'Packaging tools \n \n Tools for manipulating deb packages.', 'h5py testing \n pip install -r requirements.txt\nmake', 'pencroft \n \n \n sh\npython -m pencroft benchmark data/\npython -m pencroft benchmark data.tar\npython -m pencroft benchmark data.zip', 'CUDA with CMake 3.8 \n With a super-recent build of CMake (https://gitlab.kitware.com/cmake/cmake/merge_requests/949), try this:\n ./run.sh -DCMAKE_VERBOSE_MAKEFILE=On -DCMAKE_CXX_COMPILER_LAUNCHER=ccache -DCMAKE_CUDA_COMPILER_LAUNCHER=ccache', 'Docker multi-stage demo \n Shows how to use multi-stage docker builds to create devel and runtime images. \n bash\n$ make\n...\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nccache              runtime             c50710c43e23        1 second ago        189MB\nccache              devel               0a9a4eaff1d8        2 seconds ago       383MB \n Devel image \n \n Install development packages for dependencies \n Download source \n Build \n Install \n \n Runtime image \n \n Install runtime packages for dependencies \n Copy pre-built installation from devel image \n', 'pynvml \n Forked from https://github.com/pybind/cmake_example', "bounded_async_executor \n \n \n I found myself writing too much code that looked like this:\n```python\ndef download_urls(urls):\n    downloaded = 0 \n # Use concurrent.futures to create a pool of worker threads\nwith concurrent.futures.ThreadPoolExecutor() as executor:\n    futures = set()\n    for url in urls:\n        # Create a future for each url\n        futures.add(executor.submit(download_url, url))\n\n        # Bound the results so that `futures` doesn't take up too much memory\n        while len(futures) >= 1000:\n            done, futures = concurrent.futures.wait(futures, return_when=concurrent.futures.FIRST_COMPLETED)\n            for future in done:\n                try:\n                    future.result()\n                    downloaded += 1\n                except Exception as e:\n                    print(e)\n\n    # Process the remaining futures\n    for future in concurrent.futures.as_completed(futures):\n        try:\n            future.result()\n            downloaded += 1\n        except Exception as e:\n            print(e)\n\nprint('Downloaded {} files successfully.'.format(downloaded))\n \n ``` \n So, I wrote a library to abstract away much of that complexity:\n```python\ndef download_urls(urls):\n    downloaded = 0 \n def on_success(result):\n    nonlocal downloaded\n    downloaded += 1\n\ndef on_error(error):\n    print(error)\n\nwith bounded_async_executor.Executor(download_url, on_success, on_error) as executor:\n    for url in urls:\n        executor.add(url)\n\nprint('Downloaded {} files successfully.'.format(downloaded))\n \n ```", 'golang-websocket-echo-ping-example \n Example of a golang websocket server.\nThe server echos messages from clients back to the client.\nIt also implements ping-pong initiated from the server. \n go get ./...\ngo run server.go']
jorgeleria,[]
msher,[]
yahe,[]
jeffskelton3,["OSS License Aggregator \n Aggregates licensing information for npm based dependencies for one or more repositories.  \n NOTE: I hacked this together for a specific need I had. You'll probably want to edit the script a bit to suit your needs. Maybe in the future I'll come back to this and make it nice.  \n Usage \n First install dependencies by running  npm i \n create a file at the root of this repo called  config.json . This is where you provide the paths to all your repositories you wish to aggregate. See  config.example.json  for an example. \n Once you've created your config, then just run  npm run build . A file called   out.csv  will be generated at the root of this repository containing all the licensing information for your aggregated dependencies.", "Requirements \n \n docker \n docker-compose \n \n Starting the application \n open a terminal at the root of the project and run  docker-compose up . Everything will get handled in that one command including spinning up the server. \n After the boot sequence completes visit  http://localhost:8000/docs  you should see a swagger ui interface and all the available endpoints. \n Notable technology choices \n Full disclosure: Richie recommended FastAPI and kafka. I am glad he did. They're great. \n \n FastAPI \n postgres  +  sqlalchemy \n kafka  +  aiokafka \n docker \n \n I also set up quality of life things like pydantic and black. I found poetry for package management which was close enough to tech I am familiar with like npm and maven to be useful."]
varadhodiyil,['divolte \n ClickStream logs', 'sentiment-search', 'bankone', 'fcm', '1mg', 'face-rego-cnn \n A simple Face recognition built with Tensorflow  \n Download 68 face landmark from \n http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2', 'face-recognition', "\n 🅛🅐🅩🅨🅖🅘🅣 🤟 \n Tired of typing lengthy git commands? Well, if you are amongst who feel like me, this repo is a savior for you. This repo includes shorthand commands for the regular git commands which you can type in a jiffy. \n diff\n- I'd advice beginners to learn git commands first hand before using any shortcodes or aliases \n Installation \n \n Clone or Download the repo. \n Run  ./_install  to install  \n If you see any permissions related errors try ➡️ chmod 755 _install  and run  ./_install  again. or any error please raise a PR 😅 \n \n Short Codes List \n | Command | Shortcode | Can pass default params with this command ? | Git Repo Link |\n| --- | --- | --- | --- |\n|  git init [<options>]  |  g! [<options>]  | Yes |  :information_source:  |\n|  git add .  |  g.  | Yes |  :information_source:  |\n|  git branch  |  gb  | Yes |  :information_source:  |\n|  git checkout -b <name>  |  gb-new <name>  | No, You can only pass branch-name and  --track  or  --no-track |  :information_source:  |\n|  git blame  |  gblame  | Yes |  :information_source:  |\n|  git clone <url>  |  gcl <url>  | Yes | :information_source:  |\n|  git commit -m <message>  |  gc <message>  | No, it accepts only commit message | :information_source:  |\n|  git diff [--options] <commit> [--] [<path>…]  |  gd [--options] <commit> [--] [<path>…]  | Yes |  :information_source:  |\n|  git fetch [<options>] [<repository> [<refspec>…]  |  gf [<options>] [<repository> [<refspec>…]]  | Yes |  :information_source:  |\n|  git log --graph --decorate --all  |  gl  | No |  :information_source:  |\n|  git push [<options>] [<remote>] [<branch>]  |  gph [<options>] [<remote>] [<branch>]  | Yes |  :information_source:  |\n|  git push -f [<remote>] [<branch>] |  gph-f <remote> <branch> | Yes |  :information_source:  |\n|  git pull [options] [<repository> [<refspec>…]]  |  gpl [options] [<repository> [<refspec>…]]  | Yes |  :information_source:  |\n|  git remote [commands] [<...>]  |  gr [commands] [<...>]  | Yes |  :information_source:  |\n|  git remote -v  |  gr-v  | It acceps only  <show>  and  <update>  |  :information_source:  |\n|  git remote add  <name> <url>  |  gr-update <name> <url>  | It accepts options related to  <add>  |  :information_source:  |\n|  git remote update  |  gr-update  | Only accepts  <prune>  |  :information_source:  |\n|  git remote remove <name> |  gr-remove <name> | No other options are needed other than  <name>  |  :information_source:  |\n|  git reset [<mode>] <head> |  greset  [<mode>] <head> | Yes |  :information_source:  |\n|  git reset --hard <head> |  greset-h <head> | Takes only  <head>  |  :information_source:  |\n|  git revert [<options>] [<subcommands>] |  grevert [<options>] [<subcommands>] | Yes |  :information_source:  |\n|  git revert HEAD |  grevert-h | This reverts last commit only, Use the above for reseting other commits eg.,  grevert head~2 |  :information_source:  |\n|  git status [<options>…] [--] [<pathspec>…] |  gs [<options>…] [--] [<pathspec>…] | Yes |  :information_source:  |\n|  git stash [<command>] [<options>] |  gsh [<command>] [<options>] | Yes |  :information_source:  |\n|  git stash apply  [--index] [<stash>] |  gsh-a  [--index] [<stash>]  | Yes |  :information_source:  |\n|  git stash list [<options>] |  gsh-l [<options>] | Yes |  :information_source:  |\n|  git stash clear |  gsh-c | No It clears all the stash list |  :information_source:  |\n|  git stash pop [--index] |  gsh-p [--index] | Yes |  :information_source:  |\n|  git stash drop [<stash>] |  gsh-d [<stash>] | Yes |  :information_source:  |\n|  git tag -l --sort=v:refname \\| tail -n8  |  gt  | No |  :information_source:  |\n|  git add . && git commit -m  |  g.c <message>  | No, it accepts only commit message | :information_source:  | \n \n To Contribute \n I have added commands that I use frequently and scope for new commands (Bash Aliases) is less. But if you are interested you can raise a PR or please follow [this branch] (https://github.com/SarathSantoshDamaraju/lazyGit/tree/npm) \n \n Tasks \n \n [x]  ~~Add Bash file with basic commands~~ \n [ ] Add Install via npm  WIP \n [ ] ~~Add Install via pip~~ (If interested, work and raise a PR) \n \n \n Fell free to suggest and report, Raise a PR with [report] or [suggest] labels", 'mnist-classifier \n Code for kaggle Competition \n https://www.kaggle.com/c/digit-recognizer', 'igni', 'rest-end-points-fourthwatch', 'test-poc', 'apod \n A new Flutter project. \n Getting Started \n For help getting started with Flutter, view our online\n documentation .', 'maps \n A new Flutter project. \n Getting Started \n For help getting started with Flutter, view our online\n documentation .', "Kibana \n Kibana is your window into the  Elastic Stack . Specifically, it's a browser-based analytics and search dashboard for Elasticsearch. \n \n Getting Started \n Using a Kibana Release \n Building and Running Kibana, and/or Contributing Code \n Documentation \n Version Compatibility with Elasticsearch \n Questions? Problems? Suggestions? \n \n Getting Started \n If you just want to try Kibana out, check out the  Elastic Stack Getting Started Page  to give it a whirl. \n If you're interested in diving a bit deeper and getting a taste of Kibana's capabilities, head over to the  Kibana Getting Started Page . \n Using a Kibana Release \n If you want to use a Kibana release in production, give it a test run, or just play around: \n \n Download the latest version on the  Kibana Download Page . \n Learn more about Kibana's features and capabilities on the\n Kibana Product Page . \n We also offer a hosted version of Kibana on our\n Cloud Service . \n \n Building and Running Kibana, and/or Contributing Code \n You might want to build Kibana locally to contribute some code, test out the latest features, or try\nout an open PR: \n \n CONTRIBUTING.md  will help you get Kibana up and running. \n If you would like to contribute code, please follow our  STYLEGUIDE.md . \n Learn more about our UI code with  UI_SYSTEMS.md . \n For all other questions, check out the  FAQ.md  and\n wiki . \n \n Documentation \n Visit  Elastic.co  for the full Kibana documentation. \n For information about building the documentation, see the README in  elastic/docs . \n Version Compatibility with Elasticsearch \n Ideally, you should be running Elasticsearch and Kibana with matching version numbers. If your Elasticsearch has an older version number or a newer  major  number than Kibana, then Kibana will fail to run. If Elasticsearch has a newer minor or patch number than Kibana, then the Kibana Server will log a warning. \n Note: The version numbers below are only examples, meant to illustrate the relationships between different types of version numbers. \n | Situation                 | Example Kibana version     | Example ES version | Outcome |\n| ------------------------- | -------------------------- |------------------- | ------- |\n| Versions are the same.    | 5.1.2                      | 5.1.2              | 💚 OK      |\n| ES patch number is newer. | 5.1. 2                   | 5.1. 5           | ⚠️ Logged warning      |\n| ES minor number is newer. | 5. 1 .2                  | 5. 5 .0          | ⚠️ Logged warning      |\n| ES major number is newer. |  5 .1.2                  |  6 .0.0          | 🚫 Fatal error      |\n| ES patch number is older. | 5.1. 2                   | 5.1. 0           | ⚠️ Logged warning      |\n| ES minor number is older. | 5. 1 .2                  | 5. 0 .0          | 🚫 Fatal error      |\n| ES major number is older. |  5 .1.2                  |  4 .0.0          | 🚫 Fatal error      | \n Questions? Problems? Suggestions? \n \n If you've found a bug or want to request a feature, please create a  GitHub Issue .\nPlease check to make sure someone else hasn't already created an issue for the same topic. \n Need help using Kibana? Ask away on our  Kibana Discuss Forum  and a fellow community member or\nElastic engineer will be glad to help you out. \n", 'newtest', "keras-yolo3 \n \n Introduction \n A Keras implementation of YOLOv3 (Tensorflow backend) inspired by  allanzelener/YAD2K . \n \n Quick Start \n \n Download YOLOv3 weights from  YOLO website . \n Convert the Darknet YOLO model to a Keras model. \n Run YOLO detection. \n \n wget https://pjreddie.com/media/files/yolov3.weights\npython convert.py yolov3.cfg yolov3.weights model_data/yolo.h5\npython yolo_video.py [OPTIONS...] --image, for image detection mode, OR\npython yolo_video.py [video_path] [output_path (optional)] \n For Tiny YOLOv3, just do in a similar way, just specify model path and anchor path with  --model model_file  and  --anchors anchor_file . \n Usage \n Use --help to see usage of yolo_video.py:\n```\nusage: yolo_video.py [-h] [--model MODEL] [--anchors ANCHORS]\n                     [--classes CLASSES] [--gpu_num GPU_NUM] [--image]\n                     [--input] [--output] \n positional arguments:\n  --input        Video input path\n  --output       Video output path \n optional arguments:\n  -h, --help         show this help message and exit\n  --model MODEL      path to model weight file, default model_data/yolo.h5\n  --anchors ANCHORS  path to anchor definitions, default\n                     model_data/yolo_anchors.txt\n  --classes CLASSES  path to class definitions, default\n                     model_data/coco_classes.txt\n  --gpu_num GPU_NUM  Number of GPU to use, default 1\n  --image            Image detection mode, will ignore all positional arguments\n``` \n \n \n MultiGPU usage: use  --gpu_num N  to use N GPUs. It is passed to the  Keras multi_gpu_model() . \n \n Training \n \n \n Generate your own annotation file and class names file. \n    One row for one image; \n    Row format:  image_file_path box1 box2 ... boxN ; \n    Box format:  x_min,y_min,x_max,y_max,class_id  (no space). \n    For VOC dataset, try  python voc_annotation.py \n    Here is an example:\n     path/to/img1.jpg 50,100,150,200,0 30,50,200,120,3\n    path/to/img2.jpg 120,300,250,600,2\n    ... \n \n \n Make sure you have run  python convert.py -w yolov3.cfg yolov3.weights model_data/yolo_weights.h5 \n    The file model_data/yolo_weights.h5 is used to load pretrained weights. \n \n \n Modify train.py and start training. \n python train.py \n    Use your trained weights or checkpoint weights with command line option  --model model_file  when using yolo_video.py\n    Remember to modify class path or anchor path, with  --classes class_file  and  --anchors anchor_file . \n \n \n If you want to use original pretrained weights for YOLOv3: \n    1.  wget https://pjreddie.com/media/files/darknet53.conv.74 \n    2. rename it as darknet53.weights \n    3.  python convert.py -w darknet53.cfg darknet53.weights model_data/darknet53_weights.h5 \n    4. use model_data/darknet53_weights.h5 in train.py \n \n Some issues to know \n \n \n The test environment is \n \n Python 3.5.2 \n Keras 2.1.5 \n tensorflow 1.6.0 \n \n \n \n Default anchors are used. If you use your own anchors, probably some changes are needed. \n \n \n The inference result is not totally the same as Darknet but the difference is small. \n \n \n The speed is slower than Darknet. Replacing PIL with opencv may help a little. \n \n \n Always load pretrained weights and freeze layers in the first stage of training. Or try Darknet training. It's OK if there is a mismatch warning. \n \n \n The training strategy is for reference only. Adjust it according to your dataset and your goal. And add further strategy if needed. \n \n \n For speeding up the training process with frozen layers train_bottleneck.py can be used. It will compute the bottleneck features of the frozen model first and then only trains the last layers. This makes training on CPU possible in a reasonable time. See  this  for more information on bottleneck features. \n \n", 'neural-machine-translation', 'facenet_inception \n Facenet', 'Vision Pipeline \n Framework Used \n Model Development & Deployment \n For model development and deployment, I’ve used TensorFlow with Keras as the backend. \n API Development \n For exposing the built model as a service, I’ve used Django and Django rest framework. For documenting the API with request and response schema, I’ve used Swagger. \n Libraries Used \n requirements.txt contains all the dependencies required for the project. \n Model Architecture Used \n \n To Person Detect - YOLO model is used. \n Face Detection (Optional Can be configured in settings.py) to boost Person detection Confidence - MTNN model is used. \n Mask Detection - Transfer learning from MobileNetV2 is applied. \n \n Training Mask Detection \n train/  contains the MobileNet model backed transfer learning, with Mask Net dataset.   80% data is used for training and 20% is used for validation. Peak performance of the model observed is 98% accuracy. After training the model’s weights are stored as h5 for ease of API deployment.\nYOLO model weights trained with COCO dataset is also loaded and saved as h5 enabling faster API deployment. MTCNN package available from PyPI comes with weights inbuilt, so no further processing is required. \n Resource Allocation \n Since this is a GPU intensive application, GPU resource sharing should be optimized. Comparing the architectures of YOLO and Mask detection, Mask detection model might require less GPU resource. So, using TensorFlow’s configuration, I’ve opted to set memory growth parameter to True. This ensures that GPU is only allocated only when required and is freed when the response is consumed. \n Loading the models to memory \n Both the model’s weights are loaded on the server’s memory on start-up of the API server. This saves the throughput of the system by loading the model only once and not for each request as loading the model is a memory-intensive process. \n Approach \n The model analyses the given image if it contains a person. If a person is detected, the model then predicts if the person is wearing a mask. \n Figure 1 Flow of API \n \n When a new valid request comes in,\n* Person detect is invoked to check if the input image contains at least one person. If no person is detected. API responds with status 200, No person found. (No further processing is carried out).\n * If at least one person is detected, For each person,\n      If USE_MTCNN is set to True, Face Detection is carried out to check if the person detected has a valid human face. If No face is detected, API responds with the message No person found. Else Confidence is set to the product of confidence returned by MTCNN and that returned with YOLO.\n      Then for each person, Mask Net is invoked to check if the person is a wearing mask.\n  * YOLO model and MTCNN model returns bounding box information as well if we want to process the image further on the client-side. \n Packing and Deployment \n The Django app is set up such that no additional steps are required to deploy the application on the server. The application is self-serviced, i.e. All the additional files required for running the application is auto-downloaded if a local version of the file is not present. The API application is production-ready and can be deployed as a standalone application or through docker. TensorFlow provides a container with TensorFlow GPU and NVidia drivers preinstalled and be used to run the application. From the base docker image, we can install the libraries (from requirements.txt) this API server uses and build our container and deploy it anywhere.\nCI/CD deployment. \n This API application is built with Test-Driven Approach. This ensures that each API method is tested on how the response behaviour changes for each valid and invalid request. CI/CD pipeline is set up with the config file available at .github/workflows/Django.yml. This ensures reliability and ease of Deployment of the code changes to production with trust. \n Sample Screens \n Figure 2 API Documentation \n \n Figure 3 API response when 1 person is present in Image \n \n Figure 4 API Response when multiple persons are detected in the Image \n \n Figure 5 API response when no persons are detected in the Image \n', 'Dev PortFolio', 'sports_events \n Provides API for list, get match for betting \n Built as Rest API, this uses  django-rest-framework  and  django . Database used is  MySQL . The app is documented with Swagger and redoc. \n Setup \n Pre Requirements \n \n MySQL to be installed \n Database to be created. If not use, CREATE DATABASE IF NOT EXISTS spectate_888;\n \n \n \n All requirements needed are listed under  requirements.txt . To install \n pip install -r requirements.txt\n \n Starting the server \n To start the Django WSGI engine, \n python manage.py runserver\n \n Testing \n To test the app, \n python manage.py test\n \n This runs all the testcases defined in all the apps, defined in  tests.py  in each app. \n Sample Images \n Swagger Documentation \n \n ReDoc Documentation \n \n List all Events \n \n Get Match by Id \n \n Search by Sport - FootFall \n \n Search By Sport - Cricet \n \n Ordering By startTime \n \n Ordering , Search \n \n Update Odds \n \n NewEvent \n', 'smj-chennai-api', 'SmjChennai \n This project was generated with  Angular CLI  version 14.0.5. \n Development server \n Run  ng serve  for a dev server. Navigate to  http://localhost:4200/ . The application will automatically reload if you change any of the source files. \n Code scaffolding \n Run  ng generate component component-name  to generate a new component. You can also use  ng generate directive|pipe|service|class|guard|interface|enum|module . \n Build \n Run  ng build  to build the project. The build artifacts will be stored in the  dist/  directory. \n Running unit tests \n Run  ng test  to execute the unit tests via  Karma . \n Running end-to-end tests \n Run  ng e2e  to execute the end-to-end tests via a platform of your choice. To use this command, you need to first add a package that implements end-to-end testing capabilities. \n Further help \n To get more help on the Angular CLI use  ng help  or go check out the  Angular CLI Overview and Command Reference  page.']
k-hasan-19,["Employee Advocacy Platform(AWS SAM+OpenAPI 3.0.2 ) \n Warning : We are still at development stage. It's not stable yet \n Deployment \n sam deploy --stack-name inneed-eap --capabilities CAPABILITY_IAM\npython ./table-scripts/create_table.py\npython ./table-scripts/add_gsi.py \n API endpoint details \n TODO \n \n [ ] Integrate existing front end  \n [ ] POST endpoints for CRUD \n [ ] Social Media credential GET/POST end points \n [ ] Top score endpoint \n", "Twitter cannot stop you storing your favorite hashtag history for unlimited time \n      ___        ______     ____ _                 _  ___  \n    / \\ \\      / / ___|   / ___| | ___  _   _  __| |/ _ \\ \n   / _ \\ \\ /\\ / /\\___ \\  | |   | |/ _ \\| | | |/ _` | (_) |\n  / ___ \\ V  V /  ___) | | |___| | (_) | |_| | (_| |\\__, |\n /_/   \\_\\_/\\_/  |____/   \\____|_|\\___/ \\__,_|\\__,_|  /_/\n \n \n Warning : We are still at development stage. It's not stable yet \n Deployment using aws cloud9 \n sam deploy --guided \n Just fill up the params and your hashtag tracker is all set", 'serverless-php \n Run php on aws lambda', 'serverless-pipeline-aws \n Serverless app development codepipeline demo for dev & production stack on AWS with manual approval for prod env with documentations', 'sigv4-aws \n Example php scripts making sigv4 signed aws api requests \n Setup your  AWS_ACCESS_KEY_ID ,  AWS_SECRET_ACCESS_KEY  environment variables and use the scripts instead of heavy duty AWS SDK for PHP', 'Export logs to S3 \n Export cloudwatch log stream from log group to S3 bucket using this lambda function \n Sample payload: \n {\n  "window_length": 2,\n  "log_group_name": "<log group namespace>",\n  "bucket_name": "<S3 bucket name>",\n  "folder_name": "<S3 prefix/folder-name>"\n}', 'apex-redirect \n Redirect from apex domain to any other domain using AWS S3 website endpoint', 'simple-static-deployment \n Test deployments', 'upsellx \n TODO \n \n [x] REST API \n [x] Web Crawlers(2) \n [x] Parallelization \n [x] NoSQL Database \n [x] ETL \n [x] Pipeline Scheduler \n \n Deployment(sam-cli) \n Run following commands from the  top level directory  of the repo to build layer and to deploy the app.\n bash\ndocker run --rm -v $PWD:/usr/app node:12 bash -c "cd /usr/app/layers/node/nodejs/ && npm install"\ndocker run --rm -v $PWD:/usr/app python:3.8 bash -c "pip install -r /usr/app/layers/py/python/requirements.txt --target /usr/app/layers/py/python/ --no-cache-dir"\nsam deploy --guided \n Usage \n \n Api doc  here \n Api summary: After submiting the crawling job using  POST  you query company data using the  GET  API. Endpoint accepts  FQDN / hostname   \n To query data at temporary bucket run the  Glue Crawlers  i.e.  angel-json  and  crunchbase-json  , then goto  Athena  console and select  upsellxtemp  database to query it. \n Scheduler will compress data everyday in parquet && will append to  upsellxsilo  database. To access it early through  Athena , run  aws-data-wrangler  lambda function from aws console and refresh  Athena  table list to access it. \n \n Architecture \n', 'Apache Hudi Demo Using AWS DMS CDC, EMR, and Glue Catalog \n Demonstrate a simple Hudi data workflow in AWS using a mysql RDS as data source and S3 as storage for Hudi tables.\nThis project also showcase infrastructure as code patern by using Ansible to orchestrate the built and deployment of the components. \n Requirements \n \n AWS CLI \n Python 3.5+ \n Ansible 2.9+ \n amazon.aws ansible  module collection: \n ansible-galaxy collection install amazon.aws \n \n \n \n User should already have EC2 key-pair generated and specified in environments main.yaml. Otherwise a new one will be created.\nThe key-pair it\'s used to access EMR. \n Architecture \n \n [TBD] \n Project Structure \n [TBD] \n Deployment \n To deploy the project follow these steps: \n \n \n Be sure you installed and configured all dependencies: AWS CLI, Python3.5+ and Ansible 2.9+. Have AWS CLI configured to point to your environment.\nAlso install ansible aws module collection:  ansible-galaxy collection install amazon.aws \n \n \n Define an environment specific to your case in  ./ansible/environments/   folder. You can duplicate  ./ansible/environments/aia  to  ./ansible/environments/prod  for example\nHere is where you will set names, prefixes, instance classes etc. \n \n \n Run ansible-playbook in ./ansible folder: \n \n \n ```bash \n cd ./ansible\nansible-playbook main.yml -e "deployment_env= " \n ``` \n Before retracting the deployment, plese be sure the DMS replication task is stopped. Otherwise the retract will fail.\nTo retract all resources, run: \n ```bash \n ansible-playbook retract.yml -e "deployment_env= " \n ``` \n Using the Demo']
rakeen,['\n CF_Tracker \n tracks the performance of codeforces users \n Project Page: \n http://cfapi-rak1.c9.io/   \n About: \n Adding New User: \n Sends an ajax call to CF api to retrieve the user info. Stores them into mysql db.   \n Update: \n Retrieves all the user info from DB and send an ajax call to CF API for  ALL  the submissions! \nNote that the request takes a lot of time and currently runs only for 100s. \nOtherwise it might exceed the timelimit of the server.   \n ToDo: \n \n [ ] Restructure the project directory   \n [ ] Make a seperate server config file and use that   \n [ ] Imporve the UI   \n [ ] Find a work around of the update mechanism  \n']
readtimeout,['Cemilee \n Cemilee is an asynchronous http micro service based on  Kemal . \n Installation \n brew install crystal-lang\ngit clone https://github.com/readtimeout/cemilee\ncd cemilee\nshards install \n Build and Run \n crystal build --release src/cemilee.cr\n./cemilee \n Usage \n curl -X POST -H \'Content-Type: application/json\' -d \'[{ "method": "GET", "url": "http://httpbin.org/delay/1"}, {"method": "POST", "url": "http://httpbin.org/post", "payload": {"category": 2} }]\' http://localhost:3000 \n \n ["{"args": {}, "data": "", "files": {}, "form": {}, "headers": {  "Accept-Encoding": "gzip, deflate",   "Content-Length": "0",   "Host": "httpbin.org",   "User-Agent": "Crystal"  }, "origin": "127.0.0.1", "url": "http://httpbin.org/delay/1"}", "{"args": {}, "data": "{"category" => 2}", "files": {}, "form": {}, "headers": {  "Accept-Encoding": "gzip, deflate",   "Content-Length": "17",   "Host": "httpbin.org",   "User-Agent": "Crystal"  }, "json": null, "origin": "127.0.0.1", "url": "http://httpbin.org/post"}"] \n \n Contributing \n \n Fork it ( https://github.com/readtimeout/cemilee/fork ) \n Create your feature branch (git checkout -b my-new-feature) \n Commit your changes (git commit -am \'Add some feature\') \n Push to the branch (git push origin my-new-feature) \n Create a new Pull Request \n \n Contributors \n \n [readtimeout]  Read Timeout - creator, maintainer \n']
surbas,["dtwp-sfwporn \n Desktop Wallpaper from reddit's SFW Porn Network \n A python script that grabs the top scoring jpg image from subreddits like EarthPorn on the SFW Porn Network, and \nmakes it the current desktop wallpaper. \n It does this without needing any extra packages. Every effort will be made to keep it like this, put PIL support may be \nneeded in the future for advanced image editing techniques. \n Currently only tested on Win 7 with python 2.7, but should work with python 2.6. Other OS's will be supported soon. \n Install \n git clone https://github.com/surbas/dtwp-reddit-porn.git\n \n Run \n in dtwp-reddit-porn directory: \n python dtwp.py\n \n TODO \n \n Support for picking random image from reddit \n Need to include a parameter for sample size \n \n \n Support for picking random image from folder \n png support (prob need PIL support) \n cheeseshop \n Readme Again \n python 3 support \n mac support \n gnome support \n kde support \n Support other SFW Porn sources \n do something like this eventually http://www.kenstone.net/fcp_homepage/faking_it.html (prob need PIL support) \n"]
z123,['Pointy \n Pointy game \n Play Here', 'Snake \n Snake game', 'haas-header-project \n See gh-pages branch for code \nCheck it out at http://z123.github.io/haas-header-project/', 'hacktoberfest \n Pull request #1\nPull request #2\nPull request #3\nThanks broz', '', 'README \n This README would normally document whatever steps are necessary to get your application up and running. \n Steps to Deploy \n 1) Log into server\n2) Pull changes from github\n3) Update production instance config\n4) Push static css to server\n5) Migrate database\n6) Restart server \n What is this repository for? \n \n Quick summary \n Version \n Learn Markdown \n \n How do I get set up? \n \n Summary of set up \n Configuration \n Dependencies \n Database configuration \n How to run tests \n Deployment instructions \n \n Contribution guidelines \n \n Writing tests \n Code review \n Other guidelines \n \n Who do I talk to? \n \n Repo owner or admin \n Other community or team contact \n', 'My ReasonReact App \n Installing \n You can install all the needed dependencies by running  yarn . \n Watcher \n ```sh\n$ yarn start \n yarn run v1.12.3\n$ bsb -make-world -w\nninja: no work to do.\nninja: no work to do. \n \n \n \n \n Start compiling\nninja: no work to do.\nFinish compiling 12 mseconds\n``` \n \n \n \n \n Building and Bundling \n ```sh\n$ yarn build \n yarn run v1.12.3\n$ bsb -make-world && fpack build ./lib/js/src/index.bs.js --development\nninja: no work to do.\nninja: no work to do.\nninja: no work to do.\nCache: used\nDone in 0.040s. Bundle: 910Kb. Modules: 16. \n ✨  Done in 0.30s.\n``` \n Testing \n ```sh\n$ yarn test \n yarn run v1.12.3\n$ bsb -make-world && jest\nninja: no work to do.\nninja: no work to do.\nninja: no work to do.\nPASS  lib/js/ tests /model_test.bs.js\n some test\n     ✓ passes! (5ms) \n Test Suites: 1 passed, 1 total\nTests:       1 passed, 1 total\nSnapshots:   0 total\nTime:        1.572s\nRan all test suites.\n✨  Done in 2.81s.\n```']
hemel-cse,['flash \n Flash content', 'gocode', 'mysql-tools \n mysql-tools  is package which provides set of mysql  MySQL  tools and some little and useful go packages. \n The first one is mysql client with support of autocomplete and other features. Its main goal to make CLI interaction with  MySQL  servers as human-friendly as possible. The  mysql-tools  package provides simple  mysql-cli  command that allows to connect to a  MySQL  server and execute  SQL  or  MySQL  internal commands. \n Install \n TODO \n Usage \n TODO \n Features \n Autocomplete for following commands \n \n ~ USE ~ \n ~ CREATE DATABASE ~ \n ~ DROP DATABASE ~ \n ... \n \n Other packages \n \n termios \n terminfo \n \n Contributions \n Feel free to create issues or pull-requests if you have any problems. \n Please read  CONTRIBUTING.md  before pushing any changes. \n LICENSE \n \n Author \n @0xAX', 'xliff-translate \n Replace target tag text of xliff with the translated string from csv \n Please see the notebook file for more in details. \n Thanks.', 'Test App React', 'Installation & Run \n Requirements \n You’ll need to have Node >= 6 on your local development machine \n Commands \n Clone the repo \n git clone https://github.com/hemel-cse/number-convertion.git \n Install basic requirements \n cd number-convertion \n npm install \n Start the app at localhost \n npm start  or  yarn start \n Folder Structure \n After creation, your project should look like this: \n my-app/\n  README.md\n  node_modules/\n  package.json\n  public/\n    index.html\n    favicon.ico\n  src/\n    App.css\n    App.js\n    App.test.js\n    index.css\n    index.js\n    logo.svg \n For the project to build,  these files must exist with exact filenames : \n \n public/index.html  is the page template; \n src/index.js  is the JavaScript entry point. \n \n You can delete or rename the other files. \n Available Scripts \n In the project directory, you can run: \n npm start \n Runs the app in the development mode. \nOpen  http://localhost:3000  to view it in the browser. \n The page will reload if you make edits. \nYou will also see any lint errors in the console. \n npm test \n Launches the test runner in the interactive watch mode. \nSee the section about  running tests  for more information. \n npm run build \n Builds the app for production to the  build  folder. \nIt correctly bundles React in production mode and optimizes the build for the best performance. \n The build is minified and the filenames include the hashes. \nYour app is ready to be deployed!', 'login-rest \n \n \n Login rest. Check out the project\'s  documentation . \n Prerequisites \n \n Docker   \n Travis CLI \n Heroku Toolbelt \n \n Local Development \n Start the dev server for local development:\n bash\ndocker-compose up \n Run a command inside the docker container: \n bash\ndocker-compose run --rm web [command] \n Continuous Deployment \n Deployment is automated via Travis. When builds pass on the master or qa branch, Travis will deploy that branch to Heroku. Follow these steps to enable this feature. \n Initialize the production server: \n heroku create login-rest-prod --remote prod && \\\n    heroku addons:create newrelic:wayne --app login-rest-prod && \\\n    heroku addons:create heroku-postgresql:hobby-dev --app login-rest-prod && \\\n    heroku config:set DJANGO_SECRET_KEY=`openssl rand -base64 32` \\\n        DJANGO_AWS_ACCESS_KEY_ID="Add your id" \\\n        DJANGO_AWS_SECRET_ACCESS_KEY="Add your key" \\\n        DJANGO_AWS_STORAGE_BUCKET_NAME="login-rest-prod" \\\n        DJANGO_CONFIGURATION="Production" \\\n        DJANGO_SETTINGS_MODULE="login-rest.config" \\\n        --app login-rest-prod \n Initialize the qa server: \n heroku create login-rest-qa --remote qa && \\\n    heroku addons:create newrelic:wayne --app login-rest-qa && \\\n    heroku addons:create heroku-postgresql:hobby-dev --app login-rest-qa && \\\n    heroku config:set DJANGO_SECRET_KEY=`openssl rand -base64 32` \\\n        DJANGO_AWS_ACCESS_KEY_ID="Add your id" \\\n        DJANGO_AWS_SECRET_ACCESS_KEY="Add your key" \\\n        DJANGO_AWS_STORAGE_BUCKET_NAME="login-rest-qa" \\\n        DJANGO_CONFIGURATION="Production" \\\n        DJANGO_SETTINGS_MODULE="login-rest.config" \\\n        --app login-rest-qa \n Securely add your Heroku credentials to Travis so that it can automatically deploy your changes: \n bash\ntravis encrypt HEROKU_AUTH_TOKEN="$(heroku auth:token)" --add \n Commit your changes and push to master and qa to trigger your first deploys: \n bash\ngit commit -a -m "ci(travis): add Heroku credentials" && \\\ngit push origin master:qa && \\\ngit push origin master \n You\'re now ready to continuously ship! ✨ 💅 🛳', '', 'Create-React-App-Redux-Saga-Logger-Router-Boilerplate \n React, React Router Dom, Redux, Redux Saga, Redux Logger \n create-react-app is great but even a basic app needs more than a component renderer. Simple boilerplate to fill the gaps. \n create-react-app + \n redux - https://redux.js.org/docs/introduction/ \n react-router-dom - https://reacttraining.com/react-router/core/ \n saga - https://redux-saga.js.org/docs/introduction/ \n Redux-Logger - https://github.com/evgenyrodionov/redux-logger \n This project was bootstrapped with Create React App. ', 'shobe \n React, Redux, Saga, Logger, Selector \n Fetch Data, Sorting, Searching \n Install the packages \n npm install \n Start the projects \n yarn start', 'Mini \n React, Redux, Saga, Logger, Selector \n Fetch Data, Sorting, Searching \n Install the packages \n npm install \n Start the projects \n yarn start', "SSLCommerz for WooCommerce \n \n WooCommerce plugin for SSLCommerz payment gateway with IPN Support. \n \n Getting Started \n Prerequisites \n \n Wordpress 5.1.* \n WooCommerce 3.6.* \n cURL php extension. \n \n Installation \n \n Download zip file or Clone the repository. \n Unzip if downloaded zip file. \n Move the whole directory to  /wp-content/plugins/ \n Activate the plugin through the 'Plugins' menu in admin panel. \n \n Configuration \n \n Open Admin Panel. \n Navigate to  Woocommerce > Settings > Payments  tab. \n \n \n \n Click on SSLCommerz to edit the settings. If you do not see SSLCommerz in the list at the top of the screen make sure you have activated the plugin in the WordPress Plugin Manager. \n Enable the Payment Method, give a proper title and description to show on the checkout page,  fill up stroe id and store passowrd fields carefully, select success and fail/cancel page. \n Find the IPN URL. It will look like  [your site]/index.php?sslcommerzipn . Copy this URL, it will be needed in the next step. \n Login to your SSLCommerz merchant panel. Navigate to  My Stores > IPN Settings  from menu. Find  IPN at HTTP Listner  section at the bottom of the page. \n Paste the URL obtained from step 5. Check the box labeled  Enable HTTP Listner . Click  Save . \n Setup is complete. Check if everything is working properly. \n \n FAQ \n What is WooCommerce? \n \n WooCommerce is an open-source e-commerce plugin for WordPress.  \n \n What is SSLCommerz? \n \n SSLCOMMERZ is the first payment gateway in Bangladesh opening doors for merchants to receive payments on the internet via their online stores. \n \n What is a Payment Gateway? \n \n Payment Gateway is a service that allows merchant to accept secure credit card transactions online. It essentially connects a merchant website to a transaction processor like bank to take payment from a customer for an order. \n \n Contributors \n \n Prabal Mallick,\nC.M. Sayedur Rahman,\nRakibul Islam \n", 'This project was bootstrapped with  Create React App . \n Below you will find some information on how to perform common tasks. \nYou can find the most recent version of this guide  here . \n Table of Contents \n \n Updating to New Releases \n Sending Feedback \n Folder Structure \n Available Scripts \n npm start \n npm test \n npm run build \n npm run eject \n Supported Language Features and Polyfills \n Syntax Highlighting in the Editor \n Displaying Lint Output in the Editor \n Debugging in the Editor \n Formatting Code Automatically \n Changing the Page  <title> \n Installing a Dependency \n Importing a Component \n Code Splitting \n Adding a Stylesheet \n Post-Processing CSS \n Adding a CSS Preprocessor (Sass, Less etc.) \n Adding Images, Fonts, and Files \n Using the  public  Folder \n Changing the HTML \n Adding Assets Outside of the Module System \n When to Use the  public  Folder \n Using Global Variables \n Adding Bootstrap \n Using a Custom Theme \n Adding Flow \n Adding Custom Environment Variables \n Referencing Environment Variables in the HTML \n Adding Temporary Environment Variables In Your Shell \n Adding Development Environment Variables In  .env \n Can I Use Decorators? \n Integrating with an API Backend \n Node \n Ruby on Rails \n Proxying API Requests in Development \n "Invalid Host Header" Errors After Configuring Proxy \n Configuring the Proxy Manually \n Configuring a WebSocket Proxy \n Using HTTPS in Development \n Generating Dynamic  <meta>  Tags on the Server \n Pre-Rendering into Static HTML Files \n Injecting Data from the Server into the Page \n Running Tests \n Filename Conventions \n Command Line Interface \n Version Control Integration \n Writing Tests \n Testing Components \n Using Third Party Assertion Libraries \n Initializing Test Environment \n Focusing and Excluding Tests \n Coverage Reporting \n Continuous Integration \n Disabling jsdom \n Snapshot Testing \n Editor Integration \n Developing Components in Isolation \n Getting Started with Storybook \n Getting Started with Styleguidist \n Making a Progressive Web App \n Opting Out of Caching \n Offline-First Considerations \n Progressive Web App Metadata \n Analyzing the Bundle Size \n Deployment \n Static Server \n Other Solutions \n Serving Apps with Client-Side Routing \n Building for Relative Paths \n Azure \n Firebase \n GitHub Pages \n Heroku \n Netlify \n Now \n S3 and CloudFront \n Surge \n Advanced Configuration \n Troubleshooting \n npm start  doesn’t detect changes \n npm test  hangs on macOS Sierra \n npm run build  exits too early \n npm run build  fails on Heroku \n npm run build  fails to minify \n Moment.js locales are missing \n Something Missing? \n \n Updating to New Releases \n Create React App is divided into two packages: \n \n create-react-app  is a global command-line utility that you use to create new projects. \n react-scripts  is a development dependency in the generated projects (including this one). \n \n You almost never need to update  create-react-app  itself: it delegates all the setup to  react-scripts . \n When you run  create-react-app , it always creates the project with the latest version of  react-scripts  so you’ll get all the new features and improvements in newly created apps automatically. \n To update an existing project to a new version of  react-scripts ,  open the changelog , find the version you’re currently on (check  package.json  in this folder if you’re not sure), and apply the migration instructions for the newer versions. \n In most cases bumping the  react-scripts  version in  package.json  and running  npm install  in this folder should be enough, but it’s good to consult the  changelog  for potential breaking changes. \n We commit to keeping the breaking changes minimal so you can upgrade  react-scripts  painlessly. \n Sending Feedback \n We are always open to  your feedback . \n Folder Structure \n After creation, your project should look like this: \n my-app/\n  README.md\n  node_modules/\n  package.json\n  public/\n    index.html\n    favicon.ico\n  src/\n    App.css\n    App.js\n    App.test.js\n    index.css\n    index.js\n    logo.svg \n For the project to build,  these files must exist with exact filenames : \n \n public/index.html  is the page template; \n src/index.js  is the JavaScript entry point. \n \n You can delete or rename the other files. \n You may create subdirectories inside  src . For faster rebuilds, only files inside  src  are processed by Webpack. \nYou need to  put any JS and CSS files inside  src , otherwise Webpack won’t see them. \n Only files inside  public  can be used from  public/index.html . \nRead instructions below for using assets from JavaScript and HTML. \n You can, however, create more top-level directories. \nThey will not be included in the production build so you can use them for things like documentation. \n Available Scripts \n In the project directory, you can run: \n npm start \n Runs the app in the development mode. \nOpen  http://localhost:3000  to view it in the browser. \n The page will reload if you make edits. \nYou will also see any lint errors in the console. \n npm test \n Launches the test runner in the interactive watch mode. \nSee the section about  running tests  for more information. \n npm run build \n Builds the app for production to the  build  folder. \nIt correctly bundles React in production mode and optimizes the build for the best performance. \n The build is minified and the filenames include the hashes. \nYour app is ready to be deployed! \n See the section about  deployment  for more information. \n npm run eject \n Note: this is a one-way operation. Once you  eject , you can’t go back! \n If you aren’t satisfied with the build tool and configuration choices, you can  eject  at any time. This command will remove the single build dependency from your project. \n Instead, it will copy all the configuration files and the transitive dependencies (Webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except  eject  will still work, but they will point to the copied scripts so you can tweak them. At this point you’re on your own. \n You don’t have to ever use  eject . The curated feature set is suitable for small and middle deployments, and you shouldn’t feel obligated to use this feature. However we understand that this tool wouldn’t be useful if you couldn’t customize it when you are ready for it. \n Supported Language Features and Polyfills \n This project supports a superset of the latest JavaScript standard. \nIn addition to  ES6  syntax features, it also supports: \n \n Exponentiation Operator  (ES2016). \n Async/await  (ES2017). \n Object Rest/Spread Properties  (stage 3 proposal). \n Dynamic import()  (stage 3 proposal) \n Class Fields and Static Properties  (stage 2 proposal). \n JSX  and  Flow  syntax. \n \n Learn more about  different proposal stages . \n While we recommend to use experimental proposals with some caution, Facebook heavily uses these features in the product code, so we intend to provide  codemods  if any of these proposals change in the future. \n Note that  the project only includes a few ES6  polyfills : \n \n Object.assign()  via  object-assign . \n Promise  via  promise . \n fetch()  via  whatwg-fetch . \n \n If you use any other ES6+ features that need  runtime support  (such as  Array.from()  or  Symbol ), make sure you are including the appropriate polyfills manually, or that the browsers you are targeting already support them. \n Syntax Highlighting in the Editor \n To configure the syntax highlighting in your favorite text editor, head to the  relevant Babel documentation page  and follow the instructions. Some of the most popular editors are covered. \n Displaying Lint Output in the Editor \n \n Note: this feature is available with  react-scripts@0.2.0  and higher. \nIt also only works with npm 3 or higher. \n \n Some editors, including Sublime Text, Atom, and Visual Studio Code, provide plugins for ESLint. \n They are not required for linting. You should see the linter output right in your terminal as well as the browser console. However, if you prefer the lint results to appear right in your editor, there are some extra steps you can do. \n You would need to install an ESLint plugin for your editor first. Then, add a file called  .eslintrc  to the project root: \n js\n{\n  "extends": "react-app"\n} \n Now your editor should report the linting warnings. \n Note that even if you edit your  .eslintrc  file further, these changes will  only affect the editor integration . They won’t affect the terminal and in-browser lint output. This is because Create React App intentionally provides a minimal set of rules that find common mistakes. \n If you want to enforce a coding style for your project, consider using  Prettier  instead of ESLint style rules. \n Debugging in the Editor \n This feature is currently only supported by  Visual Studio Code  and  WebStorm . \n Visual Studio Code and WebStorm support debugging out of the box with Create React App. This enables you as a developer to write and debug your React code without leaving the editor, and most importantly it enables you to have a continuous development workflow, where context switching is minimal, as you don’t have to switch between tools. \n Visual Studio Code \n You would need to have the latest version of  VS Code  and VS Code  Chrome Debugger Extension  installed. \n Then add the block below to your  launch.json  file and put it inside the  .vscode  folder in your app’s root directory. \n json\n{\n  "version": "0.2.0",\n  "configurations": [{\n    "name": "Chrome",\n    "type": "chrome",\n    "request": "launch",\n    "url": "http://localhost:3000",\n    "webRoot": "${workspaceRoot}/src",\n    "userDataDir": "${workspaceRoot}/.vscode/chrome",\n    "sourceMapPathOverrides": {\n      "webpack:///src/*": "${webRoot}/*"\n    }\n  }]\n} \n \n Note: the URL may be different if you\'ve made adjustments via the  HOST or PORT environment variables . \n \n Start your app by running  npm start , and start debugging in VS Code by pressing  F5  or by clicking the green debug icon. You can now write code, set breakpoints, make changes to the code, and debug your newly modified code—all from your editor. \n WebStorm \n You would need to have  WebStorm  and  JetBrains IDE Support  Chrome extension installed. \n In the WebStorm menu  Run  select  Edit Configurations... . Then click  +  and select  JavaScript Debug . Paste  http://localhost:3000  into the URL field and save the configuration. \n \n Note: the URL may be different if you\'ve made adjustments via the  HOST or PORT environment variables . \n \n Start your app by running  npm start , then press  ^D  on macOS or  F9  on Windows and Linux or click the green debug icon to start debugging in WebStorm. \n The same way you can debug your application in IntelliJ IDEA Ultimate, PhpStorm, PyCharm Pro, and RubyMine.  \n Formatting Code Automatically \n Prettier is an opinionated code formatter with support for JavaScript, CSS and JSON. With Prettier you can format the code you write automatically to ensure a code style within your project. See the  Prettier\'s GitHub page  for more information, and look at this  page to see it in action . \n To format our code whenever we make a commit in git, we need to install the following dependencies: \n sh\nnpm install --save husky lint-staged prettier \n Alternatively you may use  yarn : \n sh\nyarn add husky lint-staged prettier \n \n husky  makes it easy to use githooks as if they are npm scripts. \n lint-staged  allows us to run scripts on staged files in git. See this  blog post about lint-staged to learn more about it . \n prettier  is the JavaScript formatter we will run before commits. \n \n Now we can make sure every file is formatted correctly by adding a few lines to the  package.json  in the project root. \n Add the following line to  scripts  section: \n diff\n  "scripts": {\n+   "precommit": "lint-staged",\n    "start": "react-scripts start",\n    "build": "react-scripts build", \n Next we add a \'lint-staged\' field to the  package.json , for example: \n diff\n  "dependencies": {\n    // ...\n  },\n+ "lint-staged": {\n+   "src/**/*.{js,jsx,json,css}": [\n+     "prettier --single-quote --write",\n+     "git add"\n+   ]\n+ },\n  "scripts": { \n Now, whenever you make a commit, Prettier will format the changed files automatically. You can also run  ./node_modules/.bin/prettier --single-quote --write "src/**/*.{js,jsx}"  to format your entire project for the first time. \n Next you might want to integrate Prettier in your favorite editor. Read the section on  Editor Integration  on the Prettier GitHub page. \n Changing the Page  <title> \n You can find the source HTML file in the  public  folder of the generated project. You may edit the  <title>  tag in it to change the title from “React App” to anything else. \n Note that normally you wouldn’t edit files in the  public  folder very often. For example,  adding a stylesheet  is done without touching the HTML. \n If you need to dynamically update the page title based on the content, you can use the browser  document.title  API. For more complex scenarios when you want to change the title from React components, you can use  React Helmet , a third party library. \n If you use a custom server for your app in production and want to modify the title before it gets sent to the browser, you can follow advice in  this section . Alternatively, you can pre-build each page as a static HTML file which then loads the JavaScript bundle, which is covered  here . \n Installing a Dependency \n The generated project includes React and ReactDOM as dependencies. It also includes a set of scripts used by Create React App as a development dependency. You may install other dependencies (for example, React Router) with  npm : \n sh\nnpm install --save react-router \n Alternatively you may use  yarn : \n sh\nyarn add react-router \n This works for any library, not just  react-router . \n Importing a Component \n This project setup supports ES6 modules thanks to Babel. \nWhile you can still use  require()  and  module.exports , we encourage you to use  import  and  export  instead. \n For example: \n Button.js \n ```js\nimport React, { Component } from \'react\'; \n class Button extends Component {\n  render() {\n    // ...\n  }\n} \n export default Button; // Don’t forget to use export default!\n``` \n DangerButton.js \n ```js\nimport React, { Component } from \'react\';\nimport Button from \'./Button\'; // Import a component from another file \n class DangerButton extends Component {\n  render() {\n    return  ;\n  }\n} \n export default DangerButton;\n``` \n Be aware of the  difference between default and named exports . It is a common source of mistakes. \n We suggest that you stick to using default imports and exports when a module only exports a single thing (for example, a component). That’s what you get when you use  export default Button  and  import Button from \'./Button\' . \n Named exports are useful for utility modules that export several functions. A module may have at most one default export and as many named exports as you like. \n Learn more about ES6 modules: \n \n When to use the curly braces? \n Exploring ES6: Modules \n Understanding ES6: Modules \n \n Code Splitting \n Instead of downloading the entire app before users can use it, code splitting allows you to split your code into small chunks which you can then load on demand. \n This project setup supports code splitting via  dynamic  import() . Its  proposal  is in stage 3. The  import()  function-like form takes the module name as an argument and returns a  Promise  which always resolves to the namespace object of the module. \n Here is an example: \n moduleA.js \n ```js\nconst moduleA = \'Hello\'; \n export { moduleA };\n``` \n App.js \n ```js\nimport React, { Component } from \'react\'; \n class App extends Component {\n  handleClick = () => {\n    import(\'./moduleA\')\n      .then(({ moduleA }) => {\n        // Use moduleA\n      })\n      .catch(err => {\n        // Handle failure\n      });\n  }; \n render() {\n    return (\n       \n Load \n \n    );\n  }\n}\n export default App;\n``` \n This will make  moduleA.js  and all its unique dependencies as a separate chunk that only loads after the user clicks the \'Load\' button. \n You can also use it with  async  /  await  syntax if you prefer it. \n With React Router \n If you are using React Router check out  this tutorial  on how to use code splitting with it. You can find the companion GitHub repository  here . \n Adding a Stylesheet \n This project setup uses  Webpack  for handling all assets. Webpack offers a custom way of “extending” the concept of  import  beyond JavaScript. To express that a JavaScript file depends on a CSS file, you need to  import the CSS from the JavaScript file : \n Button.css \n css\n.Button {\n  padding: 20px;\n} \n Button.js \n ```js\nimport React, { Component } from \'react\';\nimport \'./Button.css\'; // Tell Webpack that Button.js uses these styles \n class Button extends Component {\n  render() {\n    // You can use them as regular CSS styles\n    return  ;\n  }\n}\n```\n This is not required for React  but many people find this feature convenient. You can read about the benefits of this approach  here . However you should be aware that this makes your code less portable to other build tools and environments than Webpack. \n In development, expressing dependencies this way allows your styles to be reloaded on the fly as you edit them. In production, all CSS files will be concatenated into a single minified  .css  file in the build output. \n If you are concerned about using Webpack-specific semantics, you can put all your CSS right into  src/index.css . It would still be imported from  src/index.js , but you could always remove that import if you later migrate to a different build tool. \n Post-Processing CSS \n This project setup minifies your CSS and adds vendor prefixes to it automatically through  Autoprefixer  so you don’t need to worry about it. \n For example, this: \n css\n.App {\n  display: flex;\n  flex-direction: row;\n  align-items: center;\n} \n becomes this: \n css\n.App {\n  display: -webkit-box;\n  display: -ms-flexbox;\n  display: flex;\n  -webkit-box-orient: horizontal;\n  -webkit-box-direction: normal;\n      -ms-flex-direction: row;\n          flex-direction: row;\n  -webkit-box-align: center;\n      -ms-flex-align: center;\n          align-items: center;\n} \n If you need to disable autoprefixing for some reason,  follow this section . \n Adding a CSS Preprocessor (Sass, Less etc.) \n Generally, we recommend that you don’t reuse the same CSS classes across different components. For example, instead of using a  .Button  CSS class in  <AcceptButton>  and  <RejectButton>  components, we recommend creating a  <Button>  component with its own  .Button  styles, that both  <AcceptButton>  and  <RejectButton>  can render (but  not inherit ). \n Following this rule often makes CSS preprocessors less useful, as features like mixins and nesting are replaced by component composition. You can, however, integrate a CSS preprocessor if you find it valuable. In this walkthrough, we will be using Sass, but you can also use Less, or another alternative. \n First, let’s install the command-line interface for Sass: \n sh\nnpm install --save node-sass-chokidar \n Alternatively you may use  yarn : \n sh\nyarn add node-sass-chokidar \n Then in  package.json , add the following lines to  scripts : \n diff\n   "scripts": {\n+    "build-css": "node-sass-chokidar src/ -o src/",\n+    "watch-css": "npm run build-css && node-sass-chokidar src/ -o src/ --watch --recursive",\n     "start": "react-scripts start",\n     "build": "react-scripts build",\n     "test": "react-scripts test --env=jsdom", \n \n Note: To use a different preprocessor, replace  build-css  and  watch-css  commands according to your preprocessor’s documentation. \n \n Now you can rename  src/App.css  to  src/App.scss  and run  npm run watch-css . The watcher will find every Sass file in  src  subdirectories, and create a corresponding CSS file next to it, in our case overwriting  src/App.css . Since  src/App.js  still imports  src/App.css , the styles become a part of your application. You can now edit  src/App.scss , and  src/App.css  will be regenerated. \n To share variables between Sass files, you can use Sass imports. For example,  src/App.scss  and other component style files could include  @import "./shared.scss";  with variable definitions. \n To enable importing files without using relative paths, you can add the   --include-path  option to the command in  package.json . \n "build-css": "node-sass-chokidar --include-path ./src --include-path ./node_modules src/ -o src/",\n"watch-css": "npm run build-css && node-sass-chokidar --include-path ./src --include-path ./node_modules src/ -o src/ --watch --recursive", \n This will allow you to do imports like \n scss\n@import \'styles/_colors.scss\'; // assuming a styles directory under src/\n@import \'nprogress/nprogress\'; // importing a css file from the nprogress node module \n At this point you might want to remove all CSS files from the source control, and add  src/**/*.css  to your  .gitignore  file. It is generally a good practice to keep the build products outside of the source control. \n As a final step, you may find it convenient to run  watch-css  automatically with  npm start , and run  build-css  as a part of  npm run build . You can use the  &&  operator to execute two scripts sequentially. However, there is no cross-platform way to run two scripts in parallel, so we will install a package for this: \n sh\nnpm install --save npm-run-all \n Alternatively you may use  yarn : \n sh\nyarn add npm-run-all \n Then we can change  start  and  build  scripts to include the CSS preprocessor commands: \n diff\n   "scripts": {\n     "build-css": "node-sass-chokidar src/ -o src/",\n     "watch-css": "npm run build-css && node-sass-chokidar src/ -o src/ --watch --recursive",\n-    "start": "react-scripts start",\n-    "build": "react-scripts build",\n+    "start-js": "react-scripts start",\n+    "start": "npm-run-all -p watch-css start-js",\n+    "build": "npm run build-css && react-scripts build",\n     "test": "react-scripts test --env=jsdom",\n     "eject": "react-scripts eject"\n   } \n Now running  npm start  and  npm run build  also builds Sass files. \n Why  node-sass-chokidar ? \n node-sass  has been reported as having the following issues: \n \n \n node-sass --watch  has been reported to have  performance issues  in certain conditions when used in a virtual machine or with docker. \n \n \n Infinite styles compiling  #1939 \n \n \n node-sass  has been reported as having issues with detecting new files in a directory  #1891 \n \n \n node-sass-chokidar  is used here as it addresses these issues. \n Adding Images, Fonts, and Files \n With Webpack, using static assets like images and fonts works similarly to CSS. \n You can  import  a file right in a JavaScript module . This tells Webpack to include that file in the bundle. Unlike CSS imports, importing a file gives you a string value. This value is the final path you can reference in your code, e.g. as the  src  attribute of an image or the  href  of a link to a PDF. \n To reduce the number of requests to the server, importing images that are less than 10,000 bytes returns a  data URI  instead of a path. This applies to the following file extensions: bmp, gif, jpg, jpeg, and png. SVG files are excluded due to  #1153 . \n Here is an example: \n ```js\nimport React from \'react\';\nimport logo from \'./logo.png\'; // Tell Webpack this JS file uses this image \n console.log(logo); // /logo.84287d09.png \n function Header() {\n  // Import result is the URL of your image\n  return  ;\n} \n export default Header;\n``` \n This ensures that when the project is built, Webpack will correctly move the images into the build folder, and provide us with correct paths. \n This works in CSS too: \n css\n.Logo {\n  background-image: url(./logo.png);\n} \n Webpack finds all relative module references in CSS (they start with  ./ ) and replaces them with the final paths from the compiled bundle. If you make a typo or accidentally delete an important file, you will see a compilation error, just like when you import a non-existent JavaScript module. The final filenames in the compiled bundle are generated by Webpack from content hashes. If the file content changes in the future, Webpack will give it a different name in production so you don’t need to worry about long-term caching of assets. \n Please be advised that this is also a custom feature of Webpack. \n It is not required for React  but many people enjoy it (and React Native uses a similar mechanism for images). \nAn alternative way of handling static assets is described in the next section. \n Using the  public  Folder \n \n Note: this feature is available with  react-scripts@0.5.0  and higher. \n \n Changing the HTML \n The  public  folder contains the HTML file so you can tweak it, for example, to  set the page title .\nThe  <script>  tag with the compiled code will be added to it automatically during the build process. \n Adding Assets Outside of the Module System \n You can also add other assets to the  public  folder. \n Note that we normally encourage you to  import  assets in JavaScript files instead.\nFor example, see the sections on  adding a stylesheet  and  adding images and fonts .\nThis mechanism provides a number of benefits: \n \n Scripts and stylesheets get minified and bundled together to avoid extra network requests. \n Missing files cause compilation errors instead of 404 errors for your users. \n Result filenames include content hashes so you don’t need to worry about browsers caching their old versions. \n \n However there is an  escape hatch  that you can use to add an asset outside of the module system. \n If you put a file into the  public  folder, it will  not  be processed by Webpack. Instead it will be copied into the build folder untouched.   To reference assets in the  public  folder, you need to use a special variable called  PUBLIC_URL . \n Inside  index.html , you can use it like this: \n html\n<link rel="shortcut icon" href="%PUBLIC_URL%/favicon.ico"> \n Only files inside the  public  folder will be accessible by  %PUBLIC_URL%  prefix. If you need to use a file from  src  or  node_modules , you’ll have to copy it there to explicitly specify your intention to make this file a part of the build. \n When you run  npm run build , Create React App will substitute  %PUBLIC_URL%  with a correct absolute path so your project works even if you use client-side routing or host it at a non-root URL. \n In JavaScript code, you can use  process.env.PUBLIC_URL  for similar purposes: \n js\nrender() {\n  // Note: this is an escape hatch and should be used sparingly!\n  // Normally we recommend using `import` for getting asset URLs\n  // as described in “Adding Images and Fonts” above this section.\n  return <img src={process.env.PUBLIC_URL + \'/img/logo.png\'} />;\n} \n Keep in mind the downsides of this approach: \n \n None of the files in  public  folder get post-processed or minified. \n Missing files will not be called at compilation time, and will cause 404 errors for your users. \n Result filenames won’t include content hashes so you’ll need to add query arguments or rename them every time they change. \n \n When to Use the  public  Folder \n Normally we recommend importing  stylesheets ,  images, and fonts  from JavaScript.\nThe  public  folder is useful as a workaround for a number of less common cases: \n \n You need a file with a specific name in the build output, such as  manifest.webmanifest . \n You have thousands of images and need to dynamically reference their paths. \n You want to include a small script like  pace.js  outside of the bundled code. \n Some library may be incompatible with Webpack and you have no other option but to include it as a  <script>  tag. \n \n Note that if you add a  <script>  that declares global variables, you also need to read the next section on using them. \n Using Global Variables \n When you include a script in the HTML file that defines global variables and try to use one of these variables in the code, the linter will complain because it cannot see the definition of the variable. \n You can avoid this by reading the global variable explicitly from the  window  object, for example: \n js\nconst $ = window.$; \n This makes it obvious you are using a global variable intentionally rather than because of a typo. \n Alternatively, you can force the linter to ignore any line by adding  // eslint-disable-line  after it. \n Adding Bootstrap \n You don’t have to use  React Bootstrap  together with React but it is a popular library for integrating Bootstrap with React apps. If you need it, you can integrate it with Create React App by following these steps: \n Install React Bootstrap and Bootstrap from npm. React Bootstrap does not include Bootstrap CSS so this needs to be installed as well: \n sh\nnpm install --save react-bootstrap bootstrap@3 \n Alternatively you may use  yarn : \n sh\nyarn add react-bootstrap bootstrap@3 \n Import Bootstrap CSS and optionally Bootstrap theme CSS in the beginning of your  src/index.js  file: \n js\nimport \'bootstrap/dist/css/bootstrap.css\';\nimport \'bootstrap/dist/css/bootstrap-theme.css\';\n// Put any other imports below so that CSS from your\n// components takes precedence over default styles. \n Import required React Bootstrap components within  src/App.js  file or your custom component files: \n js\nimport { Navbar, Jumbotron, Button } from \'react-bootstrap\'; \n Now you are ready to use the imported React Bootstrap components within your component hierarchy defined in the render method. Here is an example  App.js  redone using React Bootstrap. \n Using a Custom Theme \n Sometimes you might need to tweak the visual styles of Bootstrap (or equivalent package). \nWe suggest the following approach: \n \n Create a new package that depends on the package you wish to customize, e.g. Bootstrap. \n Add the necessary build steps to tweak the theme, and publish your package on npm. \n Install your own theme npm package as a dependency of your app. \n \n Here is an example of adding a  customized Bootstrap  that follows these steps. \n Adding Flow \n Flow is a static type checker that helps you write code with fewer bugs. Check out this  introduction to using static types in JavaScript  if you are new to this concept. \n Recent versions of  Flow  work with Create React App projects out of the box. \n To add Flow to a Create React App project, follow these steps: \n \n Run  npm install --save flow-bin  (or  yarn add flow-bin ). \n Add  "flow": "flow"  to the  scripts  section of your  package.json . \n Run  npm run flow init  (or  yarn flow init ) to create a  .flowconfig  file  in the root directory. \n Add  // @flow  to any files you want to type check (for example, to  src/App.js ). \n \n Now you can run  npm run flow  (or  yarn flow ) to check the files for type errors.\nYou can optionally use an IDE like  Nuclide  for a better integrated experience.\nIn the future we plan to integrate it into Create React App even more closely. \n To learn more about Flow, check out  its documentation . \n Adding Custom Environment Variables \n \n Note: this feature is available with  react-scripts@0.2.3  and higher. \n \n Your project can consume variables declared in your environment as if they were declared locally in your JS files. By\ndefault you will have  NODE_ENV  defined for you, and any other environment variables starting with\n REACT_APP_ . \n The environment variables are embedded during the build time . Since Create React App produces a static HTML/CSS/JS bundle, it can’t possibly read them at runtime. To read them at runtime, you would need to load HTML into memory on the server and replace placeholders in runtime, just like  described here . Alternatively you can rebuild the app on the server anytime you change them. \n \n Note: You must create custom environment variables beginning with  REACT_APP_ . Any other variables except  NODE_ENV  will be ignored to avoid accidentally  exposing a private key on the machine that could have the same name . Changing any environment variables will require you to restart the development server if it is running. \n \n These environment variables will be defined for you on  process.env . For example, having an environment\nvariable named  REACT_APP_SECRET_CODE  will be exposed in your JS as  process.env.REACT_APP_SECRET_CODE . \n There is also a special built-in environment variable called  NODE_ENV . You can read it from  process.env.NODE_ENV . When you run  npm start , it is always equal to  \'development\' , when you run  npm test  it is always equal to  \'test\' , and when you run  npm run build  to make a production bundle, it is always equal to  \'production\' .  You cannot override  NODE_ENV  manually.  This prevents developers from accidentally deploying a slow development build to production. \n These environment variables can be useful for displaying information conditionally based on where the project is\ndeployed or consuming sensitive data that lives outside of version control. \n First, you need to have environment variables defined. For example, let’s say you wanted to consume a secret defined\nin the environment inside a  <form> : \n jsx\nrender() {\n  return (\n    <div>\n      <small>You are running this application in <b>{process.env.NODE_ENV}</b> mode.</small>\n      <form>\n        <input type="hidden" defaultValue={process.env.REACT_APP_SECRET_CODE} />\n      </form>\n    </div>\n  );\n} \n During the build,  process.env.REACT_APP_SECRET_CODE  will be replaced with the current value of the  REACT_APP_SECRET_CODE  environment variable. Remember that the  NODE_ENV  variable will be set for you automatically. \n When you load the app in the browser and inspect the  <input> , you will see its value set to  abcdef , and the bold text will show the environment provided when using  npm start : \n ```html \n \n You are running this application in  development  mode. \n \n \n \n \n ``` \n The above form is looking for a variable called  REACT_APP_SECRET_CODE  from the environment. In order to consume this\nvalue, we need to have it defined in the environment. This can be done using two ways: either in your shell or in\na  .env  file. Both of these ways are described in the next few sections. \n Having access to the  NODE_ENV  is also useful for performing actions conditionally: \n js\nif (process.env.NODE_ENV !== \'production\') {\n  analytics.disable();\n} \n When you compile the app with  npm run build , the minification step will strip out this condition, and the resulting bundle will be smaller. \n Referencing Environment Variables in the HTML \n \n Note: this feature is available with  react-scripts@0.9.0  and higher. \n \n You can also access the environment variables starting with  REACT_APP_  in the  public/index.html . For example: \n html\n<title>%REACT_APP_WEBSITE_NAME%</title> \n Note that the caveats from the above section apply: \n \n Apart from a few built-in variables ( NODE_ENV  and  PUBLIC_URL ), variable names must start with  REACT_APP_  to work. \n The environment variables are injected at build time. If you need to inject them at runtime,  follow this approach instead . \n \n Adding Temporary Environment Variables In Your Shell \n Defining environment variables can vary between OSes. It’s also important to know that this manner is temporary for the\nlife of the shell session. \n Windows (cmd.exe) \n cmd\nset REACT_APP_SECRET_CODE=abcdef&&npm start \n (Note: the lack of whitespace is intentional.) \n Linux, macOS (Bash) \n bash\nREACT_APP_SECRET_CODE=abcdef npm start \n Adding Development Environment Variables In  .env \n \n Note: this feature is available with  react-scripts@0.5.0  and higher. \n \n To define permanent environment variables, create a file called  .env  in the root of your project: \n REACT_APP_SECRET_CODE=abcdef \n .env  files  should be  checked into source control (with the exclusion of  .env*.local ). \n What other  .env  files are can be used? \n \n Note: this feature is  available with  react-scripts@1.0.0  and higher . \n \n \n .env : Default. \n .env.local : Local overrides.  This file is loaded for all environments except test. \n .env.development ,  .env.test ,  .env.production : Environment-specific settings. \n .env.development.local ,  .env.test.local ,  .env.production.local : Local overrides of environment-specific settings. \n \n Files on the left have more priority than files on the right: \n \n npm start :  .env.development.local ,  .env.development ,  .env.local ,  .env \n npm run build :  .env.production.local ,  .env.production ,  .env.local ,  .env \n npm test :  .env.test.local ,  .env.test ,  .env  (note  .env.local  is missing) \n \n These variables will act as the defaults if the machine does not explicitly set them. \nPlease refer to the  dotenv documentation  for more details. \n \n Note: If you are defining environment variables for development, your CI and/or hosting platform will most likely need\nthese defined as well. Consult their documentation how to do this. For example, see the documentation for  Travis CI  or  Heroku . \n \n Can I Use Decorators? \n Many popular libraries use  decorators  in their documentation. \nCreate React App doesn’t support decorator syntax at the moment because: \n \n It is an experimental proposal and is subject to change. \n The current specification version is not officially supported by Babel. \n If the specification changes, we won’t be able to write a codemod because we don’t use them internally at Facebook. \n \n However in many cases you can rewrite decorator-based code without decorators just as fine. \nPlease refer to these two threads for reference: \n \n #214 \n #411 \n \n Create React App will add decorator support when the specification advances to a stable stage. \n Integrating with an API Backend \n These tutorials will help you to integrate your app with an API backend running on another port,\nusing  fetch()  to access it. \n Node \n Check out  this tutorial .\nYou can find the companion GitHub repository  here . \n Ruby on Rails \n Check out  this tutorial .\nYou can find the companion GitHub repository  here . \n Proxying API Requests in Development \n \n Note: this feature is available with  react-scripts@0.2.3  and higher. \n \n People often serve the front-end React app from the same host and port as their backend implementation. \nFor example, a production setup might look like this after the app is deployed: \n /             - static server returns index.html with React app\n/todos        - static server returns index.html with React app\n/api/todos    - server handles any /api/* requests using the backend implementation \n Such setup is  not  required. However, if you  do  have a setup like this, it is convenient to write requests like  fetch(\'/api/todos\')  without worrying about redirecting them to another host or port during development. \n To tell the development server to proxy any unknown requests to your API server in development, add a  proxy  field to your  package.json , for example: \n js\n  "proxy": "http://localhost:4000", \n This way, when you  fetch(\'/api/todos\')  in development, the development server will recognize that it’s not a static asset, and will proxy your request to  http://localhost:4000/api/todos  as a fallback. The development server will only attempt to send requests without a  text/html  accept header to the proxy. \n Conveniently, this avoids  CORS issues  and error messages like this in development: \n Fetch API cannot load http://localhost:4000/api/todos. No \'Access-Control-Allow-Origin\' header is present on the requested resource. Origin \'http://localhost:3000\' is therefore not allowed access. If an opaque response serves your needs, set the request\'s mode to \'no-cors\' to fetch the resource with CORS disabled. \n Keep in mind that  proxy  only has effect in development (with  npm start ), and it is up to you to ensure that URLs like  /api/todos  point to the right thing in production. You don’t have to use the  /api  prefix. Any unrecognized request without a  text/html  accept header will be redirected to the specified  proxy . \n The  proxy  option supports HTTP, HTTPS and WebSocket connections. \nIf the  proxy  option is  not  flexible enough for you, alternatively you can: \n \n Configure the proxy yourself \n Enable CORS on your server ( here’s how to do it for Express ). \n Use  environment variables  to inject the right server host and port into your app. \n \n "Invalid Host Header" Errors After Configuring Proxy \n When you enable the  proxy  option, you opt into a more strict set of host checks. This is necessary because leaving the backend open to remote hosts makes your computer vulnerable to DNS rebinding attacks. The issue is explained in  this article  and  this issue . \n This shouldn’t affect you when developing on  localhost , but if you develop remotely like  described here , you will see this error in the browser after enabling the  proxy  option: \n \n Invalid Host header \n \n To work around it, you can specify your public development host in a file called  .env.development  in the root of your project: \n HOST=mypublicdevhost.com \n If you restart the development server now and load the app from the specified host, it should work. \n If you are still having issues or if you’re using a more exotic environment like a cloud editor, you can bypass the host check completely by adding a line to  .env.development.local .  Note that this is dangerous and exposes your machine to remote code execution from malicious websites: \n ``` \n NOTE: THIS IS DANGEROUS! \n It exposes your machine to attacks from the websites you visit. \n DANGEROUSLY_DISABLE_HOST_CHECK=true\n``` \n We don’t recommend this approach. \n Configuring the Proxy Manually \n \n Note: this feature is available with  react-scripts@1.0.0  and higher. \n \n If the  proxy  option is  not  flexible enough for you, you can specify an object in the following form (in  package.json ). \nYou may also specify any configuration value  http-proxy-middleware  or  http-proxy  supports.\n js\n{\n  // ...\n  "proxy": {\n    "/api": {\n      "target": "<url>",\n      "ws": true\n      // ...\n    }\n  }\n  // ...\n} \n All requests matching this path will be proxies, no exceptions. This includes requests for  text/html , which the standard  proxy  option does not proxy. \n If you need to specify multiple proxies, you may do so by specifying additional entries.\nYou may also narrow down matches using  *  and/or  ** , to match the path exactly or any subpath.\n js\n{\n  // ...\n  "proxy": {\n    // Matches any request starting with /api\n    "/api": {\n      "target": "<url_1>",\n      "ws": true\n      // ...\n    },\n    // Matches any request starting with /foo\n    "/foo": {\n      "target": "<url_2>",\n      "ssl": true,\n      "pathRewrite": {\n        "^/foo": "/foo/beta"\n      }\n      // ...\n    },\n    // Matches /bar/abc.html but not /bar/sub/def.html\n    "/bar/*.html": {\n      "target": "<url_3>",\n      // ...\n    },\n    // Matches /baz/abc.html and /baz/sub/def.html\n    "/baz/**/*.html": {\n      "target": "<url_4>"\n      // ...\n    }\n  }\n  // ...\n} \n Configuring a WebSocket Proxy \n When setting up a WebSocket proxy, there are a some extra considerations to be aware of. \n If you’re using a WebSocket engine like  Socket.io , you must have a Socket.io server running that you can use as the proxy target. Socket.io will not work with a standard WebSocket server. Specifically, don\'t expect Socket.io to work with  the websocket.org echo test . \n There’s some good documentation available for  setting up a Socket.io server . \n Standard WebSockets  will  work with a standard WebSocket server as well as the websocket.org echo test. You can use libraries like  ws  for the server, with  native WebSockets in the browser . \n Either way, you can proxy WebSocket requests manually in  package.json : \n js\n{\n  // ...\n  "proxy": {\n    "/socket": {\n      // Your compatible WebSocket server\n      "target": "ws://<socket_url>",\n      // Tell http-proxy-middleware that this is a WebSocket proxy.\n      // Also allows you to proxy WebSocket requests without an additional HTTP request\n      // https://github.com/chimurai/http-proxy-middleware#external-websocket-upgrade\n      "ws": true\n      // ...\n    }\n  }\n  // ...\n} \n Using HTTPS in Development \n \n Note: this feature is available with  react-scripts@0.4.0  and higher. \n \n You may require the dev server to serve pages over HTTPS. One particular case where this could be useful is when using  the "proxy" feature  to proxy requests to an API server when that API server is itself serving HTTPS. \n To do this, set the  HTTPS  environment variable to  true , then start the dev server as usual with  npm start : \n Windows (cmd.exe) \n cmd\nset HTTPS=true&&npm start \n (Note: the lack of whitespace is intentional.) \n Linux, macOS (Bash) \n bash\nHTTPS=true npm start \n Note that the server will use a self-signed certificate, so your web browser will almost definitely display a warning upon accessing the page. \n Generating Dynamic  <meta>  Tags on the Server \n Since Create React App doesn’t support server rendering, you might be wondering how to make  <meta>  tags dynamic and reflect the current URL. To solve this, we recommend to add placeholders into the HTML, like this: \n ```html \n html \n \n \n \n \n```\n Then, on the server, regardless of the backend you use, you can read  index.html  into memory and replace  __OG_TITLE__ ,  __OG_DESCRIPTION__ , and any other placeholders with values depending on the current URL. Just make sure to sanitize and escape the interpolated values so that they are safe to embed into HTML! \n If you use a Node server, you can even share the route matching logic between the client and the server. However duplicating it also works fine in simple cases. \n Pre-Rendering into Static HTML Files \n If you’re hosting your  build  with a static hosting provider you can use  react-snapshot  to generate HTML pages for each route, or relative link, in your application. These pages will then seamlessly become active, or “hydrated”, when the JavaScript bundle has loaded. \n There are also opportunities to use this outside of static hosting, to take the pressure off the server when generating and caching routes. \n The primary benefit of pre-rendering is that you get the core content of each page  with  the HTML payload—regardless of whether or not your JavaScript bundle successfully downloads. It also increases the likelihood that each route of your application will be picked up by search engines. \n You can read more about  zero-configuration pre-rendering (also called snapshotting) here . \n Injecting Data from the Server into the Page \n Similarly to the previous section, you can leave some placeholders in the HTML that inject global variables, for example: \n ```js \n html \n \n \n \n      window.SERVER_DATA = <strong>SERVER_DATA</strong>;\n     \n```\n Then, on the server, you can replace  __SERVER_DATA__  with a JSON of real data right before sending the response. The client code can then read  window.SERVER_DATA  to use it.  Make sure to  sanitize the JSON before sending it to the client  as it makes your app vulnerable to XSS attacks. \n Running Tests \n \n Note: this feature is available with  react-scripts@0.3.0  and higher. \n Read the migration guide to learn how to enable it in older projects! \n \n Create React App uses  Jest  as its test runner. To prepare for this integration, we did a  major revamp  of Jest so if you heard bad things about it years ago, give it another try. \n Jest is a Node-based runner. This means that the tests always run in a Node environment and not in a real browser. This lets us enable fast iteration speed and prevent flakiness. \n While Jest provides browser globals such as  window  thanks to  jsdom , they are only approximations of the real browser behavior. Jest is intended to be used for unit tests of your logic and your components rather than the DOM quirks. \n We recommend that you use a separate tool for browser end-to-end tests if you need them. They are beyond the scope of Create React App. \n Filename Conventions \n Jest will look for test files with any of the following popular naming conventions: \n \n Files with  .js  suffix in  __tests__  folders. \n Files with  .test.js  suffix. \n Files with  .spec.js  suffix. \n \n The  .test.js  /  .spec.js  files (or the  __tests__  folders) can be located at any depth under the  src  top level folder. \n We recommend to put the test files (or  __tests__  folders) next to the code they are testing so that relative imports appear shorter. For example, if  App.test.js  and  App.js  are in the same folder, the test just needs to  import App from \'./App\'  instead of a long relative path. Colocation also helps find tests more quickly in larger projects. \n Command Line Interface \n When you run  npm test , Jest will launch in the watch mode. Every time you save a file, it will re-run the tests, just like  npm start  recompiles the code. \n The watcher includes an interactive command-line interface with the ability to run all tests, or focus on a search pattern. It is designed this way so that you can keep it open and enjoy fast re-runs. You can learn the commands from the “Watch Usage” note that the watcher prints after every run: \n \n Version Control Integration \n By default, when you run  npm test , Jest will only run the tests related to files changed since the last commit. This is an optimization designed to make your tests run fast regardless of how many tests you have. However it assumes that you don’t often commit the code that doesn’t pass the tests. \n Jest will always explicitly mention that it only ran tests related to the files changed since the last commit. You can also press  a  in the watch mode to force Jest to run all tests. \n Jest will always run all tests on a  continuous integration  server or if the project is not inside a Git or Mercurial repository. \n Writing Tests \n To create tests, add  it()  (or  test() ) blocks with the name of the test and its code. You may optionally wrap them in  describe()  blocks for logical grouping but this is neither required nor recommended. \n Jest provides a built-in  expect()  global function for making assertions. A basic test could look like this: \n ```js\nimport sum from \'./sum\'; \n it(\'sums numbers\', () => {\n  expect(sum(1, 2)).toEqual(3);\n  expect(sum(2, 2)).toEqual(4);\n});\n``` \n All  expect()  matchers supported by Jest are  extensively documented here . \nYou can also use  jest.fn()  and  expect(fn).toBeCalled()  to create “spies” or mock functions. \n Testing Components \n There is a broad spectrum of component testing techniques. They range from a “smoke test” verifying that a component renders without throwing, to shallow rendering and testing some of the output, to full rendering and testing component lifecycle and state changes. \n Different projects choose different testing tradeoffs based on how often components change, and how much logic they contain. If you haven’t decided on a testing strategy yet, we recommend that you start with creating simple smoke tests for your components: \n ```js\nimport React from \'react\';\nimport ReactDOM from \'react-dom\';\nimport App from \'./App\'; \n it(\'renders without crashing\', () => {\n  const div = document.createElement(\'div\');\n  ReactDOM.render( , div);\n});\n``` \n This test mounts a component and makes sure that it didn’t throw during rendering. Tests like this provide a lot value with very little effort so they are great as a starting point, and this is the test you will find in  src/App.test.js . \n When you encounter bugs caused by changing components, you will gain a deeper insight into which parts of them are worth testing in your application. This might be a good time to introduce more specific tests asserting specific expected output or behavior. \n If you’d like to test components in isolation from the child components they render, we recommend using  shallow()  rendering API  from  Enzyme . To install it, run: \n sh\nnpm install --save enzyme react-test-renderer \n Alternatively you may use  yarn : \n sh\nyarn add enzyme react-test-renderer \n You can write a smoke test with it too: \n ```js\nimport React from \'react\';\nimport { shallow } from \'enzyme\';\nimport App from \'./App\'; \n it(\'renders without crashing\', () => {\n  shallow( );\n});\n``` \n Unlike the previous smoke test using  ReactDOM.render() , this test only renders  <App>  and doesn’t go deeper. For example, even if  <App>  itself renders a  <Button>  that throws, this test will pass. Shallow rendering is great for isolated unit tests, but you may still want to create some full rendering tests to ensure the components integrate correctly. Enzyme supports  full rendering with  mount() , and you can also use it for testing state changes and component lifecycle. \n You can read the  Enzyme documentation  for more testing techniques. Enzyme documentation uses Chai and Sinon for assertions but you don’t have to use them because Jest provides built-in  expect()  and  jest.fn()  for spies. \n Here is an example from Enzyme documentation that asserts specific output, rewritten to use Jest matchers: \n ```js\nimport React from \'react\';\nimport { shallow } from \'enzyme\';\nimport App from \'./App\'; \n it(\'renders welcome message\', () => {\n  const wrapper = shallow( );\n  const welcome =  Welcome to React ;\n  // expect(wrapper.contains(welcome)).to.equal(true);\n  expect(wrapper.contains(welcome)).toEqual(true);\n});\n```\n All Jest matchers are  extensively documented here . \nNevertheless you can use a third-party assertion library like  Chai  if you want to, as described below. \n Additionally, you might find  jest-enzyme  helpful to simplify your tests with readable matchers. The above  contains  code can be written simpler with jest-enzyme. \n js\nexpect(wrapper).toContainReact(welcome) \n To enable this, install  jest-enzyme : \n sh\nnpm install --save jest-enzyme \n Alternatively you may use  yarn : \n sh\nyarn add jest-enzyme \n Import it in  src/setupTests.js  to make its matchers available in every test: \n js\nimport \'jest-enzyme\'; \n Using Third Party Assertion Libraries \n We recommend that you use  expect()  for assertions and  jest.fn()  for spies. If you are having issues with them please  file those against Jest , and we’ll fix them. We intend to keep making them better for React, supporting, for example,  pretty-printing React elements as JSX . \n However, if you are used to other libraries, such as  Chai  and  Sinon , or if you have existing code using them that you’d like to port over, you can import them normally like this: \n js\nimport sinon from \'sinon\';\nimport { expect } from \'chai\'; \n and then use them in your tests like you normally do. \n Initializing Test Environment \n \n Note: this feature is available with  react-scripts@0.4.0  and higher. \n \n If your app uses a browser API that you need to mock in your tests or if you just need a global setup before running your tests, add a  src/setupTests.js  to your project. It will be automatically executed before running your tests. \n For example: \n src/setupTests.js \n js\nconst localStorageMock = {\n  getItem: jest.fn(),\n  setItem: jest.fn(),\n  clear: jest.fn()\n};\nglobal.localStorage = localStorageMock \n Focusing and Excluding Tests \n You can replace  it()  with  xit()  to temporarily exclude a test from being executed. \nSimilarly,  fit()  lets you focus on a specific test without running any other tests. \n Coverage Reporting \n Jest has an integrated coverage reporter that works well with ES6 and requires no configuration. \nRun  npm test -- --coverage  (note extra  --  in the middle) to include a coverage report like this: \n \n Note that tests run much slower with coverage so it is recommended to run it separately from your normal workflow. \n Continuous Integration \n By default  npm test  runs the watcher with interactive CLI. However, you can force it to run tests once and finish the process by setting an environment variable called  CI . \n When creating a build of your application with  npm run build  linter warnings are not checked by default. Like  npm test , you can force the build to perform a linter warning check by setting the environment variable  CI . If any warnings are encountered then the build fails. \n Popular CI servers already set the environment variable  CI  by default but you can do this yourself too: \n On CI servers \n Travis CI \n \n Following the  Travis Getting started  guide for syncing your GitHub repository with Travis.  You may need to initialize some settings manually in your  profile  page. \n Add a  .travis.yml  file to your git repository.\n```\nlanguage: node_js\nnode_js: \n 6\ncache:\n  directories: \n node_modules\nscript: \n \n \n npm run build \n npm test\n``` \n Trigger your first build with a git push. \n Customize your Travis CI Build  if needed. \n \n CircleCI \n Follow  this article  to set up CircleCI with a Create React App project. \n On your own environment \n Windows (cmd.exe) \n cmd\nset CI=true&&npm test \n cmd\nset CI=true&&npm run build \n (Note: the lack of whitespace is intentional.) \n Linux, macOS (Bash) \n bash\nCI=true npm test \n bash\nCI=true npm run build \n The test command will force Jest to run tests once instead of launching the watcher. \n \n If you find yourself doing this often in development, please  file an issue  to tell us about your use case because we want to make watcher the best experience and are open to changing how it works to accommodate more workflows. \n \n The build command will check for linter warnings and fail if any are found. \n Disabling jsdom \n By default, the  package.json  of the generated project looks like this: \n js\n  "scripts": {\n    "start": "react-scripts start",\n    "build": "react-scripts build",\n    "test": "react-scripts test --env=jsdom" \n If you know that none of your tests depend on  jsdom , you can safely remove  --env=jsdom , and your tests will run faster: \n diff\n  "scripts": {\n    "start": "react-scripts start",\n    "build": "react-scripts build",\n-   "test": "react-scripts test --env=jsdom"\n+   "test": "react-scripts test" \n To help you make up your mind, here is a list of APIs that  need jsdom : \n \n Any browser globals like  window  and  document \n ReactDOM.render() \n TestUtils.renderIntoDocument()  ( a shortcut  for the above) \n mount()  in  Enzyme \n \n In contrast,  jsdom is not needed  for the following APIs: \n \n TestUtils.createRenderer()  (shallow rendering) \n shallow()  in  Enzyme \n \n Finally, jsdom is also not needed for  snapshot testing . \n Snapshot Testing \n Snapshot testing is a feature of Jest that automatically generates text snapshots of your components and saves them on the disk so if the UI output changes, you get notified without manually writing any assertions on the component output.  Read more about snapshot testing. \n Editor Integration \n If you use  Visual Studio Code , there is a  Jest extension  which works with Create React App out of the box. This provides a lot of IDE-like features while using a text editor: showing the status of a test run with potential fail messages inline, starting and stopping the watcher automatically, and offering one-click snapshot updates. \n \n Developing Components in Isolation \n Usually, in an app, you have a lot of UI components, and each of them has many different states.\nFor an example, a simple button component could have following states: \n \n In a regular state, with a text label. \n In the disabled mode. \n In a loading state. \n \n Usually, it’s hard to see these states without running a sample app or some examples. \n Create React App doesn’t include any tools for this by default, but you can easily add  Storybook for React  ( source ) or  React Styleguidist  ( source ) to your project.  These are third-party tools that let you develop components and see all their states in isolation from your app . \n \n You can also deploy your Storybook or style guide as a static app. This way, everyone in your team can view and review different states of UI components without starting a backend server or creating an account in your app. \n Getting Started with Storybook \n Storybook is a development environment for React UI components. It allows you to browse a component library, view the different states of each component, and interactively develop and test components. \n First, install the following npm package globally: \n sh\nnpm install -g @storybook/cli \n Then, run the following command inside your app’s directory: \n sh\ngetstorybook \n After that, follow the instructions on the screen. \n Learn more about React Storybook: \n \n Screencast:  Getting Started with React Storybook \n GitHub Repo \n Documentation \n Snapshot Testing UI  with Storybook + addon/storyshot \n \n Getting Started with Styleguidist \n Styleguidist combines a style guide, where all your components are presented on a single page with their props documentation and usage examples, with an environment for developing components in isolation, similar to Storybook. In Styleguidist you write examples in Markdown, where each code snippet is rendered as a live editable playground. \n First, install Styleguidist: \n sh\nnpm install --save react-styleguidist \n Alternatively you may use  yarn : \n sh\nyarn add react-styleguidist \n Then, add these scripts to your  package.json : \n diff\n   "scripts": {\n+    "styleguide": "styleguidist server",\n+    "styleguide:build": "styleguidist build",\n     "start": "react-scripts start", \n Then, run the following command inside your app’s directory: \n sh\nnpm run styleguide \n After that, follow the instructions on the screen. \n Learn more about React Styleguidist: \n \n GitHub Repo \n Documentation \n \n Making a Progressive Web App \n By default, the production build is a fully functional, offline-first\n Progressive Web App . \n Progressive Web Apps are faster and more reliable than traditional web pages, and provide an engaging mobile experience: \n \n All static site assets are cached so that your page loads fast on subsequent visits, regardless of network connectivity (such as 2G or 3G). Updates are downloaded in the background. \n Your app will work regardless of network state, even if offline. This means your users will be able to use your app at 10,000 feet and on the Subway. \n On mobile devices, your app can be added directly to the user\'s home screen, app icon and all. You can also re-engage users using web  push notifications . This eliminates the need for the app store. \n \n The  sw-precache-webpack-plugin \nis integrated into production configuration,\nand it will take care of generating a service worker file that will automatically\nprecache all of your local assets and keep them up to date as you deploy updates.\nThe service worker will use a  cache-first strategy \nfor handling all requests for local assets, including the initial HTML, ensuring\nthat your web app is reliably fast, even on a slow or unreliable network. \n Opting Out of Caching \n If you would prefer not to enable service workers prior to your initial\nproduction deployment, then remove the call to  serviceWorkerRegistration.register() \nfrom  src/index.js . \n If you had previously enabled service workers in your production deployment and\nhave decided that you would like to disable them for all your existing users,\nyou can swap out the call to  serviceWorkerRegistration.register()  in\n src/index.js  with a call to  serviceWorkerRegistration.unregister() .\nAfter the user visits a page that has  serviceWorkerRegistration.unregister() ,\nthe service worker will be uninstalled. Note that depending on how  /service-worker.js  is served,\nit may take up to 24 hours for the cache to be invalidated. \n Offline-First Considerations \n \n \n Service workers  require HTTPS ,\nalthough to facilitate local testing, that policy\n does not apply to  localhost .\nIf your production web server does not support HTTPS, then the service worker\nregistration will fail, but the rest of your web app will remain functional. \n \n \n Service workers are  not currently supported \nin all web browsers. Service worker registration  won\'t be attempted \non browsers that lack support. \n \n \n The service worker is only enabled in the  production environment ,\ne.g. the output of  npm run build . It\'s recommended that you do not enable an\noffline-first service worker in a development environment, as it can lead to\nfrustration when previously cached assets are used and do not include the latest\nchanges you\'ve made locally. \n \n \n If you  need  to test your offline-first service worker locally, build\nthe application (using  npm run build ) and run a simple http server from your\nbuild directory. After running the build script,  create-react-app  will give\ninstructions for one way to test your production build locally and the  deployment instructions  have\ninstructions for using other methods.  Be sure to always use an\nincognito window to avoid complications with your browser cache. \n \n \n If possible, configure your production environment to serve the generated\n service-worker.js   with HTTP caching disabled .\nIf that\'s not possible— GitHub Pages , for instance, does not\nallow you to change the default 10 minute HTTP cache lifetime—then be aware\nthat if you visit your production site, and then revisit again before\n service-worker.js  has expired from your HTTP cache, you\'ll continue to get\nthe previously cached assets from the service worker. If you have an immediate\nneed to view your updated production deployment, performing a shift-refresh\nwill temporarily disable the service worker and retrieve all assets from the\nnetwork. \n \n \n Users aren\'t always familiar with offline-first web apps. It can be useful to\n let the user know \nwhen the service worker has finished populating your caches (showing a "This web\napp works offline!" message) and also let them know when the service worker has\nfetched the latest updates that will be available the next time they load the\npage (showing a "New content is available; please refresh." message). Showing\nthis messages is currently left as an exercise to the developer, but as a\nstarting point, you can make use of the logic included in  src/registerServiceWorker.js , which\ndemonstrates which service worker lifecycle events to listen for to detect each\nscenario, and which as a default, just logs appropriate messages to the\nJavaScript console. \n \n \n By default, the generated service worker file will not intercept or cache any\ncross-origin traffic, like HTTP  API requests ,\nimages, or embeds loaded from a different domain. If you would like to use a\nruntime caching strategy for those requests, you can  eject \nand then configure the\n runtimeCaching \noption in the  SWPrecacheWebpackPlugin  section of\n webpack.config.prod.js . \n \n \n Progressive Web App Metadata \n The default configuration includes a web app manifest located at\n public/manifest.json , that you can customize with\ndetails specific to your web application. \n When a user adds a web app to their homescreen using Chrome or Firefox on\nAndroid, the metadata in  manifest.json  determines what\nicons, names, and branding colors to use when the web app is displayed.\n The Web App Manifest guide \nprovides more context about what each field means, and how your customizations\nwill affect your users\' experience. \n Analyzing the Bundle Size \n Source map explorer  analyzes\nJavaScript bundles using the source maps. This helps you understand where code\nbloat is coming from. \n To add Source map explorer to a Create React App project, follow these steps: \n sh\nnpm install --save source-map-explorer \n Alternatively you may use  yarn : \n sh\nyarn add source-map-explorer \n Then in  package.json , add the following line to  scripts : \n diff\n   "scripts": {\n+    "analyze": "source-map-explorer build/static/js/main.*",\n     "start": "react-scripts start",\n     "build": "react-scripts build",\n     "test": "react-scripts test --env=jsdom", \n Then to analyze the bundle run the production build then run the analyze\nscript. \n npm run build\nnpm run analyze \n Deployment \n npm run build  creates a  build  directory with a production build of your app. Set up your favourite HTTP server so that a visitor to your site is served  index.html , and requests to static paths like  /static/js/main.<hash>.js  are served with the contents of the  /static/js/main.<hash>.js  file. \n Static Server \n For environments using  Node , the easiest way to handle this would be to install  serve  and let it handle the rest: \n sh\nnpm install -g serve\nserve -s build \n The last command shown above will serve your static site on the port  5000 . Like many of  serve ’s internal settings, the port can be adjusted using the  -p  or  --port  flags. \n Run this command to get a full list of the options available: \n sh\nserve -h \n Other Solutions \n You don’t necessarily need a static server in order to run a Create React App project in production. It works just as fine integrated into an existing dynamic one. \n Here’s a programmatic example using  Node  and  Express : \n ```javascript\nconst express = require(\'express\');\nconst path = require(\'path\');\nconst app = express(); \n app.use(express.static(path.join(__dirname, \'build\'))); \n app.get(\'/\', function (req, res) {\n  res.sendFile(path.join(__dirname, \'build\', \'index.html\'));\n}); \n app.listen(9000);\n``` \n The choice of your server software isn’t important either. Since Create React App is completely platform-agnostic, there’s no need to explicitly use Node. \n The  build  folder with static assets is the only output produced by Create React App. \n However this is not quite enough if you use client-side routing. Read the next section if you want to support URLs like  /todos/42  in your single-page app. \n Serving Apps with Client-Side Routing \n If you use routers that use the HTML5  pushState  history API  under the hood (for example,  React Router  with  browserHistory ), many static file servers will fail. For example, if you used React Router with a route for  /todos/42 , the development server will respond to  localhost:3000/todos/42  properly, but an Express serving a production build as above will not. \n This is because when there is a fresh page load for a  /todos/42 , the server looks for the file  build/todos/42  and does not find it. The server needs to be configured to respond to a request to  /todos/42  by serving  index.html . For example, we can amend our Express example above to serve  index.html  for any unknown paths: \n ```diff\n app.use(express.static(path.join(__dirname, \'build\'))); \n -app.get(\'/\', function (req, res) {\n+app.get(\'/*\', function (req, res) {\n   res.sendFile(path.join(__dirname, \'build\', \'index.html\'));\n });\n``` \n If you’re using  Apache HTTP Server , you need to create a  .htaccess  file in the  public  folder that looks like this: \n Options -MultiViews\n    RewriteEngine On\n    RewriteCond %{REQUEST_FILENAME} !-f\n    RewriteRule ^ index.html [QSA,L] \n It will get copied to the  build  folder when you run  npm run build .  \n If you’re using  Apache Tomcat , you need to follow  this Stack Overflow answer . \n Now requests to  /todos/42  will be handled correctly both in development and in production. \n On a production build, and in a browser that supports  service workers ,\nthe service worker will automatically handle all navigation requests, like for\n /todos/42 , by serving the cached copy of your  index.html . This\nservice worker navigation routing can be configured or disabled by\n eject ing  and then modifying the\n navigateFallback \nand  navigateFallbackWhitelist \noptions of the  SWPreachePlugin   configuration . \n Building for Relative Paths \n By default, Create React App produces a build assuming your app is hosted at the server root. \nTo override this, specify the  homepage  in your  package.json , for example: \n js\n  "homepage": "http://mywebsite.com/relativepath", \n This will let Create React App correctly infer the root path to use in the generated HTML file. \n Note : If you are using  react-router@^4 , you can root  <Link> s using the  basename  prop on any  <Router> . \nMore information  here . \n \nFor example:\n js\n<BrowserRouter basename="/calendar"/>\n<Link to="/today"/> // renders <a href="/calendar/today"> \n Serving the Same Build from Different Paths \n \n Note: this feature is available with  react-scripts@0.9.0  and higher. \n \n If you are not using the HTML5  pushState  history API or not using client-side routing at all, it is unnecessary to specify the URL from which your app will be served. Instead, you can put this in your  package.json : \n js\n  "homepage": ".", \n This will make sure that all the asset paths are relative to  index.html . You will then be able to move your app from  http://mywebsite.com  to  http://mywebsite.com/relativepath  or even  http://mywebsite.com/relative/path  without having to rebuild it. \n Azure \n See  this  blog post on how to deploy your React app to  Microsoft Azure . \n Firebase \n Install the Firebase CLI if you haven’t already by running  npm install -g firebase-tools . Sign up for a  Firebase account  and create a new project. Run  firebase login  and login with your previous created Firebase account. \n Then run the  firebase init  command from your project’s root. You need to choose the  Hosting: Configure and deploy Firebase Hosting sites  and choose the Firebase project you created in the previous step. You will need to agree with  database.rules.json  being created, choose  build  as the public directory, and also agree to  Configure as a single-page app  by replying with  y . \n ```sh\n    === Project Setup \n First, let\'s associate this project directory with a Firebase project.\nYou can create multiple project aliases by running firebase use --add,\nbut for now we\'ll just set up a default project.\n\n? What Firebase project do you want to associate as default? Example app (example-app-fd690)\n\n=== Database Setup\n\nFirebase Realtime Database Rules allow you to define how your data should be\nstructured and when your data can be read from and written to.\n\n? What file should be used for Database Rules? database.rules.json\n✔  Database Rules for example-app-fd690 have been downloaded to database.rules.json.\nFuture modifications to database.rules.json will update Database Rules when you run\nfirebase deploy.\n\n=== Hosting Setup\n\nYour public directory is the folder (relative to your project directory) that\nwill contain Hosting assets to uploaded with firebase deploy. If you\nhave a build process for your assets, use your build\'s output directory.\n\n? What do you want to use as your public directory? build\n? Configure as a single-page app (rewrite all urls to /index.html)? Yes\n✔  Wrote build/index.html\n\ni  Writing configuration info to firebase.json...\ni  Writing project information to .firebaserc...\n\n✔  Firebase initialization complete!\n \n ``` \n Now, after you create a production build with  npm run build , you can deploy it by running  firebase deploy . \n ```sh\n    === Deploying to \'example-app-fd690\'... \n i  deploying database, hosting\n✔  database: rules ready to deploy.\ni  hosting: preparing build directory for upload...\nUploading: [==============================          ] 75%✔  hosting: build folder uploaded successfully\n✔  hosting: 8 files uploaded successfully\ni  starting release process (may take several minutes)...\n\n✔  Deploy complete!\n\nProject Console: https://console.firebase.google.com/project/example-app-fd690/overview\nHosting URL: https://example-app-fd690.firebaseapp.com\n \n ``` \n For more information see  Add Firebase to your JavaScript Project . \n GitHub Pages \n \n Note: this feature is available with  react-scripts@0.2.0  and higher. \n \n Step 1: Add  homepage  to  package.json \n The step below is important! \n If you skip it, your app will not deploy correctly. \n Open your  package.json  and add a  homepage  field: \n js\n  "homepage": "https://myusername.github.io/my-app", \n Create React App uses the  homepage  field to determine the root URL in the built HTML file. \n Step 2: Install  gh-pages  and add  deploy  to  scripts  in  package.json \n Now, whenever you run  npm run build , you will see a cheat sheet with instructions on how to deploy to GitHub Pages. \n To publish it at  https://myusername.github.io/my-app , run: \n sh\nnpm install --save gh-pages \n Alternatively you may use  yarn : \n sh\nyarn add gh-pages \n Add the following scripts in your  package.json : \n diff\n  "scripts": {\n+   "predeploy": "npm run build",\n+   "deploy": "gh-pages -d build",\n    "start": "react-scripts start",\n    "build": "react-scripts build", \n The  predeploy  script will run automatically before  deploy  is run. \n Step 3: Deploy the site by running  npm run deploy \n Then run: \n sh\nnpm run deploy \n Step 4: Ensure your project’s settings use  gh-pages \n Finally, make sure  GitHub Pages  option in your GitHub project settings is set to use the  gh-pages  branch: \n \n Step 5: Optionally, configure the domain \n You can configure a custom domain with GitHub Pages by adding a  CNAME  file to the  public/  folder. \n Notes on client-side routing \n GitHub Pages doesn’t support routers that use the HTML5  pushState  history API under the hood (for example, React Router using  browserHistory ). This is because when there is a fresh page load for a url like  http://user.github.io/todomvc/todos/42 , where  /todos/42  is a frontend route, the GitHub Pages server returns 404 because it knows nothing of  /todos/42 . If you want to add a router to a project hosted on GitHub Pages, here are a couple of solutions: \n \n You could switch from using HTML5 history API to routing with hashes. If you use React Router, you can switch to  hashHistory  for this effect, but the URL will be longer and more verbose (for example,  http://user.github.io/todomvc/#/todos/42?_k=yknaj ).  Read more  about different history implementations in React Router. \n Alternatively, you can use a trick to teach GitHub Pages to handle 404 by redirecting to your  index.html  page with a special redirect parameter. You would need to add a  404.html  file with the redirection code to the  build  folder before deploying your project, and you’ll need to add code handling the redirect parameter to  index.html . You can find a detailed explanation of this technique  in this guide . \n \n Heroku \n Use the  Heroku Buildpack for Create React App . \nYou can find instructions in  Deploying React with Zero Configuration . \n Resolving Heroku Deployment Errors \n Sometimes  npm run build  works locally but fails during deploy via Heroku. Following are the most common cases. \n "Module not found: Error: Cannot resolve \'file\' or \'directory\'" \n If you get something like this: \n remote: Failed to create a production build. Reason:\nremote: Module not found: Error: Cannot resolve \'file\' or \'directory\'\nMyDirectory in /tmp/build_1234/src \n It means you need to ensure that the lettercase of the file or directory you  import  matches the one you see on your filesystem or on GitHub. \n This is important because Linux (the operating system used by Heroku) is case sensitive. So  MyDirectory  and  mydirectory  are two distinct directories and thus, even though the project builds locally, the difference in case breaks the  import  statements on Heroku remotes. \n "Could not find a required file." \n If you exclude or ignore necessary files from the package you will see a error similar this one: \n remote: Could not find a required file.\nremote:   Name: `index.html`\nremote:   Searched in: /tmp/build_a2875fc163b209225122d68916f1d4df/public\nremote:\nremote: npm ERR! Linux 3.13.0-105-generic\nremote: npm ERR! argv "/tmp/build_a2875fc163b209225122d68916f1d4df/.heroku/node/bin/node" "/tmp/build_a2875fc163b209225122d68916f1d4df/.heroku/node/bin/npm" "run" "build" \n In this case, ensure that the file is there with the proper lettercase and that’s not ignored on your local  .gitignore  or  ~/.gitignore_global . \n Netlify \n To do a manual deploy to Netlify’s CDN: \n sh\nnpm install netlify-cli\nnetlify deploy \n Choose  build  as the path to deploy. \n To setup continuous delivery: \n With this setup Netlify will build and deploy when you push to git or open a pull request: \n \n Start a new netlify project \n Pick your Git hosting service and select your repository \n Click  Build your site \n \n Support for client-side routing: \n To support  pushState , make sure to create a  public/_redirects  file with the following rewrite rules: \n /*  /index.html  200 \n When you build the project, Create React App will place the  public  folder contents into the build output. \n Now \n now  offers a zero-configuration single-command deployment. You can use  now  to deploy your app for free. \n \n \n Install the  now  command-line tool either via the recommended  desktop tool  or via node with  npm install -g now . \n \n \n Build your app by running  npm run build . \n \n \n Move into the build directory by running  cd build . \n \n \n Run  now --name your-project-name  from within the build directory. You will see a  now.sh  URL in your output like this: \n ``` \n \n Ready! https://your-project-name-tpspyhtdtk.now.sh (copied to clipboard)\n``` \n \n Paste that URL into your browser when the build is complete, and you will see your deployed app. \n \n \n Details are available in  this article. \n S3 and CloudFront \n See this  blog post  on how to deploy your React app to Amazon Web Services  S3  and  CloudFront . \n Surge \n Install the Surge CLI if you haven’t already by running  npm install -g surge . Run the  surge  command and log in you or create a new account. \n When asked about the project path, make sure to specify the  build  folder, for example: \n sh\n       project path: /path/to/project/build \n Note that in order to support routers that use HTML5  pushState  API, you may want to rename the  index.html  in your build folder to  200.html  before deploying to Surge. This  ensures that every URL falls back to that file . \n Advanced Configuration \n You can adjust various development and production settings by setting environment variables in your shell or with  .env . \n Variable | Development | Production | Usage\n:--- | :---: | :---: | :---\nBROWSER | :white_check_mark: | :x: | By default, Create React App will open the default system browser, favoring Chrome on macOS. Specify a  browser  to override this behavior, or set it to  none  to disable it completely. If you need to customize the way the browser is launched, you can specify a node script instead. Any arguments passed to  npm start  will also be passed to this script, and the url where your app is served will be the last argument. Your script\'s file name must have the  .js  extension.\nHOST | :white_check_mark: | :x: | By default, the development web server binds to  localhost . You may use this variable to specify a different host.\nPORT | :white_check_mark: | :x: | By default, the development web server will attempt to listen on port 3000 or prompt you to attempt the next available port. You may use this variable to specify a different port.\nHTTPS | :white_check_mark: | :x: | When set to  true , Create React App will run the development server in  https  mode.\nPUBLIC_URL | :x: | :white_check_mark: | Create React App assumes your application is hosted at the serving web server\'s root or a subpath as specified in  package.json  ( homepage ) . Normally, Create React App ignores the hostname. You may use this variable to force assets to be referenced verbatim to the url you provide (hostname included). This may be particularly useful when using a CDN to host your application.\nCI | :large_orange_diamond: | :white_check_mark: | When set to  true , Create React App treats warnings as failures in the build. It also makes the test runner non-watching. Most CIs set this flag by default.\nREACT_EDITOR | :white_check_mark: | :x: | When an app crashes in development, you will see an error overlay with clickable stack trace. When you click on it, Create React App will try to determine the editor you are using based on currently running processes, and open the relevant source file. You can  send a pull request to detect your editor of choice . Setting this environment variable overrides the automatic detection. If you do it, make sure your systems  PATH  environment variable points to your editor’s bin folder.\nCHOKIDAR_USEPOLLING | :white_check_mark: | :x: | When set to  true , the watcher runs in polling mode, as necessary inside a VM. Use this option if  npm start  isn\'t detecting changes.\nGENERATE_SOURCEMAP | :x: | :white_check_mark: | When set to  false , source maps are not generated for a production build. This solves OOM issues on some smaller machines. \n Troubleshooting \n npm start  doesn’t detect changes \n When you save a file while  npm start  is running, the browser should refresh with the updated code. \nIf this doesn’t happen, try one of the following workarounds: \n \n If your project is in a Dropbox folder, try moving it out. \n If the watcher doesn’t see a file called  index.js  and you’re referencing it by the folder name, you  need to restart the watcher  due to a Webpack bug. \n Some editors like Vim and IntelliJ have a “safe write” feature that currently breaks the watcher. You will need to disable it. Follow the instructions in  “Adjusting Your Text Editor” . \n If your project path contains parentheses, try moving the project to a path without them. This is caused by a  Webpack watcher bug . \n On Linux and macOS, you might need to  tweak system settings  to allow more watchers. \n If the project runs inside a virtual machine such as (a Vagrant provisioned) VirtualBox, create an  .env  file in your project directory if it doesn’t exist, and add  CHOKIDAR_USEPOLLING=true  to it. This ensures that the next time you run  npm start , the watcher uses the polling mode, as necessary inside a VM. \n \n If none of these solutions help please leave a comment  in this thread . \n npm test  hangs on macOS Sierra \n If you run  npm test  and the console gets stuck after printing  react-scripts test --env=jsdom  to the console there might be a problem with your  Watchman  installation as described in  facebookincubator/create-react-app#713 . \n We recommend deleting  node_modules  in your project and running  npm install  (or  yarn  if you use it) first. If it doesn\'t help, you can try one of the numerous workarounds mentioned in these issues: \n \n facebook/jest#1767 \n facebook/watchman#358 \n ember-cli/ember-cli#6259 \n \n It is reported that installing Watchman 4.7.0 or newer fixes the issue. If you use  Homebrew , you can run these commands to update it: \n watchman shutdown-server\nbrew update\nbrew reinstall watchman \n You can find  other installation methods  on the Watchman documentation page. \n If this still doesn’t help, try running  launchctl unload -F ~/Library/LaunchAgents/com.github.facebook.watchman.plist . \n There are also reports that  uninstalling  Watchman fixes the issue. So if nothing else helps, remove it from your system and try again. \n npm run build  exits too early \n It is reported that  npm run build  can fail on machines with limited memory and no swap space, which is common in cloud environments. Even with small projects this command can increase RAM usage in your system by hundreds of megabytes, so if you have less than 1 GB of available memory your build is likely to fail with the following message: \n \n The build failed because the process exited too early. This probably means the system ran out of memory or someone called  kill -9  on the process. \n \n If you are completely sure that you didn\'t terminate the process, consider  adding some swap space  to the machine you’re building on, or build the project locally. \n npm run build  fails on Heroku \n This may be a problem with case sensitive filenames.\nPlease refer to  this section . \n Moment.js locales are missing \n If you use a  Moment.js , you might notice that only the English locale is available by default. This is because the locale files are large, and you probably only need a subset of  all the locales provided by Moment.js . \n To add a specific Moment.js locale to your bundle, you need to import it explicitly. \nFor example: \n js\nimport moment from \'moment\';\nimport \'moment/locale/fr\'; \n If import multiple locales this way, you can later switch between them by calling  moment.locale()  with the locale name: \n ```js\nimport moment from \'moment\';\nimport \'moment/locale/fr\';\nimport \'moment/locale/es\'; \n // ... \n moment.locale(\'fr\');\n``` \n This will only work for locales that have been explicitly imported before. \n npm run build  fails to minify \n You may occasionally find a package you depend on needs compiled or ships code for a non-browser environment. \nThis is considered poor practice in the ecosystem and does not have an escape hatch in Create React App. \n \nTo resolve this:\n1. Open an issue on the dependency\'s issue tracker and ask that the package be published pre-compiled (retaining ES6 Modules).\n2. Fork the package and publish a corrected version yourself.\n3. If the dependency is small enough, copy it to your  src/  folder and treat it as application code. \n Something Missing? \n If you have ideas for more “How To” recipes that should be on this page,  let us know  or  contribute some!', "DatoCMS example blog using React & GraphQL Request \n Set up your own \n By clicking the following button you'll set up a project on DatoCMS with the schema and data that you need to run this example. \n \n How to start \n Add a  .env  file with your read-only API token, so that React can access the information on your project: \n echo 'REACT_APP_DATO_API_TOKEN=abc123' >> .env \n then run: \n yarn && yarn start \n Read more \n This project was bootstrapped with  Create React App . \n GraphQL Request documentation can be found  here", 'pixagram \n Mobile App for search, download and save photos from pixabay.com \n Install the library with  expo install \n Run the App with  expo start \n Initial Boilerplate with Expo Latest version (38.0.8) \n \n Solved the isssue with React Navigation \n Solved the issue with gasture handler \n']
fabiocerqueira,['Hash Hunter \n Projeto iniciado durante o BrazilJS para estudo de JavaScript, Node.JS, ExpressJS, YQL. \n Como funciona? \n O objetivo é acompanhar uma hashtag em diversas redes sociais em tempo real. \n Executando localmente \n Se você quiser executar o projeto localmente, seja para contribuir ou porque você estava sem internet quando precisou,\nbasta seguir os seguintes passos: \n Faça um clone do repositório \n git clone git://github.com/fabiocerqueira/hashunter.git\n \n Execute o servidor de desenvolvimento do node  \n node app.js\n \n Dependências \n npm install express\nnpm install yql\nnpm install jqtpl\n', 'Django Dash Project 2011 \n Pylestras \n Pylestras is a application for \n Team \n \n Fábio Cerqueira \n Italo Maia \n Mário Chaves \n \n Evento \n Antes do evento \n \n Criar evento \n Adicionar palestras \n Adicionar informações sobre evento (local, valor, hashtag...) \n Adicionar patrocinadores, apoio, realização \n Botão de "eu vou!" para share na redes sociais \n \n Durante o evento \n \n Timeline do twitter para acompanhar hashtag do evento \n Espaço para stream de vídeo usando serviço de terceiros \n \n Depois do evento \n \n atualizar palestras com slides/videos \n adicionar fotos do eventos(usando picasa ou flickr) \n', 'nbio \n SDK biometric reader Nitgen HFDU04 using Python (Linux) \n WARNING: This project was discontinued', 'CRUD comentado de PlayFramework com AngularJS', 'dotfiles \n Dependencies \n \n git \n vim \n make \n bash_completion \n ctags \n flake8 (optional) \n powerline-go \n i3 \n tmux \n \n Installation: \n bash\n$ cd ~\n$ git clone --recursive https://github.com/fabiocerqueira/dotfiles.git\n$ make install', 'IPython além do simples shell', 'Mapa da Python Brasil 11 \n Página de acesso -  https://mapa-pybr11.herokuapp.com/']
Randl,['\n Склонение падежей русских имён, фамилий и отчеств. Вы задаёте начальное имя\nв именительном падеже, а получаете в нужном вам. \n Портированная версия https://github.com/rocsci/petrovich с ruby на c++ \n Зависимости \n Для работы требуюется библиотека yaml-cpp(http://code.google.com/p/yaml-cpp/) версии 0.5.1 или выше. \n Использование \n ```cpp \n include "petrovich.h" \n include  \n std::cout << Petrovich::Instance()->Fullname("Воронов Иван Сергеевич", "instrumental"); //Вороновым Иваном Сергеевичем\nstd::cout << Petrovich::Instance()->Fullname("Макаров Пётр Антонович", "предложный");\nstd::cout << Petrovich::Instance()->Firstname("Григорий","dative","m" );\nstd::cout << Petrovich::Instance()->Lastname("Меркулов", "винительный");\nstd::cout << Petrovich::Instance()->Lastname("Мурай", "родительный","мужской");\n```\nКласс включает в себя функции Fullname, Firstname, Middlename и Lastname для полного\nимени(фио), имени, отчества и фамилии соответсвенно. Функции принимают 3 параметра: \nстроку с именем, строку с падежом(на русском или английском) и строку с полом - \nнеобязательный параметр. Пол может быть указан в одном из 4 форматов - мужской/женский,\nм/ж, male/female или m/f. Для отчества и полного имени, если пол не указан, то он \nопределяется автоматически. Пол также можно определить по отчеству с помощью функции \nDetectGender. \n | Параметр | Падеж        | Характеризующий вопрос |\n|----------------|--------------|------------------------|\n| genetive  | родительный  | Кого? Чего?            |\n| dative    | дательный    | Кому? Чему?            |\n| accusative| винительный  | Кого? Что?             |\n| instrumental   | творительный | Кем? Чем?              |\n| prepositional  | предложный   | О ком? О чём?          |', "Gambit Fruit is a free open source project based on Fruit 2.1 and Toga. The goal of Gambit Fruit is a fun free and aggresive chess engine.\nTo turn off Lazy Eval set Chess knowledge to 500. \n If you have any suggestions please email them to me. \n Bitbases can be found  here  and latest egbbdll  here  or  here \n Features \n Added:\n- 6-men EGBB\n-  Texel's Tuning Method .\n- Pawn count - no pawns is bad in endgame \n TODO:\n* SMP( Lazy SMP );\n* Bugs and warning fixes. Static analysis;\n* Converting to c++11/c++14/c++17;\n*  Progressive mobility  and  connectivity ;\n* Kingside/queenside attack;\n*  Pawn ram  handling + open/closed position based on this;\n*  Pawn islands ;\n* Tune extensions (King safety extension, sacrifice extension, extend on K+P eg) and reductions;\n* Better draw recog(e.g. Blockage detection, KQKP);\n* Values tuning (Pawn Shielding, King safety, tropism, etc);\n* Typical sacs;\n*  Pawn chains ;\n* Custom opening book;\n*  Chess960  support(( guide );\n* Center control (With B as a bonus for a given square, this is B [AT+2 OC] where AT is #attackers and OC is #occupants.);\n* Bad bishop;\n* Elephantiasis effect as suggested by Harm Geert Muller (meaning that stronger pieces lose part of their value in presence of weaker pieces);\n* Syzygy support \n Thanks for everyone contributed to Gambit Fruit, Fruit 2.1, Toga, and to those who gave me ideas. \n Building \n Gambit Fruit and all the utilities are built with cmake.", 'Tris \n Tic-tac-toe written in Qt. \n \n \n Required: \n \n QMake \n \n Compilation: \n \n cd Tris \n qmake \n make \n \n Usage: \n \n Double Click on Tris (Executable File) \n', 'This script downloads all existing  Technion  courses with their prerequisites, description, number of weeklyhours and other information.', 'Implementation of Biham–Middleton–Levine traffic model in Python', 'WassersteinGAN.tensorflow \n Tensorflow implementation of Arjovsky et al.\'s  Wasserstein GAN \n \n Prerequisites \n Results \n Observations \n References and related links \n \n Note: The paper refers to discriminators as critic. I use these names interchangably in my thoughts below. \n A pretty interesting paper that takes on the problem of stability in GANs and interpretability of the loss function during training. GANs essentially are models that try to learn the distribution of real data by minimizing f-divergence (difference in probabilty distribution) by generating adversarial data. The convergence in min max objective of the originally proposed GAN can be interpreted as minimizing the Jensen Shannon (JS) divergence. In this paper, the authors point out the shortcomings in such metrics when the support of the two distributions being compared do not overlap and propose using the earth movers/wasserstein distance as an alternative to JS. The parallel lines example provides a nice intuition to the differences in the f-divergence metrices. Note that when the f-divergence is discrete as in JS, KL we might face problems in learning models with gradients as the divergence loss is not differetiable everywhere. \n Theorem 1 proposed in the paper is probably the key takeaway for anyone wondering why wasserstein distance might help in training GANS. The theorem basically states that a distribution mapping function (critic) that is continuous with respect to its parameters and locally lipschitz has a continuous and almost everywhere differentiable wasserstein distance. \n A continuos and almost everywhere differentiable metric would mean we can strongly train the discriminator before doing an update to the generator which in turn would receive improved reliable gradients to train from the discriminator. With the earlier formulations of GAN such training was not possible since training discriminator strongly would lead to vanishing gradients. \n Given that neural networks are generally continuous w.r.t to its parameters, the thing to make sure is the critic being Lipschitz. By clipping the weight parameters in the critic, we prevent the model from saturating while the growth is made atmost linear. This would mean the gradients of the function is bounded by the slope of this linearity becoming Lipschitz bound. \n Prerequisites \n \n Code was tested in Linux system with Titan GPU.  \n Model was trained with tensorflow v0.11 and python2.7. Newer versions of tensorflow requires updating the summary statements to avoid depreceated warnings. \n CelebA dataset should be downloaded and unzipped manually.  Download link \n Default arguments to  main.py  runs GAN with cross entropy objective. \n run_main.sh  has command to run Wasserstein GAN model. \n \n Results \n \n \n The network architecture used to train the model is very similar to that used in the original DCGAN. This is different from what is implemented in the pytorch version of the code released with the paper - Both the generator and discriminator have "extra layers" of stride one. \n \n \n All bias terms in the network are removed. I\'m not quite sure what the justification for dropping the bias in generator but with the critic it might have to do with constraing the function to a smaller lipschitz bound. \n \n \n The results below are after 1e5 iterations which took approximately 18hrs in my system. This is probably not the most converged result so consider it with a pinch of salt. \n \n \n Random sample of images generated after training GAN with wasserstein distance for 1e5 itrs, lr=5e-5, RMSPropOptimizer.\n \n For comparison: Random sample of images generated using GAN with cross entropy objective for 2e4 itrs, lr=2e-4, AdamOptimizer.\n \n Observations \n \n After spending quite a while to get the theory in the paper, I was suprised and pleased at how simple the implementation was.\n  Major changes from the point of implementations are \n The discriminator/critic no longer produces sigmoid or probabilistic output. The loss in discriminator is simple the difference in output between real and generated images. \n Train critic multiple times for each generator update.  \n The weights in the critic is clamped to small values around zero. \n Requires low learning rate and optimizers that do not use momentum. \n \n Training is very slow. This should be expected, given the very low learning rate and multiple updates to discriminator for each generator update. \n \n \n Discriminator loss for Wasserstein GAN. Note that the original paper plots the discriminator loss with a negative sign, hence the flip in the direction of the plot. From what I noticed, the general trend of the discriminator is converging but it does increase at times before dropping back.  \n \n \n \n \n \n Training to minimize wasserstein distance in this problem space can be interpreted as making the critic assign low values to real data and high values to fake data. The generator on the other hand is trying to generate images that has the critic giving it low values like the ones the real images get. In other words, the model converges when the critic is no longer able to differentiate and assign different values to generated and real images - a reason why I think calling the critic a discriminator is still a reasonable thing :smile: \n \n \n The generator as mentioned above is trying the have the critic assign low values like the ones real images get. While training the generator oscillates quite a bit around zero.\n \n \n \n Weights are clipped in critic to maintain lipschitz bound and continuity. An observation here as pointed out by the author in reddit worth highlighting \n \n The weight clipping parameter is not massively important in practice, but more investigation is required. Here are the effects of having larger clipping parameter c: \n \n \n \n \n The discriminator takes longer to train, since it has to saturate some weights at a larger value. This means that you can be a risk of having an insufficiently trained critic, which can provide bad estimates and gradients. Sometimes sign changes are required in the critic, and going from c to -c on some weights will take longer. If the generator is updated in the middle of this process the gradient can be pretty bad. \n The capacity is increased, which helps the optimaly trained disc provide better gradients. \n In general it seems that lower clipping is more stable, but higher clipping gives a better model if the critic is well trained. \n \n \n \n \n Theoretically, the claims in the paper about quality corresponding to loss is understandable given the formulation but since quality is a relative term, I missed to see improvements in my generated images with loss for all generated images i.e how much of a loss improvement corresponds to image quality improvement is unclear. Having said that, it is pretty possible that all images generated after "convergence" are realistic.  \n \n \n How this new loss term would correspond to previous works associated with GANs namely semi/unsupervised learning, adaptation, adversarial losses in computer vision tasks and such is pretty exciting and interesting. \n \n \n References and related links \n \n Pytorch implementation of WasserstienGAN by authors of the paper -  link \n Interesting discussion on r/machinelearning -  link \n', 'MobileNetv2 in PyTorch \n An implementation of  MobileNetv2  in PyTorch.  MobileNetv2  is an efficient convolutional neural network architecture for mobile devices. For more information check the paper:\n Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation   \n Usage \n Clone the repo:\n bash\ngit clone https://github.com/Randl/MobileNetV2-pytorch\npip install -r requirements.txt \n Use the model defined in  model.py  to run ImageNet example:\n bash\npython imagenet.py --dataroot "/path/to/imagenet/" \n To run continue training from checkpoint\n bash\npython imagenet.py --dataroot "/path/to/imagenet/" --resume "/path/to/checkpoint/folder" \n Results \n For x1.0 model I achieved 0.3% higher top-1 accuracy than claimed. \n |Classification Checkpoint| MACs (M)   | Parameters (M)| Top-1 Accuracy| Top-5 Accuracy|  Claimed top-1|  Claimed top-5|\n|-------------------------|------------|---------------|---------------|---------------|---------------|---------------|\n|   [mobilenet_v2_1.0_224]|300         |3.47           |          72.10|          90.48|           71.8|           91.0|\n|   [mobilenet_v2_0.5_160]|50          |1.95           |          60.61|          82.87|           61.0|           83.2| \n You can test it with\n bash\npython imagenet.py --dataroot "/path/to/imagenet/" --resume "results/mobilenet_v2_1.0_224/model_best.pth.tar" -e\npython imagenet.py --dataroot "/path/to/imagenet/" --resume "results/mobilenet_v2_0.5_160/model_best.pth.tar" -e --scaling 0.5 --input-size 160', 'Improving the Improved Training of Wasserstein GANs: A Consistency Term and Its Dual Effect \n Implementation of \n "Improving the Improved Training of Wasserstein GANs: A Consistency Term and Its Dual Effect"  in pytorch.', 'ShuffleNetv2 in PyTorch \n An implementation of  ShuffleNetv2  in PyTorch.  ShuffleNetv2  is an efficient convolutional neural network architecture for mobile devices. For more information check the paper:\n ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design \n Usage \n Clone the repo:\n bash\ngit clone https://github.com/Randl/ShuffleNetV2-pytorch\npip install -r requirements.txt \n Use the model defined in  model.py  to run ImageNet example:\n bash\npython imagenet.py --dataroot "/path/to/imagenet/" \n To continue training from checkpoint\n bash\npython imagenet.py --dataroot "/path/to/imagenet/" --resume "/path/to/checkpoint/folder" \n Results \n For x0.5 model I achieved 0.4% lower top-1 accuracy than claimed. \n |Classification Checkpoint| MACs (M)   | Parameters (M)| Top-1 Accuracy| Top-5 Accuracy|  Claimed top-1|  Claimed top-5|\n|-------------------------|------------|---------------|---------------|---------------|---------------|---------------|\n|      [shufflenet_v2_0.5]|41          |1.37           |          59.86|          81.63|           60.3|              -| \n You can test it with\n bash\npython imagenet.py --dataroot "/path/to/imagenet/" --resume "results/shufflenet_v2_0.5/model_best.pth.tar" -e --scaling 0.5', 'DropBlock in PyTorch \n An implementation of  DropBlock  in PyTorch.  DropBlock  is a replacement for dropout which zero-es \nentire spatial blocks instead of single pixels. For more information, check the paper:\n DropBlock: A regularization method for convolutional networks  ', 'MNASNet in PyTorch \n An implementation of  MNASNet  in PyTorch.  MNASNet  is an efficient\nconvolutional neural network architecture for mobile devices,\ndeveloped with architectural search. For more information check the paper:\n MnasNet: Platform-Aware Neural Architecture Search for Mobile \n The model is is implemented by\n billhhh \nand the initial idea of reproducing MNASNet is by\n snakers4 \n Usage \n Clone the repo:\n bash\ngit clone https://github.com/Randl/MNASNet-pytorch\npip install -r requirements.txt \n Use the model defined in  model.py  to run ImageNet example:\n bash\npython3 -m torch.distributed.launch --nproc_per_node=8 imagenet.py --dataroot "/path/to/imagenet/" --warmup 5 --sched cosine -lr 0.2 -b 128 -d 5e-5 --world-size 8 --seed 42 \n To continue training from checkpoint\n bash\npython imagenet.py --dataroot "/path/to/imagenet/" --resume "/path/to/checkpoint/folder" \n Results \n Initially I\'ve got 72+% top-1 accuracy, but the checkpointing didn\'t\nwork properly. I believe the results are reproducable. \n |Classification Checkpoint| MACs (M)   | Parameters (M)| Top-1 Accuracy| Top-5 Accuracy|  Claimed top-1|  Claimed top-5|\n|-------------------------|------------|---------------|---------------|---------------|---------------|---------------| \n You can test it with\n bash\npython imagenet.py --dataroot "/path/to/imagenet/" --resume "results/shufflenet_v2_0.5/model_best.pth.tar" -e \n Other implementations \n \n Mnasnet.MXNet  --  A Gluon implementation of Mnasnet, 73.6% top-1 and 91.52% top-5 \n MnasNet-pytorch-pretrained  --  A PyTorch implementation of Mnasnet, 70.132% top-1 and 89.434% top-5 \n', 'Those are lecture notes for the Technion course \n"Algorithms for Submodular Optimization" (236621) teached by Roy Schwartz\nin spring 2019.', 'MobileNetV3 in PyTorch \n An implementation of  MobileNetV3  in PyTorch.  MobileNetV3  is an efficient\nconvolutional neural network architecture for mobile devices. For more information check the paper:\n Searching for MobileNetV3 \n Usage \n Clone the repo:\n bash\ngit clone https://github.com/Randl/MobileNetV3-pytorch\npip install -r requirements.txt \n Use the model defined in  MobileNetV3.py  to run ImageNet example:\n bash\npython3 -m torch.distributed.launch --nproc_per_node=8 imagenet.py --dataroot "/path/to/imagenet/" --sched clr -b 128 --seed 42 --world-size 8 --sync-bn \n``` \n To continue training from checkpoint\n bash\npython imagenet.py --dataroot "/path/to/imagenet/" --resume "/path/to/checkpoint/folder" \n Results \n WIP \n |Classification Checkpoint | MACs (M)   | Parameters (M)| Top-1 Accuracy| Top-5 Accuracy|  Claimed top-1|  Claimed top-5|  Inference time|\n|--------------------------|------------|---------------|---------------|---------------|---------------|---------------|----------------|\n|MobileNetV3 Large x1.0 224|219.80      |5.481          |          73.53|          91.14|           75.2|              -|               ~258ms|\n|   mobilenet_v2_1.0_224 |300         |3.47           |          72.10|          90.48|           71.8|           91.0|               ~461ms| \n Inference time is for single 1080 ti per batch of 128. \n You can test it with\n bash\npython imagenet.py --dataroot "/path/to/imagenet/" --resume "results/mobilenetv3large-v1/model_best0.pth.tar" -e \n Other implementations \n \n https://github.com/d-li14/mobilenetv3.pytorch : 73.152% top-1, with more FLOPs \n https://github.com/xiaolai-sqlai/mobilenetv3 : 75.45% top-1, even more FLOPs \n https://github.com/rwightman/gen-efficientnet-pytorch : 75.634% top-1, seems to be right FLOPs \n \n Code used \n \n DropBlock implementation  by  miguelvr \n FLOPS calculator  by  warmspringwinds \n Utility function for divisibility  by  Tensorflow \n Squeeze-Excitation block  by  jonnedtc \n Custom cross-entropy  by  eladhoffer \n Shadow weights implementation  by  eladhoffer \n', 'Reimplementation of  Ranger-Mish  on pure PyTorch. \n To reproduce the results, run \n python3 main.py --dataroot /path/to/imagewoof --num-classes 10 -sa -sym --lookahead --optim ralamb --flat 3.6 --sched cosine --seed 42 \n I also ran it on ImageNet, for the same settings (i.e., 128x128 input size for 5 epochs). \nAs per  "Fixing the train-test resolution discrepancy" , \nResNet-50 achieves 73.3% top-1 accuracy if both trained and validated on 128x128 inputs.\nAfter 5 epochs (100095 batches), which took 7:22 hours, \nMXResNet-50 achieved 66.02% top-1 accuracy. \n BlurPool \n This part is based on paper  "Making Convolutional Networks Shift-Invariant Again"  and is due  Dmytro Mishkin . \n However the scheme is a bit different from the proposed in paper: \n |Operation|Proposed in paper| Used|\n|---|---|---|\n|Convolution with stride | Convolution without stride + Blurred downsample | Convolution with stride|\n|Max pooling | Max pooling without stride + Blurred downsample | Max pooling without stride + Blurred downsample|\n|Avg pooling | Blurred downsample | Max pooling without stride + Blurred downsample|\nThe orginal scheme, either full or partial seems to work worse. \n Regular MXResNet-50 gives\nfor seeds [1,2,3,4,5] the following accuracies: [0.752,0.746,0.762,0.764,0.752], i.e., the accuracy is 75.52±0.67% \n BlurPool for same seeds gives [0.776,0.79,0.762,0.768,0.756], i.e., 77.04±1.18%', 'Self-Supervised Learning for Large-Scale Unsupervised Image Clustering \n \n \n This is code to run experiments for paper  "Self-Supervised Learning for Large-Scale Unsupervised Image Clustering" . \n Running the code \n For part of the models, you\'ll need to download the chekpoints manually:\n- SimCLR models: https://github.com/google-research/simclr\n- MoCo and InfoMin models: https://github.com/HobbitLong/PyContrast/blob/master/pycontrast/docs/MODEL_ZOO.md\n- SwAV models: https://github.com/facebookresearch/swav \n and put them in chekpoint folder. \n For SimCLRv2, BigBiGAN as well as supervised models checkpoints are downloaded automatically. \n Download the code and install dependencies \n Remember to clone the submodules by running \n git clone --recurse-submodules https://github.com/Randl/kmeans_selfsuper.git \nduring cloning the repo, or, if you forgot to do it, by running \n git submodule update --init --recursive \nin the repo folder. \n You\'ll need to install dependencies, by running\n pip install -r requirements.txt \n Generating features \n For SimCLRv2 and BigBiGAN run\n python3 generate_prediction_tf.py --model resnet152_simclr2\npython3 generate_prediction_tf.py --model resnet50_simclr2\npython3 generate_prediction_tf.py --model resnet152x3_simclr2\npython3 generate_prediction_tf.py --model resnet50_bigbigan\npython3 generate_prediction_tf.py --model revnet50x4_bigbigan \nFor InfoMin, MoCo v2 and SwAV, run\n python3 generate_prediction_pytorch.py --model resnext152_infomin\npython3 generate_prediction_pytorch.py --model resnet50_infomin\npython3 generate_prediction_pytorch.py --model resnet50_mocov2\npython3 generate_prediction_pytorch.py --model resnet50_swav \nFinally, for supervised models, run\n python3 generate_prediction_pytorch_supervised.py --model tf_efficientnet_l2_ns_475\npython3 generate_prediction_pytorch_supervised.py --model gluon_resnet152_v1s\npython3 generate_prediction_pytorch_supervised.py --model ig_resnext101_32x48d \nYou\'ll need large amount of RAM since the script keeps features in memory. It was tested on machine with 128 GB RAM. \n Running clustering \n To run clustering, you need to run\n python3 cluster.py --model resnet50_infomin \nwhere the model name should fit the name in generating part. For overclustering, e.g., 1.25 times more clusters \nthan classes, run\n python3 cluster.py --model resnet152_simclr2 --over 1.25 \nFor using smaller dimensions of features, e.g., 512, run\n python3 cluster.py --model resnet152_simclr2 --n-components 512 \n Citing the paper \n If you found the paper or the code useful, please cite it. You can use following bibtex entry:\n @article{zheltonozhskii2020unsupervised,\n  title = {Self-Supervised Learning for Large-Scale Unsupervised Image Clustering},\n  author = {Zheltonozhskii, Evgenii and Baskin, Chaim and Bronstein, Alex M. and Mendelson, Avi},\n  journal = {NeurIPS Self-Supervised Learning Workshop},\n  year = {2020},\n  month = aug,\n  url = {https://arxiv.org/abs/2008.10312},\n  code = {https://github.com/Randl/kmeans_selfsuper},\n  arxiv = {2008.10312},\n}', 'Realizing topologically ordered states on a quantum processor \n This repo contains reimplementaton of the paper\n" Realizing topologically ordered states on a quantum processor " by Satzinger et al.\nin Qiskit. \n The systems of size up to 5x7 (31 qubit+ancilla) can be simulated on the classical computers,\nreproducing the results of the  \n GS preparation \n Matching boundary conditions \n For matching boundary condition, boundary plaquettes are all of the same type and the ground state is unique.\nWe implement linear algorithm for preparing ground state of the toric code proposed in the paper. \n Mixed boundary conditions \n For mixed boundary condition, boundary plaquettes are of different types, and there is ground state degeneracy,\nwhich allows us to encode logical qubits in the system state.\nNot implemented yet. \n Entropy and topological entropy \n We implement the measurement of the second Rényi entropy as described in paper, and use it\nto calculate topological entropy, acquiring non-trivial value for various subsystems. \n For 2x2 and 2x3 subsystem it is possible to perform determenistic calculation, while for \n3x3 system only randomized calculation is feasible. \n Braiding \n There are 4 particle types in toric code:  e ,  m ,  psi  and  1 , for total of 6 possible \nmutual statistics and 3 exchange statistic. Out of those 4 are non-trivial --  em ,  epsi ,\n mpsi  and  psipsi , resulting in phase of π. \n We implement the required operators (without optiimization), and demonstrate part of the braidings\nand exchanges. \n Logical qubit. \n Not implemented yet.']
thomasthiebaud,['[School project] Create a compiler and an interpreter based on lex and yacc. This code is no longer maintained. \n In order to install  lex  and  yacc  run \n sudo apt-get install flex bison\n \n Create the  compiler  and  interpreter  executables using \n make\n \n Run the compiler using \n ./compiler < test.c //Where test.c is the file to compile\n \n Run the interpreter using \n ./interpreter\n', 'This repo contains projects from  android nanodegree . See course for more details about the differents projects.\nEach project can be imported using android studio.', '    \n Easy setup with docker \n Install  docker  and  docker-compose . \n Clone the repo using \n git clone https://github.com/thomasthiebaud/QuietServer.git\n \n Move into the directory \n cd QuietServer\n \n Run the app using \n docker-compose up\n \n You can run the tests using \n docker-compose run server /app/node_modules/mocha/bin/mocha --recursive\n \n Run from scratch \n Coming soon', 'QuietAndroid \n This is the Quiet application for android. Check the full readme in the  CapstoneProject  repository in order to set up a local server. \n Test using the emulator \n You can test this app with the android emulator. In order to fake a call, once the emulator is running, you can do \n echo "gsm call <phone number here>" | nc -v  localhost 5554\n \n If you have an error like \n Android Console: Authentication required\nAndroid Console: type \'auth <auth_token>\' to authenticate\nAndroid Console: you can find your <auth_token> in\n\'/xxxx/xxxx/.emulator_console_auth_token\'\n \n get the auth_token and run instead \n echo "auth <auth_token> \\n gsm call <phone_number>" | nc -v  localhost 5554\n', 'CapstoneProject \n Screenshots \n \n \n Prerequisites \n This readme asumes that you have  git  and a full Android development environment available.\nYou will also need  docker  and  docker-compose . \n Get the sources \n In order to get the sources, from a terminal, run : \n git clone https://github.com/thomasthiebaud/CapstoneProject.git\ncd CapstoneProject\ngit submodule init\ngit submodule update\n \n Get a configuration file \n Get a configuration file using the  doc .\nEnable both  Google Sign-in  and  Ànalytics . Make sure you also have a  Web application  type client ID (check  here  for more details). \n Once you have a  google-services.json  file, move it to  QuietAndroid/app \n Set up the app (1/2) \n From the configuration file, get your oauth client id.\nExport it (as an env variable) : \n export GOOGLE_CLIENT_ID=\'<your id here>\'\n \n and also add it to  QuietAndroid/gradle.properties \n QuietServerId="<your id here>"\n \n Start the server \n Go to the  QuietServer  directory and run \n docker-compose build\ndocker-compose up\n \n Set up the app (2/2) \n Now the server is running so you can retrieve its IP address using \n docker inspect --format \'{{ .NetworkSettings.IPAddress }}\' quietserver_server_1\n \n and add this address to  QuietAndroid/gradle.properties \n QuietServerUrl="<Quiet server ip address here>"\n \n Test using the emulator \n You can test this app with the android emulator. In order to fake a call, once the emulator is running, you can do \n echo "gsm call <phone number here>" | nc -v  localhost 5554\n \n If you have an error like \n Android Console: Authentication required\nAndroid Console: type \'auth <auth_token>\' to authenticate\nAndroid Console: you can find your <auth_token> in\n\'/xxxx/xxxx/.emulator_console_auth_token\'\n \n get the  auth_token  and run instead \n echo "auth <auth_token> \\n gsm call <phone_number>" | nc -v  localhost 5554\n', "DEPRECATED \nI'm archiving this repo.  Create React App  does a better job, is well maintained  and support more options. \n \n Do you prefer  typescript  ? If so, use the  dedicated branch  instead \n Setup \n This project was tested with nodejs version  8.9.4 . You can easily install this version using  nvm  and then running \n nvm install\n \n and then install modules with \n npm install\n \n Scripts \n The following scripts are available \n |Name         |Description                                                                                   |\n|-------------|----------------------------------------------------------------------------------------------|\n|build        | Alias for  build:prod                                                                        |\n|build:dev    | Create development bundle                                                                    |\n|build:prod   | Create optimized production bundle                                                           |\n|clean        | Remove existing bundle and error files                                                       |\n|lint         | Run linter                                                                                   |\n|lint:fix     | Run linter and try to fix errors                                                             |\n|start        | Alias for  start:dev                                                                         |\n|start:dev    | Start developemnt server using development bundle                                            |\n|start:prod   | Start production server using production bundle                                              |", '[firefox] Trello Branch Name \n Add a button on each trello card to create a git branch name from the card name \n Inspired by  trello-branch-name', 'htmlstring-to-react \n Why ? \n This module provide an easy way to parse a string containing html elements to an array of React elements. It tries to focus to security (using  DOMPurify ) and keeping the bundle as small as possible \n It is heavily inspired by  html2react  and  html-react-parser \n How to install ? \n npm install htmlstring-to-react\n// or\nyarn add htmlstring-to-react\n \n How to use ? \n Simple example \n import { parse } from \'htmlstring-to-react\'\nparse(\'<em key="1"><b key="2">It\\\' is working</b></em>\')\n \n Add an override \n You can use css selectors to override an element \n import { parse } from \'htmlstring-to-react\'\nparse(\'<b key="1">It</b> is <b key="2">working</b>\', {\n  overrides: {\n    b: (props, textContent) => <b onClick={console.log(\'Click\')}>{textContent}</b>\n  },\n})\n \n All valid css selectors works \n import { parse } from \'htmlstring-to-react\'\nparse(\'<b key="1">It</b> is <b key="2" class="active">working</b>\', {\n  overrides: {\n    \'b.active\': (props, textContent) => <b onClick={console.log(\'Click\')}>{textContent}</b>\n  },\n})\n \n IMPORTANT  Overrides do not support nested elements in the current stage, so this code \n import { parse } from \'htmlstring-to-react\'\nparse(\'<b key="1"><b key="2">It is working</b></b>\', {\n  overrides: {\n    b: (props, textContent) => <b onClick={console.log(\'Click\')}>{textContent}</b>\n  },\n})\n \n will drop the inner  b  but keep the textContent \n Change dom parsing configuration \n By default, we are sanitizing the html input using  DOMPurify  module. You can override the configuration we are using \n import { parse } from \'htmlstring-to-react\'\nparse(\'<b key="1">It</b> is <b key="2" class="active">working</b>\', {\n  dom: {\n    ADD_TAG: [\'script\']\n  },\n})\n \n IMPORTANT  You cannot override  RETURN_DOM ,  RETURN_DOM_FRAGMENT  and  RETURN_DOM_IMPORT  because they are used internaly by the library. \n Other options \n \n useFragment  (default  false ): Return a Fragment instead of an array. \n useAsKey  (default  [\'key\'] ): Ordered list of attributes to use as a key. Use the first one that matches or  null \n \n How to contribute ? \n This repo enforce commit style so the release process is automatic. Commits must look like: \n <SUBJECT>: Message starting with an uppercase\n \n where SUBJECT is one of:  Fix ,  Update ,  New ,  Breaking ,  Docs ,  Build ,  Upgrade ,  Chore \n Found a problem ? \n Please open an issue or submit a PR, I will be more than happy to help', '\n react-use-size \n A collection of hooks to measure things in React \n Installation \n npm i react-use-size\n// or\nyarn add react-use-size \n Usage \n useWindowSize \n ```js\nimport { useWindowSize } from "react-use-size"; \n const YourComponent = () => {\n  const { height, width } = useWindowSize(); \n return (\n     \n Height: {height} \n Width: {width} \n \n  );\n};\n``` \n useComponentSize \n ```js\nimport { useComponentSize } from "react-use-size"; \n const YourComponent = () => {\n  const { ref, height, width } = useComponentSize(); \n return (\n     \n \n        Component\n         Height: {height} \n Width: {width} \n \n \n  );\n};\n``` \n useBreakpoint \n ```js\nimport { useBreakpoint } from "react-use-size"; \n const YourComponent = () => {\n  const isSmall = useBreakpoint(640); \n if (isSmall) {\n    return  \n  } else {\n    return  \n  }\n};\n``` \n useBreakpoints \n ```js\nimport { useBreakpoints } from "react-use-size"; \n const YourComponent = () => {\n  const [isSmall, isMedium] = useBreakpoint([640, 1024]); \n if (isSmall) {\n    return  \n  } else if(isMedium) {\n    return  \n  } else {\n    return  \n  }\n};\n``` \n How to contribute? \n This repo enforce commit style so the release process is automatic. Commits must look like: \n \n SUBJECT: message starting with a lowercase \n \n where SUBJECT is one of: \n \n build \n ci \n chore \n docs \n feat \n fix \n perf \n refactor \n revert \n style \n test \n \n A commit including  BREAKING CHANGE:  in the body will create a new major release. \n More details about the conventions are available  here  and  here . \n Found a problem? \n Please open an issue or submit a PR, we will be more than happy to help', 'spacy_fastlang \n Install \n Assuming you have a working python environment, you can simply install it using \n pip install spacy_fastlang \n Usage \n The library exports a pipeline component called  language_detector  that will set two spacy extensions \n \n doc._.language = ISO code of the detected language or  xx  as a fallback \n doc._.language_score = confidence \n \n ```\nimport spacy_fastlang  # noqa: F401 # pylint: disable=unused-import\nnlp = spacy.load("...")\nnlp.add_pipe("language_detector")\ndoc = nlp(en_text) \n doc. .language == "..."\ndoc. .language_score >= ...\n``` \n Options \n Check the tests  to see more examples and available options \n License \n Everythin is under  MIT  except the default model which is distributed under  Creative Commons Attribution-Share-Alike License 3.0  by facebook  here', 'poetry-githooks \n Install \n This repository is made to work with  poetry . Assuming you have a working  poetry  setup, run \n poetry add -D poetry-githooks \n Install \n Create a  tool.githooks  section in your  pyproject.toml  file and define your git hooks, for example \n [tool.githooks]\npre-commit = "black ." \n then run \n poetry run githooks setup \n That\'s it :tada: your hooks will be ran using  poetry  when expected \n IMPORTANT  You need to rerun  poetry run githooks setup  everytime you change  [tool.githooks]', 'fastify-schema-to-typescript \n How to install? \n sh\nnpm i -D fastify-schema-to-typescript \n How to use? \n Running  npx fastify-schema-to-typescript  will convert all  schema.json  files in  src  to  schema.ts .\nYou can then import  schema.ts  directly \n ```ts\nimport { RouteGeneric, schema } from "./schema"; \n const options = {\n  schema: {\n    params: schema.params,\n    body: schema.body,\n    response: schema.response,\n  },\n}; \n app.get ("/healthcheck", options, async () => {\n  return { ok: true };\n});\n``` \n More options are available \n ```sh\nnpx fastify-schema-to-typescript -h \n Usage: fastify-schema-to-typescript [options] \n Options:\n  -g, --glob      glob matching JSON schema to convert (default: "src/**/schema.{json,yaml,yml}")\n  -p, --prefix    prefix to use before interfaces\' name (default: "")\n  -e, --ext       file extension to use for generated files (default: ".ts")\n  -m, --module    module to import the RouteHandler type from (default: "fastify")\n  -h, --help            display help for command\n``` \n Project example \n Here is a project structure that I\'m usually using with that module.\nEach endpoint has a folder with a  schema.yaml  and  index.ts \n package.json\nsrc/\n  app.ts\n  api/\n    healthcheck/\n      index.ts\n      schema.yaml\n    anotherEndpoint/\n      index.ts\n      schema.yaml\n    ... \n Running  npx fastify-schema-to-typescript  will convert  schema.yaml  to  schema.ts  that you can then import in your apps like that \n app.ts \n ```ts\nimport fastify from "fastify";\nimport { healthcheck } from "./api"; \n export async function run() {\n  const app = fastify();\n  await app.register(healthcheck); \n await app.listen(3000);\n} \n run();\n``` \n api/index.ts \n ts\nexport * from "./healthcheck"; \n api/healthcheck/schema.yaml \n ```yml\nheaders:\n  type: object\n  properties: ... \n body:\n  type: object\n  properties: ... \n query:\n  type: object\n  properties: ... \n params:\n  type: object\n  properties: ...\n``` \n api/healthcheck/index.ts \n ```ts\nimport { FastifyPluginAsync } from "fastify";\nimport { RouteGeneric, schema } from "./schema"; \n export const getHealthcheck: FastifyPluginAsync = async (app) => {\n  app.get ("/healthcheck", { schema }, async () => {\n    return { ok: true };\n  });\n};\n``` \n I usually add the following in my package.json so I\'m sure the code is in sync with the schemas \n package.json \n ...\n"scripts": {\n  ...\n  "build": "npm run clean && npm run generate && npm run compile",\n  "compile": "tsc",\n  "clean": "rimraf dist",\n  "generate": "npx fastify-schema-to-typescript",\n  ...\n} \n and I also update  .gitignore  to not include the generated  schema.ts \n .gitignore \n ...\nschema.ts\n...', 'Slonik  is 10x solwer when used with ncc \n Here are the performances compared to pg with and without ncc \n ```\n// Without ncc \n pg - query: 7.775ms\npg - end: 0.986ms\nslonik - query: 12.771ms\nslonik - end: 0.629ms \n // With ncc\nncc-pg - query: 10.152ms\nncc-pg - end: 1.357ms\nncc-slonik - query: 546.402ms\nncc-slonik - end: 0.948ms\n``` \n If you have docker setup on your machine, you can reproduce the benchmark by running \n npm install\nnpm test', "Hi there 👋 \n I'm a software engineer, previously working at trustpilot, now on a break to experiment with an idea \n \n 🔭 I’m currently working on a cool mobile app \n 🌱 I’m currently learning dart/flutter \n ⚡ Fun fact: I love to play with candles they put on the tables in restaurants \n \n", "base_uri \n A tiny wrapper around Uri to easily define a base uri. Supports api versions. \n Getting Started \n Create a base Uri \n final BASE_URI = BaseUri(\n  scheme: 'https',\n  host: 'example.com',\n  version: 'v1',\n); \n then use it accross your app \n BASE_URI.useWith('/path'); // https://example.com/v1/path\nBASE_URI.useWith('/path', queryParameters: {'test': 'yes'}); // https://example.com/v1/path?test=yes", "formi \n A minimalist form builder, with null safety and no dependency. Inspired by https://pub.dev/packages/flutter_form_builder. \n Getting Started \n formi  does not include UI components but it will help you to build yours so you have  full control . \n You can check the  example  folder that contains some components to get you started. \n 1. Create a widget that extends FormiField \n For example you could create a Checkbox field like this \n class FormiCheckbox extends FormiField<bool> {\n  final String title;\n  final String? subtitle;\n  FormiCheckbox({\n    required this.title,\n    required String name,\n    this.subtitle,\n    List<FormiValidator<bool>>? validators,\n    bool? initialValue,\n    bool? enabled = true,\n  }) : super(\n          name: name,\n          validators: validators,\n          initialValue: initialValue,\n          enabled: enabled,\n          builder: (field) {\n            return CheckboxListTile(\n              dense: true,\n              title: Text(title),\n              value: field.value ?? false,\n              onChanged: field.didChange,\n              contentPadding: EdgeInsets.zero,\n            );\n          },\n        );\n} \n 2. Wrap your form in a Formi container \n Fields in  Formi  needs a unique  name  that is used to keep the value of the field \n Formi(\n  child: Column(\n    children: [\n        Text('Privacy preferences'),\n        FormiCheckbox(\n            name: 'strict',\n            title: 'Strictly necesary',\n            subtitle: 'Required for the app to run as expected',\n            initialValue: true,\n            enabled: false,\n        ),\n        FormiCheckbox(\n            name: 'analytics',\n            title: 'Analytics',\n            subtitle: 'Allow us to collect information about how you use the app',\n        ),\n        OutlinedButton(\n            child: Text('Submit'),\n            onPressed: () {\n                final formiState = Formi.of(context);\n                formiState.save();\n                if (formiState.validate()) {\n                    // Retrieve the form state with formiState.value\n                }\n            }\n        ),\n    ]\n  ),\n) \n 3. Add custom validators \n A validator is a function that takes the field value and the form state and returns:\n- a  String  if there is an error\n-  null  otherwise \n FormiValidator<T> required<T>(\n  BuildContext context, {\n  String? errorText,\n}) {\n  return (value, state) {\n    if (value == null ||\n        (value is String && value.isEmpty) ||\n        (value is Iterable && value.isEmpty) ||\n        (value is Map && value.isEmpty)) {\n      return errorText ?? 'This field cannot be empty.';\n    }\n    return null;\n  };\n} \n 4. Create more components to fit your needs \n For example the submit button could be written as \n ```\nclass FormiSubmit extends StatelessWidget {\n  final Widget Function(FormiState) builder;\n  FormiSubmit({required this.builder}); \n @override\n  Widget build(BuildContext context) {\n    final _formiState = Formi.of(context);\n    return builder(_formiState);\n  }\n}\n```", 'push_drawer \n A sliding drawer, pushing the main view away \n Simply use it with  \n PushDrawer(\n    drawerRatio: 0.9,\n    drawer: MyDrawerView(),\n    child: ContentView(),\n), \n You might want  ContentView  to be a nested Navigator', 'CREAPP \n Minimalist and flexible tool to set up modern web apps. Inspired by  Create React App \n Philosophy \n \n Use default configuration as much as possible \n One dependency \n Easy to extends \n \n How to create an App? \n npx @creapp/cli init \n How to extend the configuration? \n @creapp/config-react  just exports a normal webpack configuration. You can import it and tweak it at will \n ```js\nconst getConfig = require("@creapp/config-react");\nconst config = getConfig(/  You can force NODE_ENV here  /); \n // Do something with the defaultConfig here\nconfig.xxx = abc;\n// For example to have less details when running start/build\nconfig.stats = "minimal"; \n module.exports = config;\n``` \n The webpack configuration is detailed on  the docs \n How to change the build/start scripts? \n creapp build  and  creapp start  are tiny CLI helpers that use  webpack  and  webpack-dev-server  under the hood.\nYou can replace them by anything that understand a webpack configuration. \n IMPORTANT  You need to set  process.env.NODE_ENV  to  production  before running a production build']
samuela,['coffeegrailbone \n Underscore Templates in GSP pages \n One issue I ran into was that the syntax for Underscore templates is very \nsimilar to the GSP syntax and the conflict was giving me some very strange \nerrors as Grails was attempting to handle the Underscore templates. In order \nto get around this, I simply changed the Underscore template syntax settings \nas discussed here: http://lauripiispanen.github.com/blog/2012/01/31/building-a-backend-for-backbone-dot-js-todos-example-with-grails-and-mongodb/. \n Of course, by no means are we required to use Underscore templates. There \nare a bunch of competitors out there (Mustache and EJS come to mind). The \nadvantage of Underscore being that it is built into Underscore which is \nrequired by Backbone. \n REST and URL Mappings \n For every Grails controller it is necessary to adjust  UrlMappings.groovy  to \ndirect REST calls to the appropriate actions. For example, \n "/todos/$id?"(controller: "todos") {\n    action = [GET:"list", POST: "save", DELETE: "delete", PUT: "edit"]\n}\n \n Generalizing this to all controllers is discussed here: \nhttp://stackoverflow.com/questions/6120570/gerneric-url-mapping-for-restful-resources-in-grails. \n It may be even simpler to just adjust  Backbone.sync  to our specific needs so that \nCRUD actions are not mapped in the REST way but to the standard Grails way. This method \nmay allow us to take advantage of Grails\'s scaffolding. This is discussed in the \nofficial Backbone documentation and here: http://stackoverflow.com/questions/5096549/how-to-override-backbone-sync \n Frontend Testing \n Although I didn\'t do any testing, I realized that it would be easy to test \nthe frontend Backbone components on their own by simulating a Grails backend. \nThis could be done by "spying" on the  Backbone.sync  function with Jasmine or \nQUnit and some SinonJS. \n Selective JSON property exclusion \n When serializing objects to JSON, it is sometimes useful to control \nwhich fields are included, how they are serialized, etc. A simple \nsolution is provided here http://compiledammit.com/2012/08/16/custom-json-marshalling-in-grails-done-right/.\nHowever, I don\'t think this is actually the simplest solution. I \nbelieve it would be much simpler to create a JSON serialization class \nwhich can be given an object as well as a set of fields in that object \nto ignore. Moreover, I can\'t think of many situations where excluding \nfields in the JSON will be necessary.', 'Bruno: A Text Editor for Programming by Demonstration \n Created by Samuel Ainsworth, Frank Goodman, Jonathan Lessinger, and Michael \nScheer \n Motivation \n Bruno\'s purpose is to provide an elegant way to demonstrate the evolution of \ncode. Software development relies heavily on iterative and incremental \ndevelopment, and Bruno was designed to showcase this process on a step-by-step \nbasis. Through its extensive plugin, simple command, and intuitive edit \nhistory interfaces, teaching and demonstrating software development has never \nbeen easier. \n Screenshots \n Screenshots here \n Quick Start \n With the proper dependencies installed in  lib , you can run Bruno by \nexecuting "ant run". Tests can be run from the Eclipse JUnit integrated tester. \n Further Information \n More information about Bruno can be found in  \n \n The specifications document at https://docs.google.com/document/d/1dRRupteqQ92E1dJae7D2WSGvhwfjffBfoJhohRLRlpA/edit?usp=sharing. \n The requirements document at https://docs.google.com/document/d/1cLiuX4f9mBY8_Sf2yGCNacP9axUcXicoGOYjhnaazdM/edit?usp=sharing \n The design presentation at https://docs.google.com/presentation/d/1UOHPyfgO3Ze4BA-ZXMqVmTOzjRbnXPFM4137bkX8JJg/edit?usp=sharing \n The demo presentation at https://docs.google.com/presentation/d/1xhShdyaQD6WXG3znCJf1sK6BQ9awnI_AxOhusSSruR0/edit?usp=sharing \n \n License \n GPL', '242chalearn \n Code for the CRF. See  naive.py  for a naive, benchmark implementation and  utils.py  for simple data loading functions, etc.', 'spawncamping-octo-ninja', 'pyDPMP \n \n \n \n pyDPMP is a Python implementation of the Diverse Particle Max-Product algorithm\n(D-PMP) for maximum a posteriori estimation in continuous, pairwise Markov\nrandom fields as described in  Pacheco et al. 2014 \nand  Pacheco et al. 2015 . \n pyDPMP is compatible with Python 2.7, 3.3, 3.4, and 3.5. \n Installation \n To install pyDPMP just run \n pip install pyDPMP \n Examples \n Check out the  examples/  directory for some examples in the form of IPython notebooks.', "motionplanning \n This is a repository containing research code attempting to connect pyDPMP and the Open Motion Planning Library (OMPL). More generally, this repository is home-base for an effort to develop new motion planning algorithms built on D-PMP. \n The code currently reflects my (@samuela aka Sam Ainsworth) personal setup, and may require some tinkering on other environments. Feel free to reach out if you have any difficulties or questions! \n Manifest \n Jupyter Notebooks \n \n basic_circle.ipynb  An illustration of D-PMP solving an elementary motion planning problem. No OMPL involved. \n grid.ipynb  A grid environment. \n \n Python code \n \n DPMPPlanner.py  Contains an implementation of an OMPL-compatible solver\nwritten using OMPL's Python bindings. Path solving done via D-PMP. \n ompl_app_dpmp.py  Is a Frankensteined version of OMPL's  ompl_app.py  GUI application. The original GUI code is very messy and OMPL is not flexible enough to allow planners to be written in Python without altering the GUI code. It may be worth diffing this file against the canonical version in order to understand what changes have been made. \n ompl_app.py  A mostly consistent version of the original OMPL version, with a number of fixes and code cleanliness adjustments. \n path_check_bug.py  Illustrates a SEGFAULT error in the OMPL Python bindings. Bug should be patched now. Not important. \n \n Dockerfiles and scripts \n I (Sam Ainsworth) run OS X. This makes compiling, running, and developing OMPL is a real pain in the ass if not downright impossible. In order to make things more approachable, I've developed a number of Docker images in order to run OMPL. These are accompanied by bash scripts that facilitate running the Docker containers and setting up the GUI to run on OS X. \n Dockerfiles \n \n docker/ompl-buildenv/  establishes all of the necessary packages and system configuration in order to compile OMPL. \n docker/ompl-official  is based on the ompl-buildenv image and builds the official OMPL 1.1.1 from scratch. \n docker/ompl-devenv  is based on the ompl-official image and adds some nice packages for development, like IPython, matplotlib, and numpy. \n \n Scripts \n Running Docker containers with X11 on OS X is a real pain. But unfortunately it's necessary in order to run the GUI. The two  run_ompl-devenv.sh  and  run_ompl-official.sh  scripts handle all of the setup and teardown necessary to run the Docker images with X11 forwarding. They also handle attaching folders on the host machine to the file system of the container and the installation of the pyDPMP pip package in the container. They are customized to run on my particular machine, but should be fairly easy to adapt to other configurations.", 'language-pyret package \n This repo is no longer actively maintained!  Development has continued over at https://github.com/brownplt/atom-language-pyret. \n An Atom package providing syntax highlighting support for the Pyret programming\nlanguage.', 'Homemade Bidirectional Typing \n (λ f. (λ g. (λ x. f (g x))))\n=> ((a -> b) -> ((c -> a) -> (c -> b))) \n A bidirectional type inference system loosely based on  Complete and Easy Bidirectional Typechecking\nfor Higher-Rank Polymorphism . \n I read a bit about bidirectional type inference about a year ago but was fuzzy on many of the details, so I figured that it would be a fun exercise to try to implement a full type inference and checking algorithm without consulting any references. This is the result. \n Exposition \n At the core is a small lambda calculus, \n haskell\ndata Expr =\n    EUnit Loc\n  | EIdent Loc Id\n  | EAnno Loc Expr Type\n  | ELam Loc Id Expr\n  | EApp Loc Expr Expr \n featuring a unit type which is simply a base type but without anything interesting that you can do with it.  Loc  here refers to a type which specifies source location of the expression. After all, two variables with the same name in different parts of the program are unique and need not have the same type. \n A corresponding interpreter is included as well, \n ```haskell\ndata Val =\n    VUnit\n  | VLam Env Id Expr \n type Env = Map Id Val \n interp :: Env -> Expr -> Maybe Val\ninterp _ (EUnit  ) = Just VUnit\ninterp env (EIdent _ x) = Map.lookup x env\ninterp env (EAnno _ body  ) = interp env body\ninterp env (ELam _ x body) = Just (VLam env x body)\ninterp env (EApp _ f a) = case ((interp env f), (interp env a)) of\n  (Just (VLam fenv y body), Just xval) -> interp (Map.insert y xval fenv) body\n  otherwise -> Nothing \n run :: Expr -> Maybe Val\nrun = interp Map.empty\n``` \n Moving along... \n Type Inference \n This is the fun part. Bidirectional typing is centered on two operations, hence the name. \n \n Type  checking , checking that an expression satisfies a given type under some context. For example, checking that  (λ x. x)  has type  a -> a  should succeed but checking it against the unit type should fail. \n Type  synthesis , given an expression and some program context synthesize a new type for the expression. We may not have enough information to determine the exact type that the expression must have, but we can make a guess and put in some placeholders where necessary. For example, we know  (λ x. <body>)  must have a function type  a -> b , but we can\'t quite be sure about what  a  and  b  are without looking into the body of the lambda. \n \n Since I was starting with very few pre-conceived notions about what a bidirectional type inference algorithm should look like I ended up diverging from the "Complete and Easy" algorithm in a few ways. In particular, instead of maintaining a so-called "ordered algorithmic context" I simply maintained type environment that mapped source expressions to  TypeId s which are then mapped to types: \n ```haskell\nnewtype TypeId = TypeId Integer \n data Type =\n    TUnit\n  | TIdent TypeId\n  | TLam Type Type \n data TypeStatus =\n    Exists\n  | Forall\n  | Typed Type \n data TypeEnv = TypeEnv Integer (Map Expr TypeId) (Map TypeId TypeStatus)\n``` \n Well, it\'s a little bit more complicated than that: a  TypeId  can also be marked as existential ( Exists ), meaning that we have not yet determined what it should be, or universal ( Forall ), meaning that it is a type variable in some polymorphic type. The added indirection here is necessary to model the fact that multiple source expressions may map to the same type. It also allows types to reference other types via  TIdent s. We also keep an  Integer  around so we know what the next unused  TypeId  should be. \n Another distinction is that I managed to reduce the checking half of bidirectional typing to simply a synthesis, followed by a subtype operation: \n haskell\ncheck :: TypeEnv -> Expr -> Type -> Maybe TypeEnv\ncheck tenv expr typ = do\n  (typ\', tenv\') <- synth tenv expr\n  tenv\'\' <- subtype tenv\' typ\' typ\n  return tenv\'\' \n I\'m not sure if this is a good or bad thing really. I view it as a simplification, personally. (This is equivalent to the Sub rule in "Complete and Easy.") \n The rest of the implementation actually reflects the "Complete and Easy" algorithm fairly closely. I also ended up with  subtype  and  synth , but happened to inline the instantiation rules. \n ```haskell\nsynth :: TypeEnv -> Expr -> Maybe (Type, TypeEnv) \n subtype :: TypeEnv -> Type -> Type -> Maybe TypeEnv\n``` \n These implementations are fairly routine. Function applications are a little tricky, but with those figured out the rest falls into place. Function subtyping also requires a little bit of care, of course.  A1 <: A2  and  B1 <: B2  does not imply  A1 -> B1 <: A2 -> B2 ! \n All in all, I\'m pretty happy with the way it turned out. In my (very partial) opinion, the implementation with an unordered mapping from expressions to types is more intuitive than thinking about an ordered context of solved and unsolved types. The implementation also doesn\'t require type markers or list splicing kung fu which is nice. \n I went back to read the paper and noted the connections between the two in code comments so it should be fairly approachable to readers who prefer reading at LaTeX. \n TODO \n \n There are a number of things that could be made more monadic/clean. PRs welcome! \n Record subtyping is next on my list. I spent a bit of time on this, but I\'ve come to the conclusion that it requires using a constraint solver approach. Should be do-able though. \n', 'Buck shared library bug on macOS \n Try running \n buck run //device:main \n to reproduce. You should see a runtime error like \n dyld: Library not loaded: @rpath/libopencv_videostab.3.2.dylib\n  Referenced from: /.../buck-out/gen/device/main\n  Reason: image not found', 'oi-vae \n \n Code for "oi-VAE: Output Interpretable VAEs for Nonlinear Group Factor Analysis" by  Samuel Ainsworth ,  Nick Foti ,  Adrian K.C. Lee , and  Emily Fox  presented at ICML 2018. \n http://proceedings.mlr.press/v80/ainsworth18a/ainsworth18a.pdf \n Usage \n Each of the files in the root of the project is a script to run one of the experiments in the paper, eg.  bars_data_oivae.py  runs the bars experiment and  mo_mo_mocap_oivae.py  runs the CMU Mocap data experiment. They all rely on the library code in the  lib/  folder which contains code generally useful across multiple scripts. In particular the exciting parts live here: https://github.com/samuela/oi-vae/blob/master/lib/oivae.py. In terms of getting started, the best entrypoint is  bars_data_oivae.py  since it’s the simplest example and doesn’t require downloading any datasets. \n Note that this code was written back when PyTorch v0.3 was the latest version, and things have changed quite a bit since then, so you’ll probably need to use PyTorch v0.3 in order to get things to work.', 'Babar', 'kindling \n A Swiss Army knife for building dope stuff with pytorch.', 'happyentropy \n A small library for happy randomness. Inspired by https://blog.asana.com/2011/09/6-sad-squid-snuggle-softly/.', 'kumo-example-repo', 'grpc-polyglot', "Karger's min-cut algorithm in OCaml \n This is an implementation of Karger's min-cut algorithm implemented in OCaml. Check out the *.in files in this repo for some example graphs. You can run them with\n $ ocaml karger.ml b0.in\nstarting...\nBest cut so far: 14\nBest cut so far: 8\n... \nThe program will keep searching for min-cuts and print updates each time it finds a better one. \n See https://en.wikipedia.org/wiki/Karger%27s_algorithm for more information.", 'research \n \n ALL THE CODEZ \n New machine setup \n On AWS: \n \n Create a new VM with Ubuntu 18.04. \n Assign it a new Elastic IP. \n \n Locally: \n \n Update  ~/.ssh/config  with a new entry: \n \n Host <name>\n HostName <ip address/url>\n User ubuntu\n IdentityFile ~/.ssh/aws-macbookpro.pem    # or wherever. \n On the machine: \n First, \n bash\nsudo apt update\nsudo apt upgrade\nsudo reboot \n \n Set the hostname. \n Install nuvemfs. \n Install mujoco. \n Install linuxbrew and pipenv. \n \n Set hostname \n On AWS Ubuntu 18.04, \n bash\nuser$ sudo su\nroot$ hostnamectl set-hostname <whatever> \n Install nuvemfs \n bash\nsudo apt install -y cifs-utils\nwget https://nuvemfscliassets.blob.core.windows.net/nuvemfs-cli-assets/stable/nuvemfs-cli-x86_64-unknown-linux-musl\nchmod +x nuvemfs-cli-x86_64-unknown-linux-musl\necho "alias nuvemfs=\\"~/nuvemfs-cli-x86_64-unknown-linux-musl\\"" >> ~/.profile\nsource ~/.profile \n Mujoco/Ubuntu setup \n \n Download and install Mujoco. \n \n bash\nsudo apt install -y unzip clang\nwget https://www.roboti.us/download/mujoco200_linux.zip\nunzip mujoco200_linux.zip\nmkdir ~/.mujoco\nmv mujoco200_linux ~/.mujoco/mujoco200\nrm mujoco200_linux.zip\necho "export LD_LIBRARY_PATH=\\$LD_LIBRARY_PATH:/home/ubuntu/.mujoco/mujoco200/bin" >> ~/.profile\nsource ~/.profile \n \n Install dependencies \n \n ```bash \n libosmesa6-dev: Fixes  fatal error: GL/osmesa.h: No such file or directory \n libglew-dev: Fixes  /usr/bin/ld: cannot find -lGL \n ffmpeg: Necessary for mujoco videos. \n sudo apt install -y libosmesa6-dev libglew-dev ffmpeg \n These seem to be only necessary on circleci/python: \n patchelf: Fixes  No such file or directory: \'patchelf\' . \n libglfw3-dev: Fixes  ImportError: Failed to load GLFW3 shared library. . \n sudo apt install -y patchelf libglfw3-dev \n These are required for slycot which is required by control... \n sudo apt install gfortran libblas-dev liblapack-dev\n``` \n Either clang will need to be set it as the default  cc  alternative ( sudo update-alternatives --config cc ) or you\'ll need to use gcc version 8. If you follow these instructions exactly (without ever installing  build-essentials ) then it should work no problemo. \n Logging in/out to fix  $PATH  may also be necessary. \n See \n \n https://github.com/openai/mujoco-py/issues/455 \n https://github.com/openai/mujoco-py/issues/394 \n \n https://github.com/ethz-asl/reinmav-gym/issues/35 \n \n \n Put the license key at  ~/.mujoco/mjkey.txt . \n \n \n bash\ncp ~/nu/skainswo/mjkey.txt ~/.mujoco/mjkey.txt \n Install linuxbrew and pipenv \n See https://docs.brew.sh/Homebrew-on-Linux. \n ```bash \n See https://stackoverflow.com/questions/24426424/unattended-no-prompt-homebrew-installation-using-expect. \n echo | sh -c "$(curl -fsSL https://raw.githubusercontent.com/Linuxbrew/install/master/install.sh)"\necho \'eval $(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\' >> ~/.profile\nsource ~/.profile\nbrew install pipenv\n``` \n CUDA/cuDNN setup \n The  nvidia-driver-430  and  nvidia-cuda-toolkit  on Ubuntu 18.04 install CUDA 9.1 which is not supported by JAX at the moment. \n \n Remove any current installation. \n \n bash\nsudo apt-get purge *cuda*\nsudo apt-get purge *nvidia*\nsudo apt-get purge *cudnn* \n and then follow the runfile uninstall steps (https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#runfile-uninstallation). \n \n Make sure that gcc is current cc alternative: \n \n bash\nsudo update-alternatives --config cc\ncc --version \n (This was necessary for CUDA 10.1. May not be necessary for 10.0.) \n \n \n Follow the installation instructions  here  for the "runfile (local)" version. Install version 10.0 since TF and pytorch do not yet support 10.1. \n \n \n Add \n \n \n bash\nexport PATH=/usr/local/cuda-10.0/bin${PATH:+:${PATH}}\nexport LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} \n to  ~/.profile . \n \n \n Download the "cuDNN Library for Linux" (https://developer.nvidia.com/rdp/cudnn-download), not the deb version. You\'ll need to be logged in order for the downloads to work. Using wget/curl isn\'t sufficient. Easiest to download them locally and then scp them to the remote machine. \n \n \n Install cuDNN (https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installlinux-tar) but note that the CUDA installation directory is  /usr/local/cuda-10.0  not  /usr/local/cuda . \n \n \n Reboot. \n \n \n Follow the pip instructions here (https://github.com/google/jax#pip-installation) in a  pipenv shell  to install the new GPU versions of  jax / jaxlib . \n \n \n See \n \n https://developer.nvidia.com/cuda-zone \n https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/install-nvidia-driver.html \n https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/optimize_gpu.html \n https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#faq2 \n https://stackoverflow.com/questions/50622525/which-tensorflow-and-cuda-version-combinations-are-compatible \n https://discuss.pytorch.org/t/when-pytorch-supports-cuda-10-1/38852 \n \n Note that the deb installation does not seem to support multiple CUDA installations living in harmony. This may become problematic as some packages like pytorch do not yet support CUDA 10.1. \n With CUDA 10.0, JAX may require the  xla_gpu_cuda_data_dir  XLA flag to be set as well: \n XLA_FLAGS=--xla_gpu_cuda_data_dir=/usr/local/cuda-10.0/ \n Expand EBS volume \n No downtime is necessary. \n \n Change the volume in the console. \n Then follow https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/recognize-expanded-volume-linux.html. Use  df -T  to get the filesystem type. \n \n Mujoco lockfile issues \n Sometimes the mujoco lockfile gets screwed up and in that case it\'s necessary to delete it. If jobs are just hanging forever without starting try deleting the lockfile: \n bash\nrm $(pipenv --venv)/lib/python3.7/site-packages/mujoco_py/generated/mujocopy-buildlock.lock \n See https://github.com/openai/mujoco-py/issues/424.', "remod \n chmod for human beings! \n If UNIX octal permissions get you down, this is the tool for you. Humans were never meant to do bit arithmetic in their heads. \n \n \n \n Installation \n npm i -g remod-cli \n Usage \n You can view and interactively edit the permissions of a file with \n $ remod foo.txt \n If you think that you want to change a file's permissions to 640 but you'd like to preview the changes first, just run \n $ remod 640 foo.txt", 'E-stops \n \n Here lives the source code for "Mo\' States Mo\' Problems: Emergency Stop Mechanisms from Observation" by  Samuel Ainsworth ,  Matt Barnes , and  Siddhartha Srinivasa  presented at NeurIPS 2019. \n Usage \n This package uses  pipenv  to manage dependencies, so you\'ll need a working pipenv installation. Then get started with: \n $ pipenv install --dev\n$ pipenv run python -m research.foo.bar', 'rustybox \n \n RustyBox is a free-range, non-GMO fork of  BusyBox  written entirely in  Rust . It includes all your favorite commands like  ls ,  mount , and  top , but without a single line of C code! Like BusyBox, rustybox weighs in at just under 1 megabyte and includes all the basic utilities you need to set up a small Linux OS. \n \n Status \n rustybox is a work-in-progress! It started out life as a direct  c2rust  transpile of the busybox project, and has been steadily improving since then. This has the benefit of ensuring that rustybox is "bug-for-bug" compatible with busybox, but it does mean that we have inherited the raw pointers and  unsafe s that come from C land. If making essential software memory-safe is your cup of tea then join the party with a PR! \n Contributing to rustybox \n Contributing to rustybox is a great way to get started with rust, dig into the bowels of linux, or to help to free the world from the diabolical tyranny of C. \n There\'s lots to be done, so we\'re happy to have you! Here are just a few ideas: \n \n Replace some  extern "C"  includes with more idiomatic  use s. Pretty straightforward find/replace-all usually does the trick. \n Pick a utility, like  cat  or  touch , and work on translating it into safer, more idiomatic rust. There are plenty of  unsafe s lying around that you can tackle! \n Try building Alpine linux with rustybox in place of busybox. This could be an awesome drop-in replacement for the popular  alpine  Docker image . \n \n Check out  the contributing doc  for more info! \n And of course please test out rustybox and report  any and all  issues, concerns, and comments! \n Building rustybox \n Rustybox requires a Linux system to build. Developing in Docker works just as well on macOS and Windows. You\'ll need GCC and its development headers (esp.  quadmath.h ) in order to build the f128 dependency. Running  sudo apt install build-essential  should do the trick on Ubuntu/Debian. \n Please open an issue if you have trouble building! \n Customizing your rustybox distribution \n By default, rustybox does not include any utility. You can include all of them: \n cargo build --all-features \n Chances are you don\'t actually need or want  everything  in rustybox, especially for a release. If you\'d like to build rustybox with only a specific set of utilities: \n cargo build --release --features "cat ls which" \n Check out the  [features]  section of  Cargo.toml  for the full list of utilities on tap. \n After building, you can remove unnecessary debug sections with  strip . This is recommended if you are particularly size-conscious. \n Acknowledgements \n There\'s simply no way this project would be possible without the hard work from the wonderful  busybox  and  c2rust  teams. Both projects are dope, and you should check them out. Much of the code you find in this repo is transpiled from the work of the busybox  AUTHORS .', 'cuddlefish', "juniper_hyper_broken \n Example code to demonstrate that juniper_hyper isn't working \n ``\njuniper_hyper_broken on \ue0a0 main is 📦 v0.0.0 via 🦀 v1.45.2 on ☁️  us-west-2\n❯ cargo build\n   Compiling juniper v0.14.2\n   Compiling h2 v0.2.6\nerror[E0463]: can't find crate for serde_json`\n  --> /Users/skainswo/.cargo/registry/src/github.com-1ecc6299db9ec823/juniper-0.14.2/src/lib.rs:98:1\n   |\n98 | extern crate serde_json;\n   | ^^^^^^^^^^^^^^^^^^^^^^^^ can't find crate \n error: aborting due to previous error \n For more information about this error, try  rustc --explain E0463 .\nerror: could not compile  juniper . \n To learn more, run the command again with --verbose.\nwarning: build failed, waiting for other jobs to finish...\nerror: build failed\n```", "nixpkgs-upkeep \n nixpkgs-upkeep is an auto-update and CI bot for nixpkgs. \n Auto-updating:  Updating packages in Nixpkgs is important but tedious work. nixpkgs-upkeep is a simple bot that checks for updates every 12 hours and creates PRs so you don't have to. \n CI:  nixpkgs-upkeep builds a set of packages on nixpkgs master every 12 hours, and automatically creates GitHub issues when there are failures. \n Currently the following packages are supported: \n | Package                  | Update | CI                                                                                                         |\n| ------------------------ | ------ | ---------------------------------------------------------------------------------------------------------- |\n| dm-haiku                 |        | y                                                                                                          |\n| elegy                    |        | y                                                                                                          |\n| flax                     |        | y                                                                                                          |\n| ipython                  |        | y                                                                                                          |\n| jax                      | y      | y                                                                                                          |\n| jaxlib                   |        | y                                                                                                          |\n| jaxlib-bin               |        | y                                                                                                          |\n| jaxlib-bin (w. CUDA)     |        | y                                                                                                          |\n| jaxlibWithCuda           |        | y                                                                                                          |\n| jmp                      |        | y                                                                                                          |\n| julia_17-bin             | y      | y                                                                                                          |\n| matplotlib               | y      | y                                                                                                          |\n| optax                    |        | y                                                                                                          |\n| plexamp                  | y      |                                                                                                            |\n| plotly                   |        | y                                                                                                          |\n| spotify                  | y      |                                                                                                            |\n| tensorflow               |        | y                                                                                                          |\n| tensorflow-bin           |        | y                                                                                                          |\n| tensorflow-bin (w. CUDA) |        | y                                                                                                          |\n| tensorflow-datasets      |        | y                                                                                                          |\n| tensorflowWithCuda       |        |  N  |\n| tqdm                     |        | y                                                                                                          |\n| treeo                    |        | y                                                                                                          |\n| treex                    |        | y                                                                                                          |\n| vscode                   | y      |                                                                                                            |\n| vscodium                 | y      |                                                                                                            |\n| wandb                    | y      | y                                                                                                          | \n Submit a PR to get your favorite Nix packages added to this list! \n All the activity goes down in the form of GitHub Actions, so go check those out to see the logs, etc. \n FAQ \n refusing to allow a Personal Access Token to create or update workflow `.github/workflows/editorconfig.yml` without `workflow` scope \n Sometimes we get errors like this when pushing to samuela/nixpkgs, because samuela/nixpkgs is behind the upstream fork. When the fork is missing changes to sensitive files like  .github/...  stuff the push is rejected. \n Fix is to manually pull from upstream and push to the fork to get things up-to-date.", 'Continuous-Time Policy Gradients (CTPG) \n \n Here lives the source code for " Faster Policy Learning with Continuous-Time Gradients " by  Samuel Ainsworth ,  Kendall Lowrey ,  John Thickstun ,  Zaid Harchaoui  and  Siddhartha Srinivasa  presented at Learning for Dynamics and Control (L4DC) 2021. \n Have you ever wondered what would happen if you took deep reinforcement learning and stripped away as much stochasticity as possible from the policy gradient estimators? Well, wonder no more! \n Usage \n Much of the code was written against Julia version 1.5.1. The MuJoCo related experiments will also require access to a MuJoCo installation. DiffTaichi experiments require access to the DiffTaichi 0.7.12 differentiable simulator. This should be installed automatically by running  ] build  in this Julia project.', 'nixos-up \n nixos-up is a dead-simple install wizard for NixOS. It\'s the fastest way to get from ISO to working installation. \n From the NixOS installation USB/CD: \n sudo nix-shell https://github.com/samuela/nixos-up/archive/main.tar.gz \n You can check out a video demonstrating the process here: https://youtu.be/f7DzbiRD99Q. \n Development \n In this directory run  servefile --tar --compression gzip --port 12345 . . Then, while that\'s running  nix-shell -p ngrok --run "ngrok http 12345" . \n Now in your VM/device, run \n nix-collect-garbage && sudo nix-shell http://blah-blah-blah.ngrok.io/nixos-up.tar.gz \n You may need  sudo umount --lazy /mnt  periodically as well.', "nixos-fix-vscode-remote \n VSCode Remote SSH into NixOS machines. By default VSCode Remote SSH ships with a node.js binary that doesn't run on NixOS systems. \n Usage \n Try and fail to connect to the NixOS machine that you'd like to connect to with VSCode Remote SSH. Once that fails, SSH into the machine manually and run this tool: \n nix-shell https://github.com/samuela/nixos-fix-vscode-remote/archive/main.tar.gz \n Next time you try to connect from VSCode it should work without an issue!", 'Getting Started with Create React App \n This project was bootstrapped with  Create React App . \n Available Scripts \n In the project directory, you can run: \n yarn start \n Runs the app in the development mode.\\\nOpen  http://localhost:3000  to view it in the browser. \n The page will reload if you make edits.\\\nYou will also see any lint errors in the console. \n yarn test \n Launches the test runner in the interactive watch mode.\\\nSee the section about  running tests  for more information. \n yarn build \n Builds the app for production to the  build  folder.\\\nIt correctly bundles React in production mode and optimizes the build for the best performance. \n The build is minified and the filenames include the hashes.\\\nYour app is ready to be deployed! \n See the section about  deployment  for more information. \n yarn eject \n Note: this is a one-way operation. Once you  eject , you can’t go back! \n If you aren’t satisfied with the build tool and configuration choices, you can  eject  at any time. This command will remove the single build dependency from your project. \n Instead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except  eject  will still work, but they will point to the copied scripts so you can tweak them. At this point you’re on your own. \n You don’t have to ever use  eject . The curated feature set is suitable for small and middle deployments, and you shouldn’t feel obligated to use this feature. However we understand that this tool wouldn’t be useful if you couldn’t customize it when you are ready for it. \n Learn More \n You can learn more in the  Create React App documentation . \n To learn React, check out the  React documentation .', 'nixos-idle-shutdown \n A NixOS service that automatically shuts down the system after a period of inactivity. This is useful in cloud environments where the system should be shut down when no one is currently logged in. Current behavior is to automatically shut down after an hour of inactivity. \n Note that nixos-idle-shutdown currently does not respect tmux sessions due to https://github.com/NixOS/nixpkgs/issues/155446. \n Usage \n In your  /etc/nixos/configuration.nix : \n nix\nimports = [\n  (fetchTarball "https://github.com/samuela/nixos-idle-shutdown/tarball/main")\n];', 'cuda-nix-testsuite \n This is a suite of tests for CUDA-enabled packages in Nixpkgs. The Nix build environment does not support GPU-access by default. So we have to do things off-the-books! \n TODO: \n \n [ ] A simple runner script that scrapes all scripts that look like  *.test.*  and runs them. \n [ ] Add support for tests in subdirectories with  shell.nix  instead of nix-shell shebang-lines. \n [ ] Add test for basic PyTorch functionality \n [ ] Add test for  jax Conv issue \n [ ] Add test for basic TF functionality \n [ ] Add test for https://github.com/NixOS/nixpkgs/issues/163739 \n [ ] Add test for https://github.com/NixOS/nixpkgs/pull/153542#issuecomment-1066180952 \n \n Development \n How do I run the test suite? \n TODO: there will be a runner script (coming soon!) \n How do I add a test? \n Create a new script in the  tests/  subdirectory with  .test.  in the filename.\nSee  jax.test.py  for an example.', 'Git Re-Basin: Merging Models modulo Permutation Symmetries \n \n Code for the paper  Git Re-Basin: Merging Models modulo Permutation Symmetries . \n Abstract: \n \n The success of deep learning is thanks to our ability to solve certain massive non-convex optimization problems with relative ease. Despite non-convex optimization being NP-hard, simple algorithms -- often variants of stochastic gradient descent -- exhibit surprising effectiveness in fitting large neural networks in practice. We argue that neural network loss landscapes contain (nearly) a single basin, after accounting for all possible permutation symmetries of hidden units. We introduce three algorithms to permute the units of one model to bring them into alignment with units of a reference model. This transformation produces a functionally equivalent set of weights that lie in an approximately convex basin near the reference model. Experimentally, we demonstrate the single basin phenomenon across a variety of model architectures and datasets, including the first (to our knowledge) demonstration of zero-barrier linear mode connectivity between independently trained ResNet models on CIFAR-10 and CIFAR-100. Additionally, we identify intriguing phenomena relating model width and training time to mode connectivity across a variety of models and datasets. Finally, we discuss shortcomings of a single basin theory, including a counterexample to the linear mode connectivity hypothesis. \n']
jalainis,[]
johnkwong,['medical_datamining_midtern', '', 'crowdfunding', 'p_embedding-a-machine-learning-model-into-a-web-app', 'p_database_m', 'codingbat', 'java-practice', 'telegram-bot-practice', '\n A dark and light theme for Jekyll, inspired by Dash UI for Atom. 🌒☀ \n \n \n \n \n \n This theme for  Jekyll  has been inspired by  dash-ui , a dark theme for  Atom . \n \n Installation \n Add this line to your Jekyll site\'s  Gemfile : \n For  Jekyll 3 :\n ruby\ngem \'jekyll-dash\', \'~> 1.0.0\' \n \n Keep in mind: Github pages generation only supports Jekyll 3.8 right now. \n \n For  Jekyll 4 :\n ruby\ngem \'jekyll-dash\', \'~> 2.0.0\' \n And add this line to your Jekyll site\'s  _config.yml : \n yaml\ntheme: jekyll-dash \n And then execute: \n $ bundle\n \n Or install it yourself as: \n $ gem install jekyll-dash -v version\n \n Configuration \n Add the following configuration to your site. Customise it to your needs! \n ```yaml \n required by disqus to display comments \n url: https://your-site-url \n jekyll-paginate \n paginate: 5\npaginate_path: "/blog/page:num/" \n jekyll-tagging (optional) \n tag_permalink_style: pretty\ntag_page_layout: tag_page\ntag_page_dir: tag \n dash:\n  date_format: "%b %-d, %Y" \n disqus:\n    shortname:  \n # generate social links in footer\n  # supported colors: green, red, orange, blue, cyan, pink, teal, yellow, indigo, purple\n  social_links:\n    - url: https://twitter.com/bitbrain_\n      icon: twitter-square\n      color: cyan\n    - url: https://bitbrain.itch.io\n      icon: itch-io\n      color: red\n    - url: https://github.com/bitbrain\n      icon: github-square\n      color: purple \n show_author: true \n Replaces the default avatar provider (gravatar) \n avatar_source: github \n github_username: bitbrain \n ``` \n Using this theme directly on Github Pages \n Please keep in mind that Github Pages does only support  a limited list of Jekyll plugins . You will be able to use this theme on Github Pages but some functionality might not be available, for example displaying tags. In order to use this theme to a full extend, you have to generate the  _site  externally, for example on  TravisCI .  \n For example, you want to host your own blog on  https://<username>.github.io . As a result, you require the following repositories: \n \n blog  - contains the actual Jekyll sources ( see example ) \n <username>.github.io  - contains generated webpage, pushed automatically via TravisCI ( see example ) \n \n You are not required to do this, but keep in mind that some functionality might not be available when using the Jekyll generator on Github directly! \n Additional Features \n Tagging  add the  jekyll/tagging  plugin to your  _config.yml  file to enable tagging. Do not forget to also add the following to your  Gemfile :\n Gemfile\ngem "jekyll-tagging" \n Gravatar  if you want to display your gravatar picture, add the  liquid-md5  to your  _config.yml  file. Do not forget to also add the following to your  Gemfile :\n Gemfile\ngem "liquid-md5" \n FAQ \n \n I have configured posts but no posts are showing? \n \n Solution:  You most probably forgot to configure  jekyll-paginate  in your _config.yml! Make sure you have the correct configuration as described above! \n \n I have added the correct configuration for  jekyll-paginate  but it is now complaining about a missing  index.html  file. What do I do? \n \n Solution  pagination only works with HTML files! Markdown is not supported there. Simply rename your  index.md  into  index.html  - that should do the trick! \n \n I have configured Disqus via _config.yml but Disqus fails to load on the page?  \n \n Solution:  Make sure you configure the correct  url  within your  _config.yml . Also make sure that your domain is trusted by Disqus. This can be configured within Disqus by adding a trusted domain. \n \n I am using this theme but I don\'t see any tags? \n \n Solution : as described above you have to add the tagging plugin. Additionally, tags do not work natively by Github Pages. You have to build your site on an external CI and push the  _site  artifacts to a hosting repository. \n \n I am getting an error that Bundler could not find compatible versions for gem \n \n Solution \n Make sure you are using a version of this theme that is compatible with Jekyll. Version 1.x is only compatible with Jekyll 3.x while Version 2.x is only compatible with Jekyll 4.x. \n Contributing \n Bug reports and pull requests are welcome on GitHub at https://github.com/bitbrain/jekyll-dash. This project is intended to be a safe, welcoming space for collaboration, and contributors are expected to adhere to the  Contributor Covenant  code of conduct. \n Development \n To set up your environment to develop this theme, run  bundle install . \n Your theme is setup just like a normal Jekyll site! To test your theme, run  bundle exec jekyll serve  and open your browser at  http://localhost:4000 . This starts a Jekyll server using your theme. Add pages, documents, data, etc. like normal to test your theme\'s contents. As you make modifications to your theme and to your content, your site will regenerate and you should see the changes in the browser after a refresh, just like normal. \n When your theme is released, only the files in  _layouts ,  _includes ,  _sass  and  assets  tracked with Git will be bundled.\nTo add a custom directory to your theme-gem, please edit the regexp in  jekyll-dash.gemspec  accordingly. \n License \n The theme is available as open source under the terms of the  Apache License 2.0 .', '\n A dark and light theme for Jekyll, inspired by Dash UI for Atom. 🌒☀ \n \n \n \n \n \n This theme for  Jekyll  has been inspired by  dash-ui , a dark theme for  Atom . \n \n Installation \n Add this line to your Jekyll site\'s  Gemfile : \n For  Jekyll 3 :\n ruby\ngem \'jekyll-dash\', \'~> 1.0.0\' \n \n Keep in mind: Github pages generation only supports Jekyll 3.8 right now. \n \n For  Jekyll 4 :\n ruby\ngem \'jekyll-dash\', \'~> 2.0.0\' \n And add this line to your Jekyll site\'s  _config.yml : \n yaml\ntheme: jekyll-dash \n And then execute: \n $ bundle\n \n Or install it yourself as: \n $ gem install jekyll-dash -v version\n \n Configuration \n Add the following configuration to your site. Customise it to your needs! \n ```yaml \n required by disqus to display comments \n url: https://your-site-url \n jekyll-paginate \n paginate: 5\npaginate_path: "/blog/page:num/" \n jekyll-tagging (optional) \n tag_permalink_style: pretty\ntag_page_layout: tag_page\ntag_page_dir: tag \n dash:\n  date_format: "%b %-d, %Y" \n disqus:\n    shortname:  \n # generate social links in footer\n  # supported colors: green, red, orange, blue, cyan, pink, teal, yellow, indigo, purple\n  social_links:\n    - url: https://twitter.com/bitbrain_\n      icon: twitter-square\n      color: cyan\n    - url: https://bitbrain.itch.io\n      icon: itch-io\n      color: red\n    - url: https://github.com/bitbrain\n      icon: github-square\n      color: purple \n show_author: true \n Replaces the default avatar provider (gravatar) \n avatar_source: github \n github_username: bitbrain \n ``` \n Using this theme directly on Github Pages \n Please keep in mind that Github Pages does only support  a limited list of Jekyll plugins . You will be able to use this theme on Github Pages but some functionality might not be available, for example displaying tags. In order to use this theme to a full extend, you have to generate the  _site  externally, for example on  TravisCI .  \n For example, you want to host your own blog on  https://<username>.github.io . As a result, you require the following repositories: \n \n blog  - contains the actual Jekyll sources ( see example ) \n <username>.github.io  - contains generated webpage, pushed automatically via TravisCI ( see example ) \n \n You are not required to do this, but keep in mind that some functionality might not be available when using the Jekyll generator on Github directly! \n Additional Features \n Tagging  add the  jekyll/tagging  plugin to your  _config.yml  file to enable tagging. Do not forget to also add the following to your  Gemfile :\n Gemfile\ngem "jekyll-tagging" \n Gravatar  if you want to display your gravatar picture, add the  liquid-md5  to your  _config.yml  file. Do not forget to also add the following to your  Gemfile :\n Gemfile\ngem "liquid-md5" \n FAQ \n \n I have configured posts but no posts are showing? \n \n Solution:  You most probably forgot to configure  jekyll-paginate  in your _config.yml! Make sure you have the correct configuration as described above! \n \n I have added the correct configuration for  jekyll-paginate  but it is now complaining about a missing  index.html  file. What do I do? \n \n Solution  pagination only works with HTML files! Markdown is not supported there. Simply rename your  index.md  into  index.html  - that should do the trick! \n \n I have configured Disqus via _config.yml but Disqus fails to load on the page?  \n \n Solution:  Make sure you configure the correct  url  within your  _config.yml . Also make sure that your domain is trusted by Disqus. This can be configured within Disqus by adding a trusted domain. \n \n I am using this theme but I don\'t see any tags? \n \n Solution : as described above you have to add the tagging plugin. Additionally, tags do not work natively by Github Pages. You have to build your site on an external CI and push the  _site  artifacts to a hosting repository. \n \n I am getting an error that Bundler could not find compatible versions for gem \n \n Solution \n Make sure you are using a version of this theme that is compatible with Jekyll. Version 1.x is only compatible with Jekyll 3.x while Version 2.x is only compatible with Jekyll 4.x. \n Contributing \n Bug reports and pull requests are welcome on GitHub at https://github.com/bitbrain/jekyll-dash. This project is intended to be a safe, welcoming space for collaboration, and contributors are expected to adhere to the  Contributor Covenant  code of conduct. \n Development \n To set up your environment to develop this theme, run  bundle install . \n Your theme is setup just like a normal Jekyll site! To test your theme, run  bundle exec jekyll serve  and open your browser at  http://localhost:4000 . This starts a Jekyll server using your theme. Add pages, documents, data, etc. like normal to test your theme\'s contents. As you make modifications to your theme and to your content, your site will regenerate and you should see the changes in the browser after a refresh, just like normal. \n When your theme is released, only the files in  _layouts ,  _includes ,  _sass  and  assets  tracked with Git will be bundled.\nTo add a custom directory to your theme-gem, please edit the regexp in  jekyll-dash.gemspec  accordingly. \n License \n The theme is available as open source under the terms of the  Apache License 2.0 .']
yuce,['Elixir Random module \n This module contains pseudo-random number generators for various distributions\nported from Python 3  random  module for  Elixir .\nThe documentation below is adapted from that module as well. \n \n For integers, there is uniform selection from a range. For sequences, there is uniform\nselection of a random element, a function to generate a random permutation of a list in-place,\nand a function for random sampling without replacement. \n On the real line, there are functions to compute uniform, normal (Gaussian), lognormal,\nnegative exponential, gamma, and beta distributions. For generating distributions of angles,\nthe von Mises distribution is available. \n \n Documentation \n \n \n Module documentation \n \n \n Python 3 random module documentation \n \n \n Build \n The only dependency is  TinyMT Erlang , which\nis available on  hex.pm \n $ mix get.deps\n$ mix\n \n Test \n $ mix test\n \n Usage \n Random  is available on  hex.pm .\nYou neeed to include  {:random, "~> 0.2.3"}  as a dependency in your project. \n Examples \n iex(1)> Random.randint(10, 20)\n14\niex(2)> Random.sample(0..10000, 4)\n[4436, 5015, 7231, 9459]\niex(3)> {n, gauss_next} = Random.gauss(1, 2)\n{-2.0056082102271917, 0.5561885306380824}\niex(4)> {n, gauss_next} = Random.gauss(1, 2, gauss_next)\n{2.112377061276165, nil}\n \n Thanks \n \n \n Kenji Rikitake  pointed out a range error in the module and provided code which enables using his  TinyMT Erlang \nlibrary to produce floats in  [0.0, 1.0)  range. \n \n \n p2k  updated the project to be compatible with Elixir 1.2. \n \n \n sashaegorov  updated the project to be compatible with Elixir 1.4. \n \n', '\n \n \n PySwip \n \n Installing the Latest Version \n The latest SWI-Prolog supported by Ubuntu 18.04 and 20.04 are 7.6.4. We generally want to support LTS releases of\nUbuntu. The compatibility of PySwip with 7.6.4 on master is broken, so we are not able to release a new version until\nthis is fixed. In the meantime, you can use the following to install PySwip from the master branch: \n pip install git+https://github.com/yuce/pyswip@master#egg=pyswip \n The End of Python 2 Support \n Python 2 has reached end of life on January 1st, 2020 as documented  here .\nSo, PySwip 0.2.10 will be the last version which officially supports Python 2. \n Do you still require Python 2 support? Let us know at: https://github.com/yuce/pyswip/issues/94 \n \n What\'s New? \n See the  CHANGELOG . \n WARNING! PySwip has no Windows installers! If you are a Windows user, see  INSTALL . There are some "free download" sites that claim to be hosting PySwip installers. DO NOT TRUST THEM! \n Thanks to all  contributors . \n Introduction \n PySwip is a Python - SWI-Prolog bridge enabling to query  SWI-Prolog  in your Python programs.\nIt features an (incomplete) SWI-Prolog foreign language interface, a utility class that makes it easy querying with Prolog and also a\nPythonic interface. \n Since PySwip uses SWI-Prolog as a shared library and ctypes to access it, it doesn\'t require compilation to be installed. \n Requirements: \n \n Python 3.6 and higher. \n PyPy is currently not supported. \n SWI-Prolog 8.2 and higher. \n libswipl  as a shared library.  This is the default on most platforms. \n Works on Linux, Windows, MacOS and FreeBSD. Should work on other POSIX. \n \n Install \n IMPORTANT: Make sure the SWI-Prolog architecture is the same as the Python architecture. If you are using a 64bit build of Python, use a 64bit build of SWI-Prolog, etc. \n See  INSTALL  for instructions. \n Examples \n Using Prolog \n ```python\nfrom pyswip import Prolog\nprolog = Prolog()\nprolog.assertz("father(michael,john)")\nprolog.assertz("father(michael,gina)")\nlist(prolog.query("father(michael,X)")) == [{\'X\': \'john\'}, {\'X\': \'gina\'}]\nfor soln in prolog.query("father(X,Y)"):\n    print(soln["X"], "is the father of", soln["Y"]) \n michael is the father of john \n michael is the father of gina \n ``` \n An existing knowledge base stored in a Prolog file can also be consulted,\nand queried. Assuming the filename "knowledge_base.pl" and the Python is\nbeing run in the same working directory, it is consulted like so: \n >>> from pyswip import Prolog\n>>> prolog = Prolog()\n>>> prolog.consult("knowledge_base.pl")\n \n Foreign Functions \n ```python\nfrom  future  import print_function\nfrom pyswip import Prolog, registerForeign \n def hello(t):\n    print("Hello,", t)\nhello.arity = 1 \n registerForeign(hello) \n prolog = Prolog()\nprolog.assertz("father(michael,john)")\nprolog.assertz("father(michael,gina)")\nprint(list(prolog.query("father(michael,X), hello(X)")))\n``` \n Pythonic interface (Experimental) \n ```python\nfrom  future  import print_function\nfrom pyswip import Functor, Variable, Query, call \n assertz = Functor("assertz", 1)\nfather = Functor("father", 2)\ncall(assertz(father("michael","john")))\ncall(assertz(father("michael","gina")))\nX = Variable() \n q = Query(father("michael",X))\nwhile q.nextSolution():\n    print("Hello,", X.value)\nq.closeQuery() \n Outputs: \n Hello, john \n Hello, gina \n ``` \n The core functionality of  Prolog.query  is based on Nathan Denny\'s public domain prolog.py. \n Help! \n \n Support Forum \n Stack Overflow \n \n Projects/Publications that Use or Reference PySwip \n Do you have a project, video or publication that uses/mentions PySwip?  file an issue  or send a pull request. \n If you would like to reference PySwip in a LaTeX document, you can use the provided  BibTeX file . \n Books \n \n Beginning Artificial Intelligence with the Raspberry Pi \n \n Publications \n \n Assessment of Graph Databases as a Viable Materiel Solution for the Army\'s Dynamic Force Structure (DFS) Portal Implementation: Part 3, Risks, Mitigation Approach, and Roadmap \n Tackling Complexity in High Performance Computing Applications \n Social Human-Robot Interaction: A New Cognitive and Affective Interaction-Oriented Architecture \n A Planning Module for a ROS-Based Ubiquitous Robot Control System  (PDF) \n A pilot framework developed as a common platform integrating diverse elements of computer aided fixture design \n Integration von Prolog und ClioPatria in Python  (PDF, German) \n SELECTSCRIPT: A Query Language for Robotic World Models and Simulations \n A Concept for Declarative Information Acquisition in Smart Environments  (PDF) \n Implementation on ADHD Diagnostic Expert System based on DSM Diagnostic Criteria  (PDF, Korean) \n Wie sehen Krebsmolekule aus? Vergleich der Gute der Klassifizierung potenziell krebserregender Molekule durch induktiv logische und merkmalsbasierte Lernverfahren  (PDF, German) \n Companion Robots Behaving with Style: Towards Plasticity in Social Human-Robot Interaction  (PDF) \n Semi-automatically Augmenting Attack Trees using an Annotated Attack Tree Library \n A Learning Framework for Tool Creation by a Robot  (PDF) \n Conceptual Maps as the First Step in an Ontology Construction Method \n Fact-Based Expert System for Supplier Selection with ERP Data \n Interactive Text Graph Mining with a Prolog-based Dialog Engine \n The Detection Of Conflicts In The Requirements Specification Based On An Ontological Model And A Production Rule System \n Dependency-based Text Graphs for Keyphrase and Summary Extraction with Applications to Interactive Content Retrieval  (PDF) \n Information Retrieval Based on Knowledge-Enhanced Word Embedding Through Dialog: A Case Study \n Exploring the world of declarative programming \n \n Videos \n \n AI - Blocks world solver interactive planner \n PySwip, Prolog, Javascript and HTML  (Spanish) \n Get out of the maze with Prolog and Python  (Spanish) \n Les robots deviennent (vraiment) intelligents ! (NAO discute avec Kylo Ren) \n \n Projects \n \n noworkflow  Supporting infrastructure to run scientific experiments without a scientific workflow management system. http://gems-uff.github.io/noworkflow \n Super Pacman \n Pokemon Weak Detector \n Food Recommendations in Hyderabad, India  Food Recommendation AI Expert System using a GUI hosted on Flask and a backend developed with PYSWIP and native Prolog. \n pyswip_envctrl  An environment control module expert system written in PySwip. \n tic-tac-toe  Tic-tac-toe game with AI in Prolog and GUI in Python (kivy framework + pyswip). \n TBM1 - "Getting to Know My Home" \n Prolog natural language parsing component to control a Scribbler II robot over bluetooth \n Cosmos  A new logic programming language. \n lib-annotated-attack-trees  Scripts and resources for creating a library of annotated attack trees and using it to refine an annotated attack tree. \n ClIDE  Command-line Intelligent Development Environment \n Artificial Intelligence INF1771 @ PUC-Rio  Projects for the Artificial Intelligence class @ PUC-Rio \n AutomobileAdvisor  Projekt na systemy ekspertowe pomagający wybrać odpowiedni samochód dla danego klienta na podstawie preferencji (Polish) \n Prolog Tetris AI \n Jupyter SWI Prolog  A Jupyter Kernel for SWI-Prolog. \n Blocks World Planner  A program that allows users to solve the blocks world problem interacting only using the natural language. \n DeepTalk  A Python+Prolog based Dialog Engine using the Python package text_graph_crafts that extracts the highest ranked sentences answering a query. \n DeepRank  The system uses dependency links for building Text Graphs, that with help of a centrality algorithm like PageRank, extract relevant keyphrases, summaries and relations from text documents. \n Prolog Tic-tac-toe  A full-stack tic-tac-toe game with AI in Prolog, backend in Python3 (+Flask) and frontend in Vue.js 3. \n MIDSI Project  Solution for data discovery in projects applicable to the\n  Semantic Web, enabling the loading of ontologies and inference of results using the WSML language. \n Popper  An inductive logic programming system. \n \n Blog Posts \n \n Calling Prolog from Python \n Python v. Prolog: Round 1: Fight! \n Path Follower: Arduino+Rasp on ROS  and its  Project code \n 10 minutes to make a GUI for your SWI-Prolog App via Python \n \n Companies using PySwip \n \n Magazino GmbH  Magazino develops and builds intelligent, mobile robots for intralogistics. \n \n License \n ```\nCopyright (c) 2007-2020 Yüce Tekol and PySwip contributors \n Permission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the "Software"), to deal in\nthe Software without restriction, including without limitation the rights to\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\nthe Software, and to permit persons to whom the Software is furnished to do so,\nsubject to the following conditions: \n The above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software. \n THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\nFOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\nCOPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\nIN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n```', 'Twitter Kit \n Introduction \n Twitter Kit is an Erlang library for Twitter REST API. \n Twitter Kit tries to do as little as it can while abstracting away the boring parts of Twitter API, like authentication and cursors/timelines. \n The library is composed of three parts: \n \n Authentication functions in  twitter_auth  module, \n Generic  GET  and  POST  request functions in  twitter_rest  module, \n Easy and concise API in the  twitter  module. \n \n You won\'t need to use  twitter_rest  module unless you need something which is not implemented in  twitter  module. \n Features: \n \n OAuth authentication, \n Application ( Bearer ) authentication, \n Twitter REST API, \n Cursor and timelines, \n Media upload, \n Comprehensive documentation, examples and tests. \n \n Todo: \n \n Easy API for the following REST endpoints: direct_messages, account, blocks, users, mutes, favorites, lists, saved_searches, geo, trends, application, help \n Streaming API \n More tests \n Test on non-recent Erlang versions and Linux \n \n Install \n Twitter Kit uses  Rebar  as its build tool. You just need to add it to your  rebar.config  as a dependency: \n {deps, [{twitter_kit, "1.*", {git, "https://github.com/yuce/twitter_kit.git", "master"}}]}.\n \n Twitter Kit uses  jsx  to decode JSON responses. You don\'t need to add it to your config. \n Usage \n Here\'s a simple example which shows how to traverse user timelines: \n % ConsumerKey and ConsumerSecret are defined somewhere.\n% AccessToken and AccessTokenSecret are defined somewhere.\n% They come from the "Keys and Access Tokens" tab of your\n% Twitter "Application Management" page, linked from\n% https://apps.twitter.com\nAuth = twitter_auth:new({consumer, ConsumerKey, ConsumerSecret},\n                        {token, AccessToken, AccessTokenSecret}),\nApi = twitter:new(Auth),\n{ok, {Pointer, Tweets}} = twitter:get(Api, {statuses, user_timeline},\n                        [{screen_name, "twitter"},\n                         {count, 10}]).\n% Do something with the tweets\n% Fetch earlier tweets\n{ok, {Pointer2, EarlierTweets}} = twitter:prev(Pointer).\n \n And another one which shows how to post a tweet with an attached photo: \n % ConsumerKey and ConsumerSecret are defined somewhere.\n% AccessToken and AccessTokenSecret are defined somewhere.\n% They come from the "Keys and Access Tokens" tab of your\n% Twitter "Application Management" page, linked from\n% https://apps.twitter.com\nAuth = twitter_auth:new({consumer, ConsumerKey, ConsumerSecret},\n                        {token, AccessToken, AccessTokenSecret}),\nApi = twitter:new(Auth),\n% MediaBinary is the image data, loaded from somewhere, e.g., file system\n{ok, Data} = twitter:post(Api, {media, upload}, {media, "Sample Photo", MediaBinary}),\n{_, MediaId} = lists:keyfind(<<"media_id">>, 1, Data),\n% Post the tweet\nText = "This is my funny tweet",\nMediaIdStr = integer_to_list(MediaId),\n{ok, Tweet} = twitter:post(Api, {statuses, update}, [{status, Text}, {media_ids, MediaIdStr}]).\n \n See the  docs  for more documentation. \n Examples and tests \n There are some  example scripts  which show how some common tasks are done. \n See:  README for tests .', '\n \n \n POT \n \n Introduction \n Version History \n Usage \n Function Reference \n Examples (Erlang) \n Examples (Elixir) \n Credits \n Licence \n \n Introduction \n POT is an Erlang library for generating one time passwords. It supports both HMAC-based one time passwords (HOTP) and time based ones (TOTP). The generated passwords are based on  RFC 4226  and  RFC 6238 , compatible with  Google Authenticator . \n POT is an almost direct translation of the Python  OneTimePass  library. \n POT should work with any recent version of  Erlang/OTP ,  Elixir , and other Erlang VM based languages. \n In order to learn more about one time password generation, see the following Wikipedia articles: \n \n Google Authenticator \n HMAC-based One-time Password Algorithm  ( RFC 4226 ) \n Time-based One-time Password Algorithm  ( RFC 6238 ) \n \n Version History \n 2021-08-07 \n \n \n Released version 1.0.2 with the following changes: \n \n \n Fix type specs  (Thanks to Krzysztof Jurewicz) \n \n Added OTP 24.0 to CI  (Thanks to Julius Beckmann) \n \n 2021-03-28 \n \n \n Released version 1.0.1 with the following changes: \n \n Migrate from Travis to GitHub Actions  (Thanks to Nicholas Lundgaard) \n Update pot.erl to support sha256 and not use deprecated :crypto.hmac  (Thanks to Francois Paul) \n \n \n \n 2020-09-15 \n \n \n Released version 1.0.0 with the following changes: \n \n Move coveralls into project_plugins  (Thanks to Bryan Paxton) \n \n \n \n 2020-03-08 \n \n \n Released version 0.11.0 with the following changes: \n \n Improved types, README documentation  (Thanks to Nicholas Lundgaard) \n Add return_interval option to valid_hotp  (Thanks to Nicholas Lundgaard) \n \n \n \n 2019-10-16 \n \n \n Released version 0.10.2 with the following change: \n \n Fix valid_totp to support upper bound on check_candidate  (Thanks to Nicholas Lundgaard) \n \n \n \n 2019-08-03 \n \n \n Released version 0.10.1 with the following change: \n \n Added pot prefix to base32 module avoid name collision  (Thanks to Girish Ramnani). This is a breaking change,  base32  module was renamed to  pot_base32 . \n \n \n \n 2019-07-09 \n \n \n Released version 0.9.8 with the following bug fix: \n \n Return boolean on pot:valid_hotp/2 and pot:valid_hotp/3  (Thanks to Zbigniew Pekala) \n \n \n \n 2018-02-12 \n \n pot:totp/2  supports setting the timestamp (Thanks to Julius Beckmann) \n \n 2017-08-04 \n \n Added options to support Android devices (Thanks to Pedro Vieira) \n \n 2016-07-30 \n \n Released version 0.9.5 with bug fixes (Thanks to Peter McLain) \n \n 2015-01-20 \n \n Embedded base32_erlang library \n \n 2015-01-18 \n \n Initial version \n \n Usage \n See the sections below on using  pot  in your Erlang and Elixir project. \n Erlang \n We recommend using  rebar3  for managing dependencies and building the library. POT is available on hex.pm, so you can just include the following in your  rebar.config : \n {deps, [pot]}. \n See the  Erlang examples \n Elixir \n Include POT in your  mix.exs  as a dependency: \n elixir\ndefp deps do\n    [{:pot, "~>0.10.1"}]\nend \n Function Reference \n The functions below refer to the following common parameters: \n | Parameter  | Type     |\n|------------|----------|\n|  Interval  | integer  |\n|  Secret    | string* |\n|  Token     | string* | \n \n Interval  is an integer that represents the counter value, the "moving factor" referenced in  RFC 4226 . It is an 8 byte unsigned integer; if a negative and/or too large integer is passed, it will be 2\'s complemented and truncated appropriately. \n Secret  is a base-32-encoded secret key. Generally, it should be at least 128 bits, preferably 160 bits.  \n Token  is a HOTP/TOTP value represented as a string*. This is generally a 6-digit number, e.g., "123456", but its length may be modulated with the  token_length  option. \n \n *Note: for  Erlang  uses of  pot , all strings should be in  binary()  format. \n Token Generation Functions \n hotp/2,3 \n Generate an  RFC 4226  compatible HOTP token.  \n Erlang: \n pot:hotp(Secret, Interval) -> Token\npot:hotp(Secret, Interval, Options) -> Token \n Elixir: \n :pot.hotp(Secret, Interval) -> Token\n:pot.hotp(Secret, Interval, Options) -> Token \n The following  Options  are allowed: \n | Option          | Type        | Default |\n|-----------------|-------------|---------|\n|  digest_method  | atom        | sha     |\n|  token_length   | integer > 0 | 6       | \n \n digest_method  controls the signing algorithm passed to the  Erlang   crypto  module\'s  hmac  function. For  RFC 4226  compliant tokens, it must be set to  sha . For  RFC 6238  compliant tokens, additional values such as  sha256  or  sha512  may be used. \n token_length  controls the number of digits in output  Token . \n \n totp/1,2 \n Generate an  RFC 6238  compatible TOTP token.  \n Erlang: \n pot:totp(Secret) -> Token\npot:totp(Secret, Options) -> Token \n Elixir: \n :pot.totp(Secret) -> Token\n:pot.totp(Secret, Options) -> Token \n The following  Options  are allowed: \n | Option            | Type        | Default/Reference        |\n|-------------------|-------------|--------------------------|\n|  addwindow        | integer     | 0                        |\n|  digest_method    | atom        | from  hotp/2,3  |\n|  interval_length  | integer > 0 | 30                       |\n|  timestamp        | timestamp   |  os:timestamp()    |\n|  token_length     | integer > 0 | from  hotp/2,3  | \n \n addwindow  acts as an offset to the  Interval  extrapolated from dividing the  timestamp  by the  interval_length  per the algorithm described in  RFC 6238 . \n interval_length  controls the number of seconds for the  Interval  computation. \n timestamp  may be passed to specify a custom timestamp (in Erlang  timestamp  format) to use for computing the  Interval  used to generate a  Token . \n \n Token Validation Functions \n valid_token/1,2 \n Validate that a given  Token  has the correct format (correct length, all digits). \n Erlang: \n pot:valid_token(Token) -> Boolean\npot:valid_token(Token, Options) -> Boolean \n Elixir: \n :pot.valid_token(Token) -> Boolean\n:pot.valid_token(Token, Options) -> Boolean \n The following  Options  are allowed: \n | Option            | Type        | Default/Reference        |\n|-------------------|-------------|--------------------------|\n|  token_length     | integer > 0 | from  hotp/2,3  | \n valid_hotp/2,3 \n Validate an  RFC 4226  compatible HOTP token. Returns  true  if the  Token  is valid.  \n Erlang: \n pot:valid_hotp(Token, Secret) -> Boolean\npot:valid_hotp(Token, Secret, Options) -> Boolean | {true, interval()} \n Elixir: \n :pot.valid_hotp(Token, Secret) -> Boolean\n:pot.valid_hotp(Token, Secret, Options) -> Boolean | {true, interval()} \n The following  Options  are allowed: \n | Option            | Type        | Default/Reference        |\n|-------------------|-------------|--------------------------|\n|  digest_method    | atom        | from  hotp/2,3  |\n|  last             | integer     | 1                        |\n|  return_interval  | boolean     | false                     |\n|  token_length     | integer > 0 | from  hotp/2,3  |\n|  trials           | integer > 0 | 1000                     | \n \n last  is the  Interval  value of the previous valid  Token ; the next  Interval  after  last  is used as the first candidate for validating the  Token . \n trials  controls the number of incremental  Interval  values after  last  to try when validating the  Token . If a matching candidate is not found within  trials  attempts, the  Token  is considered invalid. \n return_interval  controls whether the matching  Interval  of a valid  Token  is returned with the result. if set to  true , then  valid_hotp/2  will return  {true, Interval}  (e.g.,  {true, 123} ) when a valid  Token  is provided. \n \n valid_totp/2,3 \n Validate an  RFC 6238  compatible TOTP token. Returns  true  if the  Token  is valid. \n Erlang: \n pot:valid_totp(Token, Secret) -> Boolean\npot:valid_totp(Token, Secret, Options) -> Boolean \n Elixir: \n :pot.valid_totp(Token, Secret) -> Boolean\n:pot.valid_totp(Token, Secret, Options) -> Boolean \n The following  Options  are allowed: \n | Option            | Type        | Default/Reference        |\n|-------------------|-------------|--------------------------|\n|  addwindow        | integer     | from  totp/1,2  |\n|  digest_method    | atom        | from  hotp/2,3  |\n|  interval_length  | integer > 0 | from  totp/1,2  |\n|  timestamp        | timestamp   | from  totp/1,2  |\n|  token_length     | integer > 0 | from  hotp/2,3  |\n|  window           | integer > 0 | 0                        | \n \n window  is a range used for expanding  Interval  value derived from the  timestamp . This is done by considering the  window   Interval s before  and  after the one derived from the  timestamp . This allows validation to be relaxed to allow for successful validation of TOTP  Token s generated by clients with some degree of unknown clock drift from the server, as well as some client entry delay.  \n \n Examples (Erlang) \n POT works with binary tokens and secrets. \n Create a time based token \n erlang\nSecret = <<"MFRGGZDFMZTWQ2LK">>,\nToken = pot:totp(Secret),\n% Do something with the token \n Create an HMAC based token \n erlang\nSecret = <<"MFRGGZDFMZTWQ2LK">>,\nCurrentTrial = 3,\nToken = pot:hotp(Secret, CurrentTrial),\n% Do something with the token \n Check some time based token \n erlang\nSecret = <<"MFRGGZDFMZTWQ2LK">>,\nToken = <<"123456">>,\nIsValid = pot:valid_totp(Token, Secret),\n% Do something \n Check some HMAC based token \n erlang\nSecret = <<"MFRGGZDFMZTWQ2LK">>,\nToken = <<"123456">>,\nLastUsed = 5,  % last successful trial\nIsValid = pot:valid_hotp(Token, Secret, [{last, LastUsed}]),\n% Do something \n Alternatively, to get the last interval from a validated token: \n erlang\nSecret = <<"MFRGGZDFMZTWQ2LK">>,\nToken = <<"123456">>,\nLastUsed = 5,  % last successful trial\nOptions = [{last, LastUsed}, {return_interval, true}],\nNewLastUsed = case pot:valid_hotp(Token, Secret, Options) of\n                  {true, LastInterval} -> LastInterval;\n                  false -> LastUsed\n              end,\n% Do something \n Create a time based token with 30 seconds ahead \n erlang\nSecret = <<"MFRGGZDFMZTWQ2LK">>,\nToken = pot:totp(Secret, [{addwindow, 1}]),\n% Do something \n Check a time based token from a mobile device with 30 seconds ahead and a ±1 interval tolerance \n erlang\nSecret = <<"MFRGGZDFMZTWQ2LK">>,\nToken = <<"123456">>,\nIsValid = pot:valid_totp(Token, Secret, [{window, 1}, {addwindow, 1}]),\n% Do something \n Create a time based token for given time \n Time format is  {MegaSecs, Secs, MicroSecs}  received by os:timestamp() \n erlang\nSecret = <<"MFRGGZDFMZTWQ2LK">>,\nToken = pot:totp(Secret, [{timestamp, {1518, 179058, 919315}}]),\n% Token will be <<"151469">> \n Examples (Elixir) \n Create a time based token \n ```elixir\nsecret = "MFRGGZDFMZTWQ2LK"\ntoken = :pot.totp(secret) \n Do something with the token \n ``` \n Create an HMAC based token \n ```elixir\nsecret = "MFRGGZDFMZTWQ2LK"\ncurrent_trial = 3\ntoken = :pot.hotp(secret, current_trial) \n Do something with the token \n ``` \n Check some time based token \n ```elixir\nsecret = "MFRGGZDFMZTWQ2LK"\ntoken = "123456"\nis_valid = :pot.valid_totp(token, secret) \n Do something \n ``` \n Check some HMAC based token \n ```elixir\nsecret = "MFRGGZDFMZTWQ2LK"\ntoken = "123456"\nlast_used = 5  # last successful trial\nis_valid = :pot.valid_hotp(token, secret, [{:last, last_used}]) \n Do something \n ``` \n Alternatively, to get the last interval from a validated token: \n ```elixir\nsecret = "MFRGGZDFMZTWQ2LK"\ntoken = "123456"\nlast_used = 5  # last successful trial\noptions = [{:last, last_used}, {:return_token, true}]\nnew_last_used =\n    case :pot.valid_hotp(token, secret, options) do\n        {true, last_interval} -> last_interval\n        false -> last_used\n    end \n Do something \n ``` \n Create a time based token with 30 seconds ahead \n ```elixir\nsecret = "MFRGGZDFMZTWQ2LK"\ntoken = :pot.totp(secret, [addwindow: 1]) \n Do something \n ``` \n Check a time based token from a mobile device with 30 seconds ahead and a ±1 interval tolerance \n ```elixir\nsecret = "MFRGGZDFMZTWQ2LK"\ntoken = "123456"\nis_valid = :pot.valid_totp(token, secret, [window: 1, addwindow: 1]) \n Do something \n ``` \n Create a time based token for given time \n Time format is  {MegaSecs, Secs, MicroSecs}  received by :os.timestamp() \n ```elixir\nsecret = "MFRGGZDFMZTWQ2LK"\ntoken = :pot.totp(secret, [timestamp: {1518, 179058, 919315}]) \n Token will be <<"151469">> \n ``` \n Credits \n \n Yüce Tekol \n Tomasz Jaskowski:  OneTimePass  Python library \n Andrew Tunnell-Jones:  base32_erlang  library \n \n Thanks to  contributors . \n Maintainers \n \n 2020 -  ... : Nicholas Lundgaard \n 2014 - 2020 : Yüce Tekol \n \n License \n Copyright (c) 2014-2021 POT Contributors \n Permission is hereby granted, free of charge, to any person obtaining a copy of this software\nand associated documentation files (the "Software"), to deal in the Software without\nrestriction, including without limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the\nSoftware is furnished to do so, subject to the following conditions: \n The above copyright notice and this permission notice shall be included in all copies or\nsubstantial portions of the Software. \n THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\nBUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\nDAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.', 'png \n png  is a pure Erlang library for creating PNG images. It can currently create  8  and  16  bit  RGB ,  RGB with alpha ,  indexed ,  grayscale  and  grayscale with alpha  images. \n Install (Erlang) \n png  requires at least Erlang 17.0, since it uses maps. \n Include the library in your  rebar  configuration: \n {deps, [\n    {png, ".*", {git, "https://github.com/yuce/png.git", "master"}}]}.\n \n Alternatively, for  rebar 3 : \n {deps, [png]}.\n \n Taster \n ```erlang\nrandom:seed(erlang:now()),\nWidth = 30,\nHeight = 30,\n% define an 8bit RGB palette with 4 colors:\nPalette = {rgb, 8, [{255, 0, 0}, {128, 255, 128}, {64, 64, 255}, {0, 0, 0}]},\n{ok, File} = file:open("sample.png", [write]), \n % We create an 8bit indexed PNG\nPng = png:create(#{size => {Width, Height},\n                   mode => {indexed, 8},\n                   file => File,\n                   palette => Palette}), \n % make the png image row by row\nAppendRow = fun( ) ->\n                % An image row is composed of palette indices for indexed PNGs\n                Row = lists:map(fun( ) -> random:uniform(4) - 1 end,\n                                    lists:seq(1, Width)),\n                png:append(Png, {row, Row}) end,\nlists:foreach(AppendRow, lists:seq(1, Height)),\n% need to finalize the image\nok = png:close(Png),\nok = file:close(File).\n``` \n Mini-Tutorial \n Creating a PNG image using  png  is a 3-step process: \n \n Pass configuration and initialize the image, \n Append the pixels, row by row or all at once, \n Finalize the image. \n \n The configuration currently is a  map  which consists of the dimensions (size) of the image, color mode and bits per pixel (or index), an optional palette (for indexed PNGs) and finally a callback which is called every time  png  needs to write some data. \n erlang\nWidth = 100,\nHeight = 150,\nColorMode = indexed,\nBits = 8,\nPalette = {ColorMode, Bits, [{255, 0, 0}, {0, 0, 128}]},\nCallback = fun(IoData) -> io:format("Received: ~p~n", [IoData]) end,\nConfig = #{size => {Width, Height},\n           mode => {ColorMode, Bits},\n           palette => Palette,\n           call => Callback}, \n There is a shortcut for writing to a file instead of a callback, using the  file  key: \n erlang\n{ok, File} = file:open("my.png", [write]),\nConfig = #{size => {Width, Height},\n           mode => {ColorMode, Bits},\n           palette => Palette,\n           file => File}, \n Once we have the configuration, let\'s start creating the image: \n erlang\nPng = png:create(Config), \n png:create  writes the PNG header and the palette to the callback or the file and returns a  map  with the configuration + some state data. \n Next step is appending pixels to the image. \n The representation of a pixel depends on the  color mode  and  bits . Here\'s a summary of currently supported image modes: \n Color Mode Name  | Color Mode        | Bits | Representation\n------------------|-------------------|------|---------------\nGrayscale         |  grayscale        |   8  |  <<L:8>> \nGrayscale + Alpha |  grayscale_alpha  |   8  |  <<L:8, A:8>> \nIndexed           |  indexed          |   8  |  <<I:8>> \nRGB               |  rgb              |   8  |  <<R:8, G:8, B:8>> \nRGB + Alpha       |  rgba             |   8  |  <<R:8, G:8, B:8, A:8>> \nGrayscale         |  grayscale        |  16  |  <<L:16>> \nGrayscale + Alpha |  grayscale_alpha  |  16  |  <<L:16, A:16>> \nIndexed           |  indexed          |  16  |  <<I:16>> \nRGB               |  rgb              |  16  |  <<R:16, G:16, B:16>> \nRGB + Alpha       |  rgba             |  16  |  <<R:16, G:16, B:16, A:16>> \n Where,  L  is the luminance,  A  is alpha ( opacity ),  I  is the palette index,  R ,  G  and  B  are red, green and blue respectively. \n A palette is required for  indexed  images. The palette is represented as a tuple of color mode, bits and list of red, green, blue tuples: \n erlang\nColorMode = indexed,\nBits = 8,\nRed = {255, 0, 0},\nGreen = {0, 255, 0},\nBlue = {0, 0, 255},\nBlack = {0, 0, 0},\nPalette = {ColorMode, Bits, [Red, Green, Blue, Black]}, \n png  supports adding pixels row by row (PNG specification uses the term  scanline ), a list of rows or as raw data. Being able to add partial image data is important when you want to keep the required memory low, e.g., creating an image on the fly in response to a web request and sending it in chunks. \n Here are supported data layouts: \n Name     | Atom | Data Type       | Example\n---------|------|-----------------|--------\nA Row    | row  | binary          |  {row, <<1, 2, 3>>} \nRows     | rows | binary list     |  {rows, [<<1, 2, 3>>, <<2, 3, 1>>]} \nRaw data | data | binary          |  {data, <<0, 1, 2, 3>>} \n Note that, each row ( scanline ) must start with a  filter method ID . Currently only filter  0  (no filter) is supported (see  PNG specification  for more information). For  row  and  rows ,  png  prepends the filter method ID to each row, but for  data , you must prepend  0  to every row yourself. \n Appending image data is done with  png:append  ( surprise! ). An example: \n erlang\nData = {row, <<1, 2, 3>>},\nPng = png:append(Png, Data), \n You can call  png:append  as much as as necessary. After appending all data, you must finalize the image with  png:close : \n erlang\nok = png:close(Png) \n Examples \n There are some examples on https://github.com/yuce/png/tree/master/examples which show the usage and adding PNG chunks manually.', 'TypeScript-Unittest', "\n \n \n \n Hazelcast Node.js Client \n \n \n \n \n \n \n \n \n  markdownlint-disable-next-line MD013  \n \n \n \n \n Hazelcast  is an open-source distributed in-memory data store and computation platform that\nprovides a wide variety of distributed data structures and concurrency primitives. \n Hazelcast Node.js client is a way to communicate to Hazelcast clusters and access the cluster data.\nThe client provides a Promise-based API with a builtin support for native JavaScript objects. \n Installation \n Hazelcast \n Hazelcast Node.js client requires a working Hazelcast cluster to run. This cluster handles the storage and\nmanipulation of the user data. \n A Hazelcast cluster consists of one or more cluster members. These members generally run on multiple virtual or\nphysical machines and are connected to each other via the network. Any data put on the cluster is partitioned to\nmultiple members transparent to the user. It is therefore very easy to scale the system by adding new members as\nthe data grows. Hazelcast cluster also offers resilience. Should any hardware or software problem causes a crash\nto any member, the data on that member is recovered from backups and the cluster continues to operate without any\ndowntime. \n The quickest way to start a single member cluster for development purposes is to use our\n Docker images . \n bash\ndocker run -p 5701:5701 hazelcast/hazelcast \n This command fetches the latest Hazelcast version. You can find all available tags\n here . \n You can also use our ZIP or TAR  distributions \nas described  here . \n Client \n bash\nnpm install hazelcast-client \n Overview \n Usage \n ```js\nconst { Client } = require('hazelcast-client'); \n // Connect to Hazelcast cluster\nconst client = await Client.newHazelcastClient(); \n // Get or create the 'distributed-map' on the cluster\nconst map = await client.getMap('distributed-map'); \n // Put 'key', 'value' pair into the 'distributed-map'\nawait map.put('key', 'value'); \n // Get the value associated with the given key from the cluster\nconst value = await map.get('key');\nconsole.log(value); // Outputs 'value' \n // Shutdown the client\nawait client.shutdown();\n``` \n \n NOTE: For the sake of brevity we are going to omit boilerplate parts in the above code snippet.\nRefer to  this code sample \nto see the complete code. \n \n If you are using Hazelcast and the Node.js client on the same machine, the default configuration should work\nout-of-the-box. However, you may need to configure the client to connect to cluster nodes that are running on\ndifferent machines or to customize client properties. \n Configuration \n ```js\nconst { Client } = require('hazelcast-client'); \n // Initialize the client with the given configuration\nconst client = await Client.newHazelcastClient({\n    clusterName: 'cluster-name',\n    network: {\n        clusterMembers: [\n            '10.90.0.2:5701',\n            '10.90.0.3:5701'\n        ]\n    },\n    lifecycleListeners: [\n        (state) => {\n            console.log('Lifecycle Event >>> ' + state);\n        }\n    ]\n}); \n console.log('Connected to cluster');\nawait client.shutdown();\n``` \n Refer to  the documentation  to learn more about supported configuration options. \n Features \n \n Distributed, partitioned and queryable in-memory key-value store implementation, called  Map \n Eventually consistent cache implementation to store a subset of the Map data locally in the memory of the client, called  Near Cache \n Additional data structures and simple messaging constructs such as  Set ,  MultiMap ,  Queue ,  Topic \n Cluster-wide unique ID generator, called  FlakeIdGenerator \n Distributed, CRDT based counter, called  PNCounter \n Distributed concurrency primitives from CP Subsystem such as  FencedLock ,  Semaphore ,  AtomicLong \n Integration with  Hazelcast Cloud \n Support for serverless and traditional web service architectures with  Unisocket  and  Smart  operation modes \n Ability to listen client lifecycle, cluster state and distributed data structure events \n and  many more . \n \n Getting Help \n You can use the following channels for your questions and development/usage issues: \n \n GitHub repository \n Complete documentation \n API documentation \n Slack \n Google Groups \n Stack Overflow \n \n Contributing \n We encourage any type of contribution in the form of issue reports or pull requests. \n Issue Reports \n For issue reports, please share the following information with us to quickly resolve the problems. \n \n Hazelcast and the client version that you use \n General information about the environment and the architecture you use like Node.js version, cluster size,\nnumber of clients, Java version, JVM parameters, operating system etc. \n Logs and stack traces, if any. \n Detailed description of the steps to reproduce the issue. \n \n Pull Requests \n Contributions are submitted, reviewed and accepted using the pull requests on GitHub. For an enhancement or larger\nfeature, create a GitHub issue first to discuss. \n Development \n \n Clone the GitHub  repository . \n Run  npm install  to automatically download and install all the required modules. \n Do the work. \n Hazelcast Node.js client developed using TypeScript. Run  npm run compile  to compile TypeScript files to JavaScript. \n To have a consistent code style across the code base, Hazelcast Node.js client uses a style checker.\nRun  npm run lint  and fix the reported issues, if any. \n \n Testing \n In order to test Hazelcast Node.js client locally, you will need the following: \n \n Java 8 or newer \n Maven \n \n Following command starts the tests: \n bash\nnpm test \n Test script automatically downloads  hazelcast-remote-controller  and Hazelcast. The script uses Maven to download those. \n In order to run specific tests, you can give a pattern to the test command like the following: \n bash\nnpm test pattern \n This command will only run the tests matching the pattern. The pattern can be a string or regex in the same form\n grep  command accepts. \n License \n Apache 2.0 License . \n Copyright \n Copyright (c) 2008-2022, Hazelcast, Inc. All Rights Reserved. \n Visit  www.hazelcast.com  for more information.", 'Sample Typescript React project', "Yuce's dotfiles live here", 'Hazelcast Remote Controller \n Hazelcast Cluster lifecycle manager for native client tests. Server container and language clients provide easy management of server side from clients \n \n Create a cluster with a provided xml config \n Start a Member in the configured cluster \n Shutdown a member \n Terminate a member \n Remote script execution on the container \n \n Project uses Apache Thrift to provide multi language support. \n Mail Group \n Please join the mail group if you are interested in using or developing Hazelcast. \n http://groups.google.com/group/hazelcast \n License \n Hazelcast Remote Controller is available under the Apache 2 License.  \n Copyright \n Copyright (c) 2008-2016, Hazelcast, Inc. All Rights Reserved. \n Visit  www.hazelcast.com  for more info.', "Hazelcast Open Binary Client Protocol \n Hazelcast Open Binary Client Protocol  definitions and code generator for multiple programming languages. \n Hazelcast Open Binary Client Protocol Definitions \n The protocol is defined in  protocol-definitions/*.yaml  yaml files where each yaml file represents a service like Map, List, Set etc. \nCustom data types that are used in the protocol definitions are defined in  protocol-definitions/custom/Custom.yaml . \n Service definition \n A service is defined by a separate YAML file, containing all its method definitions. \n yaml\nid: Service Id (0-255)\nname: Service Name\nmethods:\n  - id: METHOD-ID-1 (1-255)\n    name: METHOD-NAME-1\n    ...\n  - id: METHOD-ID-2 (1-255)\n    name: METHOD-NAME-2\n    ... \n A method(aka Remote method call) is defined by a request-response pair and an optional events section. \n A basic method structure example: \n ```yaml\n  - id: METHOD-ID-1 (1-255)\n    name: METHOD-NAME-1\n    since: 2.0\n    doc: |\n       Documentation of the method call\n    request:\n      retryable: false\n      partitionIdentifier: None\n      params:\n        - name: parameter1\n          type: String\n          nullable: false\n          since: 2.0\n          doc: |\n             Documentation of the parameter 1\n        - name: parameter1\n          type: Data\n          nullable: false\n          since: 2.0\n          doc: |\n             Documentation of the parameter 2\n    response:\n      params:\n        - name: response parameter 1\n          type: Data\n          nullable: true\n          since: 2.0\n          doc: |\n             the response parameter 1 \n #Optional events section\nevents: \n  - name: Event-1\n    params:\n      - name: event-param-1\n        type: Data\n        nullable: true\n        since: 2.0\n        doc: |\n          Documentation of the event parameter 1\n      - name: value\n        type: Data\n        nullable: true\n        since: 2.0\n        doc: |\n          Documentation of the event parameter 2\n \n ``` \n Please refer to  schema  for details of a service definition. \n Code Generator \n The new protocol generator generates the related language codecs into the configured folder. It does not depend on Hazelcast repo. \n Setup \n You need to have python3 configured on your  PATH . After cloning the repository, install the python library dependencies: \n bash\npip3 install -r requirements.txt \n Code Generation \n You can generate codecs for a specific language by calling, \n ```bash \n ./generator.py [-r ROOT_DIRECTORY] [-l LANGUAGE] [-p PROTOCOL_DEFS_PATH] [-o OUTPUT_DIRECTORY] [-n NAMESPACE] [-b BINARY_OUTPUT_DIR] [-t TEST_OUTPUT_DIR] [--no-binary] [--no-id-check] \n ``` \n where  \n \n \n ROOT_DIRECTORY  is the root folder for the generated codecs. If left empty, default value is set to  ./output/[LANGUAGE] . \n \n \n LANGUAGE  is one of  \n \n java  : Java \n cpp  : C++ \n cs  : C# \n py  : Python \n ts  : TypeScript \n go  : Go \n md  : Markdown (Documentation) \n \n \n \n java  is the default value if no language is specified. \n \n \n PROTOCOL_DEFS_PATH  is the directory containing the  yaml  definitions of the protocol. If left empty, \nthis value is defaulted to the  ./protocol-definitions . If the protocol definitions on the custom directory use\nsome custom types, a YAML file named  Custom.yaml  must be put inside the  PROTOCOL_DEFS_PATH/custom  directory. \nFor the details of the custom type definition, see the  Custom Types  section. \n \n \n OUTPUT_DIRECTORY  is the output directory for the generated codecs relative to the  ROOT_DIRECTORY . If left empty,\nthis is inferred from the selected  LANGUAGE . \nDefault values are chosen according to the directories used by the Hazelcast clients. \n \n \n NAMESPACE  is the namespace for the generated codecs. If left empty, default value is inferred from the selected  LANGUAGE .  \n \n \n BINARY_OUTPUT_DIR  is the output directory relative to the  ROOT_DIRECTORY  that is used for the binary files for the binary compatibility tests.\nWhen left empty, default value is inferred from the selected  LANGUAGE . \n \n \n TEST_OUTPUT_DIR  is the output directory relative to the  ROOT_DIRECTORY  that is used for the test files for the binary compatibility tests.\nDefault value is inferred from the selected  LANGUAGE . \n \n \n --no-binary  flag restrains the generator from creating binary and test files for the binary compatibility tests. \n \n \n --no-id-check  flag restrains the generator from checking sequentiality of service and method ids of protocol definitions. \n \n \n If you want to generate the Java codecs into your development repo, and let's assume your local Hazelcast git repo is at \n ~/git/hazelcast/  then you can run the following command: \n bash\n./generator.py -r ~/git/hazelcast/ \n This command generates the codecs at the  ROOT_DIRECTORY/OUTPUT_DIRECTORY  which is  ~/git/hazelcast/hazelcast/src/main/java/com/hazelcast/client/impl/protocol/codec/ .\nSee that the  OUTPUT_DIRECTORY  is inferred from the language, namely  hazelcast/src/main/java/com/hazelcast/client/impl/protocol/codec/  for  java .  \n If you want to specify an output directory relative to the root directory, you can run the following command: \n bash\n./generator.py -r ~/git/hazelcast/ -o custom/out \n This command will generate the codecs at the  ~/git/hazelcast/custom/out . \n Schema Validation \n The protocol definitions should validate against the  schema . You can configure your IDE to \nuse this schema to validate and provide auto code completion. \n The generator also uses this schema during the code generation for validation purposes. It stops and reports any schema violation to the console. \n Custom Types \n If you are going to use a custom type,i.e., a complex type that is not defined in the  currently supported types , \nas the type of your parameters in the protocol definitions, you need to define how to encode and decode this in the protocol level. \n A custom type definition has the following structure: \n yaml\ncustomTypes:\n    - name: CustomType1\n      since: 2.0\n      returnWithFactory: true # optional\n      params:\n        - name: paramName1\n          type: boolean\n          nullable: false\n          since: 2.0\n        - name: paramName2\n          type: String\n          nullable: true\n          since: 2.0 \n With this definition, the code generator generates a custom codec for your type and \ncalls its encode/decode methods when encoding/decoding the parameters with this custom type. \nThere are a few points to consider as described below.  \n The codec for the custom type accesses the parameters defined in  params  using a \npredefined getter pattern in its encode method. These patterns are specific to each  LANGUAGE . \n For example, for the  java , if the parameter is a  boolean  it is accessed as  customType1.isParamName1() .\nFor other types  customType1.getParamName2()  pattern is used. So, make sure that your custom type satisfies \nthis getter contract. \n For the decode method of the custom type codec, there are two ways to generate\nan instance of the custom type. Default way is constructing the object using a constructor\nwith parameters defined in the  params  in the order of their definition. For example, by default\nthe instance of  CustomType1  is created with the  new CustomType1(paramName1, paramName2)  expression. \n If the custom type does not have a public constructor that takes the defined parameters in the order\nof their definition, then you need to write a factory method to generate the object from these parameters.\nTo use a factory method as a way to create the custom type, you should set the  returnWithFactory  option to  true . \n Then, depending on the selected  LANGUAGE , a custom factory method is called to create the object. \n For example, for  java ,  CustomTypeFactory.createCustomType1(paramName1, paramName2)  method is called.\nYou need to add the  CustomType1 createCustomType1(boolean paramName1, String paramName2)  method to the  CustomTypeFactory  class on the Hazelcast side. \n For the parameters of the custom type definition, an extra step is required for the enum types. \nEnums are represented as integers in the protocol level. So, you need to specify the type as  int  in the protocol\ndefinition and add an  encodeInt  method to the  FixedSizeTypesCodec  for the enum type that performs the conversion\nif the enums are not represented by integers in the language you try to generate codecs for. \nAlso, you need to set  returnWithFactory  to  true  and add a factory method as described above if the conversion from \nenum type to int is required. In the factory method, you will receive an integer for the enum and be expected to \nconvert it to your enum type and construct the object with it. \n Custom type definitions are also validated against a  schema . See the  Schema Validation  \nsection for details of the validation. \n Expanding the Client Protocol \n Client protocol can be expanded by adding new\n* services\n* methods\n* parameters to existing method requests, responses or events\n* events to existing methods\n* custom types\n* parameters to existing custom types \n While expanding the protocol, one needs to follow these simple guidelines:\n*  since  field of the protocol definitions of the the newly added parameters, methods, events and custom types should \nbe equal to the current protocol version. \n* New services should have the id of the 1 + the highest id of the existing services.\n* New methods should come after the existing methods on the protocol definitions and have the id of the 1 + the id \nof the method that comes before it.\n* New request, response or event parameters should come after the existing parameters on the protocol definitions \nand they should be in the increasing order of the protocol versions that is 2.1 parameters should follow \n2.0.1 parameters which should follow 2.0 parameters.\n* New parameters to custom types should come after the existing parameters on the protocol definitions and they should\nbe in the increasing order of protocol versions as described above.\n* Although not necessary, new events or custom types should come after the existing custom types or events on the \nprotocol definitions.", 'bert \n An updated Erlang BERT encode/decoder, based on Tom Preston-Werner\'s  bert.erl . This library uses maps instead of dicts, so requires Erlang/OTP R17 and higher. \n See  BERT RPC specification  for the specs. \n Thanks to  contributors . \n Requirements \n \n Erlang R17+ \n rebar3 \n \n Include in Your Project \n Add  {deps, [bert]}.  to  rebar.config \n Build \n $ rebar3 compile \n Test \n Run EUnit tests \n $ rebar3 eunit \n Run Property-based tests \n $ rebar3 as test proper \n Usage \n Encode a term: \n Term = #{key => [{some, "tuple"}, {true, 42}]},\n    bert:encode(Term).\n    > <<131,104,3,100,0,4,98,101,114,116,100,0,4,100,105,99,116,\n      108,0,0,0,1,104,2,100,0,3,107,101,...>> \n Decode a binary: \n Bin = <<131,104,3,100,0,4,98,101,114,116,100,0,4,100,105,99,116,108,0,0,0,1,104,2,\n            100,0,3,107,101,121,108,0,0,0,2,104,2,100,0,4,115,111,109,101,107,0,5,116,\n            117,112,108,101,104,2,104,2,100,0,4,98,101,114,116,100,0,4,116,114,117,101,\n            97,42,106,106>>,\n    bert:decode(Bin).\n    > #{key => [{some, "tuple"}, {true, 42}]} \n Documentation generation \n Edoc \n Generate public API \n $ rebar3 edoc \n Generate private API \n $ rebar3 as edoc_private edoc \n ExDoc \n $ rebar3 ex_doc --output edoc \n License \n Copyright (c) 2016-2022, Yuce Tekol <yucetekol@gmail.com>.\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n* Redistributions of source code must retain the above copyright\nnotice, this list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright\nnotice, this list of conditions and the following disclaimer in the\ndocumentation and/or other materials provided with the distribution.\n\n* The names of its contributors may not be used to endorse or promote\nproducts derived from this software without specific prior written\npermission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n', 'monkey \n Experimental Pooling TCP Server Library \n Build \n $ rebar3 compile\n \n Examples \n See sample apps in the  examples  directory.', 'nats_msg \n nats_msg  is a pure Erlang NATS message encoder/decoder library for\n NATS  high performance messaging platform.\nFor details about NATS protocol, see:\n NATS Protocol . It doesn\'t\nhave any dependency other than Erlang/OTP (16+  should  be OK) and optionally\n rebar3 . \n News \n \n Version 0.4.1  ( 2016-03-23 ): \n This version is incompatible with previous versions of the library. \n Decoding is lazy, in the sense that, only a single message is decoded whenever\nthe decode is called. You can use  nats_msg:decode_all/1  to simulate the old behaviour. \n HUGE performance improvements (2x - 100x). \n The parser is much more stricter now. \n Encoding functions return iodata instead of binary. \n You can use iodata anywhere a binary is expected. \n \n \n \n Install \n nats_msg  uses  rebar3  to build and tests and\nit is available on  hex.pm . Just include the following\nin your  rebar.config : \n erlang\n{deps, [nats_msg]}. \n Tests \n Run the tests using: \n $ rebar3 eunit\n \n Build \n $ rebar3 compile\n \n Usage \n \n IMPORTANT! \n Before running any decoding functions,  nats_msg:init/0  must be called once. \n \n Binaries and iodata are used exclusively throughout the library. \n Currently, no error handling is performed during encoding/decoding. You can protect\nagainst crashes by wrapping library functions between  try...catch . \n INFO  and  CONNECT  messages have a JSON object as their parameter; but in order to\nnot introduce a dependency,  nats_msg  does not encode/decode JSON objects. These parameters\nare kept or returned as binaries. You can use  jsx  or  jiffy \nto deal with JSON. See the  INFO  and  CONNECT  sections in this document for examples. \n Encoding \n Encoding a message produces an IO list.  nats_msg:encode/1  takes an atom as the name of the\nmessage or a tuple which contains the name, parameters and payload of the message. \n The general form of  nats_msg:encode/1  parameters is: \n \n Name :: atom() : For messages taking no parameters, \n {Name :: atom(), Parameters :: {iodata() | int(), ...}}  for messages taking parameters but\nnot a payload, \n {Name :: atom(), Parameters :: {iodata() | int(), ...}, Payload :: iodata()}  for messages\ntaking parameters and a payload. \n \n Messages of the same type always have the same structure, even if some of the values are\n undefined . Some examples: \n \n nats_msg:encode(ping)  produces a  PING  message, \n nats_msg:encode({sub, {<<"INBOX">>, undefined, <<"2">>}}  produces a  SUB  message with\nsubject  <<"INBOX">>  and SID  <<"2">> . This particular message has no  queue group , so\nthat field is set to  undefined . \n nats_msg:encode({pub, {<<"FOO">>, undefined, 11}, <<"Hello NATS!">>}  produces a  PUB  message\nwith subject  <<"FOO">>  and payload  <<"Hello NATS!">>  of size  11  and no  reply to  subject. \n \n The library has convenience functions for all messages, like  nats_msg:ping/0 , which are\ndiscussed later in this document. \n Decoding \n Decoding a binary/iodata produces a  {Message, RemainingBinary}  tuple.\n Message  is either  []  if the data is not sufficient to decode the message, or the a term for the message,\nand  RemainingBinary  is the part of the input which\nwasn\'t decoded and returned. The latter is very useful when dealing with streams, where\nthe input is chunked and appending chunks is required to be able to decode messages.\nIn those situations, just prepend  RemainingBinary  to the next binary chunk before attempting\nto decode it. \n The  Message   be used as an input to  nats_msg:encode , like: \n erlang\nSomeBinary = ...\n{Msg, Remaining} = nats_msg:decode(SomeBinary),\nReEncodedBinary = nats_msg:encode(Msg),\n% ReEncodedBinary = SomeBinary \n INFO Message \n NATS Spec \n Encode \n erlang\nServerInfo = #{<<"auth_required">> => true, <<"server_id">> => <<"0001-SERVER">>},\nBinaryInfo = jsx:encode(ServerInfo),\nBinaryMsg = nats_msg:info(BinaryInfo). \n Decode \n erlang\nChunk = <<"INFO {\\"auth_required\\":true,\\"server_id\\":\\"0001-SERVER\\"}\\r\\n">>,\n{Msg, _} = nats_msg:decode(Chunk),\n{info, BinaryInfo} = Msg,\nServerInfo = jsx:decode(BinaryInfo, [return_maps]). \n CONNECT Message \n NATS Spec \n Encode \n erlang\nConnectInfo = #{<<"auth_required">> => true, <<"server_id">> => <<"0001-SERVER">>},\nBinaryInfo = jsx:encode(ServerInfo),\nBinaryMsg = nats_msg:connect(BinaryInfo). \n Decode \n erlang\nChunk = <<"CONNECT {\\"verbose\\":true,\\"name\\":\\"the_client\\"}\\r\\n">>,\n{Msg, _} = nats_msg:decode(Chunk),\n{connect, BinaryInfo} = Msg,\nClientInfo = jsx:decode(BinaryInfo, [return_maps]). \n PUB Message \n NATS Spec \n Encode \n Notify subscribers of a subject: \n erlang\nBinaryMsg = nats_msg:pub(<<"NOTIFY.INBOX">>). \n Send some data ( payload ) to subscribers, providing a  reply  subject: \n erlang\nBinaryMsg = nats_msg:pub(<<"FOOBAR">>, <<"REPRAP">>, <<"Hello, World!">>). \n Send some data ( payload ) to subscribers ( without a reply subject ): \n erlang\nBinaryMsg = nats_msg:pub(<<"FOOBAR">>, undefined, <<"Hello, World!">>). \n Decode \n Publish notification: \n erlang\nChunk = <<"PUB NOTIFY 0\\r\\n\\r\\n">>,\n{Msg, _} = nats_msg:decode(Chunk),\n{pub, {Subject, ReplyTo, Payload}} = Msg,\n% Subject = <<"NOTIFY">>,\n% ReplyTo = undefined,\n% Payload = <<>>. \n Publish message with subject, replier and payload: \n erlang\nChunk = <<"PUB FRONT.DOOR INBOX.22 11\\r\\nKnock Knock\\r\\n">>,\n{Msg, _} = nats_msg:decode(Chunk),\n{pub, {Subject, ReplyTo, Payload}} = Msg,\n% Subject = <<"FRONT.DOOR">>,\n% ReplyTo = <<"INBOX.22">>,\n% Payload = <<"Knock Knock">>. \n SUB Message \n NATS Spec \n Encode \n Subscribe message with subject and SID: \n erlang\nBinaryMsg = nats_msg:sub(<<"FOO">>, <<"1">>). \n Subscribe message with subject, group queue and SID: \n erlang\nBinaryMsg = nats_msg:sub(<<"BAR">>, <<"G1">>, <<"44">>) \n Decode \n erlang\nChunk = <<"SUB FOO 1\\r\\n">>,\n{Msg, _} = nats_msg:decode(Chunk),\n{sub, {Subject, GroupQueue, Sid}} = Msg,\n% Subject = <<"FOO">>,\n% GroupQueue = undefined,\n% Sid = <<"1">>. \n UNSUB Message \n NATS Spec \n Encode \n Unsubscribe message with SID: \n erlang\nBinaryMsg = nats_msg:unsub(<<"1">>). \n Unsubscribe message with SID and  max messages : \n erlang\nBinaryMsg = nats_msg:unsub(<<"1">>, 10). \n Decode \n erlang\nChunk = <<"UNSUB 1 10\\r\\n">>,\n{Msg, _} = nats_msg:decode(Chunk),\n{unsub, {Sid, MaxMessages}} = Msg,\n% Sid = <<"1">>,\n% MaxMessages = 10 \n MSG Message \n NATS Spec \n Encode \n Message with subject and SID: \n erlang\nBinaryMsg = nats_msg:msg(<<"FOO">>, <<"5">>). \n Message with subject, sid,  reply to subject  and payload: \n erlang\nBinaryMsg = nats_msg:msg(<<"FOO">>, <<"5">>, <<"INBOX">>, <<"Hello!">>). \n Message with subject, sid and payload: \n erlang\nBinaryMsg = nats_msg:msg(<<"FOO">>, <<"5">>, undefined, <<"Hello!">>). \n Decode \n Message with subject, sid and payload: \n erlang\nChunk = <<"MSG FOO.BAR 9 13\\r\\nHello, World!\\r\\n">>,\n{Msg, _} = nats_msg:decode(Chunk),\n{msg, {Subject, Sid, ReplyTo, Payload}} = Msg,\n% Subject = <<"FOO.BAR">>,\n% Sid = <<"9">>,\n% ReplyTo = undefined,\n% Payload = <<"Hello, World!">>. \n PING Message \n NATS Spec \n Encode \n erlang\nBinaryMsg = nats_msg:ping(). \n Decode \n erlang\n{Msg, _} = nats_msg:decode(<<"PING\\r\\n">>),\n% Msg = ping \n PONG Message \n NATS Spec \n Encode \n erlang\nBinaryMsg = nats_msg:pong(). \n Decode \n erlang\n{Msg, _} = nats_msg:decode(<<"PONG\\r\\n">>),\n% Msg = pong \n +OK Message \n NATS Spec \n Encode \n erlang\nBinaryMsg = nats_msg:ok(). \n Decode \n erlang\n{Msg, _} = nats_msg:decode(<<"+OK\\r\\n">>),\n% Msg = ok \n -ERR Message \n NATS Spec \n The spec defines a predefined set of error messages, so  nats_msg  encodes/decodes these\nto/from atoms as: \n \n \'Unknown Protocol Operation\'  =>  unknown_operation \n \'Authorization Violation\'  =>  auth_violation \n \'Authorization Timeout\'  =>  auth_timeout \n \'Parser Error\'  =>  parser_error \n \'Stale Connection\'  =>  stale_connection \n \'Slow Consumer\'  =>  slow_consumer \n \'Maximum Payload Exceeded\'  =>  max_payload \n \'Invalid Subject\'  =>  invalid_subject \n Other errors are converted to  unknown_error  during decoding and kept as is during encoding. \n \n Encode \n erlang\nBinaryMsg = nats_msg:err(auth_violation). \n Decode \n erlang\nChunk = <<"-ERR \'Authorization Timeout\'\\r\\n">>,\n{Msg, _} = nats_msg:decode(Chunk),\n{ok, Error} = Msg,\n% Error = auth_timeout \n License \n ```\nCopyright (c) 2016, Yuce Tekol  yucetekol@gmail.com .\nAll rights reserved. \n Redistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet: \n \n \n Redistributions of source code must retain the above copyright\n  notice, this list of conditions and the following disclaimer. \n \n \n Redistributions in binary form must reproduce the above copyright\n  notice, this list of conditions and the following disclaimer in the\n  documentation and/or other materials provided with the distribution. \n \n \n The names of its contributors may not be used to endorse or promote\n  products derived from this software without specific prior written\n  permission. \n \n \n THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n```', 'Erlang/OTP Support for Visual Studio Code \n This extension provides Erlang/OTP support for  Visual Studio Code  and is available at the  Marketplace . \n News \n \n 0.2.5 ( 2018-02-12 ): \n Updated auto-indent. \n Updated dependencies. \n \n \n 0.2.4 ( 2017-06-23 ): \n Fixed receive ... after auto-indent and snippet. \n \n \n 0.2.3 ( 2017-06-21 ): \n Fixed  erlang.autoIndent  preference item. \n \n \n 0.2.2 ( 2017-06-18 ): \n Improved auto-indent. This feature is enabled by default; in order to disable it, set  erlang.autoIndent  to  false . \n \n \n 2016-02-17: \n Experimental module name auto-completion ( currently Erlang standard library modules only ) \n \n \n 2016-02-16: \n Added experimental support for auto-completion of Erlang standard library module functions. Enable it with setting  erlang.enableExperimentalAutoComplete  to  true  in your user settings and restart VSCode. \n \n \n \n Features \n \n Syntax highlighting \n Auto-indent \n Snippets \n Auto-complete ( experimental ) \n \n Work In Progress \n This extension is still WIP, feel free to submit ideas/bug fixes\non  Github . \n Snippets \n \n rec : receive block \n reca : receive block with after \n case : case block \n if : if block \n try : try .. catch block \n \n You can submit more snippets on  Github . \n Thanks \n \n Erlang syntax file is based on: https://github.com/pgourlain/vscode_erlang. \n', 'whatels \n Experimental Erlang language service. Requires Erlang/OTP 17+. Dependencies: \n \n rebar3 \n jsx \n monkey \n erwatch \n \n See  Erlang VSCode NEXT \nfor a  Visual Studio Code  extension and\n whatels node \nfor a  NodeJS  client. \n Build \n $ rebar3 compile\n \n Run \n $ rebar3 as prod release\n$ _build/prod/rel/whatels/bin/whatels foreground\n \n Messages \n General message format \n <MESSAGE NAME> <PAYLOAD SIZE>\\r\\n\n<PAYLOAD>\\r\\n\n \n License \n ```\nCopyright (c) 2016, Yuce Tekol  yucetekol@gmail.com .\nAll rights reserved. \n Redistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet: \n \n \n Redistributions of source code must retain the above copyright\n  notice, this list of conditions and the following disclaimer. \n \n \n Redistributions in binary form must reproduce the above copyright\n  notice, this list of conditions and the following disclaimer in the\n  documentation and/or other materials provided with the distribution. \n \n \n The names of its contributors may not be used to endorse or promote\n  products derived from this software without specific prior written\n  permission. \n \n \n THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n```', '', 'Erwatch \n Erwatch  is an Erlang/OTP application for tracking changes in\na file system. It can be used with Erlang/OTP (17+  should  be OK) and\noptionally  rebar3 .\nIt is only tested on Linux/OSX, but only Erlang/OTP standard library functions\nare used, so it should work on any platform where Erlang/OTP runs. \n News \n \n 2016-03-08 : Major change in  Erwatch  API. \n Removed  erwatch:add_wildcard/2  function. You have to specify the wildcards to  erwatch:new/1  or  erwatch:new/2  instead. \n The initial changesets are not returned. Use  filelib:wildcard/1  or  filelib:wildcard/2  instead. \n \n \n \n Install \n Erwatch  uses  rebar3  to build and tests and\nit is available on  hex.pm . Just include the following\nin your  rebar.config : \n erlang\n{deps, [erwatch]}. \n Build \n $ rebar3 compile\n \n Usage \n \n Warning!\nErwatch ( currently ) uses polling to determine file system changes.\nSetting the poll interval too frequent (lower) is not recommended\nfor wildcards which may return a large number of paths. \n \n Since  Erwatch  is an OTP application, it must be started before\nusing. You can do that by including  erwatch  in your  *.app.src  or  *.app  file, like: \n erlang\n...\n  {applications,\n   [kernel,\n    stdlib,\n    erwatch\n   ]},\n... \n Or, start it manually: \n erlang\nok = application:start(erwatch). \n rebar3  has a nice way of starting apps in the shell, you can try: \n $ rebar3 shell --apps rewatch\n \n Erwatch  supports both the synchronous/on demand and asynchronous/message based way\nof tracking changes. \n Use  erwatch:new/1  to create a watch with default options, or  erwatch:new/2 \nto create a watch with specified options. The list of wildcards is mandatory for both: \n erlang\n% on demand watch:\n{ok, Watch} = erwatch:new(["/tmp/somedir/**/src/*.erl"]),\n% a watch that fires every 3000 milliseconds, and sends a message on changes\n{ok, AnotherWatch} = erwatch:new(["/tmp/somedir/**/src/*.erl"],\n                                 [{interval, 3000}]). \n See  filelib:wildcard/1 \ndocumentation for available wildcard patterns. Note that, the given wildcard is\nrelative to the current working directory. \n Whether you use  synchronous  or  asynchronous  watches, the change sets are\nin the form of list of  {Action, Path}  pairs, where  Action  is one of\n added ,  updated  or  deleted . \n You can change the poll interval and switch between synchronous\nand asynchronous modes using  erwatch:set_interval/2 . An interval of\n 0  will make the watch synchronous and  > 0  will make it asynchronous. \n erlang\nerwatch:set_interval(1000, Watch). \n It is possible to pause an asynchronous watch using  erwatch:pause/1  and\nresume it using  erwatch:resume/1 . \n erlang\nerwatch:pause(Watch),\n% later...\nerwatch:resume(Watch). \nWhen the time comes, you can kill a watch with  erwatch:remove/1 : \n erlang\nerwatch:remove(Watch). \n Synchronous / On Demand Usage \n Create a watch: \n erlang\n{ok, Watch} = erwatch:new(["/tmp/foo1/*", "/tmp/bar2"]). \n Assuming  /tmp/foo1  is a directory and  /tmp/bar2  is a file,\ncreate, modify, delete directories, files in  /tmp/foo1  and/or\ncreate, modify, delete  /tmp/bar2 . \n Retrieve changes: \n erlang\nerwatch:get_changes(Watch). \n Returns e.g.: \n erlang\n[{added,"/tmp/foo1/myfile"},\n {updated,"/tmp/foo1/yourfile"},\n {updated,"/tmp/foo1/x"},\n {deleted,"/tmp/bar2"}] \n Asynchronous / Message Based Usage \n In this mode,  Erwatch  will send  erwatch@changes  messages to the\nparent process ( currently the process which created the watch ) on\nfile system changes. \n Create a watch with an interval: \n erlang\n{ok, Watch} = erwatch:new(["/tmp/foo1/*", "/tmp/bar2"],\n                          [{interval, 1000}]). \n Assuming  /tmp/foo1  is a directory and  /tmp/bar2  is a file,\ncreate, modify, delete directories, files in  /tmp/foo1  and/or\ncreate, modify, delete  /tmp/bar2 . \n Receive changes: \n erlang\nloop() ->\n    receive\n        {erwatch@changes, Watch, ChangeSet} ->\n            % do something with `ChangeSet`\n            loop()\n        _ ->\n            % received some other message...\n            loop()\n    end. \nThe  ChangeSet  value might be: \n erlang\n[{added,"/tmp/foo1/myfile"},\n {updated,"/tmp/foo1/yourfile"},\n {updated,"/tmp/foo1/x"},\n {deleted,"/tmp/bar2"}] \n Examples \n See  examples/watch.escript \nfor a simple file watcher. \n License \n ```\nCopyright (c) 2016, Yuce Tekol  yucetekol@gmail.com .\nAll rights reserved. \n Redistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet: \n \n \n Redistributions of source code must retain the above copyright\n  notice, this list of conditions and the following disclaimer. \n \n \n Redistributions in binary form must reproduce the above copyright\n  notice, this list of conditions and the following disclaimer in the\n  documentation and/or other materials provided with the distribution. \n \n \n The names of its contributors may not be used to endorse or promote\n  products derived from this software without specific prior written\n  permission. \n \n \n THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n```', 'Experimental Erlang/OTP Support for Visual Studio Code \n This experimental extension provides Erlang/OTP support for  Visual Studio Code . \n A  Whatels  server is required to be running for auto completion to work. See\n VSCode Erlang  for an extension without\nthe Whatels dependency (albeit without support for project modules/current module auto completion). \n Getting Started \n You will need Erlang 18+, a recent version of NodeJS (4.3+) and TypeScript (17.5+) installed.\nThe instructions were tested on Linux and OSX (the extension itself should work on Windows too,\nbut that\'s not tested). \n \n Clone this repository to  ~/,vscode/extensions , \n Run  npm install  to retrive dependencies, \n Run  tsc  to compile the extension, \n Run Whatels server (see the next section), \n Set  "erlang.enableExperimentalAutoComplete": true  in your VSCode settings, \n Run/Restart VSCode. \n \n Running the Whatels Server \n See the instructions and get Whatels at: https://github.com/yuce/whatels \n $ rebar3 as prod release\n$ WHATELS_PORT=10998 _build/prod/rel/whatels/bin/whatels foreground\n \n Thanks \n \n Erlang syntax file is based on: https://github.com/pgourlain/vscode_erlang. \n \n License \n ```\nCopyright (c) 2016, Yuce Tekol  yucetekol@gmail.com .\nAll rights reserved. \n Redistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet: \n \n \n Redistributions of source code must retain the above copyright\n  notice, this list of conditions and the following disclaimer. \n \n \n Redistributions in binary form must reproduce the above copyright\n  notice, this list of conditions and the following disclaimer in the\n  documentation and/or other materials provided with the distribution. \n \n \n The names of its contributors may not be used to endorse or promote\n  products derived from this software without specific prior written\n  permission. \n \n \n THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n```', 'State of Erlang Editor Support Survey March 2016 \n \n \n \n \n \n \n Editor Satisfaction \n \n Erlang Experience of Editor Users \n \n OS Choice of Editor Users \n', "Editor Configuration Tips and Tricks for Erlang \n Emacs \n \n Sean Cribbs' Emacs 24 Configuration: http://seancribbs.com/emacs.d/ \n", 'simpre \n Simple Process Registry. \n Build \n $ rebar3 compile\n', 'yes_msg \n Yet another simple message (YES) parser for Erlang. \n Build \n $ rebar3 compile\n \n Message Format \n Messages consist of \n \n Name: Consist of an arbitrary number of characters.\nFollowing characters cannot be used in a name:  ; ,  \\r ,  \\n . \n (Optional) payload size and payload. \n \n yes_msg  supports two kinds of messages: \n \n Atoms consist of only a message name \n Messages with payloads \n \n Atom message \n [Message Name]\\r\\n \n Message with payload \n [Message Name];[Payload length]\\r\\n[Payload]\\r\\n \n Usage \n yes_msg  is available on hex.pm. If you use rebar3, just add the\nfollowing in your  rebar.config : \n {deps, [yes_msg]}.\n \n Encode an atom message: \n erlang\nMsgName = <<"OK">>,\n{ok, Msg} = yes_msg:encode(MsgName).\n% Msg = <<"OK\\r\\n">> \n Encode a message with payload: \n erlang\nMsgName = <<"list">>,\nMsgPayload = <<"/tmp/yes">>,\n{ok, Msg} = yes_msg:encode(MsgName, MsgPayload).\n% Msg = <<"list;8\\r\\n/tmp/yes\\r\\n">> \n yes_msg:encode/1  and  yes_msg:encode/2  return  {ok, EncodedMessage}  on\nsuccess and  {error, encode_error}  on encoding failure. \n Decode a binary: \n erlang\nBinary = <<"list;8\\r\\n/tmp/yes\\r\\n">>,\n{ok, Messages, Remaining} = yes_msg:decode(Binary).\n% Nessages = [{<<"list">>, <<"/tmp/yes">>}]\n% Remaining = <<>> \n yes_msg:decode/1  return  {ok, MessagesList, RemainingBinary}  on success\nand  {error, decode_error}  on decoding failure. \n License \n ```\nCopyright (c) 2016, Yuce Tekol  yucetekol@gmail.com .\nAll rights reserved. \n Redistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet: \n \n \n Redistributions of source code must retain the above copyright\n  notice, this list of conditions and the following disclaimer. \n \n \n Redistributions in binary form must reproduce the above copyright\n  notice, this list of conditions and the following disclaimer in the\n  documentation and/or other materials provided with the distribution. \n \n \n The names of its contributors may not be used to endorse or promote\n  products derived from this software without specific prior written\n  permission. \n \n \n THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n```', 'yes_msg \n YES message parser for NodeJS. See: https://github.com/yuce/yes_msg-erlang', 'teacup \n Simple TCP client library for Erlang. \n Build \n $ rebar3 compile\n \n License \n ```\nCopyright (c) 2016, Yuce Tekol  yucetekol@gmail.com .\nAll rights reserved. \n Redistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet: \n \n \n Redistributions of source code must retain the above copyright\n  notice, this list of conditions and the following disclaimer. \n \n \n Redistributions in binary form must reproduce the above copyright\n  notice, this list of conditions and the following disclaimer in the\n  documentation and/or other materials provided with the distribution. \n \n \n The names of its contributors may not be used to endorse or promote\n  products derived from this software without specific prior written\n  permission. \n \n \n THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n```', 'teacup_http \n HTTP handler for teacup. \n Build \n $ rebar3 compile\n \n License \n ```\nCopyright (c) 2016, Yuce Tekol  yucetekol@gmail.com .\nAll rights reserved. \n Redistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet: \n \n \n Redistributions of source code must retain the above copyright\n  notice, this list of conditions and the following disclaimer. \n \n \n Redistributions in binary form must reproduce the above copyright\n  notice, this list of conditions and the following disclaimer in the\n  documentation and/or other materials provided with the distribution. \n \n \n The names of its contributors may not be used to endorse or promote\n  products derived from this software without specific prior written\n  permission. \n \n \n THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n```', 'Teacup NATS \n A  Teacup  based Erlang client library for  NATS \nhigh performance messaging platform. \n NEWS \n \n \n 2016-04-17 : Version 0.4.0: \n \n Renamed  tcnats  module to  nats . \n Added  nats:is_ready/1  function to check whether a NATS connection\nis ready (to pub, sub, unsub). \n Added initial tests. \n \n \n \n 2016-04-16 : Version 0.3.7: \n \n Re-licenced the project under  Apache 2.0 License .     \n This version fixes performance problems introduced in version 0.3.3. \n \n \n \n 2016-04-07 : Version 0.3.4: \n \n Rudimentary TLS/SSL support. Currently, this is auto-activated when the server\nsends  tls_required => true  in an  INFO  message.  \n \n \n \n 2016-04-03 : Version 0.3.3: \n \n Using  teacup 0.3.3 ,\nwhich boosts the performance by 50%. \n Implemented connect retry / reconnect strategy. \n Implemented message buffering. \n Sub and Unsub messages are queued. \n \n \n \n 2016-03-27 : You can check how the performance of  teacup_nats  compares to other NATS clients\n here . \n \n \n 2016-03-19 : Initial release. \n \n \n Getting Started \n teacup_nats  requires Erlang/OTP 18.0+. It uses  rebar3 \nas the build tool and is available on  hex.pm . Just include the following\nin your  rebar.config : \n erlang\n{deps, [teacup_nats]}. \n If you are upgrading from an earlier version, you need to: \n $ rebar3 update && rebar3 upgrade \n teacup_nats  depends on the  teacup  app to be started. Include it in your  .app.src  file: \n erlang\n...\n  {applications,\n   [kernel,\n    stdlib,\n    teacup\n   ]},\n... \n Or, start it manually: \n erlang\nok = application:start(teacup). \n rebar3  has a nice way of starting apps in the shell, you can try: \n $ rebar3 shell --apps teacup \n Running the Tests \n $ rebar3 ct \n TODO \n \n Clustering \n \n API \n Aysnchronous Connection \n When using asycnhronous connections, you need to wait for a  {Conn, ready} \nmessage before publishing messages, subcribing to/unsubscribing from subjects. \n \n Connection functions: \n nats:connect() : Connect to the NATS server at address  127.0.0.1 , port  4222 , \n nats:connect(Host :: binary(), Port :: integer()) : Connect to the NATS server\nat  Host  and port  PORT , \n nats:connect(Host :: binary(), Port :: integer(), Opts :: map()) : Similar to\nabove, but also takes an  Opts  map. Currently usable keys: \n verbose => true | false : If  verbose == true , NATS server\nsends an acknowledgement message on  pub ,  sub ,  unsub  operations and\n connect  operation becomes synchronous. \n user => User :: binary() , \n pass => Password :: binary() , \n buffer_size => MessageBufferSize :: non_neg_integer() : The number of publish messages\nto buffer before quitting. The default is 0. Setting  MesssageBufferSize  to\n infinity  enables unlimited buffering. \n reconnect => {Interval :: non_neg_integer(), MaxRetry :: non_neg_integer()} : Specifies\nreconnect strategy.  Interval  is the time in milliseconds between retrials, and  MaxRetry  is\nthe number of retrials before quitting. You can set  MaxRetry  to  infinity  to try reconnecting\nforever. The default is  {undefined, 0} , "don\'t try to reconnect". \n \n \n \n \n Publish functions: \n nats:pub(Conn :: teacup_ref(), Subject :: binary()) : Publish message with only\nthe subject, \n nats:pub(Conn :: teacup_ref(), Subject :: binary()), Opts :: map() : Publish message\nthe subject with  Options . Valid options: \n payload => Payload :: binary() , \n reply_to => Subject :: binary() \n \n \n \n \n Subscribe functions: \n nats:sub(Conn :: teacup_ref(), Subject :: binary()) : Subscribe to the  Subject , \n nats:sub(Conn :: teacup_ref(), Subject :: binary(), Opts :: map()) : Subscribe to the  Subject , with\n Options . Valid options: \n queue_group => QGroup :: binary() \n \n \n \n \n Unsubscribe functions: \n nats:unsub(Conn :: teacup_ref(), Subject :: binary()) : Unsubscribe from  Subject , \n nats:unsub(Conn :: teacup_ref(), Subject :: binary(), Opts :: map()) : Unsubscribe from  Subject , with\n Options . Valid options: \n max_messages => MaxMessages :: integer() : Automatically unsubscribe after receiving  MaxMessages . \n \n \n \n \n \n Sample \n ```erlang\nmain() ->\n    % Connect to the NATS server\n    {ok, Conn} = nats:connect(<<"demo.nats.io">>, 4222, #{buffer_size => 10}),\n    % We set the buffer_size, so messages will be collected on the client side\n    %   until the connection is OK to use \n    % Publish some message\n    nats:pub(Conn, <<"teacup.control">>, #{payload => <<"start">>}),\n    % subscribe to some subject\n    nats:sub(Conn, <<"foo.*">>),\n    loop(Conn). \n loop(Conn) ->\n    receive\n        {Conn, {msg, Subject, _ReplyTo, Payload}} ->\n            % Do something with the received message\n            io:format("~p: ~p~n", [Subject, Payload]),\n            % Wait for/retrieve the next message\n            loop(Conn)\n    end.\n``` \n Synchronous Connection \n In order to activate the synchronous mode, just pass  #{verbose => true  to  nats:connect . \n Connect, publish, subscribe and unsubscribe operations block and return either  ok  on\nsuccess or  {error, Reason :: term()}  on failure. \n Sample \n ```erlang\nmain() ->\n    % Connect to the NATS server\n    {ok, Conn} = nats:connect(<<"demo.nats.io">>, 4222, #{verbose => true}),\n    % The connection is OK to use\n    % Publish some message\n    ok = nats:pub(Conn, <<"teacup.control">>, #{payload => <<"start">>}),\n    % subscribe to some subject\n    ok = nats:sub(Conn, <<"foo.*">>),\n    loop(Conn). \n loop(Conn) ->\n    receive\n        {Conn, {msg, Subject, _ReplyTo, Payload}} ->\n            % Do something with the received message\n            io:format("~p: ~p~n", [Subject, Payload]),\n            loop(Conn)\n    end. \n ``` \n License \n ```\nCopyright 2016 Yuce Tekol  yucetekol@gmail.com \n Licensed under the Apache License, Version 2.0 (the "License");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at \n http://www.apache.org/licenses/LICENSE-2.0\n \n Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an "AS IS" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```', 'Escript vs Compiled Module Performance Benchmark \n Run the Benchmark in Escript Mode \n $ make s\nOperations per second: 7374.576884 \n Run the Benchmark in Compiled Module Mode \n make m\nOperations per second: 610821.310334', "NATS Client Benchmarks \n This repository contains the code to benchmark and compare performance\nof  NATS  clients in different languages/frameworks. \n Benchmarks for the following clients are available at this time: \n \n Elixir \n Erlang \n Go \n NodeJS \n Python (Tornado) \n \n All clients ran on a consumer grade laptop with Ubuntu\n14.04, using the following benchmark setup: \n \n Stock gnatsd 0.7.2, running with default parameters \n Host: 127.0.0.1 \n Port: 4222 \n TLS: no \n \n \n 1 publisher \n 1 subscriber \n 1_000_000 messages \n All messages had the same 16 byte subject \n All messages had the same 64 byte payload \n \n Below is the raw results for a sample single run. Please take it with a grain\nof salt. The numbers below are presented only to show the  order of magnitude \nbetween performance of clients.  \n | Client           | Time to complete (microseconds) | Messages per second |\n| ---------------- | ------------------------------: | ------------------: |\n| Go               |                         926_572 |           1_079_245 |\n| Node             |                       3_112_000 |             321_337 |\n| Erlang           |                       5_330_898 |             187_586 |\n| Elixir           |                      20_797_629 |              46_613 |\n| Python (Tornado) |                      59_586_310 |              16_782 | \n Building the Benchmark Code \n Benchmarks run fine on Linux, and should run without modifications on OSX.\nOther POSIX systems with BASH should be OK with no to little modifications.\nIf you successfully run the benchmarks on Windows, I would be happy\nto update the instructions below.  \n Benchmark code for clients exist in their own directories. After getting\nthe dependencies below, you can just run  make  in the directory corresponding\nto a client. \n Client dependencies: \n \n Elixir \n Erlang 18.x . It's pretty straightforward\nto compile Erlang from source, but Erlang Solutions provide precompiled\npackages for many systems  here . \n Elixir 1.2.x . It's pretty straightforward\nto compile Elixir from source, but Erlang Solutions provide precompiled\npackages for many systems  here . \n \n \n Erlang \n Erlang 18.x . It's pretty straightforward\nto compile Erlang from source, but Erlang Solutions provide precompiled\npackages for many systems  here . \n rebar3 3.x \n \n \n Go \n Go 1.6.x \n \n \n NodeJS \n NodeJS 5.x \n \n \n Python (Tornado) \n Python 2.7 . It's probably already\ninstalled on your system. \n virtualenv 1.11+ . It's probably\nalready installed on your system or available with your package manager. \n \n \n \n Running the Benchmark Code \n After building, just run  make run  in the directory for the corresponding\nclient. The output has the same format for all clients: \n 1000000 917084 1090411\n \n \n Number of messages \n The time in microseconds to complete \n Messages per second \n \n You can alter defaults by setting the following environment variables: \n \n NATS_URL : NATS URL to use for the benchmark, in the form of  nats://HOST:PORT . E.g.,  nats://127.0.0.1:4222 \n MSG_COUNT : Number of messages to publish/subscribe. \n \n For example: \n $ NATS_URL=nats://demo.nats.io MSG_COUNT=100 make run\n \n Contributing \n Pull requests are welcome for benchmark code for other clients\nor fixes for current code. \n Notes \n \n Go benchmark code is taken and simplied from: https://github.com/nats-io/nats/blob/master/examples/nats-bench.go \n NodeJS code is taken and simplified from: https://github.com/nats-io/node-nats/blob/master/benchmark/pub_sub_perf.js \n", 'teacup-bench', 'Erlang Requests \n Erlang port of Python requests module', 'catnats \n cat  for  NATS  gnatsd server. Works for both TLS and non-TLS connections. \n NEWS \n \n \n 2016-06-19 : Version 0.1.2: \n \n \n Bug fixes. \n \n \n 2016-03-29 : Version 0.1.1: \n \n \n Added  --raw  option. \n \n \n 2016-04-29 : Version 0.1.0: \n \n \n Changed how the server address is specified. You can now use  --addr host:port \n \n If no adddress is specified, the default  127.0.0.1:4222  is used. \n Added the following  CONNECT  message parameters:  --user ,  --pass  and  --verbose . If any of\n  these parameters is given in the command line, a  CONNECT  message is sent to the server. \n \n Requirements \n Python 2.7.6+ or Python 3.4+ is required. \n Non-TLS connections should work on all platforms. TLS connections require: \n \n on Ubuntu 14.04, Python 2.7 and Python 3.4 works out of the box \n on OSX (El-Capitan) you need to install an updated version of Python with updated OpenSSL\nusing  brew install python --with-brewed-openssl \n on Windows, Python 3.5 works \n \n Usage \n catnats.py [options] \n Options: \n \n -addr : Specify the server address and port in the form of  HOST:PORT   \n -q  or  --quiet : Suppress output \n --raw : Raw input. Do not process data coming from STDIN \n --pong : Enable automatically sending a  PONG  message to gnatsd\nwhen a  PING  message is received (to keep the connection alive) \n --no-exit : Do not exit automatically (unless there is an error) \n --user USER : Send a  CONNECT  message with  user  field set to  USER \n --pass PASSWORD : Send a  CONNECT  message with  pass  field set to  PASSWORD \n --verbose true | false : Send a  CONNECT  message with  verbose  field set to  VERBOSE \n \n Examples \n Works with pipes: \n $ printf \'connect {}\\r\\nping\\r\\n\' | ./catnats.py -q --addr demo.nats.io:4443 \n And without: \n $ ./catnats.py --pong --addr demo.nats.io:4443 \n Publish time and date every 5 seconds: \n $ while true; do time=`date` && \\\n  printf "PUB device-time.$(hostname) ${#time}\\r\\n${time}\\r\\n" | \\\n  ./catnats.py --quiet --addr demo.nats.io:4443; sleep 5; done \n Send load average every 5 seconds (on Linux) \n $ while true; do payload=`cat /proc/loadavg | cut -f 1-3 -d" "` && \\\n  printf "PUB device-avgload.$(hostname) ${#payload}\\r\\n${payload}\\r\\n" | \\\n  ./catnats.py --quiet --addr demo.nats.io:4222; sleep 5; done \n The same on OSX \n $ while true; do payload=`sysctl -n vm.loadavg | cut -f 2-4 -d" "` && \\\n  printf "PUB device-avgload.$(hostname) ${#payload}\\r\\n${payload}\\r\\n" | \\\n  ./catnats.py --quiet --addr demo.nats.io:4222; sleep 5; done \n License \n ```\nCopyright (c) 2016, Yuce Tekol  yucetekol@gmail.com .\nAll rights reserved. \n Redistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet: \n \n \n Redistributions of source code must retain the above copyright\n  notice, this list of conditions and the following disclaimer. \n \n \n Redistributions in binary form must reproduce the above copyright\n  notice, this list of conditions and the following disclaimer in the\n  documentation and/or other materials provided with the distribution. \n \n \n The names of its contributors may not be used to endorse or promote\n  products derived from this software without specific prior written\n  permission. \n \n \n THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n```', '', 'Docker Builder for i3 Window Manager \n Builds and installs i3 window manager version 4.13. \n History \n \n 2016-11-26: Initial version \n \n Requirements \n \n Docker (version 1.12.1 on Ubuntu 16.04 repositories works just fine) \n Tested only on Ubuntu 16.04, should work on any reasonably modern Linux system just fine. \n \n Usage: \n Run  bash install.sh .  xcb-util-xrm  will be installed at  /opt/xcb-util-xrm-1.2  and i3 at  /opt/i3-4.13 . A symlink will be created at  /opt/i3 .  i3.desktop  will be copied to  /usr/share/xsessions/ \n install.sh  takes an optional argument, which points to the source directory where i3 source is at  i3-4.13  and the source for  xcb-util-xrm  at:  xcb-util-xrm-1.2', 'i3gobar \n i3gobar  is a custom bar for i3 written in Golang. \n Usage \n i3gobar -log LOG_FILE -config CONFIGURATION_FILE \n See  priv/sampleconfig.json  for an example configuration.', 'Awesome Pilosa  \n A curated list of awesome articles, software and resources for Pilosa distributed bitmap index. \n Software \n Server \n \n Pilosa Server  [Apache 2] \n \n Client \n \n Go Client (official)  [BSD] \n Java Client (official)  [BSD] \n Python Client (official)  [BSD] \n Lua Client  [BSD] \n JavaScript/TypeScript Client  [BSD] \n Swift Client  [MIT] \n R Client  [MIT] \n \n Tools \n \n Pilosa Development Kit  [BSD] Implementation tooling and use case examples \n PiCon: Pilosa Console  [BSD] Text based console for Pilosa \n Pilosa Notebooks  [CC0] iPython notebooks for Pilosa demonstrations \n \n Documentation and Tutorials \n \n Main Documentation \n Getting Started \n \n Resources \n \n Project Home \n Pilosa on Twitter \n Pilosa on Crunchbase \n \n Articles \n \n Pilosa launches distributed index for accelerated data querying \n \n Podcasts, Talks and Interviews \n \n Pilosa: A New Kind of Database Index \n FLOSS Weekly #437 (video) \n The Index As a First Class Citizen  (video)   (slides) \n Go Time #76: Building a distributed index \n \n Projects Using Pilosa \n \n Go Pensieve  [MIT] Parse Krux data feed files and import into Pilosa \n', 'Javascript/Typescript Client for Pilosa \n \n \n \n \n \n Javascript/Typescript client for  Pilosa  high performance distributed bitmap index. \n Change Log \n \n \n v0.4.0  (2017-06-10): \n \n Supports Pilosa Server v0.4.0. \n Breaking Change : Changed default row ID label to  rowID  and default column ID to  column ID . \n Updated the accepted values for index, frame names and labels to match with the Pilosa server. \n Union  queries accept 0 or more arguments.  Intersect  and  Difference  queries accept 1 or more arguments. \n Added  inverse TopN  and  inverse Range  calls. \n Inverse enabled status of frames is not checked on the client side. \n https  scheme is allowed. \n \n \n \n v0.3.3  (2017-05-28): \n \n Initial version. \n Supports Pilosa Server v0.3.2. \n \n \n \n Requirements \n \n NodeJS 4 or later \n (Optional) Typescript 2.3 and higher \n \n Install \n Pilosa client is available as an  npm  package. You can install the library using: \n npm install --save pilosa \n Usage \n Quick overview (Javascript/Typescript using promises) \n Assuming  Pilosa  server is running at  localhost:10101  (the default): \n ```javascript\nvar pilosa = require("pilosa"); \n // Create the default client\nvar client = new pilosa.Client(); \n // Create an Index object\nvar myindex = new pilosa.Index("myindex"); \n // Create a Frame object\nvar myframe = myindex.frame("myframe"); \n // Make sure the index exists on the server\nclient.ensureIndex(myindex).then(() => \n // Make sure the frame exists on the server\nclient.ensureFrame(myframe)).then(() => \n // Send a SetBit query\nclient.query(myframe.setBit(5, 42))).then(_ => {\n    // Send a Bitmap query\n    client.query(myframe.bitmap(5)).then(response => {\n        if (response.result) {\n            var bits = response.result.bitmap.bits;\n            console.log("Got bits: ", bits);\n        }\n    }).catch(err => console.log("ERROR: ", err)); \n // You can batch queries to improve throughput\nclient.query(\n    myindex.batchQuery(\n        myframe.bitmap(5),\n        myframe.bitmap(19)\n    )\n).then(response => {\n    response.results.forEach(result => {\n        console.log("Got bits: ", result.bitmap.bits);\n    });\n}).catch(err => console.log("ERROR: ", err));\n \n }).catch(err => console.log("ERROR: ", err)); \n ``` \n Quick overview (Typescript using async/await) \n ```typescript\nimport * as pilosa from "pilosa"; \n async function main() {\n    // Create the default client\n    var client = new pilosa.Client(); \n // Create an Index object\nvar myindex = new pilosa.Index("myindex");\n\n// Create a Frame object\nvar myframe = myindex.frame("myframe");\n\n// Make sure the index exists on the server\nawait client.ensureIndex(myindex);\n\n// Make sure the frame exists on the server\nawait client.ensureFrame(myframe);\n\n// Send a SetBit query\nawait client.query(myframe.setBit(5, 42));\n\n// Send a Bitmap query\nvar response = await client.query(myframe.bitmap(5));\nif (response.result) {\n    var bits = response.result.bitmap.bits;\n    console.log("Got bits: ", bits);\n}\n\n// You can batch queries to improve throughput\nresponse = await client.query(\n    myindex.batchQuery(\n        myframe.bitmap(5),\n        myframe.bitmap(19)));\n\nresponse.results.forEach(result => {\n    console.log("Got bits: ", result.bitmap.bits);\n});\n \n } \n main().catch(err => console.log("ERROR: ", err));\n``` \n Data Model and Queries \n Indexes and Frames \n Index  and  frame s are the main data models of Pilosa. You can check the  Pilosa documentation  for more detail about the data model. \n Index  constructor is used to create an index object. Note that this does not create an index on the server; the index object simply defines the schema. \n javascript\nvar repository = new pilosa.Index("repository") \n Indexes support changing the column label and time quantum.  IndexOptions  objects store that kind of data. In order to apply these custom options, pass an  IndexOptions  object as the second argument to  Index : \n javascript\nvar options = {\n    columnLabel: "repo_id",\n    timeQuantum: pilosa.TimeQuantum.YEAR_MONTH\n}\nvar repository = pilosa.Index("repository", options); \n Frames are created with a call to  index.frame  method: \n javascript\nvar stargazer = repository.frame("stargazer"); \n Similar to index objects, you can pass custom options to the  index.frame  method: \n javascript\nvar options = {\n    rowLabel: "stargazer_id",\n    timeQuantum: pilosa.TimeQuantum.YEAR_MONTH_DAY\n}\nvar stargazer = repository.frame("stargazer", options); \n Queries \n Once you have indexes and frame objects created, you can create queries for them. Some of the queries work on the columns; corresponding methods are attached to the index. Other queries work on rows, with related methods attached to frames. \n For instance,  Bitmap  queries work on rows; use a frame object to create those queries: \n javascript\nvar bitmapQuery = stargazer.bitmap(1, 100);  // corresponds to PQL: Bitmap(frame=\'stargazer\', stargazer_id=1) \n Union  queries work on columns; use the index object to create them: \n javascript\nvar query = repository.union(bitmapQuery1, bitmapQuery2); \n In order to increase througput, you may want to batch queries sent to the Pilosa server. The  index.batchQuery  method is used for that purpose: \n javascript\nvar query = repository.batchQuery(\n    stargazer.bitmap(1, 100),\n    repository.union(stargazer.bitmap(100, 200), stargazer.bitmap(5, 100))\n); \n The recommended way of creating query objects is, using dedicated methods attached to index and frame objects. But sometimes it would be desirable to send raw queries to Pilosa. You can use the  index.rawQuery  method for that. Note that, query string is not validated before sending to the server: \n javascript\nvar query = repository.rawQuery("Bitmap(frame=\'stargazer\', stargazer_id=5)"); \n Check  Pilosa documentation  for PQL details. Here is a list of methods corresponding to PQL calls: \n Index: \n \n union(...bitmaps: Array<PqlBitmapQuery>): PqlBitmapQuery \n intersect(...bitmaps: Array<PqlBitmapQuery>): PqlBitmapQuery \n difference(...bitmaps: Array<PqlBitmapQuery>): PqlBitmapQuery \n count(bitmap: PqlBitmapQuery): PqlQuery \n setColumnAttrs(columnID: number, attrs: AttributeMap): PqlBitmapQuery \n \n Frame: \n \n bitmap(rowID: number): PqlBitmapQuery \n inverseBitmap(columnID: number): PqlQuery \n setBit(rowID: number, columnID: number, timestamp?: Date): PqlQuery \n clearBit(rowID: number, columnID: number): PqlQuery \n topN(n: number, bitmap?: PqlBitmapQuery, field?: string, ...values: Array<any>): PqlBitmapQuery \n inverseTopN(n: number, bitmap?: PqlBitmapQuery, field?: string, ...values: Array<any>): PqlBitmapQuery \n range(rowID: number, start: Date, end: Date): PqlBitmapQuery \n inverseRange(columnID: number, start: Date, end: Date): PqlBitmapQuery \n setRowAttrs(rowID: number, attrs: AttributeMap): PqlBitmapQuery \n \n Pilosa URI \n A Pilosa URI has the  ${SCHEME}://${HOST}:${PORT}  format:\n*  Scheme : Protocol of the URI. Default:  http .\n*  Host : Hostname or ipv4/ipv6 IP address. Default: localhost.\n*  Port : Port number. Default:  10101 . \n All parts of the URI are optional, but at least one of them must be specified. The following are equivalent: \n \n http://localhost:10101 \n http://localhost \n http://:10101 \n localhost:10101 \n localhost \n :10101 \n \n A Pilosa URI is represented by the  pilosa.URI  class. Below are a few ways to create  URI  objects: \n ```javascript\n// create the default URI: http://localhost:10101\nvar uri1 = new pilosa.URI() \n // create a URI from string address\nvar uri2 = pilosa.URI.address("db1.pilosa.com:20202") \n // create a URI with the given host and port\nvar URI uri3 = new pilosa.URI(host="db1.pilosa.com", port=20202);\n```  \n Pilosa Client \n In order to interact with a Pilosa server, an instance of  pilosa.Client  should be created. We recommend creating a single instance of the client and share it with other objects when necessary. \n If the Pilosa server is running at the default address ( http://localhost:10101 ) you can create the default client with default options using: \n javascript\nvar client = new pilosa.Client() \n To use a a custom server address, pass the address in the first argument: \n javascript\nvar client = new pilosa.Client("http://db1.pilosa.com:15000") \n If you are running a cluster of Pilosa servers, you can create a  pilosa.Cluster  object that keeps addresses of those servers: \n ```javascript\nvar cluster = new pilosa.Cluster(\n    pilosa.URI.address(":10101"),\n    pilosa.URI.address(":10110"),\n    pilosa.URI.address(":10111"),\n); \n // Create a client with the cluster\nvar client = new pilosa.Client(cluster)\n``` \n Once you create a client, you can create indexes, frames and start sending queries. All client methods return a  Promise  object. \n Here is how you would create an index and frame: \n javascript\n// materialize repository index instance initialized before\nclient.createIndex(repository).then(() =>\n// materialize stargazer frame instance initialized before\nclient.create_frame(stargazer).then(() => {\n    // actions on the frame.\n})).catch(err => {\n    // act on the error\n}); \n Using async/await syntax is obviously preferable in case you are using Javascript with NodeJS 7 and higher or Typescript: \n ```javascript\ntry {\n    // materialize repository index instance initialized before\n    await client.createIndex(repository);\n    // materialize stargazer frame instance initialized before\n    await client.create_frame(stargazer); \n // actions on the frame...\n \n }\ncatch (e)  {\n    // act on the error\n}\n``` \n If the index or frame exists on the server, you will receive a  PilosaError . You can use  ensureIndex  and  ensureFrame  methods to ignore existing indexes and frames. \n You can send queries to a Pilosa server using the  query  method of client objects. \n Using promises: \n javascript\nclient.query(frame.bitmap(5)).then(response => {\n    // act on the response\n}) \n Using async/await:\n javascript\nvar response = await client.query(frame.bitmap(5));\n// act on the response \n query  method accepts an optional argument of type  QueryOptions : \n javascript\nvar queryOptions = {\n    columns: true  // return column data in the response\n}\nclient.query(frame.bitmap(5), queryOptions).then(response => {\n    // act on the response\n}); \n Server Response \n When a query is sent to a Pilosa server, the server either fulfills the query or sends an error message. In the case of an error,  PilosaError  is returned, otherwise a  QueryResponse  object is returned. \n A  QueryResponse  object may contain zero or more results of  QueryResult  type. You can access all results using the  results  property of  QueryResponse  (which returns a list of  QueryResult  objects) or you can use the  result  property (which returns either the first result or  null  if there are no results): \n ```javascript\nclient.query(frame.bitmap(5)).then(response => {\n    // check that there\'s a result and act on it\n    var result = response.result\n    if result {\n        // act on the result\n    } \n // iterate on all results\nresponse.results.forEach(result => {\n    // act on the result\n});\n \n }).catch(err => {\n    // act on the error\n});\n``` \n Similarly, a  QueryResponse  object may include a number of column objects, if  columns=true  query option was used: \n ```javascript\n// check that there\'s a column object and act on it\nvar column = response.column\nif column {\n    // act on the column\n} \n // iterate on all columns\nresponse.columns.forEach(column => {\n    // act on the column\n})\n``` \n QueryResult  objects contain: \n \n bitmap  property to retrieve a bitmap result, \n countItems  property to retrieve column count per row ID entries returned from  topN  queries, \n count  property to retrieve the number of rows per the given row ID returned from  count  queries. \n \n ```javascript\nvar bitmap = response.bitmap\nvar bits = bitmap.bits\nvar attributes = bitmap.attributes \n var countItems = response.countItems \n var count = response.count\n``` \n Contribution \n \n Fork this repo and add it as upstream:  git remote add upstream git@github.com:yuce/js-pilosa.git . \n Make sure all tests pass (use  make test-all ) and be sure that the tests cover all statements in your code (we aim for 100% test coverage). \n Commit your code to a feature branch and send a pull request to the  master  branch of our repo. \n \n The sections below assume your platform has  make . Otherwise you can view the corresponding steps of the  Makefile . \n Running tests \n You can run unit tests with:\n make test \n And both unit and integration tests with:\n make test-all \n Generating protobuf classes \n Protobuf classes are already checked in to source control, so this step is only needed when the upstream  public.proto  changes. \n Before running the following step, make sure you have the  Protobuf compiler  installed: \n make generate \n License \n ```\nCopyright 2017 Yuce Tekol \n Redistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions\nare met: \n \n \n Redistributions of source code must retain the above copyright\nnotice, this list of conditions and the following disclaimer. \n \n \n Redistributions in binary form must reproduce the above copyright\nnotice, this list of conditions and the following disclaimer in the\ndocumentation and/or other materials provided with the distribution. \n \n \n Neither the name of the copyright holder nor the names of its\ncontributors may be used to endorse or promote products derived\nfrom this software without specific prior written permission. \n \n \n THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND\nCONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES,\nINCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\nMERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\nCONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\nBUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\nINTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\nWHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\nNEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH\nDAMAGE.\n```', 'PiCon \n \n \n Simple console for  Pilosa  high performance distributed bitmap index. \n This app uses JSON API of Pilosa directly for queries and the official  Go client  for everything else (e.g., creating indexes, frames, ...). \n Below is a screencast that shows how it works:\n \n Change Log \n \n 0.2.0  (2017-06-17) \n Initial public version \n \n \n \n Dependencies \n \n Pilosa Go Client \n Readline \n Go Pretty JSON \n \n Build \n Go 1.8 or higher is needed. UNIX and Windows platforms are supported. \n Make sure  go  is on the path and run:\n go get github.com/yuce/picon/cmd/picon\ngo install github.com/yuce/picon/cmd/picon \n Usage \n You can run the console with  picon . \n \n Commands start with  : . \n To get a list of commands, hit  :  and then  Tab . \n To exit, you can type  :exit  or hit  Ctrl+D . \n Notes start with  # . \n Queries can be run directly. \n In order to enter multiline commands/queries, finish a line with backslash ( \\ ). \n Up/down arrow keys can be used to access the history. \n Hit tab for PQL call completion. \n :use  command supports index name completion. \n :connect  command supports address completion using the history. \n If a command is made up of more than one word, they can be autocompleted. \n See the  keyboard shortcuts  you can use. \n \n Sample workflow: \n ``` \n \n :connect :10101\n:ensure index myindex col=col_id\n:ensure frame myframe inverse=true\nSetBit(frame=\'myframe\', rowID=1, col_id=100)\nBitmap(frame=\'myframe\', rowID=1)\n... Some output\n``` \n \n Available commands \n \n :connect : Connect to the Pilosa server. Usage:  :connect pilosa-address . \n :create : Create an index or a frame. Usage:  :create {index | frame} name [option1=value1, ...] . \n :delete : Delete an index or a frame. Usage:  :delete {index | frame} name1, ... . \n :ensure : Ensure that an index or a frame exists. Usage:  :ensure {index | frame} name [option1=value1, ...] . \n :http : Send a raw HTTP request to the server. See:  API Documentation . Usage:  :http method path [data] . \n :schema : Display the scheme (indexes and frames) on the server. Usage:  :schema . \n :use : Open an index. Usage:  :use index-name . \n \n :create index  and  :ensure index  commands support the following options:\n*  column_label ,  columnLabel ,  col ,  c \n*  time_quantum ,  timeQuantum ,  time ,  t \n :create frame  and  :ensure frame  commands support the following options:\n*  row_label ,  rowLabel ,  row ,  r \n*  time_quantum ,  timeQuantum ,  time ,  t \n*  inverse_enabled ,  inverseEnabled ,  inverse ,  i . \n You can use  true ,  t  or  1  for true value;  false ,  f  or  0  false value. \n Queries \n Any valid PQL query can be executed directly. See:  PQL Documentation \n License \n ```\nCopyright 2017 Yuce Tekol \n Redistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions\nare met: \n \n \n Redistributions of source code must retain the above copyright\nnotice, this list of conditions and the following disclaimer. \n \n \n Redistributions in binary form must reproduce the above copyright\nnotice, this list of conditions and the following disclaimer in the\ndocumentation and/or other materials provided with the distribution. \n \n \n Neither the name of the copyright holder nor the names of its\ncontributors may be used to endorse or promote products derived\nfrom this software without specific prior written permission. \n \n \n THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND\nCONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES,\nINCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\nMERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\nCONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\nBUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\nINTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\nWHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\nNEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH\nDAMAGE.\n```', 'Pilosa Scripts \n Requirements \n \n Python 3 \n \n License \n Copyright 2017 Pilosa Corp. \n Redistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions\nare met: \n \n \n Redistributions of source code must retain the above copyright\nnotice, this list of conditions and the following disclaimer. \n \n \n Redistributions in binary form must reproduce the above copyright\nnotice, this list of conditions and the following disclaimer in the\ndocumentation and/or other materials provided with the distribution. \n \n \n Neither the name of the copyright holder nor the names of its\ncontributors may be used to endorse or promote products derived\nfrom this software without specific prior written permission. \n \n \n THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND\nCONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES,\nINCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\nMERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\nCONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\nBUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\nINTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\nWHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\nNEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH\nDAMAGE.', 'Hazelcast Go Client \n \n Hazelcast is an open-source distributed in-memory data store and computation platform that provides a wide variety of distributed data structures and concurrency primitives. \n Hazelcast Go client is a way to communicate to Hazelcast 4 and 5 clusters and access the cluster data. \n Sample Code \n ```go\npackage main \n import (\n    "context"\n    "fmt"\n    "log" \n "github.com/hazelcast/hazelcast-go-client"\n \n ) \n func main() {\n    ctx := context.TODO()\n    // create the client and connect to the cluster on localhost\n    client, err := hazelcast.StartNewClient(ctx)\n    if err != nil {\n        log.Fatal(err)\n    }\n    // get a map\n    people, err := client.GetMap(ctx, "people")\n    if err != nil {\n        log.Fatal(err)\n    }\n    personName := "Jane Doe"\n    // set a value in the map\n    if err = people.Set(ctx, personName, 30); err != nil {\n        log.Fatal(err)\n    }\n    // get a value from the map\n    age, err := people.Get(ctx, personName)\n    if err != nil {\n        log.Fatal(err)\n    }\n    fmt.Printf("%s is %d years old.\\n", personName, age)\n    // stop the client to release resources\n    client.Shutdown(ctx)\n}\n``` \n Features \n \n Distributed, partitioned and queryable in-memory key-value store implementation, called Map. \n Additional data structures and simple messaging constructs such as Replicated Map, MultiMap, Queue, List, PNCounter, Set, Topic and others. \n Support for serverless and traditional web service architectures with Unisocket and Smart operation modes. \n Go context support for all distributed data structures. \n Hazelcast Cloud integration. \n SQL support (only on Hazelcast 5.x). \n External smart client discovery. \n Hazelcast Management Center integration. \n Ability to listen to client lifecycle, cluster state, and distributed data structure events. \n And  more ... \n \n Install \n Requirements: \n \n Hazelcast Go client is compatible only with Hazelcast 4.x and 5.x. \n We support two most recent releases of Go, currently 1.17 and 1.18. \n \n In your Go module enabled project, add a dependency to  github.com/hazelcast/hazelcast-go-client :\n```shell \n Depend on the latest release \n $ go get github.com/hazelcast/hazelcast-go-client@latest\n``` \n Quick Start \n Hazelcast Go client requires a working Hazelcast cluster. \n Check out our  Get Started  page for options. \n Starting the Client with Hazelcast Cloud \n You only need the cluster name and Hazelcast cloud token to start the client.\nIf you haven\'t already, you can  sign up  for a free Hazelcast Cloud account. \n go\nconfig := hazelcast.Config{}\ncc := &config.Cluster\ncc.Name = "YOUR HAZELCAST CLOUD CLUSTER NAME"\ncc.Cloud.Enabled = true\ncc.Cloud.Token = "YOUR HAZELCAST CLOUD TOKEN"\nclient, err := hazelcast.StartNewClientWithConfig(ctx, config)\n// handle the error \n Starting the Default Client \n Start the client with the default Hazelcast host and port using  hazelcast.StartNewClient , when Hazelcast is running on local with the default options: \n go\nctx := context.TODO()\nclient, err := hazelcast.StartNewClient(ctx)\n// handle client start error \n Starting the Client with Given Options \n Note that  Config  structs are not thread-safe. Complete creation of the configuration in a single goroutine. \n go\n// create the default configuration\nconfig := hazelcast.Config{}\n// optionally set member addresses manually\nconfig.Cluster.Network.SetAddresses("member1.example.com:5701", "member2.example.com:5701")\n// create and start the client with the configuration provider\nclient, err := hazelcast.StartNewClientWithConfig(ctx, config)\n// handle client start error \n Documentation \n Hazelcast Go Client documentation is hosted at  pkg.go.dev . \n You can view the documentation locally by using godoc:\n $ godoc -http=localhost:5500 \n godoc is not installed by default with the base Go distribution. Install it using:\n $ go get -u golang.org/x/tools/...` \n Support \n Join us at  Go Client channel  or  Hazelcast at Google Groups . \n Running the tests \n Currently, we support only Linux, MacOS and WSL (Windows Subsystem for Linux) for testing the client. \n You need to have the following installed in order to run integration tests:\n* Java 8\n* Maven 3 or better\n* Bash\n* Make \n Before running the tests, starts Hazelcast Remote Controller, which enables the test suite to create clusters:\n```shell \n Start RC with Hazelcast Community features \n $ ./rc.sh start \n Or, start RC with Hazelcast Enterprise features \n $ HAZELCAST_ENTERPRISE_KEY=ENTERPRISE-KEY-HERE ./rc.sh start \n``` \n You can run the tests using one of the following approaches:\n* Run  make test-all  to run integration tests.\n* Run  make test-all-race  to run integration tests with race detection.\n* Run  make test-cover  to generate the coverage report and  make view-cover  to view the test coverage summary and generate an HTML report. \n Testing the client with SSL support requires running the remote controller with Hazelcast Enterprise features.\nTo enable SSL connections, add  ENABLE_SSL=1  to environment variables, or prepend it to the make commands above. \n In order to turn on verbose logging, add  ENABLE_TRACE=1  to environment variables, or prepend it to the make commands above. \n License \n Apache 2 License . \n Copyright (c) 2008-2021, Hazelcast, Inc. All Rights Reserved. \n Visit  www.hazelcast.com  for more information.', 'Pilosa Test with Java Client \n Requirements \n \n JDK 8 (tested with OpenJDK) \n Maven (tested with 3.5.0) \n \n Building \n make \n Running \n cd deploy_package\n./run.sh PILOSA_ADDR MAX_ROW_ID MAX_COLUMN_ID BATCH_SIZE random [THREAD_COUNT [SLICE WIDTH]] \n THREAD_COUNT  defaults to the number of CPUs and  SLICE_WIDTH  defaults to the default Pilosa server slice width.', 'Pilosa Import Test \n Requirements \n \n Go Pilosa from https://github.com/yuce/go-pilosa/tree/optimize-imports \n D Compiler: https://dlang.org/download.html \n \n Install \n go get -u github.com/yuce/importtst \n In order to create binaries:\n make \n Usage \n pilosa-import PILOSA_ADDR PATH.csv|PATH.csv.gz BATCH_SIZE \n E.g., \n pilosa-import localhost my.csv 100_000 \n License \n ```\nCopyright 2017 Pilosa Corp. \n Redistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions\nare met: \n \n \n Redistributions of source code must retain the above copyright\nnotice, this list of conditions and the following disclaimer. \n \n \n Redistributions in binary form must reproduce the above copyright\nnotice, this list of conditions and the following disclaimer in the\ndocumentation and/or other materials provided with the distribution. \n \n \n Neither the name of the copyright holder nor the names of its\ncontributors may be used to endorse or promote products derived\nfrom this software without specific prior written permission. \n \n \n THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND\nCONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES,\nINCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\nMERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\nCONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\nBUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\nINTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\nWHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\nNEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH\nDAMAGE.\n```', '\n \n \n Experimental D Roaring Bitmaps Library \n Roaring Bitmaps  are compressed bit arrays which can store a huge amount of bits in a space efficient manner. The bitmap is organized so that adding/removing bits is very fast and don\'t require unpacking the whole bitmap. You can use bit arrays for efficient set operations. \n Check out  Pilosa  for an open source distributed index which uses roaring bitmaps. \n This library wraps  CRoaring . \n Thanks to all  Contributors . \n News \n \n \n Version  0.2.1  (2018-12-15) \n \n Bitmap.addRange ,  containsRange ,  lengthInRange  methods by  Justin Whear . \n \n \n \n Version  0.2.0  (2018-12-08) \n \n Bitmap.dup  property and  Bitmap.andNot  method by  Justin Whear \n More  Bitmap.opOpAssign  overloads by  Justin Whear \n Updated to  CRoaring  0.2.58 by  Justin Whear \n \n \n \n Version  0.1.0  (2018-04-08) \n \n Initial version \n \n \n \n Limitations \n \n Bitmap size is ( currently ) limited to  uint.max , which is  2^^32 - 1 . This is a limitation of CRoaring. See: https://github.com/RoaringBitmap/CRoaring/issues/1 \n \n Requirements \n \n A recent D compiler. Tested with DMD v2.079.0, LDC 1.8.0 and GDC v2.068.2_gcc6. \n C compiler with C11 support. \n Tested on Linux, FreeBSD, MacOS and Windows (only with 64bit). \n \n Install \n Using DUB \n Add  roaring  to your DUB dependencies. E.g.:\n json\n{\n    "name": "roar",\n    "description": "A minimal D application.",\n    "dependencies": {\n        "roaring": {\n            "version": "0.2.0"\n        }\n    }\n} \n Without DUB \n Assuming you\'ve built the library and  $DROARING_HOME  points to the DRoaring directory: \n dmd your_source.d $DROARING_HOME/ext/roaring.o -L-L$DROARING_HOME -L-lroaring -I$DROARING_HOME/source \n Example \n ```d\nvoid main()\n{\n    import std.stdio : writefln, writeln;\n    import roaring.roaring : Bitmap, bitmapOf; \n // create a new roaring bitmap instance\nauto r1 = new Bitmap;\n\n// add some bits to the bitmap\nr1.add(5);\n\n// create from an array\nauto ra = bitmapOf([1, 2, 3]);\n\n// create from a range\nimport std.range : iota;\nassert(bitmapOf(0, 1, 2, 3) == bitmapOf(4.iota));\n\n// create a new roaring bitmap instance from some numbers\nauto r2 = bitmapOf(1, 3, 5, 15);\n\n// check whether a value is contained\nassert(r2.contains(5));\nassert(5 in r2); // r2 contains 5\nassert(99 !in r2); // r2 does not contain 99\n\n// get minimum and maximum values in a bitmap\nassert(r2.minimum == 1);\nassert(r2.maximum == 15);\n\n// remove a value from the bitmap\nr2.remove(5);\nassert(!r2.contains(5));\n\n// compute how many bits there are:\nassert(3 == r2.length);\n\n// check whether a bitmap is subset of another\nconst sub = bitmapOf(1, 5);\nconst sup = bitmapOf(1, 2, 3, 4, 5, 6);\nassert(sub in sup);\n\n// iterate on a bitmap\nconst r3 = bitmapOf(1, 5, 10, 20);\nulong s = 0;\nforeach (bit; r3) {\n    s += bit;\n}\nassert(s == 36);\n\n// iterate on a bitmap and index\nforeach(i, bit; r3) {\n    writefln("%d: %d", i, bit);\n}\n\nimport roaring.roaring : readBitmap, writeBitmap;\n// serialize the bitmap\nchar[] buf = writeBitmap(r3);\n// deserialize from a char array\nconst r3Copy = readBitmap(buf);\nassert(r3 == r3Copy);\n\n// find the intersection of bitmaps\nconst r4 = bitmapOf(1, 5, 6);\nconst r5 = bitmapOf(1, 2, 3, 4, 5);\nassert((r4 & r5) == bitmapOf(1, 5));\n// find the union of bitmaps\nassert((r4 | r5) == bitmapOf(1, 2, 3, 4, 5, 6));\n\nconst r6 = bitmapOf(0, 10, 20, 30, 40, 50);\n// get the bit for the index\nassert(20 == r6[2]);\n// slice a bitmap\nassert(bitmapOf(30, 40, 50) == r6[3..$]);\n\n// convert the bitmap to a string\nwriteln("Bitmap: ", r6);\nimport std.conv : to;\nassert("{0, 10, 20, 30, 40, 50}" == to!string(r6));\n\n// the bit range!\nimport std.algorithm : filter, sum;\nimport roaring.roaring : range, BitRange;\n// sum of bits in r6 which are bit % 20==0\nassert(60 == r6.range.filter!(b => b % 20 == 0).sum);\n\n// if your bitmaps have long runs, you can compress them\nauto r7 = bitmapOf(1000.iota);\nwriteln("size before optimize = ", r7.sizeInBytes);\nr7.optimize();\nwriteln("size after optimize = ", r7.sizeInBytes);\n\n// copy a bitmap (uses copy-on-write under the hood)\nconst r8 = r7.dup;\nassert(r8 == r7);\n \n }\n``` \n Build \n Using DUB \n Using default D compiler: \n dub build \n Specifying the D compiler:\n dub build --compiler=$LDC_HOME/bin/ldc2 \n Using make \n Using default D compiler: \n make \n Specifying the D compiler:\n make DMD=$LDC_HOME/bin/ldc2 \n Running Tests \n dub test \n License \n \n \n ext/roaring.c  and  ext/roaring.h  were generated from  CRoaring  source. Copyright 2016 The CRoaring authors. See the  LICENSE . \n \n \n D Wrapper for CRoaring: Copyright 2018 Yüce Tekol. See the  LICENSE . \n \n', 'openbsd \n Python bindings for some OpenBSD-specific APIs. Currently the following are supported: \n \n pledge \n unveil \n \n Change Log \n v0.1.0 (2019-05-03) \n * Initial release.\n \n Installation \n Openbsd is on PyPI. You can install it using pip: \n pip install openbsd\n \n Prerequisites \n \n OpenBSD 6.4 or better \n Python 3.6 or better OR Python 2.7 \n \n Usage \n Import  openbsd  first:\n python\nimport openbsd \n pledge \n See: https://man.openbsd.org/pledge.2 \n python\nopenbsd.pledge("stdio rpath")\nprint(open("/etc/resolv.conf")) \n Try removing the`rpath permission. \n unveil \n See: https://man.openbsd.org/unveil.2 \n python\nopenbsd.unveil("/etc", "r")\nprint(open("/etc/resolv.conf")) \n Try opening  /bin/ksh . \n Use  openbsd.unveil()  to lock down restrictions. \n Similar Projects \n \n PyPledge : Python binding for the OpenBSD pledge(2) system call. Uses ctypes. \n \n License \n (c) 2019 Yuce Tekol \n BSD', 'Jacl Specification 0.1 \n Jacl is a straightforward configuration language with the following features: \n \n Indentation does NOT matter. \n All values in the configuration have a non-ambiguous type. \n Separators between values are optional. \n Familiar syntax. \n NO magic, NO cheap tricks. \n \n Goals: \n \n The specification should be trivial to understand and easy to implement. \n A configuration file should be easy to edit and easy to understand. \n Trivial mistakes shouldn\'t change the  meaning  of the configuration file drastically. \n \n Non-goals: \n \n Anything not necessary in a configuration file. \n \n Change Log \n v0.1.3 (2019-06-30) \n \n Added experimental specification. \n Repeated keys are not allowed. \n \n v0.1.2 (2019-06-26) \n \n Single line comments start with  //  instead of  # . \n Numbers may optionally include underscores to increase their readability, e.g.,  12_345_678_900 == 12345678900 . \n \n v0.1.1 (2019-06-23) \n \n Added extended specification. \n \n v0.1.0 (2019-06-22) \n \n Initial release. \n \n Sample \n ```\n// This is a Jacl file. Boom. \n owner: {\n    name: "Phillips Redd"\n    age: 34\n    bio: """\n        Coder.\n        Loves cats.\n        """\n} \n database: {\n    server: "192.168.1.1"\n    ports: [8001 8002 8003]\n    connection_max: 5_000\n    enabled: true\n} \n // Some code in the config. Indentation matters only when it should:\nsource: trim"""\n    def main():\n        if True:\n            print("OK, fine")\n        else:\n            print("Not fine")\n    """ \n /*\n    You can indent as you please, even if it doesn\'t make sense.\n    Tabs or spaces. Jacl doesn\'t care.\n    Oh, and this is a multiline comment!\n*/\nservers: {\n    alpha: {\n        ip: "10.0.0.1"\n        dc: "eqdc10"\n    }\n    beta: {\n        ip: "10.0.0.2"\n        dc: "eqdc10"\n    }\n}\n \n clients: {\n    data: [\n        ["gamma", "delta"]\n        [1 2]\n    ]\n}\n``` \n Libraries \n Below are the libraries that implement Jacl: \n Library | Language | Basic Spec|Extended Spec|Experimental Spec| Home\n--------|----------|-----------|-------------|-----------------|-----\ngo-jacl | Go       | Yes       |Yes          | -               | https://github.com/yuce/go-jacl \n \n Table Of Contents \n \n Base Specification \n Grammar \n Definitions \n Empty File \n Comments \n Properties \n Property name \n Property value \n \n \n Data Types \n String \n Unsigned integer \n Signed integer \n Float \n Boolean \n Array \n Map \n \n \n \n \n Extended Specification \n Raw String Functions \n trim \n pin \n \n \n \n \n Experimental Specification \n Additional Data Types \n complex \n date \n datetime \n null \n time \n \n \n \n \n \n Base Specification \n Grammar \n The Jacl grammar is written using  ANTLR4 . You can find the grammar  here . \n Definitions \n Host language: The programming language that implements this specification. \n Empty File \n An empty file is a valid Jacl file. \n Comments \n Single line comments start with  // : \n // This is a single line comment.\n \n Multiline comments start with  /*  and end with  */ : \n /*\n    This is a multiline\n    comment\n*/\n \n Properties \n Properties are defined at the top level of a file, and they have the following structure: \n [name]: [value]\n \n Property name \n Property name is a string which contains alphanumneric characters,  -  or  _ : \n key: "fun"\nanother-key: "other-fun"\nyet_another_fun: "yet another fun"\n \n Optionally with quotes ( " ) around it: \n "new prefix": "more-fun-"\n \n Escape characters are not evaluated: \n "newest\\nprefix": "most-fun-"\n \n The name above is evaluated to  newest\\nprefix , NOT: \n newest\nprefix\n \n Repeated property names are not allowed. The following is invalid: \n key: "foo"\nkey: "bar"\n \n The maximum length of a property name is 1024 characters. \n Property value \n Property value can be one of the following types: \n \n String \n Unsigned integer \n Signed integer \n Float \n Boolean \n Array \n Map \n \n Data Types \n String \n A string is always quoted with  " ,  \'\'\'  or  """ . Escapes in the former one are expanded, but not in the latter ones. Latter ones can span multiple lines. \n A simple string: \n "A simple string"\n \n Simple string with escape: \n "Simple\\tstring\\nwith escape"\n \n A raw string: \n \'\'\'a raw string\'\'\'\'\n \n A multiline raw string: \n \'\'\'\nMultiline\nstring\n\'\'\'\n \n Another multiline raw string: \n """Another\nmultiline\nstring"""\n \n Unsigned integer \n Unsigned integers always have the biggest size the host language supports natively (usually 64 bits these days). \n If a number has one of the following prefixes below, then it is an unsigned integer: \n \n 0b : binary, e.g.,  0b10101 . \n 0o : octal, e.g.,  0o744 . \n 0d : decimal, eg.,  0d987123 . \n 0x . hexadecimal, e.g.,  0xBEEF . \n \n Underscore ( _ ) is allowed between digits: \n \n 0b1_0_1_0_1 == 0b10101 \n 0d13_490_567 == 0d13490567 \n 0o12_567 == 0o12567 \n 0xAB_CD_EF12 == 0xABCDEF12 \n \n Signed integer \n Signed integers always have the biggest size the host language supports natively (usually 64 bits these days). \n Signed integers cannot have  0  as a prefix, unless the number itself is  0 . So, this is invalid:  0145 . \n Signed integers may have the sign:  -  or  + . \n -123\n    123\n    +123 \n Underscore ( _ ) is allowed between digits:  12_345_678 == 12345678 . \n Float \n Floats always have the biggest size the host language supports natively (usually 64 bits these days). \n A float should always have a decimal point; that\'s how floats are distinguished from integers. \n Underscore ( _ ) is allowed between digits:  12_345_678.910_405 == 12345678.910405 . \n NaN  and  Infinity  are not supported in the base specification. \n Boolean \n One of  true  or  false . \n Array \n An array may contain items of arbitrary types. Items in the array are written between brackets  [ ... ] . Commas between items are optional. \n Empty array is  [] . \n This array: \n ["green" true [1 2 3] {name: "John" age: 30}]\n \n is equivalent to: \n ["green", true, [1, 2, 3] {name: "John", age: 30}]\n \n which in turn is equivalent to: \n [\n    "green"\n    true\n    [1, 2, 3]\n    {name: "John", age: 30}\n]\n \n Map \n A map contains keys and their mapping values. The keys can only be strings and they have the same restrictions as a property name. Keys and values of a map are written between curly braces:  { ... } . A key and a value is separated by colons  : . Commas are optional. \n Empty map is  {} . \n This map: \n {name: "John" age: 30 groups: ["sales" "management"]}\n \n is equivalent to: \n {name: "John", age: 30, groups: ["sales", "management"]}\n \n which in turn is equivalent to: \n {\n    name: "John"\n    age: 30\n    groups: ["sales", "management"]\n}\n \n Of course, a map can be many levels deep: \n {\n    title: "An important document"\n    attributes: {\n        permissions: {\n            read: true\n            write: false\n        }\n    }\n}\n \n Repeated keys are not allowed. The following is invalid: \n {\n    key: "foo"\n    key: "bar"\n}\n \n Extended Specification \n Raw String Functions \n trim \n Sets the column of the first non-space character of ther first non-empty line as the pin point. Removes empty lines at the beginning and at the end. Removes space characters from each line so the pin point is the first column. \n Example: \n ```\nsome_text: trim""" \n This is line 1.\n    This is line 2.\n\nThis is line 3.\n"""\n \n ``` \n In the example above, pin point is the 5th column. 4 spaces are removed from each line. So  some_text  property is set to: \n ```\nThis is line 1.\n    This is line 2. \n This is line 3.\n``` \n It is an error to have a line with less spaces than the pin point. Parsing the following text returns an error. The pin point is set to 5th column, but that causes loss of characters in the 2nd line: \n ```\ninvalid_text: trim""" \n This is line 1.\n \n This is line2.\n"""\n``` \n See the  pin  function to set the pin point manually. \n trim  is especially useful for indentation-sensitive text, like Python code: \n source: trim"""\n    import sys\n\n    def main():\n        args = sys.argv[1:]\n        if args:\n            print(f"{len(args)} arguments passed.")\n            for i, arg in enumerate(args):\n                print(f"  {i}. {arg}")\n        else:\n            print("No arguments passed.")\n\n    if __name__ == "__main__":\n        main()\n    """\n \n pin \n This function allows to set the pin point manually using the caret ( ^ ). The pin line and the empty lines before it are removed, but not the empty lines after it. \n Example: \n ```\nsome_text: pin""" \n ^\nThis is line 1.\n    This is line 2.\n\nThis is line 3.\n\n"""\n \n ``` \n In the example above, pin point is the 5th column. 4 spaces are removed from each line. So  some_text  property is set to: \n ```\nThis is line 1.\n    This is line 2. \n This is line 3. \n ``` \n Note that the blank line at the end of the text was not removed. \n The same text using a different pin point: \n ```\nsome_text: pin""" \n ^\n    This is line 1.\n        This is line 2. \n This is line 3.\n\n"""\n \n ``` \n This time, pin point is the 3rd column. 2 spaces are removed from each line. So  some_text  property is set to: \n ```\n  This is line 1.\n      This is line 2. \n This is line 3. \n ``` \n Just like  trim , it is an error if the pin point choice results in loss of non-space characters. The following returns an error: \n invalid_text: pin"""\n  ^\n    This is line 1.\nThis is line2.\n""" \n Moving the pin point to the 1st column makes that configuration valid: \n valid_text: pin"""\n^\n    This is line 1.\nThis is line2.\n""" \n The pin must be the first non-space character in the text, otherwise an error is returned: \n invalid_text: pin"""\n    Here, some text.\n    ^\n    Rest of the text.\n    """ \n It is an error to not have the pin: \n invalid_text: pin"""\n    Here, some text.\n    Rest of the text.\n    """ \n pin  preserves the space on empty lines in contrast with  trim  which removes them. \n Experimental Specification \n Additional Data Types \n Jacl\'s base specification is minimal but supports many of the data types required by a configuration system. Nevertheless, there are other data types which can be useful in some situations, such as a date, complex number, etc. \n Jacl supports additional data types via a syntax similar to a function call: \n [name]( [parameter 1], [parameter 2] ... )\n \n complex \n A complex number:  complex(REAL, IMAGINERY) \n // corresponds to: 3+4i\na1: complex(3, 4)\n \n date \n A date given in the  [year]-[month]-[day]  format: \n birthdate: date("2019-06-13")\n \n datetime \n A timestamp containg both the date and time in the  [year]-[month]-[day]T[24-hour]:[minute]:[second]Z[timezone]  format.  [timezone]  is optional: \n with-timezone: datetime("2019-06-13T22:47:31Z03:00")\nwithout-timezone: datetime("2019-06-13T22:47:31Z")\n \n null \n Just the null value: \n owner: null()\n \n time \n A time given in the  [24-hour]:[minute]:[second]  format: \n next_time: time("13:34:45")\n \n Thanks \n \n Alan Bernstein for the name of the language. \n Matt Jaffee for early feedback. \n \n License \n Copyright (c) 2019 Yuce Tekol. Licensed under  MIT .', '\n \n Go-Jacl \n This module implements the base and the extended specifications of the  Jacl configuration language . \n Change Log \n 0.2.0 (2019-07-06) \n \n Added  jacl.UnmarshalStruct  function, which unmarshals a struct from a  map[string]interface{} . This is used to achieve  default struct values  and  unmarshaling from multiple texts . \n Added underflow and overflow checks for signed/unsigned integers and floats. See:  Field Underflow/Overflow . \n \n 0.1.0 (2019-06-30) \n \n Initial release. \n \n Installation \n Requirements: \n \n Go 1.12 or better (not tried with versions below). \n \n The following should be enough to install it: \n go get github.com/yuce/go-jacl\n \n Jacl Specification \n See the  Jacl configuration language  for information about the configuration language. \n Usage \n Go-Jacl has a single function,  jacl.Unmarshal , to decode configuration from text into a  map[string]interface{}  or a pointer to a struct. \n Example: \n ``go\ntext := \n    // Sample configuration \n bind: "https://01.pilosa.local:10101"\ndata-dir: "/tmp/data"\n\ncluster: {\n    coordinator: true    \n}\n\ntls: {\n    certificate: "pilosa.local.crt"\n    key: "pilosa.local.key"\n    skip-verify: true\n}\n\ngossip: {\n    seeds: [\n        "01.pilosa.local:15000"\n        "02.pilosa.local:15000"\n    ]\n    port: 15000\n    key: "pilosa.local.gossip32"\n}\n \n ` \n // Decode to a map\nconfig := map[string]interface{}{}\nerr := jacl.Unmarshal(text, &config)\nif err != nil {\n    // handle the error\n} \n // Decode to a struct\ntype ClusterConfig struct {\n    Coordinator     bool    jacl:"coordinator" \n    SomeLegacyField string  jacl:"-"  // This field is skipped.\n} \n type TLSConfig struct {\n    CertificatePath string  jacl:"certificate" \n    KeyPath         string  jacl:"key" \n    SkipVerify      bool    jacl:"skip-verify" \n} \n type GossipConfig struct {\n    Seeds   []string  jacl:"seeds" \n    Port    int       jacl:"port" \n    KeyPath string    jacl:"key" \n} \n type Config struct {\n    DataDir string         jacl:"data-dir" \n    Bind    string         jacl:"bind" \n    Cluster ClusterConfig  jacl:"cluster" \n    TLS     TLSConfig      jacl:"tls" \n    Gossip  GossipConfig   jacl:"gossip" \n} \n config := Config{}\nerr := jacl.Unmarshal(text, &config)\nif err != nil {\n    // handle the error\n}\n``` \n Decoding Into Structs \n When decoding into structs: \n \n Only exported fields of the struct are considered. \n All fields of the struct must have corresponding values in the configuration, otherwise an error is returned. \n The field name must match the property/map key, unless  jacl:"KEY_NAME"  used in the field definition. In that case the configuration key  KEY_NAME  is matched to the corresponding field. \n Use  jacl:"-"  in order to skip a field. \n \n Supported Go Data Types \n The following are the Go data types which are mapped from their Jacl counterparts. Note that trying to unmarshal to a field with a different type (e.g., a signed integer to  uint  vice versa, or a float to  int ) returns an error: \n Jacl Type        | Go Type                | Allowed Field Types\n-----------------|------------------------|--------------------\nString           | string                 | string\nUnsigned integer | uint64                 | uint, uint8, uint16, uint32, uint64\nSigned integer   | int64                  | int, int8, int16, int32, int64\nFloat            | float64                | float32, float64\nBoolean          | bool                   | bool\nArray            | []interface{}          | []interface{}, []T\nMap              | map[string]interface{} | map[string]interface{}, map[string]T \n In the table above  T  is one of  bool ,  string ,  int ,  int8 ,  int16 ,  int32 ,  int64 ,  uint ,  uint8 ,  uint16 ,  uint32 ,  uint64 ,  float32 ,  float64 . \n Field Underflow/Overflow \n If unmarshalling to a field underflows or overflows the chosen data type, then an error is returned: \n go\ntype Config struct {\n    Number int8\n}\nconfig := Config{}\nerr := jacl.Unmarshal("Number: 128", &config) \n err  above is not  nil , since 128 is bigger than math.MaxInt8. \n Default Struct Values \n Go-Jacl requires every field of a struct to be set on unmarshal unless a field is skipped with  jacl:"-" . So, if a property is missing in the configuration  jacl.Unmarshal  would return an error. \n Consider the following struct: \n go\ntype C struct {\n    F1 string\n    F2 int\n} \n In order to have defaults for fields  F1  and  F2 , you can pass a map with defaults to  jacl.Unmarshal : \n ```go\ndefaults := map[string]interface{}{\n    "F1": "default string",\n    "F2": 54,\n} \n text :=  F1: "modified string" \nerr := jacl.Unmarshal(text, &defaults)\nif err != nil {\n    // handle the error\n}\n``` \n The  defaults  map contains the following values after unmarshalling: \n go\nmap[string]interface{}{\n    "F1": "modified string",\n    "F2": int64(54),\n} \n Since the properties for all fields are set, we can pass that map to  jacl.UnmarshalStruct : \n go\nconfig := C{}\nerr = jacl.UnmarshalStruct(defaults, &config)\nif err != nil {\n    t.Fatal(err)\n} \n The value of  config  is: \n go\nC{\n    F1: "modified string",\n    F2: 54,\n} \n Unmarshaling From Multiple Texts \n Suppose we separated our configuration into multiple files since there are lots of properties to be set. Instead of having separate structs for each file, we want to have a single struct. We can use the same  jack.UnmarshalStruct  technique in the previous section to accomplish that. \n This is the sample struct: \n go\ntype C struct {\n    F1 string\n    F2 string\n    // ...\n    F9 string\n} \n These are the contents of the configuration files: \n go\ntexts := []string{\n    `F1: "field 1"`,\n    `F2: "field 2"`,\n    // ...\n    `F9: "field 9"`,\n} \n We use the same map to unmarshal each text: \n go\nprops := map[string]interface{}{}\nfor _, text := range texts {\n    err := jacl.Unmarshal(text, &props)\n    if err != nil {\n        // handle the error\n    }\n} \n Finally, unmarshal the map to a struct: \n go\nconfig := C{}\nerr := jacl.UnmarshalStruct(props, &config)\nif err != nil {\n    // handle the error\n} \n TODO \n \n Maps of typed slices and slices of typed maps are not yet supported when unmarshalling struct fields. E.g.,  Field1 []map[string]interface{}  is OK, but  Field2 []map[string]int  is not supported yet. \n \n License \n Copyright (c) 2019-2021 Yuce Tekol. Licensed under  MIT .', 'CloudEvents Java Benchmarks \n Benchmarks of CloudEvents implementations for Java. \n Usage \n Requirements: \n \n JDK 8 \n \n Run  make benchmark  on Linux, MacOS, etc. On Windows  gradlew --no-daemon jmh  should work. \n Benchmarks \n CloudEvents SDK for Java \n Official CloudEvents SDK for Java: https://github.com/cloudevents/sdk-java \n JSON \n CloudEvents 1.0 JSON spec. \n \n benchmarkSdkJsonDecodeEvent : Decode a single event \n benchmarkSdkJsonEncodeEvent : Encode a single event \n \n ScalePlan CloudEvents for Java \n SPCE CloudEvents for Java: https://github.com/scaleplandev/spce-java \n JSON \n CloudEvents 1.0 JSON spec. \n \n benchmarkSpceJsonDecodeEvent : Decode a single event \n benchmarkSpceJsonEncodeEvent : Encode a single event \n benchmarkSpceJsonDecodeEventBundle10 : Decode a bundle of 10 events \n benchmarkSpceJsonEncodeEventBundle10 : Encode a bundle of 10 events \n \n Avro \n Using CloudEvents 1.0 Avro schema at https://github.com/cloudevents/spec/blob/v1.0/spec.avsc \n \n benchmarkAvroDecodeEvent : Decode a single event \n benchmarkAvroEncodeEvent : Encode a single event \n \n Avro SPCE \n Using SPCE 1.0 Avro schema at: https://github.com/scaleplandev/spce-java/blob/master/etc/cloudevents_spce_spec.avsc \n \n benchmarkAvroSpceDecodeEvent : Decode a single event \n benchmarkAvroSpceEncodeEvent : Encode a single event \n benchmarkAvroSpceDecodeEventBundle10 : Decode a bundle of 10 events \n benchmarkAvroSpceEncodeEventBundle10 : Encode a bundle of 10 events \n \n Sample Result \n Below is a result from a typical run on my system. The exact numbers may be different on your system, but the overall picture should be the same. \n Benchmark                                                  Mode  Cnt    Score    Error   Units\nEventCodecBenchmark.benchmarkAvroDecodeEvent              thrpt   10  253.527 ±  8.203  ops/ms\nEventCodecBenchmark.benchmarkAvroEncodeEvent              thrpt   10  269.207 ±  9.189  ops/ms\nEventCodecBenchmark.benchmarkAvroSpceDecodeEvent          thrpt   10  461.694 ± 16.964  ops/ms\nEventCodecBenchmark.benchmarkAvroSpceDecodeEventBundle10  thrpt   10   39.816 ±  7.879  ops/ms\nEventCodecBenchmark.benchmarkAvroSpceEncodeEvent          thrpt   10  408.037 ± 17.207  ops/ms\nEventCodecBenchmark.benchmarkAvroSpceEncodeEventBundle10  thrpt   10   40.102 ±  3.235  ops/ms\nEventCodecBenchmark.benchmarkSdkJsonDecodeEvent           thrpt   10  268.276 ± 16.190  ops/ms\nEventCodecBenchmark.benchmarkSdkJsonEncodeEvent           thrpt   10  319.856 ± 12.765  ops/ms\nEventCodecBenchmark.benchmarkSpceJsonDecodeEvent          thrpt   10  519.292 ± 19.978  ops/ms\nEventCodecBenchmark.benchmarkSpceJsonDecodeEventBundle10  thrpt   10   50.152 ± 10.112  ops/ms\nEventCodecBenchmark.benchmarkSpceJsonEncodeEvent          thrpt   10  942.562 ± 16.331  ops/ms\nEventCodecBenchmark.benchmarkSpceJsonEncodeEventBundle10  thrpt   10   98.450 ±  5.817  ops/ms\n \n License \n (c) 2020 Scale Plan Yazılım A.Ş. https://scaleplan.io \n Licensed under  Apache 2.0 . See the  LICENSE . \n Copyright 2020 Scale Plan Yazılım A.Ş.\n\nLicensed under the Apache License, Version 2.0 (the "License");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an "AS IS" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n', 'vertx-pg-client timestamptz Sample \n See the discussion at: https://groups.google.com/u/1/g/vertx/c/vO2Y8DznNEA \n \n make build \n make run-postgres \n java -jar ./build/libs/vertx-pg-client-timestamptz-1.0-SNAPSHOT-all.jar \n', 'Hazelcast Go Client V3 + Module Sample \n Requirements \n \n Hazelcast 3 \n Go 1.15+ \n \n Usage \n \n \n Start Hazelcast 3 on localhost:5701 (defaults) \n \n \n Clone the repo somewhere  outside  your  GOPATH :\n   git clone https://github.com/yuce/hazelcast-go-client-v3-modules-sample.git \n \n \n Switch to the  hazelcast-go-client-v3-modules-sample  directory:\n   cd hazelcast-go-client-v3-modules-sample \n \n \n Run  main.go :\n   go run main.go \n \n', 'Hazelcast Go Client v1 Sample \n Requirements:\n* Hazelcast 4.x or better\n* Go 1.17 or better \n Install: \n \n Run  go mod tidy  to install the dependencies. \n Update  main.go  to match the address of your Hazelcast cluster if it does not match the default:  localhost:5701 . \n Run  main.go :\n     $ go run main.go \n', 'Hazelcast Go Client Management Interface Sample \n Build the app using:\n go build . \n The default client expects an Hazelcast instance at the default address and port: localhost:5701. \n On Linux and MacOS, run the app using:\n ./hzmc \n On Windows, run the app using:\n hzmc', 'Hazelcast Go Client Hazelcast Cloud Integration Sample \n Build the app using:\n go build . \n Update  main.go  with the Hazelcast Cloud cluster name and token. \n On Linux and MacOS, run the app using:\n ./hazelcast-go-client-cloud \n On Windows, run the app using:\n hazelcast-go-client-cloud.exe', 'Hazelcast Go Client External Discovery Sample \n Build the app using:\n go build . \n Make sure you set the correct external IP in  main.go . \n On Linux and MacOS, run the app using:\n ./hazelcast-go-client-external-discovery \n On Windows, run the app using:\n hazelcast-go-client-external-discovery.exe', 'Hazelcast CLC (Command Line Client) \n Installation \n There are three ways you can install the command line client: \n \n Using  Brew  (Linux, MacOS) \n Using script installation (Linux, MacOS) \n Install wizard (Windows only) \n \n Installing with Brew \n brew tap hazelcast/homebrew-hz\nbrew install hazelcast-commandline-client \n To have a superior experience, enable autocompletion on Brew: \n- For  Bash  users:\n  - Execute  brew install bash-completion  and follow the printed "Caveats" section. \n    Example instruction:\n    Add the following line to your ~/.bash_profile:\n     [[ -r "/home/ubuntu/.linuxbrew/etc/profile.d/bash_completion.sh" ]] && . "/home/ubuntu/.linuxbrew/etc/profile.d/bash_completion.sh" \n Note that paths may differ depending on your installation, so you should follow the Caveats section on your system. \n \n For  Zsh  users \n Follow https://docs.brew.sh/Shell-Completion#configuring-completions-in-zsh  \n \n Installation with script \n curl https://raw.githubusercontent.com/hazelcast/hazelcast-commandline-client/main/scripts/install.sh | bash \n Installing using the Windows Install Wizard \n If you are using a recent version of Windows, you may prefer to install CLC using the Install Wizard we provide.\nYou can download the Install Wizard at: https://github.com/hazelcast/hazelcast-commandline-client/releases \n Uninstallation \n Depending on how you install the command line client, choose the uninstallation option. \n Uninstallation using Brew \n brew uninstall hazelcast-commandline-client\nbrew untap hazelcast/homebrew-hz \n Uninstallation using script \n ~/.local/share/hz-cli/bin/uninstall.sh \n Uninstallation using the Windows Install Wizard \n If you have installed CLC using the Windows Install Wizard, you can use the Settings/Apps menu to uninstall it. \n Usage \n Make sure a Hazelcast 4 or Hazelcast 5 cluster is running. \n ``` \n Start interactive shell \n hzc \n Print help \n hzc --help \n Non-interactive mode \n hzc map --name myMap put --key myKey --value myValue\n``` \n Keyboard Shortcuts \n Emacs-like keyboard shortcuts are available by default (these also are the default shortcuts in Bash shell). \n | Key Binding         | Description                                     |\n|---------------------|-------------------------------------------------|\n|  Ctrl + A  | Go to the beginning of the line (Home)          |\n|  Ctrl + E  | Go to the end of the line (End)                 |\n|  Ctrl + P  | Previous command (Up arrow)                     |\n|  Ctrl + N  | Next command (Down arrow)                       |\n|  Ctrl + F  | Forward one character                           |\n|  Ctrl + B  | Backward one character                          |\n|  Ctrl + D  | Delete character under the cursor               |\n|  Ctrl + H  | Delete character before the cursor (Backspace)  |\n|  Ctrl + W  | Cut the word before the cursor to the clipboard |\n|  Ctrl + K  | Cut the line after the cursor to the clipboard  |\n|  Ctrl + U  | Cut the line before the cursor to the clipboard |\n|  Ctrl + L  | Clear the screen                                | \n \\\nWith few additions: \n | Key Binding          | Description                             |\n|----------------------|-----------------------------------------|\n|  Ctrl + C   | Cancel running command or close the app |\n|  Ctrl + ->  | Go to the end of to next word           |\n|  Ctrl + <-  | Go to the start of the previous word    | \n Connecting to Hazelcast Cloud \n The cluster creation and retrieving connection info can be done directly in command line using  Hazelcast Cloud CLI . \n \n Authenticate to Hazelcast Cloud account: \n \n hzcloud login\n  -  API Key: SAMPLE_API_KEY\n  -  API Secret: SAMPLE_API_SECRET \n \n Create a cluster: \n \n ```\n  hzcloud starter-cluster create \\\n  --cloud-provider=aws \\\n  --cluster-type=FREE \\\n  --name=mycluster \\\n  --region=us-west-2 \\\n  --total-memory=0.2 \\\n  --hazelcast-version=5.0 \n \n Cluster 2258 is creating. You can check the status using hzcloud starter-cluster list.\n  ``` \n \n \n Wait until the cluster is running: \n \n ```\n  hzcloud starter-cluster list \n \n ┌────────┬────────────┬─────────┬─────────┬──────────────┬────────────────┬───────────┬─────────┐\n  │ Id     │ Name       │ State   │ Version │ Memory (GiB) │ Cloud Provider │ Region    │ Is Free │\n  ├────────┼────────────┼─────────┼─────────┼──────────────┼────────────────┼───────────┼─────────┤\n  │ 2285   │ mycluster  │ PENDING │ 5.0     │          0.2 │ aws            │ us-west-2 │ true    │\n  ├────────┼────────────┼─────────┼─────────┼──────────────┼────────────────┼───────────┼─────────┤\n  │ Total: │ 1          │         │         │              │                │           │         │\n  └────────┴────────────┴─────────┴─────────┴──────────────┴────────────────┴───────────┴─────────┘ \n \n ... \n hzcloud starter-cluster list \n \n ┌────────┬────────────┬─────────┬─────────┬──────────────┬────────────────┬───────────┬─────────┐\n  │ Id     │ Name       │ State   │ Version │ Memory (GiB) │ Cloud Provider │ Region    │ Is Free │\n  ├────────┼────────────┼─────────┼─────────┼──────────────┼────────────────┼───────────┼─────────┤\n  │ 2285   │ mycluster  │ RUNNING │ 5.0     │          0.2 │ aws            │ us-west-2 │ true    │\n  ├────────┼────────────┼─────────┼─────────┼──────────────┼────────────────┼───────────┼─────────┤\n  │ Total: │ 1          │         │         │              │                │           │         │\n  └────────┴────────────┴─────────┴─────────┴──────────────┴────────────────┴───────────┴─────────┘ \n \n ``` \n \n Get the cluster name and discovery token: \n \n ```\n  # Get cluster name\n  hzcloud starter-cluster get --cluster-id 2285 --output json | jq \'.releaseName\' \n \n "ex-1111" \n \n # Get discovery token\n  hzcloud starter-cluster get --cluster-id 2285 --output json | jq \'.discoveryTokens[].token\' \n \n "exampleHashDiscoveryToken"\n  ``` \n \n \n Connect to the cluster using the command line client using the credentials above: \n \n hzc --cluster-name <CLUSTER NAME> --cloud-token <DISCOVERY TOKEN> \n SSL Configuration \n You can use the following configuration file to enable SSL support:\n ssl:\n    enabled: true\n    servername: "HOSTNAME-FOR-SERVER"\n    # or: insecureskipverify: true\nhazelcast:\n  cluster:\n    security:\n      credentials:\n        username: "OPTIONAL USERNAME"\n        password: "OPTIONAL PASSWORD"\n    name: "CLUSTER-NAME"\n    network:\n      addresses:\n        - "localhost:5701" \n Mutual authentication is also supported:\n ssl:\n    enabled: true\n    servername: "HOSTNAME-FOR-SERVER"\n    # insecureskipverify: true\n    capath: "/tmp/ca.pem"\n    certpath: "/tmp/cert.pem"\n    keypath: "/tmp/key.pem"\n    keypassword: "PASSWORD FOR THE KEY"\nhazelcast:\n  cluster:\n    security:\n      credentials:\n        username: "OPTIONAL USERNAME"\n        password: "OPTIONAL PASSWORD"\n    name: "CLUSTER-NAME"\n    network:\n      addresses:\n        - "localhost:5701" \n Cloud SSL configuration:\n ssl:\n    enabled: true\n    capath: "/tmp/ca.pem"\n    certpath: "/tmp/cert.pem"\n    keypath: "/tmp/key.pem"\n    keypassword: "PASSWORD FOR THE KEY"\nhazelcast:\n  cluster:\n    name: "CLUSTER NAME"\n    cloud:\n      token: "HAZELCAST CLOUD TOKEN"\n      enabled: true \n More examples \n ``` \n Get from a map \n hzc map get --name my-map --key my-key \n Put to a map \n hzc map put --name my-map --key my-key --value my-value \n Get state of the cluster \n hzc cluster get-state \n Work with JSON values \n hzc map put --name map --key b --value-type json --value \'{"english":"Greetings"}\'\nhzc map get --name map --key b \n \n {"english":"Greetings"} \n \n Change state of the cluster \n Either of these: active | frozen | no_migration | passive \n hzc cluster change-state --state  \n Shutdown the cluster \n hzc cluster shutdown \n Get the version of the cluster \n hzc cluster version\n``` \n Configuration \n ``` \n Using Custom Config \n : path of the target configuration \n hzc --config  \n Connect to a Local Hazelcast cluster \n : addresses of the members of the Hazelcast cluster \n e.g. 192.168.1.1:5702,192.168.1.2:5703,192.168.1.3:5701 \n : name of the cluster \n hzc --address   --cluster-name  \n``` \n Building from source \n The following targets are tested and supported.\nThe prior versions of the given targets would also work, but that\'s not tested.  \n \n Ubuntu 18.04 or better. \n MacOS 12 or better. \n Windows 10 or better. \n \n Requirements \n \n Go 1.18 or better \n Git \n GNU Make (on Linux and MacOS) \n Command Prompt or Powershell (on Windows)  \n go-winres: https://github.com/tc-hib/go-winres (on Windows) \n \n 1. Download the source \n You can acquire the source using Git: \n git clone https://github.com/hazelcast/hazelcast-commandline-client.git \n Or download the source archive and extract it: \n https://github.com/hazelcast/hazelcast-commandline-client/archive/refs/heads/main.zip \n 2. Build the project \n cd hazelcast-commandline-client\nmake \n Finally, run the project \n CLC starts the interactive mode by default. \n On Linux and MacOS:\n ./hzc \n On Windows:\n hzc.exe', 'Hazelcast Python Client Kerberos Authentication Support \n Requirements \n \n Hazelcast Enterprise 4.1 and up, \n Hazelcast Python Client 4.2.1 and up, \n Python 3.5 or better, \n Linux with a recent Kernel (tested on Debian Stretch and Buster), \n Kerberos 5 shared library ( libkrb5-dev  package on Debian/Ubuntu systems), \n C compiler is required to build the python-gssapi dependency. You may opt to use the system provided\n  package ( python3-gssapi  on Debian/Ubuntu systems), \n \n Install \n pip install -U git+https://github.com/hazelcast/hazelcast-python-client-kerberos.git@v1.0.0#egg=hazelcast-kerberos \n Usage \n Enabling Kerberos authentication for Hazelcast involves server and client configuration. \n Client Configuration \n On the client side, a Kerberos token provider is created and passed to Hazelcast Python Client. The token provider authenticates to KDC using the given credentials, receives and caches the Kerberos ticket and retrieves a token. The token is passed to the server-side by Hazelcast Python Client for authenticating to the server. \n Using a Cached Ticket \n If a Kerberos ticket was already cached, probably using the  kinit  command, then token provider can be created with no parameters: \n python\ntoken_provider = hzkerberos.TokenProvider() \n Authentication using a Keytab File \n You can use a keytab file for retrieving the Kerberos ticket. In this case, the principal and full path of the keytab file must be specified: \n python\ntoken_provider = hzkerberos.TokenProvider(principal="jduke@EXAMPLE.COM", keytab="/etc/krb5.keytab") \n Authentication using a Password \n You can also use a password retrieving the Kerberos ticket. In this case, the principal and the password must be specified: \n python\ntoken_provider = hzkerberos.TokenProvider(principal="jduke@EXAMPLE.COM", password="s3cr3t") \n Overriding the Generated Service Principal Name \n A service principal name (SPN) has the following structure: \n [SERVICE-NAME-PREFIX/][SERVICE-HOST][@REALM]\n \n By default, the service principal name is generated automatically, using the following components:\n* SERVICE-NAME-PREFIX:  hz/ \n* SERVICE-HOST: Host or IP address of the member\n* REALM: Blank \n You can override SPN generation by specifying one or many of  spn  , prefix  and  realm  parameters:\n python\ntoken_provider = hzkerberos.TokenProvider(spn="my-service", prefix="hz", realm="EXAMPLE.COM") \n Creating the Hazelcast Python Client \n Once the token provider is created, you can pass it to the Hazelcast Python Client constructor. The token provider will be used by the client during authentication to the server. \n python\nclient = hazelcast.HazelcastClient(token_provider=token_provider) \n Server Configuration \n Server security configuration (starting with 4.1) is documented in the  Security  section of the main Hazelcast documentation. Kerberos authentication is documented in the  Security Reams  sub-section. \n The Kerberos support in Hazelcast has 2 configuration parts: identity and authentication. The identity part is responsible for retrieving the service ticket from Kerberos KDC (Key Distribution Center). The authentication part verifies the service tickets. \n The following XML fragment can be used as an example of a working server configuration. However, it is recommended to read the completed documentation in order to fully understand the security aspects of Kerberos. \n xml\n<security enabled="true">\n    <member-authentication realm="kerberosRealm"/>\n    <client-authentication realm="kerberosRealm"/>\n    <realms>\n        <realm name="kerberosRealm">\n            <authentication>\n                <kerberos>\n                    <relax-flags-check>true</relax-flags-check>\n                    <use-name-without-realm>false</use-name-without-realm>\n                    <keytab-file>/common/krb5.keytab</keytab-file>\n                    <principal>hz/${host}@EXAMPLE.COM</principal>\n                </kerberos>\n            </authentication>\n            <identity>\n                <kerberos>\n                    <realm>EXAMPLE.COM</realm>\n                    <keytab-file>/common/krb5.keytab</keytab-file>\n                    <principal>hz/${host}@EXAMPLE.COM</principal>\n                </kerberos>\n            </identity>\n        </realm>\n    </realms>\n</security> \n Notes \n \n Only the default cache is supported for storing/loading the Kerberos ticket. The default cache name is resolved using  krb5_cc_default_name  call. \n \n Running the Tests \n The Docker setup in this setup is strictly for test purposes. \n Running the tests requires Docker Compose. \n \n \n Put  hazelcast-enterprise-4.2.2.jar ,  log4j-api-2.14.1.jar  and  log4j-core-2.14.1.jar  in the  docker  folder.  This is a temporary step to be removed in another iteration \n \n \n Create an  .env  file with the Hazelcast Enterprise license key in the root of the project:\n     HZ_LICENSEKEY=... \n \n Run docker compose, which creates KDC/KAdmin and two Hazelcast Enterprise containers:\n     docker-compose up \n Build the Docker image for tests, this is required only once:\n     docker build -t hazelcast-python-client-kerberos_app:latest -f app.Dockerfile . \n Run the app container to run the tests whenever the code changes:\n     docker run -it --rm --name hzkerberos_test \\\n        --env-file .env \\\n        --network=hazelcast-python-client-kerberos_hz \\\n        -v `pwd`:/home/hz/app \\\n        -v hazelcast-python-client-kerberos_common:/common \\\n        hazelcast-python-client-kerberos_app:latest test \n \n Notes \n \n When a container runs, it executes the corresponding default action, e.g.,  test  for the app container. In order to get a shell instead of the default action, you can use the  /bin/bash  command. \n If  VERBOSE  environment variable is  1 , tracing is enabled for entrypoint scripts. \n LOGGING_LEVEL  environment variable for Hazelcast containers controls the logging level. \n \n Accessing KDC \n When the docker compose setup is running, you can access KDC by accessing its container:\n docker-compose exec kdc /bin/bash \n And starting  kadmin.local :\n rlwrap kadmin.local', 'Client Compatibility Tests \n This repository contains Github Actions workflow files and\nutility scripts used by them to test the following suites: \n Backward Compatibility Tests \n \n \n When a new IMDG server is ready to be released, and we want to \nverify that it is backward compatible with released clients of \ndifferent languages.   \n \n \n When a new client is ready to be released, and we want to verify\nthat it is backward compatible with released IMDG servers. \n \n \n Github Actions for the following test scenarios are meant to be\nstarted manually. To use them, navigate to the  Actions  tab of \nthe repository, select the workflow you are interested in to test, \nsupply the necessary inputs and run the workflow. \n The scripts will use  rel-scripts \nrepository to get information regarding the released clients or servers. \n Kubernetes Compatibility Tests \n \n \n When a new client is ready to release, we can verify it is working\nproperly as a docker image in Kubernetes. Docker image is created with the\nbranch set as an input to workflow. Cluster is created with\nhelm chart. Workflow is also verifying cluster \nscale up, down, delete scenarios. \n \n \n When a new client is ready to release, we can verify it is able to connect\na cluster which is running on GKE. \n \n', 'Go SQL Driver for Hazelcast Sample \n This project demonstrates how to use the Hazelcast  database/sql  driver. \n Build \n \n Run  go mod tidy  to download the latest Hazelcast Go client with SQL support \n Run  make sqlrun  which builds the  sqlrun  binary.  \n \n Usage \n Make sure you use  Hazelcast Platform 5.0  and Jet is enabled in your Hazelcast configuration (it is by default): \n <jet enabled="true"/>\n \n sqlrun \n sqlrun  can execute SQL commands from a file. You can optionally specify connection arguments: \n $ sqlrun -f myfile.sql\n$ sqlrun -c "10.20.30.40:5701;ClusterName=prod" -f myfile.sql\n \n It is also possible to execute an SQL string: \n $ sqlrun -e "select * from employees"\n', 'Hazelcast Go Client Logger Sample \n This sample demonstrates how to use a  zap  custom logger for the  Hazelcast Go Client . \n Prerequisites \n \n Go 1.15 or better. \n The example uses  Hazelcast Cloud , which is free to use. But it can be adapted to use an on-prem Hazelcast instance.  \n \n Usage \n \n Clone this repo:\n    git clone https://github.com/yuce/hazelcast-go-client-logger-sample \n Download the dependencies:\n    go mod tidy \n Update  clusterName  and  cloudToken  constants in  main.go  with the correct values. \n Run the sample\n    go build .\n   ./hazelcast-go-client-logger-sample   \n \n License \n Apache 2 License . \n Copyright (c) 2008-2021, Hazelcast, Inc. All Rights Reserved.', 'hzlistindexes \n Requirements \n \n Go 1.18 or better \n \n Build \n Just running  make  will build the executable  hzlistindexes \n Usage \n ```\n$ ./hzlistindexes -c config.json -m employees \n 2022/04/28 14:59:17 INFO : trying to connect to cluster: dev\n2022/04/28 14:59:17 INFO : connected to cluster: dev\n2022/04/28 14:59:17 INFO :  \n Members {size:1, ver:1} [\n        Member 127.0.0.1:5701 - 21c4344f-e329-4327-afc0-63dfc9c6a39d\n] \n 001: name: age, type: 0 on key: __key, attrs: [age]\n``` \n Note that the JSON configuration is specific to the Go client. Find more at the  documentation .', 'SQL Elephant \n \n Start hazelcast first \n \n For all three folders: \n \n First install dependencies with "npm install" \n Then you can start the app with "npm start" for backend and bot, "npm run serve" for frontend \n \n Start order:\n1. Backend\n2. Frontend\n3. Bot', 'Hazelcast Go Client v1.3.0 Near Cache Sample \n This project contains the code sample at https://github.com/hazelcast/hazelcast-go-client/tree/previews/v1.3.0/examples/map/nearcache/json_config with the correct  go.mod  file. \n It is also possible to use programmatic configuration, see: https://github.com/hazelcast/hazelcast-go-client/tree/previews/v1.3.0/examples/map/nearcache/programmatic_config \n Requirements \n \n Hazelcast 5.x \n Go 1.18 \n \n Usage \n Git clone this project and  cd  into it: \n $ git clone https://github.com/yuce/hazelcast-go-client-v1.3.0-nearcache-sample.git\n$ cd hazelcast-go-client-v1.3.0-nearcache-sample\n \n Build the project with  make  or: \n $ go build -o nearcache-sample .\n \n That creates the  nearcache-sample .\nRun that binary with a configuration file similar to  config1.json  in the root: \n ./nearcache-sample config1.json\n \n Configuration \n A sample JSON configuration is provided in the  config1.json  file.\nCheck out the following for more configuration options: \n \n Near Cache configuration \n General configuration \n \n Documentation \n See: https://github.com/hazelcast/hazelcast-go-client/blob/previews/v1.3.0/proxy_map.go \n Copyright \n Copyright (c) 2022, Hazelcast, Inc. All Rights Reserved. \n Apache 2 License .', 'waitport \n waitport  is a simple utility that waits for a TCP port to be connectable.\nIt is very small, about 10KBs when compiled and stripped. \n Build \n Requirements: \n \n Linux (May work on other POSIX). \n C compiler with C99 support. \n \n Using Make \n Just:\n $ make \n Using CMake \n $ mkdir build && cd $_\n$ cmake ..\n$ make \n Usage \n Wait indefinitely on port 9701 at localhost, try every second:\n $ waitport 9701 \n Wait indefinitely on port 9701 at foobar.com, try every second:\n $ waitport foobar.com 9701 \n Wait 10 seconds on port 9701, try every second:\n $ waitport 9701 -t 10 \n Wait 10 seconds on port 9701, try every 100 ms:\n $ waitport 9701 -t 10 -s 0.1 \n License \n MIT', 'Hazelcast Go Client Benchmark \n Requirements \n \n Go 1.18 \n \n Build \n $ git clone https://github.com/yuce/hazelcast-go-client-benchmark.git\n$ go build .\n \n Usage \n Run  ./hazelcast-go-client-benchmark  with a configuration file, e.g.,: \n $ ./hazelcast-go-client-benchmark configs/128x1024_500keys_nc.json\n \n Configuration \n Here\'s a sample JSON configuration: \n   "Client": {\n    "Cluster": {\n      "Network": {\n        "Addresses": [\n          "localhost:5701"\n        ]\n      }\n    },\n    "NearCaches": [\n      {\n        "Name": "mymap*",\n        "InMemoryFormat": "binary",\n        "Eviction": {\n          "Policy": "lru",\n          "Size": 500\n        }\n      }\n    ]\n  },\n  "MapName": "mymap1",\n  "KeyCount": 500,\n  "Repeat": 100,\n  "GoroutineCount": 1,\n  "EntryGenerator": "sized128x1024",\n  "Warmup": false\n}\n \n Check out https://pkg.go.dev/github.com/hazelcast/hazelcast-go-client#hdr-Configuration for the  Client  configuration.\nThe program configuration is in  config.go  in this project.\nThe config fields can be mapped to JSON fields straightforwardly.\nThe following program configuration keys are supported: \n \n MapName : Map name to be used during the benchmark. \n KeyCount : Number of unique keys to be used. \n Repeat : Number of  Map.Get s to do (after the warmup, if configured) \n GoroutineCount : Number of goroutines to do  Map.Get s.  \n Warmup : If  true , runs populates the Near Cache before starting the benchmark.  \n Client : Hazelcast client configuration. Check out https://pkg.go.dev/github.com/hazelcast/hazelcast-go-client#hdr-Configuration.  \n EntryGenerator : Entry generator to use for generating keys and values. See the section below. \n \n Entry Generator \n Checkout  entry_generators.go  for sample entry generators and how to add yours.\nHere are the builtin ones: \n \n identity : Both keys and values are the  int64  index.  \n sized128x1024 : String key of size 128 bytes, value of size 1024 bytes. \n sized128x4096 : String key of size 128 bytes, value of size 4096 bytes. \n \n License \n See:  LICENSE', 'gRPC Server that uses a Hazelcast Client \n Build \n To create a runnable jar (UNIX-like): \n ./gradlew shadowJar\n \n To create a runnable jar (Windows): \n gradlew shadowJar\n \n Run \n java -jar ./build/libs/ktGrpcServer-1.0-SNAPSHOT-all.jar\n']
shuber,["Solar Theme for Jekyll \n A stylish theme for  Jekyll  blogs, based on the  Solarized  color palette. \n \n Features \n \n Two color schemes  — One for Solarized Dark and one for Solarized Light. Just swap the reference to the  colors-dark.css  file with  colors-light.css  if you don't like light-on-dark. \n Linkblog support  — Solar will turn your post title into an external link if you add  external-url: http://example.org  to a post's YAML front matter. \n Responsive Design  — Solarized adapts to fit any screen size. \n \n Installation \n There are two ways to use Solar. You can either clone-and-go, copying the repository and tweaking the contents to taste, or you can cherry-pick the files you want and integrate them into an existing Jekyll instance.  \n If you're starting a new blog, you want to clone-and-go. Just  git clone https://github.com/redwallhp/solar-theme-jekyll.git , make any changes you want to the template, pages or  _config.yml  and start blogging with Jekyll. Easy. \n If you're wanting to replace the theme of an existing Jekyll blog, either option should work. If you want to replace files individually, the files and directories you want to make sure to copy are: \n \n _layouts \n archives \n assets \n feed.xml \n index.xml \n \n You'll also want to compare Solar's  _config.yml  with your own, making any appropriate changes. \n Demo \n You can see a demo of Solar  right here on GitHub Pages. \n License \n GPLv2 or higher", " eigenclass \n       \n \n Eigenclasses aka  metaclasses  or  singleton classes  in Ruby. \n \n Check out the  metaclass  implementations in other languages for more examples. \n Note : This gem was originally written back in 2008. Since then, Ruby has introduced a couple new methods which provide the same functionality as this gem's  eigenclass  and  edefine_method  methods. \n \n Object#singleton_class \n Object#define_singleton_method \n \n Installation \n gem install eigenclass \n Requirements \n Ruby 1.8.7+ \n Usage \n Everything in Ruby is an object, including classes. \n ruby\nSomeObject = Class.new \n Every object has an  eigenclass . \n ruby\nSomeObject.eigenclass #=> #<Class:#<SomeObject:0x007f9611030300>> \n The implementation of the  eigenclass  method is pretty simple. \n ruby\nclass Object\n  def eigenclass\n    class << self\n      self\n    end\n  end\nend \n Evaluating code within an  eigenclass  lets us do some cool things like defining class level attributes. \n ```ruby\nSomeObject.eigenclass_eval do\n  attr_accessor :example\nend \n SomeObject.example = :test\nSomeObject.example #=> :test\n``` \n The convenience methods for defining class level methods makes this even easier. \n ```ruby\nclass SomeObject\n  eattr_accessor :example_accessor\n  eattr_reader :example_reader\n  eattr_writer :example_writer \n ealias_method :new_example_accessor, :example_accessor \n edefine_method(:example_class_method) do\n    1 + 1\n  end\nend \n SomeObject.example_class_method #=> 2\n``` \n When we  extend  modules, we're actually just calling  include  on an object's  eigenclass . \n ```ruby\nSomeObject.eigenclass.included_modules #=> [Eigenclass, Kernel] \n Example = Module.new\nSomeObject.extend(Example) \n SomeObject.eigenclass.included_modules #=> [Example, Eigenclass, Kernel]\n``` \n A convenience method for viewing an object's extended modules is available for us as well. \n ruby\nSomeObject.extended_modules #=> [Example, Eigenclass, Kernel] \n Since all objects have an  eigenclass , we can even define methods for a single  instance  of a class. \n ```ruby\nobject = SomeObject.new\nobject.eattr_accessor :example\nobject.example = :test\nobject.example #=> :test \n other_object = SomeObject.new\nother_object.example #=> NoMethodError undefined method `example' for # \n``` \n This is pretty incredible! We can hook in and inject behavior into  any and all  objects -  at runtime ! \n Ruby is like one big plugin framework - with an awesome standard library and amazing community! \n API \n YARD Documentation \n All methods defined by this gem are simple delegators to existing methods on the  eigenclass  object. The links below redirect to each corresponding method in the standard library documentation. \n \n ealias_method \n eattr_accessor \n eattr_reader \n eattr_writer \n edefine_method \n eigenclass \n eigenclass_eval \n eigenclass_exec \n extended_modules \n \n Testing \n bundle exec rspec \n Contributing \n \n Fork the project. \n Make your feature addition or bug fix. \n Add tests for it. This is important so I don't break it in a future version unintentionally. \n Commit, do not mess with Rakefile, version, or history. (if you want to have your own version, that is fine but bump version in a commit by itself I can ignore when I pull) \n Send me a pull request. Bonus points for topic branches. \n \n License \n MIT   - Copyright © 2008 Sean Huber", "jquery-favicons \n Adds favicons to links \n Installation \n Simply download and include  jquery.favicons.js  or  jquery.favicons.min.js \n Usage \n By default, when a favicon is added to an element, the plugin adds a configurable  class_name  ( favicon  by default) to the element, then sets the  background-url  css property. \n This behavior can be changed by overwriting the  apply  option. See  Options  below. \n So, if you're not overwriting this behavior, you're going to want to have some css rules defined for that configurable  class_name , e.g. \n .favicon {\n    background-position: top left;\n    background-repeat: no-repeat !important;\n    padding-left: 20px;\n}\n \n Then simply call  $('YOUR ANCHOR SELECTOR').favicons() , e.g.  $('#sources a').favicons() \n Options \n \n apply  - The function that is called when a favicon exists for an element. It is a function which gets passed an  element , and a  url . \n class_name  - The class name to add to an element when a favicon is loaded \n missing_url  - The favicon url to load when a domain does not have a favicon. This can be a string, a function which gets passed a  domain , or null to do nothing. \n url  - The favicon url to load. This is a function which gets passed a  domain . \n \n Note \n This plugin uses  http://www.google.com/s2/favicons?domain=DOMAIN  as the  url  option to load favicons by default. It ALWAYS returns a favicon, even if the domain doesn't have one, so the  missing_url  option will never be called. \n You can specify the  url  option as something like  function(domain) { 'http://' + domain + '/favicon.ico'; }  to overwrite this behavior.", " abstract_class \n       \n \n Abstract classes in Ruby. \n \n Like modules, abstract classes  cannot be instantiated . \n Unlike modules, abstract classes can be inherited and their  derived classes can be instantiated . \n Check out the  java  or  php  implementations for additional examples. \n Installation \n gem install abstract_class \n Requirements \n Ruby 1.8.7+ \n Usage \n To make a class  abstract , simply extend the  AbstractClass  module. \n ruby\nmodule ActiveRecord\n  class Base\n    extend AbstractClass\n  end\nend \n Any attempts to initialize or allocate an instance of an  abstract  class raises  AbstractClass::Error . \n ruby\nActiveRecord::Base.new      #=> AbstractClass::Error - abstract class ActiveRecord::Base can't be instantiated\nActiveRecord::Base.allocate #=> AbstractClass::Error - abstract class ActiveRecord::Base can't be allocated \n Child classes can inherit from an  abstract  class. \n ruby\nclass User < ActiveRecord::Base\nend \n Instantiation and allocation behaves like normal for descendants of  abstract  classes. \n ruby\nUser.new      #=> #<User:0x003d066d5a861d>\nUser.allocate #=> #<User:0x007f87588491d0> \n API \n YARD Documentation \n Testing \n bundle exec rspec \n Contributing \n \n Fork the project. \n Make your feature addition or bug fix. \n Add tests for it. This is important so I don't break it in a future version unintentionally. \n Commit, do not mess with Rakefile, version, or history. (if you want to have your own version, that is fine but bump version in a commit by itself I can ignore when I pull) \n Send me a pull request. Bonus points for topic branches. \n \n License \n MIT  - Copyright © 2011 Sean Huber", "phuby \n \n A port of  ruby  2.0 to native  php  5.4+ \n features \n \n runtime mixins with  include ,  extend ,  prepend  and their associated callbacks \n Object  and  Kernel  methods including  super ,  send ,  respond_to? ,  method_missing \n Module  methods including  alias_method ,  define_method , and  attr  accessors \n support for methods with special characters like  ?  using this syntax  $this->{'empty?'} \n instance variables are private and accessed with  $this->{'@name'} \n class variables are supported as well  $this->{'@@name'} \n even global variables are supported  $this->{'$redis'} \n ported core library including  BasicObject ,  Kernel ,  Object ,  Module ,  Method ,  UnboundMethod \n [incomplete] ported standard library including  Array ,  Hash ,  String ,  Enumerable ,  Comparable \n autoloading with ruby  underscore  naming conventions \n ruby style namespace resolution \n optional parenthesis for method calls with no arguments \n everything is an object, including classes \n \n installation \n If you're using  composer  simply require the  shuber-phuby  package. \n Otherwise you can download the  phuby  tar/zip or  git clone  this\nrepository somewhere in your php include path. \n Then inside of your php files you can  require 'phuby/phuby.php'  to load the library. \n usage \n You can integrate  Phuby  with your code in 3 ways: \n 1) class inheritance \n class Blog extends Phuby\\Object { }\n \n 2) traits \n This is useful when your class needs to inherit from an existing library and\ncan't extend  Phuby\\Object . \n class Blog extends ActiveRecord\\Base {\n    use Phuby;\n}\n \n 3) the  Phuby  function (like  jQuery 's  $  function) \n This allows you to inject  Phuby  features into  any  object. \n echo Phuby('this is a sentence.')->upcase;\n\n$evens = Phuby([1,2,3,4,5])->select(function($number) {\n  return $number % 2 == 0;\n});\n\necho Phuby(7)->days->from_now;\n \n \n string  becomes  Phuby\\String \n string  becomes  Phuby\\Module  if it is a class name \n array  becomes  Phuby\\Hash \n array  becomes  Phuby\\Array  if it has all numeric keys \n number  becomes some type of  Phuby\\Numeric  (float or int) \n object  becomes a  Phuby\\Proxy  which allows us to integrate  Phuby  into specific object instances \n \n todo \n general \n \n phpunit tests \n catch errors and raise them as exceptions \n Ruby conventions for method visibility \n Phuby()  namespace resolution \n Phuby()  aliases e.g. Array => Arr \n ✓ all classes inherit  Object \n ✓ classes may  use Phuby  instead of inheriting  Phuby\\Object \n ✓ autoloading with ruby naming conventions \n ✓ optional parenthesis for method calls \n ✓  __callStatic  delegates to  Module  instances \n \n errors \n \n ✓  ArgumentError \n ✓  Exception \n ✓  NameError \n ✓  NoMethodError \n ✓  RuntimeError \n ✓  StandardError \n \n hooks \n \n coerce \n const_missing \n marshal_dump \n marshal_load \n method_added \n method_removed \n method_undefined \n singleton_method_added \n singleton_method_removed \n singleton_method_undefined \n to_xxx \n ✓  append_features \n ✓  extend_object \n ✓  extended \n ✓  included \n ✓  inherited \n ✓  initialize_copy \n ✓  initialized \n ✓  method_missing \n ✓  prepended \n ✓  prepend_features \n \n lib \n Array \n Base64 \n BasicObject \n Comparable \n Date \n Dir \n Encoding \n Enumerable \n Enumerator \n File \n Fixnum \n Float \n Hash \n IO \n Integer \n Kernel \n Marshal \n MatchData \n Math \n Module \n Numeric \n Object \n ObjectSpace \n Proc \n Random \n Range \n Regexp \n RegexpError \n StopIteration \n String \n Struct \n Time \n notes \n Usually I try to structure methods so that they have as few  return  statements or endpoints as possible (1 ideally). In this project I'm using guard style conditions at the beginning of methods and I'm starting to see the beauty of how readable and simple the source code reads. I'm using many  if  and  return  statements in favor of  else  and  if else .  if  expressions are never written inline and the statements are separated by newlines. Brackets  { }  are only added to  if  and  foreach  loops if necessary. It makes source code files longer but everything is condensed horizontally and naturally less than 80 characters most of the time. It makes me appreciate the enforced whitespace in python more.", "Cache \n A simple php caching library with swappable backends \n Usage \n $cache = new Cache('Filesystem', '/path/to/your/cache');\n\n$cache->write('users', array('bob', 'joe', 'tim'));\nprint_r($cache->read('users'));\n$cache->delete('users');\n\n// If a fresh cache exists then it returns it, otherwise it executes the block (passing the current cache's modified time if there is one) and caches the result\nprint_r($cache->fetch('admins', function($modified_at) {\n    return Admin::all();\n}));\n", ' sub_diff \n       \n Inspect the changes of your [ String#sub ] and [ String#gsub ] replacements. \n Installation \n gem install sub_diff \n Requirements \n Ruby 2.0+ \n For older Ruby versions (1.8+), use the  1.0.7  tag. \n Usage \n This gem introduces a couple new methods to  String  objects. \n \n String#sub_diff \n String#gsub_diff \n \n These methods accept the same arguments as their  sub  and  gsub  counterparts. \n ruby\nreplaced = \'this is a test\'.gsub_diff(/(\\S*is)/, \'replaced(\\1)\') #=> #<SubDiff::Collection:0x007fc532049508> \n The difference is that it returns a  SubDiff::Collection  instead. This object behaves like a  String . \n ruby\nputs replaced #=> "replaced(this) replaced(is) a test" \n But it also allows us to check if the replacement actually  changed  anything. \n ruby\nreplaced.changed? #=> true \n For a closer look at the changes, we can iterate thru each  Diff  in the replacment. \n ```ruby\nreplaced.each do |diff|\n  puts diff.inspect\nend \n => "replaced(this)" \n => " " \n => "replaced(is)" \n => " a test" \n ``` \n Each  Diff  object behaves just like a string, but also includes a few additional methods. \n ```ruby\nreplaced.each do |diff|\n  puts "    value: #{diff.value.inspect}"\n  puts "value_was: #{diff.value_was.inspect}"\n  puts " changed?: #{diff.changed?}"\nend \n =>     value: "replaced(this)" \n => value_was: "this" \n =>  changed?: true \n =>     value: " " \n => value_was: " " \n =>  changed?: false \n =>     value: "replaced(is)" \n => value_was: "is" \n =>  changed?: true \n =>     value: " a test" \n => value_was: " a test" \n =>  changed?: false \n ``` \n API \n YARD Documentation \n \n String#sub_diff \n String#gsub_diff \n SubDiff::Diff#changed? \n SubDiff::Diff#value \n SubDiff::Diff#value_was \n SubDiff::Collection#changed? \n SubDiff::Collection#clear \n SubDiff::Collection#diffs \n SubDiff::Collection#each \n SubDiff::Collection#reset \n SubDiff::Collection#size \n \n Testing \n bundle exec rspec \n Contributing \n \n Fork the project. \n Make your feature addition or bug fix. \n Add tests for it. This is important so I don\'t break it in a future version unintentionally. \n Commit, do not mess with the version or history. \n Send me a pull request. Bonus points for topic branches. \n \n License \n MIT   - Copyright © 2011-2015 Sean Huber', 'Philt \n Generic interface to multiple PHP template engines \n Usage \n Initialization \n require \'philt/philt.php\';\n$philt = new Philt;\n \n Basic \n // => /tmp/template.php\nHello <?= $name ?>!\n\n$template = $philt->template(\'/tmp/template.php\');\nget_class($template); // Philt\\PhpTemplate\n$template->render(array(\'name\' => \'John\')); // Hello John!\n$template->render(array(\'name\' => \'Jane\')); // Hello Jane!\n \n Nested templates \n // => /tmp/template.haml.php\n.<?= $class ?>= $name\n\n$template = $philt->template(\'/tmp/template.haml.php\');\nget_class($template); // Philt\\NestedTemplate\n$template->render(array(\'class\' => \'person\', \'name\' => \'John\')); // <div class="person">John</div>\n$template->render(array(\'class\' => \'person\', \'name\' => \'Jane\')); // <div class="person">Jane</div>\n \n The  binding  object \n Properties \n $template->binding->class = \'person\';\n$template->binding->locals; // array(\'class\' => \'person\')\n$template->render(array(\'name\' => \'John\')); // <div class="person">John</div>\n$template->render(array(\'name\' => \'Jane\')); // <div class="person">Jane</div>\n \n Methods \n class AssetHelpers {\n    function asset_path($path) {\n        return \'/assets/\'.$path;\n    }\n}\n\n$template->binding->extend(\'AssetHelpers\');\n$template->binding->asset_path(\'test.png\'); // /assets/test.png\n \n Overriding methods \n class S3AssetHelpers {\n    function asset_path($path) {\n        return \'http://example.s3.amazonaws.com\'.$this->super($path);\n    }\n}\n\n$template->binding->extend(\'S3AssetHelpers\');\n$template->binding->ancestors; // array(\'S3AssetHelpers\', \'AssetHelpers\')\n$template->binding->asset_path(\'test.png\'); // http://example.s3.amazonaws.com/assets/test.png\n \n Missing/wildcard methods \n $template->binding->get_color(); // BadMethodCallException - Undefined method Philt\\Binding::get_color()\n\nclass Getters {\n    function method_missing($method, $arguments) {\n        if (preg_match(\'/^get_(.+)$/\', $method, $matches) && isset($this->{$matches[1]})) {\n            return $this->{$matches[1]};\n        } else {\n            return $this->super($method, $arguments);\n        }\n    }\n}\n\n$template->binding->extend(\'Getters\');\n\n$template->binding->color = \'Brown\';\n$template->binding->get_color(); // Brown\n\n$template->binding->get_size(); // BadMethodCallException - Undefined method Philt\\Binding::get_size()\n$template->binding->undefined(); // BadMethodCallException - Undefined method Philt\\Binding::undefined()\n \n Using individual template engines \n $template = new Philt\\PhpTemplate(\'<img src="<?= $binding->asset_path($image) ?>" />\');\n$template->render(array(\'image\' => \'test.png\')); // <img src="http://example.s3.amazonaws.com/assets/test.png" />\n', 'Kalimba \n Development \n Start with the  index.html  file. First it loads our main stylesheet  stylesheets/style.less . This file is parsed with  Less CSS  which adds additional useful features to  css  like variables, mixins, and nested definitions. It also allows us to  @import  other stylesheets so we can separate rules into different files to make it easier to navigate and develop.  stylesheets/kalimba.less  contains most of the relevant rules to render the kalimba. \n index.html  then loads some external libraries.  coffeescript.js  is a little language that compiles into JavaScript.  browser.js  allows us to compile coffeescript in the browser instead of pre-compiling it and  less.js  does the same for  .less  files. \n Then  javascripts/kalimba.coffee  is loaded. This is the class that generates the html for the kalimba. The constructor accepts a list of notes and dynamically creates tines for them. Once this file is loaded a new  Kalimba  instance is created and attached to the  <body>  tag with  var kalimba = new Kalimba(\'alto\', document.body); \n Notes \n KTabS \n KTabS is our Kalimba Tablature Software that allows you to easily create kalimba music on your computer. Inspired by the kalimba tablature created by Mark Holdaway, KTabS helps you customize tablature for your kalimba and compose, edit, play back, and print out your kalimba music with simple clicks of the mouse. \n Check out  VexFlow \n VexFlow is an open-source online music notation rendering API. It is written completely in JavaScript, and runs right in the browser. VexFlow supports HTML5 Canvas and SVG. \n Web based  Guitar Tuner \n Contains the implementation of a pure JavaScript Chromatic Tuner, which uses the Microphone input of the computer with the  Web Audio API  to perform real-time pitch detection. Currently requires the latest build of Google Canary using a Mac computer with "Web Audio Input" enabled in  chrome://flags . \n Also see other  chrome web audio input demos . \n From Stanford\'s  Specifying a tablature staff  article \n To set up a tablature staff, you use the stafflines parameter. To get a standard 6-line guitar tablature staff, you can just say  \n score\n  staffs=2\nstaff 2\n  stafflines = tab\n \n There are 2 staffs, because there is both the tablature staff and the automatically generated tabnote staff. The tablature staff is always immediately below the corresponding tabnote staff. Setting the stafflines parameter to "tab" marks staff 2 as a tablature staff.\nIf you want tablature for an instrument other than a 6-string guitar with standard tuning, you specify the pitches of the strings from top to bottom within parentheses after the "tab." For example:  \n stafflines = tab (e a d& g)\n \n would define some instrument that had 4 strings, with the string on the top line of tablature staff being an e string, the next a, the next d flat, and the bottom g. As shown in the example, pitches can include a # or & if necessary. Strings are assumed to be in octave 4 unless otherwise marked. You can specify a different octave by specifying an octave number after the string\'s pitch (using either an absolute octave number or pluses or minuses). If the instrument has more than one string with the same pitch (even if they are in different octaves), they must be distinguished by adding one or more \' marks after the pitch. The tablature definition for standard guitar is  \n stafflines = tab (e5 b g d a3 e\'3)\n \n This specifies that the top string on the staff is e in octave 5. The next three strings are in the default octave of 4, and the last two strings are in octave 3. Since there are two different strings with pitch letter of e, the lower e is marked as e\'. Note that the octaves given are how they should be printed on the tabnote staff. A standard guitar actually sounds an octave lower than written. If you just use "stafflines = tab" without specifying any strings, Mup not only assumes the standard guitar layout, and prints in the appropriate octave, but it also automatically transposes the MIDI output to the actual pitches an octave lower. If you specify strings explicitly, you will have to specify the octaves and any desired MIDI transposition values yourself. Stated another way, \n score\n  staffs = 2\nstaff 2\n  stafflines = tab\n \n is equivalent to  \n score\n  staffs=2\nstaff 2\n  stafflines = tab (e5 b g d a3 e\'3)\nstaff 1\n  ifdef MIDI transpose = down perfect 8 endif\n \n Note from this last example that MIDI is taken from the tabnote staff, not the tablature staff, so MIDI directives should be placed with the tabnote staff. \n Mup should be able to handle almost any instrument with up to 9 strings. Several strings can have the same pitch; you just need to distinguish them with \' marks. The strings must be listed in whatever order the strings are to appear on the tablature staff. As another example,  \n stafflines = tab (g3 d3 a2 e2)\n \n would define a standard bass guitar. Note that a bass guitar also sounds an octave lower than written, so it should be transposed for MIDI purposes.', ' variables \n       \n \n Variable  objects for  class  and  instance  variables \n \n Ruby already has  Method  objects, why not  Variable  objects as well? \n Why? \n Some methods already exist for interacting with  class  and  instance  variables: \n \n Module#class_variable_defined? \n Module#class_variable_get \n Module#class_variable_set \n Object#instance_variable_defined? \n Object#instance_variable_get \n Object#instance_variable_set \n \n But notice that these all share a common prefix -  instance_variable_  or  class_variable_ . \n This feels a little  smelly , let\'s try to  DRY  it up with some  Variable  objects! \n Installation \n gem install variables \n Requirements \n Ruby 1.8.7+ \n Usage \n Let\'s experiment with a simple  User  class. \n ruby\nclass User\n  def initialize(name)\n    @name = name\n  end\nend \n Objects can have any number of  instance  variables.  \n ruby\nuser = User.new(\'Bob\')              #=> #<User:0x007f8f6a84aa98>\nuser.instance_variable_get(\'@name\') #=> "Bob" \n Similar to  Object#method , the  instance_variable  method returns a  Variable  object. \n ruby\nname = user.instance_variable(:name) #=> #<InstanceVariable: #<User>@name> \n But unlike  Object#method , this method does not require a variable to actually be defined. \n ruby\nundefined = user.instance_variable(:undefined) #=> #<InstanceVariable: #<User>@undefined> \n We can check if a variable is defined by using the  defined?  method. \n ruby\nname.defined?      #=> true\nundefined.defined? #=> false \n Once we have a  Variable  object, we can  get  its value. \n ruby\nname.get      #=> "Bob"\nundefined.get #=> nil \n Similar to  Hash#fetch , the  fetch  method raises an exception if the variable is undefined. \n ruby\nname.fetch      #=> "Bob"\nundefined.fetch #=> Variables::UndefinedVariable - undefined variable "undefined" \n The  fetch  method optionally accepts a default value to return if the variable is undefined. \n ruby\nname.fetch(:default)      #=> "Bob"\nundefined.fetch(:default) #=> :default \n Default values can also be defined with a  block  which is yielded with the  Variable  name. \n ruby\nname.fetch { |name| "#{name}-default" }      #=> "Bob"\nundefined.fetch { |name| "#{name}-default" } #=> "@undefined-default" \n The  Object#instance_variable_fetch  method allows us to  fetch  a variable\'s value by name. \n ruby\nname.fetch                          #=> "Bob"\nuser.instance_variable_fetch(:name) #=> "Bob" \n We can update a  Variable  value by using the  set  method. \n ruby\nname.set(\'Steve\')                   #=> "Steve"\nuser.instance_variable_get(\'@name\') #=> "Steve" \n The  replace  method is similar to  set , but it returns the old value instead of the new value. \n ruby\nname.replace(\'Bob\')                 #=> "Steve"\nuser.instance_variable_get(\'@name\') #=> "Bob" \n We can even temporarily  replace  a value for the duration of a  block . \n ```ruby\nuser.instance_variable_get(\'@name\') #=> "Bob" \n name.replace(\'Steve\') do\n  user.instance_variable_get(\'@name\') #=> "Steve"\nend \n user.instance_variable_get(\'@name\') #=> "Bob"\n``` \n Note that when using the  block  form of  replace , the last expression of the block is returned. \n ruby\nname.replace(\'Steve\') { 1 + 1 } #=> 2 \n The  Object#instance_variable_replace  method allows us to  replace  a variable\'s value by name. \n ```ruby\nuser.instance_variable_get(\'@name\') #=> "Bob" \n user.instance_variable_replace(:name, \'Steve\') do\n  user.instance_variable_get(\'@name\') #=> "Steve"\nend \n user.instance_variable_get(\'@name\') #=> "Bob"\n``` \n The  instance_variable_replace  method also accepts a hash of variables to  replace . \n ```ruby\nuser.instance_variable_get(\'@name\') #=> "Bob"\nuser.instance_variable_get(\'@test\') #=> nil \n user.instance_variable_replace(name: \'Steve\', test: \'example\') do\n  user.instance_variable_get(\'@name\') #=> "Steve"\n  user.instance_variable_get(\'@test\') #=> "example"\nend \n user.instance_variable_get(\'@name\') #=> "Bob"\nuser.instance_variable_get(\'@test\') #=> nil\n``` \n Everything that we can do with  instance  variables can be done with  class  variables as well! \n ```ruby\nexample = User.class_variable(:example) #=> # \n example.defined? #=> false \n example.set(\'testing\') #=> "testing" \n User.class_variable_get(\'@@example\') #=> "testing"\n``` \n API \n YARD Documentation \n \n Module#class_variable \n Module#class_variable_fetch \n Module#class_variable_replace \n Object#instance_variable \n Object#instance_variable_fetch \n Object#instance_variable_replace \n Variable#defined? \n Variable#fetch \n Variable#get \n Variable#name \n Variable#owner \n Variable#replace \n Variable#set \n \n Testing \n bundle exec rspec \n Contributing \n \n Fork the project. \n Make your feature addition or bug fix. \n Add tests for it. This is important so I don\'t break it in a future version unintentionally. \n Commit, do not mess with Rakefile, version, or history. (if you want to have your own version, that is fine but bump version in a commit by itself I can ignore when I pull) \n Send me a pull request. Bonus points for topic branches. \n \n License \n MIT   - Copyright © 2015 Sean Huber', 'Postgres Twitter \n This is an experimental build of a simple "twitter" application in Postgres. The goal is to push as much logic into the database as possible. This includes contraints, validations, functions, and triggers! \n Schema \n \n Development \n \n I\'m using the  dbext  VIM plugin for  splitscreen  SQL and executed results. \n I use the binding  <leader>see  to execute all statements in the buffer. (Sql Execute Everything) \n \n Organization \n The  compile  executable combines all files under sql/ into  compiled.sql  or  development.sql . \n bin/\n  compile\nsql/\n  000_development.sql\n  001_schemas.sql\n  002_extensions.sql\n  003_functions.sql\n  004_trigger_functions.sql\n  005_behaviors.sql\n  006_tables.sql\n  007_views.sql\n  008_constraints.sql\n  009_indexes.sql\n  010_triggers.sql\n  999_development.sql\ntest/\n \n Todo \n \n [ ] seed database from twitter stream \n [x] favorites \n [x] followers \n [x] mentions \n [x] replies \n [x] retweets \n [x] tags \n [x] tweets \n [x] users \n \n Sample Input \n INSERT INTO users (username) VALUES\n  (\'bob\'),\n  (\'doug\'),\n  (\'jane\'),\n  (\'steve\'),\n  (\'tom\');\n\nINSERT INTO tweets (post, user_id) VALUES\n  (\'My first tweet!\', random_user_id()),\n  (\'Another tweet with a tag! #hello-world @missing\', random_user_id()),\n  (\'My second tweet! #hello-world #hello-world-again\', random_user_id()),\n  (\'Is anyone else hungry? #imHUNGRY #gimmefood @TOM @jane\', random_user_id()),\n  (\'@steve hola!\', random_user_id()),\n  (\'@bob I am! #imhungry #metoo #gimmefood #now\', random_user_id());\n\nINSERT INTO favorites (user_id, tweet_id)\nSELECT id as user_id, random_tweet_id() as tweet_id\nFROM users;\n\nINSERT INTO followers (follower_id, user_id)\nSELECT id as follower_id, random_user_id(id) as user_id\nFROM users;\n\nINSERT INTO replies (tweet_id, reply_id)\nSELECT id as tweet_id, random_tweet_id(id) as reply_id\nFROM tweets\nLIMIT 2;\n\nINSERT INTO retweets (tweet_id, retweet_id)\nSELECT id as tweet_id, random_tweet_id(id) as retweet_id\nFROM tweets\nLIMIT 2;\n \n Sample Queries \n SELECT id, username, followers, following, favorites, mentions, tweets FROM users;\nSELECT * FROM mentions;\n\n-------------------------------------------------------------------------------\n\nSELECT username, tweets.favorites, replies, retweets, tweets.mentions, tags\nFROM tweets JOIN users on tweets.user_id = users.id;\n\nDELETE FROM tweets\nWHERE id IN (\n  SELECT t.id\n  FROM tweets t\n  ORDER BY random()\n  LIMIT 1\n);\n\nUPDATE tweets\nSET post = \'replaced!\'\nWHERE id IN (\n  SELECT t.id\n  FROM tweets t\n  ORDER BY random()\n  LIMIT 1\n);\n\nSELECT username, tweets.favorites, replies, retweets, tweets.mentions, tags\nFROM tweets JOIN users on tweets.user_id = users.id;\n\n-------------------------------------------------------------------------------\n\nSELECT * FROM taggings;\nSELECT id, name, tweets FROM tags;\n \n Sample Output \n Connection: T(PGSQL)  D(twitter)  U(shuber)   at 12:52\n                  id                  | username | followers | following | favorites | mentions | tweets \n--------------------------------------+----------+-----------+-----------+-----------+----------+--------\n 267ce9ac-c3df-4bf4-bbed-3aee28094a52 | doug     |         2 |         1 |         1 |        0 |      1\n 46c226b4-f26f-412c-a72f-9fe1b35da997 | bob      |         0 |         1 |         1 |        1 |      1\n 9d2860c9-a713-43e5-a384-c1b24b3d1c4d | tom      |         2 |         1 |         1 |        1 |      1\n da1b62ea-c0bc-4f29-9d27-212144bc29c0 | jane     |         0 |         1 |         1 |        1 |      3\n 7bba5bea-d98b-4079-abea-fed05034dc6a | steve    |         1 |         1 |         1 |        1 |      0\n(5 rows)\n               user_id                |               tweet_id               \n--------------------------------------+--------------------------------------\n da1b62ea-c0bc-4f29-9d27-212144bc29c0 | 2d872de0-46cc-44aa-9aec-015fd5291c72\n 9d2860c9-a713-43e5-a384-c1b24b3d1c4d | 2d872de0-46cc-44aa-9aec-015fd5291c72\n 7bba5bea-d98b-4079-abea-fed05034dc6a | 1bab8cad-9cac-473d-a689-fbc710aaaea6\n 46c226b4-f26f-412c-a72f-9fe1b35da997 | 3bc21b9d-c778-4c64-84b8-798fd65072d8\n(4 rows)\n username | favorites | replies | retweets |  mentions  |              tags               \n----------+-----------+---------+----------+------------+---------------------------------\n tom      |         1 |       0 |        0 | {}         | {hello-world,hello-world-again}\n jane     |         2 |       0 |        0 | {bob}      | {gimmefood,imhungry,metoo,now}\n doug     |         0 |       1 |        0 | {}         | {}\n jane     |         0 |       1 |        0 | {steve}    | {}\n jane     |         1 |       0 |        1 | {jane,tom} | {gimmefood,imhungry}\n bob      |         1 |       0 |        1 | {missing}  | {hello-world}\n(6 rows)\n username | favorites | replies | retweets |  mentions  |              tags               \n----------+-----------+---------+----------+------------+---------------------------------\n tom      |         1 |       0 |        0 | {}         | {hello-world,hello-world-again}\n doug     |         0 |       1 |        0 | {}         | {}\n jane     |         1 |       0 |        1 | {jane,tom} | {gimmefood,imhungry}\n bob      |         1 |       0 |        1 | {missing}  | {hello-world}\n jane     |         2 |       0 |        0 | {}         | {}\n(5 rows)\n                tag_id                |               tweet_id               \n--------------------------------------+--------------------------------------\n 579e5a0f-5835-46ac-aced-528f8cd6e913 | 1d572544-8a84-4053-aef5-238260be3fa3\n 579e5a0f-5835-46ac-aced-528f8cd6e913 | 37421d41-416e-463d-9ad5-b9570f9356e8\n 0d86a554-0db2-4f60-8833-3d6bd6ed58af | 37421d41-416e-463d-9ad5-b9570f9356e8\n c2620f3c-5a5c-4fa5-88f4-f01e2e12173e | 2d872de0-46cc-44aa-9aec-015fd5291c72\n 3efbea1c-3002-4b87-917b-d3ead6483983 | 2d872de0-46cc-44aa-9aec-015fd5291c72\n(5 rows)\n                  id                  |       name        | tweets \n--------------------------------------+-------------------+--------\n 579e5a0f-5835-46ac-aced-528f8cd6e913 | hello-world       |      2\n 0d86a554-0db2-4f60-8833-3d6bd6ed58af | hello-world-again |      1\n c2620f3c-5a5c-4fa5-88f4-f01e2e12173e | gimmefood         |      1\n 3efbea1c-3002-4b87-917b-d3ead6483983 | imhungry          |      1\n(4 rows)\n \n Ideas for API \n The  random  schema \n This is mostly for development. This object contains methods to return a random record from various tables. Also add  _id  suffixed versions of the methods to return a random record\'s primary key. \n \n random.tag() \n random.tweet() \n random.user() \n \n The  tweets  schema \n Public API for interacting with tweets \n \n tweets.create \n tweets.delete \n tweets.find \n tweets.for_user \n tweets.update \n \n Public API for interacting with tags \n \n tags.find_or_create(name text) \n tags.listen(names text[]) \n tags.tweets(names text[]) \n \n Or maybe put everything under the  api  schema \n \n api.create_reply(tweet_id uuid, post text, user_id uuid) \n api.create_retweet(tweet_id uuid, post text, user_id uuid) \n api.create_tweet(post text, user_id uuid) \n api.delete_tweet(tweet_id uuid) \n api.favorite_tweet(tweet_id uuid, user_id uuid) \n api.unfavorite_tweet(tweet_id uuid, user_id uuid) \n api.follow_user(user_id uuid, follower_id uuid) \n api.unfollow_user(user_id uuid, follower_id uuid) \n', " monolith \n   \n Generates a single monolithic repository from a list of other git repositories \n Why? \n \n Google Is 2B Lines of Code, All in One Place \n On Monolithic Repositories \n Advantages of Monolithic Version Control \n \n How? \n \n Merging two, three or more git repositories keeping the log history \n \n Installation \n bash\ngem install monolith \n Usage \n Create a  monolith.yml  file in your working directory with a list of repositories \n ```yaml\npath: /path/to/your/new/monolith \n repositories:\n  admin: git@github.com:some-org/admin.git\n  auth: git@github.com:some-org/your-auth-gem.git\n  users: git@github.com:some-org/users.git \n Optional whitelist of branches to clone. By default, \n ALL branches are imported into the monolith. \n branches:\n  - master \n Optional list of commands to run right after \n all of the repositories above have been cloned. \n \n These are handy for things like rewriting history \n to remove large unused files or sensitive information. \n after_clone:\n  - ./remove_all_unused_large_files \n Optional list of commands to run after the monolith \n has been generated. \n \n These hooks are handy for things like introducing new top-level \n config files for services like Heroku, CodeClimate, CircleCI, etc. \n after_generate:\n  - ./add_global_gitignore\n  - ./add_global_slugignore\n``` \n Use the  monolith  command to generate a repository at  /path/to/your/new/monolith \n bash\nmonolith generate \n See  monolith help  for a list of other commands \n Commands:\nmonolith clone           # Clone configured repositories\nmonolith config          # List all configured repositories\nmonolith generate        # Generate a new monolith from configured repositories\nmonolith help [COMMAND]  # Describe available commands or one specific command \n Contributing \n \n Fork the project. \n Make your feature addition or bug fix. \n Add tests for it. This is important so I don't break it in a future version unintentionally. \n Commit, do not mess with the version or history. \n Send me a pull request. Bonus points for topic branches. \n \n License \n MIT  - Copyright © 2015 Sean Huber", ' tmux-git \n Display  git  information in your  tmux  status line \n Installation \n Install  tmux plugin manager  and add the following to the end of your  .tmux.conf \n tmux\nset -g @plugin "shuber/tmux-git" \n Interpolation variables \n \n #{git_branch}  - display the currently checked out git branch \n #{git_sha}  - display the sha of the latest commit \n #{git_shortsha}  - display the short sha of the latest commit \n #{git_subject}  - display the last commit subject (short message) \n [TODO]  #{git_additions}  - display number of uncommitted additions \n [TODO]  #{git_ahead}  - display number of commits ahead of master \n [TODO]  #{git_behind}  - display number of commits behind master \n [TODO]  #{git_deletions}  - display number of uncommitted deletions \n [TODO]  #{git_dirty}  - display number of uncommitted dirty files if any \n \n Usage \n Add  git  interpolation variables to your  status-left  or  status-right  options in  .tmux.conf \n tmux\nset -g status-left "#{git_dirty}"\nset -g status-right "#{git_ahead}/#{git_behind} | #{git_branch}" \n Adjust the  status-interval  refresh rate if necessary \n tmux\nset -g status-interval 15 # refresh status line every 15 seconds \n Or manually refresh the status line instead \n bash\ntmux refresh-client -S \n Contributing \n \n Fork the project. \n Make your feature addition or bug fix. \n Send me a pull request. Bonus points for topic branches. \n \n License \n MIT  - Copyright © 2015 Sean Huber', ' vim-promiscuous \n Instant  context switching  using git and vim sessions. \n What does it do? \n It basically takes a snapshot of the following: \n \n All of your vim tabs, buffers, splits, and folds along with their sizes and positions \n The location of your cursor for each buffer \n The actively selected tab/buffer \n Your undo history (each branch\'s undo history is saved separately) \n Your git stage with all tracked/untracked files and staged/unstaged hunks \n \n When you switch to different branches using  :Promiscuous your-branch-name , it takes a snapshot of the current branch and working directory, then checks out the new branch, and loads its corresponding snapshot if one exists. \n If no snapshot exists, you are presented with a "fresh" vim instance that only has one tab and an empty buffer. \n When  :Promiscuous  is called with no arguments, an  :FZF  fuzzy finder window is presented with a list of existing branches. From there we can either select an existing branch, or type out a new branch to checkout. \n If you\'re using  tmux  then your status line will automatically refresh when  :Promiscuous  checks out a branch. This is very convenient when you display  git information in your status line . \n Similar projects: \n \n http://www.eclipse.org/mylyn/ \n https://github.com/szw/vim-ctrlspace \n \n Installation \n Load  shuber/vim-promiscuous  using your favorite plugin manager e.g.  Vundle \n It currently depends on  :FZF , but this dependency will be optional in the future. \n Usage \n vim\n:Promiscuous [branch] \n It\'s recommended to make a custom key binding for this. I\'ve been using the following mapping: \n vim\nnmap <leader>gb :Promiscuous<cr> \n I also use an additional mapping to checkout the previous branch (usually  master ): \n vim\nnmap <leader>gg :Promiscuous -<cr> \n Configuration \n These are the defaults. Feel free to override them. \n ```vim\n" The directory to store all sessions and undo history\nlet g:promiscuous_dir = $HOME . \'/.vim/promiscuous\' \n " The callback used to load a session (receives branch name)\nlet g:promiscuous_load = \'promiscuous#session#load\' \n " The prefix prepended to all commit, stash, and log messages\nlet g:promiscuous_prefix = \'[Promiscuous]\' \n " The callback used to save a session (receives branch name)\nlet g:promiscuous_save = \'promiscuous#session#save\' \n " Log all executed commands with echom\nlet g:promiscuous_verbose = 0\n``` \n vim\nset sessionoptions=blank,buffers,curdir,folds,help,tabpages,winsize\nset undolevels=1000\nset undoreload=10000 \n How does it work? \n ```vim\ncall promiscuous#helpers#clear()\ncall promiscuous#git#stash()\ncall promiscuous#git#commit() \n let l:branch_was = promiscuous#git#branch()\ncall call(g:promiscuous_save, [l:branch_was], {}) \n call promiscuous#session#clean()\ncall promiscuous#git#checkout(l:branch) \n let l:branch = promiscuous#git#branch()\ncall call(g:promiscuous_load, [l:branch], {}) \n call promiscuous#git#commit_pop()\ncall promiscuous#git#stash_pop()\ncall promiscuous#tmux#refresh()\n``` \n The output below occurred when switching from  master  to  something-new  then back to  master . \n ```\n[Promiscuous] !clear\n[Promiscuous] !(git diff --quiet && git diff --cached --quiet) || (git stash save Code_vim_promiscuous_master && git stash apply)\n[Promiscuous] !git add . && git commit -am \'[Promiscuous]\'\n[Promiscuous] mksession! /Users/Sean/.vim/promiscuous/Code_vim_promiscuous_master.vim\n[Promiscuous] bufdo bd\n[Promiscuous] !git checkout - || git checkout -b -\n[Promiscuous] source /Users/Sean/.vim/promiscuous/Code_vim_promiscuous_something_new.vim\n[Promiscuous] Checkout something-new \n [Promiscuous] !clear\n[Promiscuous] !(git diff --quiet && git diff --cached --quiet) || (git stash save Code_vim_promiscuous_something_new && git stash apply)\n[Promiscuous] !git add . && git commit -am \'[Promiscuous]\'\n[Promiscuous] mksession! /Users/Sean/.vim/promiscuous/Code_vim_promiscuous_something_new.vim\n[Promiscuous] bufdo bd\n[Promiscuous] !git checkout - || git checkout -b -\n[Promiscuous] source /Users/Sean/.vim/promiscuous/Code_vim_promiscuous_master.vim\n[Promiscuous] Checkout master\n``` \n Contributing \n \n Fork the project. \n Make your feature addition or bug fix. \n Commit, do not mess with the version or history. \n Send me a pull request. Bonus points for topic branches. \n \n License \n MIT  - Copyright © 2015 Sean Huber', ' owners \n       \n Take ownership of your code. \n Knowing who owns a project or section of a code base is very helpful when asking questions or requesting feedback. This gem allows developers to define  OWNERS  files throughout their repository to provide a human and machine readable way to determine who the maintainers are for specific files of code. \n These files can be used to: \n \n find the right people to ask when you have questions \n notify maintainers when changes occur in the files that they care about \n enforce approval from the appropriate people in pull requests \n \n Installation \n gem install owners \n Usage \n Define an  OWNERS  file in any directory within your repository. This file should contain a newline separated list of subscribers to notify when files within the directory have changed. The  OWNERS  files are searched recursively up the tree to make organizing owners more convenient. \n Subscribers can be anything that suits your needs e.g. emails, GitHub handles, Slack channels, etc. \n ```\nbob@your-org.com\njane@your-org.com\n@some_github_handle\n@your_github_org/group \n some_slack_channel \n ``` \n Filters \n The  OWNERS  file also supports filtering paths with regular expressions. Any whitespace between these filters and their corresponding subscribers is ignored. \n ```\n@data         app/models/.*\n@ui           .(css|haml|js|scss)$\nbob@demo.com  lib/bobs_special_file.rb \n whitespace   path/with spaces/is all part/of the filter.txt \n ``` \n Multiple subscribers \n Subscribers can be listed multiple times in an  OWNERS  file. \n @data app/models\n@data db \n Multiple comma separated subscribers can be listed for the same filter. \n @data,@team-leads db \n Comments \n Comments are supported by prefixing lines with  // . \n ```\n// this comment will be ignored\n//// this one two\n  // even this one with whitespace \n @data,@team-leads db \n internal .env \n ``` \n Finding owners \n Find the owners for specific files by passing them to the  Owners.for  method.\nThis returns an array of  Owner  objects which are simple wrappers around\n String  with a few extra methods. \n ruby\nowners = Owners.for("db/schema.rb", ".env") #=> ["@data", "#internal"] \n The owner\'s  type  can be one of  %i(alert email group label mention tag) . \n ruby\nowners.first.type #=> :mention\nowners.last.type  #=> :tag \n The  paths  method returns an array of owned file paths that triggered the match. \n ruby\nowners.first.paths #=> ["db/schema.rb"]\nowners.last.paths  #=> [".env"] \n The  subscriptions  method allows us to inspect the rules that triggered a match. \n ```ruby\nowners.each do |owner|\n  puts owner \n owner.subscriptions.each do |path, subscriptions|\n    puts "  #{path}" \n subscriptions.each do |subscription|\n  puts "  #{subscription.file}:#{subscription.line} => #{subscription.filter}"\nend\n \n end\nend\n``` \n ```\n@data\n  db/schema.rb\n  OWNERS:5 => (?-mix:db) \n internal \n .env\n  OWNERS:7 => (?-mix:.env)\n``` \n Finding files without owners \n Stay on top of owner subscriptions by finding files that don\'t have any owners yet. \n ruby\nOwners.missing_for(".env", "some/other/file.rb") #=> ["some/other/file.rb"] \n Git diff integration \n To find the owners for files changed with  git diff  use the  Owners.for_diff  method. \n ruby\nOwners.for_diff("your-feature-branch-or-ref", "optional-base-ref-defaults-to-master") \n Command line interface \n This gem also comes with a convenient  owners  CLI. Each owner is printed out and separated by newlines. \n bash\nowners for .env app/controllers/posts_controller.rb app/models/user.rb \n ``` \n infrastructure \n @api/internal\n@data\n``` \n Debugging and managing subscriptions \n The  --debug  option outputs additional information for subscriptions. \n bash\nowners for --debug .env app/controllers/posts_controller.rb app/models/user.rb \n ``` \n infrastructure \n tag\n  .env\n  OWNERS:1 => (?-mix:.env) \n @api/internal\ngroup\n  app/controllers/posts_controller.rb\n  OWNERS:2 => (?-mix:app/(controllers|models)) \n app/models/user.rb\n  OWNERS:2 => (?-mix:app/(controllers|models)) \n @data\nmention\n  ./app/models/user.rb\n  ./OWNERS:3 => (?-mix:app/models)\n``` \n Git diff integration \n The  git diff  integration works in the command line as well. \n bash\nowners for_diff my-feature-branch \n See  owners help  for more information. \n API \n YARD Documentation \n \n Owners.file \n Owners.file= \n Owners.for \n Owners.for_diff \n Owners.missing_for \n Owners::Owner#type \n Owners::Owner#subscriptions \n Owners::Subscription#file \n Owners::Subscription#filter \n Owners::Subscription#line \n Owners::Subscription#metadata? \n Owners::Subscription#root \n Owners::Subscription#subscribed? \n Owners::Subscription#subscribers \n Owners::Subscription#subscription \n \n Testing \n bundle exec rspec \n Contributing \n \n Fork the project. \n Make your feature addition or bug fix. \n Add tests for it. This is important so I don\'t break it in a future version unintentionally. \n Commit, do not mess with the version or history. \n Send me a pull request. Bonus points for topic branches. \n \n License \n MIT  - Copyright © 2015 Sean Huber', "pg_brainfuck \n \n https://en.wikipedia.org/wiki/Brainfuck \n https://esolangs.org/wiki/Brainfuck_implementations \n \n ```sql\n-- Hello, World!\nselect brainfuck('+[-->-[>>+>-----<<]<--<---]>-.>>>+.>>..+++[.>]<<<<.+++.------.<<-.>>>>+.'); \n -- test from stdin\nselect brainfuck(',[.,]', 'test from stdin'::bytea);\nselect brainfuck(',[.[-],]', 'test from stdin'::bytea);\nselect brainfuck(',[>,]<[.<]', 'nidts morf tset'::bytea);\n``` \n ```sql\ncreate or replace function brainfuck(\n  in program text,\n  in stdin bytea default ''::bytea,\n  out stdout text\n) as $$\n  declare\n    commands text[] := regexp_split_to_array(program, '');\n    commands_size int := array_upper(commands, 1); \n cells int[] := array[]::int[];\njumps int[] := array[]::int[];\nstack int[] := array[]::int[];\n\ndepth int;\nindex int := 1;\npointer int := 0;\nread int := 0;\nsubindex int;\n \n begin\n    while index <= commands_size loop\n      case commands[index]\n      when '>' then\n        pointer = pointer + 1;\n      when '<' then\n        pointer = pointer - 1;\n      when '+' then\n        cells[pointer] = coalesce(cells[pointer], 0) + 1;\n        cells[pointer] = coalesce(nullif(cells[pointer], 256), 0);\n      when '-' then\n        cells[pointer] = coalesce(cells[pointer], 0) - 1;\n        cells[pointer] = coalesce(nullif(cells[pointer], -1), 255);\n      when '.' then\n        continue when coalesce(cells[pointer], 0) = 0;\n        stdout = coalesce(stdout, '') || chr(cells[pointer]);\n      when ',' then\n        begin\n          cells[pointer] = get_byte(stdin, read);\n          read = read + 1;\n        exception when array_subscript_error then\n          index = index + 1;\n        end;\n      when ']' then\n        if array_length(stack, 1) > 0 then\n          subindex = stack[array_upper(stack, 1)];\n          stack = array_remove(stack, subindex);\n          jumps[subindex] = index;\n          index = subindex - 1;\n        end if;\n      when '[' then\n        if coalesce(cells[pointer], 0) != 0 then\n          stack = array_append(stack, index);\n        elsif jumps[index] is not null then\n          index = jumps[index];\n        else\n          depth = 1;\n          index = index + 1;\n          subindex = index; \n       while subindex <= commands_size loop\n        case commands[subindex]\n        when '[' then depth = depth + 1;\n        when ']' then depth = depth - 1;\n        else end case;\n\n        if depth = 0 then\n          jumps[index] = subindex;\n          index = subindex;\n          exit;\n        end if;\n\n        subindex = subindex + 1;\n      end loop;\n    end if;\n  else end case;\n\n  index = index + 1;\nend loop;\n \n end;\n$$ language plpgsql immutable;\n```"]
VirrageS,['TDL \n ToDoList (TDL) is application written in Swift for iOS devices. It helps you to manage your tasks and have them presented in convinient way. \n SCREENSHOTS \n \n \n \n \n \n \n', 'Przygoda iOS \n The best way to find enthusiasts of bike tours. \n Przygoda iOS is native application of  main application  which make it easier\nto connect and create awesome bike rides. \n Main application \n Check out also  main application .', 'Przygoda |   |   |  \n The best way to find enthusiasts of bike tours. \n Application make it easier to connect and create bike rides. \n About \n Application is build on microframework  Flask . \n Preinstall \n Clone GitHub project \n Our first move is to clone GitHub project into our computer. \n $ sudo apt-get install git\n$ git clone https://github.com/VirrageS/przygoda\n$ cd przygoda\n \n Virtual Environment initialization \n Now we have to install virtual env and get all python packages we need.\nSo lets get python packages first: \n przygoda$ sudo apt-get update\nprzygoda$ sudo apt-get install python3-pip python3-dev\nprzygoda$ sudo apt-get build-dep python3-psycopg2\n \n Now we need to install virtual env: \n przygoda$ sudo pip3 install virtualenv\n \n Lets create virtual env in our folder: \n przygoda$ virtualenv env\n \n Now we have to start our virtual env (if ever would want to leave virtual env just type  deactivate ): \n przygoda$ source env/bin/activate\n \n And finally install requirements which we need to make our app running.\nThis code will install all dependencies which our app is using. You can open  requirements.txt  to see what they are. \n (env)przygoda$ pip3 install -r requirements.txt\n \n Database \n Now we have to create simply database which will handle our queries.\nTo make one, type: \n (env)przygoda$ python3 shell.py\n>>> db.create_all()\n>>> exit()\n \n IMPORTANT : you have to be in virtual environment \n Unfortunetly if we use sqlite database we need delete our database and\ncreate fresh one every time we code new model or add something to existing one. \n Run app \n Now we can run our app by just typing  python3 run.py . \n przygoda$ ./run-redis; ./run-celery\nprzygoda$ . ./env/bin/activate\n(env)przygoda$ python3 run.py\n * Running on http://127.0.0.1:5000/\n * Restarting with reloader\n \n IMPORTANT : you have to be in virtual environment \n Hurray! Our app is alive. Open [http://127.0.0.1:5000] in your browser and that\'s it! \n Testing \n To run unit tests type: \n (env)przygoda$ nosetests --with-coverage --cover-erase --cover-package=app --cover-html\n \n Babel \n To run babel \n (env)przygoda$ pybabel extract -F babel.cfg -o messages.pot app\n(env)przygoda$ pybabel extract -F babel.cfg -k lazy_gettext -o messages.pot app\n(env)przygoda$ pybabel init -i messages.pot -d app/translations -l pl\n(env)przygoda$ pybabel compile -d app/translations\n \n to update \n (env)przygoda$ pybabel extract -F babel.cfg -o messages.pot app\n(env)przygoda$ pybabel extract -F babel.cfg -k lazy_gettext -o messages.pot app\n(env)przygoda$ pybabel update -i messages.pot -d app/translations\n(env)przygoda$ pybabel compile -d app/translations\n \n Stress tests \n To check if our site is able to handle a lot of traffic we can preform stress tests: \n ab -k -r -n 50000 -c 500 http://..../\n \n Parameter | Desc | Value\n--- | --- | ---\n-n | Set how much packets will be send to our server | 50000\n-c | Simulate simultaneous user connections (most important parameter) | 500 \n \n Extras \n Gunicorn \n Now we have to create script that will run our server. First step is to type: \n sudo nano /etc/init/przygoda.conf\n \n and put this code: \n ```\ndescription \\"Gunicorn application server running PROJECT_NAME\\" \n start on runlevel [2345]\nstop on runlevel [!2345] \n respawn\nsetuid USER\nsetgid www-data \n env PATH=/home/USER/PROJECT_NAME/env/bin\nenv CONFIG=Development \n env MAIL_USERNAME=CHANGE_THIS_!!!\nenv MAIL_PASSWORD=CHANGE_THIS_!!!\nenv DATABASE_USERNAME=CHANGE_THIS_!!!\nenv DATABASE_PASSWORD=CHANGE_THIS_!!!\nenv DATABASE_HOST=CHANGE_THIS_!!!\nenv DATABASE_PORT=CHANGE_THIS_!!!\nenv DATABASE_NAME=CHANGE_THIS_!!!\nenv CREDENTIALS_FB_ID=CHANGE_THIS_!!!\nenv CREDENTIALS_FB_SECRET=CHANGE_THIS_!!!\nenv API_KEY=CHANGE_THIS_!!! \n chdir /home/USER/PROJECT_NAME\nexec gunicorn --workers 3 --bind unix:PROJECT_NAME.sock -m 007 run:app\n``` \n Before saving. Change  USER  and  PROJECT_NAME  to our current user and project name\nfor example:  ubuntu  and  przygoda  respectively.\nNow lets test our script and set it running. \n sudo start przygoda\n \n NGINX \n First step is to remove default sites because we will not need them. Type: \n sudo rm -rf /etc/nginx/sites-enabled/default\nsudo rm -rf /etc/nginx/sites-available/default\n \n Create new site by: \n sudo nano /etc/nginx/sites-available/PROJECT_NAME\n \n and put code like this: \n ```\nserver {\n    listen 80;\n    server_name SERVER_IP_ADDRESS; \n location / {\n    include proxy_params;\n    proxy_pass http://unix:/home/USER/PROJECT_NAME/PROJECT_NAME.sock;\n    proxy_connect_timeout 30s;\n    proxy_read_timeout 30s;\n}\n \n }\n``` \n Now we have to connect our site to enabled sites. Type: \n sudo ln -s /etc/nginx/sites-available/PROJECT_NAME /etc/nginx/sites-enabled\n \n Check if our nginx configuration is properly set: \n sudo nginx -t\n \n And start nginx: \n sudo service nginx restart\n \n Now, if we type  SERVER_IP_ADDRESS  into our browser we should see our app up and running :)!', "(MNIST) Handwritten Digits Recognition \n Convolutional neural network to recognize handwritten digits (MNIST dataset). \n MNIST Dataset is downloaded from  https://s3.amazonaws.com/torch7/data/mnist.t7.tgz  which contains images ( 32x32 )\nwith classes from 1 to 10 divided into  60'000 training images  and  10'000 test images .\nTo use this script you have to download  Torch  first. \n Below you can see example usage of this convolutional neural network. \n Kaggle \n You can also use this model to predict output for Kaggle  Digit Recognizer  challenge.\nUnfortunately Kaggle MNIST Dataset have different images geometry ( 28x28 ) and different file format what makes code a little bit uglier :C\nKaggle dataset consist of  42'000 training images  with classes from 0 to 9 and  28'000 images to predict .\nIn  cnn_model.lua  you can find exact model which is used in learning. \n To use Kaggle model type: \n $ th train.lua -kaggle 1 -train_size 42000 -test_size 28000\n \n Tip:  after each iteration script saves new predictions into  data/submission.csv  file. \n Max score I have achieved on Kaggle (with about 10 minutes of learning):  0.98686 \n Parameters \n You can set parameters which will help you to better control the network. \n | Parameter | Optional usage | Description | Default |\n| :-------: | :------------: | :--------: | :-----: |\n| -gpuid | --enable_gpu | Enables CUDA (use only if you have NVIDIA GPU) | -1 |\n| -kaggle | --enable_kaggle | Switches to Kaggle challenge prediction | nil |\n| -train_size | --traing_data_size | Size of training sets | 60000 |\n| -test_size | --test_data_size | Size of testing sets | 10000 |\n| -rate | --learning_rate | Learning rate | 0.05 |\n| -batch | --batch_size | Numbers of sets in batch | 10 |\n| -t | --threads | Number of threads used during usage | 2 | \n Examples \n Normal: \n $ th train.lua --learning_rate 0.01 --batch_size 100\n$ th train.lua --learning_rate 0.02 --threads 8 --traing_data_size 2000 --test_data_size 1000\n \n Kaggle: \n $ th train.lua -kaggle 1 -train_size 42000 -test_size 28000\n$ th train.lua -kaggle 1 -train_size 42000 -test_size 28000 --learning_rate 0.005 --batch_size 50\n", "Dota Game Prediction \n It is Python script which predicts Dota's game result. Written for  Dota Prediction Challenge . \n Score:  23.20  points (about  61.6%  correct predictions). \n About model \n Script is using  Logistic Regression  from  sklearn.linear_model .\nIt predicts binary output using a weighted sum of predictor variables.\nIt is quite simple model and that is why it fails to capture any synergy or correlation between heroes.\nThis is probably the reason why global correct is only around 60%. \n You can also choose  SGDClassifier  (also from  sklearn.linear_model ) as model for this task.\nUnfortunately this doesn't give you much more points - the scores are roughly the same as in  Logistic Regression . \n Usage \n python3 dota.py < input_3000.in > tmp.out; diff -b tmp.out output_3000.out | wc -l;\n \n This will tell you how different is your output  tmp.out  from  output_3000.out .\nThe less, the better.", 'IO-Kawiarnie \n \n \n \n Workflow \n Każdą nową funkcjonalność dodajemy poprzez stworzenie nowego brancha,\nnapisanie funkcjonalności, napisanie do niej testów i wmergowania\nfunkcjonalności do mastera. \n $ git checkout -b [nazwa_brancha]\n$ ... commits ...\n \n Odpalamy przeglądarkę, wchodzimy na githubie na naszego brancha i\nklikamy  Pull request . Następnie czekamy na łapki w góre i jeżeli\nwszystko jest zatwierdzone mergujemy pull requesta. Później usuwamy\nnieużywane branche: \n $ git checkout master\n$ git pull\n$ git push origin --delete [nazwa_brancha]\n$ git fetch --all --prune\n$ git branch --merged master | grep -v \'master$\' | xargs git branch -d\n \n Potem znowu tworzymy nowego brancha  git checkout -b ...  i zaczynamy\ncykl od nowa. \n Setup \n Jeżeli nie macie  virtualenv  to ściągnijcie: \n $ pip install virtualenv\n \n Teraz tworzymy środowisko \n $ git clone git@github.com:VirrageS/io-kawiarnie.git\n$ cd io-kawiarnie\n$ virtualenv -p python3 venv\n$ . venv/bin/activate\n(venv)$ pip install --upgrade pip\n(venv)$ pip install -r requirements.txt\n \n Teraz jesteśmy gotowi, aby odpalić serwer: \n (venv)$ cd caffe\n(venv)$ python manage.py migrate\n(venv)$ python manage.py runserver\n \n Testing \n Do testowania używamy  coverage . Aby użyć tego narzędzia wystarczy wpisać. \n (venv)$ coverage run --source="caffe" --omit="*migrations*, *venv*, *env*" caffe/manage.py test caffe\n(venv)$ coverage report\n(venv)$ coverage html\n(venv)$ open htmlcov/index.html\n \n \n W pierwszej linijce, na końcu możemy wyspecifkować jaką aplikację teraz\ntestujemy np.  reports  lub  employees . \n W drugiej generujemy raport na podstawie testów jakie przeszły. Możemy także\ndodać opcję  -m , która dokładnie pokaże nam linijki, które nigdy się nie\nwykonały w testach (!). \n W trzeciej linijce generujemy html\'owy raport, w którym dokładnie możemy\nsprawdzić, jakie części kodu się\xa0wykonały, a które nie. Super przydatne i ładne. \n Czwarta linijka to proste makro, które odpala nam html\'owy raport w\nprzeglądarce. \n \n Code Quality \n Jednym z narzędzi, których będziemy używać jest  pep8 . Używamy go\nw katologu, który chcemy sprawdzić jakość naszego kodu. Przechodzi\nrekursywnie po folderach, więc jest spoko. \n (venv)$ pep8 .\n \n Drugim z nich (narzędzi) jest  pylint . Bardzo fajne narzędzie, bo przyznaje\npunkty dla naszego kodu :) Musimy mieć zainstalowny  pylint-django  żeby był\nmądry i nie wywalał błędów, które wynikają ze struktury Django i jego aplikacji. \n (venv)$ pylint --load-plugins=pylint_django reports\n \n Ostatni parametr  reports  mówi dla  pylint , którą aplikację chcemy\nprzetestować. Czasami błędy są głupie, no i nie da ich poprawić, wtedy po prostu\nzostawiamy - trudno... \n Rules \n \n Komentarze w kodzie piszemy po angielsku \n Nazwy commitów piszemy po angielsku \n Nazwy zmiennych, funkcji piszemy po angielsku \n Staramy się\xa0niszczyć rzeczy we własnych branchach, a nie masterze \n Wszystkie możliwe funkcjonalności powinny mieć swoje testy - najlepiej przed\nwrzuceniem do mastera. \n Próbujmy pisać ładny kod, dlatego używamy  pep8  i  pylint \n', 'Chat \n Simple chat written in C. \n Usage \n To start server type:  \n $ make\n$ ./server [port]\n \n Port parameter is optional. If not set, it will\nbe listening on  20160 . \n To start client type: \n $ make\n$ ./client [host] [port]\n \n To connect client to server we need to provide host\n(if testing on one machine just type  localhost ).\nPort parameter is optional. If not set, it will try\nto connect to port  20160 .', 'headline-go \n Backend for website to display trending links \n Getting started \n I assume that you have installed Golang. If not visit: https://golang.org/doc/code.html \n After that you should get this repository to path\n $GOPATH/src/github.com/VirrageS/headline-go ,\nthen install all dependencies and make executable file: \n $ git clone https://github.com/VirrageS/headline-go\n$ cd headline-go\n$ go get\n$ go install\n$ headline-go\n \n after that visit  localhost:8080/github  and you should get all trending repositories.', 'scrape \n A jquery like interface for Go website scrapping. \n Usage \n ```go\nimport (\n    "fmt"\n    "net/http" \n "golang.org/x/net/html"\n\n"github.com/VirrageS/scrape"\n \n ) \n func main() {\n    response, err := http.Get("https://github.com/trending")\n    if err != nil {\n        return\n    } \n root, err := html.Parse(response.Body)\nif err != nil {\n    return\n}\n\nrepos := scrape.Find(root, ".repo-list-item")\nfor _, repo := range repos {\n    // get url\n    link := scrape.Find(repo, ".repo-list-name a")[0]\n    url := "https://github.com" + scrape.Attr(link, "href")\n\n    // get name\n    name := scrape.Text(link)\n\n    fmt.Printf("[REPO] name: %s; url: %s\\n", name, url)\n}\n \n }\n```', 'cache \n Simple, in-memory, thread-safe TTL cache for Go \n Usage \n ```go\nimport (\n    "fmt"\n    "time" \n "github.com/VirrageS/cache"\n \n ) \n func API(c *cache.Cache) []string {\n    cached_items, exists := c.Get("api")\n    if exists {\n        return cached_items\n    } \n items := []string{"a", "lot", "of", "data"}\nc.Set("api", items)\nreturn items\n \n } \n func main() {\n    cache := NewCache(time.Minute)\n    items := API(cache) \n fmt.Printf("items: %v", items)\n \n }\n```', 'headline \n Frontend for Headline website. Backend can be found at  headline-go . \n Getting started \n You should get  Node > 6.x  and  npm > 3.x . \n Now run \n $ npm install --global typescript webpack webpack-dev-server tslint\n$ npm install\n$ npm start\n \n now open browser to  localhost:3000  and done! :)', 'Chirp \n \n \n \n \n Chirp is simplified Twitter written in Angular 2 and Go. You can start\nfully working website with just one line. \n Setup \n Before we begin we have to install  docker-compose  command  Install .\nImportant is that  docker-compose  should have  >1.6  version! \n $ make production\n \n To be able to use system you need to add to  /etc/hosts : \n 127.0.0.1   backend.show frontend.show \n It is because we are not using any external domains yet. Then you can just hit\n frontend.show/  and now you are able to access fully working project ^^.', 'grmr \n \n \n \n grmr  is simple command for improving writing and grammar checking. \n Scripts were inspired by: http://matt.might.net/articles/shell-scripts-for-passive-voice-weasel-words-duplicates/ \n Installation \n $ cargo install grmr \n Usage \n $ grmr paper.tex ~/folder/*.tex            # general check of documents\n$ grmr weasel paper.tex ~/folder/*.tex     # check documents for weasel words\n$ grmr passive paper.tex ~/folder/*.tex    # check documents for passive voice\n$ grmr ill paper.tex ~/folder/*.tex        # check documents for illusion/duplicate words\n$ grmr -h                                  # show help']
hackerkid,["Hey, how are you doing today!! \n I moved the blog to Ghost. It's acessible at https://blog.vishnuks.com. So this repo is less likely to receive updates in the future. \n Hi, I'm Vishnu Ks and this is the source code for my blog, http://vishnuks.com/blog. Feel free to browse the source, fork at will, and ask me questions. Sharing is caring! \n How it's built \n All my posts are written in Markdown. The blog is powered by Jekyll, a static site generator that takes Markdown blog posts and converts them into HTML files. The benefit of this approach are many: \n The blog can be served with almost any web server, since the output of Jekyll is just flat HTML files.\nThe whole blog can easily be version controlled.\nThe blog requires less maintainance (goodbye out-of-date Wordpress installations!)\n \n I host the actual site on Github pages(as it is evident from the repo name) since I don't want to startup a server just for hosting some static pages. The theme is built on top of  Dasper .  \n That's pretty much it.", 'Contains most of the musings and findings of a Hacker', " Rocket Chat \n An awesome open source group chat software build in Node.js for running alongside Apache.  \n \n Installation \n Prerequisites: \n \n Git \n Node \n \n Now just clone and start the app: \n ```sh\ngit clone https://github.com/Hackerkid/Rocket-Chat.git\ncd Rocket.Chat\nnpm install\ncd server\nnode app.js \n ```\nMake sure to clone the contents in the www/html directory of the Apache server. Open the index.php file in browser to enjoy the chat. \n Features \n Current \n \n BYOS (bring your own server) \n Multiple Rooms \n Direct Messages \n Public Channels \n Transcripts / History \n \n Up Coming features \n \n WebRTC signalling \n File uploads \n Full text search \n REST-like APIs \n Off-the-Record (OTR) Messaging \n LDAP / Kerberos Authentication \n XMPP Multi-user chat (MUC) \n Native Mobile App \n Native Desktop App \n \n Contributions \n We Need Your Help! \n A lot of work has already gone into Rocket Chat, but we have much bigger plans for it! \n So if you'd like to be part of the project, please check out the  roadmap  and  issues  to see if there's anything you can help with. \n License \n Note that Rocket Chat is distributed under the  MIT License .", 'Hacker News Syncer For Wordpress \n A plugin for syncing the Hackernews posts with Wordpress blog. It syncs your Wordpress blog with Hacker news posts which satisfies a minimum Karma. \n Screenshots \n Plugin Settings Page \n \n Sample Posts \n \n Send pull request for extending features. ', "Project-Euler \n Contains my submissions for  Project Euler . Don't use this untill your solution get accepted", "LightOJ Solutions \n LightOJ  Solutions with Hints in C++. \n \n  🔥🔥\n  Check out my latest project -  diff.blog \n \n \n Guidelines \n HINTS \n Hints to selected problems of LightOJ. Feel free to add or update the hints by sending a pull a request. If the  hints  are not present for a problem, refer to the LightOJ forum.  \n 1002 - Country Roads \n You need to use a modification of Dijkstra's algorithm for solving thr peorblem. They key catch is in Dijkstra we use  \n a[i][j] + d[i] < d[j] \n d[j] = a[i][j] + d[i] \n While here we should use \n max(a[i][j], d[i]) < d[j] \n d[j] = max(a[i][j], d[i]) \n 1003 - Drunk \n The task is to check whether cycle exists in the directed graph. If cycle exists he can't drink all. You can use a hash function or map to convert the drinks to numbers. Once converted use DFS or a suitable algo for checking the existance of cycles. I used  DFS  for the search of cycles.  \n 1004 - Monkey Banana Problem \n This is a basic Dynamic Programming problem. The tricky part is to write the loops. While calculating the maximum bananas the second inner loop will run forward till i < n and for the remaining it would run backward. Check the solution for understanding the logic behind this.  \n 1005 - Rooks \n Normal recursion problem.  \n 1006 - Hex-a-bonacci \n This question is quite simple and straight forward as compared to the previous questions. All you need to dos is to replace the recursion with a for loop. Also make sure to take the modulo before storing the values in array as the values can be quite large \n 1007 - Mathematically Hard \n You should take exereme care while solving this problem. The input require very first IO method. Scanf and Printf would be the best choice if you are using C++. You can need to use a modified version of the Sieve method for cracking the problem. The approach I used was to first store the prime numbers using the sieve method and then to use a sieve like method to calculate  phi of n . Take extra care to use  llu  while printing out the answer as the output is in the range of unsigned long long. \n 1008 - Fibsieve`s Fantabulous Birthday \n Try to find the pattern in which the numbers are appearing. If you look carefully you can obtain the pattern and device an formula to find the coordinates.  \n 1009 - Back to Underworld \n I didn't use any normal graph algorithm for solving this problem. Even tough the problem uses a little concept of connected componenents. Try approaching the problem by considering the rivals as nodes in a graph. Check the code if you are stuck for a long time \n 1010 - Knights in Chessboard \n Adhoc problem. Try finding the pattern \n 1011 - Marriage Ceremonies \n This is a normal Memoization with Bit masking type problem. Normal recursion without memoization causes a TLE.  \n 1012 - Guilty Prince \n This is one is a really simple graph problem. Keep marking the visited nodes to avoid visiting them again \n 1013- Love Calculator \n Awesome LCS type problem. Use the solution of first to find the solution of second in a similiar method. \n 1014 - Ifter Party \n This is also a normal Ad-Hoc problem. The simplified task is to find the divisors of p - l which are greater than l. \n 1016 - Brush II \n This is yet another adhoc problem. What you need to do is to ietrate over all the y points in ascending order in such a way that you increase the counter only when the difference of ym, ym+1, ....yn becomes greater than w. The counter would give you the answer.  \n 1017 - Brush III \n Normal DP problem. \n 1019 - Brush IV \n This is a trivial application of Shortest Path algorithms. Implementing Floyd Warshal solution for this can accept the solution in a few lines of code \n 1020 - A Childhood game \n This is a problem under Game theory. You can write the solution for this problem by just observing the pattern of win or loss. If Alice is taking the stone first she will get a win for 0, 2, 3, 5 .... and loss for 1, 4, 7 .... You can easily use this observation for solving the problem.  \n 1022 - Circle in Square \n The only tricky part in this question is to use double for all the calculations. Always use double instead of float in programming contests.  \n 1023 - Discovering Permutations \n This is a pretty simple questions. You can use the C++  next_permutation  for cracking this problem in a few lines of code \n 1025 - The Specials Menu \n This is a dp problem \n 1027 - A Dangerous Maze \n The below example have been taken from the forum \n Let's consider test case #3: \n 3\n3 -6 -9 \n There are 3 doors, and we are to find the expected amount of time it will take to get out of the maze. Let's call that expected value E. Now, E has to be equal to the average of the times that it takes to get out from each door, correct? Let's say T1 is the amount of time to get out of the maze when taking door #1, and T2 and T3 are defined similarly. Then: \n E = (T1 + T2 + T3) / 3 \n It's easy to see that T1 is 3, but what about T2 and T3? You can think about it like this: if you take door number 2, you will spend 6 minutes going through the maze, and you end up at the starting point, and from there, how much time will it take you to get out? Well, E is the expected time it will take you to get out from the start, right? So, you can say: \n T2 = 6 + E\nT3 = 9 + E \n Putting all that together, the original equation becomes: \n E = 3/3 + (6/3 + E/3) + (9/3 + E/3 \n E = 6 + 2E/3 \n E/3 = 6 \n E = 18 \n Now convert the logic into simple code \n 1028 - Trailing Zeroes (I) \n If a number can be expressed as p1 ^ c1 * p2 ^ c2 * ..... pn ^ cn where p1 .. pn are the prime factors and c1 ... cn are the powers of the factors, then the total number of divisors of the number is given by  \n n = (c1 + 1) * (c2 + 1) * ... * (cn + 1) \n Check the code carefully for the implementation \n 1029 - Civil and Evil Engineer \n This is a simple application of minimum spanning trees. Use a priority queue for reducing the time complexity. \n 1033 - Generating Palindromes \n Find the LCS of the given string and the reverse string. Then subtract the LCS from the actual length of the string. Find LCS using DP.   \n 1034 - Hit the Light Switches \n Use Kosaraju’s algorithm for finding the connected componenets of the directed Graph. In Kosaraju’s algorithm we perform the second DFS in the transpose graph. Skipping the creation of transpose graph result in the generation of cconnected componenents instead of the  SCC  which is exactly we want \n 1035 - Intelligent Factorial Factorization \n I don't think you would need any hint for soloving this question. Don't try to generate factorial to solve this question.  \n 1037 - Agent 47 \n This can be solved using bit masking. Write a recurise solution and save the states.  \n 1040 - Donation \n Use Prims MST algorithm accordingly for solving this problem.  \n 1041 - Road Construction \n First you need to find the connected components of the graph. Then you need apply PRIMS MST in the connected components to find the minimum cost.  \n 1042 - Secret Origins \n This is an adhoc problem. Don't bruteforce. See how the bits changes from input to output answer.  \n 1044 - Paliandrome Partitioning \n This is a DP problem. you need to solve 2 seperate tasks for solving the problem. First task is to precompute the indexes of all the palidrome substrings. The second task is to solve the problem using the pre computed value with (1) lookup time.  \n 1045 - Digits Of A  Factorial \n You can find the number of digits required ny taking log of the factorial. This is equalent to \n log(N!) = log(1) + log(2) + .... + log(N) \nPrecompute and store the values to avoid doing repetetive tasks.  \n 1046 - Rider \n Use BFS for solving this problem. You need to find the shortest distance from each rider to all the cells in the chess board using BFS. Then use a simple loop for choosing the best cell.  \n 1047 - Neighbour House \n Normal DP problem. Try solving it yourself.  \n 1049 - One Way Roads \n This is a normal ad hoc problem. You can solve this by trying the two possible way of road arrangements, one being mere the compliment of the other.  \n 1051 - Good Or Bad \n DP problem. I solved it by using memoization and considering only characters of the string from the point you are evaulating to save the state for memoization. I used only 5 characters because they only decide the outcome of the that point.  \n 1053 - Higher Math \n If the triangle is right angle  a * a + b * b == c * c \n 1057 - Collecting Gold \n Yet another dynamic programming problem.  \n 1082 - Array Queries \n Trivial solution. Use a segment tree.  \n 1093 - Ghakjini \n Use two priority queues. One for tracking maximum element in memeory and one for tracking minimum element.  \n 1094 - Farthest Node In Tree \n Do to DFS. First one would return one node. The seond one from that node will get return you the other \n 1326 - Race \n Assume that there are N number of ways for finishing in M places. How can you find the number of ways to finish M+1 places?", '\n \n \n How it works? \n \n The home page of Wikifeedia consist of a news feed of Featured articles. Featured articles are considered to be the best articles in Wikipedia. As of now there is no API for getting a list of featured articles that changes each time one make a request. To facilitate this I had to to make a new API server along with some Javascript tweaks. \n The backend of Wikifeedia can be found  here . \n Installation \n git clone https://github.com/hackerkid/Wikifeedia/\ncd Wikifeedia\npython -m SimpleHTTPServer \nNow browse to http://localhost:8000 .  \n Equivalent python3 command is  python3 -m http.server   \n Contributing \n Your code goes live the moment it gets merged. Excited? Here is a small list of ToDos you can start working right now.  \n TODO \n \n Improve the UI. \n The search results can be improved. \n Custom home page \n Implement any MV* framework ( preferably Backbone / React ) \n \n Your own ideas are also welcome as pull requests. ', 'HEFT-Scheduling Algorithm \n C++ implemntation of Performance-effective and low-complexity task scheduling for heterogeneous computing \n About \n Efficient application scheduling is critical for achieving high performance in heterogeneous computing environments. The \napplication scheduling problem has been shown to be NP-complete in general cases as well as in several restricted cases.\nBecause of its key importance, this problem has been extensively studied and various algorithms have been proposed in the \nliterature which are mainly for systems with homogeneous processors. Although there are a few algorithms in the literature \nfor heterogeneous processors, they usually require significantly high scheduling costs and they may not deliver good quality\nschedules with lower costs. In this paper, we present two novel scheduling algorithms for a bounded number of heterogeneous \nprocessors with an objective to simultaneously meet high performance and fast scheduling time, which are called the\nHeterogeneous Earliest-Finish-Time (HEFT) algorithm and the Critical-Path-on-a-Processor (CPOP) algorithm. The HEFT algorithm \nselects the task with the highest upward rank value at each step and assigns the selected task to the processor, which\nminimizes its earliest finish time with an insertion-based approach. On the other hand, the CPOP algorithm uses the summation \nof upward and downward rank values for prioritizing tasks. Another difference is in the processor selection phase, which \nschedules the critical tasks onto the processor that minimizes the total execution time of the critical tasks. In order to \nprovide a robust and unbiased comparison with the related work, a parametric graph generator was designed to generate weighted\ndirected acyclic graphs with various characteristics. The comparison study, based on both randomly generated graphs and the \ngraphs of some real applications, shows that our scheduling algorithms significantly surpass previous approaches in terms of\nboth quality and cost of schedules, which are mainly presented with schedule length ratio, speedup, frequency of best results,\nand average scheduling time metrics. \n Research Paper \n Link \n Authors \n \n Topcuoglu, H - Comput. Eng. Dept., Marmara Univ., Istanbul, Turkey  \n Hariri, S. - Min-You Wu \n', '  \n Wikipedia turned into a Newsfeed \n \n How it works? \n The home page of Wikifeedia consist of a news feed of Featured articles. Featured articles are considered to be the best articles in Wikipedia. As of now there is no API for getting a list of featured articles that changes each time one make a request. To facilitate this I had to to make a new API server along with some Javascript tweaks. \n The frontend of Wikifeedia can be found  here . \n Contributing \n Your code goes live the moment it gets merged. Excited? Here is a small list of ToDos you can start working right now.  \n TODO \n \n Improve the UI. \n The search results can be improved. \n Custom home page \n Implement any MV* framework ( preferably Backbone / React ) \n \n Your own ideas are also welcome as pull requests. ', "Mind Expanding Books \n \n  🔥🔥\n  Check out my latest project -  diff.blog \n \n \n \n \n Books everyone should read! \n \n 🌱 You might be interested in the beta website at  https://books.vishnuks.com . Source code live in  app/  directory. \n Table of Contents \n \n Mind Expanding Books \n Table of Contents \n Books \n Startups and Business \n Philosophy And Psychology \n Autobiographies and Biographies \n History \n Science and Medicine \n Logic and Problem Solving \n Politics \n Economics \n Gender \n Sexuality \n Race \n Education \n Writing \n Theater and Film \n Shakespeare \n Fiction \n Classics \n Fantasy \n Historical Fiction \n Humor \n Mystery \n Science Fiction \n Horror \n Miscellaneous \n \n \n Health \n Design \n Travel \n Language \n Nature \n Art \n Credits \n License \n \n Books \n Startups and Business \n | Name | Author | Goodreads Rating | Year Published | \n|------|--------|------------------|----------------| \n| Shoe Dog: A Memoir by the Creator of Nike | Phil Knight |  4.46  | 2016 |\n| The Ride of a Lifetime: Lessons Learned from 15 Years as CEO of the Walt Disney Company | Robert Iger, Joel Lovell |  4.44  | 2019 |\n| Bad Blood: Secrets and Lies in a Silicon Valley Startup | John Carreyrou |  4.4  | 2018 |\n| The Compound Effect | Darren Hardy |  4.40  | 2010 | \n| Never Split the Difference | Chris Voss |  4.39  | 2016 |\n| Failing to Succeed: The Story of India's First E-Commerce Company | K. Vaitheeswaran |  4.39  | 2017 |\n| High Output Management | Andy Grove |  4.38  | 1995 | \n| Outwitting the Devil: The Secret to Freedom and Success | Napoleon Hill |  4.38  | 2011 |\n| How to Get From Where You Are to Where You Want to Be : The 25 Principles of Success | Jack Canfield, Janet Switzer |  4.26  | 2007 | \n| The Big Short: Inside the Doomsday Machine | Michael Lewis |  4.26  | 2015 |\n| The Future Is Faster Than You Think: How Converging Technologies Are Transforming Business, Industries, and Our Lives | Peter H. Diamandis, Steven Kotler |  4.25  | 2020 |\n| The Middle Finger Project | Ash Ambirge |  4.22  | 2020 | \n| The Score Takes Care of Itself: My Philosophy of Leadership | Bill Walsh, Steve Jamison, Craig Walsh |  4.21  | 2009 | \n| The Hard Thing About Hard Things: Building a Business When There Are No Easy Answers | Ben Horowitz |  4.20  | 2014 | \n| Think and Grow Rich | Napoleon Hill |  4.17  | 1937 |\n| AI Superpowers: China, Silicon Valley, and the New World Order |  Kai-Fu Lee |  4.16  | 2018 |\n| Nail it then Scale it: The Entrepreneur's Guide to Creating and Managing Breakthrough | Nathan Furr |  4.13  | 2011 |\n| Outliers: Story of Success | Malcolm Gladwell |  4.11  | 2008 | \n| The Strategy and Tactics of Pricing : A Guide to Move More Profitable | Thomas Nagle, John Hogan, Joseph Zale |  4.09  | 2005 | \n| Principles: Life and Work | Ray Dalio |  4.07  | 2017 |\n| Start with Why: How Great Leaders Inspire Everyone to Take Action | Simon Sinek |  4.07  | 2009 | \n| Leaving Microsoft to Change the World: An Entrepreneur's Odyssey to Educate the World's Children | John Wood |  4.07  | 2006 | \n| Hackers and Painters | Paul Graham |  4.06  | 2004 | \n| How Google Works | Eric Schmidt, Jonathan Rosenberg |  4.06  | 2014 |\n| Good to Great: Why Some Companies Make the Leap And Others Don't | Jim Collins |  4.05  | 2001 | \n| The Lean Startup: How Constant Innovation Creates Radically Successful Businesses | Eric Ries |  4.04  | 2011 | \n| The Power of Full Engagement: Managing Energy, Not Time, is the Key to High Performance and Personal Renewal | Jim Loehr, Tony Schwartz |  4.03  | 2005 | \n| The Checklist Manifesto: How To Get Things Right | Atul Gawande |  4.03  | 2009 | \n| Where Good Ideas Come From | Steven Johnson |  4.03  | 2010 | \n| Delivering Happiness: A Path to Profits, Passion, and Purpose | Tony Hsieh |  4.01  | 2010 | \n| The Innovator's Dilemma | clayton m. christensen |  4.00  | 1997 | \n| Influencer: The New Science of Leading Change | Joseph Grenny, Kerry Patterson |  3.98  | 2007 | \n| The E-Myth Revisited: Why Most Small Businesses Don't Work and What to Do About It | Michael E. Gerber |  3.98  | 1995 | \n| Getting Things Done: The Art of Stress-Free Productivity | David Allen |  3.98  | 2001 | \n| Losing My Virginity: How I Survived, Had Fun, and Made a Fortune Doing Business My Way | Richard Branson |  3.97  | 1998 | \n| SuperFreakonomics: Global Cooling, Patriotic Prostitutes, and Why Suicide Bombers Should Buy Life Insurance | Steven D. Litt |  3.96  | 2009 | \n| The Ten Faces of Innovation | Tom Kelley |  3.96  | 2005 | \n| Rich Dad Poor Dad : What The Rich Teach Their Kids About Money That The Poor And Middle Class Don't | Robert T. Kiyosaki |  3.95  | 1997 | \n| Founders At Work: Stories Of Startups Early Days | Jessica Livingston |  3.94  | 2007 | \n| The Paypal Wars | Eric M. Jackson |  3.94  | 2010 | \n| Lean In : Women, Work and the Will to Lead | Sheryl Sandberg |  3.94  | 2013 | \n| ReWork: Change the Way You Work Forever | David Heinemeier Hansson |  3.93  | 2010 | \n| The 22 Immutable Laws Of Branding | Laura Ries and Al Ries |  3.93  | 1998 | \n| Freakonomics: A Rogue Economist Explores the Hidden Side of Everything | Steven D. Levitt, Stephen J. Dubner |  3.93  | 2006 | \n| The Tipping Point: How Little Things Can Make a Big Difference | Malcolm Gladwell |  3.92  | 2002 | \n| The Four Steps to the Epiphany | Steve Blank |  3.92  | 2005 | \n| The Curse of the Mogul | Jonathan A. Knee, Bruce C. Greenwald, Ava Seave |  3.91  | 2009 | \n| David and Goliath: Underdogs, Misfits, and the Art of Battling Giants | Malcolm Gladwell |  3.90  | 2013 | \n| Strategy for Sustainability: A Business Manifesto | Adam Werbach |  3.89  | 2009 | \n| The Black Swan: The Impact of the Highly Improbable | Nassim Nicholas Taleb |  3.88  | 2007 | \n| Contagious: Why Things Catch On | Jonah Berger |  3.88  | 2013 | \n| Who Says Elephants Can't Dance?: Leading a Great Enterprise through Dramatic Change | Louis V Gerstner, Jr |  3.88  | 2003 | \n| Linchpin: Are You Indispensable | Seth Godin |  3.87  | 2010 | \n| The $100 Startup: Reinvent the Way You Make a Living, Do What You Love, and Create a New Future | Chris Guillebeau |  3.85  | 2012 | \n| Too Big to Fail | Andrew Ross Sorkin |  3.85  | 2011 | \n| The Paradox of Choice: Why More Is Less | Barry Shwartz |  3.84  | 2005 | \n| The Art of the Start: The Time-Tested, Battle-Hardened Guide for Anyone Starting Anything | Guy Kawasaki |  3.82  | 2004 | \n| Crowdsourcing: Why the Power of the Crowd Is Driving the Future of Business | Jeff Howe |  3.82  | 2008 | \n| Cognitive Surplus: Creativity and Generosity in a Connected Age | Clay Shirky |  3.82  | 2010 | \n| Data Driven | D.J. Patil |  3.80  | 2015 |\n| Zero to One | Peter Thiel, Blake Masters |  3.75  | 2014 | \n| One Click: Jeff Bezos and the Rise of Amazon.com | Richard Brandt |  3.75  | 2011 | \n| Big Data: A Revolution That Will Transform How We Live, Work, and Think | Kenneth Cukier |  3.70  | 2013 | \n| The 4-Hour Work Week: Escape the 9-5, Live Anywhere and Join the New Rich | Timothy Ferriss |  3.61  | 2012 | \n| The Accidental Billionaires : The Founding of Facebook, a Tale of Sex, Money, Genius and Betrayal | Ben Mezrich |  3.50  | 2009 |   \n If you like Mind Expanding books you should check out my new project http://diff.blog. It's a platform that help you keep updated with the developer community. \n Philosophy And Psychology \n | Name | Author | Goodreads Rating | Year Published | \n|------|--------|------------------|----------------|\n| Karma Yoga | Swami Vivekanand |  4.51  | 1999 |\n| The Ascent of Humanity | Charles Eisenstein |  4.43  | 2007\n| Tattoos on the Heart: The Power of Boundless Compassion | Gregory Boyle |  4.41  | 2010 | \n| The Slight Edge - Secret to a Successful Life | Jeff Olson |  4.39  | 2005 |\n| Education and the Significance of Life | Krishnamurti |  4.38  | 2008 | \n| Passions Within Reason: The Strategic Role of the Emotions | Robert H. Frank |  4.34  | 1989 |\n| Man's Search for Meaning | Viktor E. Frankl |  4.33  | 2006 | \n| Atomic Habits: An Easy & Proven Way to Build Good Habits & Break Bad Ones| James Clear |  4.32  | 2018 |\n| The User Illusion: Cutting Consciousness Down to Size | Tor Norretranders |  4.30  | 1999 | \n| Born to Run: A Hidden Tribe, Superathletes, and the Greatest Race the World Has Never Seen | Christopher McDougall |  4.27  | 2009 | \n| Prometheus Rising | Robert Anton Wilson |  4.26  | 2010 | \n| The Concise 48 Laws of Power | Robert Greene |  4.25  | 2011 | \n| The Hero with a Thousand Faces | Joseph Campbell |  4.24  | 1972 | \n| Zen Mind, Beginner's Mind | Shunryu Suzuki, David Chadwick |  4.23  |  | \n| The Age of Insight: The Quest to Understand the Unconscious in Art, Mind, and Brain, from Vienna 1900 to the Present | Eric Kandel |  4.22  | 2012 |\n| Meditations | Marcus Aurelius |  4.21  |  |\n| Influence: The Psychology of Persuasion | Robert B. Cialdini |  4.18  | 2006 | \n| The Gift of Fear: Survival Signals That Protect Us from Violence | Gavin de Becker |  4.18  | 2000 | \n| Gifts of Imperfection | Brene Brown |  4.18  |  | \n| Metamagical Themas: Questing For The Essence Of Mind And Pattern | Douglas Hofstadter |  4.16  | 1996 | \n| The Truth About Everything: An Irreverent History of Philosophy : With Illustrations | Matthew Stewart |  4.15  | 1997 | \n| Illusions: The Adventures of a Reluctant Messiah | Richard Bach |  4.15  | 1977 | \n| How to Win Friends and Influence People | Dale Carnegie |  4.14  | 1998 | \n| Creative Visualization: Use the Power of Your Imagination to Create What You Want in Your Life | Shakti Gawain |  4.13  | 2002 | \n| Bulfinch's Mythology | Thomas Bulfinch |  4.13  | 1998 | \n| Ego Is the Enemy | Ryan Holiday |  4.12  | 2016 | \n| Awaken the Giant Within: How to Take Immediate Control of Your Mental, Emotional, Physical and Financial Destiny! | Anthony Robbins |  4.11  | 1991 | \n| The Power of Now: A Guide to Spiritual Enlightenment | Eckhart Tolle |  4.11  | 1997 | \n| Tuesdays with Morrie | Mitch Albom |  4.11  | 1997 |\n| Predictably Irrational | Dan Ariely |  4.10  | 2008 | \n| Flow: The Psychology of Optimal Experience | Mihaly Csikszentmihalyi |  4.10  | 1990 | \n| Thinking, Fast and Slow | Daniel Kahneman |  4.09  | 2011 | \n| Irrational Man: A Study in Existential Philosophy | William Barrett |  4.09  |  |\n| Happy: Why More or Less Everything is Absolutely Fine | Derren Brown |  4.08  | 2016 |\n| The Beginning of Infinity: Explanations That Transform the World | David Deutsch |  4.08  | 2011 | \n| Metaphors We Live By | George Lakoff, Mark Johnson |  4.08  | 2003 | \n| Grit | Angela Duckworth |  4.08  | 2016 | \n| In Defense of Food: An Eater's Manifesto | Michael Pollan |  4.07  | 2008 | \n| The Blank Slate: The Modern Denial of Human Nature | Steven Pinker |  4.07  | 2003 | \n| The History of Western Philosophy | Bertrand Russell |  4.06  | 2008 | \n| Moral Tribes: Emotion, Reason, and the Gap Between Us and Them | Joshua Greene |  4.05  | 2013 |\n| Thus Spoke Zarathustra | Friedrich Nietzsche |  4.05  | 1883 |\n| The Elephant in the Brain: Hidden Motives in Everyday Life | Kevin Simler, Robin Hanson |  4.05  | 2017 |\n| Quiet: The Power of Introverts in a World That Can't Stop Talking | Susan Cain |  4.04  | 2012 | \n| The Future of the Mind: The Scientific Quest to Understand, Enhance, and Empower the Mind | Michio Kaku |  4.04  | 2014 | \n| Every Thing Must Go: Metaphysics Naturalized | James Ladyman, Don Ross, et al |  4.04  | 2007 | \n| Antifragile: Things That Gain from Disorder (Incerto) | Nassim Nicholas Taleb |  4.04  | 2012 | \n| Mindset: The new psychology of success | Carol S. Dweck,Ph.D. |  4.04  | 2006 |\n| The Power of Habit: Why We Do What We Do In Life And Business | Charles Duhigg |  4.03  | 2012 | \n| The Upside of Irrationality: The Unexpected Benefits of Defying Logic | Dan Ariely |  4.00  | 2010 | \n| Outsmarting IQ: The Emerging Science of Learnable Intelligence | David Perkins |  4.00  | 1995 | \n| The Art of War | Sun Tzu |  3.95  | 2005 | \n| Willpower: Rediscovering the Greatest Human Strength | Ron P. Baumeister, John Tierney |  3.95  | 2011 | \n| A Whole New Mind: Why Right-Brainers Will Rule the Future | Daniel H. Pink |  3.95  |2006  | \n| Vagabonding: An Uncommon Guide to the Art of Long-Term World Travel | Rolf Potts |  3.92  |2002  | \n| Supernormal: Science, Yoga, and the Evidence for Extraordinary Psychic Abilities | Dean Radin |  3.91  | 2013 | \n| Prisoner's Dilemma | William Poundstone |  3.91  | 1993 |\n| The Stuff of Thought: Language as a Window into Human Nature | Steven Pinker |  3.90  | 2007 | \n| Consilience: The Unity of Knowledge | Edward O. Wilson |  3.90  | 1999 | \n| Why We Make Mistakes: How We Look Without Seeing, Forget Things in Seconds, and Are All Pretty Sure We Are Way Above Average | Joseph T. Hallinan |  3.87  | 2009 | \n| You Just Don't Understand: Women and Men in Conversation | Deborah Tannen |  3.86  | 2007 | \n| Just the Arguments: 100 of the Most Important Arguments in Western Philosophy | Michael Bruce, Steven Barbone |  3.85  | 2011 | \n| What the Dog Saw: and other adventures | Malcolm Gladwell |  3.82  | 2009 | \n| Spent: Sex, Evolution, and Consumer Behavior | Geoffrey Miller |  3.82  | 2009 | \n| Games People Play: The Basic Handbook of Transactional Analysis | Eric Berne. |  3.81  | 1964 | \n| This Will Make You Smarter: New Scientific Concepts to Improve Your Thinking | John Brockman |  3.81  | 2012 | \n| Intuition Pumps And Other Tools for Thinking | Daniel C. Dennett |  3.80  | 2014 | \n| Jonathan Livingston Seagull | Richard Bach |  3.80  |  1970| \n| Why Does the World Exist?: An Existential Detective Story | Jim Holt |  3.77  | 2012 | \n| The Social Contract | Jean-Jacques Rousseau, Maurice Cranston |  3.75  | 2009 | \n| The Ethical Brain: The Science of Our Moral Dilemmas | Michael S. Gazzaniga |  3.70  | 2006 | \n| Why Smart People Can Be So Stupid | Robert J Sternberg |  3.68  | 2003 | \n| Alone Together: Why We Expect More from Technology and Less from Each Others | Sherry Turkle |  3.62  | 2011 | \n| How to Win Every Argument: The Use and Abuse of Logic | Madsen Pirie |  3.55  | 2006 | \n| Rapt: Attention and the Focused Life: Winifred Gallagher | Winifred Gallagher |  3.54  | 2009 | \n| Utilitarianism | Geoffrey Scarre |  3.50  | 1996 | \n| The Fine Art of Small Talk: How To Start a Conversation, Keep It Going, Build Networking Skills -- and Leave a Positive Impression | Debra Fine |  3.36  | 2005 | \n| Snoop: What Your Stuff Says About You: Sam Gosling | Sam Gosling |  3.33  | 2001 |  \n Autobiographies and Biographies \n | Name | Author | Goodreads Rating | Year Published | \n|------|--------|------------------|----------------| \n| Becoming | Michelle Obama |  4.53  | 2018 | \n| Unbroken: A World War II Story of Survival, Resilience, and Redemption | Laura Hillenbrand |  4.39  | 2010 | \n| Everything Beautiful in Its Time: Seasons of Love and Loss | Jenna Bush Hager |  4.38  | 2020 |\n| Long Walk to Freedom | Nelson Mandela |  4.31  | 1995 | \n| My Life and My Work | Henry Ford |  4.29  | 1922 | \n| Surely You're Joking, Mr. Feynman!: Adventures of a Curious Character | Richard Feynman |  4.29  | 1997 | \n| The Autobiography of Malcolm X | Malcolm X, Alex Haley |  4.29  |1965  | \n| The Third Door:The Wild Quest to Uncover How the World's Most Successful People Launched Their Careers| Alex banayan| 4.28  | 2018 |\n| Elon Musk: Inventing the Future | Ashlee Vance |  4.25  | 2015 | \n| The Last Lecture | Randy Pausch |  4.25  | 2008 | \n| Freedom in Exile | Dalai Lama XIV |  4.24  | 1991 | \n| Madame Curie: A Biography | Marie Curie |  4.22  | 2001 | \n| Autobiography of a Yogi | Paramahansa Yogananda |  4.21  | 1946 |\n| Wings of Fire | A.P.J. Abdul Kalam |  4.20  | 2011 | \n| The Man Who Knew Infinity : A Life of the Genius Ramanujan | Robert Kanigel |  4.15  | 1991 | \n| An Astronaut's Guide to Life on Earth | Chris Hadfield |  4.14  | 2013 | \n| The Diary of a Young Girl | Anne Frank |  4.10  | 1993 | \n| The Innovators: How a Group of Hackers, Geniuses and Geeks Created the Digital Revolution | Walter Isaacson |  4.10  | 2014 |\n| Einstein: His Life and Universe | Walter Isaacson |  4.09  | 2007 |\n| Steve Jobs | Walter Issacson |  4.09  | 2011 | \n| My Experiments with Truth | Mahatma Gandhi |  4.07  | 1993 | \n| The Story of My Life | Helen Keller |  4.07  | 1990 | \n| The Immortal Life of Henrietta Lacks | Rebecca Skloot |  4.05  | 2010 |\n| Losing My Virginity: The Autobiography | Richard Branson |  3.97  |1999| \n| My Inventions: The Autobiography of Nikola Tesla | Nikola Tesla |  3.96  | 2006 |\n| Ghost in the Wires:   My Adventures as the World's Most Wanted Hacker | Kevin Mitnick |  3.95  | 2011 | \n| An Autobiography: Toward Freedom | Jawaharlal Nehru |  3.85  | 1989 | \n| iWoz: Computer Geek to Cult Icon: How I Invented the Personal Computer, Co-Founded Apple, and Had Fun Doing It | Steve Wozniak |  3.82  | 2007 |   \n History \n | Name | Author | Goodreads Rating | Year Published | \n|------|--------|------------------|----------------| \n| Sapiens: A Brief History of Humankind | Yuval Noah Harari |  4.42  | 2014 | \n| How the Internet Happened: From Netscape to the iPhone | Brian McCullough |  4.36  | 2018 |\n| The Discoverers | Daniel J. Boorstin |  4.34  | 2012 | \n| Homo Deus: A History of Tomorrow | Yuval Noah Harari |  4.27  | 2017 |\n| One Minute to Midnight: Kennedy, Khrushchev, and Castro on the Brink of Nuclear War | Michael Dobbs |  4.20  | 2008 | \n| The Codebreakers: The Comprehensive History of Secret Communication from Ancient Times to the Internet | David Kahn |  4.18  | 1996 | \n| Blindness | José Saramago |  4.13  | 1995 |\n| From Dawn to Decadence: 500 Years of Western Cultural Life 1500 to the Present | Jacques Barzun |  4.12  | 2001 | \n| Why the West Rules--for Now: The Patterns of History, and What They Reveal About the Future | Ian Morris |  4.09  | 2010 | \n| The Nazi Doctors: Medical Killing and the Psychology of Genocide | Robert Jay Lifton |  4.07  | 1986 | \n| God Created The Integers: The Mathematical Breakthroughs that Changed History | Stephen Hawking |  4.06  |  | \n| Churchill's Secret War: The British Empire and the Ravaging of India during World War II | Madhusree Mukerjee |  4.05  | 2010 | \n| El Tiempo Entre Costuras | María Dueñas |  4.04  |2009 | \n| Guns, Germs, And Steel : The Fates Of Human Societies | Jared M. Diamond |  4.00  | 2005 | \n| Discovery of India | Jawaharlal Nehru |  3.98  | 1946 | \n| The Rational Optimist: How Prosperity Evolves (P.S.) | Matt Ridley |  3.98  | 2010 | \n| Kingpin: How One Hacker Took Over the Billion-Dollar Cybercrime Underground | Kevin Poulson |  3.96  | 2011 | \n| The Ghost Map: The Story of London's Most Terrifying Epidemic--and How It Changed Science, Cities, and the Modern World | Steven Johnson |  3.91  |2006 | \n| The Rise and Fall of the British Empire | Lawrence James |  3.89  | 1997 | \n| The Master Switch: The Rise and Fall of Information Empires | Tim Wu |  3.89  | 2010 | \n| Spycraft | Robert Wallace, H. Keith Melton, Henry R. Schlesinger |  3.81  |1987  | \n| Civilization: The West and the Rest: Niall Ferguson | Niall Ferguson |  3.78  | 2011 | \n| The Revenge of Geography: What the Map Tells Us About Coming Conflicts and the Battle Against Fate | Robert D. Kaplan |  3.68  | 2012 | \n| The World Is Flat: A Brief History of the Twenty-first Century | Thomas L. Friedman |  3.66  | 2006 |   \n Science and Medicine \n | Name | Author | Goodreads Rating | Year Published | \n|------|--------|------------------|----------------| \n| Behave: The Biology of Humans at Our Best and Worst | Robert M. Sapolsky |  4.42  | 2017 | \n| Asimov's New Guide to Science | Isaac Asimov |  4.36  | 1985 |\n| Factfulness: Ten Reasons We're Wrong About the World – and Why Things Are Better Than You Think | Hans Rosling |  4.36  | 2018 |\n| The Gene: An Intimate History | Siddhartha Mukherjee |  4.35  | 2016 |\n| Origins of Form: The Shape of Natural and Man-Made Things | Christopher Williams |  4.31  | 2013 | \n| The Body: A Guide for Occupants | Bill Bryson |  4.31  | 2019 | \n| Gödel, Escher, Bach: An Eternal Golden Braid | Douglas R. Hofstadter |  4.30  | 1979 | \n| The Little Schemer | Daniel P. Friedman, Matthias Felleisen, Duane Bibby, Gerald J. Sussman |  4.27  | 1995 | \n| Scientific Genius: A Psychology of Science | Dean Keith Simonton |  4.25  | 1988 | \n| Out of Control: The New Biology of Machines, Social Systems, and the Economic World | Kevin Kelly |  4.24  | 1995 | \n| The Brain: The Story of You | David Eagleman |  4.20  | 2015 |\n| A Short History of Nearly Everything | Bill Bryson |  4.19  |2003  | \n| Things to Make and Do in the Fourth Dimension |  Matt Parker |  4.19  | 2014 | \n| The Symbolic Species: The Co-evolution of Language and the Brain | Terrence W. Deacon |  4.17  | 1998 | \n| A Brief History of Time | Stephen Hawking |  4.12  | 1998 | \n| The Selfish Gene | Richard Dawkins |  4.11  |1976  | \n| Figments of Reality: The Evolution of the Curious Mind | Ian Stewart, Jack Cohen |  4.11  | 2009 | \n| The New Executive Brain: Frontal Lobes in a Complex World | Elkhonon Goldberg |  4.07  |  | \n| The Extended Phenotype: The Long Reach of the Gene | Richard Dawkins |  4.07  | 1999 | \n| Rare Earth: Why Complex Life is Uncommon in the Universe | Peter D. Ward, Donald Brownlee |  4.06  | 2003 | \n| Stiff: The Curious Lives of Human Cadavers | Mary Roach |  4.05  | 2004 | \n| The Man Who Mistook His Wife for A Hat and Other Clinical Tales | Oliver Sacks |  4.05  | 1998 | \n| Darwin's Dangerous Idea | Daniel C. Dennett |  4.03  |1995  | \n| The Inevitable: Understanding the 12 Technological Forces That Will Shape Our Future | Kevin Kelly |  4.02  | 2016 | \n| The Outer Limits of Reason: What Science, Mathematics, and Logic Cannot Tell Us | Noson S. Yanofsky |  4.00  | 2013 | \n| The Structure of Scientific Revolutions: 50th Anniversary Edition | Thomas S. Kuhn |  3.99  | 1996 | \n| At Home in the Universe: The Search for the Laws of Self-Organization and Complexity | Stuart Kauffman |  3.97  | 1996 | \n| The Wealth of Networks: How Social Production Transforms Markets and Freedom | Yochai Benkler |  3.95  | 2006 | \n| Mind Children: The Future of Robot and Human Intelligence |  Hans Moravec. |  3.95  | 1990 | \n| Profiles of the Future: An Inquiry into the Limits of the Possible | Arthur C. Clarke |  3.94  | 1962 |\n| How to Find a Habitable Planet | James Kasting |  3.93  | 2010 | \n| The 10,000 Year Explosion: How Civilization Accelerated Human Evolution | Gregory Cochran, Henry Harpending |  3.91  | 2009 | \n| The Disappearing Spoon: And Other True Tales of Madness, Love, and the History of the World from the Periodic Table of the Elements | Sam Kean |  3.91  | 2010 | \n| Evolution for Everyone: How Darwin's Theory Can Change the Way We Think About Our Lives | David Sloan Wilson |  3.90  | 2007 |\n| Life at the Speed of Light: From the Double Helix to the Dawn of Digital Life | J. Craig Venter |  3.87  | 2013 |\n| The Shallows: What the Internet is Doing to Our Brains | Nicholas Carr |  3.85  | 2011 | \n| Catching Fire: How Cooking Made Us Human | Richard Wrangham |  3.83  | 2009 | \n| Heaven in a Chip: Fuzzy Visions of Society and Science in the Digital Age | Bart Kosko |  3.82  | 2000 | \n| The Quark and the Jaguar: Adventures in the Simple and the Complex | Murray Gell-Mann |  3.81  |1994  | \n| Tomorrowland: Our Staggering Journey from Science Fiction to Science Fact | Steven Kotler |  3.80  | 2015 | \n| Origins of Genius: Darwinian Perspectives on Creativity | Dean Keith |  3.74  | 1999 | \n| Waters of the World: the story of the scientists who unravelled the mysteries of our seas, glaciers, and atmosphere and made the planet whole | Sarah Dry |  3.65  | 2019 |\n| Mind Wars: Brain Research and National Defense | Jonathan D. Moreno |  3.45  | 2007 |   \n Logic and Problem Solving \n | Name | Author | Goodreads Rating | Year Published | \n|------|--------|------------------|----------------| \n| What Is the Name of This Book?: The Riddle of Dracula and Other Logical Puzzles | Raymond M. Smullyan |  4.24  | 2011 | \n| A Mind for Numbers: How to Excel at Math and Science even If You Flunked Algebra | Barbara Oakley |  4.22  | 2014 | \n| How to Solve It: A New Aspect of Mathematical Method | George Polya |  4.16  | 2015 |\n| Guide to Competitive Programming: Learning and Improving Algorithms Through Contests | Antti Laaksonen |  4.15  | 2016 |\n| Superforecasting: The Art and Science of Prediction |  Philip E. Tetlock, Dan Gardner |  4.10  | 2015 |\n| Problem Solving 101: A Simple Book | Ken Watanabe |  4.10  | 2009 | \n| Labyrinths of Reason: Paradox, Puzzles, and the Frailty of Knowledge | William Poundstone |  4.05  | 1989 | \n| Mazes for the Mind: Computers and the Unexpected | Clifford A. Pickover |  4.05  | 1992 |\n| Outnumbered: Exploring the Algorithms That Control Our Lives | David Sumpter |  3.99  | 2018 | \n Politics \n | Name | Author | Goodreads Rating | Year Published | \n|------|--------|------------------|----------------|\n| The New Jim Crow: Mass Incarceration in the Age of Colorblindness | Michelle Alexander |  4.49 | 2010\n| Justice: What's the Right Thing to Do? | Michael J. Sandel |  4.25  |  | \n| The Righteous Mind: Why Good People are Divided by Politics and Religion |  Jonathan Haidt |  4.16  | 2012 | \n| Strangers in Their Own Land: Anger and Mourning on the American Right | Arlie Russell Hochschild |  4.15  | 2016 | \n| Game Change: Obama and the Clintons, McCain and Palin, and the Race of a Lifetime | John Heilemann, Mark Halperin |  4.10  | 2010 | \n| Bolo'bolo | P.M. |  4.05  | 1984 | \n| The Myth of the Rational Voter: Why Democracies Choose Bad Policies | Bryan Caplan |  3.96  | 2007 | \n| The Art of Deception: Controlling the Human Element of Security | Kevin D. Mitnick |  3.76  | 2003 | \n| Resource Wars: The New Landscape of Global Conflict | Michael Klare |  3.73  |2001  | \n| The Audacity of Hope: Thoughts on Reclaiming the American Dream | Barack Obama |  3.70  | 2006 |   \n Economics \n | Name | Author | Goodreads Rating | Year Published | \n|------|--------|------------------|----------------| \n| The Simple Path to Wealth: Your road map to financial independence and a rich, free life |  J.L. Collins |  4.47  | 2016 | \n| Origin of Wealth | Eric D. Beinhocker |  4.23  | 2006 | \n| Every Shot Counts: Using the Revolutionary Strokes Gained Approach to Improve Your Golf Performance and Strategy | Mark Broadie |  4.23  | 2014 | \n| Economic Facts and Fallacies | Thomas Sowell |  4.18  | 2008 | \n| Debt - Updated and Expanded: The First 5,000 Years | David Graeber |  4.17  | 2011 | \n| Capital in the Twenty-First Century | Thomas Piketty |  4.01  | 2014 | \n| Freakonomics: A Rogue Economist Explores the Hidden Side of Everything |  Steven D. Levitt, Stephen J. Dubner |  3.98  | 2006 |\n| 23 Things They Don't Tell You About Capitalism | Ha-Joon Chang |  3.97  | 2010 | \n| The Signal and the Noise: Why So Many Predictions Fail--but Some Don't | Nate Silver |  3.96  | 2012 | \n| Currency Wars: The Making of the Next Global Crisis | James Rickards |  3.96  | 2011 | \n| The Ascent of Money: A Financial History of the World | Niall Ferguson |  3.88  |2008  | \n| The Bottom Billion: Why the Poorest Countries are Failing and What Can Be Done About It | Paul Collier |  3.85  | 2007 | \n| The Thank You Economy | Gary Vaynerchuk |  3.83  | 2011 | \n| Super Crunchers: Why Thinking-By-Numbers is the New Way To Be Smart | Ian Ayres |  3.70  | 2007 | \n| A Farewell to Alms : A Brief Economic History of the World | Gregory Clark |  3.67  | 2007 |   \n Gender \n | Name | Author | Goodreads Rating | Year Published | \n|------|--------|------------------|----------------| \n| Cemetary Boys | Aiden Thomas |  4.48  | 2020\n| Delusions of Gender: How Our Minds, Society, and Neurosexism Create Difference | Cordelia Fine |  4.15  | 2010\n| Paul Takes the Form of a Mortal Girl | Andrea Lawlor |  4.00  | 2017\n| Becoming A Man | P. Carl |  3.95  | 2020 \n Sexuality \n | Name | Author | Goodreads Rating | Year Published | \n|------|--------|------------------|----------------| \n| Sex at Dawn: The Prehistoric Origins of Modern | Christopher Ryan, Cacilda Jetha |  4.02  | 2010 | \n| The Ethical Slut: A Guide to Infinite Sexual Possibilities |  Dossie Easton, Catherine A. Liszt. |  3.83  | 2004 | \n| The Sex Myth | Brooke Magnanti |  3.50  | 2015 | \n Race \n | Name | Author | Goodreads Rating | Year Published | \n|------|--------|------------------|----------------| \n| So You Want to Talk About Race | Ijeoma Oluo |  4.53  | 2018 | \n| The Origin of Others | Toni Morrison |  4.22  | 2017 | \n Education \n | Name | Author | Goodreads Rating | Year Published | \n|------|--------|------------------|----------------|\n| Educated: A Memoir | Tara Westover|  4.47  | 2018 |\n| Mindstorms: Children, Computers, And Powerful Ideas | Seymour A. Papert |  4.32  | 1993 | \n| The Little Prince | Antoine de Saint-Exupéry |  4.31  | 1943 |\n| How Children Fail | John Holt |  4.22  | 1995 | \n| Deep Work: Rules for Focused Success in a Distracted World | Cal Newport |  4.20  | 2016 |\n| Make It Stick: The Science of Successful Learning | Peter C. Brown, Henry L. Roediger III, Mark A. McDaniel |  4.20  | 2014 |\n| Teaching as a Subversive Activity | Neil Postman, Charles Weingartner |  4.19  | 1971 | \n| Summerhill School: A New View of Childhood | A. S. Neill, Albert Lamb |  4.09  | 1995 | \n| The Art of Learning: An Inner Journey to Optimal Performance | Josh Waitzkin |  4.08  | 2016 |\n| Wounded by School: Recapturing the Joy in Learning and Standing Up to Old School Culture | Kirsten Olson, Sara Lawrence-Lightfoot, Parker J. Palmer |  3.92  | 2009 | \n| Curious Minds: How a Child Becomes a Scientist | John Brockman |  3.72  | 2005 |   \n Writing \n | Name | Author | Goodreads Rating | Year Published | \n|------|--------|------------------|----------------| \n| Hat Box: The Collected Lyrics | Stephen Sondheim |  4.77  |  | \n| On Writing: A Memoir of the Craft | Stephen King |  4.31  | 2002 |\n| Bird by Bird: Some Instructions on Writing and Life | Anne Lamott |  4.23  | 1995 | \n| Clear and Simple as the Truth | Francis-Noël Thomas, Mark Turner |  4.06  | 1996 | \n| The Sense of Style: The Thinking Person's Guide to Writing in the 21st Century | Steven Pinker |  4.03  | 2014 |   \n Theater and Film \n | Name | Author | Goodreads Rating | Year Published | \n|------|--------|------------------|----------------| \n| Different Every Night: Putting the play on stage and keeping it fresh | Mike Alfreds |  4.69  | 2008 | \n| Impro: Improvisation and the Theatre | Keith Johnstone |  4.27  |  | \n| The Actor and the Target | Declan Donnellan |  4.25  | 2006 | \n| In the Blink of an Eye: A Perspective on Film Editing | Walter Murch |  4.24  | 1995 | \n| Notes on Directing: 130 Lessons in Leadership from the Director's Chair | Frank Hauser, Russell Reich |  4.21  |  | \n| How to Stop Acting | Harold Guskin |  4.17  | 2003 | \n| A Practical Handbook for the Actor | Melissa Bruder, Lee Michael Cohn, Madeleine Olnek, Nathaniel Pollack, Robert Previtio, Scott Zigler, David Mamet |  3.87  | 1986 |   \n Shakespeare \n | Name | Author | Goodreads Rating | Year Published | \n|------|--------|------------------|----------------| \n| Thinking Shakespeare: A How-to Guide for Student Actors, Directors, and Anyone Else Who Wants to Feel More Comfortable With the Bard | Barry Edelstein. |  4.62  |  | \n| Hamlet and Revenge | Eleanor Prosser |  4.50  | 1971 | \n| Shakespeare's Metrical Art | George T. Wright |  4.39  | 1991 | \n| Hamlet in Purgatory | Stephen Greenblatt |  3.98  | 2002 |   \n Fiction \n Classics \n | Name | Author | Goodreads Rating | Year Published | \n|------|--------|------------------|----------------| \n| The Master and Margarita | Mikhail Bulgakov |  4.32  | 1966 | \n| Infinite Jest | David Foster Wallace |  4.31  | 1996 | \n| The Brothers Karamazov | Fyodor Dostoevsky, Richard Pevear, Larissa Volokhonsky |  4.30  | 1880 | \n| Pride And Prejudice | Jane Austen |  4.24  | 1813 | \n| A Prayer for Owen Meany | John Irving |  4.22  | 1989 | \n| Letters From The Earth | Mark Twain |  4.22  | 1962 | \n| One Flew Over the Cuckoo's Nest | Ken Kesey |  4.18  | 1963 | \n| Crime and Punishment | Fyodor Dostoyevsky |  4.18  | 1866 | \n| 1984 | George Orwell |  4.17  | 1949 |   \n| The Giver | Lois Lowry |  4.13  | 1993 | \n| Steppenwolf: A Novel | Hermann Hesse, Basil Creighton |  4.11  | 1927 | \n| The Glass Bead Game | Hermann Hesse, Richard and Clara Winston |  4.11  | 1943 | \n| War and Peace | Leo Tolstoy |  4.11  | 1867 | \n| Jane Eyre | Charlotte Brontë |  4.10  | 1847 | \n| Germinal | Emile Zola |  4.10  | 1885 | \n| If on a Winter's Night a Traveler | Italo Calvino and William Weaver |  4.08  | 1979 | \n| Slaughterhouse-Five | Kurt Vonnegut Jr. |  4.08  | 1969 |\n| The Wall: (Intimacy) and Other Stories | Jean-Paul Sartre, Lloyd Alexander |  4.07  | 1948 | \n| Sense and Sensibility | Jane Austen |  4.06  | 1811 | \n| The Picture of Dorian Gray | Oscar Wilde |  4.06  | 1890 | \n| Watership Down: A Novel | Richard Adams |  4.05   |1972| \n| One Hundred Years of Solitude | Gabriel Garcia Marquez, Gregory Rabassa |  4.04  | 1967 | \n| The Death of Ivan Ilych | Leo Tolstoy |  4.04  | 1886  |\n| Siddhartha | Hermann Hesse |  4.03  | 1922 | \n| Anna Karenina | Leo Tolstoy |  4.02  | 1878 | \n| Bleak House | Charles Dickens |  4.00  | 1853 | \n| J'accuse | Emile Zola |  3.99  | 1898 | \n| Catch 22 | Joseph Heller |  3.98  | 1961 | \n| The House of Mirth | Edith Wharton |  3.94  | 1905 | \n| Confessions of a Mask | Yukio Mishima |  3.91  | 1949 | \n| De L'esprit Des Lois | Montesquieu |  3.91  | 1748 | \n| Love in the Time of Cholera | Gabriel García Marquéz |  3.9  | 1985 |\n| Don Quixote | Miguel de Cervantes |  3.85  | 1605 | \n| Mansfield Park | Jane Austen |  3.84  | 1814 | \n| Wuthering Heights | Emily Bronte |  3.82  | 1847 | \n| Père Goriot | Honoré de Balzac |  3.82  | 1834 | \n| Adventures of Huckleberry Finn | Mark Twain |  3.80  | 1884 | \n| The Catcher in the Rye | J.D. Salinger |  3.79  | 1951 | \n| Eugénie Grandet | Honoré de Balzac |  3.79  | 1833 |\n| Zen and the Art of Motorcycle Maintenance: An Inquiry Into Values  | Robert M. Pirsig |  3.76  | 1974 |\n| Candide | Voltaire |  3.76  | 1759 | \n| Atlas Shrugged | Ayn Rand |  3.68  | 1957 |\n| Madame Bovary | Gustave Flaubert |  3.65  | 1856 | \n| Confessions | Jean-Jacques Rousseau |  3.61  | 1782 | \n| Lettres persanes | Montesquieu |  3.54  | 1721 | \n| The Director | David Ignatius |  3.43  | 2015 |    \n Fantasy \n | Name | Author | Goodreads Rating | Year Published | \n|------|--------|------------------|----------------| \n| The Kingkiller Chronicle | Patrick Rothfuss |  4.55  | 2007 | \n| The Lord of the Rings | J.R.R. Tolkien |  4.47  | 1954 | \n| A Song of Ice and Fire | George R.R. Martin |  4.45  | 2005 | \n| Harry Potter | J.K. Rowling |  4.44  | 1997 | \n| Mistborn | Brandon Sanderson |  4.43  | 2006 |\n| Harry Potter and the Methods of Rationality | Eliezer Yudkowsky |  4.38  | 2015 | \n| Discworld | Terry Pratchett |  4.32  | 2001 | \n| The Dark Tower | Stephen King |  4.27  | 1982 | \n| His Dark Materials | Philip Pullman |  4.25  | 1995 | \n| The Last Wish  | Andrzej Sapkowski |  4.2  | 1993 |\n| The Chronicles of Narnia | Clive Staples Lewis |  4.24  | 2002 | \n| The Wheel of Time | Robert Jordan |  4.19  | 1990 |\n| The City & The City | China Mieville |  3.91  | 2009 | \n| The Broken Empire | Mark Lawrence |  3.87  | 2011 |   \n Historical Fiction \n | Name | Author | Goodreads Rating | Year Published | \n|------|--------|------------------|----------------| \n| Lonesome Dove | Larry McMurtry |  4.47  | 1985 | \n| The Help | Kathryn Stockett |  4.45  | 2009 | \n| A Thousand Splendid Suns | Khaled Hosseini |  4.4  | 2007 |\n| The Book Thief | Markus Zusak |  4.36  | 2005 | \n| All the Light We Cannot See | Anthony Doerr |  4.31  | 2014 | \n| The Kite Runner | Khaled Hosseini |  4.3  | 2003 | \n| The Pillars of the Earth | Ken Follett |  4.29  | 1989 | \n| Kane and Abel | Jeffrey Archer |  4.27  | 1979 | \n| Memoirs of a Geisha | Arthur Golden |  4.12  | 1997 |\n| One Day in the Life of Ivan Denisovich | Aleksandr Solzhenitsyn |  4  | 1962 |\n| Emma | Jane Austen |  3.99  | 1815 | \n| Sophie's World: A Novel About the History of Philosophy | Jostein Gaarder and Paulette Moller |  3.88  | 1991 |   \n Humor \n | Name | Author | Goodreads Rating | Year Published | \n|------|--------|------------------|----------------| \n| I Hope They Serve Beer In Hell | Tucker Max |  3.51  | 2006 |   \n Mystery \n | Name | Author | Goodreads Rating | Year Published | \n|------|--------|------------------|----------------| \n| Sherlock Holmes | Arthur Conan Doyle |  4.30  | 1998 | \n| And Then There Were None | Agatha Christie |  4.26  | 1939 |\n| The Murder of Roger Ackroyd | Agatha Christie |  4.20  | 2006 | \n| The Eight | Katherine Neville |  4.17  |  |\n| The Girl with the Dragon Tattoo | Stieg Larsson |  4.14  | 2005 |\n| Memory Man | David Baldacci |  4.05  | 2015 |\n| The Turn of the Key | Ruth Ware |  3.94  | 2019 |\n| The Girl on the Train | Paula Hawkins |  3.88  | 2015 | \n| The Da Vinci Code | Dan Brown |  3.79  | 2006 |   \n Science Fiction \n | Name | Author | Goodreads Rating | Year Published | \n|------|--------|------------------|----------------| \n| The Martian | Andy Weir |  4.40  | 2012 |\n| The Hitchhiker's Guide to the Galaxy | Douglas Adams |  4.38  | 1979|\n| Champion | Marie Lu |  4.32  | 2013 |\n| Ready Player One | Ernest Cline |  4.31  | 2011 |\n| 11/22/63 | Stephen King |  4.31  | 2011 | \n| Ender's Game - Saga | Orson Scott Card |  4.30  | 1994 | \n| The Player of Games | Iain M. Banks |  4.26  | 1997 | \n| The Fifth Sacred Thing | Starhawk |  4.25  | 1994 |\n| Prodigy | Marie Lu |  4.24  | 2013\n| Hyperion | Dan Simmons |  4.21  | 1989 | \n| Cosmicomics | Italo Calvino |  4.2  | 1965 |\n| The Diamond Age | Neal Stephenson |  4.19  | 2000 | \n| Dune Chronicles | Frank Herbert |  4.19  | 2006 |\n| The Forever War | Joe Haldemann |  4.15  | 1974\n| Foundation | Isaac Asimov |  4.13  | 2004 | \n| Snow Crash | Neal Stephenson |  4.02  | 2000 | \n| Brave New World | Aldous Huxley |  3.99  | 1932 |\n| Fahrenheit 451 | Ray Bradbury |  3.97  | 2006 | \n| Little Brother | Cory Doctorow |  3.94  | 2008 |\n| The Time Machine | H.G Wells |  3.89  | 1895 | \n Horror \n | Name | Author | Goodreads Rating | Year Published | \n|------|--------|------------------|----------------| \n| It | Stephen King |  4.24  | 1986 |\n| House of Leaves | Mark Z. Danielewski |  4.13  | 2000 | \n| If it bleeds | Stephen King |  4.10  | 2020 |\n| Rosemary's Baby | Ira Levin |  4.00  | 1967 |\n| Dracula | Bram Stoker |  3.98  | 1986 | \n| Frankenstein | Mary Shelley, Maurice Hindle |  3.75  | 2003 |   \n Miscellaneous \n | Name | Author | Goodreads Rating | Year Published | \n|------|--------|------------------|----------------| \n| The Queen's Gambit | Walter Tevis |  4.11  |  | \n| The Bone People | Keri Hulme |  4.06  | 1986 | \n| Cat's Eye | Margaret Atwood |  3.92  | 1998 | \n| Xeelee Sequence | Stephen Baxter |  3.69  | 1992 | \n| Ghost Fleet | P.W. Singer and August Cole |  3.62  | 2015 | \n| Future Crimes | Marc Goodman |  3.42  | 1990 |   \n Health \n | Name | Author | Goodreads Rating | Year Published | \n|------|--------|------------------|----------------| \n| How Not to Die: Discover the Foods Scientifically Proven to Prevent and Reverse Disease | Michael Greger, M.D. |  4.54  | 2015 |\n| Overcoming Gravity: A Systematic Approach to Gymnastics and Bodyweight Strength (2nd edition) | Steven Low |  4.3  | 2016 |\n| Convict Conditioning | Paul Wade |  4.13  | 2012 | \n| Eat and Run: My Unlikely Journey to Ultramarathon Greatness |  Scott Jurek, Steve Friedman |  3.95  | 2012 | \n| 8 Weeks to Optimum Health | Andrew Weil |  3.86  |  | \n| Grain Brain: The Surprising Truth about Wheat, Carbs, and Sugar--Your Brain's Silent Killers |  David Perlmutter |  3.89  | 2013 | \n Design \n | Name | Author | Goodreads Rating | Year Published | \n|------|--------|------------------|----------------| \n| Pattern Language: Towns, Buildings, Construction | Christopher Alexander, Sara Ishikawa, Murray Silverstein, Max Jacobson, Ingrid Fiksdahl-King and Shlomo Angel |  4.38  | 1977 | \n| Data visualization handbook | Juuso Koponen, Jonatan Hildén |  4.33  | 2019 | \n| How Buildings Learn: What Happens After They're Built | Stewart Brand |  4.32  | 1995 |\n| Don't Make Me Think, Revisited: A Common Sense Approach to Web Usability | Steve Krug |  4.24  | 2014 | \n| The Design of Everyday Things | Donald Norman |  4.18  | 2002 | \n| Mismatch: How Inclusion Shapes Design |  Kat Holmes, John Maeda |  4.14  | 2018 | \n| The Art of Looking Sideways | Alan Fletcher |  4.10  | 2001 |  \n Travel \n | Name | Author | Goodreads Rating | Year Published | \n|------|--------|------------------|----------------| \n| Atlas Obscura: An Explorer's Guide to the World's Hidden Wonders | Joshua Foer |  4.24  | 2016 |   \n Language \n | Name | Author | Goodreads Rating | Year Published | \n|------|--------|------------------|----------------| \n| Le Ton beau de Marot: In Praise of the Music of Language | Douglas R. Hofstadter |  4.22  | 1997 | \n| The Language Instinct: How the Mind Creates Language | Steven Pinker |  4.02  | 2000 | \n Nature \n | Name | Author | Goodreads Rating | Year Published | \n|------|--------|------------------|----------------| \n| The Wild Places |  Robert Macfarlane |  4.27  | 2007 | \n| The Sixth Extinction: An Unnatural History | Elizabeth Kolbert |  4.13  | 2014 |\n| Pilgrim at Tinker Creek | Annie Dillard |  4.11  | 1974 |\n| A Walk in the Woods: Rediscovering America on the Appalachian Trai | Bill Bryson |  4.06  | 2006 |\n| The Uninhabitable Earth: Life After Warming | David Wallace-Wells |  4.06  | 2019 | \n Art \n | Name | Author | Goodreads Rating | Year Published | \n|------|--------|------------------|----------------| \n| The New Drawing on the Right Side of the Brain | Betty Edwards |  3.86  | 1989 |\n| Steal Like an Artist: 10 Things Nobody Told You About Being Creative | Austin Kleon |  3.92  | 2012 |\n| Show Your Work!: 10 Ways to Share Your Creativity and Get Discovered | Austin Kleon |  4.10  | 2014 | \n Credits \n \n @hackerkid  for starting the list. \n @geritol  for building the framework for maintaining the list. \n All the  contributors  for keeping the list updated by adding new books. \n \n License \n \n To the extent possible under law,  Vishnu Ks  has waived all copyright and related or neighboring rights to this work.", "Hero: Get Help    From The Awesome People Of Github \n Have a problem you can't find solution on Stack Overflow. Then ask it here. The Heros of Github would come and fix your problem.  \n Choose the programming language to see the latest unanswered Questions \n \n \n \n \n \n \n Click  here  to ask a new Question \n \n The moderators would tag your question accordingly.  \n Duplicate questions would be closed without any warning. \n \n Want to be a super hero? Click  here  for getting alert when a devloper is in trouble.", 'Awesome-Data-Structures \n C++ implementation of basic data structures and algorithms', 'is-awesome \n Checks whether the string is awesome', "Sockets \n Sockets are used for communicating with other programs with the help of Unix file descriptors.  \n There are two types of sockets\n* Stream sockets\n* Datagram sockets \n Stream Sockets \n Stream sockets are mainly used in two way communication. If you send A, B, C through stream socket it will arrive in that particular order. Stream sockets are error free meaning data you send would be always recieved at the other end without any error. This is achived with the help of  TCP  (Transmission Control Protocoal). TCP makes sure that the data which has arrived is sequential and error free. This feature make stream sockets reliable. Stream socket is popularly denoted as  SOCK_STREAM . \n Datagram Sockets \n Datagram sockets unlike Stream sockets is not reliable. The data arrives in no particular order. The advantage of Datagram sockets over stream sockets is the absence of an open connection at the time of transmission. The only thing you need to do is create a packet and send it. Datagram socket don't care about whether the data has been safely recived at the other end. It just keep sending the data. Datagram use  UDP  just like Steam sockets use TCP. Datagram socket can represnted as  SOCK_DGRAM . \n Packet \n A packet is a formatted unit of data carried by a packet-switched network. In simple words you send information over network in the form of small packets. A packet contains two types of data \n \n Control data \n User data \n \n The control information provides data the network needs to deliver the user data, for example: source and destination network addresses, error detection codes, and sequencing information. Typically, control information is found in packet  headers  and trailers, with payload data in between. \n Why use UDP when it is not reliable? \n Sometimes you need to trade reliability for speed. If you are going something like Whatsapp then you should probably use TCP as you don't want to miss any messages. But what would you do while implementing the Uber car tracking system. If you are sending 10 positional updates of the car it doesn't matter if you miss some packets. UDP is good choice here. ", 'github-desktop-notifications \n Get Github notifications in desktop', 'Library \n Collection of awesome functions and powerups', 'Foss DEMO for Trihacker \n Rules \n There are some repos listed in this file. Your task is to contribute to these repos. Contribution includes Sending Pull requests with fixes or new features, finding new bugs and reporting it as issues. \n Points for Contribution \n \n 20 points for merged pull request \n 3 points for valid pull request \n 5 Points for valid bug report in issues \n Negative 5 points (each) for invalid comments, pull requests and issues . \n \n ##Repos \n \n https://github.com/sindresorhus/awesome \n https://github.com/feross/mad-science-modules \n https://github.com/hackerkid/Mind-Exapanding-Books \n https://github.com/hackerkid/Wikifeedia \n https://github.com/moose-team/friends \n \n Evaluation \n \n Clone this repo. Make a file called contributions.md. List all the contributions you made with links. Send the pull request to this repo.  \n Deadline 11.59PM Aug 20 2015 \n', 'UVA-Online-Judge \n My solutions for  UVA Online Judge', 'Peer Chat \n \n A Simple multi client chat with encryption implemented in Java Swing \n \n How to run the Chat \n Download PeerChatServer.java and PeerChatClient.java. Compile and run with the following steps\n*  javac PeerChatServer.java \n*  javac PeerChatClient.java \n*  java PeerChatServer in terminal  (Started Server)\n*  java PeerChatClient in terminal  (Started Client)\n* Enter  java PeerChatClient  again so that there are now two clients connected to\nserver \n Precuations \n \n Make sure that PeerChatServer is already started before running the Clients \n The software is tested only in Ubuntu/Linux. \n If port 8888 is used by some other application please change it. \n \n How it works? \n The PeerChatServer acts only as the server. Start atleast two peerChatClient files to\nstart communication with each other along with the Server file running. \n How to connect to server? \n When this screen pops up enter localhost.\nOptional: If you are using two diffrent computers enter the ip address of the server. \n How to login? \n The following login credentials can be used to login to the systemUsername \n \n snape iiita \n potter malfoy \n jonsnow iiita \n hulk iiita \n ironman tonystark \n batman darkknight \n \n Once logged in the clients can send messages to each other. \n Awesome Features \n \n Secure communication with JCE(DES/ECB/PKCS5) \n Group chat \n Validation of sign in details in Server side \n Simple to use GUI \n Cross Platform (Windows, MAC, Linux) \n', 'Awesome Programming Problems \n Chinese Postman Problem \n The Chinese postman problem is a mathematical problem of graph theory. It is also known as route inspection problem. Suppose there is a mailman who needs to deliver mail to a certain neighbourhood. The mailman is unwilling to walk far, so he wants to find the shortest route through the neighbourhood, that meets the following criteria: \n \n It is a closed circuit (it ends at the same point it starts). \n He needs to go through every street at least once. \n \n Solution \n \n \n Step 1: Find all the nodes of odd order. \n \n \n Step 2: For each pair of odd nodes find the connecting path of minimum weight. \n \n \n Step 3: Pair up all the odd nodes so that the sum of the weights of the connecting paths from Step 2 is minimised. \n \n \n Step 4: In the original graph, duplicate the minimum weight paths found in Step 3. \n \n \n Step 5:  Find a trail containing every arc for the new (Eulerian) Graph. \n \n \n Code \n The link to the code is  here', "Zulip overview \n Zulip is a powerful, open source group chat application that combines the\nimmediacy of real-time chat with the productivity benefits of threaded\nconversations. Zulip is used by open source projects, Fortune 500 companies,\nlarge standards bodies, and others who need a real-time chat system that\nallows users to easily process hundreds or thousands of messages a day. With\nover 700 contributors merging over 500 commits a month, Zulip is also the\nlargest and fastest growing open source group chat project. \n \n \n \n \n \n \n \n \n \n \n Getting started \n Click on the appropriate link below. If nothing seems to apply,\njoin us on the\n Zulip community server \nand tell us what's up! \n You might be interested in: \n \n \n Contributing code . Check out our\n   guide for new contributors \n  to get started.  Zulip prides itself on maintaining a clean and\n  well-tested codebase, and a stock of hundreds of\n   beginner-friendly issues . \n \n \n Contributing non-code .\n   Report an issue ,\n   translate  Zulip\n  into your language,\n   write \n  for the Zulip blog, or\n   give us feedback . We\n  would love to hear from you, even if you're just trying the product out. \n \n \n Supporting Zulip . Advocate for your organization to use Zulip, become a  sponsor , write a\n  review in the mobile app stores, or\n   upvote Zulip  on\n  product comparison sites. \n \n \n Checking Zulip out . The best way to see Zulip in action is to drop by\n  the\n   Zulip community server . We\n  also recommend reading Zulip for\n   open source , Zulip for\n   companies , or Zulip for\n   communities . \n \n \n Running a Zulip server . Use a preconfigured  DigitalOcean droplet ,\n   install Zulip \n  directly, or use Zulip's\n  experimental  Docker image .\n  Commercial support is available; see  https://zulip.com/plans  for details. \n \n \n Using Zulip without setting up a server .  https://zulip.com \n  offers free and commercial hosting, including providing our paid\n  plan for free to fellow open source projects. \n \n \n Participating in  outreach\n  programs \n  like Google Summer of Code. \n \n \n You may also be interested in reading our  blog  or\nfollowing us on  Twitter .\nZulip is distributed under the\n Apache 2.0  license.", 'Codemonk \n Solutions to HackerEarth codemonk series', 'codeforces \n Solutions to Codeforces round', 'snowden \n Yet another proxy server in C++ \n Installing \n \n Extract the compressed file \n run  make  in terminal for compiling and building the binary \n run  ./proxy Port  to start the proxy server in the desired PORT \n \n IIITA Proxy \n \n If you are testing the proxy server in IIIT-A network then change the value of  CONNECT_TO_IRONPORT  defined in line 15 proxy.cpp to  true .  \n If you are using mobile data then there is no need of chaning the value of  CONNECT_TO_IRONPORT . Leave it as  false .  \n The python tests would only work under mobile data.   \n \n Design \n The proxy server would act as both a client and server. \n \n \n Each client would connect to the proxy server which in turn fetches the desired web page back to the client. \n To support multiple clients I used thread programming (pthread library).  \n The request made to the proxy server by the client can be validated with the help of  ParsedRequest_parse  function in  proxy_parse.h .  \n HTTPS is not supported becuase the data send between the client and server is heavily encrypted. No sense can be made of data and it would be useless to cache anything in the proxy server. \n \n Testing \n Go to the  Edit  menu in Firefox.\n* Select  Preferences . Select  Advanced  and then select  Network.\n* Under Connection , select Settings .\n* Select Manual Proxy Configuration . If you are using localhost, remove the\ndefault No Proxy for: localhost 127.0.0.1 . Enter the hostname (eg localhost) and port where\nyour proxy program is running.\n* Save your changes by selecting OK in the connection tab and then select Close`\nin the preferences tab.', 'snowden (HTTP Proxy Serever) Docs \n This repo contains the docs for the HTTP proxy server which I (Vishnu Ks, IIT2013075) made for the networking course. The source code would be made public once the evaluation is over at http://github.com/hackerkid/snowden \n Installing \n \n Extract the compressed file \n run  make  in terminal for compiling and building the binary \n run  ./proxy Port  to start the proxy server in the desired PORT \n \n IIITA Proxy \n \n If you are testing the proxy server in IIIT-A network then change the value of  CONNECT_TO_IRONPORT  defined in line 15 proxy.cpp to  true .  \n If you are using mobile data then there is no need of chaning the value of  CONNECT_TO_IRONPORT . Leave it as  false .  \n The python tests would only work under mobile data.   \n \n Design \n The proxy server would act as both a client and server. \n \n \n Each client would connect to the proxy server which in turn fetches the desired web page back to the client. \n To support multiple clients I used thread programming (pthread library).  \n The request made to the proxy server by the client can be validated with the help of  ParsedRequest_parse  function in  proxy_parse.h .  \n HTTPS is not supported becuase the data send between the client and server is heavily encrypted. No sense can be made of data and it would be useless to cache anything in the proxy server. \n \n Testing \n Go to the  Edit  menu in Firefox.\n* Select  Preferences . Select  Advanced  and then select  Network.\n* Under Connection , select Settings .\n* Select Manual Proxy Configuration . If you are using localhost, remove the\ndefault No Proxy for: localhost 127.0.0.1 . Enter the hostname (eg localhost) and port where\nyour proxy program is running.\n* Save your changes by selecting OK in the connection tab and then select Close`\nin the preferences tab.', 'web-tox \n Tox for node and the browser ', 'ToolBox \n Collection of useful tools and tricks for competitive programming \n Modular Exponentiation \n ```c++\nint modPow(long long b, long long e, int m) {\n    long long res = 1;\n    for (; e; e >>= 1, b = b b%m) if (e & 1) res = res b%m;\n    return res;\n}\n```` \n Modular Multiplicative Inverse \n ```c++\nvoid extEuclid(int a, int b, int &x, int &y, int &gcd) {\n    x = 0; y = 1; gcd = b;\n    int m, n, q, r;\n      for (int u=1, v=0; a != 0; gcd=a, a=r) {\n          q = gcd / a; r = gcd % a;\n          m = x-u q; n = y-v q;\n          x=u; y=v; u=m; v=n;\n      }\n} \n // The result could be negative, if it\'s required to be positive, then add "m"\n \n int modInv(int n, int m) {\n    int x, y, gcd;\n    extEuclid(n, m, x, y, gcd);\n    if (gcd == 1) return x % m;\n    return 0;\n} \n ``` \n Example \n ```\nAnswer = SumDiv( 36^30 ) % MOD                      // Where MOD = 1,000,000,007 \n A = SumDiv( (2^2 * 3^2) ^ 30 ) % MOD                // Prime factorization of 36\nA = SumDiv( 2^60 * 3^60 ) % MOD                     // Exponent rule\nA = ( (2^61 - 1) / 1 ) * ( (3^61 - 1) / 2 ) % MOD   // Sum of divisors \n // At this point, using modular arithmetic brings us to the answer \n A = ( ( (2^61 - 1) / 1 ) % MOD ) *  ( ( (3^61 - 1) / 2 ) % MOD ) % MOD \n A =   ( (2^61 - 1) % MOD ) * ( 1^-1 ) % MOD\n    * ( (3^61 - 1) % MOD ) * ( 2^-1 ) % MOD\n    % MOD \n // Where x^-1 is the MMI of x, modulo MOD \n A =   ( ( modPow(2, 61, MOD) - 1 ) % MOD * modInv(1, MOD) ) % MOD\n    * ( ( modPow(3, 61, MOD) - 1 ) % MOD * modInv(2, MOD) ) % MOD\n    % MOD\n```', 'Popular-Unix-text-editors-and-how-to-exit-them', "\n \n \n Alpha  quality you probably only want to use this if you like to send pull requests fixing things :) \n A detailed write-up on how Pluto works will be added to this repo in the near future. For now see the source code. \n Prerequisites \n You'll need the newest  io.js  and npm ( >= 1.8.1 ,  >= 2.8.3 ) \n You'll also need to install  Tox  library.  \n Build \n Clone the sources locally: \n ```sh\n$ git clone https://github.com/hackerkid/pluto \n ``` \n Install project dependencies: \n sh\n$ cd pluto\n$ npm install \n Usage \n sh\n$ npm start", 'learn-prolog-by-solving', 'advent-of-code \n Solutions to advent of code puzzle series', '2do \n An awesome to do app built with backbone', 'Book Library \n \n The book library example present in Backbone fundamentals with support to Express 4.x \n \n Setting Up \n bash\ngit clone https://github.com/hackerkid/book-library\ncd book-library\nnpm install\nnode server.js \n Browse to  localhost:4711  for accessing the bookshelf. Make sure to have MongoDB installed before trying out this. ', 'Hack In The North Website \n Contributing \n Feel free to send a pull request that you think makes the website better. We are always open to ideas and innovations.  \n How to run the website \n bash\ngit clone https://github.com/HackInTheNorth/HackInTheNorth.github.io\ncd HackInTheNorth.github.io\npython -m SimpleHTTPServer \n Now browse to http://localhost:8000', 'compiler-design \n Compiler Design course solutions', 'hacker-shellscripts \n \n Collection of useful shell scripts \n \n Development \n \n Alternative script to  jhbuild sysdeps --install  command \n', 'codejam \n Solutions to Google Codejam', "Solutions to Ruby Koans \n \n My solutions to Ruby Koans. Please don't send any pull requests. I am trying to figure this out myself. \n", 'voltage-ninja \n A study on the effect of varying voltage in Linux', 'Awesome-P2P \n Curated list of awesome Peer to Peer projects \n Protocols \n Named Data Networking \n Inspired by Content Centric Networking, Named Data Networking (NDN) is a next-generation Future Internet architecture, that focuses on content, rather than location. In other words, instead of looking to a certain location for a file, just look for the file, which may exist in many locations. I think this concept could even be applied to a lot of publishing and social media platforms right now to eliminate a lot of overhead. \n SecuShare \n SecuShare is a combination of the PSYC protocol \nwhich is "the fastest \nyet extensible text-based protocol...providing a messaging infrastructure for human conversation and social exchange of possibly binary data," providing a huge improvement over XML-based \nsolutions like XMPP/Jabber and GNUnet \n, "a framework for secure peer-to-peer networking that does not use any centralized or otherwise trusted services," offering stronger anonymity and other benefits \nas compared to similar projects.\n \n Prophet  \n While in need of a new maintainer, Prophet is "a grounded, semirelational, peer to peer replicated, disconnected, versioned, property database with self-healing conflict resolution." It\'s "a new kind of database designed for the post Web-2.0 world. It\'s made to let you collaborate with your friends and coworkers without needing any kind of special server or internet provider." Let me know if anything more recent has applied these concepts, because they offer a lot.\n \n Dash Evolution  \n Dash \nalready offers a lot more than most currently available cryptocurrencies, primarily truly strong privacy through built-in distributed mixing (whereas for example with bitcoin you have to trust third-party centralized mixers), and Evolution is just the next step offering even more including a decentralized shared file system, decentralized wallets, and social features.\n \n B𝜀trFS  \n B-Trees are pretty popular in the current generation, but what about fractal trees \n? That\'s what BetrFS uses, and what it offers is pretty cool. I\'d also be interested in a dynamic non-hierarchical file system \n.\n \n TokuDB  \n Also using a fractal tree index is TokuDB, not much explanation needed.\n \n Camp \n/Darcs  \n The concepts \nbehind these two are how revision control should be approached, though perhaps they could also benefit from the concepts offered by Simple Defects \n, "a peer-to-peer bug tracker that’s built for sharing and use both online and offline," which allows you to "sync your bugs back and forth between other instances of SD, and even between SD and other bug trackers that SD supports," based on the aforementioned Prophet P2P database. A complete dream package I think would also offer integrated distributed issue tracking and wiki like Veracity\n \n Daala  \n This "next-next generation" video codec leaps ahead of the single next-gen codecs VP9 and HEVC (H.265) by using lapped transform instead of discrete cosine transform \nwhich also incidentally avoids most of the patents in the video codec arena.\n \n Happstack  \n This Haskell \n-based web framework is powerful, elegant, and supremely fast (soon to be faster once the new Hyperdrive server \nis complete. The maintainer also has the right idea in working on Clckwrs \na flat-file CMS aiming to be a just-as-easy alternative to Wordpress.\n \n Tox  \n This is the most usable solution to offer decentralized end-to-end encrypted text, audio, and video messaging and conferencing.\n \n Mercury  \n This programming language is functional like Haskell, but also offers the additional ability of logic programming.\n', 'rails-micro-blog \n A micro blog made with Ruby On Rails. The website is under active development. For preview checkout  Glowing Rails', 'Awesome Gem \n My first gem \n Installation \n bash\ngem install awesome-gem \n Build Locally \n bash\ngit clone https://github.com/hackerkid/awesome-gem\ncd awesome-gem\ngem build awesome.gemspec\ngem install awesome-0.0.1.gem', 'koans \n Koans Ruby Solutions', 'parallel-matrix-multiplication \n Input format \n ```\nn m p\nk1 k2 \n x1 y1 w1\n....\n....\nxk1 yk1 wk1 \n x1 y1 w1\n....\n....\nxk2 yk2 wk2 \n ``` \n where n m p denotes the dimensions n m and m p\nk1 and k2 denotes the number of non zero elements in matrix a and b \n Generating Random input \n If you want to create two random sparse matrices of dimensions n,m and m,p follow these steps.  \n $ git clone https://github.com/hackerkid/parallel-matrix-multiplication\n$ cd parallel-matrix-multiplication\n$ g++   create_matrix_mul_input.cpp\n$ ./a.out\nn\nm\np \n where n, m, p are the dimensions of the matrices. The program creates an input file called  input.in  of the for', "WebTox \n \n Bringing Tox to the Browser.  \n \n WebTox wants to bring Tox to the browser. The project wants to make building web applications using Tox as simple as requiring Tox.js . \n RoadMap \n \n Since Browsers don't support UDP due to security reasons WebTox would be implemented with the help of WebRTC. Initially there would be only WebTox clients. They communicate with each other with the help of WebRTC. Since the normal Tox clients don't have WebRTC support there is no way of WebTox clients to communicate with the normal Tox clients.  \n This can be fixed by making Hybrid clients. Hybrid clients which runs on server(like Node.js) supports both WebRTC and UDP. They can communicate with both WebTox clients and normal Tox clients. They can pass the messages from Tox clients to WebTox clients and vice versa.  \n The hybrid clients can be eliminated once the Tox applications add support WebTox. ", 'Classroom for GitHub \n \n Classroom for GitHub is a  Ruby on Rails  application designed to automate repository creation and access control, making it easy for teachers to distribute starter code and collect assignments on GitHub \n \n How it works \n Assignments are the core of Classroom for GitHub. Teachers can easily create an assignment and distribute it to students using a private invitation URL. Optional starter code can be provided for individual or group work. It\'s even possible to delegate assignment creation and management to co-teachers and teaching assistants by adding them as organization administrators. \n Hacking on Classroom for GitHub \n Get started \n New to Ruby? No worries! You can follow these instructions to install a local server. \n Installing a Local Server \n First things first, you\'ll need to install Ruby 2.3.0. We recommend using the excellent  rbenv ,\nand  ruby-build \n bash\nrbenv install 2.3.0\nrbenv global 2.3.0 \n Next, you\'ll need to make sure that you have Nodejs, PostgreSQL, Redis, Memcached, and Elasticsearch installed. This can be done easily :\n* For OSX using  Homebrew  : You don\'t have to do anything! When you run  script/setup  later on this will be taken care of for you.\n* For Linux :  apt-get install nodejs postgresql redis-server memcached . For Elasticsearch, follow the instructions on  their website . \n You will want to set PostgreSQL to autostart at login via launchctl, if not already. See  brew info postgresql . Redis and memcached may be setup similarly via launchctl or setup project wide by using foreman, described below. \n Now, let\'s install the gems from the  Gemfile  ("Gems" are synonymous with libraries in other\nlanguages). \n bash\ngem install bundler && rbenv rehash \n Setup Classroom for GitHub \n If you are using Linux, configure PostgreSQL : \n \n Edit  /etc/postgresql/9.3/main/postgresql.conf  and uncomment  #unix_socket_permissions = 0777 \n Create a user and give him the rights to create a database :  su postgres -s /bin/bash -c "psql -c \'CREATE USER classroom_user; ALTER USER classroom_user CREATEDB\'"  (Change  classroom_user  to the username that will run the classroom server) \n \n Once bundler is installed (and PostgreSQL correctly configured for Linux users) go ahead and run the  setup  script : \n script/setup \n Production environment variables \n ENV Variable | Description |\n:-------------------|:-----------------|\n AIRBRAKE_API_KEY  | the API key for airbrake.io, if set Airbrake will be enabled\n CANONICAL_HOST  | the preferred hostname for the application, if set requests served on other hostnames will be redirected\n GOOGLE_ANALYTICS_TRACKING_ID  | identifier for Google Analytics in the format  UA-.* \n PINGLISH_ENABLED  | Enable the  /_ping  endpoint with relevant health checks\n MOTD  | Show the message of the day banner at the top of the site \n Development environment variables \n These values must be present in your  .env  file (created by  script/setup ). \n ENV Variable | Description |\n:-------------------|:-----------------|\n GITHUB_CLIENT_ID | the GitHub Application Client ID.\n GITHUB_CLIENT_SECRET | the GitHub Application Client Secret.\n NON_STAFF_GITHUB_ADMIN_IDS  | GitHub  user_ids  of users to be granted staff level access. \n Testing environment variables \n Classroom for GitHub uses  VCR  for recording and playing back API fixtures during test runs. These cassettes (fixtures) are part of the Git project in the  spec/support/cassettes  folder. If you\'re not recording new cassettes you can run the specs with existing cassettes with: \n bash\nscript/test \n Classroom for GitHub uses environmental variables for storing credentials used in testing, these values are located in your  .env  file (created by  script/setup ).\nIf you are recording new cassettes, you need to make sure all of these values are present. \n ENV Variable | Description |\n:-------------------|:-----------------|\n TEST_CLASSROOM_OWNER_ID  | The GitHub  user_id  of an organization admin.\n TEST_CLASSROOM_OWNER_GITHUB_TOKEN  | The  Personal Access Token  for the classroom owner\n TEST_CLASSROOM_STUDENT_ID  | Test OAuth application client ID.\n TEST_CLASSROOM_STUDENT_GITHUB_TOKEN  | The  Personal Access Token  for the student\n TEST_CLASSROOM_OWNER_ORGANIZATION_ID  | GitHub ID (preferably one created specifically for testing against).\n TEST_CLASSROOM_OWNER_ORGANIZATION_LOGIN  | GitHub login (preferably one created specifically for testing against). \n Running the application \n Foreman is setup to manage redis, memcached, sidekiq, and elasticsearch in development mode. Postgresql must be running prior executing foreman. \n After that, you may start the rails server in a separate terminal with: \n bash\nscript/server \n And that\'s it! You should have a working instance of Classroom for GitHub located  here \n Deployment \n We strongly encourage you to use  https://classroom.github.com , but if you would like your own version Classroom for GitHub can be easily deployed to Heroku. \n \n Contributing \n We\'d love to have you participate. Please check out  contributing guidelines . \n Contributors \n Classroom is developed by these  contributors . \n Shout out to  GitHub Summer of Code  student,  Mark Tareshawty , from  The Ohio State University  for his work on Classroom for GitHub.', 'Classroom for GitHub \n \n Classroom for GitHub is a  Ruby on Rails  application designed to automate repository creation and access control, making it easy for teachers to distribute starter code and collect assignments on GitHub \n \n How it works \n Assignments are the core of Classroom for GitHub. Teachers can easily create an assignment and distribute it to students using a private invitation URL. Optional starter code can be provided for individual or group work. It\'s even possible to delegate assignment creation and management to co-teachers and teaching assistants by adding them as organization administrators. \n Hacking on Classroom for GitHub \n Get started \n New to Ruby? No worries! You can follow these instructions to install a local server. \n Installing a Local Server \n First things first, you\'ll need to install Ruby 2.3.0. We recommend using the excellent  rbenv ,\nand  ruby-build \n bash\nrbenv install 2.3.0\nrbenv global 2.3.0 \n Next, you\'ll need to make sure that you have Nodejs, PostgreSQL, Redis, Memcached, and Elasticsearch installed. This can be done easily :\n* For OSX using  Homebrew  : You don\'t have to do anything! When you run  script/setup  later on this will be taken care of for you.\n* For Linux :  apt-get install nodejs postgresql redis-server memcached . For Elasticsearch, follow the instructions on  their website . \n You will want to set PostgreSQL to autostart at login via launchctl, if not already. See  brew info postgresql . Redis and memcached may be setup similarly via launchctl or setup project wide by using foreman, described below. \n Now, let\'s install the gems from the  Gemfile  ("Gems" are synonymous with libraries in other\nlanguages). \n bash\ngem install bundler && rbenv rehash \n Setup Classroom for GitHub \n If you are using Linux, configure PostgreSQL : \n \n Edit  /etc/postgresql/9.3/main/postgresql.conf  and uncomment  #unix_socket_permissions = 0777 \n Create a user and give him the rights to create a database :  su postgres -s /bin/bash -c "psql -c \'CREATE USER classroom_user; ALTER USER classroom_user CREATEDB\'"  (Change  classroom_user  to the username that will run the classroom server) \n \n Once bundler is installed (and PostgreSQL correctly configured for Linux users) go ahead and run the  setup  script : \n script/setup \n Production environment variables \n ENV Variable | Description |\n:-------------------|:-----------------|\n AIRBRAKE_API_KEY  | the API key for airbrake.io, if set Airbrake will be enabled\n CANONICAL_HOST  | the preferred hostname for the application, if set requests served on other hostnames will be redirected\n GOOGLE_ANALYTICS_TRACKING_ID  | identifier for Google Analytics in the format  UA-.* \n PINGLISH_ENABLED  | Enable the  /_ping  endpoint with relevant health checks\n MOTD  | Show the message of the day banner at the top of the site \n Development environment variables \n These values must be present in your  .env  file (created by  script/setup ). \n ENV Variable | Description |\n:-------------------|:-----------------|\n GITHUB_CLIENT_ID | the GitHub Application Client ID.\n GITHUB_CLIENT_SECRET | the GitHub Application Client Secret.\n NON_STAFF_GITHUB_ADMIN_IDS  | GitHub  user_ids  of users to be granted staff level access. \n Testing environment variables \n Classroom for GitHub uses  VCR  for recording and playing back API fixtures during test runs. These cassettes (fixtures) are part of the Git project in the  spec/support/cassettes  folder. If you\'re not recording new cassettes you can run the specs with existing cassettes with: \n bash\nscript/test \n Classroom for GitHub uses environmental variables for storing credentials used in testing, these values are located in your  .env  file (created by  script/setup ).\nIf you are recording new cassettes, you need to make sure all of these values are present. \n ENV Variable | Description |\n:-------------------|:-----------------|\n TEST_CLASSROOM_OWNER_ID  | The GitHub  user_id  of an organization admin.\n TEST_CLASSROOM_OWNER_GITHUB_TOKEN  | The  Personal Access Token  for the classroom owner\n TEST_CLASSROOM_STUDENT_ID  | Test OAuth application client ID.\n TEST_CLASSROOM_STUDENT_GITHUB_TOKEN  | The  Personal Access Token  for the student\n TEST_CLASSROOM_OWNER_ORGANIZATION_ID  | GitHub ID (preferably one created specifically for testing against).\n TEST_CLASSROOM_OWNER_ORGANIZATION_LOGIN  | GitHub login (preferably one created specifically for testing against). \n Running the application \n Foreman is setup to manage redis, memcached, sidekiq, and elasticsearch in development mode. Postgresql must be running prior executing foreman. \n After that, you may start the rails server in a separate terminal with: \n bash\nscript/server \n And that\'s it! You should have a working instance of Classroom for GitHub located  here \n Deployment \n We strongly encourage you to use  https://classroom.github.com , but if you would like your own version Classroom for GitHub can be easily deployed to Heroku. \n \n Contributing \n We\'d love to have you participate. Please check out  contributing guidelines . \n Contributors \n Classroom is developed by these  contributors . \n Shout out to  GitHub Summer of Code  student,  Mark Tareshawty , from  The Ohio State University  for his work on Classroom for GitHub.', 'Classroom for GitHub \n \n Classroom for GitHub is a  Ruby on Rails  application designed to automate repository creation and access control, making it easy for teachers to distribute starter code and collect assignments on GitHub \n \n How it works \n Assignments are the core of Classroom for GitHub. Teachers can easily create an assignment and distribute it to students using a private invitation URL. Optional starter code can be provided for individual or group work. It\'s even possible to delegate assignment creation and management to co-teachers and teaching assistants by adding them as organization administrators. \n Hacking on Classroom for GitHub \n Get started \n New to Ruby? No worries! You can follow these instructions to install a local server. \n Installing a Local Server \n First things first, you\'ll need to install Ruby 2.2.4. We recommend using the excellent  rbenv ,\nand  ruby-build \n bash\nrbenv install 2.3.0\nrbenv global 2.3.0 \n Next, you\'ll need to make sure that you have PostgreSQL, Redis, Memcached, and Elasticsearch installed. This can be\ndone easily on OSX using  Homebrew \n bash\nbrew install postgresql redis memcached elasticsearch \n You will want to set postgresql to autostart at login via launchctl, if not already. See  brew info postgresql . Redis and memcached may be setup similarly via launchctl or setup project wide by using foreman, described below. \n Now, let\'s install the gems from the  Gemfile  ("Gems" are synonymous with libraries in other\nlanguages). \n bash\ngem install bundler && rbenv rehash \n Setup Classroom for GitHub \n Once bundler is installed go ahead and run the  setup  script\n script/setup \n Production environment variables \n ENV Variable | Description |\n:-------------------|:-----------------|\n AIRBRAKE_API_KEY  | the API key for airbrake.io, if set Airbrake will be enabled\n CANONICAL_HOST  | the preferred hostname for the application, if set requests served on other hostnames will be redirected\n GOOGLE_ANALYTICS_TRACKING_ID  | identifier for Google Analytics in the format  UA-.* \n PINGLISH_ENABLED  | Enable the  /_ping  endpoint with relevant health checks\n MOTD  | Show the message of the day banner at the top of the site \n Development environment variables \n These values must be present in your  .env  file (created by  script/setup ). \n ENV Variable | Description |\n:-------------------|:-----------------|\n GITHUB_CLIENT_ID | the GitHub Application Client ID.\n GITHUB_CLIENT_SECRET | the GitHub Application Client Secret.\n NON_STAFF_GITHUB_ADMIN_IDS  | GitHub  user_ids  of users to be granted staff level access. \n Testing environment variables \n Classroom for GitHub uses  VCR  for recording and playing back API fixtures during test runs. These cassettes (fixtures) are part of the Git project in the  spec/support/cassettes  folder. If you\'re not recording new cassettes you can run the specs with existing cassettes with: \n bash\nscript/test \n Classroom for GitHub uses environmental variables for storing credentials used in testing, these values are located in your  .env  file (created by  script/setup ).\nIf you are recording new cassettes, you need to make sure all of these values are present. \n ENV Variable | Description |\n:-------------------|:-----------------|\n TEST_CLASSROOM_OWNER_ID  | The GitHub  user_id  of an organization admin.\n TEST_CLASSROOM_OWNER_GITHUB_TOKEN  | The  Personal Access Token  for the classroom owner\n TEST_CLASSROOM_STUDENT_ID  | Test OAuth application client ID.\n TEST_CLASSROOM_STUDENT_GITHUB_TOKEN  | The  Personal Access Token  for the student\n TEST_CLASSROOM_OWNER_ORGANIZATION_ID  | GitHub ID (preferably one created specifically for testing against).\n TEST_CLASSROOM_OWNER_ORGANIZATION_LOGIN  | GitHub login (preferably one created specifically for testing against). \n Running the application \n Foreman is setup to manage redis, memcached, sidekiq, and elasticsearch in development mode. Postgresql must be running prior executing foreman. It assumes that redis, memcached, and elasticsearch are not already running on the system. Alternatively, you may run  script/sidekiq , if you have to have redis, memcached, and elasticsearch always running system wide. To execute foreman, and this application\'s dependencies, run: \n bash\nscript/workers \n After that, you may start the rails server in a separate terminal with: \n bash\nscript/server \n And that\'s it! You should have a working instance of Classroom for GitHub located  here \n Deployment \n We strongly encourage you to use  https://classroom.github.com , but if you would like your own version Classroom for GitHub can be easily deployed to Heroku. \n \n Contributing \n We\'d love to have you participate. Please check out  contributing guidelines . \n Contributors \n Classroom is developed by these  contributors . \n Shout out to  GitHub Summer of Code  student,  Mark Tareshawty , from  The Ohio State University  for his work on Classroom for GitHub.', 'Jekyll Desktop \n An easy to use Jekyll Desktop Client powered by Electron \n \n Under development. You should probably use this only if you want to send pull request and stuff.  \n', 'White Paper \n White Paper is a jekyll theme for tech blogs. It is clean and simple with good semantic\nstructure. \n \n Usage: \n \n Install Jekyll from here(http://jekyllrb.com/) \n Fork the project. \n Change the fields in _config.yml. \n Change the links in header.html. \n \n Test the website live while editing the code: \n sh\njekyll server --watch \n Issues: \n Report issues or feature request  here .', 'ReadMe \n Add your library like this :\nPlease paste your .rs file without any main function in the src folder. Then in src, add your file in lib.rs, save and close it. \n Also in the main file in src, please add use etig:: , (see lines 15-21 in main file). After this, go to main function, call your function like this: \n your_file_name::function_name(&mut graph, na); \n Also make sure the calling function is declared like this in your module:\npub fn function_name(args<>) {}\n{See the main file for further help, or praneel.rs or samarth.rs for running examples.} \n We have given some inputs but if you want to give any others, please feel free to paste your input files out of the src folder. \n To run or test your files, run the command :-\ncargo build\ncargo run input_file.txt \n To test or run the library, download the zipped repo to your system.Extract it and do the above changes. After adding your modules and other files, run the main.rs or do cargo build and cargo run input_file.txt \n Update -- use 350*350 array size now, for integration, and i32 only.', '\n \n \n \n     \n \n \n  🔥🔥\n  Check out my latest project -  diff.blog \n \n \n Table of Contents \n \n Platform Documentation \n Tools For Building Bots \n Tools For Bot Analytics \n Tools For Bot Conversation Mockups \n Libraries \n Bot Stores \n Tutorials \n Communities \n Developers \n Testing \n \n Platform Documentation \n \n IBM Bot Asset Exchange \n Facebook messenger \n Slack \n Discord \n Telegram \n Kik \n Zulip \n HipChat \n Skype \n Cisco Spark \n Microsoft Bot Framework \n WeChat \n VKontakte \n Hangouts Chat \n \n Tools For Building Bots \n \n wit.ai   - Easily create text or voice based bots that humans can chat with. \n Botkit  - Botkit eases the process of designing and running bots that live inside Slack. \n Dialogflow  - Build natural and rich conversational experiences. \n Text It  - Visually build SMS and voice apps to engage your customers. \n Chatfuel  - The intuitive bot builder with AI navigation. No coding required. \n Watson  - Bring the power of cognitive computing to your apps. \n Beep Boop  - Beep Boop is a simple hosting platform for your Slack and Messenger bots. \n Dexter  - Get a Slack bot that responds with answers from your own Google Sheet. \n Converse AI  - Respond to customer questions instantly with intelligent chatbot technology. \n Gupshup  - Easiest & fastest way to build & deploy your bots on any channel. \n Haven OnDemand  - They are similar APIs as that of watson, but far well documented   and have a freemium version. \n Recast.AI  - Collaborative Bot Platform for developers: build conversational bots easily. \n ChatterBot  - Machine-learning based conversational dialog engine build in Python \n Google Cloud Platform  - Use sentiment analysis on a block of text to add decision tree logic for a conversation bot. \n Messenger Demo Viewer  - Demo FB Messenger bots without showing personal chats \n BotStar  - Powerful platform for designing & developing chatbots visually with smart training. \n \n Tools For Bot Analytics \n \n ChatMetrics  - Engagement platform for bots. ChatMetrics allows to reactivate inactive users and improve retention by sending "smart" notifications to users. \n Dialog  - Chatbot analytics platform for marketers to drive user engagement, retention, conversion and understand behavior. \n Dashbot  - Actionable Bot Analytics: Increase user engagement, acquisition, and monetization. \n Botmetrics  - Grow and retain your chatbot customers. Enterprise grade opensource conversational analytics, marketing automation, and CRM rolled into a single snippet of code. \n \n Tools For Bot Conversation Mockups \n \n Bot Society  - Bot conversation Mockups for presentation and demo. \n Bot Sketch  - Sketch Chatbot UI (Mac Only) \n Botframe  - a playground to design bots (conversation mockups). \n Botmock  - Create conversation flows and interactive prototypes from a simple drag and drop editor, then share or test those prototypes and get feedback in one centralized location. \n BotStar  - A fullfledged chatbot platform but the visual editor can be used as a mockup tool as well. \n \n Libraries \n General \n \n Qtypes  - Rule based Answer Type classification system in Node.js. \n BotKit  - Botkit is a toolkit for making bot applications. \n Claudia Bot Builder  - Create chat bots for FB, Slack, Skype and Telegram and deploy to AWS Lambda in minutes. \n Universal Bot Framework  - Create crossplatform chat bots for FB Messenger, Kik, Telegram and Skype with just one logic. \n Pandorabots  - web service for building and deploying chatbots. \n Microsoft Bot Framework  - Node.js/.NET library for building, connecting, testing, and deploying powerful and intelligent bots.  \n Bottender  - Make Bots in Your Way, Fast and Flexibly. \n BotMan  - PHP Bot Framework supporting over a dozen platforms (FB, Instagram, MS Bot Framework, etc) \n \n Facebook Messenger \n \n Node.js \n Go \n \n Slack \n \n Node.js (Official) \n Node.js \n Python (Official) \n Relax (scalable RTM library to scale to 1000\'s of teams) \n \n Telegram \n \n Node.js \n Node.js \n Node.js \n PHP \n PHP \n Python \n C# \n Go \n Java \n \n Discord \n Node.js \n \n Eris \n Discord.js \n Discordie \n Discord.io \n \n C \n \n Discord.Net \n DSharpPlus \n \n Python \n \n discord.py \n \n Java \n \n JDA \n \n Rust \n \n Serenity \n \n Lua \n \n Discordia \n \n Go \n \n Discordgo \n \n Kik \n \n Python \n Node.JS \n \n Bot Stores \n \n There is a bot for that  - Search engine for bots \n ChatBottle  - Chatbots search engine \n BotList  - An App Store For Bots. \n Slack Store  - The official Slack bot store. \n Telegram Store  - The unofficial Telegram bot store. \n Product Hunt  - Product Hunt bot list. \n Dashbot  - Dashbot bot directory. \n Skype Bot Directory  - The official list of skype bots by Microsoft Bot Framework \n Teamchat  - Bot store by teamchat. \n Discord Bots  - An unofficial list of discord bots. \n Discord Bot List  - An unofficial list of discord bots. \n Telegram Botlist  - A bot list for Telegram. \n Mayo Bot List  - An unofficial list of discord bots. \n Terminal Bot List  - An unofficial list of discord bots. \n \n Tutorials \n \n The Complete Beginner’s Guide To Chatbots . \n The Secret To Making Your Own Facebook Messenger Bot In Less Than 15 Minutes. \n How to create a chatbot without coding a single line \n How To Build Bots for Messenger. \n Creating A Chat Bot. \n A Beginner’s Guide To Your First Bot. \n Pair Programming a Facebook Messenger Bot. \n Facebook Chatbot Tutorial \n Design Framework For Chatbots \n UX of Chatbots \n \n Communities \n \n Bots  - Facebook group. \n UX for Bots  - Facebook group. \n Chat Bot Magazine  - Medium. \n HH Bots  - Facebook group. \n Messenger Platform Developer Community  - Official Facebook group. \n Chatbots and Conversational Agents  - Quora Topic \n Bot Developer Hangouts  - Community on Slack for all bot platforms. \n BotList Chat  - Telegram group. \n Bot Development  - Telegram group. \n \n Conferences \n \n ChatbotConf \n Talkabot \n \n Developers \n \n BotMakers  - The best place to hire chatbot developers \n Botgig  - Hire a top chatbot dev from a vetted talent pool. \n \n Testing \n \n ChatbotTest  - Chatbottest is an open source guide that helps you identify chatbot\'s design issues under 7 different categories. \n BotTesting  - Test your bot! Send your email and bot link to us, we’ll examine your bot’s performance and usability \n \n License \n \n To the extent possible under law,  Vishnu Ks  has waived all copyright and related or neighboring rights to this work.', 'Dotfiles \n Collection of My dotfiles. \n Installation \n Terminator \n Copy the terminator folder to ~/.config folder. \n VIM \n Copy the vimrc file to /etc/vim.  \n License \n \n To the extent possible under law,  Vishnu Ks  has waived all copyright and related or neighboring rights to this work.', 'onebot', 'Program Filename Extensions \n \n Filename extensions of programming languages \n \n | Language      | Source File Extension |\n| ------------- |:---------------------:|\n| Bash          | .sh                   |\n| C             | .c                    |\n| C#            | .cs                   |\n| C++           |  .C, .cc, .c++, .cp, .cpp, .CPP, .cxx|\n| Java          | .java                 |\n| Java Servlet  | .do                   |\n| JavaScript    | .js                   |\n| JSP           | .jsp                  |\n| Objective-C   | .m                    |\n| Perl          | .pl                   |\n| PHP           | .php                  |\n| Python        | .py                   |\n| Ruby              | .rb                           |\n| Simula        | .sim                  |\n| Swift         | .swift                |', 'assign-issue-bot \n A Github bot for assigning issues to team members', 'dev-gear', 'Mentorships \n I would be posting the assignments and materials here. Please keep on checking them every few days. Make sure that you try to complete all the assignments in time. If you are in trouble or want to talk with other students or me join the  chat . If you have any quizes of lab exams let me know before hand. I will change the deadlines accordingly.  \n Assignment 1 \n Deadline - September 9, 2016 \n \n Create an account on  Github .  \n Create an account on  Gitter  by logging with the Github account you just created.  \n Write a  C  program for checking whether a number is prime or not. Name the file as  check_prime.cpp . The program will take a number  as input and outputs  True  if it\'s prime and  False  if it\'s not. \n Don\'t copy. I will check the code for plagiarism myslef.  \n Share the check_prime.cpp file as soon as you complete in the group  chat . \n \n Resources \n \n Learn C -  If you have some experience with programming before read this book.  The C Programming Language  else refer to Programming in ANSI C by Balaguruswamy \n Edx cs50 - The best CS course in the world. An introduction to the intellectual enterprises of computer science and the art of programming. \n \n Assignment 2 \n Deadline - September 13, 2016 \n \n Make a repository called programs in Github. \n Add the  check_prime.cpp  file to  programs  repository. \n The commit message should be "Add program to check whether number is prime." \n Share the link of your repository in the group chat.  \n \n Resources \n Go through  this guide  carefully. \n Assignment 3 \n Deadline - September 21, 2016 \nAdd a new language and extension to the README.md file of  this repositary . \n Extra points if you commit using Git rather than using Github :)\nHINT: You have to create a new pull request for making me accept the change. ', '\n HEXBIN \n \n How It Works \n Code \n JSON Security \n', 'habit', 'slides', 'Talks \n | Topic                     | Event                                     | Date      |Slide|\n|---------------------------|-------------------------------------------|-----------|-----|\n| Building a profitable business around your open source project| TU Munich open source course |02-Feb-2021 | PDF ||\n| What I learned from contributing to open source| GeekHaven QuickTalks powered by GitHub|24-Jan-2017|  PDF  |\n| Art of Open Source        |Google Summer of Code Intro - GDG Allahabad|21-Aug-2016|  PDF  |']
mrecachinas,["mrecachinas.github.io \n Build Setup \n ```bash \n install dependencies \n $ npm install \n serve with hot reload at localhost:3000 \n $ npm run dev \n build for production and launch server \n $ npm run build\n$ npm run start \n generate static project \n $ npm run generate\n``` \n For detailed explanation on how things work, check out the  documentation . \n Special Directories \n You can create the following extra directories, some of which have special behaviors. Only  pages  is required; you can delete them if you don't want to use their functionality. \n assets \n The assets directory contains your uncompiled assets such as Stylus or Sass files, images, or fonts. \n More information about the usage of this directory in  the documentation . \n components \n The components directory contains your Vue.js components. Components make up the different parts of your page and can be reused and imported into your pages, layouts and even other components. \n More information about the usage of this directory in  the documentation . \n layouts \n Layouts are a great help when you want to change the look and feel of your Nuxt app, whether you want to include a sidebar or have distinct layouts for mobile and desktop. \n More information about the usage of this directory in  the documentation . \n pages \n This directory contains your application views and routes. Nuxt will read all the  *.vue  files inside this directory and setup Vue Router automatically. \n More information about the usage of this directory in  the documentation . \n plugins \n The plugins directory contains JavaScript plugins that you want to run before instantiating the root Vue.js Application. This is the place to add Vue plugins and to inject functions or constants. Every time you need to use  Vue.use() , you should create a file in  plugins/  and add its path to plugins in  nuxt.config.js . \n More information about the usage of this directory in  the documentation . \n static \n This directory contains your static files. Each file inside this directory is mapped to  / . \n Example:  /static/robots.txt  is mapped as  /robots.txt . \n More information about the usage of this directory in  the documentation . \n store \n This directory contains your Vuex store files. Creating a file in this directory automatically activates Vuex. \n More information about the usage of this directory in  the documentation .", 'Dotfiles', 'Page Faults \n This was a challenge for Spring 2014\'s CS 4414: Operating Systems class at the University of Virginia taught by Professor David Evans. \n Preliminary Questions \n \n When does a page fault occur? \n A page fault occurs \n \n \n Question: Is there a way to exploit this to write a program that generates approximately  N  page faults? \n Yes! \n \n \n \n Breakdown of the Code \n \n \n By using  getpagesize()  or something similar, we grab the machine\'s given page size.  \n \n By default, on an x86 machine, this should be 4 KB = 4096 bytes. \n \n \n \n Next, by using  sizeof(int) , we can grab the language\'s  signed integer  size. \n \n For C, this should default to 4 bytes. \n For Rust, this defaults to 8 bytes. \n For Python, this defaults to 24 bytes. \n \n \n \n Then, we can find the number of  int s per page. \n \n For C, 4096/4 = 1024. \n For Rust, 4096/8 = 512. \n For Python, 4096/24 = 170.67 ~ 171. \n \n \n \n By using a 2D int array  int arr[N][ints_per_page] , where  N  = number of desired page faults and  ints_per_page  = page sized normalized to an int, we loop in column major order, which skips over an entire page. Therefore, each iteration of the inner loop results in a minor page fault. Having N rows means we ultimately perform this N times, which yields approximately N page faults. \n \n \n Running the Code \n What you\'ll need: \n \n C Compiler:  gcc ,  clang , or  llvm  compiler \n My 2011 13" Macbook Air Mac OS X 10.9.1 runs  Apple LLVM version 5.0 (clang-500.2.79) (based on LLVM 3.3svn) \n \n \n Rust Compiler:  rustc 0.9 \n Python Interpreter: (Anything really) 2.7.5 or higher \n Mac OS X comes preinstalled with a Python interpreter \n \n \n \n How to Run Automated Test Script: \n \n First, ensure all print statements within page_fault.c, page_fault.rs, and page_fault.py are commented out. \n I have provided a Python test script which will run page_fault.c, page_fault.rs, and page_fault.py 500 times with varying inputs. \n To run:  python run_script.py \n This will output several new files of the form  c_0.txt  denoting the language (C) and the number of desired page faults (0). \n \n \n Then, using  cat <textfile.txt> | grep "[0-9]*  page reclaims" | sed "s/[a-z]//g"> <output.out>  you can grab the specific number of minor page faults and pipe the result to a new file. \n Finally, to average each file, just run  python average.py < <output.out> . \n \n How to Run without Test Script: \n \n For C:  gcc -o c_page_fault page_fault.c && /usr/bin/time -l ./c_page_fault <number of desired page faults> . \n Note: if you\'re using  clang , just replace  gcc  with  clang  in previous command. \n \n \n For Rust:  rustc page_fault.rs && /usr/bin/time -l ./page_fault <number of desired page faults> \n For Python:  /usr/bin/time -l python page_fault.py <number of desired page faults> \n \n Note : Also, if you run  top  and view the  FAULTS  column, this will list the approximate number of page faults for any given process. If you would like to see this program in  top , just include the print statements so the programs execute for a longer duration. \n Note : use  /usr/bin/time -l  because  time -l  is not a valid flag for that command. \n Note : you will most likely see  page faults: 0 , this is OK!  page faults  indicates number of  major  page faults (to be discussed later...). What we\'re interested in is  page reclaims , which are  minor  page faults. \n Results \n From my test script, I found the following results: \n For 0 desired page faults and 500 runs: \n \n C : 145.41 \n Rust : 434.89 \n Python : 1501.182 \n \n For 500 desired page faults and 500 runs: \n \n C : 641.648 \n Rust : 937.048 \n Python : 1664.762 \n \n Difference between major and minor (and invalid!) page faults \n Major Page Faults \n The page is still in memory, but the MMU doesn’t know about it. \n Minor Page Faults \n The page is only on disk and must be retrieved with the disk penalty.  \n Invalid Page Faults \n When MMU/TLB goes to disk address, null pointer or permission denial -> program terminates via segmentation fault and core is dumped. \n Can we write a program to generate  N   major  page faults? \n Maybe. The reason why major page faults are more difficult to program is because the OS will try to predict which pages you may want to access. \n The elephant, or  python , in the room \n Why does Python generate 1000+ minor page faults every time the program runs? \n If you\'re interested, the following article offers a great explanation as to the internal workings of Python. From this, you may be able to see where these page faults might be generated. \n Why Your Python Runs Slow Pt. 1, - Data Structures \n Sources \n \n CS 4414 \n The Internet \n', "Beer to Cats \n Installation \n \n Ensure that you have python 2.7.5+ \n Install  pip \n On Ubuntu:  sudo apt-get install python-pip \n On Windows, OS X, or Ubuntu:  \n First, download  get-pip.py   $ curl https://raw.github.com/pypa/pip/master/contrib/get-pip.py \n Then run  $ python get-pip.py \n \n \n \n \n $ pip install -r requirements.txt  (may require  sudo ) \n \n Installation Errors? \n We did not experience any issues installing natively. We did experience issues installing PIL on  \n Usage \n There are two primary ways to use the project: \n \n \n Run the program via the command line and input images as command line parameters.\n     $ python beer_to_cats.py <image.{png,jpg,jpeg,gif}> \n \n \n Run the program on the webapp we've created. You can do that ~~either by navigating to  BeerToCats.com ~~ (Edit: we've been having issues with pip and Heroku, so we're currently moving to an EC2 instance) by executing the following commands:\n    ```\n    $ python yolocups.py \n \n Running on http://127.0.0.1:5000/ \n Restarting with reloader\n```\nThen navigate to http://127.0.0.1:5000/ and follow the instructions on the website as you would otherwise. \n \n \n \n Demo \n Input: \n \n Output: \n \n Input: \n \n Output: \n \n Theory \n Let's say we have the following image where we want to find all of the solo cups and cover them with a cat. We will refer to this image in the following paragraphs. \n \n There are several widely-used methods for feature detection – e.g.  Haar-like Feature Detection ,  Histogram-of-Oriented Gradient (HOG) Feature Detection , etc. \n For this project, we chose HOG detection. Therefore, we also needed to use a  Support Vector Machine (SVM)  as the classifier. \n To get the best results, we need a training set. We peroused the web for images of cans, bottles, and solo cups. Then, since this involves supervised learning, we went through drawing boxes around all of the cans, bottles, and cups. Using the previous image, we end up with the following: \n \n Once we have a good set of these, we generate an XML file of these images with the box annotations. From this, we then compute the HOG representation of the boxed images. We'll choose the most prominent solo cup for this example. \n Before HOG: \n \n How the computer sees it: \n \n After computing the HOG of the image: \n \n Overlayed HOG Representation (computed using Mathematica) \n \n \n \n Pure HOG Representation (computed using Matlab) \n \n \n \n Another Pure HOG Representation (using  MIT's HOGgles ) \n \n \n These are fed into the SVM, with all of the possible orientations replicated and fed into the SVM as well. \n OK, so What is Histogram of Oriented Gradient (HOG)? \n The essence of HOG, or any feature descriptors for computer vision revolves around the fact that we want the computer to understand what the object it's looking at. HOG relies on the fact that the local object will be composed of pixels with similar values while the background will be composed of contrasting pixels. Thus, based on these differences in neighboring pixel values, we can determine the edge orientation of a particular pixel. The combination of all these histograms represent the actual features. \n What's a Support Vector Machine (SVM)? \n A support vector machine is a common, yet robust, supervised learning classifier. The idea of supervised learning is that given a training data points each belonging to a particular category, we use a supervised learning model like Support Vector Machine, Logistic Regression, Naïve-Bayes, Neural Networks, Decision Trees, etc. to predict which category a new data point belongs to. \n Support Vector Machines take a set of features and map it to a set of hyperplanes in some high dimensional space. This allows the model to have non-linearity. SVMS are used to maximize the margin of separation between classes. SVMs use a kernel function that is used to map the data points to fit the maximum-margin hyperplane. Depending on the data, we can use different kernel functions:  Wikipedia - Support Vector Machine . \n For our program, after we transform our image to get the HOG features, we take our window of mxn pixels and reshape it into 1x(mn). Each of the data points in the vector represents a pixel value, which represents a feature for our supervised learning model. The class of each data point is either 1 (for cup) or (-1) for non-cup. With these vectors as our training data, we run a support vector machine with linear kernel to create our model. \n Results \n In depth analysis of results coming soon. \n Future Improvements \n \n Train with more images \n Compare with Haar-like feature detection \n Use neural nets for a deeper learning \n \n TODO \n For program:\n-  Known Issue : program only works currently on jpgs and NOT pngs. \n For documentation:\n- Edit  README.md : ensure instructions actually work... ugh I hate instructions that don't work.", "sigplot-bitarray \n A Rust implementation of SigPlot's bluefile.js BitArray in Rust and cross-compiled to wasm using  wasm-pack . \n \n About \n 📚 Read this template tutorial! 📚 \n This template is designed for compiling Rust libraries into WebAssembly and\npublishing the resulting package to NPM. \n Be sure to check out  other  wasm-pack  tutorials online  for other\ntemplates and usages of  wasm-pack . \n 🚴 Usage \n 🐑 Use  git  to Clone this Repository \n git clone https://github.com/mrecachinas/sigplot-bitarray.git sigplot-bitarray\ncd sigplot-bitarray \n 🛠️ Build with  wasm-pack build \n wasm-pack build \n 🔬 Test in Headless Browsers with  wasm-pack test \n wasm-pack test --headless [--firefox|--chrome] \n 🎁 Publish to NPM with  wasm-pack publish \n wasm-pack publish \n 🔋 Batteries Included \n \n wasm-bindgen  for communicating\n  between WebAssembly and JavaScript. \n console_error_panic_hook \n  for logging panic messages to the developer console. \n wee_alloc , an allocator optimized\n  for small code size. \n", "Hi, I'm Mike 👋 \n \n \n**mrecachinas/mrecachinas** is a ✨ _special_ ✨ repository because its `README.md` (this file) appears on your GitHub profile.\n\nHere are some ideas to get you started:\n\n- 🔭 I’m currently working on ...\n- 🌱 I’m currently learning ...\n- 👯 I’m looking to collaborate on ...\n- 🤔 I’m looking for help with ...\n- 💬 Ask me about ...\n- 📫 How to reach me: ...\n- 😄 Pronouns: ...\n- ⚡ Fun fact: ...\n"]
y0ast,['Curriculum-Builder', 'Automatic newsletter for AUC and AUCSA \n Heroku dependencies: sendgrid, postgres \n Intended for use with MailChimp. \n Config.yml requires:\n- db : [Database url]\n- username: [Gmail username]\n- password: [Gmail password] \n brew install postgres\ninitdb /usr/local/var/postgres\ncreatedb newsletter \n Url then becomes  db: postgres://localhost/newsletter \n To start and stop postgres:\n pg_ctl -D /usr/local/var/postgres start\npg_ctl -D /usr/local/var/postgres stop', 'My personal website. \n Built using Bootstrap, with some inspiration from https://jonbarron.info/. \n Blogs are rendered using VSCode and the Markdown All in One plugin, with two\nadditional lines for styling: \n <link rel="stylesheet" href="blog_style.css">\n<meta name="viewport" content="width=device-width, initial-scale=1.0">', 'Variational Auto-encoder \n This is an improved implementation of the paper  Stochastic Gradient VB and the Variational Auto-Encoder  by D. Kingma and Prof. Dr. M. Welling. This code uses ReLUs and the adam optimizer, instead of sigmoids and adagrad. These changes make the network converge much faster. \n In my other  repository  the implementation is in Torch7 (lua), this version is based on Theano (Python).\nTo run the MNIST experiment: \n python run.py \n Setting the continuous boolean to true will make the script run the freyfaces experiment. It is necessary to tweak the batch_size and learning rate parameter for this to run smoothly. \n There used to be a scikit-learn implementation too, but it was very slow and outdated. You can still find it by looking at the code at  this commit \n The code is MIT licensed.', 'Variational Auto-encoder \n This is an improved implementation of the paper  Stochastic Gradient VB and the Variational Auto-Encoder  by D. Kingma and Prof. Dr. M. Welling. This code uses ReLUs and the adam optimizer, instead of sigmoids and adagrad. These changes make the network converge much faster. \n In my other  repository  the implementation is in Python (Theano), this version is based on Torch7. \n To run the MNIST experiment: \n th main.lua \n Setting the continuous boolean to true will make the script run the freyfaces experiment. \n The code is MIT licensed. \n I gratefully reused MNIST downloading and reading code written by  Rahul G. Krishnan .', 'This is the main model class file used for the  VRAE  (ICLR 2015 workshops) \n For preprocessing the MIDI files into numpy ndarrays, we used code from the RNN-RBM model by Boulanger-Lewandowski et al. found  here . Direct download via  here , see specifically the util.py file. \n For some example code on how to use this class refer to  Variational-Autoencoder , which uses largely the same structure.', 'Variational Inference for Monte Carlo objectives (VIMCO) \n This repository contains all the code necessary to reproduce the results of the  VIMCO  paper using Theano. This code follows the original implementation as closely as possible. For 5 samples it obtains a better score than what is mentioned in the original paper (-91.7 vs -93.6). \n Usage for CPU: \n python main.py experiment/ \n Usage for GPU (with GPU also responsible for rendering desktop): \n THEANO_FLAGS=device=gpu,floatX=float32,lib.cnmem=0.8 python main.py experiment/ \n Usage for GPU (with GPU running headless and needing no memory for desktop): \n THEANO_FLAGS=device=gpu,floatX=float32,lib.cnmem=1 python main.py experiment/ \n Expected output: \n ```\n[parameters]\nlearning_rate = 0.001\nb1 = 0.95\nb2 = 0.999\nbatch_size = 24\nlayers = 200,200,200\nmax_epoch = 2000\nsamples = 5\nlam = 1 \n not using any saved parameters\nE: 1, S: 2.08e+03, L: -166.194836549, T: 10.9810910225\nthe likelihood on the validation set is:  -145.713702181\nE: 2, S: 4.17e+03, L: -140.226117218, T: 10.8992979527\nE: 3, S: 6.25e+03, L: -133.945647934, T: 10.9269690514\nE: 4, S: 8.33e+03, L: -130.410928354, T: 10.9022259712\n...\nE: 1998, S: 4.16e+06, L: -93.4796990533, T: 11.2385241985\nE: 1999, S: 4.16e+06, L: -93.4315080534, T: 11.1370959282\nE: 2000, S: 4.17e+06, L: -93.4644727743, T: 11.1671891212\nthe likelihood on the validation set is:  -96.1817625556\n``` \n E means Epoch, S means Steps (as reported in the original paper), L means Log-likelihood (paper reports negative log-likelihood, which is just a sign flip) and T means Time in seconds. ~11 seconds is the runtime on a GTX 750 gpu with an Athlon II X3 425 triple core processor, on a Titan X the performance will likely be much better. \n To compute a score on the test set: \n THEANO_FLAGS=device=gpu,floatX=float32,lib.cnmem=0.8 python main.py experiment/ --test \n After 2000 epochs this leads to a variational lower bound on the test set of -91.65 for 5 samples, see also the  config file  (VIMCO paper obtains -93.6). \n To run the experiment with other hyper parameters, one only needs to change the  config file . \n Notes on implementations: \n The sampling is implemented separately from forward/backward for gradients. This leads to computing the forward of the inference (q) network twice, which is not optimally efficient. Also the first activation is computed on a replicated batch, an alternative is to just compute the first activation for the original batch and then sample for "repeat" amount of times. The result can be reshaped and used in further computation. \n List of files: \n \n main.py  contains the code that runs everything. It supports the -r/--restart flag for restarting despite of saved parameters and -t/--test flag for computing the score on the test set and then exiting \n VIMCO.py  contains the VIMCO specific code including the novel estimator \n Model.py  contains general model code, such as initialization of parameters, Adam updates and loading/saving of parameters. \n SBN.py  contains train/test/valid functions and compiles the full Theano functions. \n utils.py : contains some useful utilities that are class independent \n', 'Glow \n This repository implements the  Glow  model using PyTorch on the CIFAR-10 and SVHN dataset. We use the trained Glow to reproduce some of the results of the paper  "Do Deep Generative Models Know What They Don\'t Know?" : \n \n To create histogram :\nSee  notebook .\nPretrained model (on CIFAR-10):  download  (unzip before use). \n Note this pretrained model was created using the  affine  coupling layer, so it does not work well for generative sampling (see qualitative vs quantitative models in the Glow paper). The pretrained model achieves 3.39 bpd, while the original paper gets 3.35. The difference between our pretrained model and the paper is that we use batch size 64 (single GPU) and the paper uses 512 (8 GPU). \n This code uses some layers and groundwork from  glow-pytorch , but is more modular, extendable, faster, easier to read and supports training on CIFAR-10 and SVHN. There are fewer dependencies and a consistent interface for new datasets. Thanks to  Milad  for comments and help with debugging. \n Setup and run \n The code has minimal dependencies. You need python 3.6+ and up to date versions of: \n pytorch (tested on 1.1.0)\ntorchvision\npytorch-ignite\ntqdm \n To install in a local conda: \n conda install pytorch torchvision pytorch-ignite tqdm -c pytorch \n To train your own model: \n python train.py --download \n Will download the CIFAR10 dataset for you, and start training. The defaults are tested on a  1080Ti , Glow is a memory hungry model and it might be necessary to tune down the model size for your specific GPU. The output files will be send to  output/ . \n Everything is configurable through command line arguments, see \n python train.py --help \n for what is possible. \n Evaluate \n There are two notebooks available for evaluation: \n \n The  first notebook  reproduces a plot from "Do Deep Generative models know what they don\'t know?" (see above) and computes the average bpd on the CIFAR-10 and SVHN test sets. \n The  second notebook  allows you to visualise samples from the model (This works best with a model trained using the  additive  coupling layer). \n \n Extensions \n There are several possible extensions: \n \n Multiclass conditional training \n port over the  tests \n \n PRs for any of these would be very welcome. If you find any problem, feel free to make an  issue  too. \n The model is trained using  adamax  instead of  adam  as in the original implementation. Using  adam  leads to a NLL of 3.48 (vs. 3.39 with  adamax ). Note: when using  adam  you need to set  warmup  to 1, otherwise optimisation gets stuck in a poor local minimum. It\'s unclear why  adamax  is so important and I\'m curious to hear any ideas! \n References: \n ```\n@inproceedings{kingma2018glow,\n  title={Glow: Generative flow with invertible 1x1 convolutions},\n  author={Kingma, Durk P and Dhariwal, Prafulla},\n  booktitle={Advances in Neural Information Processing Systems},\n  pages={10215--10224},\n  year={2018}\n} \n @inproceedings{nalisnick2018do,\n    title={Do Deep Generative Models Know What They Don\'t Know? },\n    author={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},\n    booktitle={International Conference on Learning Representations},\n    year={2019},\n    url={https://openreview.net/forum?id=H1xwNhCcYm},\n}\n```', "Slurm for Machine Learning \n Many labs have converged on using  Slurm  for managing their shared compute resources.\nIt is fairly easy to get going with Slurm, but it quickly gets unintuitive when wanting to run a hyper-parameter search.\nIn this repo, I provide some scripts to make starting many jobs painless and easy to control. \n Starting a single job \n This is easy, but it's here for completeness: \n sbatch generic.sh train_script.py --output_folder CIFAR10/baseline --dataset CIFAR10 --learning_rate 1e-4 --method baseline \n You simply pass the arguments you normally pass to  python  to  generic.sh  and it'll start the job for you on Slurm with a gpu and a  conda  environment set up. \n Note:  the  --output_folder  argument is required here. It is necessary down the line to be able to skip jobs that were done already! \n Starting many jobs \n For this use case, Slurm has introduced  Job Arrays .\nSlurm assigns separate jobs a simple job array id, which is an integer that starts counting from 1.\nThis does not map well onto the usual machine learning jobs that requires running over a grid of hyperparameters. \n For this use case, I present an easy flow:\n1) Define a grid and go through all the resulting jobs (and skip jobs if you later extend the grid and rerun)\n2) Robust against failures (e.g. server crashing, kill jobs mid run etc.)\n3) Easily limit parallelism - simply set max number of GPUs to use \n The solution involves creating a file with all jobs you want to run (could be created by a Python/Bash script itself).\nWe then iterate through this file by using the Slurm job array id to index a  line !\nWhen iterating, we check if a job finished ( results.json  found in the output folder) and skip it if so. \n Step 1: \n ./create_jobs.sh \n This command creates a job list, with all the jobs we want to run.\nThere are two requirements: \n1) your command has an  --output_folder  flag which is respected in the training code\n2) a successful job creates a  results.json  in that folder. \n If we later want to add jobs, we can simply update this script, generate new jobs, and slurm will skip jobs that were successfully run before! \n Step 2: \n ./run_file.sh MNIST_jobs.txt \n This will start 8 jobs in parallel using Slurm job arrays.\nYou can easily change the number of jobs run in parallel by editing the top of  run_file.sh .\nIt'll check (in  generic.sh ) if a job succeeded before, and skip if that's the case. \n Setup requirements summarised \n \n conda  - by default in the folder  miniconda3  along side these scripts. Change the paths in  generic.sh  to match your setup. \n Within Python, save your final results to a file called  results.json  in the folder specified by  --output_folder  so the script can check if the jobs was successful. You can also edit this check for your particular setup (e.g. check for a final model saved). \n \n If you update your conda environment as part of the script, then  run_locked.sh  is necessary because  conda  is not thread safe by itself, and calling update multiple times in different processes leads to incorrect behaviour.\nIf you don't update your conda environment as part of the script, then you can skip this line. \n I have attempted to comment  generic.sh  as much as possible, so it's easy to see what to change for your Slurm setup! \n Happy Slurming! \n Let me know if you have any issues with the scripts, or if you see room for improvement. I am happy to accept PRs. \n Useful Commands \n Count all GPUs available in partition  normal :\n sinfo -N --partition normal -o %G | awk -F ':' '{sum += $3} END {print sum}' \n Count all GPUs that are part of running jobs in all partitions:\n squeue -t R --format='%b' --all  | awk -F ':' '{sum += $NF} END {print sum}' \n Depending on your Slurm setup you will want to tweak the partition (perhaps add a reservation) and maybe not use  --all  in  squeue . \n Other resources \n Check out my other help scripts:\n1.  Train a ResNet to 94% accuracy on CIFAR-10 with only 150 lines of PyTorch \n2.  FastMNIST - a drop in replacement for PyTorch' MNIST that avoids unnecessary processing - leading to 2-3x speed up on a  GPU \n3.  Obtain a dataset with a subset of ImageNet  classes  in PyTorch with minimal changes", 'Deterministic Uncertainty Quantification (DUQ) \n This repo contains the code for  Uncertainty Estimation Using a Single Deep Deterministic Neural Network , which is accepted for publication at ICML 2020. \n If the code or the paper has been useful in your research, please add a citation to our work: \n @article{van2020uncertainty,\n  title={Uncertainty Estimation Using a Single Deep Deterministic Neural Network},\n  author={van Amersfoort, Joost and Smith, Lewis and Teh, Yee Whye and Gal, Yarin},\n  booktitle={International Conference on Machine Learning},\n  year={2020}\n} \n Dependencies \n The code is based on PyTorch and requires a few further dependencies, listed in  environment.yml .\nThe code was tested with the versions specified in the environment file, but should work with newer versions as well (except for ignite=0.4.3).\nIf you find an incompatibility, please let me know and I\'ll gladly update the code for the newest version of each library. \n Datasets \n Most datasets will be downloaded on the fly by Torchvision. Only NotMNIST needs to be downloaded in advance in a subfolder called  data/ : \n mkdir -p data && cd data && curl -O "http://yaroslavvb.com/upload/notMNIST/notMNIST_small.mat" \n FastFashionMNIST is based on  this code .\nThe default Torchvision implementation first creates a PIL image (see  here ) which creates a CPU bottleneck (while training on GPU).\nThe FastFashionMNIST class provides a significant speed up. \n Running \n The Two Moons experiments can be replicated using the  Two Moons notebook .\nThe FashionMNIST experiment is implemented in  train_duq_fm.py .\nFor both experiments, the paper\'s default are hardcoded and can be changed in place. \n The ResNet18 based CIFAR experiments are implemented in  train_duq_cifar.py .\nAll command line parameter defaults are as listed in the experimental details in Appendix A of the paper.\nWe additionally include a Wide ResNet based architecture. \n For example: CIFAR-10 with gradient penalty with weight 0.5 and full training set: \n python train_duq_cifar.py --final_model --l_gradient_penalty 0.5 \n Note that ommitting  --final_model  will lead to 20\\% of the training data to be used for validation, such that hyper parameter selection can be done in a responsible manner.\nThe code also supports the Wide ResNet with  --architecture WRN . \n I also include code for my implementation of Deep Ensembles.\nIt\'s a very simple implementation that achieves good results (95\\% accuracy in 75 epochs using 5 models). \n python train_deep_ensemble.py --dataset CIFAR10 \n This command will train a Deep Ensemble consisting of 5 models (the default) on CIFAR10. \n Questions \n For questions about the code or the paper, feel free to open an issue or email me directly.\nMy email can be found on my GitHub profile, my  website  and the paper above. \n', "👈 I'm Joost, a PhD student in ML at Oxford \n👉  Google Scholar  for my research \n👇 For my projects", 'DUE and SNGP \n Deterministic Uncertainty Estimation \n This repo contains the official code for  On Feature Collapse and Deep Kernel Learning for Single Forward Pass Uncertainty . \n Spectral Normalized GP \n It also contains an implementation of  Simple and Principled Uncertainty Estimation with Deterministic Deep Learning via Distance Awareness  (or SNGP), for easy comparison.\nWe only implement the exact predictive covariance version, which is both simpler and performs better than the momentum based scheme. \n Run Example \n Make sure the dependencies listed in  environment.yml  are available and run: \n python train_due.py \n which will automatically download the dataset ( CIFAR10  by default), and start training.\nThere are several command line flags available for changing the hyper-parameters. \n A model trained using the defaults is available from  here . \n A regression example is implemented in  toy_regression.ipynb . \n If you want to train SNGP, simply add the flag (and adjust the learning rate): \n python train_due.py --sngp --learning_rate 0.05 \n Library \n The repository is split into a reusable library and utils only used for the specific training script. You can install the library part using pip: \n pip install --upgrade git+https://github.com/y0ast/DUE.git \n Alternatively you can just copy over the components you want! \n Citation \n If you use this repository, please cite:\n @article{van2021on,\n  title={On Feature Collapse and Deep Kernel Learning for Single Forward Pass Uncertainty},\n  author={van Amersfoort, Joost and Smith, Lewis and Jesson, Andrew and Key, Oscar and Gal, Yarin},\n  journal={arXiv preprint arXiv:2102.11409},\n  year={2021}\n} \n If you use the SNGP model, then please cite the original paper:\n @article{liu2020simple,\n  title={Simple and principled uncertainty estimation with deterministic deep learning via distance awareness},\n  author={Liu, Jeremiah and Lin, Zi and Padhy, Shreyas and Tran, Dustin and Bedrax Weiss, Tania and Lakshminarayanan, Balaji},\n  journal={Advances in Neural Information Processing Systems},\n  volume={33},\n  pages={7498--7512},\n  year={2020}\n} \n Acknowledgements \n Thanks to  Lewis  for the RFF implementation,  John  for testing DUE,  Jishnu  for evaluating SNGP, and  Jeremiah  for checking SNGP.', "This is a collection of PyTorch snippets that have helped me (and others) significantly with doing ML research. \n \n Minimal CIFAR-10  A very simple training script to get 94% accuracy with just ~150 lines of code, for an easy but strong baseline. \n FastMNIST : a drop-in replacement for the standard MNIST dataset which speeds up training on the  GPU  (by 2x !) by avoiding unnecessary preprocessing that pegs the cpu at 100% for small models. \n Subset of ImageNet : it's remarkably difficult to train on a subset of the ImageNet classes with the default TorchVision datasets. This snippet makes the minimal changes to them to make it very easy. \n ImageNet dogs vs not dogs : a standardized setup for the ImageNet Dogs vs Not Dogs out-of-distribution detection task. \n \n In a separate repository,  Slurm for ML , I explain how I use  slurm  job arrays without pain using a simple 1 file shell script.", 'Exact Nearest Neighbor \n This repo implements exact nearest neighbor search on the  sift datasets .\nIt requires rust-nightly, because it uses the  portable-simd  crate.\nThe code can be run natively on "Apple Silicon" as well. \n It matches the speed of  Faiss \' IndexFlatL2 when running in single element mode.\nNote that Faiss uses hand-rolled SIMD instructions, while this repository keeps that part much simpler (see the l2 function in  main.rs ). \n For optimized running:\n RUSTFLAGS="-C target-cpu=native" cargo run --release \n For downloading the datasets:\n mkdir data\ncurl ftp://ftp.irisa.fr/local/texmex/corpus/siftsmall.tar.gz | tar -xzC data\ncurl ftp://ftp.irisa.fr/local/texmex/corpus/sift.tar.gz | tar -xzC data']
iemelyanov,['xray-tracer \n Toy raytracer implementation based on "Ray Tracing In One Weekend" by Peter Shirley', 'xtiny-raytracer \n Implementation based on "Ray Tracing In One Weekend" by Peter Shirley \n', 'rs-dsa \n Implementation of some data structures and algorithms for play with Rust.', 'rs-aiosrv \n Simple async server on epoll selector', 'rslru \n Basic rust LRU', 'aof2021 \n Advent of Code 2021', 'rusty-arena \n Simple arena', 'toygc \n Toy mark-sweep GC', 'rusty-skiplist \n Rust SkipList', 'xskiplist \n C SkipList']
hgbrian,['Bayesian-Linear-Regression-Experiments \n Bayesian Linear Regression with MCMC in IPython Notebook \n View it as an\n IPython notebook \n To generate the notebook: \n ipython nbconvert Bayesian\\ Linear\\ Regression.ipynb --execute --to html --ExecutePreprocessor.timeout=3600 --stdout >BLR.html\n', 'geneticengineering \n Tools and documents for genetic engineering', 'd3 recipes \n See http://blog.megafaunasoft.com/2015/09/d3-recipes-using-aws-lambda-and.html', 'ipynb_experiments \n ipynb experiments', 'Blender US Map \n Use an SVG file to build a map of the US in Blender.\nThe Blender file is generated with a python script. \n \n', 'Running nextflow cloud on AWS \n \n Create a simple nextflow pipeline \n Run it in Docker and push the Docker image to AWS EC2 Container Service \n Run the pipeline on AWS using  nextflow cloud  and  nxf_cloud.sh \n \n 1. Running the pipeline locally \n A minimal nextflow pipeline,  blast.nf , from \n nextflow-io/examples  is included.\nThe pipeline reads in a local fasta,  sample.fa  \nand blasts it against a local protein database,  examples/pdb/tiny \n If nextflow and blastp are installed on your local computer, the pipeline can be run as follows: \n nextflow run blast.nf\n \n 2. Running the pipeline in Docker \n By running the pipeline in Docker, we can:\n(a) freeze the environment;\n(b) deploy the Docker image to remote servers.\nThe\xa0included Dockerfile is a minimal miniconda image (based on debian-jessie) \nthat just basically just installs blast. \n First, build the miniconda Docker image (you must have docker installed): \n export NXF_username="NXF_$(whoami)"\ndocker build . -t "${NXF_username}/blast"\n \n Then run  docker images  and you should see something like this: \n NXF_hgbrian/blast    latest   18d0ebc2f62e   19 hours ago   528.8 MB\n \n Then to run the pipeline with docker, nextflow.config must include the container name: \n process {\n  container = \'${NXF_username}/blast\'\n}\n \n Run and the output should be the same: \n nextflow run blast.nf -with-docker\n \n 3. Running the pipeline in Docker on AWS \n To run this pipeline on the cloud, we need to do two things:\n- move the code over to the cloud\n- move the data over to the cloud \n There are two scripts being used:\n-  nxf_cloud.sh  : a bash script to help manage the VPC, cloud, etc.\n-  nextflow.config  : the nextflow config file also includes information on nextflow\'s cloud setup \n Because these two files need to share information, \nI\'ll first set four environment variables:\n-  NXF_username     : a username for this project (e.g., NXF_hgbrian)\n-  NXF_github_repo  : the location of the code (e.g., hgbrian/nextflow_scripts)\n-  NXF_static_path  : the location of the data (e.g., /pdb, s3://${static_bucket}/pdb)\n-  NXF_out_path     : the location where results should be written (e.g., results.txt, s3://${out_bucket}/results.txt) \n I\'ll also need to set up an AWS user. These credentials must be stored somewhere:\n-  NXF_accessKey \n-  NXF_secretKey \n Create a new user to get the secret key: \n aws iam create-access-key --user-name ${NXF_username} > ${NXF_username}_credentials.txt\n \n The user must have access to EC2, EFS, s3, etc. \nFor example, administrators have all these permissions.\nCheck what groups are assigned to this user (probably none) and add to group "administrators". \n aws iam list-groups-for-user --user-name ${NXF_username} --output text\naws iam add-user-to-group --group-name administrators --user-name ${NXF_username}\n \n 3b. Set up a cluster on AWS \n Create a filesystem associated with this $username (one filesystem per user)\n    ./nfvpc.sh setup_vpc\n    ./nfvpc.sh setup_efs \n Log into ECR (create a repository for $username if it doesn\'t exist)\n    ./nfvpc.sh setup_ecr\n    #319133706199.dkr.ecr.eu-west-1.amazonaws.com/nextflowuser_repo \n 3c. Docker and ECR \n This  blast  pipeline will be run inside docker: \n docker build . -t nextflowuser/blast\n \n The docker image must be tagged with the ECR repo: \n docker tag ${NXF_username}/blast:latest ${NXF_AWS_container_id}\ndocker push ${NXF_AWS_container_id}\n \n nxf_cloud.sh \n A script that creates and/or destroys a single-use VPC on AWS for use by  nextflow cloud . \nThe idea is to be able to create a VPC to enable running a nextflow job on the cloud, \nthen tear down everything so that there are no lingering elements on AWS.  \n For this to work you need: \n \n an account on AWS \n a user with sufficient privileges (e.g., nextflowuser, a member of the Administrator group) \n a key pair for that user (e.g., generated with  ssh-keygen -t rsa -f ~/.ssh/ssh-key-nextflowuser ) \n \n Other notes: \n \n this is not tested very much, but works for me \n ami-43f49030  (preconfigured for  nextflow cloud ) is only present in the eu-west (Ireland) region\n  (see [https://www.nextflow.io/blog/2016/deploy-in-the-cloud-at-snap-of-a-finger.html]) \n if  nextflow cloud shutdown  takes too long to shut down instances then the vpc teardown\n  will not work \n \n To run \n ./nfvpc.sh setup_vpc\n./nfvpc.sh describe_vpc\n./nfvpc.sh create_nextflow_cluster\n# ssh in and run `nextflow run hello`\n./nfvpc.sh shutdown_nextflow_cluster\n./nfvpc.sh shutdown_vpc\n \n To run with EFS \n ./nfvpc.sh setup_vpc\n./nfvpc.sh setup_efs\n./nfvpc.sh setup_ecr\n./nfvpc.sh describe_vpc\n./nfvpc.sh create_nextflow_cluster\n# ssh in and run `nextflow run hello`\n./nfvpc.sh shutdown_nextflow_cluster\n./nfvpc.sh shutdown_vpc\n \n Minimal nextflow.config \n cloud {\n    userName = \'nextflowuser\'\n    keyFile = \'~/.ssh/ssh-key-nextflowuser.pub\'\n    imageId = \'ami-43f49030\'\n    instanceType = \'t2.nano\'\n    subnetId = \'subnet-xxx\'\n}\n\naws {\n    accessKey = \'yyy\'\n    secretKey = \'zzz\'\n    region = \'eu-west-1\'\n}\n', 'Internet of lab things \n See http://blog.booleanbiotech.com/iolt.html for details. \n The nodemcu firmware was compiled at https://nodemcu-build.com with the following modules (not all of these are necessary, for example,  ws2812  is for controlling LEDs): \n cjson, dht, file, gpio, http, mqtt, net, node, rtcfifo, rtcmem, rtctime, struct, tmr, uart, wifi, ws2812.\n \n The firmware was flashed using  esptool  and the following command: \n esptool.py --port=/dev/cu.wchusbserial1420 write_flash -fm=dio -fs=32m 0x00000 nodemcu-master-9-modules-2016-12-31-01-53-36-float.bin\n \n The --port parameter may be different depending on your USB serial driver. ', 'Dylos data logger \n Read data from a Dylos particle counter (using a Serial to USB cable and a raspberry pi). \n Push the data to DataStore on GCP via pubsub using a Google Cloud Function.', 'Needleman Wunsch \n A basic no-frills implementation. \n Testing Nim vs Python vs Javascript \n wget https://bitbucket.org/brentp/biostuff/raw/282b504ac9020fe1449e23f800b20b5bd7d12061/nwalign/pairwise.py -O nw_align_original.py\nsed -i s/print\\ global_align\\(sys.argv\\\\[1\\\\],\\ sys.argv\\\\[2\\\\]\\)/print\\("\\n".join(global_align\\(sys.argv\\[1\\],\\ sys.argv\\[2\\]\\)\\)\\)/ nw_align_original.py\nsed -i s/print\\ read_matrix/#print\\ read_matrix/ nw_align_original.py\nnim c -d:release nw_align.nim\nnim js -d:release nw_align.nim\necho "console.log(nw_align_js(process.argv[2], process.argv[3]));" >>nimcache/nw_align.js\npython test_nw_align.py\n', 'equibind \n git clone https://github.com/HannesStark/EquiBind\ncd EquiBind\nconda env create -f environment.yml \n proteomes \n example:\n curl -O https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000005640_9606_HUMAN_v2.tar', "limmy_blender_video \n This a  python script  /  .blend  that creates a version of a Limmy video using an image and a song.\nExample: https://youtu.be/SiC2qVCB98s \n \n To run, first download the 1080p video from\n yt5s.com-Limmy's Show - Benny Harvey R.I.P. (4K Remaster)-(1080p).mp4 \nto this folder (md5= b3b172b7eb77db9f4592c8d79c47c257 ). Note the filename must match! \n Then overwrite  limmy_tshirt.jpeg  and  limmy_song.m4a \nwith a square-ish  jpeg  and an  m4a \n( m4a  is the audio from an  mp4 ;\nit sometimes has the extension  .mp4  but it's the same so you can just rename it  .m4a ). \n Then run this (tested with Blender 3.0.0): \n blender --factory-startup -b limmy.blend -P limmy.py -x 1 -o //limmy_ -a -- -v 0.5 -a1 10 -a2 10 \n \n -x 1  = use extension \n -o //limmy_  = output name \n -a  = render all frames \n --  = everything after this is an argument to  limmy.py \n -v 0.5  = volume 50% \n -a1 10  = add 10 frames before fist pump, to time the video with the music \n -a2 10  = add 10 frames after fist pump, to time the video with the music \n \n See https://docs.blender.org/manual/en/latest/advanced/command_line/arguments.html for all Blender command line options.", 'smiles_to_properties \n input SMILES, output molecular properties \n colab: https://colab.research.google.com/drive/1rTlkzvImvyPh_gz_W6y3ipFWFh_6MnIm']
ccrjohn8787,['startup-class \n Coursera Startup Engineering class', 'Collected']
pannous,['lists', 'Netbase - Semantic Graph Database \n Netbase the Knowledge Graph of World Data, containing over 600000000 entities and statements from all the internet\'s finest sources, \nincluding  Freebase ,  WikiData ,  DBPedia ,  Yago ,  Wordnet , babelnet, ... ) \n Usage \n Online Examples \n Browse  our  Knowledge Graph , run locally\nor host as your own graph database, it is very quick to load. \n API \n From simple entities to complex queries, this is by far the fastest db of its kind.\nYou can start  experimenting  with the  API  in \n Python ,\n Ruby ,\n Java  (alpha) and\n Node.js , \n Installation \n bash\ngit clone https://github.com/pannous/netbase\ncd netbase\n./netbase :help \n After you have imported some data (see ./import/ folder),\nstart the server via  ./netbase :server \n Summary \n https://en.wikipedia.org/wiki/List_of_lists_of_lists \n Netbase is much more than a hyperfast triple store: It is a semantic inference system which has the  wordnet  ontology built-in. Semantic relations are the back-bone of netbase, so that you can quickly query with synonyms, transitive class hierarchies, part meronyms etc. \n Instead of asking about the "names of the daughters of Barack Obama" you could just search for "Obamas kids". \n Extremely fast data import is provided for all common graph/data formats including n3, rdf, owl, csv, mysql, sql and xml;\nThe same formats are available for data access and data export together with the  blueprints  standard. \n This database is already used by millions of users through the Genie/ Voice Actions  app.\nGermans might also have contact with this technology as enterprise customers of  Quasiris Recharge . Here we show the power of natural language queries to get instant answers.', 'supertoy \n Supertoy website and stuff', 'Jeannie for Chrome \n Jeannie is a siri like virtual assistant with over 3 million users in the past years!\nNow you can fully extend the functionality of your bot.\nThis project is the source code to the web client. \n Try  our demo  in the browser now!\nFor other clients and look  here . \n \n What can I ask? \n Jeannie performs all the standard tasks required by modern assistants, and much more: \n \n Who is Obama? \n Whats the weather? \n When is the next TV show for Simpsons? \n Show me an elephant \n Route from Berlin Neukölln to paris \n Remind me tomorrow at 4 pm about the dentist \n Who are you? \n Find pizza \n What time is it? \n Where am I? \n "Is there a traffic jam in Berlin?" And then: "What is the weather there?" \n ... and much more!  \n \n If you are not sure what to ask Jeannie, then click on the Examples-Tab and click some of the examples!\nMore examples are given in the API documentation below. \n Local Setup \n Just open app/index.html in a browser! Works best in  Chrome . \n Optionally: \n \n To make calls skype is necessary (your browser has to support  skype URLs ) and your contact needs to have a telephon number associated (in google contacts). Coming soon: SIP! \n Text to speech only works on our domain. You can create your own avatar on  sitepal . \n Set your own clientId and apiKey under app/js/googlehandling.js \n \n API Documentation \n The documentation for our Jeannie API is  here .\nPlease get in touch with us if you want to create your own bot or modify the responses / actions etc.\nWe had a very beautiful interface for this which is currently in private Beta. \n License \n This JavaScript project stands under the Apache License 2.0. Of course our logos and images are excluded and must not be used in your application. Additionally we are using several projects with a different license \n \n JQuery , MIT license \n jquery.dd.js , MIT license \n history.js , BSD compliant \n moment.js , MIT license \n Google APIs , Apache License 2.0 \n For some images we use  this  collection \n \n Limitations \n \n Only Chrome supports speech recognition via Google, but other browsers will work too via keyboard. \n The Google login is currently  not working  for Safari - we\'d like your pull request! \n The web client currently does not play sound actions, but this can be easily achieved with a JavaScript library. \n \n Browser Permission \n \n Allow location access to make Jeannie smarter guessing your location. Read  on mozilla docs  more about it. E.g. this will allow to answer general questions like "Whats the weather?" or location specific ones like "Where am I?". \n Allow speech recognition in Chrome to make Jeannie listening to your voice. Attention: Pages hosted on HTTPS do not need to ask repeatedly for permission, whereas HTTP hosted pages do. \n Allow popups to view intermediate results like youtube or google search.  \n Allow Jeannie to access your Google account. This is necessary for contact, call, sms and email requests but this also ensures that the same user data is used as with our  Android Jeannie app . \n', "Let colors fight over your screen canvas.        \n // POPULATION UPDATE \n // Inspired by Conway's game of life:\n//  if there is a sufficient population, then grow in numbers:\nif(b0>=100)b=b0*1.01;\n//  if the population is small then shrink\nif(b0<100)b=b0*(0.99-fr/100);\n//  if the population is too big then collapse\nif(b0>240)b=0;\n//  repopulate collapsed populations\nif(b0<3)b=120;//zb*((1.5-f)+fr*f);\n \n Updated the dot's color based on some crazy experimental whatever function.\nFeel free to wildly experiment for stunning art! \n The fun starts once you couple the different colours,\nfor example b=b0*(0.99-fr/100) == shrink blue faster when there is more green \n Uses sdl2 as canvas: http://libsdl.org/ \n Compile and run with\ngcc  -I /usr/local/include/SDL2 -lSDL2 game_of_colors.c -o game_of_colors &&\n./game_of_colors \n", "The  Diffie-Hellman key exchange  is a simple yet so far practically impeccably unhackable method to encrypt data transported between computers. Its foundation is so easy that it can be understood by any high school student. Still it is likely to be NSA proof for a couple more decades. \n This is THE example for a magical mechanism called  perfect forward secrecy , where two clients can safely communicate without any prior knowledge and without depending on a corruptible third authorization party. If you are familiar with Diffie-Hellman go-ahead and point out that some minor steps are necessary to fulfill the promise. If you are new to Diffie-Hellman don't hesitate to play through the wiki example yourself to get a feeling for this magic. \n Once the Diffie-Hellman key exchange provided both parties with a shared encryption key, it should be used with safe algorithms such as RSA 4096 bit or AES 512 bit, as recommendated by the CCC and others. Fancy 'modern' elliptic encryption algorithms don't offer much except enormous complexity and potential backdoors.", "Welcome to  Netbase   * ,\n a high-performance semantic graph database, optimized for natural language queries on static freebase data. \n Netbase supports  blueprints , a graph database standard allowing for toolchaining, making databases available to all different  tools . \n Libraries and binaries are included for Mac and Linux. \n You can feed it with any CSV XML or N3 data.\nOptimized for the quick import of Wordnet, DBPEDIA and Freebase (and \n private data  ) \n Thanks to shared memory and binary dumps you can load and process billions of nodes+statements(edges) and hundreds of gigabytes in no time. \n Requirements: \n* Lots of RAM favorable\n* For java-binding / blueprints : no requirements\n* For server: lots data favorable\n* For command line access: 'readline' lib \n sudo port install readline \n sudo yum install readline \n sudo apt-get install readline \n Playing with it: \n ./bin/netbase \n USING SHARED MEMORY\nContext#0 name:ROOT CONTEXT nodes:430338000, statements:355550000\nWarnings: 0\nNode limit: 1048576000\nStatements: 1048576000\n\nNetbase C++ Version z.a\nnetbase> ?\navailable commands\nhelp :h or ?\nload :l [force] fetch the graph from disk or mem\nimport :i [<file>]\nexport :e (all to csv)\ndelete :d <node|statement|id|name>\nsave :s or :w\nserver\nquit :q\nclear :cl\nlimit <n>\nType words, ids, or queries:\nall animals with feathers\nselect * from dogs where terrier\nall city with countrycode ZW\nPopulation of Gehren\nopposite of bad\nnetbase> import\n \n Live example queries:\n*  Episodes of South Park – Season 4 \n*  Places opened in 1974 \n*  Chemical structures \n* All african birds with orange beaks. \n Have fun! See  here for more and hosted samples .", "Trakker \n The Game \n first draft, to be modified, extended, commented, torn apart etc\nShould different aspects of the rules be tested out, or fought out? (A/B testing vs consensus vs decisions) \n The setting \n planet earth, virtually and physically (osm*simcity buildings)\ntoday, in the pasts and in the future / utopia\nthe goals: 1)survival/health 2)wisdom 3)friendship/love 3)power 3)independence 4)strength 5)fun 6) last b.n.l.: PEACE,\nall virtually and physically\n[people who die become zombies until they are resurrected] \n Storyline: \n1)your life \n2)history, past and developing \n3)made up stories [ours and crowd 'storylets'] -> tasks \n4)'near future' singularity\n5)surveillance / tracking -> escape / give in ??\nOur initial story: ... \n The rules \n people outside the realm must not be bothered,\nplayers who have logged out, or who are in peace mode must not be bothered,\nneither in the virtual world, nor in reality \n peace mode \n Players in peace mode cannot be killed, nor attack others.\nTo be in peace mode the player has to rest for n minutes, both virtually and in reality.\nPlayers who die will respawn in peace mode.\nIt takes n seconds to leave peace mode, the environment will receive a warning beforehand. \n elements of the game \n (3D) COSTUMES, to be 'invisible' in history adventures, otherwise you will be 'outed'\nDISCOVERY: find real world flowers,birds,items,... photo -> virtual inventory + achievements \n items \n items can be dropped in the virtual world, the real wor•ld or both.\nreal world items can be passive tags (qr-tags), active tags (nfc etc), real items or both.\nitems which are designated to be fixed, belonging to other players or groups should not be removed, robbery might result in penalties.\nvirtual items which are locked to physical items can only be assigned to other physical items if the following conditions are met: blah.\nvirtual items which are locked to physical items can be used virtually in the following cases: blah. example: The Lightbringer sword\nphysical items can be transferred and used virtually in the following cases: blah.  \n achievements \n physical achievements can be transferred to virtuality in the following cases: blah. \nExamples: going to the real gym -> increase virtual stamina. \nVisiting other countries/friends    -> increase virtual wisdom / experience / etc\nEveryone can create storylets and tasks, rewards can be assigned out of own 'stack' or public reward pool\nAccess to public resources increases with (virtual) karma/power. \n development \n read-only open source:  contributors will be rewarded according to shared profit license, shares in the project and community valued contributions. \n voting \n every contributor gets a vote on every subject, votes can be delegated, thus giving leaders more votes. \n https://github.com/pannous/Trakker \n differences to dayz \n http://www.youtube.com/watch?v=IVLPs9SP5ws\nhttp://www.youtube.com/watch?v=ehFUM8H24ss\nhttp://dayzdb.com/map/chernarusplus#7.067.126\n1) global world (earth)\n2) augmented reallity (Trakker app)\n3) the game should be compatible with toddlers (less shooting / in PEACE mode?) \n.... see emails \n join https://github.com/pannous/Trakker or make other suggestions ;}", '\n This is English as a programming language.\nThe main purpose of this language is to make programming accessible to many more people, more fun and to facilitate programming computers via voice. \n The guiding philosophy is to have forgiving interfaces yet strict implementations, and to make sigil special chars (!@#$%^&*[}...) completely optional. \n UPDATE: There now is a python implementation of English Script called  angle , compiling to  bytecode . \nSoon: Compilation to  WebAssembly  … and Ruby 2.3 gets bytecode as well! \n 📓 Examples \n Here are some of our favorite examples from the  tests ,  working today : \n assert two minus 1½ equals 0.5 \n beep three times \n(There will be a generation of programmers who will shake their heads that there ever was a programming language which did not interpret that sentence correctly.) \n assert square of [1,2 and 3] equals 1,4,9 \n assert 3rd word in \'hi my friend\' is \'friend\' \n x is 2; if all 0,2,4 are smaller 5 then increase x; assert x equals 3 \n beep every three seconds \n last item in \'hi\',\'you\' is equal to \'you\' \n While Peter is online on Skype\n    make a beep\n    sleep for 10 seconds\nDone \n To check if person is online on Skype:\n    Skype.checkStatus(person)\n    if result is "online": return yes \n    else return no\nEnd \n 🖥 INSTALL \n git clone --recursive git@github.com:pannous/english-script.git \n cd english-script \n ./install.sh \n 🐁 EXPERIMENT \n Warning: The project is still in alpha, don\'t expect all tests to pass! \n experiment  by typing \n ./bin/angle "6 plus six" \n ./bin/angle samples/factorial.e  or \n Start the shell :  ./bin/angle  or  rake shell \n ⦠ 6 plus six \n ⦠ beep three times \n ⦠ x is 2; if all 0,2,4 are smaller 5 then increase x \n Check out the  samples  and  tests ! \n Test \n Run the tests :  rake test \n Run an angle file:  rake run[examples/test.e] \n ⏳ In progress \n add one to every odd number in 1,2,3 == 2,2,4 \n The implicit list filter \' that \' applies a selection criterion to all elements. \n delete all files in my home folder that end with \'bak\'  translates to ruby:\n folder(:home).files.select{|that|that.end_with?("bak")}.map{|file| file.delete} \n Implicit lambda variable \' it \' \n for all mails by peter: mark it as read if its subject contains \'SPAM\'  translates to ruby:\n mails(by: Peter).each{|it| it.mark(:read) if it.subject.match(\'SPAM\')} \n The last example also illustrates what we call  matching by type name .\n To delete an email\n  move that email to the trash folder\nEnd \nHere \'mail\' acts as argument name and argument type at once.\nNo more Java style Mail mail=new Mail().getMail(); \n 📑 Language Specification \n Angle is a multi-paradigm programming language with  gradual typing . \n Read the  DOSSIER  for a more complete  language specification , vision and some background.  \n The grammar is not meant to be linguistically complete, but  functionality complete  and easily extendable. It is currently running in the \n*  ruby  and  python  environment, but will soon compile to the \n* WEB(!!) thanks to  WebAssembly \n* JVM thanks to  Mirah ,  zippy  and  truffle \n*  .Net/CLR/DLR  (via  Cecil , maybe Mirah too), \n* As a final aim: run  natively , maybe similar to  Crystal ,  Vala  or RPython \n Having a  self-hosted "bootstrapped" compiler  is an important mid-term goal. \n "Premature optimization is the root of all evil." Many programming languages \'optimize\' on the syntax level in order to optimize the resulting applications. Maybe  this  is a mistake. \n To check out the current capabilities of English Script have a look at the  tests ,\n keywords  and\n grammar \n 📰UPDATE: Since we love to compile our language to native or at least bytecode, we focussed on the  python implementation  of English script. \nFortunately finally Ruby now supports bytecode as well, since version  2.3.0 ! \n 👷 Todos \n \n Use the abstract syntax tree to compile instead of interpret (export via XML and Lisp s-expressions) \n Better (real) function argument matching: Integrate the sine curve in the interval 1 to 10 with step size .1 \n Implement event system: Beep three times whenever the disc space is over 80% \n Hook into more existing libraries (java,ifttt,rubyosa?,...) \n IntelliJ plugin \n Promote \n \n This language might soon be used in our successful beloved Jeannie assistant, which has over 5 million downloads so far:\nhttp://www.voice-actions.com \n For a background story/vision/philosophy/future of this project read the  DOSSIER  👾🏺', 'EnglishScript.tmbundle \n Textmate bundle for EnglishScript \n install by typing  \n git clone git@github.com:pannous/EnglishScript.tmbundle.git ~/Library/Application\\ Support/TextMate/Bundles/EnglishScript.tmbundle/', 'Wikipedia River Plugin for Elasticsearch \n The Wikipedia River plugin allows index wikipedia. \n In order to install the plugin, run: \n sh\nbin/plugin -install elasticsearch/elasticsearch-river-wikipedia/2.3.0 \n You need to install a version matching your Elasticsearch version: \n |       Elasticsearch    | Wikipedia River Plugin|                                                             Docs                                                                   |\n|------------------------|-------------------|------------------------------------------------------------------------------------------------------------------------------------|\n|    master              | Build from source | See below                                                                                                                          |\n|    es-1.x              | Build from source |  2.4.0-SNAPSHOT |\n|    es-1.3              |     2.3.0         |  2.3.0                   |\n|    es-1.2              |     2.2.0         |  2.2.0       |\n|    es-1.0              |     2.0.0         |  2.0.0       |\n|    es-0.90             |     1.3.0         |  1.3.0       | \n To build a  SNAPSHOT  version, you need to build it with Maven: \n bash\nmvn clean install\n~/elasticsearch/bin/plugin --remove river-wikipedia\n~/elasticsearch/bin/plugin --install river-wikipedia --url file:target/releases/elasticsearch-river-wikipedia-3.0.1-SNAPSHOT.zip \n Create river \n A simple river to index  Wikipedia  (English pages). Create it using: \n sh\ncurl -XPUT localhost:9200/_river/wiki/_meta -d \'{    "type" : "wikipedia" }\'\n    "type" : "wikipedia",\n    "wikipedia" : {\n        "url" : "file:///Users/me/dev/ai/nlp/qa/elasticsearch-river-wikiphrases/enwiki-latest-pages-articles.xml.bz2"\n    }\n}\n\' \n The default download is the latest  wikipedia dump . It can be changed using: \n javascript\n{\n    "type" : "wikipedia",\n    "wikipedia" : {\n        "url" : "url to link to wikipedia dump"\n    }\n} \n The index name defaults to the river name, and the type defaults to  page . Both can be changed in the index section: \n javascript\n{\n    "type" : "wikipedia",\n    "index" : {\n        "index" : "my_index",\n        "type" : "my_type"\n    }\n} \n Since 1.3.0, by default,  bulk  size is  100 . A bulk is flushed every  5s . Number of concurrent requests allowed to be executed is 1.\nYou can modify those settings within index section: \n javascript\n{\n    "type" : "wikipedia",\n    "index" : {\n        "index" : "my_index",\n        "type" : "my_type",\n        "bulk_size" : 1000,\n        "flush_interval" : "1s",\n        "max_concurrent_bulk" : 3\n    }\n} \n Mapping \n By default, wikipedia river will generate the following mapping: \n javascript\n{\n   "page": {\n      "properties": {\n         "category": {\n            "type": "string"\n         },\n         "disambiguation": {\n            "type": "boolean"\n         },\n         "link": {\n            "type": "string"\n         },\n         "redirect": {\n            "type": "boolean"\n         },\n         "redirect_page": {\n            "type": "string"\n         },\n         "special": {\n            "type": "boolean"\n         },\n         "stub": {\n            "type": "boolean"\n         },\n         "text": {\n            "type": "string"\n         },\n         "title": {\n            "type": "string"\n         }\n      }\n   }\n} \n License \n This software is licensed under the Apache 2 license, quoted below.\n\nCopyright 2009-2014 Elasticsearch <http://www.elasticsearch.org>\n\nLicensed under the Apache License, Version 2.0 (the "License"); you may not\nuse this file except in compliance with the License. You may obtain a copy of\nthe License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an "AS IS" BASIS, WITHOUT\nWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\nLicense for the specific language governing permissions and limitations under\nthe License.\n', 'caffe-ocr \n OCR with the  caffe  deep learning framework \n DEPRECATED -> Moved to https://github.com/pannous/tensorflow-ocr \n use script to generate more training data, or\nget more training data from here:\nhttp://ufldl.stanford.edu/housenumbers/ \n Also see https://github.com/mateogianolio/mlp-character-recognition for simple js example and \nhttps://github.com/tmbdev/ocropy for a very comprehensive phython toolchain.', "Speech Recognition with BVLC caffe \n Speech Recognition with the  caffe  deep learning framework \n UPDATE: We are migrating to  tensorflow \n This project is quite fresh and only the first of three milestones is accomplished: \nEven now it might be useful if you just want to train a handful of commands/options (1,2,3..yes/no/cancel/...) \n 1)  training spoken  numbers :\n  * get spectogram training images from http://pannous.net/spoken_numbers.tar (470 MB)\n  * start ./train.sh\n  * test with  ipython notebook test-speech-recognition.ipynb \n    or  caffe test ...  or  <caffe-root>/python/classify.py \n  * 99% accuracy, nice!\n  * online recognition and learning with  ./recognition-server.py  and  ./record.py  scripts \n \n Sample spectrogram, Karen uttering 'zero' with 160 words per minute. \n 2) training  words :\n * 4GB of training  data \n * net topology: work in progress ...\n * todo: use  upcoming new  caffe  LSTM  layers etc\n * UPDATE  LSTMs get rolling ,  still not merged \n * UPDATE since the caffe project leaders have a hindering merging policy and this pull request was shifted many times without ever being merged, we are migrating to  tensorflow \n * todo: add extra categories for a) silence b) common noises like typing, achoo c) ALL other noises \n 3) training  speech :\n * todo!\n * 100GB of training data here: http://www.openslr.org/12/\n *  TIMIT dataset  $27,000.00 membership fee or  $250 for non-members + $2400 under research-only license ?\n * combine with google n-grams \n Theoretical background:  papers \n A. Graves and N. Jaitly. Towards end-to-end speech recognition with recurrent neural networks.  In ICML, 2014 \n O. Vinyals, S. V. Ravuri, and D. Povey. Revisiting recurrent neural networks for robust ASR.  In ICASSP, 2012 \n Andrew Ng et al  /  Baidu \n Hinton et al / Toronto \n good old Hinton \n Schmidhuber et al  using new 'ClockWork-RNNs' \n The  book :\n Automatic Speech Recognition: A Deep Learning Approach   (Signals and Communication Technology) Hardcover – November 11, 2014 by Dong Yu (Author) and Li Deng (Author)  \n Related work \n Also see the  Kaldi  project, which seems a bit messy but already uses deep learning with  LSTM \nAnother experimental LSTM network, which works out-of-the-box:  Currennt", "xipher 🔐 \n A very simple yet powerful  XOR cipher  scheme \n With a 'key' file containing truly random noise, this encryption is unbreakable even in theory. \n The  XOR cipher  is so easy, every computer science student should be able to implement this cipher by heart in a couple of lines! If it is 100% secure and incredibly easy to implement, why isn't the whole world using it? \n It is in fact used all over the world albeit as part of more complicated cypher schemes.\nThe problem with the naïve application of xor is that you lose perfect security if you apply the key too often (more than once).\nAlso for big files you need to have big keys, to avoid or minimize repetition. \n This project contains a very simple Algorithm to mitigate the above limitations. \n In the age of the slow Internet connections, exchanging big keys with your trustees was impractical,\nunless you provided your friends with the key on physical devices. These days you can just create a key which is a couple of megabytes in size and give it to your communication partners, preferably/necessarily using a secure channel/medium like a CD**. \n Here is how it works: \n In your Mac, Linux or Windows shell type: \n git clone https://github.com/pannous/xipher.git \n cd xipher \n make \n ./bin/random > key \n ./bin/encrypt README.md key > encrypted \n ./bin/decrypt encrypted key > README.yay \n Why don't we just use the existing encryption mechanisms? \n Number one:  trust \n Do you really feel secure if you use an encryption scheme which you don't understand?\nEven if you have access to the source code: Do you really understand everything that happens in all steps?\nAre you sure that there is no backdoor somewhere? \n With this project you can be perfectly sure:\nLook at the simple source code, understand it and compile yourself.\nAll you need is a truly a random key.\nWhat if you don't trust the random key generator on your computer?\nJust xor the generated key with some other files.\nIf you combine randomness with noise and chaos, you get almost/practically perfect randomness. \n Number two:  fun and insight \n Sometimes the topic of encryption can get overwhelming even for people with mathematical background.\nUnderstanding the XOR operation couldn't be any simpler and writing your own encryption feels very empowering. \n Number three:  simplicity \nThis scheme is arguably even simpler then the usual ssh-keygen/openssl approach.\nYou and your friends can start right away with any arbitrary file as key to get reasonable encryption:\n ./bin/xor README.md any_random_file > reasonably_encrypted \n ./bin/xor key any_random_file > practically_perfect_key \n Extra: speed\nOnce the key is generated, the encryption and decryption runs in  linear time . \n CAUTION: In its current implementation once the master key is stolen* somehow, then all files encrypted with it can be deciphered.\nThis can easily be mitigated by using several keys, but a better approach is desirable.\nIf your key is too small or if you are using it too often, you may be reducing security.\nHowever for a key of significant size (i.e. 4GB) it is almost infinitely more likely that the key will get stolen then being reverse-engineered. \n NOTE: You can increase security significantly if you xor/encrypt zipped files, as they already contain very little structure! \n OUTLOOK: This encryption can also be used locally for your own files, if you put the key on a USB stick. And it can be used in the future for peer-to-peer communication applications. \n PS: * Ideally you would have a little offline device which encrypts every file/keystroke that you make before it reaches your computer. \n PS3: ** You can even securly share the key over the internet using  perfect forward encryption , for example through extensions of  Diffie Hellman . If you trust ssh/scp/sftp these might be appropriate as well. \n 💡", 'Cast - Canonical AST \n Canonical AST, the only Abstract Syntax Tree you need, with importers+exporters to all languages (planned) \n Currently used by  English script ,  Angle language  and as a Ruby to python converter. See ast-export.rb and ast_import.py ! Todo: cleanup, improve, write connectors for jvm/java clr/csharp \n AST Treenode specification: see XML/ XSD ,  kast.py  and  kast.rb , inspired and mostly based on  pythons ast !', '\n Angle  is the Python implementation of  English  as a programming language. Since Angle compiles to Python bytecode, it is can be used as a drop-in-replacement for classic Python and is fully debuggable, even in PyCharm. \nIt is currently in development to be run directly in WASM via  wasp . \n The main purpose of this language is to facilitate programming computers via voice.\n Angle  is the first speakable programing language and thus makes programing accessible to many more people. \n 🖥 INSTALL \n pip install angle \n  `pip install anglang`  \n angle examples \n Or from source: \n git clone --recursive git@github.com:pannous/angle.git \n cd angle; ./install.sh \n Start the shell :  ./bin/angle   \n 📓 Examples \n Here are some of our favorite working examples from the  tests : \n assert two minus 1½ equals 0.5 \n beep three times \n(There will be a generation of programmers who will shake their heads that there ever was a programming language which did not interpret that sentence correctly.) \n assert square of [1,2 and 3] equals 1,4,9 \n assert 3rd word in \'hi my friend\' is \'friend\' \n x is 2; if all 0,2,4 are smaller 5 then increase x; assert x equals 3 \n beep every three seconds \n last item in \'hi\',\'you\' is equal to \'you\' \n While Peter is online on Skype\n    make a beep\n    sleep for 10 seconds\nDone \nDespite being a  natural  language,  angle  has brevity and readabilty as its highest goals. Verbosity is optional, as are types and sigils. \n To check if person is online on Skype:\n    Skype.checkStatus(person)\n    if result is "online": return yes \n    else return no\nEnd \n Function pointers and aliases\n with class map: to merge is to update \n Status: \n Beauty is more important in computing than anywhere else in technology because software is so complicated. Beauty is the ultimate defence against complexity. \n     — David Gelernter\n \n ALPHA, partly usable, some  tests  not yet passing: \n \n Operators: \n \n \n |  pipe : output of last command as input for next command.  ls ~ | sort \n ,  list : turn two nodes into a list. Append if one is a list. \'cons\' in lisp \n :  pair : turn two nodes into a pair,  a:3  (hashed+symbolic). almost identical to: \n =  value : turn two nodes into a variable pair,  a=3 \n ;  list : end expressions/statements, list inside a block. if 1 : 0 ; \n ., of, in  selection: garden of house == house.garden \n  space acts as comma in lists \n  newline, dedent: acts as comma in lists or closing \'bracket\' if matching block start \n \n usual math operators  add   plus   times   ^  … and logical  and   or   xor   not \n brackets: content of () is evaluated, {} is deferred (block/lambda) \n angle uses  mark  as data and code format: \n cat{\n    size:3\n    color:{r=1 g=0 b=0}\n    form{\n        dimensions=(3,size*2)\n    }\n} \nequivalent to\n```\na cat\n   size is 3\n   color is 1 for red, 0 for green and blue\n   a form\n     dimensions are 3 , size * 2\n   end\nend cat \n All code is data and all data can be \'executed\': \ncat().dimensions returns (3,6) because last statement == return value\ncat(!) returns cat fully evaluated: cat{size:3,…,form:dimensions:{3,6}} \n print(size) // prints value of size()\nprint{size} // prints function size\ncolors={red green blue}\ncolors=(red green blue) \n sort{.size} // ok\nsort{it.size} // ok\nsort{it\'s size} // ok\nsort(size) // warn unless value of size() returns lambda\nsort{size} // todo: read as it.size\nsort by size  // todo \n [] is evaluated as property index/match or deferred if assigned:\ncat[size] = 3\ncat[size:3] = true\npattern=[size:3]\ncat[pattern] = true\ndifference to cat.size : in cat[size], size can be a variable. to be sure use symbol or string cat[#size] cat[\'size\'] \n switch takes a usual hash in which keys can be patterns:\nswitch :: a -> { b -> c } -> c()\nswitch(time){\n    5pm: read\n    [hour<5am]: sleep\n    [it.minute=0]: smoke\n    other: work\n}\nswitch(time,my_block)\nmy_block[time()] \n fallthrough must be forced with … if desired \n how to force evaluation inside deferred block:\ncat{\n    born=time()  // instant\n    born:=time()  // deferred\n    born:time()  // deferred \n } \n blocks can be given \'arguments\' when evaluated:\ncat(time:5pm) == cat{born:5pm}\nsame rules apply: arguments can be values or blocks\ncat(time:calculate()) == cat{born:calculate()}\ncat(time=calculate()) == cat{born:5pm} \n ``` \n ⏳ In progress \n  `add one to every odd number in 1,2,3 == 2,2,4`  \n Angles implicit list filter \' that \' applies a selection criterion to all elements: \n delete all files in my home folder that end with \'bak\'   \n translates to ruby: \n folder(:home).files.select{|that|that.end_with?("bak")}.each{|file| file.delete} \n Implicit lambda variable \' it \'  \n for all mails by peter: mark it as read if its subject contains \'SPAM\'   \n translates to ruby: \n mails(by: Peter).each{|it| it.mark(:read) if it.subject.match(\'SPAM\')} \n The last example also illustrates what we call  matching by type name .\n To delete mail:\n  move that mail to trash folder\nEnd \n count char in "שָלוֹם" = 4\ncount byte in "שָלוֹם" = 12\ncount codepoints in "שָלוֹם" = 6 \nas loop:\n for char in "שָׁלוֹם ": print char.clean\nשלום \n Here \'mail\' acts as argument name and argument type at once.\nNo more Java style Mail mail=new Mail().getMail(); \n  Self documenting code is not about the "how", it\'s about the "what". Ex: A method name should be FilterOutOddNumbers(). Not MapModulo2Predicate().  \n 🐁 EXPERIMENT \n Run it and see yourself! \n experiment  by typing \n angle "6 plus six" \n angle examples/test.e \n angle  (no args to start the shell) \n ⦠ 1/4 \n ⦠ 6 plus six \n ⦠ beep three times \n ⦠ x is 2; if all 0,2,4 are smaller 5 then increase x \n ⦠ ls | item 2 \n 📑 Language Specification \n Angle is a multi-paradigm programming language with  gradual typing . \n Read the  DOSSIER  for a more complete  language specification , vision and some background.  \n The grammar is not meant to be linguistically complete, but  functionality complete  and easily extendable.\n"Premature optimization is the root of all evil." Many programming languages \'optimize\' on the syntax level in order to optimize the resulting applications. Maybe  this  is a mistake. \n To check out the current capabilities of English Script have a look at the  tests ,\n keywords  and\n grammar \n 🕶 Future \n English Script / Angle is currently running in the \n*  ruby  and  python  environment, but will soon compile to the \n* WEB and  natively  thanks to  WebAssembly \n* Pure  JavaScript  version as intermediate. \n Having a  self-hosted "bootstrapped" compiler  is an important mid-term goal. \n \n**precedence**\nOne very hot idea is to allow modifying the language grammar on the fly, at least to a limited extend.\nThis goes beyond normal meta programming, macros and #defines\nOne first step would be to enable setting the precedence of functions.\nThis would yield very natural and sweet mathematical expressions, especially combined with Unicode names:\n```\nclass Complex alias ℂ (re, im)\n    to add number x\n        ℂ(this.real+x.real, this.im+x.im)\n    end\n    alias \'+\' = add\nend \nℂ.add.precedence=Number.add.precedence-1\nī := √-1\nī + 3ī == 4ī\n```\nThis would run against the goal to avoid sigil special chars though.\n \n Why the new implementation in python🐍? \n We can  compile  English script /  Angle  directly to python byte-code:\nAs opposed to Ruby, Python(3) comes with a very nice and clean abstract syntax tree as well as byte code capabilities preinstalled.\nCompiling is so much nicer & faster than interpreted code.\nAlso the Python execution model is a bit more friendly than the Ruby VM, but both have their  advantages and drawbacks . The biggest advantage of Python is that objects can be given attributes at any time o.x=\'y\'! However pythons limited block/lamda capabilities are a painful limitation.  \n "There should be one -- and preferably only one -- obvious way to do it"\nBeautiful is better than ugly.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.          \n For a background story/vision/philosophy/future of this project read the  DOSSIER \n Also check out:  Program Synthesis from Natural Language\nUsing Recurrent Neural Networks \n  \nTrying to express implicit and fuzzy relationships in ways that are explicit and sharp doesn’t clarify the meaning, it destroys it.\n\n        — Clay Shirky [http://www.shirky.com/writings/semantic_syllogism.html] \n\nHaving a language that almost looks like English is actually harder to program in than one that has a well defined strict syntax.\n\nIt falls into a sort of uncanny valley. Apple tried that sort of successfully with Hypertalk but really badly with Appkescript        ', 'English script  samples for the  angle  programming language', "WebAssembly \n The new bytecode for the internet \n You are probably looking for  webassembly  on github.\nOr on  wiki  or on  HackerNews \n The whole purpose of this repository is to save you one more Google search;\n  it can be removed once github creates an alias webasm->webassembly ;) \n Play an  impressive demo  of native wasm in your browser now.\nOr a new  even more impressive demo  if you have a fast machine. \n Check out a  wasm Hello World \n https://github.com/mbasso/awesome-wasm \n https://github.com/appcypher/awesome-wasm-langs \n To generate binary wasm yourself, see  binaryen ,  wabt  or  emscripten . Soon you can just use your default clang to compile c to wasm (right now you need the latest versions of llvm/lcc to do so)! \n WebAssembly is  not  a binary bytecode format for standard js. \nThere are reasons against overly specialized  bytecode . However all browsers could and should come with a compressed javascript AST reader. Someone please specify the 'jast' file format. \n Compile  Ruby ,  Python , C++ -> LLVM ->  asm.js  -> binary.wasm and load it with native speed in current(!) browsers, or in old browsers via polyfill.", 'tensor-caffe \n TensorFlow graph importer from caffe prototxt  \n TODO pre-alpha version 0.0000\n100% Work in progress!!\ndo not try to use yet \n See https://github.com/ethereon/caffe-tensorflow for orthogonal approach', "Tensorflow Speech Recognition \n Speech recognition using google's  tensorflow  deep learning framework,  sequence-to-sequence  neural networks. \n Replaces  caffe-speech-recognition , see there for some background. \n Update  Mozilla  released  DeepSpeech \n They achieve good  error rates . Free Speech is in good hands, go  there  if you are an end user.\nFor now  this  project is only maintained for educational purposes. \n Ultimate goal \n Create a decent standalone speech recognition for Linux etc.\nSome people say we have the models but not enough training data.\nWe disagree: There is plenty of training data (100GB  here  and 21GB  here on openslr.org  , synthetic Text to Speech snippets, Movies with transcripts, Gutenberg, YouTube with captions etc etc) we just need a simple yet powerful model. It's only a question of time... \n \n Sample spectrogram, Karen uttering 'zero' with 160 words per minute. \n Installation \n clone code \n git clone https://github.com/pannous/tensorflow-speech-recognition\ncd tensorflow-speech-recognition\ngit clone https://github.com/pannous/layer.git\ngit clone https://github.com/pannous/tensorpeers.git \n pyaudio \n requirements portaudio from http://www.portaudio.com/ \n git clone  https://git.assembla.com/portaudio.git\n./configure --prefix=/path/to/your/local\nmake\nmake install\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/path/to/your/local/lib\nexport LIDRARY_PATH=$LIBRARY_PATH:/path/to/your/local/lib\nexport CPATH=$CPATH:/path/to/your/local/include\nsource ~/.bashrc \n install pyaudio \n pip install pyaudio \n Getting started \n Toy examples:\n ./number_classifier_tflearn.py \n ./speaker_classifier_tflearn.py \n Some less trivial architectures:\n ./densenet_layer.py \n Later:\n ./train.sh \n ./record.py \n \n  ╮⚆ᴥ⚆╭  \n Update: Nervana  demonstrated  that it is possible for 'independents' to build speech recognizers that are state of the art.  \n  ᖗ*﹏*ᖘ  \n Fun tasks for newcomers \n \n Watch video : https://www.youtube.com/watch?v=u9FPqkuoEJ8 \n Understand and correct the corresponding code:  lstm-tflearn.py   \n Data Augmentation :  create on-the-fly modulation of the data: increase the speech frequency, add background noise, alter the pitch etc,... \n \n  ᕮ◔‿◔ᕭ  \n Extensions \n Extensions  to current tensorflow which are probably needed:\n*  WarpCTC on the GPU  see  issue \n* Incremental collaborative snapshots (' P2P learning ') !\n* Modular graphs/models + persistance \n  ⤜(⨱ᴥ⨱)⤏  \n Even though this project is far from finished we hope it gives you some starting points. \n Looking for a tensorflow collaboration / consultant / deep learning contractor? Reach out to  info@pannous.com \n \n Notes\nSTT https://github.com/sotelo/parrot/blob/master/model.py t\n parrot\n\n", "Karpathy neuralnets python code \n Python code accompanying Andrej Karpathy's great  Hacker's guide to Neural Networks \n Please check out this great beginners tutorial if you haven't already. \n This code is only for reference, one day in your life you have to write it yourself to really dig/own it. \n Some but not all of the examples make use of the http://tensorflow.org/ framework for experimentation/demonstration purposes.\nFor pure python code see  linear_regression.py, Backprop_Ninja.py,  nn_4neurons.py or  here .", 'Lyri \n Meet Lyri, your Siri-like assistant for your Terminal command line.', 'node-netbase \n node.js module for  netbase : a semantic Graph Database with wordnet, wikidata, freebase, csv, xml, ... import and export facilities. \n Installation \n stable release: \n sudo npm install -g netbase \n install development head: \n sudo npm install -g git://github.com/pannous/node-netbase \n Usage \n ```javascript\nnetbase = require(\'netbase\'), \n netbase.setGerman()\nmerkel=netbase.get("Angela Merkel")\nassert(merkel.Vorname="Angela") \n show = netbase.show,\nquery = netbase.query; \n netbase.import(\'wordnet\')\nresult=netbase.query(\'opposite of bad\')\nshow(result)\n```', 'FreeStore \n The alternative to the walled garden App Store for macOS, with free and paid apps.\nFree as in liberty, the greatest goods of civilization.\n Browse store \n Upload app \n We hate political walls. Walled garden App Stores are no exception.\n \nWe love tearing down walls.', 'tensorflow-ocr \n 🖺 OCR using tensorflow with attention, batteries included \n Installation \n ```\ngit clone --recursive http://github.com/pannous/tensorflow-ocr \n sudo apt install python3-pip \n cd tensorflow-ocr\npip install -r requirements.txt\n``` \n Evaluation \n You can detect the text under your mouse pointer with \n mouse_prediction.py \n it takes 10 seconds to load the network and startup, then it should return multiple results per second\n.  \n text_recognizer.py \n To combine our approach with real world images we forked the  EAST  boundary boxing. \n Customized training \n To get started with a minimal example similar to the famous MNIST try\n ./train_letters.py  ;\nIt automatically generates letters for all different font types from your computer in all different shapes and trains on it. \n For the full model used in the demo start  ./train.py', 'layer \n Internal tensorflow custom convenience comfort wrapper. \n so far used in \n* https://github.com/pannous/tensorflow-ocr \n* https://github.com/pannous/tensorflow-speech-recognition/\n* https://github.com/pannous/angle', "Tensorpeers \n P2P peer-to-peer training of deep learning  tensorflow  models \n Not to be confused with  locally distributed training ,\nHowever presumably a lot can be learned from tf.train.Supervisor etc \n Community Power \n In the Golden age of deep learning, baidu and others have shown that training time can scale almost linearly with the number of GPUs.\nThis gives large corporations an advantage over startups in the run for the best A.I. systems ... until now. \n Tensorpeers will empower the community to combine their efforts and GPU time into quickly converging wonderful models. \n Architecture \n The architecture has to be slightly different from existing 'parameter server' schemes, because of relatively slow Internet connections. However our optimistic guess is that this won't hinder the success of this project: as long as we find any  merging scheme, which successfully combines the  gained knowledge  of two separate runs, we should be fine. \n To speed things up, we base this project on python-libtorrent. \n Install dependency: \n MAC:\n brew install libtorrent-rasterbar --with-python \nLINUX:\n apt-get install python-libtorrent  or\n apt-get install python3-libtorrent \n Open questions \n This is a wildly wide open research area, so if you want to make the world a better place (and or need a PhD thesis):\nHerewith you have full leverage. \n \nThis is a wildly wide open research area, so if you want to make the world a better place (and or need a PhD thesis):\nHerewith you have full leverage.\n\nNot to be confused with [exxact p2p deep-learning](https://exxactcorp.com/deep-learning-p2p.php)", 'shapenet \n Deep Learning network learning geometric shapes', 'netbase-python', 'netbase-ruby', 'WasmFiddle \n This repository contains the WasmFiddle website source code. \n Running your own local copy of the website \n To run a local copy, you will need to install node.js and webpack on your computer, then run the following commands: \n npm install\nnpm install --dev \n To build WasmFiddle whenever a file changes run: \n npm run build-watch \n To start a dev web server run: \n webpack-dev-server', 'Angle script  is the browser variant of the  angle  programming language, interpreting a modern syntax in javascript, compiling to native webassembly. \n Angle is  optionally speakable : \n assert two minus 1½ equals 0.5 \n Angle is  optionally typed : \n x=1;number y=π²;int y=2; assert x,y,z are numbers \n string s;s=3;s=="3"  save, because int x=cast "3" or throw \n Angle has  semantic indexing : \n assert 3rd word in \'hi my friend\' is \'friend\' \n Angle does what we call  matching by type name \n To delete mail:\n  move that mail to trash folder\nend \nHere \'mail\' acts as argument name and argument type at once. \nNo more Java style  Mail mail=new Mail().getMail();  never again with angle!   \n Angle has  contextual keywords   it  …:\n for all mails by peter: \n   if its subject contains \'SPAM\':\n      mark it as read \nthe last example translates to ruby: \n mails(by: Peter).each{|it| it.mark(:read) if it.subject.match(\'SPAM\')} \n Angle uses  mark  as data and code format:\n cat{\n    size:3\n    color:{r=1 g=0 b=0}\n    form{\n        dimensions = (3,size*2)\n    }\n    eat(){\n      size++\n    }\n    hunger{\n      strike=true #local variable is basically key:value pair\n      if(strike)\n        size--\n    }\n} \nAll code is data and all data can be \'executed\':\n cat.eat().dimensions is (3,8)  normal\n cat.size=5; cat.dimensions=(3,size*2)  bound but not yet evaluated\n cat().dimensions == cat.dimensions() is (3,10)  innovation \n classes are maps of keys/names to properties/fields/functions/variable-references!\nsymbols are names to references\nmaps are lists of pairs (duples), implicit triples: (index,name?,value?)\nlists are maps of index to value\ng = “I\'m a global”\nhash={\n  1 to \'a\'\n  2 : \'b\' # careful with ints as keys!\n  var c = \'3\' \n  d = \'4\' # no need for \'var\': keys are local variables by default\n  global e = 5 # relatively ugly by design\n  set f = 6\n  g = 7 # hmmm ...\n  if(g>6)\n    kill! #only invoked when hash! is invoked\n  8:return \'blocks are just maps of (hidden) line-numbers to statements\n  goto 2 # oh yes;) \n }\nhash()\ne == 5\nf   // error, not predefined\ng == 7  // now ok, but before?? \n internal block representation of hash == [\n(0,1,\'a\')\n(1,2,\'b\')\n(2,c,\'3\')\n(3,d,\'4\')\n(4,e,5) # where are modifiers? global(e) wrapper?\n(5,ø,set(f,6))\n(6,ø,if(g>6,kill!))\n(7,8,return)\n]\nhash[2]==hash.c==\'3\' \n constant  data is evaluated (\'reduced\') immediately. \n These are the basics, some future refinements can be read [[later.md]] \n Speakable lisp with less parentheses, brackets and optional sigils  [({;$})] \ninspired by  XTalk \n everything acts is an expressions\neverything acts is an object \n unified architecture combines blocks and lists and maps: \n blocks  {x+y,x>1}   {1,2,3}  are evaluated lazily \n lists or tuples are evaluated immediately  x=2;[1,2,x+x]==[1,2,4] \n blocks are evaluated when used   x=2;{1,2,x+x}==[1,2,4] \n Why? simplicity is good in itself \n maps look different to JavaScript!\n (a:1,b:2,c:3)\n(a=1,b=2,c=3)\n{a:1,b:2,c:3}  // evaluated lazily, as ”code“!\n(a,b,c)==(0:a,1:b,2:c) // like js \'objects\'\n(a,b,c)==[a,b,c] iff a,b,c are constant \n blocks can be applied to blocks or evaluated via \'argument\' maps:\n```\na={x+x}\nb={x=2}\nc=(x=2) \n a(b)=4\na(c)=4\na(x=2)=4\na{x=2}=4\n blocks can be defined via \':=\' \na:=x+x\nb={x+x}\na == b\n when applying blocks all variables must be bound via context or arguments: \na(y=2) undefind / error unless x is available in the context\na(2) undefind / error unless \'it\' keyword in block\na[2] acts as selector, not as evaluator !\n``` \n the arguments of the cooling block can be accessed by name or number or \'it\'\n {print b}(a:1,b:2) == 2\n{print $b}(a:1,b:2) == 2\n{print $1}(\'a\',\'b\') == \'b\'\n{print it*2}(2) == 4 \n methods get evaluated immediately without brackets\n to test:\n    print \'ok\'\ntest == \'ok\' \n methods can be (de)referenced via proceeding \'to\' keyword, similar to #symbol in ruby or method.name in python\n my_result=test // \'ok\', invoked\nmy_funk=to test // the method \nmethods are passed as symbols to functions that expect acts or blocks\n bigger = { $1 > $0 }\nto sort array by block{…}\nsort([3,2,1],bigger) \n to act in number n seconds:\n    sleep n\n    act \n what is the difference between functions, blocks, methods and acts? None really, just different names for synonyms. \n keep in mind that blocks can be parameterized with arguments (see above). those are usually called methods or functions but that\'s just convention. \n one nice way to think about \'blocks\' is that they are blocking execution until needed:\n```\nx=2\na=x+x // eval on the spot\nb:=x+x\nc={x+x}\nd={x+x}() \n a == 4\nb == ${x+x} == 4  // soft symbol\nc == {x+x} // symbolical\nc() == 4\nd == 4 \n x=3\nb == 6 // eval on the spot\nc() == 6\n``` \n e=(x=5){x+x}\ne == 10\ne(1) == 2 \noptional arguments give strictness to code\n f=(){x+x}  error x is neither an argument nor a global\nf2(){x+x}  error x is neither an argument nor a global\ng={x+x}  // ok for now \nall objects can act as classes (like js, really?)\n```\nx={a:1}\ny extends x\ny.a == 1\ny.b = 2 \n Circle={number radius,area:radius*radius}\nCircle(radius=3).area == 9 # no \'new\' \n ``` \n Preferably \'=\' should be used for leaves and \':\' for nodes with children.\ncolor=blue color:{blue dark}\nAs in js, even lists and entity/maps can be unified:\n [a b c]={1:a 2:b 3:c}\n{a:1 b:2 c:3}=[a:1 b:2 c:3] // todo hash as list of pairs as in kotlin\n{a b c}=[a b c] \nA list is just a special entity/map in which its keys are numbers\nA map is just a special list in which its entries are pairs.', "Hieros \n Human Origins \n Raw material and  anthology  for Egyptian and “international” hieroglyphics. \n To see the hieroglyphs you may need to install a  font , unless you can alredy see this hieroglyph '𓅭' as a duck. \n Then you can head over to the  ARTICLE ,  wiki  or  reddit .   \n You can search the Egytian dictionary using the  hieroglyph keyboard . \n The Cauchy-Schwarz Inequality \n$$\\left( \\sum_{k=1}^n a_k b_k \\right)^2 \\leq \\left( \\sum_{k=1}^n a_k^2 \\right) \\left( \\sum_{k=1}^n b_k^2 \\right)$$", 'North Korean concentration camps \n According to reports from Amnesty International and the U.S. Committee for Human Rights in North Korea, by 2017 an estimated 200,000 prisoners are incarcerated in camps and subjected to forced labor, physical abuse, execution and human experimentation.  North Korean Prisons Are Worse Than Nazi Concentration Camps, Says Holocaust Survivor   \n How can I help to close Korean concentration camps? \n \n Spread the  evidence ,  write  to newspapers, governors, friends \n Let us  keep this issue on the frontpage  until it is resolved \n Find  organizations  dedicated to the task and  support  them with words, money or work \n inform  everyone about one of the biggest half-secret humanitarian desasters of all time \n Face your fears. Some who investigated these matters lost sleep to nightmares. \n Standing up against repression might be detrimental to your journalistic or political career. Show civil  courage .  \n Fork  this repository and keep reminding me, yourself and others about this shameful tragedy \n', '🐝 Wasp : Wasm Programming Language \n Wasp  is a new unified notation for both markup/object data and code. \n Wasp is the foundation layer of the higher order programming language  angle . \n «Data is Code and Code is Data» \n Wasp Syntax \n For example, a HTML registration form: \n ```html \n \n comment \n \n Email address: \n \n \n Submit \n \n Can be represented in Wasp as: text\nform{                                \n  //comment                         \n  div{ class:"form-group"            \n    label{ for:email                 \n      "Email address:"               \n    }\n    input{ type:email id:email}    \n  }\n  button{ class:[\'btn\' \'btn-info\'] \n    \'Submit\'                       \n  }\n}\n``` \n The wasp runtime is available as\n* native  binary  for Mac, Linux and Windows\n* small standalone  webassembly  file (~50kb), with or without \n  a wasm  compiler * (~100kb) in wasm: \n Angle Language \n Angle  is a new Programming Language using Wasp as data format, \n"Lisp with Hashes" and optional braces. \n Hello World in Angle is \n "Hello World"   \n The deep formal reason why this is a valid program is that the last object in a block is its return value \nand the last result in the root block of the main file is printed. \n Fibonacci in Angle is\n fibonacci number = if number<2 : 1 else fibonacci(number - 1) + fibonacci it - 2 \n Note how number simulataneously acts as type and variable name. \nNote how the unique argument  number  can be accessed via  it  keyword and some brackets are optional \n Auto typed  fibonacci in Angle is\n fib := if it<2 : 1 else fib(it-1) + fib it - 2 \n Angle runs as wasm file inside browsers an as small lambdas in edge computing.\nAngle programms  compiled to wasm , which dont make use of the standard api can be extremely small <1kb, just like handwritten  wast  but with much more comfort. \n Wasp and Angle are  free of dependencies  (other than gcc, if the (wasm) runtime is to be compiled from scratch).\nOnly some optional features can make use of external dependencies: \n The natives runtimes can be configured to ship with a JIT wasm runtime (wasm3, wasmer, wasm-micro-runtime) \n The  native  Wasp/Angle binary contains a small  WebView  connector making use of the host\'s browser component (Edge or WebKit).\nSince the Angle language can be compiled from wasm to wasm, this gives a whole self sufficient programming environment in less than 200kb, \nwith similar power to  electron  and QT. \n Note: The full wasp_compiler.wasm is currently 2MB but can be shrunk back close to the 70kb of wasp_runtime.wasm once the lazy external references are removed\nagain. \n Until a smart way is found to write directly to the WebViews canvas, angle ships with a low\noverhead  graphics  adapter (using SDL) for fast fullscreen painting in native wasp bundles. \n Documentation \n \n Syntax specification \n Features \n Inventions \n Examples \n \n Wasp Data Model \n Wasp , is a new unified notation for both object and markup data. The notation is a superset of what can be represented by JSON, HTML and XML, but overcomes many limitations these popular data formats, yet still having a very clean syntax and simple data model. \n \n It has  clean syntax  with  fully-type  data model  (like JSON or even better) \n It is  generic  and  extensible   (like XML or even better) \n It has built-in  mixed content  support  (like HTML5 or even better) \n It supports  high-order  composition  (like S-expression or even better) \n \n |                        |  Wasp / Mark                            | JSON     | HTML | XML                            | S-expr                             | YAML                                  |\n| ---------------------- | ------------------------------ | -------- | ---- | ------------------------------ | ---------------------------------- | ------------------------------------- |\n| Clean syntax           | yes | yes| no   | yes | yes| yes|\n| Fully-typed            | yes | yes| no   | no| yes| yes |\n| Generic                | yes | yes| no   | yes | yes| yes |\n| Mixed content support  | yes | hard     | yes | yes | hard | hard                                  |\n| High-order composition | yes | possible | no   | yes | yes| possible                              |\n| Wide adoption          | not yet | yes| yes | yes | limited                            | limited                               | \n Wasp has a simple and fully-typed data model.  \n Documentation \n \n Data model \n FAQ \n Examples \n', 'test', 'hieroglyph-keyboard \n Web keyboard for hieroglyph entry and search \n   \n https://hierokeys.netlify.com/', 'ethymap', 'swadesh \n Collection of  swadesh lists  in CSV table format with possible connections to Indo European, thanks to https://en.wiktionary.org/wiki/Appendix:Swadesh_lists  \n Hieros \n 207 words are not enough? \ncheck out the wonderful Counterparts feature of the \nIntercontinental Dictionary Series project: \nhttps://ids.clld.org/values\nhttp://www.northeuralex.org/', 'tapboard \n Use your tablet/iPad as an ergonomic keyboard', "test-lld-wasm \n try to merge/concat/link/combine two wasm files via wasm-ld \n compile.sh \n compile-via-wat.sh   \n Maybe it's not a bug, but incorrect usage from our side. We ran into similar problems as this blog post:\n https://iandouglasscott.com/2019/07/18/experimenting-with-webassembly-dynamic-linking-with-clang/ \n wasm-ld --entry main lib.wasm main.wasm -o linked.wasm\nwasm-ld: error: entry symbol not defined (pass --no-entry to supress): main \n Update: It works", 'images \n https://github.com/pannous/hieros/wiki/%F0%93%90%93', 'Python + Poetry GitHub Action Template \n Getting started from the template \n \n Rename the  src/action_python_poetry  package. \n Globally replace instances of  action-python-poetry  and  action_python_poetry  with your project and package name. \n If your repo is private, set it up on  CodeCov  and add a codecov token to your repo under the  CODECOV_TOKEN  secret. \n Create and test your action.  __main__.py  in your package will be executed when the action is run. The environment variables your tests use can be  set in  pyproject.toml  and/or managed in test fixtures. \n Update  action.yml ,  README.md , and  .github/workflows/test-action.yml  to reflect your action\'s specification. \n Update  LICENSE.md  as appropriate, making sure to retain the original copyright and permissions notices in your distribution according to the MIT license that this template is distributed under. \n Remove this section from  README.md . \n Happy hacking! \n \n Like this template? \n \n Quickstart \n ```yml\nname: Run Action\non:\n  workflow_dispatch: \n jobs:\n  action-python-poetry:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: k2bd/action-python-poetry@v1\n        with:\n          helloName: k2bd\n          repeats: 3\n``` \n Action Specification \n helloName \n Required \n The name of the person to say hello to \n repeats \n Optional  - default 1 \n Number of times to say hello to this person \n Developing \n Install  Poetry  and  poetry install  the project \n Useful Commands \n Note: if Poetry is managing a virtual environment for you, you may need to use  poetry run poe  instead of  poe \n \n poe autoformat  - Autoformat code \n poe lint  - Linting \n poe test  - Run Tests \n \n Testing the action \n The action can be tested locally by building the Dockerfile, e.g. \n sh\ndocker run -e INPUT_HELLONAME=k2bd -e INPUT_REPEATS=2 $(docker build -q .) \n Additionally, there is a manual invocation action on the repo called "Test Action" that can be used to invoke the repo\'s version of the action from the Actions tab of the repo. \n Releasing \n Release a new version by creating a new annotated semver tag e.g.  git tag -a v1.2.3 -m "Release version 1.2.3"  and pushing it ( git push --tags ). Then create a new release from that tag in GitHub. \n There is an autoversioning action that keeps major version tags ( v1 ,  v2 , ...) and  latest  up-to-date when a new release is published.', 'Angle script  is the browser variant of the  angle  programming language, interpreting a modern syntax in javascript, compiling to native webassembly. \n Angle is  optionally speakable : \n assert two minus 1½ equals 0.5 \n Angle is  optionally typed : \n x=1;number y=π²;int y=2; assert x,y,z are numbers \n string s;s=3;s=="3"  save, because int x=cast "3" or throw \n Angle has  semantic indexing : \n assert 3rd word in \'hi my friend\' is \'friend\' \n Angle does what we call  matching by type name \n To delete mail:\n  move that mail to trash folder\nend \nHere \'mail\' acts as argument name and argument type at once. \nNo more Java style  Mail mail=new Mail().getMail();  never again with angle!   \n Angle has  contextual keywords   it  …:\n for all mails by peter: \n   if its subject contains \'SPAM\':\n      mark it as read \nthe last example translates to ruby: \n mails(by: Peter).each{|it| it.mark(:read) if it.subject.match(\'SPAM\')} \n Angle uses  mark  as data and code format:\n cat{\n    size:3\n    color:{r=1 g=0 b=0}\n    form{\n        dimensions = (3,size*2)\n    }\n    eat(){\n      size++\n    }\n    hunger{\n      strike=true #local variable is basically key:value pair\n      if(strike)\n        size--\n    }\n} \nAll code is data and all data can be \'executed\':\n cat.eat().dimensions is (3,8)  normal\n cat.size=5; cat.dimensions=(3,size*2)  bound but not yet evaluated\n cat().dimensions == cat.dimensions() is (3,10)  innovation \n classes are maps of keys/names to properties/fields/functions/variable-references!\nsymbols are names to references\nmaps are lists of pairs (duples), implicit triples: (index,name?,value?)\nlists are maps of index to value\ng = “I\'m a global”\nhash={\n  1 to \'a\'\n  2 : \'b\' # careful with ints as keys!\n  var c = \'3\' \n  d = \'4\' # no need for \'var\': keys are local variables by default\n  global e = 5 # relatively ugly by design\n  set f = 6\n  g = 7 # hmmm ...\n  if(g>6)\n    kill! #only invoked when hash! is invoked\n  8:return \'blocks are just maps of (hidden) line-numbers to statements\n  goto 2 # oh yes;) \n }\nhash()\ne == 5\nf   // error, not predefined\ng == 7  // now ok, but before?? \n internal block representation of hash == [\n(0,1,\'a\')\n(1,2,\'b\')\n(2,c,\'3\')\n(3,d,\'4\')\n(4,e,5) # where are modifiers? global(e) wrapper?\n(5,ø,set(f,6))\n(6,ø,if(g>6,kill!))\n(7,8,return)\n]\nhash[2]==hash.c==\'3\' \n constant  data is evaluated (\'reduced\') immediately. \n These are the basics, some future refinements can be read [[later.md]] \n Speakable lisp with less parentheses, brackets and optional sigils  [({;$})] \ninspired by  XTalk \n everything acts is an expressions\neverything acts is an object \n unified architecture combines blocks and lists and maps: \n blocks  {x+y,x>1}   {1,2,3}  are evaluated lazily \n lists or tuples are evaluated immediately  x=2;[1,2,x+x]==[1,2,4] \n blocks are evaluated when used   x=2;{1,2,x+x}==[1,2,4] \n Why? simplicity is good in itself \n maps look different to JavaScript!\n (a:1,b:2,c:3)\n(a=1,b=2,c=3)\n{a:1,b:2,c:3}  // evaluated lazily, as ”code“!\n(a,b,c)==(0:a,1:b,2:c) // like js \'objects\'\n(a,b,c)==[a,b,c] iff a,b,c are constant \n blocks can be applied to blocks or evaluated via \'argument\' maps:\n```\na={x+x}\nb={x=2}\nc=(x=2) \n a(b)=4\na(c)=4\na(x=2)=4\na{x=2}=4\n blocks can be defined via \':=\' \na:=x+x\nb={x+x}\na == b\n when applying blocks all variables must be bound via context or arguments: \na(y=2) undefind / error unless x is available in the context\na(2) undefind / error unless \'it\' keyword in block\na[2] acts as selector, not as evaluator !\n``` \n the arguments of the cooling block can be accessed by name or number or \'it\'\n {print b}(a:1,b:2) == 2\n{print $b}(a:1,b:2) == 2\n{print $1}(\'a\',\'b\') == \'b\'\n{print it*2}(2) == 4 \n methods get evaluated immediately without brackets\n to test:\n    print \'ok\'\ntest == \'ok\' \n methods can be (de)referenced via proceeding \'to\' keyword, similar to #symbol in ruby or method.name in python\n my_result=test // \'ok\', invoked\nmy_funk=to test // the method \nmethods are passed as symbols to functions that expect acts or blocks\n bigger = { $1 > $0 }\nto sort array by block{…}\nsort([3,2,1],bigger) \n to act in number n seconds:\n    sleep n\n    act \n what is the difference between functions, blocks, methods and acts? None really, just different names for synonyms. \n keep in mind that blocks can be parameterized with arguments (see above). those are usually called methods or functions but that\'s just convention. \n one nice way to think about \'blocks\' is that they are blocking execution until needed:\n```\nx=2\na=x+x // eval on the spot\nb:=x+x\nc={x+x}\nd={x+x}() \n a == 4\nb == ${x+x} == 4  // soft symbol\nc == {x+x} // symbolical\nc() == 4\nd == 4 \n x=3\nb == 6 // eval on the spot\nc() == 6\n``` \n e=(x=5){x+x}\ne == 10\ne(1) == 2 \noptional arguments give strictness to code\n f=(){x+x}  error x is neither an argument nor a global\nf2(){x+x}  error x is neither an argument nor a global\ng={x+x}  // ok for now \nall objects can act as classes (like js, really?)\n```\nx={a:1}\ny extends x\ny.a == 1\ny.b = 2 \n Circle={number radius,area:radius*radius}\nCircle(radius=3).area == 9 # no \'new\' \n ``` \n Preferably \'=\' should be used for leaves and \':\' for nodes with children.\ncolor=blue color:{blue dark}\nAs in js, even lists and entity/maps can be unified:\n [a b c]={1:a 2:b 3:c}\n{a:1 b:2 c:3}=[a:1 b:2 c:3] // todo hash as list of pairs as in kotlin\n{a b c}=[a b c] \nA list is just a special entity/map in which its keys are numbers\nA map is just a special list in which its entries are pairs.']
ykumards,['project-euler \n Working my way through Project Euler puzzles. Solutions are in Python and C++ \n \n Summary \n | Lang   | Count|\n|--------|------|\n| Python |  27  |\n| Java   |  12  |\n| C++    |  11  |', 'Collection of MR scripts', 'Algorithms \n Collection of algorithms, data structures and interesting problems, implemented from scratch', 'write-you-a-haskell \n Building a functional compiler from scratch \n An ambitious project following along http://dev.stephendiehl.com/fun/', "Goblin  \n Goblin paints your github calender green. It modifies your repository's date environmental variable to make commits in the past.  \n Goblin's main purpose was to help me learn the basics of running external commands (mostly git) in Go, so it requires  Go  compiler to function. \n Usage \n Make sure you clone the repo using ssh and configure github with your public rsa key (details  here ). In case you've cloned the repo using https,  this  might be useful. \n For running the script, place goblin.go inside the repo and use:\n     go run goblin.go  days \n days , is the number of many days you'd like goblin to go back. \n This will take a while. \n This has been tested on Ubuntu 14.04 (trusty).", 'Art of C Programming \n Random problems in C', "clojure-99-problems \n 99 Problems in Clojure is based on that 99 Scala problems compiled  here . Problem difficulties indicated by '*'s with '***' being hardest. \n Problems \n Working with lists \n \n P01 (*)  Find the last element of a given list   :heavy_check_mark: \n P02 (*)  Find the last but one element of a given list  :heavy_check_mark: \n P03 (*)  Find the Kth element in a list  :heavy_check_mark: \n P04 (*)  Find the number of elements in a list  :heavy_check_mark: \n P05 (*)  Reverse a list  :heavy_check_mark: \n P06 (*)  Findout whether a list is a palindrome  :heavy_check_mark: \n P07 (**)  Flatten a nested list structure  :heavy_check_mark: \n P08 (**)  Eliminate consecutive duplicates of list elements  :heavy_check_mark:  \n P09 (**)  Pack consecutive duplicates of list elements into sublists  :heavy_check_mark: \n P10 (*) Run-length encoding of a list :bow: \n", 'cs231n-code \n Working through Stanford Vision cNN course ', "scala-coursera \n Scribbles from the Scala course at Coursera \n This repo contains some practice code and random notes from the scala  course . The course follows along the contents of the  Scala by Example  book. \n Honoring the coursera honor code, I'll omit solutions to the HW problems. \n Contents \n \n Currying \n Fixed Point \n \n Other Resources \n \n Twitter Scala School \n", 'CLRS-code-bank \n Implementations of CLRS algorithms in C++', "emacs config \n I used to use spacemacs for my emacs profile, but it just got bloated out of control. When things went wrong, I had no idea why it went wrong and had to waste hours googling for a solution. It is better to start fresh with a clean emacs distribution.  \n Still had to use a starter kit (who likes to write boilerplate :D), so am using this awesome dot-file collection from Daniel Higginbottom. \n \n If you're new to emacs, check out\n this introductory tutorial ! \n I've tried to separate everything logically and document the purpose\nof every line.  init.el  acts as a kind of table of contents.", 'react-first \n First react app', "4clojure-solved \n Repo containg  my  solutions to the  4clojure  problems. I've been wrestling with Clojure all summer and these problems are a great way to test my understanding of its concepts. \n Also, I'll try make my solution as concise as possible. Obviously, it's going to look ugly at first, but pay attention to the growth gradient. :grin: \n Status \n \n \n This project was inspired by similar attempts by  katox ,  qiuxiafei ,  prakhar1989 ...", 'seven-langs \n Code for 7 Languages in 7 Weeks \n \n https://pragprog.com/book/btlang/seven-languages-in-seven-weeks', "sicp-clojure-code-bank \n Implmenting SICP exercises in Clojure \n \n I'm finally doing this! As the summer vacation nears completion, time to start working through SICP :smile:", 'scala-notes \n Repo for Scala notes and insights\nWorking through the Scala track in  Coursera', 'Scala Koans \n Solving  Scala Koans . \n \n Code can be found  here .', 'Go Koans Solution \n \n Taking the path to enlightenment in Go programming language. \n This time am running the tests in the Docker VM sandbox. \n To run this using Docker:\n  1. Install  Docker  for your OS.\n  2. Verify that Docker is installed by running  docker --version  in terminal.\n  3. Install  VirtualBox  if you dont already have it.\n  4. Setup the docker machine using the following\n $ docker-machine create -d virtualbox golang\n$ eval $(docker-machine env golang)\n$ docker pull library/golang:1.6.0-alpine \n  -  Navigate to the folder where you\'ve clone this repository and run\n $ docker run --rm -ti -v "$PWD":/usr/src/koans -w /usr/src/koans golang:1.6.0-alpine /bin/sh \n  - Once you\'re in the VM\'s prompt, run  go test  and work through the cases. \n \n Thanks original  go-koans  for getting me started with this.', 'Learn You Node Solutions', 'Github Battle \n Project built during the  React JS course .', 'learnyoureact-sol \n Solutions to  learnyoureact', 'hello-world \n This is a test repo for the Agile Development class @ NYU. \n To run the hello.py file, use\n   python hello.py', 'SOA-SpellChecker \n Spelling Checker based on Noisy Channel model', 'HPCbridge', 'leetcode-dark \n Dark Userstyle for Leetcode \n \n You can use  Stylish  in Chrome or Firefox to add this. ', 'click-mate \n A chrome extension for clickbait free browsing \n \n Notes \n \n Import the Glove vectors from  here  (WARNING: this is an 822MB file!) and unzip it in  data  folder \n \n Logo Credits \n Many many thanks to  Flaticon  for providing this  awesome logo  for free.', 'gtsrb \n Code for the Computer Vision assignment for German Traffic Sign Recognition \n \n Code is super messy, I promise to clean it up asap :bowtie:', 'dynalist-new-tab \n New Tab Page in chrome loads your Dynalist page \n \n Credits \n This extension relies heavily on the workflowy variant created by  Ryan Noon  called  WorkFlowy New Tab .', 'sublime-keymap-mod \n snippets of modified sublime keymaps', 'clojure-wars \n katas to stay in touch with clojure', 'clojure-koans \n \n Source  ', 'ruby-koans', "pomodorify \n You can use this script to run a simple pomodoro timer that plays music from Spotify while you work from the command line on Mac.  \n \n Prereqs \n \n Spotify desktop app (obviously) \n Macvim is used as the default editor, you can use anything else (I've also tested it with sublime) \n \n Installing \n \n Clone this repo \n Change the permissions for  pomodify.sh  and  spotify  using  chmod 700 \n Change the logger file path (or comment out all lines related to logging) \n Open Spotify app \n Change the Spotify playlist URI in  pomodify.sh  file. The default is a  coding playlist  that I love. \n Run the script using  ./pomodify.sh . You could also add an alias to the  .bash_profile . \n \n How it works \n \n You can point the script to any Spotify playlist of your choice \n The music plays when you work (default 25 minute work sessions) \n The music pauses for 5 mins during the break time  \n A logger file opens up during the break time, you can type anything you like here \n The process repeats until you hit Ctrl + C \n \n Why does this logger thingy popup during breaks? \n You can disable this by commenting out the appropriate lines if you'd like, but the main idea was to keep a log of the task that was done, or copying some nagging thoughts onto the dump file, or anything else you want. \n Default Editor \n I've used MacVim as the default editor here coz vim is lightweight and is configurable (like, you can open it in such a way that the cursor is always at the end) \n Credits \n \n The inspiration for this was  christiangenco 's  comment on  this  HN thread. \n The credit for spotify cli integration ENTIRELY goes to  @hnarayanan  and the other contributors of  shpotify . \n \n License \n Meh.", 'dot-spacemacs \n syncing .spacemacs', 'build-your-own-lisp', 'org-pomodoro-mod \n \n Credits \n https://github.com/lolownia/org-pomodoro', 'advent-of-code-2017', 'dockerfiles', 'ml_pearls \n machine learning code snippets', 'pixel-cnn \n Did this as part of Advance Deep Learning course. Model works on MNIST, so does not implement the convolutional masking across the channels.  \n To train the model, run  python main.py  from the src folder.  \n The plots and generated samples are saved in the  experiments  folder. \n \n Credits \n The paper does not explain the implementation properly, so I had to refer/borrow/steal ideas from these sources. Thanks to the authors! \n \n https://github.com/openai/pixel-cnn \n https://github.com/kundan2510/pixelCNN \n https://github.com/jrbtaylor/conditional-pixelcnn ||  blog  <- repo skeleton based on this \n https://github.com/rampage644/wavenet ||  blog \n https://github.com/kkleidal/GatedPixelCNNPyTorch \n', 'kaggle-generative-dogs \n \n \n Codebase for 21st place solution for  Kaggle Generative Dog Images competition . This was a kernels-only competition so my actual submission was a messy ipynb script, I cleaned it up a bit in this repo. The relevant files are in the  src  folder. \n \n \n I have used Conditional  Self-Attentive GANS  in this work. The top solutions for this competitions either used SA-GANS or  BigGANs .  \n \n \n This is also one of the first projects where I tried using  pytorch-ignite . I was tired of rewriting boilerplate model training code in Pytorch for every project.  \n \n \n To train the model, first grab the dataset from kaggle, place it in  input  folder and run  src/main.py . The script runs for approximately 12 hours (this can be tuned by modifying the  CUTOFF_SECONDS  global variable in  src/main.py   \n \n \n After 265 epochs, here are some generated images \n \n \n \n \n Credits \n \n Kaggle Kernels \n https://www.kaggle.com/phoenix9032/gan-dogs-starter-24-jul-custom-layers \n https://www.kaggle.com/speedwagon/ralsgan-dogs \n Github Repos \n https://github.com/shayneobrien/generative-models \n https://github.com/ozanciga/gans-with-pytorch \n https://github.com/voletiv/self-attention-GAN-pytorch \n https://github.com/AlexiaJM/RelativisticGAN \n', "Segmentation models \n   \n Segmentation models is python library with Neural Networks for Image Segmentation based on PyTorch. \n The main features of this library are: \n \n High level API (just two lines to create neural network) \n 4 models architectures for binary and multi class segmentation (including legendary Unet) \n 30 available encoders for each architecture \n All encoders have pre-trained weights for faster and better convergence \n \n Table of content \n \n Quick start \n Examples \n Models   \n Architectures \n Encoders \n Pretrained weights \n \n \n Models API \n Installation \n License \n \n Quick start  \n Since the library is built on the PyTorch framework, created segmentation model is just a PyTorch nn.Module, which can be created as easy as:\n```python\nimport segmentation_models_pytorch as smp \n model = smp.Unet()\n```\nDepending on the task, you can change the network architecture by choosing backbones with fewer or more parameters and use pretrainded weights to initialize it: \n python\nmodel = smp.Unet('resnet34', encoder_weights='imagenet') \n Change number of output classes in the model: \n python\nmodel = smp.Unet('resnet34', classes=3, activation='softmax') \n All models have pretrained encoders, so you have to prepare your data the same way as during weights pretraining:\n```python\nfrom segmentation_models_pytorch.encoders import get_preprocessing_fn \n preprocess_input = get_preprocessing_fn('resnet18', pretrained='imagenet')\n``` \n Examples  \n \n Training model for cars segmentation on CamVid dataset  here . \n Training model with  Catalyst  (high-level framework for PyTorch) -  here . \n \n Models  \n Architectures  \n \n Unet \n Linknet \n FPN \n PSPNet \n \n Encoders  \n | Type       | Encoder names                                                                               |\n|------------|---------------------------------------------------------------------------------------------|\n| VGG        | vgg11, vgg13, vgg16, vgg19, vgg11bn,  vgg13bn, vgg16bn, vgg19bn                             |\n| DenseNet   | densenet121, densenet169, densenet201, densenet161                                          |\n| DPN        | dpn68, dpn68b, dpn92, dpn98, dpn107, dpn131                                                 |\n| Inception  | inceptionresnetv2                                                                           |\n| ResNet     | resnet18, resnet34, resnet50, resnet101, resnet152                                          |\n| ResNeXt    | resnext50_32x4d, resnext101_32x8d, resnext101_32x16d, resnext101_32x32d, resnext101_32x48d  |\n| SE-ResNet  | se_resnet50, se_resnet101, se_resnet152                                                     |\n| SE-ResNeXt | se_resnext50_32x4d,  se_resnext101_32x4d                                                    |\n| SENet      | senet154                                                                                    |\n| EfficientNet | efficientnet-b0, efficientnet-b1, efficientnet-b2, efficientnet-b3, efficientnet-b4, efficientnet-b5, efficientnet-b6, efficientnet-b7 \n Weights  \n | Weights name                                                              | Encoder names                                                                                                                                                                                                                                                                                                                                                                       |\n|---------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| imagenet+5k                                                               | dpn68b, dpn92, dpn107                                                                                                                                                                                                                                                                                                                                                               |\n| imagenet                                                                  | vgg11, vgg13, vgg16, vgg19, vgg11bn,  vgg13bn, vgg16bn, vgg19bn,   densenet121, densenet169, densenet201, densenet161, dpn68, dpn98, dpn131,   inceptionresnetv2,   resnet18, resnet34, resnet50, resnet101, resnet152,   resnext50_32x4d, resnext101_32x8d,   se_resnet50, se_resnet101, se_resnet152,   se_resnext50_32x4d,  se_resnext101_32x4d,   senet154,   efficientnet-b0, efficientnet-b1, efficientnet-b2, efficientnet-b3, efficientnet-b4, efficientnet-b5, efficientnet-b6, efficientnet-b7 |\n|  instagram  | resnext101_32x8d, resnext101_32x16d, resnext101_32x32d, resnext101_32x48d                                                                                                                                                                                                                                                                                                           | \n Models API  \n \n model.encoder  - pretrained backbone to extract features of different spatial resolution   \n model.decoder  - segmentation head, depends on models architecture ( Unet / Linknet / PSPNet / FPN )   \n model.activation  - output activation function, one of  sigmoid ,  softmax \n model.forward(x)  - sequentially pass  x  through model`s encoder and decoder (return logits!)   \n model.predict(x)  - inference method, switch model to  .eval()  mode, call  .forward(x)  and apply activation function with  torch.no_grad() \n \n Installation  \n PyPI version:\n bash\n$ pip install segmentation-models-pytorch\n````\nLatest version from source: bash\n$ pip install git+https://github.com/qubvel/segmentation_models.pytorch\n```` \n License  \n Project is distributed under  MIT License \n Run tests \n bash\n$ docker build -f docker/Dockerfile.dev -t smp:dev .\n$ docker run --rm smp:dev pytest -p no:cacheprovider", "dl-stencil (WIP) \n Template for a typical pytorch deep learning project based on  pycls . Several modules (like tensorboard logging) also take from  Pytorch Ignite . \n The goal of this template is to have a functioning, minimalistic framework to run some quick experiments. I wouldn't recommend this for anything fancy like in production environments. \n Features (still WIP): \n \n Sensible config management using  yacs \n Tensorboard + json logging \n Early stopping \n LR policies like Exp, Cosine \n Should also largely work for multi-gpu setups, but am not that rich, so can't test it yet \n \n Branches are useful \n Scroll through different branches for different training setups:\n* master - Image classification on MNIST using Resnet50\n* vae - Variational Autoencoder on MNIST\n* lm (WIP) - Language model on IMDB using a simple LSTM\n* gan (WIP) - Train a GAN model on TBD \n After creating a new repo with this template, you might have to make the corresponding branch as the default branch using either  this  or  this . \n Workflow, how to train, etc \n \n Place all the config files in  ./configs  folder. \n Experiments are logged to the  ./experiments  folder. Each config file can (optionally) have an experiment name. The corresponding experiments are logged under the experiment name. \n Further, within each experiment folder, the logs are separated into separate subfolders \n saved_models  - holds checkpoint files (model, optimizers, configs, etc) \n stdout.log  - dump of iteration logs in json format \n tb_logs  - logs for tensorboard. Point the tensorboard to this folder to view them. \n Start training using the  train_model.py  script in the  src  folder\nEg.,\n python train_model.py --cfg ../configs/resnet18/resnet18_baseline.yml \n", 'i-don-t-know-jax \n pure functions! 😍', 'Hi there 👋 \n \n  [![Top Langs](https://github-readme-stats.vercel.app/api/top-langs/?username=ykumards)](https://github.com/anuraghazra/github-readme-stats)  \n \n**ykumards/ykumards** is a ✨ _special_ ✨ repository because its `README.md` (this file) appears on your GitHub profile.\n\nHere are some ideas to get you started:\n\n- 🔭 I’m currently working on ...\n- 🌱 I’m currently learning ...\n- 👯 I’m looking to collaborate on ...\n- 🤔 I’m looking for help with ...\n- 💬 Ask me about ...\n- 📫 How to reach me: ...\n- 😄 Pronouns: ...\n- ⚡ Fun fact: ...\n', 'ml_notebooks', 'dotfiles']
arpit15,['colladatoxml \n colladatoxml.py - for generating .xml tellmedave env from .dae collada env \nformat_tmd.py - for converting objects into the tellmedave format \nobjects.txt - contains dissimilar objects in the planit env in tellmedave format', 'Collection of important projects which are not open-sourced \n \n place recognition with convolutional Autoencoders(CAe) and Mutual Information(MI) \n Natural Language Instruction grounding with  LSTM-RNN \n', 'adapted from Itai Caspi \n learningDoom \n To learn to play doom  \n Dependencies \n ⋅⋅1. openai/gym\n2. tensorflow \n Results \n Defend the center\n \n Death Match \n \n \n Usage \n \n To visualize the output from conv layers look into agent.py \n to run an experiment make a copy of train_ .py  \n', '15418_assignments \n cmu parallel programming assignments', "Your GitHub Learning Lab Repository for Introducing GitHub \n Welcome to  your  repository for your GitHub Learning Lab course. This repository will be used during the different activities that I will be guiding you through. \n Oh! I haven't introduced myself... \n I'm the GitHub Learning Lab bot and I'm here to help guide you in your journey to learn and master the various topics covered in this course. I will be using Issue and Pull Request comments to communicate with you. In fact, I already added an issue for you to check out. \n \n I'll meet you over there, can't wait to get started!", "Your GitHub Learning Lab Repository for Communicating Using Markdown \n Welcome to  your  repository for your GitHub Learning Lab course. This repository will be used during the different activities that I will be guiding you through. \n Oh! I haven't introduced myself... \n I'm the GitHub Learning Lab bot and I'm here to help guide you in your journey to learn and master the various topics covered in this course. I will be using Issue and Pull Request comments to communicate with you. In fact, I already added an issue for you to check out. \n \n I'll meet you over there, can't wait to get started!", 'skill-based-exploration-drl \n code for reproducing results in paper on skill based exploration \n To train an DDPG agent with e-normal exploration \n $ python HER/examples/run.py --env-id=<env name>  --eval-env-id=<test env name> --log-dir=<give logging dir path> --nb-rollout-steps=800 --nb-epochs=200 \n To train parameterized action space agent with hierarchical hindsight experience replay \n $python HER/examples/prun.py --env-id=<env name> --eval-env-id=<test env name>  --log-dir=<give logging dir path>  --commit-for=10 --nb-rollout-steps=80 --nb-epochs=200 --skillset=set14 --her \n To train the DDPG with skill library(will need to modify skill library paths in skills/set .py  ) \n $python HER/examples/succlookaheadrun.py --env-id=<env name> --eval-env-id=<test env name>  --log-dir=<give logging dir path> --commit-for=10 --nb-rollout-steps=800 --nb-epochs=200 --skillset=<skill set> \n Some example commands used to train picknmove env \n $ python HER/examples/run.py --env-id=picknmove-v2  --eval-env-id=picknmovet-v2 --log-dir=$HOME/new_RL3/corl_paper_results/clusters-v1/picknmove-v2/run1 --nb-rollout-steps=800 --nb-epochs=200 \n $ python HER/examples/succlookaheadrun.py --env-id=picknmove-v2 --eval-env-id=picknmovet-v2  --log-dir=$HOME/new_RL3/corl_paper_results/clusters-v1/picknmoveflat-v2/run1 --actor-lr=0.01  --critic-lr=0.01 --commit-for=10 --nb-rollout-steps=800 --nb-epochs=200 --skillset=set14 \n $python HER/examples/prun.py --env-id=picknmove-v2 --eval-env-id=picknmovet-v2  --log-dir=$HOME/new_RL3/corl_paper_results/clusters-v1/picknmovehie-v2/run1  --commit-for=10 --nb-rollout-steps=80 --nb-epochs=200 --skillset=set14 \n Acknowledgments \n Our code is built over the high quality baselines DDPG implementation.\nWe would like to thanks all the contributors to OpenAI gym, baselines and mujoco_py \n @misc{baselines,\n  author = {Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai},\n  title = {OpenAI Baselines},\n  year = {2017},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/openai/baselines}},\n}', "matlab_octave_syntax \n Opens matlab .m files with octave syntax highlighting \n Prerequisites \n \n JupyterLab \n \n Installation \n To install using pip: \n bash\njupyter labextension install matlab_octave_syntax \n Development \n For a development install (requires npm version 4 or later), do the following in the repository directory: \n ```bash \n Clone the repo to your local environment \n Move to matlab_octave_syntax directory \n Install dependencies \n npm install \n Install your development version of the extension \n jupyter labextension install .\n``` \n You run JupyterLab in watch mode to watch for changes in the extension's source and automatically rebuild. \n ```bash \n Run jupyterlab in watch mode \n jupyter lab --watch\n``` \n Now every change will be built locally and bundled into JupyterLab. Be sure to refresh your browser page after saving file changes to reload the extension (note: you'll need to wait for webpack to finish, which can take 10s+ at times). \n Uninstall \n bash\njupyter labextension uninstall matlab_octave_syntax", "Mitsuba IM — Physically Based Renderer (Interactive Fork) \n \n \n \n Mitsuba IM is a fork of the comprehensive physically-based renderer mitsuba (http://mitsuba-renderer.org/) by Wenzel Jakob (and other contributors), which has proven an invaluable framework for the scientific evaluation of both classic rendering algorithms and novel rendering research. This IM fork pursues the following additional goals: \n \n Responsive interactive preview of rendering algorithms (with interactive camera & settings) \n Easier implementation of new rendering algorithms \n New simplified interface for responsive integrators (see  include/mitsuba/render/integrator2.h ) \n Integrator parameters no longer need to be specified redundantly, they are automatically extracted from the integrator plugins (no modifications required) \n Comes with responsive wrappers for almost all rendering algorithms in classic mitsuba, which serve as examples \n \n \n Responsive imgui frontend that is easily hackable for additional feautres and visualizations (+no more Qt dependencies) \n Compile out of the box on modern C++ compilers (with one recursive git clone) \n Replace binary dependencies by git submodules \n Replace boost libraries by C++ standard library \n \n \n Compatibility with previous mitsuba interfaces and thus rendering research (a render button still exists, if integrator is not automatically wrappable, interactive preview then falls back to path tracer) \n \n Note: This is a preview release \n Building \n Requires git, CMake, and a compiler with C++17 support (sorry, but at least frees you from boost binaries). \n Tested on Ubuntu w/ GCC 7 and on Windows w/ MSVC 2017. You might need to install a GCC 7 package manually. \n $ git clone https://github.com/tszirr/mitsuba-im --recursive\n$ mkdir mitsuba-im/projects\n$ cd mitsuba-im/projects\n$ cmake ..\n(On Windows replace by: $ cmake .. -Ax64)\n$ make\n$ cd ..\n$ ln -s projects/binaries/im-mts \n About (Original official description) \n http://mitsuba-renderer.org/ \n Mitsuba is a research-oriented rendering system in the style of PBRT, from which it derives much inspiration. It is written in portable C++, implements unbiased as well as biased techniques, and contains heavy optimizations targeted towards current CPU architectures. Mitsuba is extremely modular: it consists of a small set of core libraries and over 100 different plugins that implement functionality ranging from materials and light sources to complete rendering algorithms. \n In comparison to other open source renderers, Mitsuba places a strong emphasis on experimental rendering techniques, such as path-based formulations of Metropolis Light Transport and volumetric modeling approaches. Thus, it may be of genuine interest to those who would like to experiment with such techniques that haven't yet found their way into mainstream renderers, and it also provides a solid foundation for research in this domain. \n The renderer currently runs on Linux, MacOS X and Microsoft Windows and makes use of SSE2 optimizations on x86 and x86_64 platforms. So far, its main use has been as a testbed for algorithm development in computer graphics, but there are many other interesting applications. \n Mitsuba comes with a command-line interface as well as a graphical frontend to interactively explore scenes. While navigating, a rough preview is shown that becomes increasingly accurate as soon as all movements are stopped. Once a viewpoint has been chosen, a wide range of rendering techniques can be used to generate images, and their parameters can be tuned from within the program. \n Documentation \n For compilation, usage, and a full plugin reference, please see the  official documentation . \n Releases and scenes \n Pre-built binaries, as well as example scenes, are available on the  Mitsuba website .", 'Repository for 2D intersection tests \n The repository contains code to find ray intersection with a cubic basis curve\n-  pip install -r requirements.txt \n-  python test_curve.py  # test_curve_intersection']
washt,["washt.xyz \n tttuckerrr's personal blog, built with some minor tweaks of  Pool", 'OpenCV-Projects \n Test images are sourced from the [SIPI Image Database] (http://sipi.usc.edu/database/database.php?volume=misc).', "Python implementation of various machine learning techniques with a feed forward neural network \n \n \n Features \n \n nnet  class provides basic structure and accessor methods for a generic feed-forward Neural Network. ( completed ) \n geneticAlg  class implements a genetic algorithim for the  nnet  class, based on  ai-junkie's C++ implementation.  ( partially   implemented ) \n neat  class is an implementation of the GE decribed in the  NEAT Paper , again using the  nnet  class. ( partially   implemented ) \n \n Dependencies \n Implementation \n* numpy -- For matrix math\n* sklearn -- For test datasets\n* matplotlib -- Not really nessessary, mainly to visualize test datasets nicely\n* ad -- For easy differentiation \n Testing \n* nose\n* coveralls", 'SSVEP \n SSVEP Interface written in OpenGL,GLFW,and GLEW \n Dependencies \n \n g++ \n Boost \n OpenGL \n GLFW \n GLEW \n \n Currently builds on linux, but code should compile and run on windows with a custome build script.  \n Usage \n Edit  build.sh  with the number of windows and their respective frequencies.\n chmod a+x build.sh , then  ./build.sh', "hacktohelp \n Dominion Enterprises Hack to Help Hampton Roads \n \n Proposal : \n \n We should set out to solve the Elizabeth river infrastructure problem using a robot that trolls the path and sends path discovery data   in real time to a public accessible site.  This site uses the bot data, along with data from fitness tracking APIs and vDOT data, to formulate the best/safest/most scenic route for path users to enjoy. \n \n Possible Features \n \n A system where users can report obstructions along the highlighted route. Then the bot can go out and verify if the obstruction is there and update the route accordingly. This would promote community involvement. \n If there are obstructions on the route then, there should be a little description of the obstruction. (e.g If there's construction going on and it's noisy ... then maybe a person that is jogging might go through there versus a tourist trying to enjoy scenery) \n \n Concerns/Problems \n \n what happens when the bot is disrupted/tampered with during its surveying of a route? \n \n to prove concept we may need the following: \n hardware : \n \n \n wheels \n housing for computing hardware/sensors \n kinect \n telemetry \n dev board w/ cpu, motor controllers, GPIO, gyro, accelerometer \n long lasting battery \n charging station \n a machine to host the webpage \n \n software : \n \n website: \n \n website-frontend \n bot-communication-website-backend \n best-path-builder-website-backend \n data-aggregator-website-backend \n data-normalizer-website-backend \n request-for-obstruction-removal-website-backend \n \n robot: \n \n motor-control-robot \n kinect-vision-robot \n bot-communication-robot \n localized-path-discovery-robot \n self-docking-algorithm-robot \n \n Data : \n \n Users: \n \n Surveys-potential-users \n Market-Research-Past-Projects \n Cost-estimate \n Timeline \n", 'InSeasonScraper \n python scraper for DE Hack to Help', 'dotfiles \n install \n \n clone directory and  cd  into directory \n ~~ chmod +x install.sh && ./install.sh ~~ \n vim +PluginInstall +qall   \n', 'hashtagFREE \n :pizza: :pizza: :pizza: :pizza: :pizza: :pizza: :pizza: :pizza: :pizza: :pizza: :pizza: :pizza: :pizza:', 'HRT Bus App Status Checker \n Setup \n \n Clone this repo and  cd  into it \n npm install \n npm start \n \n License \n The MIT License (MIT) \n Copyright (c) 2016 Tucker Wash  twash@cs.odu.edu \n Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the "Software"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions: \n The above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software. \n THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.', 'raft \n Implementation of the Raft Consensus Algorithm https://raft.github.io/', 'Pizza Peel 🍕 \n \n a tool to help pizza restaurateur\'s keep their eyes peeled 👀 for potential new locations. \n \n Install and Run \n chmod +x venv/bin/activate \n ./venv/bin/activate \n pip3 install -r requirements.txt \n Run server \n FLASK_APP=server flask run \n API \n Given a set of coordinates, return the top five nearest bars, along with their respective area. \n Request \n POST /nearest_bars { "lat":  "-77.0380051", "long": "38.9039033" }  \n curl --location --request POST \'localhost:5001/nearest_bars/\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{ "lat":  "-77.0380051", "long": "38.9039033"}\' \n Response \n results will be returned as a JSON array with each entry containing an array of  name ,  coordinates , and  site area  in meters squared \n {[\n    [\n        "Barcode",\n        [\n            -77.0380051,\n            38.9039033\n        ],\n        26616.163640099174\n    ],\n    [\n        "Off The Record",\n        [\n            -77.038828,\n            38.9028416\n        ],\n        10051.345871848793\n    ],\n    [\n        "Bravo Bravo",\n        [\n            -77.0392331,\n            38.9029189\n        ],\n        10051.345871848793\n    ],\n    [\n        "Archibald\'s/Fast Eddies Billiards Cafe",\n        [\n            -77.035585,\n            38.9021339\n        ],\n        1579.3149046856142\n    ],\n    [\n        "The Bottom Line",\n        [\n            -77.0401739,\n            38.9009945\n        ],\n        23084.5228926401\n    ]\n]} \n Data Joining Process \n My joining algo can be seen in  preprocess.py , which was used to create  data/joined_data.json  for the main application. I had some trouble initially loading in the  osm-bars  data because I\'ve never worked with  .shp  files before..this set me back ~1 hour. \n I brute forced a solution to match location points with their respective footprint data by using  shapely.Polygon.within , then calulating the area. \n There are definitely some optimizations available here..however my thinking was since building footprints shouldn\'t change very often and given the time limit of the exercise a brute force approach would be okay for now.  preprocess.py  took about 5 minutes to run on my 2020 Macbook Pro. If the input data were widened to the entire country, I\'d probably use a balanced tree to reduce the search time in the joining process. \n I don\'t think the projection calculation is correct based on the numbers I\'m seeing in the result (way too big!!). Given the time constraint I had to abandon investingating why but I have a hunch that I may be using the wrong geodetic transform value. Additionally, the footprint calculation assumes a 1:1 with building size, which isn\'t very accuate (floor plan layout, dead space, multitple floors, etc..) I\'d pull in MLS or zoning data to get a more accurate square footage number. \n Server Process \n I use  scipy s  KDTree  to store and find nearest bars for a given latitude and longitude. \n If I had more time,,, \n I would add some more documentation + unit tests for the transform functionm, KDTree lookup and some e2e tests for api request/response. \n I\'d add a persistant data layer and define schema for the bar location data. Although to be honest, I\'m not sure how to find the closest neighbors efficiently with a SQL query...I\'d probably take an hour or two more of research.  \n I also had some issues with my local python enviornment, so I might add containerization to make dev setup easier.', 'Symptom Checker \n Installation \n API \n \n \n pipenv install \n \n \n set up database in  psql \n \n \n pipenv run ./manage.py runserver \n Frontend \n npm install \n npm start']
ivan-bocharov,["master-presentation \n Master's presentation", 'Some experiments with Naive Bayes classifier implemented in Python. \n In order to install all the requirements, run:\n pip install -r requirements.txt \nfrom the root folder of the repository. \n Usage example: \n python main.py -c 5 -std 5.0 --plot --logging \n All the command line arguments: \n -c  Classes number, integer \n -s  Samples number, integer \n -f  Features number, integer \n -box  The box of centers of classes, string (a list of numbers, separated with spaces. Example: 1 2 3 4) \n -std  Standard deviation of class elements distribution, floating point number \n -a  Averaging method, string (possible variants are micro, macro and binary) \n --logging  Logging enabling \n --plot  Plotting enabling \n Help is accessible via -h (or --help) argument.', "Brainfuck.jl \n It is a simple Brainfuck interpreter written in Julia. Why? Because we can! \n While developing the interpreter, I'll be using the table below. \n Brainfuck command | C equivalent\n---------         | ------------\n(Program Start)   | char array[infinitely large size] = {0};\n                  | char *ptr=array; \n \n               | ++ptr;\n \n <                   | --ptr;\n+  |++*ptr;\n-   |--*ptr;\n.   |putchar(*ptr);\n,   |*ptr=getchar();\n[   |while (*ptr) {\n]   |} \n", '.dotfiles \n Current tools and scripts', 'stan-vscode README \n This package adds syntax highlighting, code folding and snippets for  Stan  files in  Visual Studio Code . \n The grammar was originally converted from the  atom-language-stan  Stan package. The grammar has since been updated to support Stan v2.26. \n Features \n Syntax highlighting \n \n Snippets \n \n Code Folding \n \n Installation \n code --install-extension ivan-bocharov.stan-vscode \n or find and install it from the Extensions view.', 'ForneyLab.jl \n \n \n ForneyLab.jl is a Julia package for automatic generation of (Bayesian) inference algorithms. Given a probabilistic model, ForneyLab generates efficient Julia code for message-passing based inference. It uses the model structure to generate an algorithm that consists of a sequence of local computations on a Forney-style factor graph (FFG) representation of the model. For an excellent introduction to message passing and FFGs, see  The Factor Graph Approach to Model-Based Signal Processing  by Loeliger et al. (2007). Moreover, for a comprehensive overview of the underlying principles behind this tool, see  A Factor Graph Approach to Automated Design of Bayesian Signal Processing Algorithms  by Cox et. al. (2018). \n We designed ForneyLab with a focus on flexible and modular modeling of time-series data. ForneyLab enables a user to: \n \n Conveniently specify a probabilistic model; \n Automatically generate an efficient inference algorithm; \n Compile the inference algorithm to executable Julia code. \n \n The current version supports  belief propagation  (sum-product message passing),  variational message passing  and  expectation propagation . \n The  ForneyLab project page  provides more background on ForneyLab as well as pointers to related literature and talks. For a practical introduction, have a look at  the demos . \n Documentation \n Full documentation is available at  BIASlab website . \n It is also possible to build documentation locally. Just execute \n bash\n$ julia make.jl \n in the  docs/  directory to build a local version of the documentation. \n Installation \n Install ForneyLab through the Julia package manager:\n jl\n] add ForneyLab \nIf you want to be able to use the graph visualization functions, you will also need to have  GraphViz  installed. On Linux, just use  apt-get install graphviz  or  yum install graphviz . On Windows, run the installer and afterwards manually add the path of the GraphViz installation to the  PATH  system variable. On MacOS, use for example  brew install graphviz . The  dot  command should work from the command line. \n Some demos use the  PyPlot  plotting module. Install it using  ] add PyPlot . \n Optionally, use  ] test ForneyLab  to validate the installation by running the test suite. \n Getting started \n There are  demos  available to get you started.\nAdditionally, the  ForneyLab project page  contains a talk and other resources that might be helpful. \n License \n (c) 2019 GN Store Nord A/S. Permission to use this software for any non-commercial purpose is granted. See  LICENSE.md  file for details.']
tusharsoni08,['HTML5 Asteroids \n This game was created Doug McInnes. His code can be found\n here , and you can play his\nversion of the game online at his website\n here . \n Caroline Buckey and Sarah Spikes modified the repository to create exercises for\nthe Udacity course  Version Control Using Git and Github . These\nmodifications included introducing bugs and other changes into Doug’s commits he\ndid not in fact create! The bugs are intended to give learners experience using\nGit to find the commit where a bug was introduced. To play the modified version\nof the game, simply open the index.html file in your web browser. \n Many thanks to Doug for creating this awesome game.', "xogame \n You can access it from here:  https://nameless-depths-5926.herokuapp.com/\n \nMultiplayer and twisted version of old tic-tac-toe game with Node.js & Socket.io \n About X/O GAME- \n 1. Match first 4 same X/O\n2. Who hit topmost 4th X/O element which match with just previous 3 X/O elements, will get +1 Point\n3. Your focus on top 4 elements No matter what are the previous element (from 5 to last)\n4. Only 4th Hit will give you +1 (When complete sequence)\n5. You can break the sequence by Hit opposite key So other players not complete\n6. THIS GAME IS NOT A PERSONAL GAME, ITS GROUP GAME. SO PLAY THIS WITH FRIENDS :)\n7. ALL USER OF THIS GAME ARE PLAYER OF THIS GAME\n8. Share your Score by click on FB-Share button\n9. Let's Play! Have FuN* (More Player More Fun!!)\n", "twitter_finder \n Find Trends(#) & Most Popular person/company(@) in a time period on Twitter \n how it works: \n Input- \n - Range of time period (from - to)\n- input.txt (4 text files): list of screen name (or username) of twitter's profile\n \n Output- \n - First it will print Trends(#hashtags) & Most Popular person/company(@mentions) and their frequencies in descending order for all screen names from input file\n- Then, it will print Trends(#hashtags) & Most Popular person/company(@mentions) for whole list of profiles and their frequencies in descending order by taking intersection with all individual profile's #hashtags & @mentions, respectively in a time period\n- At last, it will print Trends(#hashtags) & Most Popular person/company(@mentions) and generate the bar graphs by taking the intersection of all four text input file's hashtags and mentions as a cross groups matching and print the results.\n \n Note: It will only print frequencies which is more than one.", "The MatriX Game \n A multiplayer networking game. \n System Design \n \n How it's work? \n This game is played between two players. You can start by finding a random player. If someone searched subsequently after you searched, you both are connected or vice-versa and after connection his/her name will be shown on top-right corner, then you need to fill up squares with blue color as fast as possible. For each win you will get +5 Points. The person with the most covered cells at the end of game will then be crowned as the winner! \n The result for each match will be published by Chat Bot with top 10 Ranks in Leaderboard chat, also user can group chat. \n \n \n \n \n", 'DeepTraffic \n 70.94 MPH\n', "Exchange Connector (Unified Inbox [https://uib.ai/]) \n Programming Language \n - C/C++\n \n Required Dependencies \n - Boost: sudo apt-get install libboost-all-dev\n- OpenSSL: sudo apt-get install libssl-dev\n \n Compilation \n g++ -DWITH_OPENSSL -DDEBUG -o ewsClient sendMsg.cpp soapC.cpp soapExchangeServiceBindingProxy.cpp stdsoap2.cpp ./gsoap/custom/duration.c -lssl -lcrypto\n \n Run Binaries \n ./ewsClient\n \n Log Files \n 1. Request logs from UE to Connector:\n    - ue_connector_log.txt\n\n2. Request(SENT) logs from Connector to Exchange Server:\n    - SENT.log\n\n3. Response(RECV) logs from Exchange Server to Connector:\n    - RECV.log\n\n4. *(Don't use it) Only for Internal details (Hardware-level) of SOAP request/response\n    - TEST.log\n \n Configuration File (config.txt) \n - Details about PORT number of Connector and QUEUE_SIZE of the request from UE to Connector. \n- Connector is concurrently processing request from queue. \n- If queue is full then Connector will refuse the further request from UE.\n\nNOTE: Please make sure in editing config.txt file, that Do not change anything in formatting. You are only allowed to change the values. Do not enter new line or space. Otherwise program will failed to read file correctly.\n \n Figure for communication \n UE <--------------------------> Exchange_Connector <------------------------> Exchange_Server\n \n Contact \n For any help please contact me: \nE-mail: soni.tushar93@gmail.com or tushar.soni@unifiedinbox.com\n", 'Hey Google... talk to Train Alarm! \n Create reminder and get a notification when your train is approaching your destination station! \n Train Alarm is an Indian train assistant. Tell your \'PNR Number\' and you are all set. Be informed when your train is about to reach your destination station within a radius of 10 km to 35 km, so you\'ll never miss your stop. \n Getting Started \n Just ask your Google Assistant: " Talk to Train Alarm " and follow through the instructions. \n Here  is the cron job code, which will send a notification to user when their train is about to reach their destination station. \n API interactions \n \n \n The end-user types or speaks an expression. \n Your service sends this end-user expression to Dialogflow in a detect intent request message. \n Dialogflow sends a detect intent response message to your service. This message contains information about the matched intent, the action, the parameters, and the response defined for the intent. \n Your service performs actions as needed, like database queries or external API calls. \n Your service sends a response to the end-user. \n The end-user sees or hears the response. \n \n This is how user will have notification:  \n \n And here is the T-shirt which i have received from Google after publishing my first action for Google Assistant :)\n', '30-Day-LeetCoding-Challenge \n https://leetcode.com/explore/other/card/30-day-leetcoding-challenge/', 'Openmined-FL-Recommendation-System \n privacy preserving recommendation system research']
ramakumar1729,['This is an experimental incremental seq-seq MT system', 'Sparse Predictive Analysis of Social Information Networks using K-Support Norm', 'AMML \n Advanced Multimodal Learning', 'Reinforcement Learning Algorithms \n This repository contains implementations of reinforcement learning algorithms, used to build agents that can play Atari games. \n Algorithms:\nQ-learning variants \n \n Linear Q-learning \n Linear Q-Learning with experience replay \n Deep Q-learning \n Double Deep Q-learning \n \n Video of DDQN agent playing Space Invaders. \n', "NN4NLP \n Members \n \n Rama Kumar Pasumarthi (ramakumar1729) \n Evangelia Spiliopoulou (spilioeve) \n Hiroaki Hayashi (rooa) \n \n How to run \n \n Install requirements. \n Run as follows\n sh\npython -m src.train \\\n    --save-dir exp1 \\  # Unique directory name.\n    --batch-size 16 \\\n    --embed-dim 100 \\\n    --tolerate 5 \\\n    --embedding-file PATH/TO/GLOVE/EMBEDDINGS \\ # If using pretrained emb\n    --cuda # If using gpu \n \n What's saved in the  --save-dir \n \n model_best.pt  : PyTorch model. \n train.log  : Training/Dev loss and F-1 history. \n \n Running relation classifier model. \n python run.py train relation_classifier/experiments/concat_reprs.json -s /tmp/relation_classifier_model \n Running tests. (Install pytest) \n py.test", 'Conditioned Seq2Seq model. \n Using tf.Datasets and tf.Estimators for seq2seq model. \n References \n \n Creating custom input pipelines (input fn) . \n Creating Custom Estimators . \n Google NMT . \n tf_seq2seq . \n', 'Gated-Attention Architectures for Task-Oriented Language Grounding \n This is a PyTorch implementation of the AAAI-18 paper: \n Gated-Attention Architectures for Task-Oriented Language Grounding \nDevendra Singh Chaplot, Kanthashree Mysore Sathyendra, Rama Kumar Pasumarthi, Dheeraj Rajagopal, Ruslan Salakhutdinov \nCarnegie Mellon University \n Project Website: https://sites.google.com/view/gated-attention \n \n This repository contains: \n \n Code for training an A3C-LSTM agent using Gated-Attention \n Code for Doom-based language grounding environment \n \n Dependencies \n \n ViZDoom \n PyTorch \n Opencv  \n \n (We recommend using  Anaconda ) \n Usage \n Using the Environment \n For running a random agent:\n python env_test.py \nTo play in the environment:\n python env_test.py --interactive 1 \nTo change the difficulty of the environment (easy/medium/hard):\n python env_test.py -d easy \n Training Gated-Attention A3C-LSTM agent \n For training a A3C-LSTM agent with 32 threads:\n python a3c_main.py --num-processes 32 --evaluate 0 \nThe code will save the best model at  ./saved/model_best . \n To the test the pre-trained model for Multitask Generalization:\n python a3c_main.py --evaluate 1 --load saved/pretrained_model \nTo the test the pre-trained model for Zero-shot Task Generalization:\n python a3c_main.py --evaluate 2 --load saved/pretrained_model  \nTo the visualize the model while testing add \'--visualize 1\': \n python a3c_main.py --evaluate 2 --load saved/pretrained_model --visualize 1  \nTo test the trained model, use  --load saved/model_best  in the above commands. \n All arguments for a3c_main.py:\n -h, --help            show this help message and exit\n  -l MAX_EPISODE_LENGTH, --max-episode-length MAX_EPISODE_LENGTH\n                        maximum length of an episode (default: 30)\n  -d DIFFICULTY, --difficulty DIFFICULTY\n                        Difficulty of the environment, "easy", "medium" or\n                        "hard" (default: hard)\n  --living-reward LIVING_REWARD\n                        Default reward at each time step (default: 0, change\n                        to -0.005 to encourage shorter paths)\n  --frame-width FRAME_WIDTH\n                        Frame width (default: 300)\n  --frame-height FRAME_HEIGHT\n                        Frame height (default: 168)\n  -v VISUALIZE, --visualize VISUALIZE\n                        Visualize the envrionment (default: 0, use 0 for\n                        faster training)\n  --sleep SLEEP         Sleep between frames for better visualization\n                        (default: 0)\n  --scenario-path SCENARIO_PATH\n                        Doom scenario file to load (default: maps/room.wad)\n  --interactive INTERACTIVE\n                        Interactive mode enables human to play (default: 0)\n  --all-instr-file ALL_INSTR_FILE\n                        All instructions file (default:\n                        data/instructions_all.json)\n  --train-instr-file TRAIN_INSTR_FILE\n                        Train instructions file (default:\n                        data/instructions_train.json)\n  --test-instr-file TEST_INSTR_FILE\n                        Test instructions file (default:\n                        data/instructions_test.json)\n  --object-size-file OBJECT_SIZE_FILE\n                        Object size file (default: data/object_sizes.txt)\n  --lr LR               learning rate (default: 0.001)\n  --gamma G             discount factor for rewards (default: 0.99)\n  --tau T               parameter for GAE (default: 1.00)\n  --seed S              random seed (default: 1)\n  -n N, --num-processes N\n                        how many training processes to use (default: 4)\n  --num-steps NS        number of forward steps in A3C (default: 20)\n  --load LOAD           model path to load, 0 to not reload (default: 0)\n  -e EVALUATE, --evaluate EVALUATE\n                        0:Train, 1:Evaluate MultiTask Generalization\n                        2:Evaluate Zero-shot Generalization (default: 0)\n  --dump-location DUMP_LOCATION\n                        path to dump models and log (default: ./saved/) \n Demostration videos: \n Multitask Generalization video: https://www.youtube.com/watch?v=YJG8fwkv7gA \n Zero-shot Task Generalization video: https://www.youtube.com/watch?v=JziCKsLrudE \n Different stages of training: https://www.youtube.com/watch?v=o_G6was03N0 \n Cite as \n \n Chaplot, D.S., Sathyendra, K.M., Pasumarthi, R.K., Rajagopal, D. and Salakhutdinov, R., 2017. Gated-Attention Architectures for Task-Oriented Language Grounding. arXiv preprint arXiv:1706.07230. ( PDF ) \n \n Bibtex: \n @article{chaplot2017gated,\n  title={Gated-Attention Architectures for Task-Oriented Language Grounding},\n  author={Chaplot, Devendra Singh and Sathyendra, Kanthashree Mysore and Pasumarthi, Rama Kumar and Rajagopal, Dheeraj and Salakhutdinov, Ruslan},\n  journal={arXiv preprint arXiv:1706.07230},\n  year={2017}\n} \n Acknowledgements \n This repository uses ViZDoom API (https://github.com/mwydmuch/ViZDoom) and parts of the code from the API. The implementation of A3C is borrowed from https://github.com/ikostrikov/pytorch-a3c. The poisson-disc code is borrowed from https://github.com/IHautaI/poisson-disc.', 'TensorFlow Ranking \n TensorFlow Ranking is a library for Learning-to-Rank (LTR) techniques on the\nTensorFlow platform. It contains the following components: \n \n Commonly used loss functions including pointwise, pairwise, and listwise\n    losses. \n Commonly used ranking metrics like Mean Reciprocal Rank (MRR) and Normalized\n    Discounted Cumulative Gain (NDCG). \n Multi-item (also known as groupwise) scoring functions . \n LambdaLoss  implementation for\n    direct ranking metric optimization. \n Unbiased Learning-to-Rank \n    from biased feedback data. \n \n We envision that this library will provide a convenient open platform for\nhosting and advancing state-of-the-art ranking models based on deep learning\ntechniques, and thus facilitate both academic research as well as industrial\napplications. \n 1-Click Demos \n We provide two demos, with no installation required, to get started on using\nTF-Ranking. These demos run on a\n colaboratory notebook ,\nan interactive Python environment. \n \n \n TF-Ranking on a dummy data set with dense features in\n     the LIBSVM format :\n     \n \n \n Using sparse features and embeddings in TF-Ranking.\n     \n This demo demonstrates how to: \n \n Use sparse/embedding features \n Process data in TFRecord format \n Tensorboard integration in colab notebook, for Estimator API \n \n \n \n Linux Installation \n Stable Builds \n To install the latest version from  PyPI , run the following: \n ```shell \n Installing with the  --upgrade  flag ensures you\'ll get the latest version. \n pip install --user --upgrade tensorflow_ranking\n``` \n To force a Python 3-specific install, replace  pip  with  pip3  in the above\ncommands. For additional installation help, guidance installing prerequisites,\nand (optionally) setting up virtual environments, see the  TensorFlow\ninstallation guide . \n Note: Since TensorFlow is  not  included as a dependency of the TensorFlow\nRanking package (in  setup.py ), you must explicitly install the TensorFlow\npackage ( tensorflow  or  tensorflow-gpu ). This allows us to maintain one\npackage instead of separate packages for CPU and GPU-enabled TensorFlow. \n Installing from Source \n \n \n To build TensorFlow Ranking locally, you will need to install: \n \n \n Bazel , an open\n    source build tool. \n shell\n$ sudo apt-get update && sudo apt-get install bazel \n \n \n Pip , a Python package manager. \n shell\n$ sudo apt-get install python-pip \n \n \n VirtualEnv , a tool\n    to create isolated Python environments. \n shell\n$ pip install --user virtualenv \n \n \n \n \n Clone the TensorFlow Ranking repository. \n shell\n$ git clone https://github.com/tensorflow/ranking.git \n \n \n Build TensorFlow Ranking wheel file and store them in  /tmp/ranking_pip \n    folder. \n shell\n$ cd ranking  # The folder which was cloned in Step 2.\n$ bazel build //tensorflow_ranking/tools/pip_package:build_pip_package\n$ bazel-bin/tensorflow_ranking/tools/pip_package/build_pip_package /tmp/ranking_pip \n \n \n Install the wheel package using pip. Test in virtualenv, to avoid clash with\n    any system dependencies. \n shell\n$ ~/.local/bin/virtualenv -p python3 /tmp/tfr\n$ source /tmp/tfr/bin/activate\n(tfr) $ pip install tensorflow  #  or tensorflow-gpu, if GPU support is needed.\n(tfr) $ pip install /tmp/ranking_pip/tensorflow_ranking*.whl \n \n \n Run all TensorFlow Ranking tests. \n shell\n(tfr) $ bazel test //tensorflow_ranking/... \n \n \n Invoke TensorFlow Ranking package in python (within virtualenv). \n shell\n(tfr) $ python -c "import tensorflow_ranking" \n \n \n Running Script \n For ease of experimentation, we also provide the LIBSVM demo in the form of an\nexecutable\n script .\nThis is particularly useful for hyperparameter tuning, where the hyperparameters\nare supplied as flags to the script. \n \n \n Set up the data and directory. \n shell\nOUTPUT_DIR=/tmp/output && \\\nTRAIN=tensorflow_ranking/examples/data/train.txt && \\\nVALI=tensorflow_ranking/examples/data/vali.txt && \\\nTEST=tensorflow_ranking/examples/data/test.txt \n \n \n Build and run. \n shell\nrm -rf $OUTPUT_DIR && \\\nbazel build -c opt \\\ntensorflow_ranking/examples/tf_ranking_libsvm_py_binary && \\\n./bazel-bin/tensorflow_ranking/examples/tf_ranking_libsvm_py_binary \\\n--train_path=$TRAIN \\\n--vali_path=$VALI \\\n--test_path=$TEST \\\n--output_dir=$OUTPUT_DIR \\\n--num_features=136 \\\n--num_train_steps=100 \n \n \n TensorBoard \n The training results such as loss and metrics can be visualized using\n Tensorboard . \n \n \n (Optional) If you are working on remote server, set up port forwarding with\n    this command. \n shell\n$ ssh <remote-server> -L 8888:127.0.0.1:8888 \n \n \n Install Tensorboard and invoke it with the following commands. \n shell\n(tfr) $ pip install tensorboard\n(tfr) $ tensorboard --logdir $OUTPUT_DIR \n \n \n Jupyter Notebook \n An example jupyter notebook using\n the LIBSVM format \nis available in  tensorflow_ranking/examples/tf_ranking_libsvm.ipynb . \n \n \n To run this notebook, first follow the steps in installation to set up\n     virtualenv  environment with tensorflow_ranking package installed. \n \n \n Install jupyter within virtualenv. \n shell\n(tfr) $ pip install jupyter \n \n \n Start a jupyter notebook instance on remote server. \n shell\n(tfr) $ jupyter notebook tensorflow_ranking/examples/tf_ranking_libsvm.ipynb \\\n        --NotebookApp.allow_origin=\'https://colab.research.google.com\' \\\n        --port=8888 \n \n \n (Optional) If you are working on remote server, set up port forwarding with\n    this command. \n shell\n$ ssh <remote-server> -L 8888:127.0.0.1:8888 \n \n \n Running the notebook. \n \n \n Start jupyter notebook on your local machine at\n     http://localhost:8888/  and browse to the\n    ipython notebook. \n \n \n An alternative is to use colaboratory notebook via\n     colab.research.google.com  and open\n    the notebook in the browser. Choose local runtime and link to port 8888. \n \n \n \n \n References \n \n \n Rama Kumar Pasumarthi, Sebastian Bruch, Xuanhui Wang, Cheng Li, Michael\n    Bendersky, Marc Najork, Jan Pfeifer, Nadav Golbandi, Rohan Anil, Stephan\n    Wolf.  TF-Ranking: Scalable TensorFlow Library for Learning-to-Rank. \n KDD 2019. \n \n \n Qingyao Ai, Xuanhui Wang, Nadav Golbandi, Michael Bendersky, Marc Najork.\n     Learning Groupwise Scoring Functions Using Deep Neural Networks. \n CoRR abs/1811.04415 (2018) \n \n \n Xuanhui Wang, Michael Bendersky, Donald Metzler, and Marc Najork.\n     Learning to Rank with Selection Bias in Personal Search. \n SIGIR 2016. \n \n \n Xuanhui Wang, Cheng Li, Nadav Golbandi, Mike Bendersky, Marc Najork.\n     The LambdaLoss Framework for Ranking Metric Optimization .\n     CIKM 2018. \n \n \n Citation \n If you use TensorFlow Ranking in your research and would like to cite it, we\nsuggest you use the following citation: \n @inproceedings{TensorflowRankingKDD2019,\n   author = {Rama Kumar Pasumarthi and Sebastian Bruch and Xuanhui Wang and Cheng Li and Michael Bendersky and Marc Najork and Jan Pfeifer and Nadav Golbandi and Rohan Anil and Stephan Wolf},\n   title = {TF-Ranking: Scalable TensorFlow Library for Learning-to-Rank},\n   booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},\n   year = {2019},\n   pages = {(to appear)}\n   location = {Anchorage, AK}\n}\n']
cwillhelm,['gen.py \n Cryptographically secure password generator --- uses SystemRandom() to generate secure function from ASCII characters. \n There are some things that are still a work in progress, and will be updated when I have the time. \n usage: \n python gen.py passwordLength numberOfPasswords', "Run Hugo in Docker with SSL \n The easiest way to get a blog online with SSL and a valid domain! \n Docker Compose \n In order for SSL Encryption to work, you will need to have a valid domain. You can get one from any DNS registrar of your choice, and setup the appropriate forwarding rules there too.. \n In the  docker-compose.yml  file, you will want to update the following ENV vars in the  web  portion of the file. \n VIRTUAL_HOST=yourdomain.com\nLETSENCRYPT_HOST=yourdomain.com\nLETSENCRYPT_EMAIL=youremail@domain.com \n Additionally, if you want to change the theme or baseURL for your website, you can add these to the  hugo  portion of the file. \n HUGO_THEME=your-theme\nHUGO_BASEURL=yourdomain.com\nHUGO_REFRESH_TIMEOUT=3600 \n Further Configuration \n The  /src  directory is where you can modify all of hugo's settings, and when you re-run the  docker-compose  file those changes will be output to the  output  directory. You can add and reference themes like how you normally would using hugo outside of docker. "]
finaldie,['\n \n \n \n \n \n \n Common Libraries (Linux Platform) \n Lastest Release \n See  Here \n ChangeLog \n See  change log \n Library contains \n Lib Name | Description |\n---------|-------------|\nflist    | Lockfree FIFO single-linked list in one-producer one-consumer scenario |\nfdlist   | Double-linked list |\nfhash    | Hash table |\nflock    | A wraper, which is safer and easier to use pthread condition |\nflog     | A High Performance Logging Library |\nfmbuf    | A light-weight ring-buffer |\nfconf    | A simple configuration file library |\nftime    | Easy to create system timer |\nfthread_pool | Simple thread pool, which is easy to use |\nfnet     | Wrap the system APIs, easy to use the networking api |\nfev      | Event framework, including buffer, networking, timer service... |\nfcache   | A simple cache with LRU |\nfco      | C coroutine.  Notes:  Legacy library, use it carefully |\n~~fut~~  | C Unit test framework.  Notes:  Migrated to  fcunit  |\n~~fmempool~~ | Thread-Cache memory pool.  Notes:  Migrated to  skull-malloc  |\n~~fpcap~~| Pcap file conversion library.  Notes:  Migrated to  fpcap  | \n API Documents \n See  Wiki \n Compile \n console\ngit clone git@github.com:finaldie/final_libs.git flibs\ncd flibs\ngit submodule update --init --recursive\nmake\nmake check\nmake valgrind-check \n Benchmark \n console\nmake bench\nmake bench-run \n Flags \n \n Change the compiler, such as using clang:\n console\nmake CC=clang \n Build shared library instead of static library:\n console\nmake SHARED=true \n Build debug version without any optimization parameters\n console\nmake debug=true \n Build  32 bit libraries under  64 bit platform\n console\nmake BIT=32 \n Build in parallel\n console\nmake -j4   # Adjust the number \'4\' according to the real cpu cores to speed up\nthe compile time \n Skip building legacy library\nCurrently,  fco  is defined as a legacy library, maybe it won\'t working well or\nlose support in some archs, use a specify macro to skip building it.\n console\nmake FLIB_CFLAGS=-DFLIB_SKIP_LEGACY \n Custom CFLAGS and LDFLAGS\nSometimes, we want to define some different macros/compilation flags to control\ncompiling/linking results, in  flibs , instead of standard  CFLAGS  and\n LDFLAGS , we can use  FLIB_CFLAGS  and  FLIB_LDFLAGS \n \n Example (on 64bit platform) \n \n Build 64bit static-link libraries\n console\nmake -j4 && make -j4 check \n Build 64bit dynamic-link libraries\n console\nmake SHARED=true -j4 && make SHARED=true check \n Install  flibs  to system\nAfter compilation, just call  install  target, to install  flibs  into system:\n console\nmake install \n notes:  By default, the  flibs  will be installed in  /usr/local/ : \n Headers in:  /usr/local/include \n Libraries in:  /usr/local/lib \nIf we want to change the location, maybe to  /usr/ , just run:\n console\nmake prefix=/usr/ install \n \n Use  flibs  in a Project \n After installing  flibs  into system, basically we need few steps to use it:\n * Include the headers from the source file\n * Link the statis/dynamic library from the  Makefile \n Let\'s see an example  Source file  and  Makefile , e.g.  fhash :\n```c\n// main.c \n include  \n include  \n include  \n int main(int argc, char*  argv)\n{\n    // 1. Create string hash table\n    fhash  tbl = fhash_str_create(0, FHASH_MASK_AUTO_REHASH); \n // 2. Set a key-value into table\nconst char* key = "hello";\nfhash_str_set(tbl, key, "world");\n\n// 3. Get the value from table\nconst char* value = fhash_str_get(tbl, key);\nprintf("Key: %s, value: %s\\n", key, value);\n\n// 4. Destroy the hash table\nfhash_str_delete(tbl);\nreturn 0;\n \n }\n makefile \n Makefile \n all:\n    gcc -Wall -g -O2 -o demo main.c -lflibs\n``` \n Then Build and Run it:\n console\nfinal@ubuntu1404: ~/code/github/flibs/demo>make\ngcc -Wall -g -O2 -o demo main.c -lflibs\nfinal@ubuntu1404: ~/code/github/flibs/demo>./demo\nKey: hello, value: world \n Have fun :)', "Lastest Versions: \n +--------+--------------------+------------+--------------------------------------+\n| Branch | Lastest Stable Tag |  Released  | Description                          |\n+--------+--------------------+------------+--------------------------------------+\n|  0.2   |                    |            | Under development                    |\n+--------+--------------------+------------+--------------------------------------+\n|  0.1   | 0.1.5              | 2013-07-10 | 4 basic types of response simulation |\n+--------+--------------------+------------+--------------------------------------+\n \n Description: \n A http mock service, for performance testing, debugging or something else.\nIt support 4 types of response:\n    1. specific length response\n    2. chunked response\n    3. mix type ( contain two types above )\n    4. replay pcap file data, to simulate the real server response\nNOTE: The response content is all filled by 'F'\nNOTE: It require kernel version greater than 2.6.31\nNOTE: It require libpcap library\n \n How to Run: \n 1. ./bootstrap\n2. make\n3. make install\n4. cd bin && ./httpd_mock -c ../etc/httpd_mock.cfg\n \n ChangeLog: \n 0.1.5 2013-09-04\n  Cleanup the source code environment\n0.1.4 2013-07-29\n  Correct the content-length value in non-chunked response\n0.1.3 2013-07-10\n  Issue #6: Support deal with muddled tcp/ip package and retransmit package\n0.1.2 2013-04-27\n  Fix coredump issue when handle the un-http response header format data\n0.1.1 2013-04-08\n  Support loading pcap file, to simulate the real server response\n0.1.0 2012-12-07\n  basic framework, support 3 types response:\n  * specific length response\n  * chunked response\n  * mix type\n", 'fmagic_note \n Description: \n This is a magic tool, with which you can not only write down and manage\nyour notes easily but also search/edit/run them easily.\n\nYou can also use another wrap tool -> magicnote_git to manage your notes, which\nhold in git repository, that may help you sync your notes easier if you prefer to\nuse git\n \n DEPENDENCE \n Require Lua 5.1+\nRequire Linux or MacOS Operation System\n \n NOTE: \n 1. Better to separate using magicnote and magicnote_git, or your data repository may\nneed extra efforts to repair\n2. If you want to use command auto complete, just include magicnote_complete.bash or\nmagicnote_complete_git.bash in your bashrc\n \n Install: \n ./install.sh\nor\n./install.sh prefix=/usr/local/\n \n Command: \n bash-$ magicnote\nusage:\n  \\_ magicnote addsource\n  \\_ magicnote ls [-p] [tag1 [ tag2 ...]]\n  \\_ magicnote add [-t tagname1 [tagname2 ...]] [file | tag@index]\n  \\_ magicnote rm tag@index [tag2@index2 ...]\n  \\_ magicnote edit tag@index [tag2@index2 ...]\n  \\_ magicnote find [-p] tag1 tag2 ...\n  \\_ magicnote run tag@index [tag2@index2 ...]\n  \\_ magicnote revert\n  \\_ magicnote show [-v]\n  \\_ magicnote gc\n\nbash-$ magicnote_git\nusage:\n  \\_ magicnote_git addsource source\n  \\_ magicnote_git ls [-p] [tag1 [ tag2 ...]]\n  \\_ magicnote_git add [-t tagname1 [tagname2 ...]] [file | tag@index]\n  \\_ magicnote_git rm tag@index [tag2@index2 ...]\n  \\_ magicnote_git edit tag@index [tag2@index2 ...]\n  \\_ magicnote_git find [-p] tag1 tag2 ...\n  \\_ magicnote_git run tag@index [tag2@index2 ...]\n  \\_ magicnote_git status\n  \\_ magicnote_git push\n  \\_ magicnote_git pull\n  \\_ magicnote_git revert\n  \\_ magicnote_git show [-v]\n  \\_ magicnote_git gc\n \n Samples \n addsource \n   bash-$ magicnote addsource\n \n list \n   bash-$ magicnote ls\n  ssh\n    |- @1 #1: ssh 127.0.0.1\n    |- @2 #1: ssh -P 1234 127.0.0.1\n \n run \n   bash-$ magicnote run ssh@1\n  Last login: Mon Dec 17 14:23:10 2012 from localhost\n  Property of Final, Inc.\n  Unauthorized use prohibited.\n  bash:~$ exit\n  logout\n  Connection to 127.0.0.1 closed.\n  bash-$\n \n find \n   bash-$ magicnote find 1234\n  ssh\n    |- @2 #1: ssh -P 1234 127.0.0.1\n  bash-$ magicnote find 123\n  ssh\n    |- @2 #1: ssh -P 1234 127.0.0.1\n', 'fnote_data \n note data', "Fenv \n \n The project is Basic Development Environment, including: \n \n bashrc \n vimrc, plugins \n gitconfig \n ... \n \n This environment is a simple, clean and easy to manage. \n Editors \n \n Vim  latest  8.x  version \n NeoVim   0.3.3  version \n \n Vim Plugins \n The following are the total plugins in here: \n \n Common Plugins \n scrooloose/NERDTree \n easymotion/vim-easymotion \n majutsushi/tagbar \n ludovicchabant/vim-gutentags \n ctrlpvim/ctrlp.vim \n tpope/vim-surround \n Status Bar \n vim-airline/vim-airline \n Git \n tpope/vim-fugitive \n airblade/vim-gitgutter \n Coding \n Valloric/YouCompleteMe \n autozimu/languageclient-neovim \n honza/vim-snippets \n dense-analysis/ale \n Omnifunc Management \n Shougo/deoplete.nvim \n \n Code Browsing/Completion \n | Language   | Language Server                               |\n| ---------- | --------------------------------------------- |\n| C/C++      | YouCompleteMe(libclang), cquery, clangd, ccls |\n| Python     | YouCompleteMe(jedi), pyls                     |\n| Java       | YouCompleteMe(eclipse-jdt)                    |\n| Javascript | YouCompleteMe(tsserver)                       |\n| Bash       | bash-language-server                          | \n Code Linters \n | Language   | Linter                     |\n| ---------- | -------------------------- |\n| C/C++      | clangtidy                  |\n| Python     | flake8, pylint             |\n| Bash       | shellcheck                 |\n| YAML       | yamllint                   |\n| Markdown   | alex, write-good           |\n| Json       | jsonlint                   |\n| XML        | xmllint                    |\n| HTML       | htmllint, alex, write-good |\n| Javascript | eslint                     | \n Code Fixers/Formatters \n | Language   | Fixers/Formatters |\n| ---------- | ----------------- |\n| C/C++      | clang-format      |\n| Python     | isort, yapf       |\n| Bash       | N/A               |\n| YAML       | prettier          |\n| Markdown   | prettier          |\n| Json       | jq, prettier      |\n| XML        | xmllint           |\n| HTML       | prettier          |\n| Javascript | prettier, eslint  | \n All-in-one Docker Image \n Besides go through the  Installation  steps, the eaisest way to use this environment is pulling the  Docker Image , and start it directly: \n Pull Docker Image \n console\ndocker pull finaldie/dev \n This docker image is based on  Ubuntu 18.10  release, and includes all basic build toolchains which work out of box. \n Start \n From the project you want to jump in the dev environment, then type: \n console\ndocker run -it --rm -v `pwd`:/workspace finaldie/dev \n Then, we enter the dev environment with all the scripts we want. \n Notes:  Configure above into  .bash_profile  would save a lot time in the future. \n console\nalias dev='docker run -it --rm -v `pwd`:/workspace finaldie/dev' \n How to Install \n If we want to build it from scratch, follow the step 1 to 4 :) \n Step 1 - Clone and Deploy \n First click  Fork  Button, then clone the code from your own repository and install it \n c\ngit clone git@github.com:$yourname/final_dev_env.git fenv\ncd fenv\ngit submodule update --init\nmake \n Step 2 - Link Bashrc \n \n After  make , you will see the following text: \n \n bash\n  if [ -f /path/to/fenv/user_env/all.bashrc ]; then\n      . /path/to/fenv/user_env/all.bashrc\n  fi \n \n Copy this into your own ~/.bash_profile \n Restart Bash \n \n NOTE:  After that, the two global environment variables already there: \n \n FENV_HOME - All the bashrcs, git config and vim plugins here \n FENV_GIT - the place of this repository in your computer \n \n Step 3 - Install Vim Plugins \n All the vim plugins are managed by  vim-plug , so you need to install them for the first time \n \n Run  vim +PlugInstall +qall \n \n NOTE:  You also can open vim and run  :PluginInstall  for the first time \n NOTE:  For updating plugins, you can run  :PluginUpdate  to update all your plugins \n Step 4 - Special Plugin \n For who also install the  YouCompleteMe , to complete this plugin's installation step: \n \n go to its folder  $FENV_HOME/vim/YouCompleteMe \n Run  ./install.py --clang-completer \n \n Using System Clang \n If in Mac, use system clang sometimes may be a solution, but not recommended \n \n go to its folder  $FENV_HOME/vim/YouCompleteMe \n Run  ./install.py --clang-completer --system-libclang \n \n NOTE:  If you are a Mac User, please download the lastest  MacVim . \n Enjoy \n Now, your development environment is ready, any question/problem please file an issue here. If you also like it, please  Star  it. \n Furture Read \n Update Vim Plugins \n If you only want to update your vim plugins, just run the following in your shell: \n bash$ vim +PluginUpdate +qall \n Update all scripts/configs \n make \n NOTES:  This won't break your vim plugins, only re-deploy your scripts/configs \n Install Powerline-Font \n To give a better experience, install  SourceCodePro  font for the terminal.  \nDownload it and install it in the laptop, and configure it from the terminal settings. \n Contribution \n If you have more suggestions or recommendations, it's great to send a pull request.", 'rasptools \n raspberry tools, scripts', '\n ftracer \n A toolkit for tracing C/C++ program(including multi-thread program), it\'s used to generate a time-line based callgraph, which will guide us to understand the core of the program extremely fast at the beginning. \n The devil is in the details... \n Imagine that, if there is a 10 years old and huge project will be transferred to us, and the corresponding requirements will come soon as well (like the new features, bug fix..), how can we locate to the core path in a very limited time? So we need a powerful tool to help us. This time-line based callgraph will save us a lot of time to deal with the complex details, we can focus on the more important things, and make the life easier. \n And also we can use it as a source code index in the entire working cycles, especially for a very complex program. Everytime we want to remember something from the high level, we can use it, just follow the callgraph flow, to see how it works, as well as we might identify some problems from it :)  \n There are some existing  Callgraphs  for the  RocksDB ,  Lua  and  Redis , we can use them directly without any addtional efforts. \n \n Before Start \n Please note that, the tracer works for us in the following scenarios:\n* We can touch the source code, and easy to re-compile it in our dev environment\n* We need a time-line based call graph \n If NOT, we might need some other callgraph tools. \n Exceptions \n \n The program has core dump issue, it may not help us, currently we should use gdb to fix it first \n \n How to Use it \n Check out \n bash\ngit clone git@github.com:finaldie/ftracer.git \n Compile ftracer \n bash\nmake \n Re-Compile Target Program \n To make the tracer working, we should re-compile the application with  -g -finstrument-functions  flags\n bash\nmake CFLAGS+="-g -finstrument-functions -O0" \n NOTE:  Make sure there is no optimization option like  -O2 , if exist, replace it with  -O0  or just drop it. Btw, we can try the  examples  in ftracer \n Generate Call Graph Report \n \n \n PRELOAD ftracer.so in the wrapper script\n    ```bash\n    bash $ cat run.sh\n    #!/bin/sh\n    export LD_PRELOAD=/path/to/ftracer.so:libdl.so \n ./yourapp\n``` \n \n \n Run it\n     bash\n    ./run.sh \n \n \n Generate the Report\n     bash\n    cd tools\n    ./gen_report.sh -e yourapp -f /tmp/trace.txt \n \n \n Advanced \n env variables \n \n \n Start tracer when entering into a specific function address\n     export FTRACER_FUNC_ENTRY=xxx  # xxx is the function address, like 0000123 \n \n \n Start tracer when receiving a specific signal\n     export FTRACER_SIG_NUM=10 # 10 is SIGUSR1, kill -s SIGUSR1 PID to start tracer \n \n \n Specific a output tracer file\n     export FTRACER_FILE=/tmp/your_tracer_file \n \n \n NOTE:  About the two features  signal  and  function address entrance , they can not enable in the same time, if that, the signal feature will not be take effect. \n gen_report Options \n \n \n -e  Program Location \n The absolute program path, for example  /bin/ls \n*  -f  Raw trace file dumped by the program \n By default, the raw trace file is at  /tmp/trace.txt , use  -f /tmp/trace.txt  all the time should be OK\n*  -s  Regex Symbol Filter \n Sometimes, we deal with C++ programs, there are a lot of noises in there, like  std ,  boost ... so we should filter them out. \nFor this, we should use  -s  arg, for example:\n console\ngen_report.sh -e app -f /tmp/trace.txt -s "^std::" \n*  -S  Filter by file/path \n For this, the  -S  arg will help us, for example, if we want to filter all the c++ related information out:\n console\ngen_report.sh -e app -f /tmp/trace.txt -S /include/c++ \n*  -p  Keep at most N level of path \n If a path is too long, it will be a noise for us, so the -p parameter will help to keep at most N level of path, for example, there is a path  /path/a/b/c/d.c , use  -p 1  the path in the report will be  c/d.c . \nIf no  -p  or  -p  value is a negative number, this feature will be ignored \n \n \n -o  Specific output folder \n The default output folder is  /tmp , but if we want to specific another folder, \n -o output  will help us. \n \n \n -d  Don\'t cleanup the temporay data \n If we get some wrong data when running  gen_report.sh , the temporay data will help us to debug what\'s happened, so if we want to debug it, pass the  -d  paramter. \n \n \n -r  Read symbols from the dynamic libraries \n Most of the time, we only care about the symbols from the program itself, but in case we want to read the symbols from the dynamic libraries, enable this flag then. Please  note , enable this flag will slow down the report generation. \n \n \n -v  Show debug info \n If we need more information during the report generating, pass  -v  in \n \n \n -t  Start N process to generate report \n The addr2line is slow, sometimes we need to start N processes to generate the report in parallel, it will reduce the generating time. For example  -t 4 \n \n \n -F  output format \n Default output format is  plain , and we also can specific  html  format, for example  -F html , this will be greatly helpful when we are dealing with a very big call graph.  Lua5.3.3 Callgraph \n \n \n Enjoy and Analyze the Report \n For now, open the  /tmp/trace_report.txt.threadid  and enjoy it. The example like:\n c\n 1x main(/home/username/github/ftracer/example/test.c:44) - (called from ??:0)\n.. 3x a(/home/username/github/ftracer/example/test.c:36) - (called from test.c:45)\n.... 1x b(/home/username/github/ftracer/example/test.c:21) - (called from test.c:39)\n...... 1x c(/home/username/github/ftracer/example/test.c:16) - (called from test.c:25)\n.... 1x b(/home/username/github/ftracer/example/test.c:21) - (called from test.c:39)\n...... 2x d(/home/username/github/ftracer/example/test.c:11) - (called from test.c:27)\n...... 1x e(/home/username/github/ftracer/example/test.c:6) - (called from test.c:31) \n More detail see the  example , and  Contact Me , let\'s do it better :D', 'convert2markdown \n Convert doxygen xml output to markdown format, which will be useful for writing github api wiki. \n How to Install \n No need to install it, just clone it is enough.\n git clone git@github.com:finaldie/convert2markdown.git \n How to Use it \n Prepare doxygen config \n Take  flibs  as an example\n```bash\ndoxygen -s -g \n ```\nThen modify it like [this][2] \n Generate the doxygen xml format output \n bash\ndoxygen doxy.config \n Convert xml to markdown format \n bash\nsrc/xml2markdown.py -f doc/xml/target.xml > out.md \n [2]: ', "\n \n \n \n \n \n \n \n Skull \n Skull  is an event-driven serving framework with multiple modern designs to allow user:\n* Fast to create a prototype\n* Easy to maintain even a huge project\n* Write code in lock-free environment \n It helps people to build an application/server extremely fast and strong, high scalability and flexibility in application layer, and with strong performance in engine level. Read more  here . \n Skull  can be used in generic serving layer or embedded device. E.g.  web logic server ,  game server , etc. Want an example? Check this  DNSTurbo  project, and watch its  Trailer on Youtube  for a quick start :) \n Releases \n Changelog \n Latest Feature Demo \n Memory tracing reports :\n*  Memory Usage \n*  Cross-Scope Malloc/Free \n*  Memory Leak \n \n Documentations \n \n High Level Introduction \n How To Start \n How to Test \n How to Deploy \n Monitoring \n API Docs - Cpp \n API Docs - Python \n Integrate with Nginx \n \n How to Build \n Use  Ubuntu18.04  as an example. (Tested on  Debian jessie/stretch ,  Ubuntu 14.04/16.04/18.04 ,  alpine 3.6/3.8 ,  RHEL6/7  and  Raspbian jessie/stretch ) \n Basic System Requirements \n Skull  requires:\n -  Platform :  Linux \n -  C/C++ :  Compiler supports at least  c99 / c++11  standards or higher \n -  Python :  Python3 \n Install Dependencies \n ```console \n Install System Dependencies \n sudo apt-get install autoconf libtool valgrind expect python3-dev python3-pip;\nsudo pip3 install PyYAML pympler WebOb; \n Clone and Build Dependencies (For example: project folder is 'skull') \n cd skull && git submodule update --init --recursive; \n Compile and install dependencies \n make -j4 dep; sudo make install-dep;\n``` \n Build and Install \n console\nmake -j4\nsudo make install \n Notes: \n * To disable  jemalloc , use  make -j4 disable_jemalloc=true  to build it\n * For some  Linux  Releases, we might need to use  CFLAGS ,  CXXFLAGS ,  LDFLAGS  to finish the build \n More Options: \n *  python_path : By default it's python3, but we can override it to another path for testing purpose \n Docker Images \n Also, the  Docker images  are ready now, if people don't want to waste time to set up a brand new environment, we can run the  Docker  image directly within 1 min :) \n Assume that we've already  installed  Docker , then apply the following commands to pull and run the image to bring user into a development ready environment super fast:\n console\n-bash$ docker pull finaldie/skull:1.2-build\n-bash$ docker run -it finaldie/skull:1.2-build \n And the below table is the current images: \n Tag              | Dockerfile         | Notes                         |\n-----------------|--------------------|-------------------------------|\n1.4              | ( Dockerfile ) | 1.4 Debian runtime image      |\n1.4-build        | ( Dockerfile ) | 1.4 Debian dev/building image |\n1.4-ubuntu       | ( Dockerfile ) | 1.4 Ubuntu runtime image      |\n1.4-ubuntu-build | ( Dockerfile ) | 1.4 Ubuntu dev/building image |\n1.4-alpine       | ( Dockerfile ) | 1.4 Alpine runtime image      |\n1.4-alpine-build | ( Dockerfile ) | 1.4 Alpine dev/building image | \n A Quick Demo \n After installing  Skull  into the system, run the following steps to\ncreate a  Skull  project, have fun :) \n Create a Skull Project \n \n Notes: \n * By default, a new module contains the example code which is used to echo-back message\n * Above creation is for  C++  module, type  py  in language selection step if needed \n Playing with Skull \n \n Existing Services \n Name                  | Description |\n----------------------|-------------|\n DNS Client  | Async DNS client for A/AAAA record |\n Http Client  | Async http client service, easy to send/receive http request/response | \n Other Resources \n Name                  | Description |\n----------------------|-------------|\n Skull-Perf Cases  | Including some basic perf cases |\n DNSTurbo         | Smart DNS Client based on  Skull .  Trailer  | \n Contribution and Discussion \n To discuss any issues, there are some ways we can use:\n - Open an issue on Github directly\n - Discuss at Reddit:  https://www.reddit.com/r/skullengine/ \n - Send an email to skull-engine@googlegroups.com\n - Go directly to the  Mail Group , and post the questions. \n To fix a bug or add a new feature, just  Fork  the repo, then apply the fixes/features via a PR.", 'skull-admin-c (Deprecated) \n Skull Admin module (C version) \n Installation \n cd $SKULL_PROJ_ROOT/src/modules\ngit clone git@github.com:finaldie/skull-admin-c.git admin\ncp admin/idl/admin.proto ../../idls/workflow\ncd admin && skull module --conf-gen\nskull workflow -add # then input concurrent: no, idl: admin, port: yes -> 7759\nskull module -add   # then input the module name: "admin", workflow: "1"\nskull build\nskull deploy \n Usage \n Currently, it only support  show  command, after start  skull ,\nthen try the  show  command by  telnet 0 7759 , for example:\n```\ntelnet 0 7759\nTrying 0.0.0.0...\nConnected to 0.\nEscape character is \'^]\'. \n help\ncommands help:\n - help\n - show \n show\nskull.core.g.global.connection: 1.000000\nskull.core.g.global.response: 1.000000\nskull.core.g.global.latency: 347.000000\nskull.core.g.global.request: 4.000000\nskull.core.t.worker.accept.worker0: 1.000000\nskull.core.t.worker.latency.worker0: 347.000000\nskull.core.t.worker.connection.worker0: 1.000000\nskull.core.t.worker.response.worker0: 1.000000\nskull.core.t.worker.request.worker0: 4.000000\n```', 'openwrt-scripts \n Useful scripts of openwrt', 'skull-malloc \n A light-wight thread cache mempool \n Compile \n make', 'fpcap \n A library for analysis the tcp traffic (based on the libpcap) \n Compile and Install \n Compile the dependency \n For example, in ubuntu: \n apt-get install libpcap0.8 libpcap0.8-dev\nmake dep \n Compile \n make \n Installation \n make install \n notes:  by default, it will be installed to  /usr/local', 'fcunit \n A light-weight unit test framework for C \n API Macros \n FCUNIT_ASSERT(expression)\nFCUNIT_RUN(unit_test) \n Example \n ```c \n include  \n include "fcunit.h" \n void test_sample()\n{\n    int a = 1;\n    int b = 2;\n    FCUNIT_ASSERT(b > a);\n} \n int main(int argc, char** argv)\n{\n    FCUNIT_RUN(test_sample);\n    return 0;\n}\n``` \n Run Example(Output) \n test_sample: ok', 'skull-service-dns \n This is an example of skull service for building a Async DNS client. \n \n Main Features: \n \n [x] Fully Async DNS Query \n [x] Implemented a simple local cache(not LRU) \n [x] Support loading /etc/resolv.conf \n [ ] Support loading /etc/hosts \n [ ] Build a LRU cache \n \n How to use it? \n \n Install the dependence package:  libc-ares-dev  (In Debian/Ubuntu:  apt-get install libc-ares-dev ) \n cd $project_root/src/services \n git clone $this_repo dns \n Use  skull service --import dns  to load this into your skull project \n \n Custom Name Server \n Define a shell environment variable to specify a name server, e.g.:\n  *  SKULL_DNS_NS:   192.168.1.1 \n  *  SKULL_DNS_NS:   8.8.8.8 \n Screen Output \n ```console\nmodule(test): init\nskull service init\nskull service init\ninit name server: 192.168.31.1\nmodule_unpack(test): data sz:6\nreceive data: asdf \n ServiceCall ret: 0\nskull service api: query\ndns query from cache failed\nep dns _unpack len: 59\ndns _ep_cb: response len: 59, status: 0, latency: 62\ngot 1dns replies\n - ip: 172.217.2.36; ttl: 63\ndns query result:\nreturn ip: 172.217.2.36\n_dnsrecord_updating done, domain: www.google.com\nmodule_pack(test): data sz:6 \n module_unpack(test): data sz:9\nreceive data: aaaaaaa \n ServiceCall ret: 0\nskull service api: query\nTry to find a valid ip, ttl: 63\ndns query result:\nreturn ip: 172.217.2.36\nmodule_pack(test): data sz:9\n```', 'skull-ft \n Functional Test Framework', 'skull-perf \n Performance Test Framework for Skull \n Preparation \n First, this project assumed that the  Skull  has already been installed in your environment. \n Build \n \n Clone and Build\n console\ngit submodule update --init --recursive \n \n Run Perf Case \n console\nmake case=echo_back \n Notes:  After you tested one case, run  killall skull-engine  and then start next case. \n Cases \n Case Name                | Description\n-------------------------|------------\necho_back                | A simple echo back server, which accept HTTP GET request and return the response\nmodule_call_service      | A module call a service api, then response to client\nmodule_call_service_ep   | A module call a service api, and in the service api it create a ep client to callout another service\nmodule_call_service_jobr | A module call a server api, and in the servce api it create read-job\nmodule_call_service_jobw | A module call a server api, and in the servce api it create write-job\npython echo back         | A simple python echo back server, which accept HTTP GET request and return the response\npython module call service | A python module call a service api, then response to client\npython module http parsing | A python module handle http parsing, then response a standard http response', "skull service httpcli \n Skull Service of Http Client \n Introduction \n The http client skull-service is for helping us to send http request as well as receive http response easier. We only need to fill out the  idl/http-query_req.proto  from skull-module, then extract the  idl/http-query_resp.proto  when we receive the http response. \n How to Install \n Clone this into  $skull_project_top/src/services , then go to its folder, run:\n console\ngit submodule update --init --recursive\nskull service --import $service_name \n That's it :)", "\n \n \n \n \n \n DNSTurbo \n A middleware between browser and local DNS client/server, to speed up the Internet according to the network speed. It optimizes the A/AAAA records by latency detection mechanism, offer the best records the user wanted. \n Why the GEO-based DNS is not enough? As we know, there are a lot of GEO-based DNS services, like openDNS, google DNS, etc. They can calculate the distance of the user location to the target host, to return the nearest records, but unfortunately it's hard to say the returned records are always the best ones, since the real user experience depends on the user local network quality, for the different time, different ISP and different website, the results are totally different. \n So, it's better to have another program to detect the quality of the DNS records from user's point of view, then filter the bad ones out, and keep the good ones as much as possible.  DNSTurbo  was born under this situation, plug it into the DNS pipeline, always keep the records as good as possible for the users. \n DNSTurbo  do the  Latency Detection , and does not do the  Bandwidth Detection , because it's highly depend on the  Congestion Control (For TCP connections), different congestion module would give the different results, and the  Bandwidth Detection  is not only very heavy especially for the mobile device in  LTE  network, but also would compete with\xa0 Browser  to make the network worse than ever. And another hand, all the records returned should have the very similar network quality, so to maximize the  Bandwidth , we should rely on the target program or  TCP Congestion Control  (Like Linux-4.9, the new  CC  module would try the best to use the  Bandwidth  by the new algo) \n Trailer \n \n Click here to watch the Trailer on Youtube \n Click here to watch the Trailer on Youku \n \n \n Architecture \n \n Compatible Environments \n \n [x] Linux \n [x] 32/64 bit compatible \n [x] x86/ARM compatible \n \n Features \n \n Builtin \n [x] Optimized A Record \n [x] Optimized AAAA Record \n [x] Http Latency Detection \n \n [x] [Docker Image] 3 \n \n \n Coming Soon: \n \n [ ] ICMP Latency Detection \n \n Install Dependencies \n ```console \n Install System Dependencies \n sudo apt-get install autoconf valgrind expect libyaml-dev python3-dev python3-pip libc-ares-dev;\nsudo pip3 install PyYAML pympler WebOb dnslib; \n Fork/Clone and Build Project Dependencies (For example: project folder is 'DNSTurbo') \n cd DNSTurbo\ngit submodule update --init --recursive\nmake dep; sudo make install-dep; make skull; sudo make install-skull;\n``` \n Build \n console\nskull build && skull deploy \n Start \n console\nsudo skull start -D \n Docker Images \n The  Docker images  are ready now, if people who don't want to waste time to build it from scratch, it's super easy to run it via docker image. \n Assume that we've already  installed  Docker , then apply the below commands:\n console\nbash$> docker pull finaldie/dnsturbo:0.8\nbash$> docker run -p 53:53/udp finaldie/dnsturbo:0.8 \n \n Setup \n Now, the DNSTurbo is all set, let's chain it in DNS pipe. For example, the DNSTurbo is set up in  192.168.31.221 \n \n For Mac: \nOpen  System Preferences  ->  Network  ->  Advanced  ->  DNS , then add  192.168.31.221  in the left window. \n \n \n \n For Windows7/10: \nOpen  Network Apaptor  ->  Property  ->  TCP/IPv4 , then select  Manual DNS  option, and add  192.168.31.221  there. \n \n After that, enjoy the new experience :) \n Environment Variables \n Sometimes we don't want to use the default upstream DNS from  resolv.conf , then there is an  Shell Environment Variable  can override it, just define an ENV var  SKULL_DNS_NS , e.g.:\n console\nexport SKULL_DNS_NS=192.168.1.1\nexport SKULL_DNS_NS=8.8.8.8", '\n Dockerfiles \n Dockerfiles for  skull ,  DNSTurbo , etc... \n Skull Dockerfiles \n Supported tags and respective  Dockerfile  links \n \n 1.4 ( Dockerfile ) \n 1.4-build ( Dockerfile ) \n 1.4-ubuntu ( Dockerfile ) \n 1.4-ubuntu-build ( Dockerfile ) \n 1.4-alpine ( Dockerfile ) \n 1.4-alpine-build ( Dockerfile ) \n \n Repo Name \n finaldie/skull \n Pull Images \n console\ndocker pull finaldie/skull:1.4\ndocker pull finaldie/skull:1.4-build \n DNSTurbo Dockerfiles \n Supported tags and respective  Dockerfile  links \n \n 0.8 ( Dockerfile ) \n \n Repo Name \n finaldie/dnsturbo \n Pull Images \n console\ndocker pull finaldie/dnsturbo:0.8', 'misc \n Some random resources', 'Theia-Plantuml \n Self hosted Theia Clould IDE (plantuml extension enabled) with plantuml-server backend. \n \n Prep \n \n Install Docker \n \n Start \n make build\nmake start \n Enjoy \n Open browser, go to  http://localhost:3030 , start planting!!!', '\n**finaldie/finaldie** is a ✨ _special_ ✨ repository because its `README.md` (this file) appears on your GitHub profile.\n### Hi there 👋\n\nHere are some ideas to get you started:\n\n- 🔭 I’m currently working on ...\n- 🌱 I’m currently learning ...\n- 👯 I’m looking to collaborate on ...\n- 🤔 I’m looking for help with ...\n- 💬 Ask me about ...\n- 📫 How to reach me: ...\n- 😄 Pronouns: ...\n- ⚡ Fun fact: ...\n \n\n### [Recent Blogs][0]\n* [Tmux: Network RTT Detection][3]\n* [Data partitioning: Consistent-Hashing][1]\n* [Load Balancer: Will our system survive?][2]\n\nMore on [finaldie.com/blog][0]\n\n \n\n### [Youtube Videos][10000]\n* [System design: Multithreading][10054]\n* [System design: Realtime POI][10053]\n* [Object collision][10052]\n* [Big data transmission][10051]\n* [Service decompsition][10050]\n* [Supply chain simulation][10049]\n* [System Design: Modular hashing][10048]\n* [System Design: Consistent hashing - Virtual node][10047]\n* [System Design: Consistent hashing][10046]\n* [System Design: Layer 7 load balancing][10045]\n* [System Design: Layer 4 load balancing][10044]\n* [System Design: CAP theorem][10043]\n* [Graphics programming: Lightning show][10042]\n\n\n\n \n* [System Design: Cache][10041]\n* [System Design: Message Queue][10040]\n* [Trading bot: Introduction][10039]\n* [System Design: Data Partitioning][10038]\n* [System Design: Rate Limiting][10037]\n* [System Design: Horizontal Scaling][10036]\n* [WebGL Shader: A million points][10035]\n* [System Design: Data Replication][10034]\n* [Data Structure & Algorithm: Closest Point - Bidirectional BFS][10033]\n* [Data Structure & Algorithm: Partition Equal Sum - Dynamic Programming][10032]\n* [Data Structure & Algorithm: Unique Paths][10031]\n* [Data Structure & Algorithm: Maze Escape - BFS][10030]\n* [Data Structure & Algorithm: Longest Substring - Sliding window][10029]\n* [Data Structure & Algorithm: Longest Increasing Sequence - Dynamic Programming][10028]\n* [Data Structure & Algorithm: Minimum Path Sum - Dijkstra][10027]\n* [Data Structure & Algorithm: Shortest Path - BFS][10026]\n* [Data Structure & Algorithm: Rainbow Sort][10025]\n* [Data Structure & Algorithm: Monotonic Stack III][10024]\n* [Data Structure & Algorithm: Monotonic Stack II][10023]\n* [Data Structure & Algorithm: Monotonic Stack][10022]\n* [Data Structure & Algorithm: Prefix Sum][10021]\n* [Data Structure & Algorithm: Binary Indexed Tree][10020]\n* [Data Structure & Algorithm: Bucket-Sort][10019]\n* [Data Structure & Algorithm: Quick-Select][10018]\n* [Data Structure & Algorithm: Cuckoo-Filter][10017]\n* [Data Structure & Algorithm: Count-min Sketch][10016]\n* [Data Structure & Algorithm: Bloom-Filter][10015]\n* [Data Structure & Algorithm: Union-Find][10014]\n* [Data Structure & Algorithm: BinarySearch and Quicksort][10013]\n* [Consensus Algorithm: Distributed Lock][10012]\n* [Consensus Algorithm: Log Replication in Raft][10010]\n* [Consensus Algorithm: Leader Election in Raft][10009]\n* [Load Balancer: Is consistent-hashing good enough?][10001]\n* [Load Balancer: Will our system survive?][10002]\n* [System design: Write-around Cache][10003]\n* [System design: Write-back cache][10004]\n* [System design: Write-through Cache][10005]\n* [System design: Mod-hashing vs Consistent-hashing][10006]\n* [Data partitioning: Hot-key problem][10007]\n* [Message Queue vs Mesh Network][10008]\n* [Consensus Algorithm: Fault Tolerance in Raft][10011]\n \n\nMore on [Youtube][10000]\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  Blog  \n  Blog links  \n  Youtube  \n  Youtube links ']
undefinedbehavior,['ChangeLog \n 12/28/2012\n+ operator precedence parser (operator_precedence_parser.py), which can parse the rule of a operator grammar and\n  generate the priority relationship or priority function (if exists one) according to the input rule file.\n+ regex parser (re.py), which can accept BRE and part of the ERE (still in process). It will parse the regex to NFA\n  and simulate NFA on the fly. Moreover, it will generate DFA state will simulating NFA, and use DFA state instead of\n  NFA state later to achieve higher performance. According to dragon book, this implementation will always achieve a\n  good balance between DFA and NFA.', 'VIMRC by Yirui \n \n Requirement \n \n latest fat vim and python \n to support better theme, change terminal to support 256 color \n to support better status bar, change terminal to use patched font (search powerline-font for detail) \n \n Plugin Special Install \n \n \n in your $HOME/.vimrc, define var VimConfigPath to this vimrc folder and source vimrc.vim under vimrc \n git submodule update --init --recursive \n cd YCM and exe install.sh \n cd vimproc and exe make \n cd zsh-git-prompt and type  cabal build \n Create  ~/vim_projects  folder in order to save the vim session. \n Install tmux plugin manager \n Install tmux plugins as what described above. \n brew install reattach-to-user-namespace \n \n \n TODO \n \n YCM adv setting \n script to do this README \n', '']
rockt,['ChemSpot \n ChemSpot 2.0 is a set of tools for named entity recognition and classification of chemicals in natural language texts, including trivial names, abbreviations, molecular formulas and IUPAC entities. Since the different classes of relevant entities have rather different naming characteristics, ChemSpot uses a combined approach of employing a Conditional Random Field and a dictionary, as well as pattern-based recognition, a classifier model and several methods for consolidating all annotations. ChemSpot also performs named entity normalization by assigning identifiers from several chemical databases. It achieves an F1 measure of 79.0% on the SCAI corpus. \n ChemSpot is released under the Common Public License 1.0 (see LICENSE). \n The warning message "Couldn\'t open cc.mallet.util.MalletLogger resources/logging.properties file." can be ignored. \n Running ChemSpot: \n \n \n Extract chemspot.zip into a directory\n unzip chemspot.zip \n \n \n To tag a sample text file, run\n java -Xmx16G -jar chemspot.jar -t sample.txt -o predict.txt \n \n \n To update the dictionary, run\n java -Xmx5G -jar chemspot.jar -u \n \n \n If you would like to reduce memory consumption and do not need ChemSpot to assign identifiers to chemicals, you can run it without the ids file. Note however that this will completely disable named entity normalization.\n java -Xmx12G -jar chemspot.jar -t sample.txt -o predict.txt -i "" \n If you would like to further reduce the memory footprint, you can run ChemSpot without the dictionary or multi-class model as well. Note however that this will result in worse NER performance.\n java -Xmx7G -jar chemspot.jar -t sample.txt -o predict.txt -i "" -d ""\njava -Xmx9G -jar chemspot.jar -t sample.txt -o predict.txt -i "" -M "" \n Parameters \n \n \n arguments: \n \n -m path to a CRF model file (internal default model file will be used if not provided) \n -s path to a OpenNLP sentence model file (internal default model file will be used if not provided) \n -d path to a zipped set of brics dictionary automata (parameter defaults to \'dict.zip\' if not provided)" \n -i path to a zipped tab-separated text file representing a map of terms to ids (parameter defaults to \'ids.zip\' if not provided) \n -M path to a multi-class model file (parameter defaults to \'multiclass.bin\' if not provided) \n \n \n \n flags: \n \n -e if this flag is set, the performance of ChemSpot on an IOB gold-standard corpus (cf. -c) is evaluated" \n -u if this flag is set, ChemSpot will update the dictionary and ids file \n -T number of threads to create when processing a document collection \n \n \n \n input control: \n \n -c path to a directory containing corpora in IOB format \n -g path to a directory containing gzipped text files \n -t path to a text file \n -f path to a directory of text files \n \n \n \n output control: \n \n -o path to output file \n -I if this flag is set, the output will be converted into the IOB format \n \n \n \n Using ChemSpot in your Code \n ```java\nChemSpot tagger = ChemSpotFactory.createChemSpot("dict.zip", "ids.zip", "multiclass.bin");\nString text = "The abilities of LHRH and a potent LHRH agonist ([D-Ser-(But),6, " +\n  "des-Gly-NH210]LHRH ethylamide) inhibit FSH responses by rat " +\n  "granulosa cells and Sertoli cells in vitro have been compared."; \n for (Mention mention : tagger.tag(text)) {\n  System.out.printf("%d\\t%d\\t%s\\t%s\\t%s,\\t%s%n", \n    mention.getStart(), mention.getEnd(), mention.getText(), \n    mention.getCHID(), mention.getSource(), mention.getType().toString());\n}\n``` \n Reproducing our Results \n \n Download the SCAI corpus (chemicals-test-corpus-27-04-2009-v3.iob.gz) and put it in the same directory \n To reproduce our results, run\n java -Xmx16G -jar chemspot.jar -c chemicals-test-corpus-27-04-2009-v3.iob.gz -o predict.txt -e \n \n Acknowledgements \n We would like to thank Daniel Lowe and Philippe Thomas for many valuable suggestions. ', 'See http://rockt.github.com/SETH/ \n Contributors \n \n Philippe Thomas \n Tim Rocktäschel \n Yvonne Mayer \n Johannes Kirschnick \n Eugene Brevdo \n', 'sprawl', 'talks', 'Tensorflow Implementation of the Multi-Function Recurrent Unit \n \n requires TensorFlow 0.9 \n', 'dl4nlp \n Compile PyTorch Slides \n ```sh\njupyter nbconvert \\\n  RemoveElements --TagRemovePreprocessor.remove_input_tags={\\"Output\\",\\"TensorFlow\\"} \\\n  RemoveElements --TagRemovePreprocessor.remove_all_outputs_tags={\\"Input\\",\\"TensorFlow\\"} \\\n  --to slides --post serve scratch.ipynb \n```` \n Compile TensorFlow Slides \n sh\njupyter nbconvert \\\n  RemoveElements --TagRemovePreprocessor.remove_input_tags=\\{\\"Output\\",\\"PyTorch\\"\\} \\\n  RemoveElements --TagRemovePreprocessor.remove_all_outputs_tags=\\{\\"Input\\",\\"PyTorch\\"\\} \\\n  --to slides --post serve scratch.ipynb']
mlagunas,['GreenHub - Smart gardener \n Introduction \n The system consists of an Android application in the smartphone of the user where he/she can manage the plants that are in the garden. Then, some tasks are created, according to the parameters collected by some sensors connected to a microcontroller in the garden, in order to guide the user throughout the process of sowing, watering and harvesting plants. In order to know what plant to plant the user will be able to search information about different plants. \n The microcontroller that we chose is an Arduino UNO with different sensors for measuring temperature, humidity and lightness. Data gathered in the garden is stored in the cloud and, then, retrieved by the phone to analyse it. Moreover, we created a plant database in the cloud so that the phone memory is not full of plants, so we synchronize the device with the cloud every time the user launches the app or when an action is performed on the plants. \n GreenHub consists of three parts: \n \n Smartphone application : we developed an Android application which acts as the main part of the project. In the application the user can manage his/her plants and keep up with the tasks. \n Meteorological station : the user must install a microcontroller and three sensors in order to gather data about the ambient conditions. The user will not interact with this device. \n External server : as mentioned before, we developed an external server to store all the plants. We synchronize the device with the cloud every time the user launches the app or when an action is performed on the plants. In this way, the same user can access his/her account on different devices. Moreover, we also store in this server the ambient conditions measured by the microcontroller. \n \n System description \n GreenHub is a gardening assistant for amateur gardeners who want to engage sowing, flourishing and harvesting their own plants. \n In our case, we decided to use the Android framework because Android development was one the courses we were attending. Besides, Android has a bigger share of the smartphone market so our app could be more used. On the other hand, we chose Arduino UNO board because it has a similar microcontroller to the one we are learning in the corresponding course and because it allowed us to use a WiFi connection. In addition, Arduino has a very big community and lot of libraries to support our project. \n Hardware and software requirements \n Arduino UNO board \n The Arduino UNO board is a microcontroller board based on the ATmega328. We decided to use this board because it provided us with the necessary features for the system. It allows us to connect the sensors so that we collect the data from the ambient, as well as using a WiFi shield in order to send that data to an external server and then, retrieve it with the smartphone. \n Sensors \n GreenHub is composed by three sensors which measure the ambient conditions in real time. We use one sensor alone for measuring lightness (TEMT6000) and a combined one for measuring humidity and temperature (DHT11). The latter one already had an Arduino library available which we implemented in our code. The lightness sensor, however, we have had to connect it as an analog input, so that we read the lightness parameter and convert it to digital with the ADC integrated in the Arduino board. \n External database \n We decided Microsoft Azure for the external database where we store the list of plants, the input from the sensors and the weather conditions for each plant. Microsoft Azure is a cloud computing platform and infrastructure, created by Microsoft, for building, deploying and managing applications and services through a global network of datacenters. \n We decided to use this external server in the cloud in order to allow the user synchronize his/her garden across different devices. Hence, the database must have two different plant tables: one with all the plants available for the user to add to his/her garden and another one with the added plants. On the other hand, the application must download the ambient conditions every hour to check if the user should perform a task in order to keep his/her plants. \n Moreover, we decided to store the user credentials in a separate MySQL database. This database is saved in an external PHP server in one of our laptops. We chose to use this approach apart from Azure so that we also worked with a home-made server.', 'lmUR \n Description \n lmUR is an application used to move a robot, the UR10, which is controlled by an input device. The different input devices used in the project are Leap Motion (Main goal of the project), a joystick (Logitech 3D extreme pro) and a keyboard. All of them send different coordinates to the robot through a computer. Furthermore, the background used is the operating system ROS, an operating system made to work with robots. This background enables the transfer of information easily between the robot and the input devices. All the drivers are already programmed inside lmUR. \n Installation \n In order to run the application we need at least one of the input devices mentioned above and the UR10 connected with the computer with an RJ45 cable, robot and the computer must be in the same network. Also, you need the library pyGame installed and ROS Hydro, running on Ubuntu. \n Usage \n First of all we have to go inside the folder catkin_ws, once we are there we have to run  catkin_make  to compile all the files. After that the command  rosrun lmur ur10_controller.py  has to be run. Now we can easily control the robot regarding the position of the tool or regarding the position of the joints and we can move it using either Leap Motion, the Joystick or the Keyboard.', "Hackathon_zaragozapps \n \n Description \n Application developed during the Zaragozappstore's Hackathon.", 'PilarApp', 'RecuperacionInformacion \n This repository has all the practical work we have done for the subject: Recuperacion de la Informacion', 'informatica-grafica \n Código de los trabajos de la asignatura de Informática gráfica', 'Spam_Filter \n Spam filter implemented in python using Naive Bayes, during the course Artificial Intelligence. \n This programme use the enron folders which contains examples of ham/spam mails, you can find them here https://www.cs.cmu.edu/~./enron/\nYou should edit the line of the code to update the path to the folders in order to use it.', 'OpenCV_Android \n Code for the course Computer Vision', 'SpaceApps 2016 \n Hub with links to all the repositories with the code developed for the hackathon SpaceApps 2016.\nIt includes the backend, mobile app and web app.', 'Huffman compressor/ decompressor \n Code developed for educational purposes during the course Basic algorithms at the University of Zaragoza, Spain.\nThe code implements a basic Huffman compressor/ decompressor. \n USAGE \n Compress/ decompress \n To compress or decompress call the script with -c (compress) or -d (decompress)\n python huffman.py -c|-d file \n    -c : To compress file, compressed file will be "file.huf" \n -d : To decompress file without .huf at the end, decompressed file will be "file"\n\nfile: file to compress/decompress\n \n Checking the resulting file \n In case you want to test 2 files call the script with -t flag, it will print the differences between both files.\n python huffman.py -t file1 file2 \n    -t : Testing mode \n file(1,2): files to compare\n', 'Palindrome creation \n Code developed for educational purposes during the course Basic algorithms at the university of Zaragoza. This repository has\nimplemented a basic palindrome creator with the dynamic programming technique. It is based on the edit distance problem (Levenshtein distance). \n USAGE \n python palindrome.py text\n\ntext is the string to transform into a palindrome\n', '[Under construction] \n Evolutionary Algorithms Library (EAL) \n The following library wraps the evolutionary process of the evolutionary algorithms to make them easier to use.\nIt has a modular structure that makes easy to implement new operators for the selection, crossover, mutation, replacement operations or optimization functions. \n The EAL library includes:\n- [x] Single-run Logger\n- [x] Multi-run Logger\n- [x] File logging and plotting\n- [x] Optimization Functions\n- [x] Built-in simple mutations\n- [x] Built-in simple crossovers\n- [x] Built-in simple selection methods\n- [x] Built-in simple replacement methods\n- [x] Genetic Algorithms process\n- [x] Evolutionary strategies process (simple version (1 sigma))\n- [ ] Evolutionary strategies (array of sigmas)\n- [x]  A Grid-based Genetic Algorithm for Multimodal Real Function Optimization  by Jose Chaquet and Enrique Carmona \n Optimization functions \n The optimization functions are adapted pieces of code, obtained from the  web \n In particular, the library has implemented the following functions:\n-  Ackley \n-  Forrester \n-  Beale \n-  Rothyp \n-  Booth \n-  Easom \n-  Griweank \n-  Matyas \n-  Powell \n-  Zakharov \n-  Sphere \n-  Schwefel \n-  Rastrigin \n An example of some of the functions plotted can be found  here \n EAL class \n The EAL class is a wrapper of the evolutionary process. It accepts a great number of parameters that\nset up the evolutionary algorithm.\nIt puts together all the implemented features and operators and supports different\nevolutionary paradigms like:  Genetic Algorithms (ga) ,  Evolution Strategies (es)  or  Grid-Based Genetic Algorithms (gga) \n Initialization Parameters \n \n n_dimensions : number of dimensions used to solve the problem.  [10] \n n_population : size of the population (its number of chromosomes).  [100] \n n_iterations : number of iterations to do.  [1000] \n n_children : number of childrens to generate.  [100] \n xover_prob : croosover probability.  [0.8] \n mutat_prob : mutation probability.  [0.1] \n minimization : True if we want to minimize the objetive function.  [False] \n seed : set the seed for reproducibility.  [12345] \n initialization : How to initialize the population. [\'uniform\'] \n \'permutation\' : Each chromosome is a permutation of n_dimensions. \n \'uniform\' : Initialize each chromosome randomly sampling it from a uniform distribution. \n problem : sets the objetive function.  [Acley] \n )]* \n selection : sets the selection function (check  Selection section ).  [\'wheel\'] \n \'Tournament\' : \n \'Wheel\' \n \n \n crossover : sets the crossover operator (check  Crossovers section ).  [\'blend\'] \n \'one-point\' \n \'one-point-permutation\' \n \'two-point\' \n \'blend\' \n \n \n mutation : sets the mutation operator(check  Mutation section ).  [\'non-uniform\'] \n \'pos-swap\' \n \'uniform\' \n \'non-uniform\' : \n \'gaussian\' \n \n \n replacement : (check  Replacement section ).  [\'elitist\'] \n \'Elitist\' : \n \'Worst-parents\' : \n \n \n tournament_competitors : number of competitors in the tournament selection.  [3] \n tournament_winners : number of winners in the tournament selection.  [1] \n replacement_elitism : rate of eletism for the eletist replacement.  [0.5] \n \n Initializations \n [TODO]: rewrite and explain each method\n-  uniform  uses a uniform distribution to sample the elements.\n-  permutation  creates a permutation of  N  elements. \n Selections \n [TODO]: rewrite and explain each method\n-  wheel : sample from the parents population with a probability of each member proportional to the value of their fitness\n-  tournament : \n Mutations \n [TODO]: rewrite and explain each method\n-  position swap :\n-  uniform :\n-  non-uniform :\n-  gaussian : (Note: this is the mutation used for Evolutionary Strategies (es)) \n Crossovers \n [TODO]: rewrite and explain each method\n-  one-point :\n-  one-point (permutation) :\n-  two-point :\n-  blend : \n Replacements \n [TODO]: rewrite and explain each method\n-  worst-fitness : removes the chromosomes inside the parensts population with the worst fitness.\n-  elitist : \n Code example \n Use of Genetic Algorithms \n ```python\nfrom evolutionary import EAL, optim_functions as functions \n Example of a Genetic Algorithm to solve the ackley function \n eal_ga = EAL(\n    seed=82634,\n    minimization=False,\n    problem=functions.Ackley,\n    n_dimensions=10,\n    n_population=100,\n    n_iterations=1000,\n    n_children=100,\n    xover_prob=0.8,\n    mutat_prob=0.1,\n    selection=\'wheel\',\n    crossover=\'blend\',\n    mutation=\'non_uniform\',\n    replacement=\'elitist\'\n)\neal_ga.fit(type="ga")\n``` \n Graph returned after the 1000 iterations using a Genetic Algorithm with Wheel selection, Blend\nCrossover, Non-Uniform mutation and Elitist replacement. It tries to find the global minima in the\nAckley Function with a croosover probability of 0.8 and mutation probability of 0.1. The number of \nchromosomes in the population is 100 and the number of generated children is also 100. \n \n Use of Evolutionary Strategies \n ```python\nfrom evolutionary import EAL, optim_functions as functions \n Example of a Evolutionary Strategy to solve the ackley function \n eal_es = EAL(\n    seed=82634,\n    minimization=False,\n    problem=functions.Ackley,\n    n_dimensions=10,\n    n_population=50,\n    n_iterations=1000,\n    n_children=50,\n    xover_prob=0.8,\n    mutat_prob=0.2,\n    selection=\'tournament\',\n    crossover=None,\n    mutation=\'gaussian\',\n    replacement=\'elitist\'\n) \n eal_es.fit(type="es")\n``` \n Graph returned after the 1000 iterations using an Evolutionary Strategy with Tournament selection, it doesn\'t apply any croosover operator, Gaussian mutation with a global sigma value and Elitist replacement. It tries to find the global minima in the\nAckley Function with a mutation probability of 0.1. The number of \nchromosomes in the population is 50 and the number of generated children is also 50. \n', 'Training a clip-art image classifier in Torch-7 with VGG19 \n This code shows how to fine-tune the net VGG-19 so it can precisely predict clip-art images into 23 classes. We fine-tunned the network based on the assumption that the different between natural images (photographs) and illustrations (cliparts) rely on the low-level features of the images. \n The classification is done using a Support Vector Machine (SVM) whose input is the second Fully-Connected layer (FC) of the fine-tunned VGG19. \n Required libraries and implementation \n The following libraries have been used and are required to run the code: \n \n cutorch \n cudnn \n cunn \n hdf5 \n matio \n \n Options \n ```lua\ncmd:option(\'-option\', "", \'[classify] to classify images.\n                           [path] to get the paths from a group of images.\') \n cmd:option(\'-net\',         "trainf",  \'CNN to use [trainf (optimized vgg19)|vgg19]\')\ncmd:option(\'-backend\',     "cudnn",   \'[cudnn|nn]\')\ncmd:option(\'-svm\',         false,     \'[true] to use the SVM to classify features\')\ncmd:option(\'-layer\',       42,        \'Layer of the network where we will extract the features\')\ncmd:option(\'-batch\',       10,        \'Size of the batch\') \n cmd:option(\'-dataset\',     "",        \'Name of the dataset we are using\')\ncmd:option(\'-path\',        "",        \'Path to the image paths\')\ncmd:option(\'print_output\', false,     \'Either to print the predictions output or to keep it clear\')\ncmd:option(\'-test\',        "",        \'Some string to make a difference between test and final results\')\ncmd:option(\'-out\',         "",        \'Name of the outputfile. Do not give any extension to it.\'\')\ncmd:option(\'-keep\',        false,     \'Either to keep the temporary files or to remove them\')\ncmd:option(\'-nresults\',    10,        \'Number of predictions to show, sorted in decreasing order\') \n cmd:option(\'-image_paths\', "",        \'Path to the folder with the images to extract. The structure has to be folder/train/class/image\')\ncmd:option(\'-image_out\',   "",        \'Path to store the files where the images splitted in train, val and all together\')\n``` \n Extract the paths \n In order to extract the paths from each image of the dataset (assuming our dataset is located at  ../data/curated/ ) we have to run the following commands (NOTE that the ouput should be  *_train.txt  and  *_val.txt ):\n```\nth main.lua -option path -image_out ../data/paths/paths_train.txt -image_paths ../data/curated/train/ \n th main.lua -option path -image_out ../data/paths/paths_val.txt -image_paths ../data/curated/val/\n``` \n Classify Images \n To classify a group of images we have to especify the dataset name, the network to use and the path to the file with the paths of the images in that dataset. Also we can add the flag  -svm  to classify it using the SVM. Without this flag the predicitions will be done using the CNN. \n ```\nth main.lua -dataset ../data/curated -option classify -net trainf  -path ../data/paths/paths -svm \n th main.lua -dataset ../data/curated -option classify -net vgg19  -path ../data/paths/paths\n``` \n Citation and details \n Details about the method used can be found in the following paper:  paper . \n If you find it useful for your research please cite:\n @inproceedings {ceig.20171213,\n    booktitle = {Spanish Computer Graphics Conference (CEIG)},\n    title = {{Transfer Learning for Illustration Classification}},\n    author = {Lagunas, Manuel and Garces, Elena},\n    year = {2017},\n    publisher = {The Eurographics Association},\n    ISSN = {-},\n    ISBN = {978-3-03868-046-8},\n    DOI = {10.2312/ceig.20171213}\n} \n This software is published for academic and non-commercial use only.', "Pytorch np_transforms \n Pytorch transforms based on numpy arrays. I decided to re-write some of the standard pytorch transforms using only numpy operations that allow for High Dynamic Range image manipulation.\nThe file  exr_data.py  include some methods for loading HDR images in  exr  format into numpy arrays and writing numpy arrays into  exr  files. \n A list of implemented transforms based on numpy arrays are:\n - Bilateral Filter\n - Median Filter\n - Image Rotation (either randomly sample an angle between two bounds or with a fixed angle)\n - Random Horizontal Flip\n - Random Vertical Flip\n - Random Crop \n - Center Crop\n - Five Crops (4 courners + center)\n - Normalize 0-1 (Normalize the image between 0-1)\n - Random Erasing (Random Erasing Data Augmentation by Zhong et al.)\n - To Tensor\n - rgb2xyz (Change in the color space)\n - xyz2rgb (The opposite change in color space)\n - Lambda (Based on torchvision.transforms.Lambda)\n - Compose (Based on torchvision.transforms.Compose)\n - Normalize (Based on torchvision.transforms.Normalize) \n Dependencies \n \n numpy \n torch  http://pytorch.org/ \n torchvision  http://pytorch.org/ \n scipy   \n (Only if you want to load/ save HDR images)  OpenEXR  http://www.excamera.com/sphinx/articles-openexr.html \n \n Usage examples \n Create a dataset that loads hdr images in  .exr  format: \n ```python\nimport exr_data \n trf = np_transforms.Compose([\n    np_transforms.Scale(size=(256, 256)),\n    np_transforms.RandomCrop(size=(224, 224)),\n    np_transforms.RandomVerticalFlip(prob=0.5),\n    np_transforms.RandomHorizontalFlip(prob=0.5),\n    np_transforms.RotateImage(angles=(-15, 15)),\n    np_transforms.ToTensor(),\n]) \n data_train = exr_data.exrData(root=os.path.join(ROOT_DIR, 'train'),\n                                  loader=exr_data.exr_loader,\n                                  transform=trf) \n ``` \n Write a numpy array into a  .exr  file: \n ```python\nimport exr_data \n let's assume we have a numpy array called 'pic' with the image stored in the form [HxWxC] \n (Rs, Gs, Bs) = [pic[:, :, channel].tostring() for channel in range(pic.shape[-1])]\nexr_data.exr_writer(out_path, size=im_size, pixel_values={'R': Rs, 'G': Gs, 'B': Bs}) \n ```", 'pytorch_project_start \n Files I use when I start a new Pytorch project \n features \n runner  class to run the training/testing loops \n plotter  generates learning curve plots \n logger  automatically logs the current training in a folder storing the model, metric on each iteration... ', 'mlagunas-resume \n Personal resume that follows a one page two column layout. \nIt is mainly based on the work of https://github.com/deedy/Deedy-Resume. This version changes fonts sizes and colors and modifies the citations style. \n Preview \n Here is how this resume looks like:\n', 'learning-icons-appearance-similarity \n Sample code that select similar icons using the method from the paper  Learning Icons Appearance Similarity . \n Project structure \n \n ./small_dataset  folder containing a small subset downloaded from  the noun project . \n model_icons.pth  stores the weights of the model. \n model_icons.py  contains the description of the model. \n plot_similar.py  plots similar icons to a given reference. \n \n How to run the code \n First make sure that you have installed the following packages for python: \n torch\ntorchvision\nmatplotlib\ntqdm \n Download the weights of the trained model  using this  link \nand make sure they are placed in the root folder of the project.\nThen, in order to find similar icons in the given dataset run:\n python3 plot_similar.py \n If you did not modify the code, after running the script, a new folder  similar_icons  will be created containing the images of the reference together with the  k  closest icons to it in ascending order of distance (the distance is also written in the image title). \n Note that we have tested the code using Python 3.6 \n Useful information \n If you found this code useful please cite our work:\n @Article{Lagunas2018,\n  author="Lagunas, Manuel and Garces, Elena and Gutierrez, Diego",\n  title="Learning icons appearance similarity",\n  journal="Multimedia Tools and Applications",\n  year="2018",\n  month="Sep",\n  issn="1573-7721",\n  doi="10.1007/s11042-018-6628-7"\n} \n For more information and related work visit  my personal website . \n If you have further questions feel free to open an issue or send an email to  mlagunas at unizar dot es .', "  \n\n# A Similarity Measure for Material Appearance   \n[![Project page](https://img.shields.io/badge/-Project%20page-blue)](http://webdiis.unizar.es/~mlagunas/publication/material-similarity/)\n[![Paper](https://img.shields.io/badge/Paper-PDF-red)](http://webdiis.unizar.es/~mlagunas/papers/similarity_siggraph_19_small.pdf)\n[![Conference](https://img.shields.io/badge/SIGGRAPH-2019-green)](https://dl.acm.org/citation.cfm?id=3323036)\n[![Journal](https://img.shields.io/badge/TOG-2019-green)](https://dl.acm.org/citation.cfm?id=3323036)\n\n \n \n Abstract \n We present a model to measure the similarity in appearance between different materials, which correlates with human similarity judgments. We first create a database of 9,000 rendered images depicting objects with varying materials, shape and illumination. We then gather data on perceived similarity from crowdsourced experiments; our analysis of over 114,840 answers suggests that indeed a shared perception of appearance similarity exists. We feed this data to a deep learning architecture with a novel loss function, which learns a feature space for materials that correlates with such perceived appearance similarity. Our evaluation shows that our model outperforms existing metrics. Last, we demonstrate several applications enabled by our metric, including appearance-based search for material suggestions, database visualization, clustering and summarization, and gamut mapping. \n Setting it up \n Note that this has been tested using python 3.7 \n Dependencies \n First, clone and install dependencies  \n```bash \n clone project \n git clone https://github.com/mlagunas/material-appearance-similarity.git    \n cd material-appearance-similarity \npip install scipy numpy matplotlib umap-learn Pillow \n install pytorch/torchvision (https://pytorch.org) \n ```    \n Get model pretrained weights\n- download model  weights \n-  (optional)  download  all images \n How to run the code \n Training a new model \n Make sure that you have downloaded all the  training images \nAlso, make sure that you have users' judgements on material similarity. Those\n are two  json  files inside  ./data , namely  answers_processed_train.json , and  answers_processed_test.json .\nAlso make sure that you have the uncropped images of each material with\n Havran geometry ( ./data/havran1_ennis_298x298_LDR ). \n Then, set up those arguments in the trianing script and run it:\n bash\npython train.py --train-dir data/split_dataset --test-dir data\n/havran1_ennis_298x298_LDR \nUsing the default values in the script, the trained model yields an agreement of\n 81.99% with users' answers. \n Getting image feature vectors \n Next, get the feature vectors for some images. First, modify paths inside  get_embs.py .\n python\n...\nweights_path = 'data/model_best.pth.tar'\nimgs_path = 'data/havran1_stpeters_256x256_LDR'\nembs_path = 'data/embs.mat' # we will store the obtained feature vectors in this path\n... \n Then, get the feature vectors for the downloaded images\n bash\npython3 get_embs.py \n Get similar images \n We can obtain similar images to a given reference using the previously computed feature vectors. First, set the path and necesary variables in  plot_similar.py . This will store the 5 more similar images to the reference according to our metric in the path  data/nickel .\n python\n...\nembs_path = 'data/embs.mat'  # /mat file with the embeddings\nn_close_elems = 5  # number of close elements to find\nreference_img = 'data/havran1_stpeters_256x256_LDR/nickel.png'\ndo_unit_norm = False # normalize feature vectors to have unit norm\n... \n \n \n \n Generate UMAP plot \n We can visualize the feature vectors generated for the images using dimensionality reduction algorithms like UMAP. \nFirst we set the path of the feature vectors inside  plot_umap.py . \n python\n...\nembs_path = 'data/embs.mat'\ndo_unit_norm = False # normalize feature vectors to have unit norm\n... \nTo generate the plot we run:\n bash\npython3 plot_umap.py \n \n \n \n Citation \n If you found this code useful please cite our work as:\n @article{lagunas2019similarity,\n    author = {Lagunas, Manuel and Malpica, Sandra and Serrano, Ana and\n    Garces, Elena and Gutierrez, Diego and Masia, Belen},\n    title = {A Similarity Measure for Material Appearance},\n    journal = {ACM Transactions on Graphics (SIGGRAPH 2019)},\n    volume = {38},\n    number = {4},\n    year = {2019}\n}  ", "Single-image Full-body Human Relighting \n Implementation of the paper: Single-image Full-body Human Relighting \n Requirements \n \n First make sure that you have Pytorch running in your machine: https://pytorch.org/ (tested with version 1.9) \n Install all the python dependencies with  pip install -r requirements.txt \n You will need  ffmpeg  in order to generate the relighted videos:  sudo apt install ffmpeg \n Download the pretrained model from  here  and place it under  ./data/model/ \n \n Note that this code has been tested out using Ubuntu 20.04 and Python 3.8 \n Relighting your photos \n Before running  photo_relighting.py :\n- You can change the lights and the photos to use by modifying the following lines:\n photos_dir = './data/photos'\nlight_dir = './data/lights/pisa' \n Note that the  photos  folder has the following structure:\n /data/\n |  photos/\n |  |  mask/\n |  |  |  your_photo.png \n |  |  original/\n |  |  |  your_photo.png \n If you want to relight your own images, make sure that they follow the aforementioned structure. To extract the mask from your photographs, you can rely on freely available services such as  that one .  \n Note that both the mask and the original image should have the same spatial resolution. You can use the script  removebg_img_split.py  to automatically split the image you downloaded with the masked background. Make sure that you correctly set the  img_path  and  out_dir  variables in the script. \n Things to be done \n \n Upload the training code. \n Add script to generate your own light coefficients from any input image in lat-long format. \n \n Citation \n If you find this code useful please cite our work with:\n @inproceedings{Lagunas2021humanrelighting,\n    title={Single-image Full-body Human Relighting},\n    booktitle={Eurographics Symposium on Rendering (EGSR)},\n    publisher={The Eurographics Association},\n    author={Lagunas, Manuel and Sun, Xin and Yang, Jimei and Villegas, Ruben and Zhang, Jianming and Shu, Zhixin and Masia, Belen and Gutierrez, Diego},\n    year={2021},\n    DOI = {10.2312/sr.20211301}\n}"]
abhshkdz,['ProjectEuler.net Solutions \n Repository of all my succesful solutions to problems on  Project Euler', "8tracks-downloader \n A bash script to download playlists from 8tracks. It's a very hackish way of doing it, but works for me. \n Usage \n ./script.sh <mix-url> <play-token>\n \n Requirements \n \n Underscore-CLI  for parsing JSON \n Axel  for downloading tracks \n \n Steps \n \n Go to an 8tracks mix page (like  this ) in a new incognito window (basically none of the tracks should have been played earlier) and open up Chrome developer tools. \n Copy the url. This is the  mix-url  in the command above. \n Get the  play-token  from the console (you may need to enable XHR logging) or the network tab, where it typically appears as  .../sets/<play-token>/... \n Execute the above command and tracks from the mix are downloaded in a new folder with the same name as the mix. \n \n \n", 'Graph Plotter \n Project made in class XII as part of the CBSE Computer Science course. \n Features \n \n Plot polynomials, trigonometric functions, parabolas. \n Solve multiple polynomials graphically. \n Save a graph for later use. \n \n Screenshots \n \n \n \n \n \n \n \n Works in Turbo C++ only \n License \n The MIT License (MIT) \nCopyright © 2012 Abhishek Das \n Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the “Software”), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions: \n The above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software. \n THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.', 'PHP Lecture \n Lecture on Introduction to PHP delivered at IIT Roorkee. Made using  reveal.js .', 'Indian Academy of Sciences \n Summer Research Fellowship Programme 2013 \n Complete List of Guides \n The official list is available  here . I felt a need for sorting by interest, specialization etc., and better search hence built this. \n IAS obviously holds all rights to update their list whenever. So that should be consulted as the official list.', 'dotfiles \n Repo of my configuration files for Git, Bash, Zsh, Vim, Sublime Text 2, etc.', 'da.s \n A simple URL Shortener \n Features \n \n Shortens any url to a single character id (random or by choice). \n Customizable domain, via config file. \n Stores URL hits as well. \n Really easy to set up. \n \n Installation \n \n Set up the db from  schema/structure.sql . \n Copy over  config.example.php  to  config.php , and update DB connection details and root domain. \n Add virtual host entry by copying  vhost/da.s  to  /etc/apache2/sites-available/ , edit it to your liking and link it to  /etc/apache2/sites-enabled/ . Also, add a line in  /etc/hosts \n Restart apache and everything should be fine. \n \n Todo \n \n Update the encoding functions to allow more URLs. \n Work on analytics. \n Make a better UI. \n', 'Apparatus \n Repository of my frequently used scripts. \n Git \n git-repositories-by-username.sh \n This script is passed usernames as arguments from which it creates a repository listing by username (owner) in  $HOME/git_<username>.txt . \n It looks for git repositories under  $HOME/projects/ . \n Usage \n . git-repositories-by-username.sh abhshkdz\n \n Muzi \n muzi-albums-download.sh \n Requirements :  Underscore-CLI  for parsing JSON \n This script is passed Muzi album URLs as arguments. \n It creates folders as album names in the current directory and downloads the entire album. \n Usage \n . muzi-albums-download.sh https://sdslabs.co.in/muzi/ajax/album/?id=109 https://sdslabs.co.in/muzi/ajax/album/?id=111 https://sdslabs.co.in/muzi/ajax/album/?id=112\n \n muzi-artists-download.sh \n Requirements :  Underscore-CLI  for parsing JSON \n This script is passed Muzi band URLs as arguments. \n It creates folders as artist->album names in the current directory and downloads the entire discography. \n Usage \n . muzi-artists-download.sh https://sdslabs.co.in/muzi/ajax/band/albums.php?id=7038 https://sdslabs.co.in/muzi/ajax/band/albums.php?id=7065\n \n Latest \n latest-jquery.sh \n Gets the latest version of jQuery from  jQuery CDN . \n Usage \n . latest-jquery.sh\n \n latest-bootstrap.sh \n Gets Twitter Bootstrap 2.1.0 CSS files by theme from  Boostrap CDN . \n Pass theme names as arguments. \n Usage \n . latest-bootstrap.sh amelia default spruce\n \n Backup \n backup-db.sh \n Backs up all databases separately as backup/dbname.sql \n Usage \n . backup-db.sh db_pass\n \n Twitter \n log.sh \n Moves the individual tweet files to backup folder and adds tweet text to the file  tweets \n gist.sh \n Adds the latest tweets to  Gist on Github \n Usage \n . log.sh\n. gist.sh\n \n Download \n axel-file.sh \n Downloads from a list of files using Axel \n Usage \n . axel-file.sh list.txt\n', 'IMDB Lookup \n A movie search application to get data from IMDB api, formatted and displayed nicely. \n Screenshots \n \n Contact \n Abhishek Das - das.abhshk@gmail.com \n This was a side project made a long time back to be part of a bigger application which never took shape. ', 'HackFlowy \n \n An open-source  Workflowy  clone.  Static demo available here . \n \n Installation \n \n Edit  config/development.json  and  config/database.json  to your needs \n npm install \n npm install bower \n bower install \n node server.js \n \n Heroku deploy \n You can use our one-click heroku deploy (Select "United States" as region, when prompted): \n \n Or proceed manually as follow: \n \n heroku create --stack cedar \n heroku addons:add heroku-postgresql:dev \n heroku config:set NODE_ENV=production \n git push heroku master \n \n Controls \n \n UP  &  DOWN : navigate through tasks \n CNTRL+UP  &  CNTRL+DOWN : shuffle tasks \n TAB : right-indent \n SHIFT  +  TAB : left-indent \n BACKSPACE : Remove an empty task \n ENTER : New task \n Click on a bullet point to fold it \n Hover on a bullet point and click complete to complete it \n \n Technologies used \n \n Node + Socket.io \n Backbone \n Backbone.marionette \n Backbone.localforage \n Foundation \n \n To-do \n \n ~~Work on sub-lists. The parent id of the Backbone task model has to be set for it to be saved properly. The template should be modified to have the  children  ul as part of every task.~~ \n Search & Tags \n Themes \n \n Feel free to try it out and contribute. \n License \n MIT', "Graf    \n Graf is a simple git log analyzer gem. \n Installation \n $ gem install graf\n \n Usage \n \n In case you are wondering how I made that GIF,  here it is . \n Contributing \n \n Fork it \n Create your feature branch ( git checkout -b my-new-feature ) \n Commit your changes ( git commit -am 'Add some feature' ) \n Push to the branch ( git push origin my-new-feature ) \n Create new Pull Request \n", "Air Maps \n Winning entry in Microsoft Code.Fun.Do Hackathon and Finalists' Forum. \n Air Maps is an application to navigate the Google Earth Plugin with gestures and speech commands using a Microsoft Kinect Sensor. \n Stack \n \n C# \n Microsoft Kinect SDK \n Alchemy Websocket Server \n Google Earth Plugin \n Google Maps and StreetView API \n", 'Lagrange Approximation \n Work done as part of the Digital Signal Processing course project. \n \n Lagrange Interpolation \n Optimal Polynomial Approximation \n Range Reduction \n Subinterval Division \n \n References \n Chapter 26,  Streamlining Digital Signal Processing', 'Visualizing using t-SNE \n Visualization of top 100 Indian Twitter accounts using t-SNE. \n Source: http://karpathy.github.io/2014/07/02/visualizing-top-tweeps-with-t-sne-in-Javascript/ \n Credits:  Andrej Karpathy \n Implementation Details \n \n data/get_tweets.py  and  data/get_images.py  download tweets and images from Twitter. \n main.py  creates td-idf vector and calculates distance matrix. \n index.html  picks up data from  data.json . \n', "neural-vqa \n \n This is an experimental Torch implementation of the\nVIS + LSTM visual question answering model from the paper\n Exploring Models and Data for Image Question Answering \nby Mengye Ren, Ryan Kiros & Richard Zemel. \n \n Setup \n Requirements: \n \n Torch \n loadcaffe \n \n Download the  MSCOCO  train+val images and  VQA  data using  sh data/download_data.sh . Extract all the downloaded zip files inside the  data  folder. \n ```\nunzip Annotations_Train_mscoco.zip\nunzip Questions_Train_mscoco.zip\nunzip train2014.zip \n unzip Annotations_Val_mscoco.zip\nunzip Questions_Val_mscoco.zip\nunzip val2014.zip\n``` \n If you had them downloaded already, copy over the  train2014  and  val2014  image folders\nand VQA JSON files to the  data  folder. \n Download the  VGG-19  Caffe model and prototxt using  sh models/download_models.sh . \n Known issues \n \n To avoid memory issues with LuaJIT, install Torch with Lua 5.1 ( TORCH_LUA_VERSION=LUA51 ./install.sh ).\nMore instructions  here . \n If working with plain Lua,  luaffifb  may be needed for  loadcaffe ,\nunless using pre-extracted fc7 features. \n \n Usage \n Extract image features \n th extract_fc7.lua -split train\nth extract_fc7.lua -split val \n Options \n \n batch_size : Batch size. Default is 10. \n split : train/val. Default is  train . \n gpuid : 0-indexed id of GPU to use. Default is -1 = CPU. \n proto_file : Path to the  deploy.prototxt  file for the VGG Caffe model. Default is  models/VGG_ILSVRC_19_layers_deploy.prototxt . \n model_file : Path to the  .caffemodel  file for the VGG Caffe model. Default is  models/VGG_ILSVRC_19_layers.caffemodel . \n data_dir : Data directory. Default is  data . \n feat_layer : Layer to extract features from. Default is  fc7 . \n input_image_dir : Image directory. Default is  data . \n \n Training \n th train.lua \n Options \n \n rnn_size : Size of LSTM internal state. Default is 512. \n num_layers : Number of layers in LSTM \n embedding_size : Size of word embeddings. Default is 512. \n learning_rate : Learning rate. Default is 4e-4. \n learning_rate_decay : Learning rate decay factor. Default is 0.95. \n learning_rate_decay_after : In number of epochs, when to start decaying the learning rate. Default is 15. \n alpha : Alpha for adam. Default is 0.8 \n beta : Beta used for adam. Default is 0.999. \n epsilon : Denominator term for smoothing. Default is 1e-8. \n batch_size : Batch size. Default is 64. \n max_epochs : Number of full passes through the training data. Default is 15. \n dropout :  Dropout for regularization. Probability of dropping input. Default is 0.5. \n init_from : Initialize network parameters from checkpoint at this path. \n save_every : No. of iterations after which to checkpoint. Default is 1000. \n train_fc7_file : Path to fc7 features of training set. Default is  data/train_fc7.t7 . \n fc7_image_id_file : Path to fc7 image ids of training set. Default is  data/train_fc7_image_id.t7 . \n val_fc7_file : Path to fc7 features of validation set. Default is  data/val_fc7.t7 . \n val_fc7_image_id_file : Path to fc7 image ids of validation set. Default is  data/val_fc7_image_id.t7 . \n data_dir : Data directory. Default is  data . \n checkpoint_dir : Checkpoint directory. Default is  checkpoints . \n savefile : Filename to save checkpoint to. Default is  vqa . \n gpuid : 0-indexed id of GPU to use. Default is -1 = CPU. \n \n Testing \n th predict.lua -checkpoint_file checkpoints/vqa_epoch23.26_0.4610.t7 -input_image_path data/train2014/COCO_train2014_000000405541.jpg -question 'What is the cat on?' \n Options \n \n checkpoint_file : Path to model checkpoint to initialize network parameters from \n input_image_path : Path to input image \n question : Question string \n \n Sample predictions \n Randomly sampled image-question pairs from the VQA test set,\nand answers predicted by the VIS+LSTM model. \n \n Q: What animals are those?\nA: Sheep \n \n Q: What color is the frisbee that's upside down?\nA: Red \n \n Q: What is flying in the sky?\nA: Kite \n \n Q: What color is court?\nA: Blue \n \n Q: What is in the standing person's hands?\nA: Bat \n \n Q: Are they riding horses both the same color?\nA: No \n \n Q: What shape is the plate?\nA: Round \n \n Q: Is the man wearing socks?\nA: Yes \n \n Q: What is over the woman's left shoulder?\nA: Fork \n \n Q: Where are the pink flowers?\nA: On wall \n Implementation Details \n \n Last hidden layer image features from  VGG-19 \n Zero-padded question sequences for batched implementation \n Training questions are filtered for  top_n  answers,\n top_n = 1000  by default (~87% coverage) \n \n Pretrained model and data files \n To reproduce results shown on this page or try your own\nimage-question pairs, download the following and run\n predict.lua  with the appropriate paths. \n \n vqa_epoch23.26_0.4610.t7 (Serialized using Lua51) [ GPU ] [ CPU ] \n answers_vocab.t7 \n questions_vocab.t7 \n data.t7 \n \n References \n \n Exploring Models and Data for Image Question Answering , Ren et al., NIPS15 \n VQA: Visual Question Answering , Antol et al., ICCV15 \n \n License \n MIT", 'Summaries of papers on deep learning. \n 2018 \n \n World Models [ Paper ] [ Review ] \n David Ha, Jürgen Schmidhuber, ArXiv, 2018 \n \n \n \n 2017 \n \n A Deep Compositional Framework for Human-like Language Acquisition in Virtual Environment [ Paper ] [ Review ] \n Haonan Yu, Haichao Zhang, Wei Xu, ArXiv, 2017 \n \n \n A simple neural network module for relational reasoning [ Paper ] [ Review ] \n Adam Santoro, David Raposo, David G.T. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, Timothy Lillicrap, NIPS, 2017 \n \n \n Are You Talking to Me? Reasoned Visual Dialog Generation through Adversarial Learning [ Paper ] [ Review ] \n Qi Wu, Peng Wang, Chunhua Shen, Ian Reid, Anton van den Hengel, ArXiv, 2017 \n \n \n From Red Wine to Red Tomato: Composition with Context [ Paper ] [ Review ] \n Ishan Misra, Abhinav Gupta, Martial Hebert, CVPR, 2017 \n \n \n Towards Diverse and Natural Image Descriptions via a Conditional GAN [ Paper ] [ Review ] \n Bo Dai, Sanja Fidler, Raquel Urtasun, Dahua Lin, ICCV, 2017 \n \n \n \n 2016 \n \n Actions ~ Transformations [ Paper ] [ Review ] \n Xiaolong Wang, Ali Farhadi, Abhinav Gupta, CVPR, 2016 \n \n \n Building Machines That Learn and Think Like People [ Paper ] [ Review ] \n Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, Samuel J. Gershman, Behavioral and Brain Sciences, 2016 \n \n \n Deep Compositional Question Answering with Neural Module Networks [ Paper ] [ Review ] \n Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein, CVPR, 2016 \n \n \n Deep Networks with Stochastic Depth [ Paper ] [ Review ] \n Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, Kilian Weinberger, ArXiv, 2016 \n \n \n Deep Reinforcement Learning for Dialogue Generation [ Paper ] [ Review ] \n Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, Dan Jurafsky, ArXiv, 2016 \n \n \n Deep Residual Learning for Image Recognition [ Paper ] [ Review ] \n Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, ArXiv, 2016 \n \n \n Delving Deeper into Convolutional Networks for Learning Video Representations [ Paper ] [ Review ] \n Nicolas Ballas, Li Yao, Chris Pal, Aaron Courville, ICLR, 2016 \n \n \n Dynamic Capacity Networks [ Paper ] [ Review ] \n Amjad Almahairi, Nicolas Ballas, Tim Cooijmans, Yin Zheng, Hugo Larochelle, Aaron Courville, ICML, 2016 \n \n \n Identity Mappings in Deep Residual Networks [ Paper ] [ Review ] \n Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, ArXiv, 2016 \n \n \n Net2Net: Accelerating Learning via Knowledge Transfer [ Paper ] [ Review ] \n Tianqi Chen, Ian Goodfellow, Jonathon Shlens, ICLR, 2016 \n \n \n Perceptual Losses for Real-Time Style Transfer and Super-Resolution [ Paper ] [ Review ] \n Justin Johnson, Alexandre Alahi, Li Fei-Fei, ArXiv, 2016 \n \n \n Recurrent Batch Normalization [ Paper ] [ Review ] \n Tim Cooijmans, Nicolas Ballas, César Laurent, Aaron Courville, ArXiv, 2016 \n \n \n Residual Networks are Exponential Ensembles of Relatively Shallow Networks [ Paper ] [ Review ] \n Andreas Veit, Michael Wilber, Serge Belongie, ArXiv, 2016 \n \n \n Residual Networks of Residual Networks: Multilevel Residual Networks, ArXiv, 2016 [ Paper ] [ Review ] \n Ke Zhang, Miao Sun, Tony X. Han, Xingfang Yuan, Liru Guo, Tao Liu, ArXiv, 2016 \n \n \n \n 2015 \n \n Deep Visual Analogy-Making [ Paper ] [ Review ] \n Scott E. Reed, Yi Zhang, Yuting Zhang, Honglak Lee, NIPS, 2015 \n \n \n DenseCap: Fully Convolutional Localization Networks for Dense Captioning [ Paper ] [ Review ] \n Justin Johnson, Andrej Karpathy, Li Fei-Fei, ArXiv, 2015 \n \n \n DRAW: A Recurrent Neural Network For Image Generation [ Paper ] [ Review ] \n Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, Daan Wierstra, ICML, 2015 \n \n \n Neural Machine Translation by Jointly Learning to Align and Translate [ Paper ] [ Review ] \n Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, ICLR, 2015 \n \n \n Object Detectors Emerge in Deep Scene CNNs [ Paper ] [ Review ] \n Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba, ICLR, 2015 \n \n \n Spatial Transformer Networks [ Paper ] [ Review ] \n Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu, NIPS, 2015 \n \n \n Stacked Attention Networks for Image Question Answering [ Paper ] [ Review ] \n Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Smola, ArXiv, 2015 \n \n \n Striving for Simplicity: the All Convolutional Net [ Paper ] [ Review ] \n Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, Martin Riedmiller, ICLR, 2015 \n \n \n You Only Look Once: Unified, Real-Time Object Detection [ Paper ] [ Review ] \n Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi, ArXiv15 \n \n \n \n 2014 \n \n Convolutional Neural Networks for Sentence Classification [ Paper ] [ Review ] \n Yoon Kim, EMNLP, 2014 \n \n \n Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps [ Paper ] [ Review ] \n Karen Simonyan, Andrea Vedaldi, Andrew Zisserman, ICLR, 2014 \n \n \n Going Deeper with Convolutions [ Paper ] [ Review ] \n Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, ArXiv, 2014 \n \n \n How transferable are features in deep neural networks? [ Paper ] [ Review ] \n Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson, NIPS, 2014 \n \n \n Intriguing Properties of Neural Networks [ Paper ] [ Review ] \n Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus, ICLR, 2014 \n \n \n Learning Deep Features for Scene Recognition using Places Database [ Paper ] [ Review ] \n Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, Aude Oliva, NIPS, 2014 \n \n \n Network in Network [ Paper ] [ Review ] \n Min Lin, Qiang Chen, Shuicheng Yan, ICLR, 2014 \n \n \n Neural Turing Machines [ Paper ] [ Review ] \n Alex Graves, Greg Wayne, Ivo Danihelka, ArXiv, 2014 \n \n \n Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation [ Paper ] [ Review ] \n Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik, CVPR, 2014 \n \n \n Sequence to Sequence Learning with Neural Networks [ Paper ] [ Review ] \n Ilya Sutskever, Oriol Vinyals, Quoc V. Le, NIPS, 2014 \n \n \n Very Deep Convolutional Networks for Large-Scale Image Recognition [ Paper ] [ Review ] \n Karen Simonyan, Andrew Zisserman, ArXiv, 2014 \n \n \n Visualizing and Understanding Convolutional Networks [ Paper ] [ Review ] \n Matthew D Zeiler, Rob Fergus, ECCV, 2014 \n \n \n \n 2012 \n \n ImageNet Classification with Deep Convolutional Neural Networks [ Paper ] [ Review ] \n Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton, NIPS, 2012 \n \n \n What Question Would Turing Pose Today? [ Paper ] [ Review ] \n Barbara Grosz, AI Magazine, 2012 \n \n \n', "neural-vqa-attention \n Torch implementation of an attention-based visual question answering model ( Stacked Attention Networks for Image Question Answering, Yang et al., CVPR16 ). \n \n \n Train your own network \n Extract image features \n Preprocess VQA dataset \n Training \n \n \n Use a pretrained model \n Pretrained models and data files \n Running evaluation \n \n \n Results \n \n Intuitively, the model looks at an image, reads a question, and comes up with an answer to the question and a heatmap of where it looked in the image to answer it. \n The model/code also supports referring back to the image multiple times ( Stacked Attention ) before producing the answer. This is supported via a  num_attention_layers  parameter in the code (default = 1). \n NOTE : This is NOT a state-of-the-art model. Refer to  MCB ,  MLB  or  HieCoAtt  for that.\nThis is a simple, somewhat interpretable model that gets decent accuracies and produces  nice-looking results .\nThe code was written about ~1 year ago as part of  VQA-HAT , and I'd meant to release it earlier, but couldn't get around to cleaning things up. \n If you just want to run the model on your own images, download links to pretrained models are given below. \n Train your own network \n Preprocess VQA dataset \n Pass  split  as  1  to train on  train  and evaluate on  val , and  2  to train on  train + val  and evaluate on  test . \n cd data/\npython vqa_preprocessing.py --download True --split 1\ncd .. \n python prepro.py --input_train_json data/vqa_raw_train.json --input_test_json data/vqa_raw_test.json --num_ans 1000 \n Extract image features \n Since we don't finetune the CNN, training is significantly faster if image features are pre-extracted. We use image features from VGG-19. The model can be downloaded and features extracted using: \n sh scripts/download_vgg19.sh\nth prepro_img.lua -image_root /path/to/coco/images/ -gpuid 0 \n Training \n th train.lua \n Use a pretrained model \n Pretrained models and data files \n All files available for download  here . \n \n san1_2.t7 : model pretrained on  train + val  with 1 attention layer (SAN-1) \n san2_2.t7 : model pretrained on  train + val  with 2 attention layers (SAN-2) \n params_1.json : vocabulary file for training on  train , evaluating on  val \n params_2.json : vocabulary file for training on  train + val , evaluating on  test \n qa_1.h5 : QA features for training on  train , evaluating on  val \n qa_2.h5 : QA features for training on  train + val , evaluating on  test \n img_train_1.h5  &  img_test_1.h5 : image features for training on  train , evaluating on  val \n img_train_2.h5  &  img_test_2.h5 : image features for training on  train + val , evaluating on  test \n \n Running evaluation \n model_path=checkpoints/model.t7 qa_h5=data/qa.h5 params_json=data/params.json img_test_h5=data/img_test.h5 th eval.lua \n This will generate a JSON file containing question ids and predicted answers. To compute accuracy on  val , use  VQA Evaluation Tools . For  test , submit to  VQA evaluation server on EvalAI . \n Results \n Format : sets of 3 columns, col 1 shows original image, 2 shows 'attention' heatmap of where the model looks, 3 shows image overlaid with attention. Input question and answer predicted by model are shown below examples.\n \n More results available  here . \n Quantitative Results \n Trained on  train  for  val  accuracies, and trained on  train + val  for  test  accuracies. \n VQA v2.0 \n | Method                | val     | test    |\n| ------                | ---     | ----    |\n| SAN-1                 | 53.15   | 55.28   |\n| SAN-2                 | 52.82   | -       |\n|  d-LSTM + n-I      | 51.62   | 54.22   |\n|  HieCoAtt          | 54.57   | -       |\n|  MCB               | 59.14   | -       | \n VQA v1.0 \n | Method                | test-std    |\n| ------                | --------    |\n| SAN-1                 | 59.87       |\n| SAN-2                 | 59.59       |\n|  d-LSTM + n-I      | 58.16       |\n|  HieCoAtt          | 62.10       |\n|  MCB               | 65.40       | \n References \n \n Stacked Attention Networks for Image Question Answering , Yang et al., CVPR16 \n Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering , Goyal and Khot et al., CVPR17 \n VQA: Visual Question Answering , Antol et al., ICCV15 \n \n Acknowledgements \n \n Data preprocessing script borrowed from  VT-vision-lab/VQA_LSTM_CNN \n \n License \n MIT", 'House3D: A Rich and Realistic 3D Environment \n Yi Wu ,  Yuxin Wu ,  Georgia Gkioxari  and  Yuandong Tian \n For questions regarding House3D contact Yuxin Wu \n \n \n House3D is a virtual 3D environment which consists of thousands of indoor scenes equipped with\na diverse set of scene types, layouts and objects sourced from the  SUNCG dataset .\nIt consists of over 45k indoor 3D scenes, ranging from studios to two-storied houses\nwith swimming pools and fitness rooms. All 3D objects are fully annotated with category labels.\nAgents in the environment have access to observations of multiple modalities, including RGB images,\ndepth, segmentation masks and top-down 2D map views. The renderer runs at\nthousands frames per second, making it suitable for large-scale RL training. \n Usage instructions can be found at  INSTRUCTION.md \n Existing Research Projects with House3D \n A. RoomNav ( paper ) \n Yi Wu, Yuxin Wu, Georgia Gkioxari, Yuandong Tian \n In this work we introduce a concept learning task, RoomNav, where an agent is asked to navigate to a destination specified by a high-level concept, e.g.  dining room .\nWe demonstrated two neural models: a gated-CNN and a gated-LSTM, which effectively improve the agent\'s sensitivity to different concepts.\nFor evaluation, we emphasize on generalization ability and show that our agent can  generalize across environments \ndue to the diverse and large-scale dataset. \n \n \n \n B. Embodied QA ( project page  |  EQA paper  |  NMC paper ) \n Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra \n Embodied Question Answering is a new AI task where an agent is spawned at a random location in a 3D environment and asked a natural language question ("What color is the car?").\nIn order to answer, the agent must first intelligently navigate to explore the environment, gather information through first-person (egocentric) vision, and then answer the question ("orange"). \n \n \n \n Citation \n If you use our platform in your research, you can cite us with:\n @article{wu2018building,\n  title={Building generalizable agents with a realistic and rich 3D environment},\n  author={Wu, Yi and Wu, Yuxin and Gkioxari, Georgia and Tian, Yuandong},\n  journal={arXiv preprint arXiv:1801.02209},\n  year={2018}\n} \n License \n House3D is released under the  Apache 2.0 license .']
lantiga,['Solo ( Live Demo ) \n Solo  is a Jekyll theme that supports  single-page websites  only, but supports them well. Yes, it\'s responsive. \n \n Solo is useful if... \n \n You want to create an "about me" page from a single markdown file and host it under a custom domain name. \n You want to create a single-page website that\'s mostly text, like  Know Your Company . \n You want to share a single markdown file and tried GitHub Gist ( example ), but would like something nicer-looking. \n You want something like GitHub\'s  automatic page generator  for a non-code repository. \n \n Examples \n \n This page itself is built with Solo. Try it out from  the github repo . \n My personal site  is built with Solo. It\'s generated from  this markdown file . \n Two of my Japanese personal sites ( #1  and  #2 ) are also built with Solo. \n \n Usage \n First,  install Jekyll . Then download Solo from its  GitHub Repository . Start Jekyll and you should see this page up and running. \n The main file you\'ll be editing is  _includes/index.md . This becomes the content for the main  index.html . \n Other Files \n \n Edit  _config.yml  to change the site title. \n Edit  _includes/head.html  to add custom code to  <head> . \n Edit  _includes/scripts.html  to add custom code before  </body> . \n Edit  CNAME  to host on a custom domain. \n Edit  README.md  before pushing your code. \n \n Don\'t use  <h1>  tags \n Wthin  index.md , do not use  <h1>  tags -  <h1>  is reserved for the site title. \n Supported Tags \n Solo supports lists,  <hr> s,  <table> s, \n \n blockquotes, and... \n \n ~~~html \n code blocks with syntax highlighting. \n ~~~ \n Keep Solo up to date \n Instead of downloading, you can  fork Solo  and use the "upstream" strategy described on  this page  to keep Solo up to date. \n Author \n Shu Uesugi ( Twitter / GitHub / G+ ). \n \n License \n MIT License \n', 'hirop \n A Clojure(Script) data layer targeted at multi-document consistency and asynchronous updates. \n Usage \n Leiningen:\n clojure\n[hirop "0.1.0-SNAPSHOT"] \n Test:\n clojure\nlein test \n Documentation \n Please refer to the  hirop wiki . \n License \n Copyright © 2013 Orobix Srl \n Distributed under the Eclipse Public License, the same as Clojure.', 'hirop-compojure \n REST frontend for  hirop  based on  Compojure . \n Usage \n Leiningen:\n clojure\n[hirop-compojure "0.1.0-SNAPSHOT"] \n Test:\n clojure\nlein test \n Documentation \n Please refer to the  hirop wiki . \n License \n Copyright © 2013 Orobix Srl \n Distributed under the Eclipse Public License, the same as Clojure.', 'hirop-orientdb \n A  OrientDB  backend for  hirop . \n Usage \n Leiningen:\n clojure\n[hirop-orientdb "0.1.0-SNAPSHOT"] \n Test:\n clojure\nlein test \n Documentation \n Please refer to the  hirop wiki . \n License \n Copyright © 2013 Orobix Srl \n Distributed under the Eclipse Public License, the same as Clojure.', "CLI Interface \n CLI Interface is a prototype for a next generation command line module interface for 3DSlicer, CTK and CTK-based projects. \n It tries to overcome a few limitations of the current status quo in a couple of ways: \n \n it provides a replacement for GenerateCLP, allowing easy addition of new tags and dynamic querying of parameters (as opposed to the current, monolythic C++ macro-based solution) \n it allows for parameters to be either parsed from the command line arguments as well as queried through standard i/o.  \n \n The main motivation is to allow command line modules to query some of the parameters during their execution by emitting a query XML string in standard output (similarly to what happens now for progress reporting) and have the input get back on standard input (e.g. by a user on the command line, by a user through a 3DSlicer widget or by another process on its standard output), parsed and assigned to a variable during execution. \n This would allow CLI modules to be  mildly  interactive (e.g. specify a threshold on the base of an incremental result, or select a point on an intermediate image), greatly expanding the reach of CLI module use cases and maintaining interoperability of CLI modules among CTK-compliant applications. \n The code uses tclap for parsing command line arguments and TinyXML2 for parsing XML. \n You can find a sample of what a command line module might look like in the future under Testing. \n This project started as experiment with Steve Pieper at the  Bologna 2012 CTK Hackfest . \n TODO \n This is work in progress. \n There are several parts that still need to be implemented, some of which have to be ported from GenerateCLP: \n \n Unpacking complex arguments \n Generating query XML fragments \n Embedding the XML within the executable \n Populating ModuleDescriptionParser data structures \n Adding process information, etc needed by callees \n \n Essentially, a module should be able to get its variables either from command line or from stdin. In the latter case, values would be provided by the callee after the module has queried them in its stdout. \n Variables that have to be provided at stdin are the ones declared in the XML module description (i.e. no other variables are allowed). Similarly, any intermediate outputs that are needed for the informing the callee about what's being queried have to be defined as outputs in the XML module description and provided in the query XML fragment (either as text in a   tag or as filenames of a file written by the module, in the same way outputs are generated by command line modules).", 'hirop-couchdb \n A  CouchDB  backend for  hirop . \n Usage \n Leiningen:\n clojure\n[hirop-couchdb "0.1.0-SNAPSHOT"] \n Test:\n clojure\nlein test \n Documentation \n Please refer to the  hirop wiki . \n License \n Copyright © 2013 Orobix Srl \n Distributed under the Eclipse Public License, the same as Clojure.', 'hirop-redis-store \n Redis  context store for  hirop  based on  Carmine . \n Usage \n Leiningen:\n clojure\n[hirop-redis-store "0.1.0-SNAPSHOT"] \n Documentation \n Please refer to the  hirop wiki . \n Todo \n \n Use watch/multi/exec with retry in update-context \n \n License \n Copyright © 2013 Orobix Srl \n Distributed under the Eclipse Public License, the same as Clojure.', 'Exoref \n Exoref aims at providing a  Redis -based implementation of  Clojure  reference types. This is a rather natural fit given the lock-free, optimistic concurrency approach of Redis transactions. \n Exoref allows to use Clojure reference types and concurrency primitives in distributed applications by storing their values on a Redis server. This enables to develop and deploy distributed applications on local setups or on PaaS providers (e.g.  Heroku ) while leveraging on Clojure reference type semantics. \n Exoref is based on  Carmine  Redis client library. \n Installation \n To include Exoref in your project, simply add the following to your  project.clj  dependencies: \n clojure\n[exoref "0.1.4"] \n Usage \n This is work in progress.  \n As of 0.1.4, Exoref provides Redis-based counterparts of  \n \n Atom (since version 0.1.0) \n Promise (since version 0.1.1) \n Delay (since version 0.1.2) \n \n Exoatom \n An Exoatom provides a complete Redis-based implementation of a Clojure atom, including meta, validator and watches. \n To instantiate an Exoatom, just make sure you have a redis server running and go: \n ```clojure\n(ns hello-world\n  (:use [exoref.connection :only [with-conn make-conn-pool make-conn-spec]]\n        [exoref.atom :only [exoatom swap!! reset!! compare-and-set!!]])) \n (def eatom (exoatom "some:redis:key" {:a 1 :b "hey"})) \n @eatom\n``` \n A Redis key has to be provided, in order to allow other components of the distributed system (or different workers running the same code on different machines) to access the Exoatom value. \n To connect to a remote Redis server, use the  with-conn  macro in  exoref.connection  (for more details refer to  Carmine ):\n```clojure\n(def conn-pool (make-conn-pool))\n(def conn-spec (make-conn-spec :host "redis://redishost.com" :port 6379 :password "changeme")) \n (def eatom (with-conn conn-pool conn-spec (exoatom "some:redis:key" {:a 1 :b "hey"})))\n``` \n The  with-conn  macro can be used with all exoref reference types. \n Meta and validators are supported as in standard Clojure Atoms:\n```clojure\n(def eatom (exoatom "some:redis:key" {:a 1 :b "hey"} :meta {:foo "bar"} :validator #(odd? (:a %))) \n (reset-meta! eatom {:foo "biz"}) \n (set-validator! eatom #(= "hey" (:b %)))\n``` \n As well as watches: \n clojure\n(add-watch eatom "watch-key" \n  (fn [k r oldval newval] (prn [k oldval newval]))) \n To change the value of the Exoatom use double-bang (as in  Avout ) version of the atom functions, namely  compare-and-set!! ,  swap!! ,  reset!! : \n ```clojure\n(swap!! eatom update-in [:a] inc) \n (reset!! eatom {:c "foo"}) \n (compare-and-set!! eatom {:c "foo"} {:c "bar"})\n``` \n Exopromise \n An Exopromise is a Redis-based implementation of a Clojure promise. To create a promise, go: \n ```clojure\n(ns hello-world\n  (:use [exoref.promise :only [exopromise]])) \n (def epromise (exopromise "some:redis:key")) \n @epromise\n``` \n You can then dereference and deliver the exopromise as usual \n clojure\n(realized? epromise) \n;; false\n(deliver epromise {:a 1})\n@epromise\n;; {:a 1} \n Dereferencing is blocking, as for standard promises. If two processes share a promise (i.e. two exopromises are created with the same Redis key), dereferencing in one process and delivering in the other process will unblock in both. \n Exodelay \n An Exodelay is a Redis-based implementation of a Clojure delay. An exodelay can be created in separate processes with a Redis key and a body of code, or with the key alone. As soon as one of the exodelays sharing the same Redis key is dereferenced, the body is executed in one (and only one) of the processes in which the exodelay has been created with a body. All calls to  deref  will block until the body has been executed, at which point the value is cached for subsequent calls in any process. \n To create a delay, go: \n ```clojure\n(ns hello-world-1\n  (:user [exoref.delay :only [exodelay])) \n (def edelay-1 (exodelay "some:redis:key"))\n``` \n and in another process \n ```clojure\n(ns hello-world-2\n  (:user [exoref.delay :only [exodelay])) \n (def edelay-2 (exodelay "some:redis:key" (+ 1 2))\n``` \n If the delay is deref\'d in the first process\n clojure\n@edelay-1 \n the body  (+ 1 2)  will be triggered in the second process and the deref in the first process will block until the body is done. At this point, both  @edelay-1  and  @edelay-2  will return  3 . \n Limitations \n Right now exoref relies on raw Redis PubSub, which is  Fire and Forget . This means that if the connection is lost temporarily, an exoref might not be notified of a change to the value of a ref. This affects atom watches, promise and delay. In particular, for the latter two, a deref might block indefinitely. \n There are several ways to solve this issue on the exoref side, e.g. relying on notification queues + client registration, possibly managed through a server-side Lua script. There are of course right  tools   for   the   job , but we\'ll try to stick with Redis for the proof of concept and see how far it takes us. \n It also looks like reliable PubSub might be a feature scheduled for future Redis releases (see  Redis docs  and a  couple   of  tweets). \n Install to local repo \n Install  lein-localrepo  and run  \n lein localrepo install target/exoref-0.1.x-SNAPSHOT.jar exoref/exoref 0.1.x-SNAPSHOT\n \n after replacing  0.1.x  with the actual version number. \n Acknowledgements \n Many thanks to @ptaoussanis (Peter Taoussanis) for clarifications on the use of  Carmine . \n Contact \n For questions please contact me at luca dot antiga at  orobix  dot com. Pull requests are greatly welcome. \n License \n Copyright © 2013 Luca Antiga. \n Distributed under the Eclipse Public License, the same as Clojure.', "hyperplumbing \n A playground for use of  Prismatic 's  plumbing  library (aka  Graph ) in distributed environments based on  exoref  (a Redis-based reference types implementation). \n Details \n This is work in progress, not intended for use in production. \n This project is intended to demonstrate a possible design for distributed Graph based on Redis, in which different computing nodes reside in different processes and computations are triggered by consumers through lazy evaluation.  The system should be resilient with respect to the failure of individual processes (or Redis itself). \n Contact \n For questions please contact me at luca dot antiga at  orobix  dot com. Pull requests are greatly welcome. \n License \n Copyright © 2013 Luca Antiga. \n Distributed under the Eclipse Public License, the same as Clojure.", 'clj-toml \n clj-toml is  TOML  for Clojure. TOML is Tom\'s Obvious, Minimal Language.  \n \n TOML is like INI, only better (Tom Preston-Werner) \n \n clj-toml uses  Kern  for parsing. Kern does all the heavy lifting, we\'re just sitting pretty. \n clj-toml comes with a decent  collection of tests . It successfully parses the TOML  hard example . Easy peasy. \n Supported TOML version:  v0.1.0 \n Usage \n Leiningen: \n clojure\n  [clj-toml "0.3.1"] \n Test: \n clojure\n  lein test \n Use: \n ```clojure\n  (use \'clj-toml.core) \n (parse-string "\n   title = \\"TOML\\"\n   [Foo]\n   bar=[1,2,3]")\n  ;; {"title" "TOML" "foo" {"bar" [1 2 3]}}\n  ``` \n TODO \n The parser is pretty solid (thanks to Kern) and complete.  \n In a way it implements a superset of TOML, since it successfully parses \n \n non-homogeneous arrays \n TOML with duplicate keys \n multiline strings \n \n As the TOML specification stabilizes, we\'ll raise errors according to specification. \n License \n Copyright © 2013 Luca Antiga. \n Distributed under the Eclipse Public License, the same as Clojure.', 'React.hiccup \n React 0% JSX, 100% hiccup \n Dig  React  but JSX feels a bit weird? React.hiccup to the rescue! \n React.hiccup is a complete replacement for JSX written in  sweet.js . \n React.hiccup uses a very clean, minimalistic notation - no HTML tags and no curly braces in HTML elements. \n Syntax \n React.hiccup syntax is heavily inspired by  hiccup , a popular  Clojure  HTML rendering library. \n In short, the syntax for a React.hiccup element is \n js\nhiccup [tag#id.class1.class2 {property1: value1, property2: value2} child1 child2 ...] \n e.g. \n js\nhiccup [div#foo.bar.baz {some: "property", another: this.props.anothervalue} \n         [p "A child element"] "Child text"] \n where the id, classes, property object and children are all optional. The className can be also specified \namong the properties, in this case it will be merged with the class names given after the tag. \n A child can be a string, a number or a multiline string (use the backtick)\n js\nhiccup [div#foo.bar.baz `This\nis a\n  multiline \n  comment`] \n A child can also be a JavaScript variable identifier \n js\nvar comment = "Some comment";\nhiccup [div#foo.bar.baz "The comment is: " comment] \n but in case it is anything more complex, e.g. an expression, it needs to be surrounded by parentheses \n js\nhiccup [div#foo.bar.baz "The comment is: " (this.state.comment)] \n or \n js\nvar one_or_two = 1;\nvar comment1 = "First comment";\nvar comment2 = "Second comment";\nhiccup [div#foo.bar.baz "The comment is: " (one_or_two == 1 ? comment1 : comment2 )] \n Note that this is not required in the property object \n js\nvar one_or_two = 1;\nvar comment1 = "First comment";\nvar comment2 = "Second comment";\nhiccup [div#foo.bar.baz {someprop: 1 > 0 ? true : false, someother: "other" + "prop" } "A child"] \n Shorthand react class declaration \n React.hiccup also comes with an optional macro  rclass  for declaring a React class \n js\nrclass FooBar = {\n  render: function() { ... }\n} \n expands to (omitting the sweet.js gensym) \n js\nvar FooBar = React.createClass({\n  render: function() { ... }\n}); \n while  \n js\nrclass window.FooBar = {\n  render: function() { ... }\n} \n expands to \n js\nwindow.FooBar = React.createClass({\n  render: function() { ... }\n}); \n Get it \n First install  sweet.js  if you don\'t have it already \n $ npm install -g sweet.js\n \n If you do have it, update it (0.4.0 is required) \n $ npm update -g sweet.js\n \n Then \n $ npm install react.hiccup\n \n All set. Now to compile a React.hiccup js file into a plain js file do \n $ sjs -m react.hiccup/macros -o foo_build.js foo.js\n \n To watch the file and have it automatically compiled whenever the file changes on disk \n $ sjs -m react.hiccup/macros -o foo_build.js -w foo.js\n \n Examples \n React frontpage examples \n Here\'s how  React frontpage examples  can be \nwritten  using React.hiccup . \n React tutorial \n For something more involved you can take a look at the  React tutorial . \n Here\'s the code in  JSX , and\nhere\'s the same code in  React.hiccup . \n License \n MIT license http://www.opensource.org/licenses/mit-license.php/ \n Copyright (C) 2014 Luca Antiga http://lantiga.github.io', "ki \n lisp + mori, sweet.js \n \n See  ki-lang.org  for more information or  try ki  straight in your browser. \n ki is a lisp with Clojure data structures and semantics that can be intermixed with Javascript code at any level. \n ki is a thin macro layer on top of  mori  plus a few constructs. \n ki is in flux, feel free to test it out but expect glitches. \n Currently available functions / special forms \n All of  mori . \n The following list of functions / special forms\n [] {} [$ ] {$ } add and apply atom bind catch chain cond def defmethod defmulti \ndefn deref div do doto eq falsey finally fn fnth geq gt if ifNot js leq letc \nlet lt loop mod mul neq nil not ns or prn recur reset str sub swap threadf \nthreadl truthy try use when whenNot while \n Plus destructuring and source maps. \n Take a look at the  tests  to keep up with the latest additions. \n Examples \n Require ki (this in turns expands into an appropriate require for mori)\n ki require core \n Mori's persistent data structures and Clojure(Script)-like api at your fingertips\n js\nvar foo = ki (vector 1 2 3)\nki (conj foo 4)\n// => [1 2 3 4] \n Plus lambdas\n js\nki (map (fn [a] (inc a)) (range 5))\n// => (1 2 3 4 5) \n Interoperability: write js in a ki form\n js\nvar fn1 = ki (js function (a,b) { return a + b + 2; }) \nat any level - e.g. you can use infix where it makes sense\n js\nvar fn2 = ki (fn [a b] (js a + b + 2)) \n and you can use ki wherever in js code\n js\nfunction somefunc (a) {\n  ki (toJs (filter (fn [el] (isEven el)) (range a))).forEach(function(el) {\n      console.log(el);\n      });\n  return [0, 1, 2, 3, 4].filter(ki (fn [el] (isEven el)));\n}\nconsole.log(somefunc(5));\n// => 0 \n// => 2 \n// => 4 \n// [0 2 4] \n Like a pro\n js\nki (take 6 (map (fn [x] (js x * 2)) (range 1000)))\n// => (0 2 4 6 8 10) \n Get it \n $ npm install -g ki\n \n All set. Now to compile a JavaScript file containing ki code into a plain JavaScript file do \n $ ki -o foo_build.js foo.js\n \n To watch the file and have it automatically compiled whenever the file changes on disk \n $ ki -w -o foo_build.js foo.js\n \n License \n MIT license http://www.opensource.org/licenses/mit-license.php/ \n Copyright (C) 2014-2015 Luca Antiga http://lantiga.github.io", 'redlock-clj \n Redlock  is an algorithm for distributed locks on top of a cluster of uncoordinated Redis instances. redlock-clj is a redlock implementation in Clojure leveraging on the  Carmine  Redis client. \n Algorithm  and  reference implementation  by  Salvatore Sanfilippo . \n Installation \n Leiningen: add the following to the  :dependencies  vector in  project.clj \n clojure\n[redlock-clj "0.1.0"] \n Usage \n Require redlock-clj \n clojure\n(require \'[redlock-clj.core :as redlock]) \n Define a cluster as a vector of connection options (see  Carmine  for details, in particular the  docs for wcar ): \n clojure\n(def cluster [{:pool {<opts1>} :spec {<opts1>}}\n              {:pool {<opts2>} :spec {<opts2>}}\n              {:pool {<opts3>} :spec {<opts3>}}]) \n e.g. \n clojure\n(def cluster [{:spec {:host "http://127.0.0.1" :port 6379}}\n              {:spec {:host "http://127.0.0.1" :port 6380}}\n              {:spec {:host "http://127.0.0.1" :port 6381}}]) \n The API consists of two functions, used for locking and unlocking a resource: \n clojure\n(defn lock! [cluster resource ttl & {:keys [retry-count retry-delay clock-drift-factor]}]) \n returning either a  lock  map  {:validity <validity-time> :resource <resource> :value <value>} , where  value  is a random  uuid  created by the client acquiring the lock, or  nil  in case the lock can\'t be acquired; \n clojure\n(defn unlock! [cluster lock]) \n which unlocks a previously acquired lock. \n Look at the  test  for an example of a distributed lock around a file-based counter. \n Run the test with  lein test  after starting at least two out of three  redis-server  instances with ports  6379 ,  6380 ,  6381 . \n License \n Copyright © 2014 Luca Antiga \n Distributed under the Eclipse Public License either version 1.0 or (at\nyour option) any later version.', 'tinydnnc \n Minimal C bindings for  tiny-dnn . \n WARNING : IN FLUX. \n Build \n To build the library:\n sh get_deps.sh\nmkdir build\ncd build\ncmake -DCMAKE_BUILD_TYPE:STRING=Release ../ && make \n To build the examples (mnist classification and XOR regression):\n cd example\nmake \n License \n BSD license https://opensource.org/licenses/BSD-3-Clause \n Copyright 2016, Luca Antiga, Orobix Srl (www.orobix.com).', 'pytorch2c \n NOTE: PyTorch is evolving rapidly. With the advent of tracing during execution and the upcoming GraphExecutor in ATen, that will be the way to run computation graphs in C++. \n ~~ NOTE: this project is currently under being reworked; instead of graph traversal, it will be based on the new tracing functionality being implemented in PyTorch after 0.2.0. This will allow cleaner code, more compact emitted code and proper handling of recurrent models.  ~~ \n A Python module for compiling (static)  PyTorch  graphs to C (relying on TH and THNN).  \n PyTorch2c inspects the computation graph and emits C code that performs the same computation. As long as a network is static (i.e. the graph doesn\'t change dynamically) it should produce a C source file that links to TH and THNN and can be compiled stand-alone. Interestingly, compiled graphs can be tested automatically by comparing what PyTorch produces to what the compiled code produces, given the same input. \n Caveats: \n* things are guaranteed to change in the PyTorch graph dept. Hopefully we\'ll be able to catch up with the changes as they happen.\n* in these initial phases there are lots of layers and operations missing (help is very welcome)\n* I\'m developing on macOS and Python 3.5 at the moment\n* PyTorch2c currently supports PyTorch version 0.1.10 \n TODO \n \n [x] Solve storage serialization issues \n [ ] Complete testing infrastructure (generate a number of input-output pairs) \n [x] Generate CMakeLists.txt as part of output for tests \n [-] Implement wrappers for the complete API (in progress) \n \n Trying things out \n Install  PyTorch , clone this repository and  cd pytorch2c . Then run the following scripts to download PyTorch and build TH and THNN:\n sh scripts/get_deps.sh\nsh scripts/build_deps.sh \nNow you can execute tests with  sh scripts/run_test.sh [test-name] , where  test-name  is the name of the corresponding Python script in the  test  directory, e.g.\n sh scripts/run_test.sh base\nsh scripts/run_test.sh feedforward\nsh scripts/run_test.sh mnist # currently broken due to PyTorch being in flux (issue with ConvNdBackward not being inspectable) \nTests return  1  if the value of the output tensor from the compiled code matches the value of the output tensor computed from PyTorch while compiling. \n To see the compiled files, look into the  out  directory. \n Example \n Example on a simple feedforward network:\n```python\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F \n import torch2c \n define the network \n import torch.nn as nn\nimport torch.nn.functional as F \n fc1 = nn.Linear(10,20)\nfc1.weight.data.normal_(0.0,1.0)\nfc1.bias.data.normal_(0.0,1.0) \n fc2 = nn.Linear(20,2)\nfc2.weight.data.normal_(0.0,1.0)\nfc2.bias.data.normal_(0.0,1.0) \n model = lambda x: F.log_softmax(fc2(F.relu(fc1(x)))) \n create an input variable \n data = Variable(torch.rand(10,10)) \n compile the graph and the test \n torch2c.compile(model(data),\'feedforward\',out_path,compile_test=True)\n``` \n Generated output (don\'t look at the ugly storage reading stuff for now):\n```C \n ifndef  FEEDFORWARD \n define  FEEDFORWARD \n include "TH.h" \n include "THNN.h" \n void feedforward(THFloatTensor  x_4510941984, THFloatTensor  x_4510944688)\n{\n  THFloatStorage  storage_x_4510941880 = THFloatStorage_newWithSize(2);\n  {\n  FILE  f = fopen("data/x_4510941880.th","rb");\n  if (!f) {\n  THError("cannot open file data/x_4510941880.th for reading");\n  }\n  long size;\n  size_t result = fread(&size,sizeof(long),1,f);\n  char  bytes = (char  ) storage_x_4510941880->data;\n  uint64_t remaining = sizeof(float) * storage_x_4510941880->size;\n  result = fread(bytes,sizeof(float),storage_x_4510941880->size,f);\n  fclose(f);\n  }\n  THLongStorage  size_x_4510941880 = THLongStorage_newWithSize1(2);\n  THLongStorage  stride_x_4510941880 = THLongStorage_newWithSize1(1);\n  THFloatTensor  x_4510941880 = THFloatTensor_newWithStorage(storage_x_4510941880,0,size_x_4510941880,stride_x_4510941880);\n  THLongStorage_free(size_x_4510941880);\n  THLongStorage_free(stride_x_4510941880);\n  THFloatStorage  storage_x_4510941776 = THFloatStorage_newWithSize(40);\n  {\n  FILE  f = fopen("data/x_4510941776.th","rb");\n  if (!f) {\n  THError("cannot open file data/x_4510941776.th for reading");\n  }\n  long size;\n  size_t result = fread(&size,sizeof(long),1,f);\n  char  bytes = (char  ) storage_x_4510941776->data;\n  uint64_t remaining = sizeof(float) * storage_x_4510941776->size;\n  result = fread(bytes,sizeof(float),storage_x_4510941776->size,f);\n  fclose(f);\n  }\n  THLongStorage  size_x_4510941776 = THLongStorage_newWithSize2(2,20);\n  THLongStorage  stride_x_4510941776 = THLongStorage_newWithSize2(20,1);\n  THFloatTensor  x_4510941776 = THFloatTensor_newWithStorage(storage_x_4510941776,0,size_x_4510941776,stride_x_4510941776);\n  THLongStorage_free(size_x_4510941776);\n  THLongStorage_free(stride_x_4510941776);\n  THFloatStorage  storage_x_4510941672 = THFloatStorage_newWithSize(20);\n  {\n  FILE  f = fopen("data/x_4510941672.th","rb");\n  if (!f) {\n  THError("cannot open file data/x_4510941672.th for reading");\n  }\n  long size;\n  size_t result = fread(&size,sizeof(long),1,f);\n  char  bytes = (char  ) storage_x_4510941672->data;\n  uint64_t remaining = sizeof(float) * storage_x_4510941672->size;\n  result = fread(bytes,sizeof(float),storage_x_4510941672->size,f);\n  fclose(f);\n  }\n  THLongStorage  size_x_4510941672 = THLongStorage_newWithSize1(20);\n  THLongStorage  stride_x_4510941672 = THLongStorage_newWithSize1(1);\n  THFloatTensor  x_4510941672 = THFloatTensor_newWithStorage(storage_x_4510941672,0,size_x_4510941672,stride_x_4510941672);\n  THLongStorage_free(size_x_4510941672);\n  THLongStorage_free(stride_x_4510941672);\n  THFloatStorage  storage_x_4510941568 = THFloatStorage_newWithSize(200);\n  {\n  FILE  f = fopen("data/x_4510941568.th","rb");\n  if (!f) {\n  THError("cannot open file data/x_4510941568.th for reading");\n  }\n  long size;\n  size_t result = fread(&size,sizeof(long),1,f);\n  char  bytes = (char  ) storage_x_4510941568->data;\n  uint64_t remaining = sizeof(float) * storage_x_4510941568->size;\n  result = fread(bytes,sizeof(float),storage_x_4510941568->size,f);\n  fclose(f);\n  }\n  THLongStorage  size_x_4510941568 = THLongStorage_newWithSize2(20,10);\n  THLongStorage  stride_x_4510941568 = THLongStorage_newWithSize2(10,1);\n  THFloatTensor  x_4510941568 = THFloatTensor_newWithStorage(storage_x_4510941568,0,size_x_4510941568,stride_x_4510941568);\n  THLongStorage_free(size_x_4510941568);\n  THLongStorage_free(stride_x_4510941568);\n  THFloatTensor  x_4510617224 = THFloatTensor_new();\n  THFloatTensor  addBuffer_x_4510617224 = THFloatTensor_new();\n  THNN_FloatLinear_updateOutput(NULL,x_4510941984,x_4510617224,x_4510941568,x_4510941672,addBuffer_x_4510617224);\n  THFloatTensor  x_4510961736 = THFloatTensor_new();\n  THNN_FloatThreshold_updateOutput(NULL,x_4510617224,x_4510961736,0,0,0);\n  THFloatTensor  x_4510961888 = THFloatTensor_new();\n  THFloatTensor  addBuffer_x_4510961888 = THFloatTensor_new();\n  THNN_FloatLinear_updateOutput(NULL,x_4510961736,x_4510961888,x_4510941776,x_4510941880,addBuffer_x_4510961888);\n  THFloatTensor  x_4510962040 = THFloatTensor_new();\n  THNN_FloatLogSoftMax_updateOutput(NULL,x_4510961888,x_4510962040);\n  THFloatTensor_copy(x_4510944688,x_4510962040);\n  THFloatTensor_free(x_4510962040);\n  THFloatTensor_free(x_4510961888);\n  THFloatTensor_free(addBuffer_x_4510961888);\n  THFloatTensor_free(x_4510961736);\n  THFloatTensor_free(x_4510617224);\n  THFloatTensor_free(addBuffer_x_4510617224);\n  THFloatTensor_free(x_4510941568);\n  THFloatStorage_free(storage_x_4510941568);\n  THFloatTensor_free(x_4510941672);\n  THFloatStorage_free(storage_x_4510941672);\n  THFloatTensor_free(x_4510941776);\n  THFloatStorage_free(storage_x_4510941776);\n  THFloatTensor_free(x_4510941880);\n  THFloatStorage_free(storage_x_4510941880);\n} \n endif \n ``` \n License \n MIT license http://www.opensource.org/licenses/mit-license.php/ \n Copyright (C) 2017 Luca Antiga, Orobix Srl', 'Hello MNIST \n Simple MNIST classifier written in PyTorch Lightning. \n Install Dependencies \n bash\npip install -r requirements.txt \n PyTorch / Lightning \n ```bash \n lightning \n python pl_mnist.py\n```', 'Lightning JupyterLab Manager \n This template app demonstrates how to create JupyterLab notebook with Lightning and manage them for multiple users. \n \n \n \n Testing \n Find the  e2e  tests associated with this application.']
syhw,["Homebrew \n Features, usage and installation instructions are  summarised on the homepage . \n What Packages Are Available? \n \n Type  brew search  for a list. \n Or visit  braumeister.org  to browse packages online. \n Or use  brew desc  to browse packages from the command line. \n \n More Documentation \n brew help ,  man brew  or check  our documentation . \n Troubleshooting \n First, please run  brew update  and  brew doctor . \n Second, read the  Troubleshooting Checklist . \n If you don't read these it will take us far longer to help you with your problem. \n Security \n Please report security issues to security@brew.sh. \n Who Are You? \n Homebrew's current maintainers are  Misty De Meo ,  Adam Vandenberg ,  Jack Nagel ,  Mike McQuaid ,  Brett Koonce  and  Tim Smith . \n Homebrew was originally created by  Max Howell . \n License \n Code is under the  BSD 2 Clause (NetBSD) license . \n Donations \n We accept tips through  Gratipay . \n", 'You need to put probt in a probt/ folder here', 'Requirements \n make \npython \npyreplib (optional, for the verification/sorting/trashing part) \n Pipeline \n Download replays \n You need to download replays from replays sites, it can take a while\n(2 days with the no leeching policy of ICCUP for instance): \n cd gosugamers|iccup|teamliquid && make\n \n Unify replays \n You may have downloaded the same replay numerous times, so we have \nto unify them, we do that with a hash (sha256) of the file: \n cd unifier && vim unifier.py\n# change to match the folders of the recently downloaded replays\nmake\n \n Verify and sort replays \n Some of the replays may be corrupted and should be trashed, some others\nmay be sorted in the wrong match-up folder. You will now verify and sort \nthem, for that, you need the pyreplib python library: \n cd match_ups && python verify_and_sort.py PATH_TO_REPLAY_FOLDER\n \n In PATH_TO_REPLAY_FOLDER/trash, you have the corrupted replays, other valid\nreplays should be in their right match-up folder. \n Special case of ICCUP \n ICCUP server has some kind of anti-leech policy, so the download script waits\nfor some time after each failure. Also in the iccup/crawl.py, you can change\n(commented line) if you want to use the users replays, the current default is\nto use only the gosus replays of ICCUP. \n Archive \n Do not forget about excluding all the trash your OS can put in: \n tar czf archive-name.tar.gz --exclude *.DS_Store replays/\n', "Requirements \n BWAPI   \nMS Visual C++ 2008 (optional,  a DLL is here )  \nStarCraft: Broodwar   \n Pipeline used \n First, get some replays (I did get mine from ICCUP, TeamLiquid and GosuGamers),\n with  scripts . \nI use ChaosLauncher to inject BWAPI in StarCraft: Broodwar, in Release mode\n(Debug will not be able to deal with un-analysed/un-serialized \n BWTA  maps), with: \n     auto_menu = SINGLE_PLAYER\n    auto_restart = ON\n    map = maps\\replays\\some_folder\\*.rep\n    mapiteration = SEQUENCE\n \n Regions \n Serialization \n To serialize, we  hash  BWTA's regions and ChokeDepReg regions on their TilePosition center. \n ChokeDepReg: Choke dependant regions \n ChokeDepReg are regions created from the center of chokes to MAX(MIN_CDR_RADIUS(currently 9), CHOKE_WIDTH) build tiles (TilePositions) away, in a Voronoi tiling fashion. Once that is done, ChokeDepRegs are completed with BWTA::Regions minus existing ChokeDepRegs. \n Result for one replay \n Data is partly redundant, in a way that eases analysis. \n RGD file \n     [Replay Start]\n    RepPath: $(PATH/TO/REP)\n    MapName: $MAPNAME\n    NumStartPositions: $N\n    The following players are in this replay:\n    <list of \n    $PLAYER_ID, $PLAYER_NAME, $START_LOC\n    separated by newlines>\n    Begin replay data:\n    <list of\n    $FRAME_NUMBER,$PLAYER_ID,$ACTION,[$ACTION_DEP_ARGS]\n    separated by newlines>\n    [EndGame]\n \n Actions can be:\n-  Created ,$UNIT_ID,$UNIT_TYPE,($POS_X,$POS_Y),$CDR_HASH,$REGION_HASH \n-  Destroyed ,$UNIT_ID,$UNIT_TYPE,($POS_X,$POS_Y) \n-  Discovered ,$UNIT_ID,$UNIT_TYPE \n-  R ,$MINERALS,$GAS,$GATHERED_MINERALS,$GATHERED_GAS, \n$SUPPLY_USED,$SUPPLY_TOTAL \n-  ChangedOwnership ,$UNIT_ID \n-  Morph ,$UNIT_ID,$UNIT_TYPE,($POS_X,$POS_Y) \n- $FIRST_FRAME,$DEFENDER_ID, IsAttacked ,($ATTACK_TYPES),\n($INIT_POSITION.X,$INIT_POSITION.Y), \n$INIT_CDR, $INIT_REGION,\n{$PLAYER_ID:{$TYPE:$MAX_NUMBER_INVOLVED}}, \n($SCORE_GROUND_CDR,$SCORE_GROUND_REGION,$SCORE_AIR_CDR, \n$SCORE_AIR_REGION,$SCORE_DETECT_CDR,$SCORE_DETECT_REGION, \n$ECO_IMPORTANCE_CDR,$ECO_IMPORTANCE_REGION, \n$TACT_IMPORTANCE_CDR,$TACT_IMPORTANCE_REGION), \n{$PLAYER_ID:{$TYPE:$NUMBER_AT_END}},($LAST_POSITION.X,$LAST_POSITION.Y), \n{$PLAYER_ID:$NB_WORKERS_DEAD},$LAST_FRAME,$WINNER_ID(OPTIONAL)   \n $ATTACK_TYPES are in {DropAttack, GroundAttack, AirAttack, InvisAttack, UnknownAttackError}.   \n $TACT_IMPORTANCE  and  $ECO_IMPORTANCE  are from in-game heuristics.   \n ROD file \n     <list of\n    $FRAME,$PLAYER_ID,$ORDER,TargetOrPosition,$POS_X,$POS_Y\n    separated by newlines>\n \n with TargetOrPositions being  T  if the order in on a unit,  P  if it's a map position.   \n RLD file \n     Regions,$REGIONS_IDS_COMMA_SEPARATED\n    $REGION_ID, $DIST, $DIST, ...\n    $REGION_ID, $DIST, $DIST, ...\n    .\n    .\n    .\n    ChokeDepReg,$REGIONS_IDS_COMMA_SEPARATED\n    $REGION_ID, $DIST, $DIST, ...\n    $REGION_ID, $DIST, $DIST, ...\n    .\n    .\n    .\n    [Replay Start]\n    <list of\n    $FRAME,$UNIT_ID,$POS_X,$POS_Y\n    $FRAME,$UNIT_ID,Reg,$REGION_ID\n    $FRAME,$UNIT_ID,CDR,$CDR_ID\n    separated by newlines>\n \n With new lines uniquely when the unit moved (of Position and/or Region and/or ChokeDepReg) in the last refresh rate frames (100 atm). \n Tuning \n You can tune these defines. \n Final words \n This work is an extension of  bwrepanalysis", 'Analyze BW Data \n This is the code to perform (exploratory) analysis of replays-extracted\nBroodwar data with  this syntax , \nusing  this code , \nexample datasets are: \n \n 7708 Broodwar gosu games , \nand the  corresponding replays   \n 1029 Broodwar ICCUP users games ,\nand the  corresponding ICCUP replays \n \n Possibilities \n Models \n Special Tactics: a Bayesian Approach to Tactical Decision-making, Gabriel Synnaeve, Pierre Bessière, CIG (IEEE) 2012 \n is in battles/tactics.py \n A Dataset for StarCraft AI & an Example of Armies Clustering, Gabriel Synnaeve, Pierre Bessière, AIIDE Workshop on AI in Adversarial Real-time games 2012  the model in here (also end of the "Strategy" chapter in my Ph.D thesis) is in clusterize/armies.py \n Also see other directories.', 'How to install: \n pip install dotfiles\ncd\ngit clone this-repo\nmkdir ~/.vim/backup\nmkdir ~/.vim/swap\ndotfiles -s -C Dotfiles/dotfilesrc\n \n Vundle \n git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim\nvim +PluginInstall +qall\n \n Mac OS X \n \n System Preferences -> {Accessibility -> Zoom} and {Trackpad -> Scoll & Zoom} \n Iterm2 -> Preferences -> untick Native Full Screen \n Oh My Zsh \n MacVim  +  ln -s /Applications/MacVim.app/Contents/bin/vim /usr/local/bin/vim \n FUSE \n (optional)  Anaconda \n Karabiner-Elements  with "For all devices" caps_lock -> escape, and "Apple Internal Keyboard / ..." fn -> left_control and left_control -> fn \n (optional)  Rectangle \n (optional)  git clone git@github.com:skywind3000/z.lua.git \n Homebrew: \n /usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)" \n minimal Casks:\n brew install ccache clang-format cmake coreutils doxygen ffmpeg freetype fzf gflags htop imagemagick ninja node openssl optipng pv ripgrep rlwrap sloccount sshfs tldr vim wget x264 x265 xsv zsh-completions zsh-history-substring-search zsh-syntax-highlighting zstd cargo \n further brew installs:\n brew tap cantino/mcfly && brew install zoxide mcfly && brew install MisterTea/et/et \n fd and ripgrep:  cargo install fd-find ripgrep \n config term: solarized dark + darker background +  git clone https://github.com/powerline/fonts  + copy Meslo Dotted in font book + replace font in profile/text for Meslo LG S, font size 16 + in profile/terminal silence bell \n', 'hackernews \n Seeks the most popuplar (by karma and comments) articles on HN and make a \ntext corpus out of them. \n Usage \n lein run\n \n and you get the articles in data, see:\n    folder-prefix\n    textonly\n    sortby\nin src/hackernews/core.clj \n Temporary workflow: once articles are downloaded (in text format) in data/\n    ./mahout-distribution-0.7/bin/mahout seqdirectory --input data --output seqfiles\n    ./mahout-distribution-0.7/bin/mahout seq2sparse -i seqfiles -o mahoutvectors -wt tf --minDF 5 --maxDFPercent 90\n    ./mahout-distribution-0.5/bin/mahout lda -i mahoutvectors/tf-vectors -o hackernews-lda -k 42 -v 1000 -x 20 -ow \n Development \n If you use vimclojure with nailgun, you will need to give it more memory: \n java -Xmx512m -Xms512m -cp "$LEIN_CLASSPATH" ...\n \n License \n Copyright (C) 2012 Gabriel Synnaeve \n Distributed under the Eclipse Public License, the same as Clojure. \n TODO \n Compare our tika (including boilerpipe) solution to:\n- decruft\n- unfluff \n Use stemming \n Hacker News specialized fetcher (self.HN links that require login)', 'tika-server \n It serves tika applied on an URL for webhooks (Celery). \n Usage \n server: \n lein run\n \n client (HTTP GET): \n http://IP_SERVICE:9998/?doc=URLENCODED_URL_FOR_TIKA_TO_PARSE\n \n do not forgot the  ? \n for instance test: \n lein run\ncurl -X GET http://127.0.0.1:9998/?doc=https%3A%2F%2Fgithub.com%2Fring-clojure%2Fring%2Fwiki%2FGetting-Started\n \n License \n Copyright (C) 2012 Gabriel Synnaeve / Cloverfeed \n Distributed under the Eclipse Public License, the same as Clojure.', 'Preparing the dataset \n With the TIMIT dataset (.wav sound files, .wrd words annotations and .phn \nphones annotations): \n \n \n Encode the wave sound in MFCCs:\nrun  python mfcc_and_gammatones.py --htk-mfcc $DATASET/train  and\n python mfcc_and_gammatones.py --htk-mfcc $DATASET/test  producing the  .mfc  \nfiles with HCopy according to  wav_config  ( .mfc_unnorm  is no normalization) \n \n \n Adapt the annotations given in .phn in frames into nanoseconds in .lab\nrun  python timit_to_htk_labels.py $DATASET/train  and \n python timit_to_htk_labels.py $DATASET/test  producing the  .lab  files \n \n \n Replace phones according to the seminal HMM paper of 1989:\n"Speaker-independant phone recognition using hidden Markov models", phones \nnumber (i.e. number of lines in the future labels dictionary) should go from \n61 to 39.\nrun  python substitute_phones.py $DATASET/train  and \n python substitute_phones.py $DATASET/test \n \n \n run  python create_phonesMLF_and_labels.py $DATASET/train  and \n python create_phonesMLF_and_labels.py $DATASET/test \n \n \n You can also do that with a  make prepare dataset=DATASET_PATH . \n You\'re ready for training with HTK (mfc and lab files)! \n Training the HMM models \n Train monophones HMM: \n make train_monophones dataset_train_folder=PATH_TO_YOUR_DATASET/train\nmake test_monophones dataset_test_folder=PATH_TO_YOUR_DATASET/test\n \n Or, train triphones: \n TODO\nmake train_triphones dataset_train_folder=PATH_TO_YOUR_DATASET/train\nmake test_triphones dataset_test_folder=PATH_TO_YOUR_DATASET/test\n \n Replacing the GMM by DBNs \n \n \n Do full states forced alignment of the  .mlf  files with  make align .  \n \n \n Do a first preparation of the dataset with  src/timit_to_numpy.py  or \n src/mocha_timit_to_numpy.py  (depending on the dataset) on the above aligned \n .mlf  files. \n \n \n Train the deep belief networks on it, either using  DBN/DBN_timit.py  or \n DBN/DBN_Gaussian_timit.py  or  DBN/DBN_Gaussian_mocha_timit.py  (see inside \nthese files for parameters). Save (pickle at the moment) the DBN objects and \nthe states/indices mappings. \n \n \n Use the serialized DBN objects and states/indices mappings with \n viterbi.py , just  cd  to  DBN  and do: \n python ../src/viterbi.py output_dbn.mlf /fhgfs/bootphon/scratch/gsynnaeve/TIMIT/test/test.scp ../tmp_train/hmm_final/hmmdefs --d ../dbn_5.pickle ../to_int_and_to_state_dicts_tuple.pickle \n \n', "Binaural audio (for headphones) from surround sound. \n This is a quick hack trying to recreate binaural sounds with MIT's KEMAR HRTF dataset (Dirac recordings).", 'PhotoResto \n Restaurant reviews from picture on Android. \n If this application is useful to you, you can donate: \n License \n WTFPL - Do What the Fuck You Want to Public License \n http://www.wtfpl.net', 'DTW in Cython \n Easy to use Dynamic Time Warping in Cython. You can set your own distance in\n _example_dist.pyx  by respecting the function name and the type. \n \n Dependencies:\n - Cython\n - Numpy\n - (optional) Matplotlib \n Use/test: \n (optional) cython -a dtw/_dtw.pyx && open dtw/_dtw.html\npython -c "import pyximport; import numpy as np;\\\npyximport.install(setup_args={\'include_dirs\':[np.get_include()]});\\\nimport dtw; dtw._dtw.test()"\n \n Install: \n make install\n \n See  dtw.DTW(...)  and  dtw._dtw.test()  for the usage of the DTW function. \n I keep  DTW_a  and  DTW_f  separated to be able to restrict the search (by a \nmaximum warp or a heuristic) in  DTW_f , while I can experiment with parallel \ncomputing of the  dist_array  for  DTW_a  (on GPUs).', 'colloc_topic_model \n Collocation Topic Model \n Usage \n lein run\n \n will run it on NIPS abstracts. \n License \n Copyright © 2013 Gabriel Synnaeve and Benjamin Boerschinger \n Distributed under the Eclipse Public License either version 1.0 or (at\nyour option) any later version.', 'segmample \n seg_colloc.py implements collocation model proto-type, gives sensible results but is probably way too inefficient to run on real data \n not sure about sampling scheme, current one works but may mix badly \n How to run on stupid test data? \n python seg.py test.ylt\n\npython seg_colloc.py test.ylt\n \n or run the fast (Cython) version on the Brent Ratner? \n python run_fast_seg.py br.ylt\n', 'Speech Embeddings \n Using embedding-based loss functions for phonetics/speech recognition. \n ABX-distance based embeddings: \n emb_from_ab_dist.py\n \n TODO write doc \n "phn2vec" embeddings: \n Phonetic annotations \n There is no silver bullet, you need phonetically annotated speech corpora\n(e.g. TIMIT or the Buckeye corpus). \n Phonemic annotations \n Then you can also work on the phonemic annotations, for that you need to\ntransform words into phonemes. I did a hack-job using the CMU phonemic dict.: \n python timit_words_to_phonemes.py\n \n You need to have the TIMIT corpus with a  train.scp  leading to  *.xyz  files\nhaving corresponding  *.wrd  files with word-level annotation (look at the\nconstant at the start of  timit_words_to_phonemes.py ). \n How to train the embedding? (Using word2vec from gensim) \n python mlf_to_text.py < ~/postdoc/datasets/TIMIT_train_dev_test/train/train.mlf >> timit_train_from_phones.txt\n \n or  \n python mlf_to_text.py --forcealigned --timitfoldings < ~/postdoc/datasets/TIMIT_train_dev_test/aligned_train.mlf >> timit_train_from_phones.txt\npython train_word2vec.py timit_train_from_phones.txt\n \n Same for the Buckeye corpus. \n Comparing two embeddings is as simple as: \n python train_word2vec.py timit_train_from_phones.txt timit_train_from_words.txt\n \n or  \n python train_word2vec.py timit_train_from_phones.txt buckeye_train_from_phones.txt\n \n Notes on the phone(me)s annotations: \n For the Buckeye corpus, "tq" (glotal stop in "cat") folded to "sil". \n For the TIMIT corpus, "dx" (flap in "butter") inexistent in "words" (phonemic\nannotation) version.', 'This is our entry to  the kaggle MEG\ncontest \n Example: \n wget data && untar it in ./data/\npython train.py\n \n Get started with the data analysis \n ipython notebook --pylab=inline \n And load the notebook that is here. \n TODO: \n \n other classifiers (sklearn) \n time-frequency representations (STFTs / Wavelets) \n HMM for the sequentiality/temporality (as in speech) \n structured learning https://pystruct.github.io \n deep learning (2D convnets, 1D temporal/seq nets) \n', "synth_imit \n The synthetic TIMIT dataset! \n This is a short script to recreate the TIMIT dataset with Mac OS X synthetic \nvoices using  say  (Speech Synthesis Manager). \n It's Friday afternoon > 5pm, I have rights to be silly! That could actually be\nuseful... \n OK, OK: How to? \n bash voice_it.sh\n", 'Requirements:\n  - Adaptor Grammar software (py-cfg) compiled in PY_CFG (Makefile)\n    (http://web.science.mq.edu.au/~mjohnson/Software.htm)\n  - Python\n    - gensim\n    - (optional) pattern \n To launch it on default settings (see in the Makefile, Providence corpus, \nchildren Naima, start age 11 months, end age 22 months), you need to put a .txt\nfile in ProvidenceFinal/ToSegment/my_corpus.txt with document boundaries @ \n(known) or @? (possible boundary but unknown). Then run:\n  - make prepare_topics\n  - make all \n You can play with the CHILD / EAGE / SAGE variables in the Makefile. E.g. : \n make just_basic_and_single CHILD=naima SAGE=11 EAGE=22 NITER=500\nmake test_wo_prefix_topic CHILD=naima SAGE=11 EAGE=22 NITER=500\nfor eage in  range 12 22 ; do make just_basic_and_single CHILD=naima SAGE=11 EAGE=$eage NITER=500; done \n To get all the data points and plot them, use e.g.: \n for eage in  range 12 22 ; do make just_basic_and_single CHILD=naima SAGE=11 EAGE=$eage NITER=500; done\npython src/plot_AGs_results.py \n the 500-520 iterations and 1000-1010 iterations are ran with a Gamma(100,0.001) PYP b parameter prior\nthe 600-620 iterations are ran with a Gamma(100,0.01) PYP b parameter prior', 'abnet \n ABNet is a "same/different"-based loss trained neural net. \n Data preprocessing \n To reproduce the results in the IEEE SLT 2014 paper, you need:\n - TIMIT with the standard train/dev/test split\n - To apply  make prepare_timit dataset=PATH_TO_YOUR_TIMIT  with  the timit tools , that will create all the needed features (Mel filterbanks), for this step,  spectral  is a requirement. \n Training a (deep) ABnet \n Then you can:\n - Align words and extract their dynamic time warped paths with:\n python align_words.py PATH_TO_TIMIT_TRAIN_FOLDER && python align_words.py PATH_TO_TIMIT_DEV_FOLDER && python align_words.py PATH_TO_TIMIT_TEST_FOLDER \nSee in this  align_words.py  for variants / size of words. This step needs  DTW_Cython \n - Train the ABnet on this DTW aligned word patterns, e.g. with:\n THEANO_FLAGS="device=gpu0" python run_exp_AB.py --dataset-path=dtw_words_train.joblib --dataset-name="timit_dtw" --prefix-output-fname="deep_cos_cos2" --iterator-type=dtw --nframes=7 --network-type=ab_net --debug-print=0 --debug-plot=0 --debug-time \n ABX evaluation \n If you want to evaluate it with ABX, you need to:\n - Create a folder with  *.npz  files containing timing and features, for that copy every filterbank numpy array and stack them as needed, e.g. with:\n for name in `find . -name "*_fbanks.npy" | grep train`; do cp $name npz7_train/`echo $name | awk -F \'/\' \'{print $4"_"$5}\'`; done\npython stack_fbanks.py npz7_train/*.npy \n - Use your trained ABnet to make the transformation of these filterbanks into the embedded features of the ABnet:\n mkdir deep_cos_cos2 && python embed_fbanks.py deep_cos_cos2_timit_dtw_fbank7_ab_net_adadelta.pickle PATH_TO_npz7_train deep_cos_cos2 \n - Make an ABX compatible  *.features  HDF5 file using:  python npz2h5features.py deep_cos_cos2 deep_cos_cos2.features \n - You can now do an ABX evaluation e.g. with:\n python ABX_repo/ABX_score.py deep_cos_cos2.features timit_ABX_train.phone.talker.task --ncore=8 --force\npython ABX_repo/collpanda.py timit_ABX_train.phone.talker.deep_cos_cos2.score timit_ABX_train.phone.talker.task timit_ABX_train.phone.talker.deep_cos_cos2.output\nbash ABX_repo/avg timit_ABX_train.phone.talker.deep_cos_cos2.output', 'DL4H \n Deep learning for hackers: a hands-on approach to machine learning and deep learning. \n Currently only covers "from logistic regression to adding hidden layers". \n The only requirement is to fully code something from python/numpy (or C/Cython) before allowing it to be done in Theano / using other\'s CUDA kernels \n TODO:\n  - more machine learning principles (e.g. cross-validation)\n  - convolutions in numpy\n  - RNNs', 'wer_are_we \n WER are we? An attempt at tracking states of the art(s) and recent results on speech recognition.  Feel free to correct! \n(Inspired by  Are we there yet? ) \n WER \n LibriSpeech \n (Possibly trained on more data than LibriSpeech.) \n | WER test-clean | WER test-other | Paper          | Published | Notes   |\n| :------------- | :------------- | :------------- | :-------- | :-----: |\n| 5.83% | 12.69% |  Humans   Deep Speech 2: End-to-End Speech Recognition in English and Mandarin  | December 2015 |  Humans  |\n| 1.8% | 2.9% |  HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units  | June 2021 | CNN-Transformer + Transformer LM (Self-Supervised, Libri-light-60K Unlabeled Data) |\n| 1.9% | 3.9% |  Conformer: Convolution-augmented Transformer for Speech Recognition  | May 2020 | Convolution-augmented-Transformer(Conformer) + 3-layer LSTM LM (data augmentation:SpecAugment) |\n| 1.9%  | 4.1%   |  ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context  | May 2020 | CNN-RNN-Transducer(ContextNet) + 3-layer LSTM LM (data augmentation:SpecAugment)  |\n| 2.0%  | 4.1%   |  End-to-end ASR: from Supervised to Semi-Supervised Learning with Modern Architectures  | November 2019 | Conv+Transformer AM (10k word pieces) with ConvLM decoding and Transformer rescoring + 60k hours unlabeled |\n| 2.1%  | 4.1%   |  Efficient Training of Neural Transducer for Speech Recognition  | May 2022 | Conformer Transducer (efficient 3-stage progressive training: 35 epochs in total) + Transformer LM |\n| 2.3% | 4.9%  |  Transformer-based Acoustic Modeling for Hybrid Speech Recognition   | October 2019 | Transformer AM (chenones) + 4-gram LM + Neural LM rescore (data augmentation:Speed perturbation and SpecAugment) |\n| 2.3%  | 5.0%   |  RWTH ASR Systems for LibriSpeech: Hybrid vs Attention  | September 2019, Interspeech | HMM-DNN + lattice-based sMBR + LSTM LM + Transformer LM rescoring (no data augmentation) |\n| 2.3%  | 5.2%   |  End-to-end ASR: from Supervised to Semi-Supervised Learning with Modern Architectures  | November 2019 | Conv+Transformer AM (10k word pieces) with ConvLM decoding and Transformer rescoring |\n| 2.2%  | 5.8%   |  State-of-the-Art Speech Recognition Using Multi-Stream Self-Attention With Dilated 1D Convolutions  | October 2019 | Multi-stream self-attention in hybrid ASR + 4-gram LM + Neural LM rescore (no data augmentation) |\n| 2.5%  | 5.8%   |  SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition  | April 2019 | Listen Attend Spell |\n| 3.2%  | 7.6%  |  From Senones to Chenones: Tied Context-Dependent Graphemes for Hybrid Speech Recognition  | October 2019 | LC-BLSTM AM (chenones) + 4-gram LM (data augmentation:Speed perturbation and SpecAugment) |\n| 3.19% | 7.64% |  The CAPIO 2017 Conversational Speech Recognition System  | April 2018 | TDNN + TDNN-LSTM + CNN-bLSTM + Dense TDNN-LSTM across two kinds of trees + N-gram LM + Neural LM rescore\n| 2.44%  |  8.29%   |  Improved Vocal Tract Length Perturbation for a State-of-the-Art End-to-End Speech Recognition System  | September 2019, Interspeech | encoder-attention-decoder + Transformer LM |\n| 3.80% | 8.76%  |  Semi-Orthogonal Low-Rank Matrix Factorization for Deep Neural Networks  | Interspeech, Sept 2018 | Kaldi recipe , 17-layer TDNN-F + iVectors|\n| 2.8%  | 9.3%   |  RWTH ASR Systems for LibriSpeech: Hybrid vs Attention  | September 2019, Interspeech | encoder-attention-decoder + BPE + Transformer LM (no data augmentation) |\n| 3.26% | 10.47% |  Fully Convolutional Speech Recognition  | December 2018 | End-to-end CNN on the waveform + conv LM|\n| 3.82% | 12.76% |  Improved training of end-to-end attention models for speech recognition  | Interspeech, Sept 2018 | encoder-attention-decoder end-to-end model |\n| 4.28% | |  Purely sequence-trained neural networks for ASR based on lattice-free MMI  | September 2016 | HMM-TDNN trained with MMI + data augmentation (speed) + iVectors + 3 regularizations |\n| 4.83% | |  A time delay neural network architecture for efficient modeling of long temporal contexts  | 2015 | HMM-TDNN + iVectors |\n| 5.15% | 12.73% |  Deep Speech 2: End-to-End Speech Recognition in English and Mandarin  | December 2015 | 9-layer model w/ 2 layers of 2D-invariant convolution & 7 recurrent layers, w/ 100M parameters trained on 11940h |\n| 5.51% | 13.97% |  LibriSpeech: an ASR Corpus Based on Public Domain Audio Books  | 2015 | HMM-DNN + pNorm *  |\n| 4.8%  | 14.5% |  Letter-Based Speech Recognition with Gated ConvNets  | December 2017 | (Gated) ConvNet for AM going to letters + 4-gram LM |\n| 8.01% | 22.49% | same,  Kaldi  | 2015 | HMM-(SAT)GMM |\n| | 12.51% |  Audio Augmentation for Speech Recognition  | 2015 | TDNN + pNorm + speed up/down speech | \n WSJ \n (Possibly trained on more data than WSJ.) \n | WER eval\'92    | WER eval\'93    | Paper          | Published | Notes   |\n| :------------- | :------------- | :------------- | :-------- | :-----: |\n| 5.03% | 8.08% |  Humans   Deep Speech 2: End-to-End Speech Recognition in English and Mandarin  | December 2015 |  Humans  |\n| 2.9% | |  End-to-end Speech Recognition Using Lattice-Free MMI  | September 2018 | HMM-DNN LF-MMI trained (biphone) |\n| 3.10% | |  Deep Speech 2: End-to-End Speech Recognition in English and Mandarin  | December 2015 | 9-layer model w/ 2 layers of 2D-invariant convolution & 7 recurrent layers, w/ 100M parameters |\n| 3.47% | |  Deep Recurrent Neural Networks for Acoustic Modelling  | April 2015 | TC-DNN-BLSTM-DNN |\n| 3.5%  | 6.8%  |  Fully Convolutional Speech Recognition  | December 2018 | End-to-end CNN on the waveform + conv LM|\n| 3.63% | 5.66% |  LibriSpeech: an ASR Corpus Based on Public Domain Audio Books  | 2015 | test-set on open vocabulary (i.e. harder), model = HMM-DNN + pNorm *  |\n| 4.1% | |  End-to-end Speech Recognition Using Lattice-Free MMI  | September 2018 | HMM-DNN E2E LF-MMI trained (word n-gram) |\n| 5.6% | |  Convolutional Neural Networks-based Continuous Speech Recognition using Raw Speech Signal  | 2014 | CNN over RAW speech (wav) |\n| 5.7%  | 8.7%  |  End-to-end Speech Recognition from the Raw Waveform  | June 2018 | End-to-end CNN on the waveform| \n Hub5\'00 Evaluation (Switchboard / CallHome) \n (Possibly trained on more data than SWB, but test set = full Hub5\'00.) \n | WER (SWB) | WER (CH) | Paper          | Published | Notes   |\n| :-------  | :------- | :------------- | :-------- | :-----: |\n| 4.9% | 9.5%  |  An investigation of phone-based subword units for end-to-end speech recognition  | April 2020 | 2 CNN + 24 layers Transformer encoder and 12 layers Transformer decoder model with char BPE and phoneme BPE units. |\n| 5.0% | 9.1%  |  The CAPIO 2017 Conversational Speech Recognition System  | December 2017 | 2 Dense LSTMs + 3 CNN-bLSTMs across 3 phonesets from  previous Capio paper  & AM adaptation using parameter averaging (5.6% SWB / 10.5% CH single systems) |\n| 5.1% | 9.9%  |  Language Modeling with Highway LSTM  | September 2017 | HW-LSTM LM trained with Switchboard+Fisher+Gigaword+Broadcast News+Conversations, AM from  previous IBM paper |\n| 5.1% |       |  The Microsoft 2017 Conversational Speech Recognition System  | August 2017 | ~2016 system + character-based dialog session aware (turns of speech) LSTM LM |\n| 5.3% | 10.1% |  Deep Learning-based Telephony Speech Recognition in the Wild  | August 2017 | Ensemble of 3 CNN-bLSTM (5.7% SWB / 11.3% CH single systems)\n| 5.5% | 10.3% |  English Conversational Telephone Speech Recognition by Humans and Machines  | March 2017 | ResNet + BiLSTMs acoustic model, with 40d FMLLR + i-Vector inputs, trained on SWB+Fisher+CH, n-gram + model-M + LSTM + Strided (à trous) convs-based LM trained on Switchboard+Fisher+Gigaword+Broadcast |\n| 6.3% | 11.9% |  The Microsoft 2016 Conversational Speech Recognition System  | September 2016 | VGG/Resnet/LACE/BiLSTM acoustic model trained on SWB+Fisher+CH, N-gram + RNNLM language model trained on Switchboard+Fisher+Gigaword+Broadcast |\n| 6.3% | 13.3%  |  An investigation of phone-based subword units for end-to-end speech recognition  | April 2020 | 2 CNN + 24 layers Transformer encoder and 12 layers Transformer decoder model with char BPE and phoneme BPE units. Trained only on SWBD 300 hours. |\n| 6.6% | 12.2% |  The IBM 2016 English Conversational Telephone Speech Recognition System  | June 2016 | RNN + VGG + LSTM acoustic model trained on SWB+Fisher+CH, N-gram + "model M" + NNLM language model |\n| 6.8% | 14.1% |  SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition  | April 2019 | Listen Attend Spell |\n| 8.5% | 13% |  Purely sequence-trained neural networks for ASR based on lattice-free MMI  | September 2016 | HMM-BLSTM trained with MMI + data augmentation (speed) + iVectors + 3 regularizations + Fisher |\n| 9.2% | 13.3% |  Purely sequence-trained neural networks for ASR based on lattice-free MMI  | September 2016 | HMM-TDNN trained with MMI + data augmentation (speed) + iVectors + 3 regularizations + Fisher (10% / 15.1% respectively trained on SWBD only) |\n| 12.6% | 16% |  Deep Speech: Scaling up end-to-end speech recognition  | December 2014 | CNN + Bi-RNN + CTC (speech to letters), 25.9% WER if trained  only  on SWB |\n| 11% | 17.1% |  A time delay neural network architecture for efficient modeling of long temporal contexts  | 2015 | HMM-TDNN + iVectors |\n| 12.6% | 18.4% |  Sequence-discriminative training of deep neural networks  | 2013 | HMM-DNN +sMBR |\n| 12.9% | 19.3% |  Audio Augmentation for Speech Recognition  | 2015 | HMM-TDNN + pNorm + speed up/down speech |\n| 15% | 19.1% |  Building DNN Acoustic Models for Large Vocabulary Speech Recognition  | June 2014  | DNN + Dropout |\n| 10.4% | |  Joint Training of Convolutional and Non-Convolutional Neural Networks  | 2014 | CNN on MFSC/fbanks + 1 non-conv layer for FMLLR/I-Vectors concatenated in a DNN |\n| 11.5% | |  Deep Convolutional Neural Networks for LVCSR  | 2013 | CNN |\n| 12.2% | |  Very Deep Multilingual Convolutional Neural Networks for LVCSR  | September 2015 | Deep CNN (10 conv, 4 FC layers), multi-scale feature maps |\n| 11.8% | 25.7% |  Improved training of end-to-end attention models for speech recognition  | Interspeech, Sept 2018 | encoder-attention-decoder end-to-end model, trained on 300h SWB | \n Rich Transcriptions \n | WER RT-02 | WER RT-03 | WER RT-04 | Paper          | Published | Notes   |\n| :-------  | :-------- | :-------- | :------------- | :-------- | :-----: |\n| 8.1% | 8.0%  |       |  The CAPIO 2017 Conversational Speech Recognition System  | April 2018 | 2 Dense LSTMs + 3 CNN-bLSTMs across 3 phonesets from  previous Capio paper  & AM adaptation using parameter averaging  |\n| 8.2% | 8.1%  | 7.7%  |  Language Modeling with Highway LSTM  | September 2017 | HW-LSTM LM trained with Switchboard+Fisher+Gigaword+Broadcast News+Conversations, AM from  previous IBM paper |\n| 8.3% | 8.0%  | 7.7%  |  English Conversational Telephone Speech Recognition by Humans and Machines  | March 2017 | ResNet + BiLSTMs acoustic model, with 40d FMLLR + i-Vector inputs, trained on SWB+Fisher+CH, n-gram + model-M + LSTM + Strided (à trous) convs-based LM trained on Switchboard+Fisher+Gigaword+Broadcast | \n Fisher (RT03S FSH) \n | WER     | Paper  | Published | Notes   |\n| :------ | :----- | :-------- | :-----: |\n| 9.6% |  Purely sequence-trained neural networks for ASR based on lattice-free MMI  | September 2016 | HMM- BLSTM  trained with MMI + data augmentation (speed) + iVectors + 3 regularizations + SWBD |\n| 9.8% |  Purely sequence-trained neural networks for ASR based on lattice-free MMI  | September 2016 | HMM- TDNN  trained with MMI + data augmentation (speed) + iVectors + 3 regularizations + SWBD | \n TED-LIUM \n | WER Test | Paper          | Published | Notes   |\n| :------- | :------------- | :-------- | :-----: |\n| 5.6% |  The RWTH ASR System for TED-LIUM release 2: Improving Hybrid HMM with SpecAugment  | April 2020 | HMM-BLSTM + iVectors + SpecAugment + sMBR + Transformer LM |\n| 6.5% |  The CAPIO 2017 Conversational Speech Recognition System  | April 2018 | TDNN + TDNN-LSTM + CNN-bLSTM + Dense TDNN-LSTM across two kinds of trees |\n| 11.2% |  Purely sequence-trained neural networks for ASR based on lattice-free MMI  | September 2016 | HMM-TDNN trained with LF-MMI + data augmentation (speed perturbation) + iVectors + 3 regularizations |\n| 15.3% |  TED-LIUM: an Automatic Speech Recognition dedicated corpus  | May 2014 | Multi-layer perceptron (MLP) with bottle-neck feature extraction | \n CHiME6 (multiarray noisy speech) \n | WER (fixed LM) | WER (unlimited LM) | Paper | Published | Notes |\n| :---- | :---- | :------------- | :---- | :-------------  |\n| 31.0% | 30.5% |  The USTC-NELSLIP Systems for CHiME-6 Challenge  | May 2020 | WPE + SSA + GSS + Data Augment (Speed, Volume) + SpecAugment + 8 AMs fusion (2 Single-feature AM + 6 Multi-feature AM)\n| 35.1% | 34.5% |  The IOA Systems for CHiME-6 Challenge  | May 2020 | WPE + multi-stage GSS + SpecAugment + Data Augment (Noise, Reverberation, Speed) + 3 AMs fusion (CNN-TDNNF / CNN-TDNN-BLSTM / CNN-BLSTM)\n| 35.8% | 33.9% |  The STC System for the CHiME-6 Challenge  | May 2020 | WPE + GSS + SpecAugment + 3 AMs fusion ( 2 TDNN-F / CNN-TDNNF + stats + SpecAugment + self-attention + sMBR) + MBR Decoding |\n| 51.3% | 51.3% |  CHiME-6 Challenge: Tackling Multispeaker Speech Recognition for Unsegmented Recordings | May 2020 | WPE + GSS + Data Augment (Noise, Reverberation, Speed) + TDNNF | Baseline | \n CHiME (noisy speech) \n | clean | real | sim | Paper | Published | Notes |\n| :------ | :----- | :----- | :----- | :----- | :-----: |\n| 3.34% | 21.79% | 45.05% |  Deep Speech 2: End-to-End Speech Recognition in English and Mandarin  | December 2015 | 9-layer model w/ 2 layers of 2D-invariant convolution & 7 recurrent layers, w/ 68M parameters |\n| 6.30% | 67.94% | 80.27% |  Deep Speech: Scaling up end-to-end speech recognition  | December, 2014 |  CNN + Bi-RNN + CTC (speech to letters) | \n TODO \n PER \n TIMIT \n (So far, all results trained on TIMIT and tested on the core test set.) \n | PER     | Paper  | Published | Notes   |\n| :------ | :----- | :-------- | :-----: |\n| 12.9%   |  Instantaneous Frequency Filter-Bank Features for Low Resource Speech Recognition Using Deep Recurrent Architectures  | September 2021 | Li-GRU with FMLLR + IFFB + FBANK + IFFB-FMLLR features |\n| 13.8%   |  The Pytorch-Kaldi Speech Recognition Toolkit  | February 2019 | MLP+Li-GRU+MLP on MFCC+FBANK+fMLLR.  Silence phones are removed from reference and hypothesis transcripts!  |\n| 14.9%   |  Light Gated Recurrent Units for Speech Recognition  | March 2018 | Removing the reset gate in GRU, using ReLU activation instead of tanh and batch normalization |\n| 16.5%   |  Phone recognition with hierarchical convolutional deep maxout networks  | September 2015 | Hierarchical maxout CNN + Dropout |\n| 16.5%   |  A Regularization Post Layer: An Additional Way how to Make Deep Neural Networks Robust  | 2017 | DBN with last layer regularization |\n| 16.7%   |  Combining Time- and Frequency-Domain Convolution in Convolutional Neural Network-Based Phone Recognition  | 2014 | CNN in time and frequency + dropout, 17.6% w/o dropout |\n| 16.8%   |  An investigation into instantaneous frequency estimation methods for improved speech recognition features  | November 2017 | DNN-HMM with MFCC + IFCC features |\n| 17.3%   |  Segmental Recurrent Neural Networks for End-to-end Speech Recognition  | March 2016 | RNN-CRF on 24(x3) MFSC |\n| 17.6%   |  Attention-Based Models for Speech Recognition  | June 2015 | Bi-RNN + Attention |\n| 17.7%   |  Speech Recognition with Deep Recurrent Neural Networks  | March 2013 | Bi-LSTM + skip connections w/ RNN transducer (18.4% with CTC only) |\n| 18.0%   |  Learning Filterbanks from Raw Speech for Phone Recognition  | October 2017 | Complex ConvNets on raw speech w/ mel-fbanks init |\n| 18.8%   |  Wavenet: A Generative Model For Raw Audio  | September 2016 | Wavenet architecture with mean pooling layer after residual block + few non-causal conv layers |\n| 23%     |  Deep Belief Networks for Phone Recognition  | 2009 | (first, modern) HMM-DBN | \n LM \n TODO \n Noise-robust ASR \n TODO \n BigCorp™®-specific dataset \n TODO? \n Lexicon \n \n WER: word error rate \n PER: phone error rate \n LM: language model \n HMM: hidden markov model \n GMM: Gaussian mixture model \n DNN: deep neural network \n CNN: convolutional neural network \n DBN: deep belief network (RBM-based DNN) \n TDNN-F: a factored form of time delay neural networks (TDNN) \n RNN: recurrent neural network \n LSTM: long short-term memory \n CTC: connectionist temporal classification \n MMI: maximum mutual information (MMI), \n MPE: minimum phone error \n sMBR: state-level minimum Bayes risk \n SAT: speaker adaptive training \n MLLR: maximum likelihood linear regression \n FMLLR: Feature space Maximum Likelihood Linear Regression \n LDA: (in this context) linear discriminant analysis \n MFCC:  Mel frequency cepstral coefficients \n FB/FBANKS/MFSC:  Mel frequency spectral coefficients \n IFCC: Instantaneous frequency cosine coefficients (https://github.com/siplabiith/IFCC-Feature-Extraction) \n IFFB: Instantaneous frequency filter-bank features \n VGG: very deep convolutional neural networks from Visual Graphics Group, VGG is an architecture of 2 {3x3 convolutions} followed by 1 pooling, repeated \n', 'vgml_workshop_nips2016', 'AUTOGRADPP \n This is an experimental C++ frontend to pytorch\'s C++ backend. Use at your own\nrisk. \n How to build:\n```\ngit submodule update --init --recursive \n On Linux: \n cd pytorch; python setup.py build; \n On macOS \n cd pytorch; LDSHARED="cc -dynamiclib -undefined dynamic_lookup" python setup.py build; \n cd ..; mkdir -p build; cd build\ncmake .. -DPYTHON_EXECUTABLE:FILEPATH=$(which python)  # helpful if you use anaconda\nmake -j\n``` \n Stuff \n \n Check out the  MNIST example , which tries to replicate PyTorch\'s MNIST model + training loop \n The principled way to write a model is probably something like \n AUTOGRAD_CONTAINER_CLASS(MyModel) {\n  // This does a 2D convolution, followed by global sum pooling, followed by a linear.\n public:\n  void initialize_parameters() override {\n    myConv_ = add(Conv2d(1, 50, 3, 3).stride(2).make(), "conv");\n    myLinear_ = add(Linear(50, 1).make(), "linear");\n  }\n  variable_list forward(variable_list x) override {\n    auto v = myConv_->forward(x);\n    v = v.mean(-1).mean(-1);\n    return myLinear_.forward({v});\n  }\n private:\n  Container myLinear_;\n  Container myConv_;\n} \n \n Some things are not implemented:\n- CUDNN is waiting for  this massive PR  and more work on the PyTorch end to port it to C++\n- Lookuptables are not implemented. Even if they are sparse will be some work to get working\n- No Batchnorm but this is pretty easy.\n- Only SGD is implemented and the rest of the optimizers are just copying Python code from PyTorch over. \n Otherwise, everything else works. There may be breaking API changes as I make it easier to write models.', 'syhw.github.io \n Gabriel Synnaeve   \n \n Just some links \n \n Twitter \n Pinboard \n Résumé \n', 'This is a short script to parse a raw text dump (done with  DiscordChatExporter ) of the 1v1-report channel from the CivFR discord and compute  TrueSkill  ratings for each player. For 1v1 this is equivalent to ELO. This would converge to a representative rating faster in team and FFA games though. \n \n leaderboard.txt  gives the rankings according to the order in which the match  were actually  played. \n leaderboard_N.txt  gives the rankings for 10 random shuffling of the matches orders, to show you the variability in such a ranking. \n If you look in the code, there is an example for how to use it for getting a probability of a balanced match:\n CivFRMalm rating 22.9, Snippy rating 9.4, CivFRMalm vs. Snippy draw chance 0.23\nLege rating 23.1, CivFRMalm rating 22.9, Lege vs. CivFRMalm draw chance 0.72 \n command used for the  DiscordChatExporter : \n dotnet bin/Debug/net6.0/DiscordChatExporter.Cli.dll export -c 934946383566356582 -f PlainText \n \n Quick hack provided under the: \n DO WHAT THE FUCK YOU WANT TO PUBLIC LICENSE\n                   Version 2, December 2004 \n Copyright (C) 2004 Sam Hocevar  sam@hocevar.net \n Everyone is permitted to copy and distribute verbatim or modified\ncopies of this license document, and changing it is allowed as long\nas the name is changed. \n        DO WHAT THE FUCK YOU WANT TO PUBLIC LICENSE\n \n TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION \n \n You just DO WHAT THE FUCK YOU WANT TO. \n', 'Gabriel Synnaeve']
JJediny,['Cartoview \n Geospatial Enterprise Web Applications Framework developed using  Django \n Explore the  demo \n \n Compatible with ESRI and OGC \n Deploy and manage ready made enterprise web applications \n App Market, very fast deployment of web apps directly from the browser \n Windows installer , get up and running in minutes. Installation is very simple, just download your  app(s) , install and configure on your server and you are ready to go. \n The apps and architecture of CartoView make it easy to efficiently develop and maintain enterprise applications. \n \n Screenshots \n \n \n \n Cartoview is developed with \n \n Python 2.7 \n Django 1.6 \n PostGresql \n PostGIS \n GeoDjango \n GDAL/OGR \n OpenDataCatalog \n Leaflet \n Esri Leaflet \n JQuery \n Bootstrap \n \n License \n Licensed under the Apache License Version 2.0 which grants you the right to customize, extend, modify, repackage, resell, and many other potential uses of the system.', 'Web-Map-Context-Documentation \n XML/JSON Examples and approaches to Solving the issue of how a specific grouping of one or more maps from one or more map servers can be described in a portable, platform-independent format for storage in a repository or for transmission between clients  ', 'datajson-builder \n Description \n This Jekyll projects provides an easy to manage and fast method for generating a data.json file from YAML files.   \n See  Project Open Data  for more information on the  data.json schema . \n This project is only an  alpha  prototype and is not ready for production, yet. \n Installation \n No installation is required to add new datasets or modify existing datasets. \n If you want to decompile an existing data.json into YAML/Markdown posts, then you\'ll need to install NPM.  Installation instructions for a development build are below. \n ```\nsudo apt-get install nginx\nsudo mkdir /var/www\nsudo chown -R vagrant:vagrant /var/www \n Edit /etc/nginx/sites-availble/default to listen to :8000 and point to /var/www \n sudo /etc/init.d/nginx restart \n \n sudo su -\ngpg --keyserver hkp://keys.gnupg.net --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3\n\\curl -sSL https://get.rvm.io | bash -s stable\nsource /usr/local/rvm/scripts/rvm\nrvm get stable\nrvm list known\nrvm install ruby-2.0.0-p353\nrvm --default use ruby-2.0.0-p353\ngem install jekyll \n \n sudo apt-get install npm\nsudo ln -s /usr/bin/nodejs /usr/bin/node\ncd ~/datajson-builder\nnpm install\nsudo npm install -g gulp \n \n gulp\n``` \n Usage \n The Jekyll template will automatically build the data.json file.  If not using GitHub pages, run  jekyll build  locally. \n To decompile an existing data.json file, such as  http://state.gov/data.json , run the following from the root directory: \n Shell\n./decompile_datajson.js --dest DEST --file FILE\n./decompile_datajson.js --dest DEST --url URL \n For example: \n Shell\n./decompile_datajson.js --dest \'_posts/external\' --file \'data.json\'\n./decompile_datajson.js --dest \'_posts/external\' --url \'http://state.gov/data.json\' \n If combining an external data.json and a locally managed data.json, it\'s a good idea to decompile the external data.json into a distinct folder, such as  _posts/external . \n Contributing \n Pull requests are welcome.  The code for this project is available on GitHub at  https://github.com/project-open-data/datajson-builder . \n LICENSE \n This project constitutes a work of the United States Government and is not subject to domestic copyright protection under 17 USC § 105. \n However, because the project utilizes code licensed from contributors and other third parties, it therefore is licensed under the MIT License. http://opensource.org/licenses/mit-license.php. Under that license, permission is granted free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the conditions that any appropriate copyright notices and this permission notice are included in all copies or substantial portions of the Software. \n THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.', 'FederalProjectSchema \n DRAFT ideation to establish a standard schema for Federally Funded Science and Infrastructure "Projects" \n Public Domain \n This project constitutes a work of the United States Government and is not subject to domestic copyright protection under 17 USC § 105. Additionally, we waive copyright and related rights in the work worldwide through the  CC0 1.0 Universal public domain dedication . \n All contributions to this project will be released under the CC0 dedication. By submitting a pull request, you are agreeing to comply with this waiver of copyright interest.', 'Compliance Masonry \n \n \n \n Compliance Masonry is a CLI that allows users to construct certification documentation using the  OpenControl Schema . See  Benefits  for more explanation. \n Quick Start with CLI \n Installing \n \n \n Install Go \n \n \n Install the tool\n bash\ngo get github.com/opencontrol/compliance-masonry-go \n \n Run the CLI\n compliance-masonry-go \n \n Examples \n Compliance Masonry examples in the wild:\n*  cloud.gov compliance data repository  +  documentation generated by Compliance Masonry \n Documentation Format \n Compliance Masonry uses the  OpenControl v2 Schema . \n Benefits \n Modern applications are built on existing systems such as S3, EC2, and Cloud Foundry. Documentation for how these underlying systems fulfill NIST controls or PCI SSC Data Security Standards is a prerequisite for receiving authorization to operate (ATO). Unlike most  System Security Plan documentation , Compliance Masonry documentation is built using  OpenControl Schema , a machine readable format for storing compliance documentation. \n Compliance Masonry simplifies the process of certification documentations by providing:\n1. a data store for certifications (ex FISMA), standards (ex NIST-800-53), and the individual system components (ex AWS-EC2).\n2. a way for government project to edit existing files and also add new control files for their applications and organizations.\n3. a pipeline for generating clean and standardized certification documentation.', 'datagov-infrastructure \n scripts, documentation, and anything needed to build the infrastructure around Data.gov \n copy .template files to  name .tf and fill in appropriate values before running\n terraform plan  and  terraform apply \n variables can also be filled in by copying  terraform.tfvars.template  to \nterraform.tfvars and filling in there as listed by the example', 'OpenStandards \n A curation of various standards, specifications, and schemas used in the US Government', "Terria Map Static \n \n This is a static HTML (serverless) version of  Terria Map . \n That means you can create your own working version just by forking this repository! \n Features missing in this serverless version: \n \n No CORS proxy, so you can only use services that have CORS enabled, or through an existing CORS proxy. \n No authenticated proxy service. \n No Proj4 conversion service, so some files may not display, such as GeoJSON files using an obscure projection. \n No server-side OGR2OGR service, so the user can't drag Shapefiles or other non-standard format files into the browser. GeoJSON, CSV and KML still work. \n \n Updating this repository \n How this version was made: \n \n Check out https://github.com/TerriaJS/TerriaMap \n npm install \n gulp release \n This repository is the contents of the  wwwroot/build  directory. \n", 'Salt Master Server via CloudFormation \n This CloudFormation template creates a VPC, subnets, security groups and EC2 instances\nfor a Salt Master Server in AWS.', 'saltpatches \n Bitfusion.io patches for Saltstack \n patches \n Quilt stack containing:\n- docker_device - add a passthrough for the Docker "devices" keyword \n To install this patch you need the "quilt" utility \n This patch applies to version 2015.8.1 of Saltstack and should be installed on the  minion  as well as the master \n You should use the latest version of docker-py. \n   # pip install --upgrade docker-py\n \n If you get a client/server mismatch error, force the protocol version using \n   # salt \'myminion\' grains.setval docker.version \'"1.19"\'\n \n Note the quoting to ensure this is a string value type not a float', 'CurrentProjects \n Test Git Repo of all repos as mirrors for Data.gov', 'Peekr-API \n Peekr API allows running vulnerability scanning of Docker images through the free Peekr service. \n Basic Usage \n Step 1 \n Log in to Peekr  https://peekr.aquasec.com  and Get user name & API Key from UI\n \n Step 2 \n Generate  Athorization  header through Base64(user:API Key). This header should be included with all Peekr REST API calls.\n Authorization: Basic dXNlcm5hbWU6YXBpdG9rZW4= \n Step 3 \n POST /apiscan \n Scan your image by providing repository name, image tag and registry name. Don\'t forget to send the Authorization header as part of the request.\nNote: By default Peekr comes with 2 registries: \n Docker Hub \n Quay.io \n You can add your own private registries through the Peekr UI.\n```json\n    POST https://peekr.aquasec.com/apiscan \n {\n    "registry_name":"Docker Hub",\n    "image":"mongo",\n    "tag":"3.3.5"\n}\n \n ```\nThe response is a JSON that contains the scan_id. \n Step 4 \n Get scan results for the image using  scan_id  received from the  /apiscan  API\n GET https://peekr.aquasec.com/apiscan/<scan_id> \nIf scan is finished The response is a JSON contains scan results.\nIf still under scan you will get  "state": "PENDING" \n API \n \n Scan \n POST \n \n \n Scan Results \n GET   \n \n \n User Scans \n GET \n \n \n \n Scan \n POST /apiscan \n Description \n Returns all repository tags. \n Request (example) \n ```\n    Request URL:https://peekr.aquasec.com/apiscan\n    Request Method:POST \n Authorization: Basic dXNlcm5hbWU6YXBpdG9rZW4=\nContent-Type: application/json\n\n{\n    "registry_name":"Docker Hub",\n    "image":"mongo",\n    "tag":"latest"\n}\n \n ``` \n Response (example) \n ```json\n{\n  "scan_id": "4KLuSmKc3IAafUHWLR1e",\n  "state": "FINISHED"\n} \n ``` \n Scan Results \n GET /apiscan/:scan_id \n Description \n Will return  scan  and  scan_results .  scan_results  include: \n INSPECT : inspect data of the image (docker inspect) \n OS_INFO : basic OS image info \n SCAN_RESULTS : all vulnerabilities found nested by file/package they affect    \n Request (example) \n ```\n    Request URL:https://peekr.aquasec.com/apiscan/4KLuSmKc3IAafUHWLR1e\n    Request Method:GET \n Authorization: Basic dXNlcm5hbWU6YXBpdG9rZW4=\n \n ``` \n Response (example) \n {\n  "scan":{"scan_id": "4KLuSmKc3IAafUHWLR1e", "image_id": "sha256:2af0a84bf165c291e2033327584a67158b5befe2fa2d739e13f6c8c71998b634", "image": "mongo",…},\n  "scan_results": [\n    {\n        "scan_id": "4KLuSmKc3IAafUHWLR1e",\n        "type": "INSPECT",\n        "result":{"Id": "cf57aeefb867a91f97821d20ff490b63a83266bf5b9a331c1f04c3d71d581b74", "Args":["mongod" ], "Name": "/P9NZ9ZBsYi89",…}\n    },\n    {\n        "scan_id": "4KLuSmKc3IAafUHWLR1e",\n        "type": "OS_INFO",\n        "result": "Debian GNU/Linux 7 (wheezy)"\n    },\n    {\n        "scan_id": "4KLuSmKc3IAafUHWLR1e",\n        "type": "SCAN_RESULTS",\n        "result":[{"sha1": "", "filename": "libc6_2.13-38+deb7u10_amd64.deb", "vulnerabilities":[{"name": "CVE-2015-0235",…]\n    },\n    {\n        "scan_id": "4KLuSmKc3IAafUHWLR1e",\n        "type": "PROFILE_RESULTS",\n        "result":{"Name": "P9NZ9ZBsYi89", "Stat":{"cpu": "0.800000", "nthr": "16",…}\n    }\n  ]\n} \n User Scans \n GET /user_scans \n Description \n Returns all user performed scans.  \n Request (example) \n Request URL:https://peekr.aquasec.com/user_scans\n    Request Method:GET \n Response (example) \n ```json \n[\n  {\n    "scan_id": "4KLuSmKc3IAafUHWLR1e",\n    "image_id": "",\n    "image": "mongo",\n    "tag": "latest",\n    "registry": "https://index.docker.io/",\n    "date": 1462351404,\n    "date_original": 0,\n    "vulns_critical": 0,\n    "vulns_high": 22,\n    "vulns_medium": 63,\n    "vulns_low": 14,\n    "archived": false,\n    "state": "FINISHED",\n    "error_msg": "",\n    "reused_scan": false\n  },\n  {\n    "scan_id": "GfMy130vyTU14OvpmEem",\n    "image_id": "",\n    "image": "centos",\n    "tag": "latest",\n    "registry": "https://index.docker.io/",\n    "date": 1462273244,\n    "date_original": 0,\n    "vulns_critical": 0,\n    "vulns_high": 15,\n    "vulns_medium": 38,\n    "vulns_low": 9,\n    "archived": false,\n    "state": "FINISHED",\n    "error_msg": "",\n    "reused_scan": false\n  },\n] \n ```', 'OpenControl YAML editor \n A yaml generator for the OpenControl component format \n Public domain \n This project is in the worldwide  public domain . As stated in  CONTRIBUTING : \n \n This project is in the public domain within the United States, and copyright and related rights in the work worldwide are waived through the  CC0 1.0 Universal public domain dedication . \n All contributions to this project will be released under the CC0 dedication. By submitting a pull request, you are agreeing to comply with this waiver of copyright interest. \n', 'FedRAMP Tailored \n Supporting pages for FedRAMP Tailored. \n Developing on the site locally \n This site uses  Jekyll  and requires  Ruby 2.x . \n Install dependencies with Bundler: \n bundle install \n And run the site with Jekyll: \n bundle exec jekyll serve \n If all goes well, visit the site at  http://localhost:4000 . \n Public domain \n This project is in the worldwide  public domain . As stated in  CONTRIBUTING : \n \n This project is in the public domain within the United States, and copyright and related rights in the work worldwide are waived through the  CC0 1.0 Universal public domain dedication . \n All contributions to this project will be released under the CC0 dedication. By submitting a pull request, you are agreeing to comply with this waiver of copyright interest. \n', "brotop - Top for bro logs. \n \n \n Brotop lets you stream your bro logs to the browser for easy \ndebugging and a real-time glimpse into whats being processed. \n Features \n \n 100% dep free binary \n Auto-detect log locations. If BroTop can't find them use the --path switch. \n \n Download \n \n Releases \n \n Usage \n Just run  brotop  and everything would work. \n  Then open your browser to the port you set. (default port is 8080) \n ```\nusage: brotop [ ] \n Flags:\n  --help           Show help.\n  --debug          Enable debug mode.\n  --path=PATH      Bro log path.\n  -p, --port=PORT  Web server port.\n  -q, --quiet      Remove all output logging.\n  --version        Show application version.\n  ``` \n Building \n Make sure you have go installed. \n \n go get github.com/tools/godep \n go get github.com/jteeuwen/go-bindata/... \n make \n \n Package as dep or rpm \n make package", 'Table of Contents \n \n Getting Started \n Introduction \n Install Terraform \n Add the Terraform Directory to the PATH \n Verify the Terraform Installation \n \n \n Install the Plug-In \n Usage \n Credentials \n Basic Fuctions \n Advanced Example \n \n \n 1&1 Terraform Resources \n Server \n Public IP \n Shared Storage \n Private Network \n Firewall Policy \n VPN \n Monitoring Policy \n Load Balancer \n \n \n \n Getting Started \n Before you begin you will need to have signed up for a 1&1 account. The credentials you create during sign-up will be used to authenticate against the API. \n Introduction \n This is a plug-in for  Terraform . Terraform enables you to safely and predictably create, change, and improve production infrastructure. It is an open source tool that codifies APIs into declarative configuration files that can be shared amongst team members, treated as code, edited, reviewed, and versioned. \n Install Terraform \n Terraform must first be installed on the machine where you plan to run it. Terraform is distributed as a binary package for various platforms and architectures. \n To install Terraform, download  the appropriate package for your system . Terraform is packaged as a zip archive. \n After downloading, unzip the package into a directory where Terraform will be installed. (Example:  ~/terraform  or  c:\\terraform ) \n Add the Terraform Directory to the PATH \n Note: The adjustments to the PATH environment variable as outlined below are temporary. There are numerous examples available on the internet describing how to make permanent changes to environment variables for each particular operating system.  \n If you prefer not to change the PATH, you can invoke Terraform directly by going to the directory and using the command  ./terraform , or from anywhere using the full path such as  ./home/jdoe/terraform . \n To add the Terraform directory to your PATH: \n Linux   \n If you plan to run Terraform in a shell on Linux and placed the binary in the  /home/[username]/terraform/  directory, you would use the command: \n PATH=$PATH:/home/[username]/terraform\n \n Mac OSX \n If you plan to run Terraform in a shell on a Mac and placed the binary in the  /Users/[username]/terraform/  directory, you would use the command: \n PATH=$PATH:/Users/[YOUR-USER-NAME]/terraform\n \n Windows   \n If you plan to run  terraform.exe  in PowerShell on Windows and placed the binary in  c:\\terraform  directory, you would first find the existing value of PATH: \n echo $env:Path\n \n If it ends with a ;, then run: \n $env:Path += "c:\\terraform"\n \n If it does NOT end with a ;, then run: \n $env:Path += ";c:\\terraform"\n \n Verify the Terraform Installation \n After installing Terraform, verify the installation by executing  terraform  or  terraform.exe . You should see the default "usage" output similar to this: \n ```\n$ terraform\nusage: terraform [--version] [--help]   [ ] \n Available commands are:\n    apply       Builds or changes infrastructure\n    destroy     Destroy Terraform-managed infrastructure\n    get         Download and install modules for the configuration\n    graph       Create a visual graph of Terraform resources\n    init        Initializes Terraform configuration from a module\n    output      Read an output from a state file\n    plan        Generate and show an execution plan\n    push        Upload this Terraform module to Atlas to run\n    refresh     Update local state file against real resources\n    remote      Configure remote state storage\n    show        Inspect Terraform state or plan\n    taint       Manually mark a resource for recreation\n    validate    Validates the Terraform files\n    version     Prints the Terraform version\n``` \n Install the Plug-In \n Download the desired release archive from  the 1&1 Terraform Provider Releases . Extract the binary from the archive and place it in the same location you used for the Terraform binary in the previous step. It should have the name  terraform-provider-oneandone  or  terraform-provider-oneandone.exe . \n Build the Plug-In from Source \n The build process requires that the  GO  language be installed and configured on your system. \n Once you have GO installed and working, then retrieve the Terraform 1&1 provider source code using the following command: \n go get github.com/1and1/terraform-provider-oneandone\n \n Then change to the project directory and run  make install : \n cd $GOPATH/github.com/1and1/terraform-provider-oneandone\n\nmake install\n \n The resulting binary can be copied to the same directory you installed Terraform in. \n Usage \n We will go through a basic example of provisioning a server inside a Virtual Data Center after providing Terraform with our credentials. \n Credentials \n You can provide your credentials using the  ONEANDONE_TOKEN  environment variables, representing your 1&1 token, respectively. \n $ export ONEANDONE_TOKEN="oneandone-token"\n \n Or you can include your credentials inside the  main.tf  file: \n provider "oneandone" {\n    token = "oneandone-token"\n    retries = 100\n}\n \n Note:  retries  describes the number of retries while waiting for a resource to be provisioned. The default value is 50. \n Basic Functions \n Terraform uses text files to describe infrastructure and to set variables. These text files are called "Terraform configurations," and end in  .tf . \n First create a folder to hold your  .tf  files and change your current directory to the newly created one: \n $xslt\nmkdir example\ncd example\n \n Create the  main.tf  file, and copy the following into  main.tf : \n ```\n$xslt\nprovider "oneandone" {\n    token = "oneandone-token"\n} \n resource "oneandone_server" "server" {\n  name = "Example"\n  description = "Terraform 1and1 tutorial"\n  image = "ubuntu"\n  datacenter = "GB"\n  vcores = 1\n  cores_per_processor = 1\n  ram = 2\n  ssh_key_path = "/path/to/prvate/ssh_key"\n  hdds = [\n    {\n      disk_size = 60\n      is_main = true\n    }\n  ] \n provisioner "remote-exec" {\n    inline = [\n      "apt-get update",\n      "apt-get -y install nginx",\n    ]\n  }\n}\n``` \n This example will create a single 1&1 Server, and install  nginx  on it. Section  provisoner  uses  remote-exec  provisioner to install  nginx .   \nTo confirm that the  .tf  file uses correct syntax the best way is to run: \n $xslt\nterraform plan \n Which should produce output similar to this: \n ```$xslt\nThe refreshed state will be used to calculate this plan, but\nwill not be persisted to local or remote state storage. \n The Terraform execution plan has been generated and is shown below.\nResources are shown in alphabetical order for quick scanning. Green resources\nwill be created (or destroyed and then created if an existing resource\nexists), yellow resources are being changed in-place, and red resources\nwill be destroyed. Cyan entries are data sources to be read. \n Note: You didn\'t specify an "-out" parameter to save this plan, so when\n"apply" is called, Terraform can\'t guarantee this is what will execute. \n \n oneandone_server.server\n    cores_per_processor: "1"\n    datacenter:          "GB"\n    description:         "Terraform 1and1 tutorial"\n    hdds.#:              "1"\n    hdds.0.disk_size:    "60"\n    hdds.0.id:           " "\n    hdds.0.is_main:      "true"\n    image:               "ubuntu"\n    ips.#:               " "\n    name:                "Example"\n    ram:                 "2"\n    ssh_key_path:        "/path/to/prvate/ssh_key"\n    vcores:              "1"\n```\nIf Terraform configuration is not correct you will be seeing something like this: \n \n ```$xslt\nErrors: \n \n oneandone_server.server: "image": required field is not set\n``` \n \n The next step is to create infrastructure defined in  main.tf . \n $xslt\nterraform apply \n Which should produce output similar to this (truncated): \n ```$xslt\noneandone_server.server: Creating...\n  cores_per_processor: "" => "1"\n  datacenter:          "" => "GB"\n  description:         "" => "Terraform 1and1 tutorial"\n  hdds.#:              "0" => "1"\n  hdds.0.disk_size:    "" => "60"\n  hdds.0.id:           "" => " "\n  hdds.0.is_main:      "" => "true"\n  image:               "" => "ubuntu"\n  ips.#:               "" => " "\n  name:                "" => "1and1 Example"\n  ram:                 "" => "2"\n  ssh_key_path:        "" => "/Users/jasmingacic/.ssh/id_rsa"\n  vcores:              "" => "1"\noneandone_server.server: Still creating... (10s elapsed)\n...\noneandone_server.server: Still creating... (4m10s elapsed)\noneandone_server.server: Provisioning with \'remote-exec\'...\noneandone_server.server (remote-exec): Connecting to remote host via SSH...\noneandone_server.server (remote-exec):   Host: 77.68.14.77\noneandone_server.server (remote-exec):   User: root\noneandone_server.server (remote-exec):   Password: true\noneandone_server.server (remote-exec):   Private key: true\noneandone_server.server (remote-exec):   SSH Agent: true\noneandone_server.server (remote-exec): Connected!\noneandone_server.server (remote-exec): 0% [Working]\noneandone_server.server (remote-exec): Hit http://security.ubuntu.com trusty-security InRelease\n...\noneandone_server.server (remote-exec): Setting up nginx-core (1.4.6-1ubuntu3.7) ...\noneandone_server.server (remote-exec): Setting up nginx (1.4.6-1ubuntu3.7) ...\noneandone_server.server (remote-exec): Processing triggers for libc-bin (2.19-0ubuntu6.9) ...\noneandone_server.server: Creation complete \n Apply complete! Resources: 1 added, 0 changed, 0 destroyed. \n The state of your infrastructure has been saved to the path\nbelow. This state is required to modify and destroy your\ninfrastructure, so keep it safe. To inspect the complete state\nuse the  terraform show  command. \n State path: terraform.tfstate\n``` \n If terraform has failed to created desired infrastructure you can check for errors in  crash.log  file. If you want to retry simply remote  terraform.tfstate  and run  apply  again.  \n If you want to get detailed "DEBUG" onscreen information while running 1&1 provider plugin output, you can set the  TF_LOG  environment variable. \n From a shell on Linux or Mac, this can be done using export: \n export TF_LOG=1 \n In PowerShell on Windows: \n $env:TF_LOG = 1 \n After the infrastructure has been provisoned and you wish to update newly created server simply edit  main.tf . For example to rename the server do this: \n ```$xslt\nresource "oneandone_server" "server" {\n  name = "1and1 Example renamed"\n  description = "Terraform 1and1 tutorial"\n  image = "ubuntu"\n  datacenter = "GB"\n  vcores = 1\n  cores_per_processor = 1\n  ram = 2\n  ssh_key_path = "/path/to/prvate/ssh_key"\n  hdds = [\n    {\n      disk_size = 60\n      is_main = true\n    }\n  ] \n provisioner "remote-exec" {\n    inline = [\n      "apt-get update",\n      "apt-get -y install nginx",\n    ]\n  }\n}\n``` \n ```$xslt\n$ terraform plan\nRefreshing Terraform state in-memory prior to plan...\nThe refreshed state will be used to calculate this plan, but\nwill not be persisted to local or remote state storage. \n oneandone_server.server: Refreshing state... (ID: 30E10107507669DF1F27A0336D1EB672) \n The Terraform execution plan has been generated and is shown below.\nResources are shown in alphabetical order for quick scanning. Green resources\nwill be created (or destroyed and then created if an existing resource\nexists), yellow resources are being changed in-place, and red resources\nwill be destroyed. Cyan entries are data sources to be read. \n Note: You didn\'t specify an "-out" parameter to save this plan, so when\n"apply" is called, Terraform can\'t guarantee this is what will execute. \n ~ oneandone_server.server\n    name:     "1and1 Example" => "1and1 Example renamed" \n Plan: 0 to add, 1 to change, 0 to destroy.\n``` \n To remove the infrastructure you just created, run: \n ```$xslt\n$terraform destroy \n Do you really want to destroy?\n  Terraform will delete all your managed infrastructure.\n  There is no undo. Only \'yes\' will be accepted to confirm. \n Enter a value: yes\n``` \n Advanced Example \n A more advanced example is located from the  repository .\nDownload  main.tf , and after you customized it to your liking run: \n $xslt\n$terraform plan \n Review the output and then apply the configuration by running: \n $xslt\n$terraform apply \n 1&1 Terraform Resources  \n Server \n Example Server \n $xslt\nresource "oneandone_server" "server" {\n  name = "test_server"\n  description = "test description"\n  image = "ubuntu"\n  datacenter = "GB"\n  vcores = 1\n  cores_per_processor = 1\n  ram = 2\n  ip = "${oneandone_public_ip.ip.ip_address}"\n  ssh_key_path = "/path/to/private/key"\n  hdds = [\n    {\n      disk_size = 60\n      is_main = true\n    }\n  ]\n  monitoring_policy_id = "${oneandone_monitoring_policy.mp.id}"\n  firewall_policy_id = "${oneandone_firewall_policy.fw.id}"\n  loadbalancer_id = "${oneandone_loadbalancer.lb.id}"\n} \n Argument Reference \n | Parameter | Required | Type | Description |\n|---|---|---|---|\n| name | Yes | string | The name of the server. |\n| description | No | string | Description of the server |\n| image | Yes | String |  The name of a desired image to be provisioned with the server |\n| vcores | Yes | integer | Number of virtual cores. |\n| cores_per_processor | yes | int | Number of cores per processor |\n| ram | Yes | float | Size of ram. |\n| ssh_key_path | No | string | Path to private ssh key |\n| password | No | String | Desired password. |\n| datacenter | No | String | Location of desired 1and1 datacenter ["DE", "GB", "US", "ES" ] |\n| ip | No | String | IP address for the server |\n| hdds | Yes | Collection | List of HDDs. One HDD must be main. |\n|  disk_size | Yes | integer | The size of HDD  |\n|  is_main | No | Boolean | Indicates if HDD is to be used as main hard disk of the server  |\n| firewall_policy_id | No | String | ID of firewall policy |\n| monitoring_policy_id | No | String | ID of monitoring policy |\n| loadbalancer_id | No | String | ID of the load balancer | \n Public IP \n Example Public IP \n $xslt\nresource "oneandone_public_ip" "ip" {\n  "ip_type" = "IPV4"\n  "reverse_dns" = "test.1and1.com"\n  "datacenter" = "GB"\n} \n Argument Reference \n | Parameter | Required | Type | Description |\n|---|---|---|---|\n| ip_type | Yes | string | IPV4 or IPV6 |\n| reveres_dns| Yes | string |  |\n| datacenter_id | No | String |  TLocation of desired 1and1 datacenter ["DE", "GB", "US", "ES" ] | \n Shared Storage \n Example Shared Storage \n ```$xslt\nresource "oneandone_shared_storage" "storage" {\n  name = "test_storage1"\n  description = "1234"\n  size = 50 \n storage_servers = [\n    {\n      id = "${oneandone_server.server.id}"\n      rights = "RW"\n    },\n    {\n      id = "${oneandone_server.server02.id}"\n      rights = "RW"\n    }\n  ]\n}\n``` \n Argument Reference \n | Parameter | Required | Type | Description |\n|---|---|---|---|\n| name | Yes | string | The name of the private network. |\n| description | No | string | Description for the private network |\n| size | Yes | String |  Sized of the shared storage |\n| datacenter | No | String | Location of desired 1and1 datacenter ["DE", "GB", "US", "ES" ] |\n| storage_servers | No | Collection |  List of servers that will have access to the stored storage |\n| id | Yes | String |  ID of the server |\n| rights | Yes | String | Access rights to be assigned to the server ["RW","R"] | \n Private Network \n Example Private Network \n $xslt\nresource "oneandone_private_network" "pn" {\n  name = "pn_test",\n  description = "new stuff001"\n  datacenter = "GB"\n  network_address = "192.168.7.0"\n  subnet_mask = "255.255.255.0"\n  server_ids = [\n    "${oneandone_server.server.id}",\n    "${oneandone_server.server02.id}",\n  ]\n} \n Argument Reference \n | Parameter | Required | Type | Description |\n|---|---|---|---|\n| name | Yes | string | The name of the private network. |\n| description | No | string | Description for the private network |\n| datacenter | No | String |  Location of desired 1and1 datacenter ["DE", "GB", "US", "ES" ] |\n| network_address| No | String |  Network address for the private network |\n| subnet_mask | No | String |  Subnet mask for the private network |\n| server_ids | No | Collection |  List of servers that are to be associated with the private network | \n Firewall Policy \n Example Firewall Policy \n $xslt\nresource "oneandone_firewall_policy" "fw" {\n  name = "test_fw_011"\n  rules = [\n    {\n      "protocol" = "TCP"\n      "port_from" = 80\n      "port_to" = 80\n      "source_ip" = "0.0.0.0"\n    },\n    {\n      "protocol" = "ICMP"\n      "source_ip" = "0.0.0.0"\n    },\n    {\n      "protocol" = "TCP"\n      "port_from" = 43\n      "port_to" = 43\n      "source_ip" = "0.0.0.0"\n    },\n    {\n      "protocol" = "TCP"\n      "port_from" = 22\n      "port_to" = 22\n      "source_ip" = "0.0.0.0"\n    }\n  ]\n} \n Argument Reference \n | Parameter | Required | Type | Description |\n|---|---|---|---|\n| name | Yes | string | The name of the VPN. |\n| description | No | string | Description for the VPN |\n| rules | Yes | Collection |  Collection of firewall policy rules |\n| protocol | Yes | String |  The protocol for the rule ["TCP", "UDP", "TCP/UDP", "ICMP", "IPSEC"]|\n| port_from | No | String |  Defines the start range of the allowed port |\n| port_to | No | String |  Defines the end range of the allowed port |\n| source_ip | No | String |  Only traffic directed to the respective IP address | \n VPN \n Example VPN \n $xslt\nresource "oneandone_vpn" "vpn" {\n  datacenter = "GB"\n  name = "test_vpn_01"\n  description = "ttest descr"\n} \n Argument Reference \n | Parameter | Required | Type | Description |\n|---|---|---|---|\n| name | Yes | string | The name of the VPN. |\n| description | No | string | Description for the VPN |\n| datacenter | No | String |  Location of desired 1and1 datacenter ["DE", "GB", "US", "ES" ] |\n| download_path | No | String |  Location where VPN configuration will be downloaded. If "download_path" is not provided VPN config will be downloaded where terraform is executed. | \n Monitoring Policy \n Example Monitoring Policy \n ```$xslt\nresource "oneandone_monitoring_policy" "mp" {\n  name = "test_mp"\n  agent = true\n  email = "jasmin@stackpointcloud.com" \n thresholds = {\n    cpu = {\n      warning = {\n        value = 50,\n        alert = false\n      }\n      critical = {\n        value = 66,\n        alert = false\n      } \n }\nram = {\n  warning = {\n    value = 70,\n    alert = true\n  }\n  critical = {\n    value = 80,\n    alert = true\n  }\n},\nram = {\n  warning = {\n    value = 85,\n    alert = true\n  }\n  critical = {\n    value = 95,\n    alert = true\n  }\n},\ndisk = {\n  warning = {\n    value = 84,\n    alert = true\n  }\n  critical = {\n    value = 94,\n    alert = true\n  }\n},\ntransfer = {\n  warning = {\n    value = 1000,\n    alert = true\n  }\n  critical = {\n    value = 2000,\n    alert = true\n  }\n},\ninternal_ping = {\n  warning = {\n    value = 3000,\n    alert = true\n  }\n  critical = {\n    value = 4000,\n    alert = true\n  }\n}\n \n }\n  ports = [\n    {\n      email_notification = true\n      port = 443\n      protocol = "TCP"\n      alert_if = "NOT_RESPONDING"\n    },\n    {\n      email_notification = false\n      port = 80\n      protocol = "TCP"\n      alert_if = "NOT_RESPONDING"\n    },\n    {\n      email_notification = true\n      port = 21\n      protocol = "TCP"\n      alert_if = "NOT_RESPONDING"\n    }\n  ] \n processes = [\n    {\n      email_notification = false\n      process = "httpdeamon"\n      alert_if = "RUNNING"\n    },\n    {\n      process = "iexplorer",\n      alert_if = "NOT_RUNNING"\n      email_notification = true\n    }]\n}\n``` \n Argument Reference \n | Parameter | Required | Type | Description |\n|---|---|---|---|\n| name | Yes | string | The name of the VPN. |\n| description | No | string | Description for the VPN |\n| email | No | String |  Email address to which notifications monitoring system will send ] |\n| agent| Yes | Boolean |  Indicates which monitoring type will be used. True - this monitoring type, you must install an agent on the server.  False - monitor a server without installing an agent, then you cannot retrieve information such as free hard disk space or ongoing process.|\n| thresholds | Yes | Collection | Collection of thresholds for different types of resources  |\n|  cpu | Yes | Type |  CPU thresholds |\n|  warning | Yes | Type |  Warning alert |\n|  value | Yes | Integer |  Warning to be issued when the threshold is reached. from 1 to 100 |\n|  alert | Yes | Boolean |  If set true warning will be issued. |\n|  critical | Yes | Type |  Critical alert |\n|  value | Yes | Integer |  Warning to be issued when the threshold is reached. from 1 to 100 |\n|  alert | Yes | Boolean |  If set true warning will be issued. |\n|  ram | Yes | Type |  RAM threshold |\n|  warning | Yes | Type |  Warning alert |\n|  value | Yes | Integer |  Warning to be issued when the threshold is reached. from 1 to 100 |\n|  alert | Yes | Boolean |  If set true warning will be issued. |\n|  critical | Yes | Type |  Critical alert |\n|  value | Yes | Integer |  Warning to be issued when the threshold is reached. from 1 to 100 |\n|  alert | Yes | Boolean |  If set true warning will be issued. |\n|  disk | Yes | Type |  Hard Disk threshold |\n|  warning | Yes | Type |  Warning alert |\n|  value | Yes | Integer |  Warning to be issued when the threshold is reached. from 1 to 100 |\n|  alert | Yes | Boolean |  If set true warning will be issued. |\n|  critical | Yes | Type |  Critical alert |\n|  value | Yes | Integer |  Warning to be issued when the threshold is reached. from 1 to 100 |\n|  alert | Yes | Boolean |  If set true warning will be issued. |\n|  transfer | Yes | Type |  Data transfer threshold |\n|  warning | Yes | Type |  Warning alert |\n|  value | Yes | Integer |  Warning to be issued when the threshold is reached. from 1 to 100 |\n|  alert | Yes | Boolean |  If set true warning will be issued. |\n|  critical | Yes | Type |  Critical alert |\n|  value | Yes | Integer |  Warning to be issued when the threshold is reached. from 1 to 100 |\n|  alert | Yes | Boolean |  If set true warning will be issued. |\n|  internal_ping | Yes | type |  Ping threshold |\n|  warning | Yes | Type |  Warning alert |\n|  value | Yes | Integer |  Warning to be issued when the threshold is reached. from 1 to 100 |\n|  alert | Yes | Boolean |  If set true warning will be issued. |\n|  critical | Yes | Type |  Critical alert |\n|  value | Yes | Integer |  Warning to be issued when the threshold is reached. from 1 to 100 |\n|  alert | Yes | Boolean |  If set true warning will be issued. |\n| ports | No | Collection | Collection of ports that are to be monitored for the given condition |\n|  email_notification | Yes | boolean |  If set true email will be sent. |\n|  port | Yes | Integer |  Port number. |\n|  protocol | Yes | String |  The protocol of the port ["TCP", "UDP", "TCP/UDP", "ICMP", "IPSEC"]|\n|  alert_if | Yes | String |  Condition for the alert to be issued. |\n| processes | No | Collection | Collection of processes that are to be monitored for the given condition |\n|  email_notification | Yes | Boolean |  If set true email will be sent. |\n|  process | Yes | Integer |  Process name. |\n|  alert_if | Yes | String |  Condition for the alert to be issued. | \n Load Balancer \n Example Load Balancer \n $xslt\nresource "oneandone_loadbalancer" "lb" {\n  name = "test_lb"\n  method = "ROUND_ROBIN"\n  persistence = true\n  persistence_time = 60\n  health_check_test = "TCP"\n  health_check_interval = 300\n  datacenter = "GB"\n  rules = [\n    {\n      protocol = "TCP"\n      port_balancer = 8080\n      port_server = 8089\n      source_ip = "0.0.0.0"\n    },\n    {\n      protocol = "TCP"\n      port_balancer = 9090\n      port_server = 9099\n      source_ip = "0.0.0.0"\n    }\n  ]\n} \n Argument Reference \n Parameter | Required | Type | Description |\n|---|---|---|---|\n| name | Yes | String | The name of the load balancer. |\n| description | No | String | Description for the load balancer |\n| method | Yes | String | Balancing procedure ["ROUND_ROBIN", "LEAST_CONNECTIONS"] |\n| datacenter | No | String |  Location of desired 1and1 datacenter ["DE", "GB", "US", "ES" ] |\n| persistence | No | Boolean |  True/false defines whether persistence should be turned on/off |\n| persistence_time | No | Integer | Persistance duration in seconds |\n| health_check_test | No | String | ["TCP", "ICMP"]  |\n| health_check_test_interval | No | String |   |\n| health_check_test_path | No | String |   |\n| health_check_test_parser | No | String |   |\n| rules | Yes | Collection |  Collection of load balancing rules |\n|  protocol | Yes | String |  The protocol for the rule ["TCP", "UDP", "TCP/UDP", "ICMP", "IPSEC"]|\n|  port_balancer | Yes | String |   |\n|  port_server | Yes | String |   |\n|  source_ip | Yes | String |   |', 'OpenControl YAML editor \n A yaml generator for the OpenControl component format \n Derived work from: \n https://github.com/jdorn/json-editor \n JSON work - MIT License \n https://github.com/jdorn/json-editor/blob/master/LICENSE \n YAML work - Public domain \n This project is in the worldwide  public domain . As stated in  CONTRIBUTING : \n \n This project is in the public domain within the United States, and copyright and related rights in the work worldwide are waived through the  CC0 1.0 Universal public domain dedication . \n All contributions to this project will be released under the CC0 dedication. By submitting a pull request, you are agreeing to comply with this waiver of copyright interest. \n', 'MakeLinuxMint \n Makefile for Installing and Managing a Personal Linux Mint Desktop', 'Hugo template for Netlify CMS with Netlify Identity \n This is a small business template built with  Victor Hugo  and  Netlify CMS , designed and developed by  Darin Dimitroff ,  spacefarm.digital . \n Getting started \n Use our deploy button to get your own copy of the repository.  \n \n This will setup everything needed for running the CMS: \n \n A new repository in your GitHub account with the code \n Full Continuous Deployment to Netlify\'s global CDN network \n Control users and access with Netlify Identity \n Manage content with Netlify CMS \n \n Once the initial build finishes, you can invite yourself as a user. Go to the Identity tab in your new site, click "Invite" and send yourself an invite. \n Now you\'re all set, and you can start editing content! \n Local Development \n Clone this repository, and run  yarn  or  npm install  from the new folder to install all required dependencies. \n Then start the development server with  yarn start  or  npm start . \n Layouts \n The template is based on small, content-agnostic partials that can be mixed and matched. The pre-built pages showcase just a few of the possible combinations. Refer to the  site/layouts/partials  folder for all available partials. \n Use Hugo’s  dict  functionality to feed content into partials and avoid repeating yourself and creating discrepancies. \n CSS \n The template uses a custom fork of Tachyons and PostCSS with cssnext and cssnano. To customize the template for your brand, refer to  src/css/imports/_variables.css  where most of the important global variables like colors and spacing are stored. \n SVG \n All SVG icons stored in  site/static/img/icons  are automatically optimized with SVGO (gulp-svgmin) and concatenated into a single SVG sprite stored as a a partial called  svg.html . Make sure you use consistent icons in terms of viewport and art direction for optimal results. Refer to an SVG via the  <use>  tag like so: \n <svg width="16px" height="16px" class="db">\n  <use xlink:href="#SVG-ID"></use>\n</svg>', 'Ansible Coding Conventions \n General \n \n YAML files - All yaml files should use 2 space indents and end with  .yaml \n Variables - Use jinja variable syntax over deprecated variable syntax.   {{ var }}  not  $var \n Use spaces around jinja variable names.  {{ var }}  not  {{var}} \n Variables that are environment specific and that need to be overridden should be in ALL CAPS. \n Variables that are internal to the role should be lowercase. \n Prefix all variables defined in a role with the name of the role. Example:  EDXAPP_FOO \n Keep roles self contained - Roles should avoid including tasks from other roles when possible \n Plays should do nothing more than include a list of roles except where pre_tasks and post_tasks are required (to manage a load balancer for example) \n Tests - The .gitignore should ignore  *test*  to allow for local testing without polluting the repository \n Handlers - Any service requiring to restarted should be done via handlers. In other words, no hardcoded reboots of any service to include the host itself. \n \n Conditionals and return status \n \n Always use  when:  for conditionals - To check if a variable is defined  when: my_var is defined  or  when: my_var is not defined \n To learn more  (see  conditional execution ) \n \n Formatting \n Break long lines using yaml line continuation \n - debug: >\n      msg={{ test }} \n - debug:\n      msg: "{{ test }}" \n Roles \n Role Variables \n \n group_vars/all - Contains variable definitions that apply to all roles. \n "common" role - Contains variables and tasks that apply to all roles. \n Roles variables - Variables specific to a role should be defined in  /vars/main.yml. All variables should be prefixed with the role name. \n Variables that are environment specific and that need to be overridden should be in all caps. \n \n Role naming conventions \n \n Role names - Terse, lowercase, one word if possible \n Role task names - Terse, descriptive, spaces are OK \n Role handlers - Terse, descriptive, spaces are OK \n \n Secure vs. Insecure data \n As a general policy we want to protect the following data: \n \n Usernames (should be avoided in public repos) \n Public keys (keys are OK to be public) \n Hostnames (should never be hardcoded in play) \n Passwords, API keys (should never be hardcoded in a play) \n', '“Indian Land Cessions in the United States,” compiled by Charles C. Royce \n Sixty-seven maps from “Indian Land Cessions in the United States,” compiled by Charles C. Royce and published as the second part of the two-part Eighteenth Annual Report of the Bureau of American Ethnology to the Secretary of the Smithsonian Institution, 1896-1897 have been scanned, georeferenced in JPEG2000 format, and digitized to create this feature class of cession maps. The mapped cessions and reservations included in the 67 maps correspond to entries in the Schedule of Indian Land Cessions, “indicating the number and location of each cession by or reservation for the Indian tribes from the organization of the Federal Government to and including 1894, together with descriptions of the tracts so ceded or reserved, the date of the treaty, law or executive order governing the same, the name of the tribe or tribes affected thereby, and historical data and references bearing thereon,” as set forth in the subtitle of the Schedule. Go to this URL for full metadata: http://data.fs.usda.gov/geodata/edw/edw_resources/meta/S_USA.TRIBALCEDEDLANDS.xml Each Royce map was georeferenced against one or more of the following USGS 1:2,000,000 National Atlas Feature Classes contained in \\NatlAtlas_USGS.gdb: cities_2mm, hydro_ln_2mm, hydro_pl_2mm, plss_2mm, states_2mm. Cessions were digitized as a file geodatabase (GDB) polygon feature class, projected as NAD83 USA_Contiguous_Lambert_Conformal_Conic, which is the same projection used to georeference the maps. The feature class was later reprojected to WGS 1984 Web Mercator (auxiliary sphere) to optimize it for the Tribal Connections Map Viewer. Polygon boundaries were digitized as to not deviate from the drawn polygon edge to the extent that space could be seen between the digitized polygon and the mapped polygon at a viewable scale. Topology was maintained between coincident edges of adjacent polygons. The cession map number assigned by Royce was entered into the feature class as a field attribute. The Map Cession ID serves as the link referencing relationship classes and joining additional attribute information to 752 polygon features, to include the following: 1. Data transcribed from Royce’s Schedule of Indian Land Cessions: a. Date(s), in the case of treaties, the date the treaty was signed, not the date of the proclamation; b. Tribe(s), the tribal name(s) used in the treaty and/or the Schedule; and c. Map Name(s), the name of the map(s) on which a cession number appears; 2. URLs for the corresponding entry in the Schedule of Indian Land Cessions (Internet Archive) for each unique combination of a Date and reference to a Map Cession ID (historical references in the Schedule are included); 3. URLs for the corresponding treaty text, including the treaties catalogued by Charles J. Kappler in Indian Affairs: Laws and Treaties (HathiTrust Digital Library), executive order or other federal statute (Library of Congress and University of Georgia) identified in each entry with a reference to a Map Cession ID or IDs; 4. URLs for the image of the Royce map(s) (Library of Congress) on which a given cession number appears; 5. The name(s) of the Indian tribe or tribes related to each mapped cession, including the name as it appeared in the Schedule or the corresponding primary text, as well as the name of the present-day Indian tribe or tribes; and 6. The present-day states and counties included wholly or partially within a Map Cession boundary. During the 2017-2018 revision of the attribute data, it was noted that 7 of the Cession Map IDs are missing spatial representation in the Feature Class. The missing data is associated with the following Cession Map IDs: 47 (Illinois 1), 65 (Tennessee and Bordering States), 128 (Georgia), 129 (Georgia), 130 (Georgia), 543 (Indian Territory 3), and 690 (Iowa 2), which will be updated in the future. This dataset revises and expands the dataset published in 2015 by the U.S. Forest Service and made available through the Tribal Connections viewer, the Forest Service Geodata Clearinghouse, and Data.gov. The 2018 dataset is a result of collaboration between the Department of Agriculture, U.S. Forest Service, Office of Tribal Relations (OTR); the Department of the Interior, National Park Service, National NAGPRA Program; the U.S. Environmental Protection Agency, Office of International and Tribal Affairs, American Indian Environmental Office; and Dr. Claudio Saunt of the University of Georgia. The Forest Service and Dr. Saunt independently digitized and georeferenced the Royce cession maps and developed online map viewers to display Native American land cessions and reservations. Dr. Saunt subsequently undertook additional research to link Schedule entries, treaty texts, federal statutes and executive orders to cession and reservation polygons, which he agreed to share with the U.S. Forest Service. OTR revised the data, linking the Schedule entries, treaty texts, federal statues and executive orders to all 1,172 entries in the attribute table. The 2018 dataset has incorporated data made available by the National NAGPRA Program, specifically the Indian tribe or tribes related to each mapped cession, including the name as it appeared in the Schedule or the corresponding primary text and the name of the present-day Indian tribe or tribes, as well as the present-day states and counties included wholly or partially within a Map Cession boundary. This data replaces in its entirety the National NAGPRA data included in the dataset published in 2015. The 2015 dataset incorporated data presented in state tables compiled from the Schedule of Indian Land Cessions by the National NAGPRA Program. In recent years the National NAGPRA Program has been working to ensure the accuracy of this data, including the reevaluation of the present-day Indian tribes and the provision of references for their determinations. Changes made by the OTR have not been reviewed or approved by the National NAGPRA Program. The Forest Service will continue to collaborate with other federal agencies and work to improve the accuracy of the data included in this dataset. Errors identified since the dataset was published in 2015 have been corrected, and we request that you notify us of any additional errors we may have missed or that have been introduced. Please contact Rebecca Hill, Policy Analyst, U.S. Forest Service, Office of Tribal Relations, at rebeccahill@fs.fed.us with any questions or concerns with regard to the data included in this dataset.  \n Citation: \n https://www.loc.gov/resource/g3701em.gct00002/?sp=2&st=gallery \n License: \n https://creativecommons.org/publicdomain/zero/1.0/', 'Introduction \n Netlify StatusKit is a template to deploy your own Status pages on Netlify. \n \n Netlify StatusKit is released under the  MIT License .\nPlease make sure you understand its  implications and guarantees . \n \n Project Status \n This project is no longer being maintained by netlify staff. This is a community led project and if you are looking to support this project, please get in touch via an issue. \n Netlify\'s Statement \n \n [Netlify] doesn\'t currently have the staff to process such contributions. \n \n Initial configuration \n Click in the Deploy to Netlify button above to create your own site directly and push this repository to your own account.\nBefore creating the site, Netlify will ask you to fill required environment variables listed here: \n \n STATUSKIT_PAGE_TITLE  - Title to show in the browser for your status site. \n STATUSKIT_COMPANY_LOGO  - URL to your company\'s logo. \n STATUSKIT_SUPPORT_CONTACT_LINK  - URL to a support page for your users to talk with you. \n STATUSKIT_RESOURCES_LINK  - URL to documentation for your users. \n \n Extra configuration \n After the site is created, you can modify the code as much as you want and push it to your GitHub repository. Netlify will pick up changes from there. \n Reporting systems \n You can add systems you want to report about to your Status page. For instance, you might want to tell your users about a status change in your CDN infrastructure but not in your API. \n Go to  site/config.toml  and change the global  systems  variables. Once that\'s done, you\'ll be able to change the status of each one of those systems individually when you open or modify an incident. \n Full customization \n This template is based in  Netlify\'s Victor-Hugo  boilerplate.\nTo work on it you\'ll need NPM installed. To download dependencies type  npm run dependencies , that will check if you have Hugo installed and will download it for you if you don\'t. It will also run  npm install  for the first time to download extra dependencies. After that, you can run  npm install  every time you want to install packages. \n Managing incidents \n Incidents are plain markdown files inside the  site/content/incidents  directory. \n Creating new incidents \n Adding incidents to your status page is as simple as adding a new document to the incidents collection.\nCreate a new incident using npm: \n npm run new-incident \n You\'ll be asked a series of questions about the incident, then Hugo will generate a new file pre-filled with your responses. \n After explaining the current situation in the incident, you can just push the file to GitHub. Netlify will deploy the indicent announcement for you in a matter of seconds. \n Resolving incidents \n Everything will be operational again when all incidents are marked with  resolved = true  in the incident frontMatter: \n toml\n+++\n...\naffectedsystems = ["API"]\nresolved = true\n+++ \n Tracking activity \n When there is an update in your incident you can track activity by inserting a timestamp with the update. For example: \n md\n**Update**: We\'ve identified the issue. {{< track "2016-11-22T14:34:00.000Z" >}} \n Development \n Netlify StatusKit uses NPM to manage dependencies. It also bundles a version of Hugo to work out of the box. \n \n Use  npm install  to download dependencies. \n Use  npm start  to start the development server. \n', 'Introduction \n Netlify StatusKit is a template to deploy your own Status pages on Netlify. \n \n Netlify StatusKit is released under the  MIT License .\nPlease make sure you understand its  implications and guarantees . \n \n Project Status \n This project is no longer being maintained by netlify staff. This is a community led project and if you are looking to support this project, please get in touch via an issue. \n Netlify\'s Statement \n \n [Netlify] doesn\'t currently have the staff to process such contributions. \n \n Initial configuration \n Click in the Deploy to Netlify button above to create your own site directly and push this repository to your own account.\nBefore creating the site, Netlify will ask you to fill required environment variables listed here: \n \n STATUSKIT_PAGE_TITLE  - Title to show in the browser for your status site. \n STATUSKIT_COMPANY_LOGO  - URL to your company\'s logo. \n STATUSKIT_SUPPORT_CONTACT_LINK  - URL to a support page for your users to talk with you. \n STATUSKIT_RESOURCES_LINK  - URL to documentation for your users. \n \n Extra configuration \n After the site is created, you can modify the code as much as you want and push it to your GitHub repository. Netlify will pick up changes from there. \n Reporting systems \n You can add systems you want to report about to your Status page. For instance, you might want to tell your users about a status change in your CDN infrastructure but not in your API. \n Go to  site/config.toml  and change the global  systems  variables. Once that\'s done, you\'ll be able to change the status of each one of those systems individually when you open or modify an incident. \n Full customization \n This template is based in  Netlify\'s Victor-Hugo  boilerplate.\nTo work on it you\'ll need NPM installed. To download dependencies type  npm run dependencies , that will check if you have Hugo installed and will download it for you if you don\'t. It will also run  npm install  for the first time to download extra dependencies. After that, you can run  npm install  every time you want to install packages. \n Managing incidents \n Incidents are plain markdown files inside the  site/content/incidents  directory. \n Creating new incidents \n Adding incidents to your status page is as simple as adding a new document to the incidents collection.\nCreate a new incident using npm: \n npm run new-incident \n You\'ll be asked a series of questions about the incident, then Hugo will generate a new file pre-filled with your responses. \n After explaining the current situation in the incident, you can just push the file to GitHub. Netlify will deploy the indicent announcement for you in a matter of seconds. \n Resolving incidents \n Everything will be operational again when all incidents are marked with  resolved = true  in the incident frontMatter: \n toml\n+++\n...\naffectedsystems = ["API"]\nresolved = true\n+++ \n Tracking activity \n When there is an update in your incident you can track activity by inserting a timestamp with the update. For example: \n md\n**Update**: We\'ve identified the issue. {{< track "2016-11-22T14:34:00.000Z" >}} \n Development \n Netlify StatusKit uses NPM to manage dependencies. It also bundles a version of Hugo to work out of the box. \n \n Use  npm install  to download dependencies. \n Use  npm start  to start the development server. \n', '\n Federalist + U.S. Web Design System + Gatsby \n This site is developed using the  U.S. Web Design System v 2.0  and is focused on providing developers a starter kit and reference implementation for Federalist websites. \n This code uses the  Gatsby  framework and built with Javascript and  React . If you prefer to use Ruby, check out  federalist-uswds-jekyll , which uses  Jekyll  site engine. \n This project assumes that you are comfortable editing source code. If you prefer to launch a website without editing any source code, checkout  uswds-jekyll , which allows you to change the layout and content with configuration files. \n This project strives to be compliant with requirements set by  21st Century IDEA Act . The standards require that a website or digital service: \n \n is accessible to individuals with disabilities; \n has a consistent appearance; \n does not duplicate any legacy websites (the legislation also requires agencies to ensure that legacy websites are regularly reviewed, removed, and consolidated); \n has a search function; \n uses an industry standard secure connection; \n “is designed around user needs with data-driven analysis influencing management and development decisions, using qualitative and quantitative data to determine user goals, needs, and behaviors, and continually test the website, web-based form, web-based application, or digital service to ensure that user needs are addressed;” \n allows for user customization; and \n is mobile-friendly. \n \n 🖐Previous Versions 🖐 \n If you\'re looking for the original starter that included a more integrated approach to using USWDS with React, it is still available  here , but will not be maintained. \n Features \n ✅  USWDS v2  javascript, styles, images, and fonts are included by default. Styles can be customized with either SASS or CSS in  src/styles/index.scss . \n ✅ Publish blog posts using Markdown. Any markdown files in the  src/blog-posts  directory will be turned into pages at  /blog/<path>  in the application using the  src/templates/blog-post.js  template, where  <path>  is specified in the frontmatter of the file. An index of all posts will be displayed at  /blog  using the  src/templates/blog.js  template. \n ✅ Publish documentation pages using Markdown. Any markdown files in the  src/documentation-pages  directory will be turned into pages at  /<path>  in the application using the  src/templates/documentation-page.js  template, where  <path>  is specified in the frontmatter of the file. Side navigation for documentation pages can be controlled by setting  sidenav: true  or  sidenav: false  to your page front matter. \n ✅ Publish custom pages using React. Any javascript files in the  src/pages  directory will be turned into pages at  /<filename>.html , where  <filename>  is the actualy name of the file. \n ✅ Customize SEO information for each page by adding the  src/components/seo.js  component to any page or template and providing the desired information. Ex.  Home page . \n ✅  Search.gov  integration - Once you have registered and configured Search.gov for your site by following  these instructions , add your "affiliate" and "access key" to  gatsby-config.js . Ex. \n ```\nsearchgov: { \n // You should not change this.\n  endpoint: \'https://search.usa.gov\', \n // replace this with your search.gov account\n  affiliate: \'federalist-uswds-example\', \n // replace with your access key\n  access_key: \'xX1gtb2RcnLbIYkHAcB6IaTRr4ZfN-p16ofcyUebeko=\', \n // this renders the results within the page instead of sending to user to search.gov\n  inline: true,\n}\n``` \n The logic for using Search.gov can be found in the  src/components/search-form.js  component and supports displaying the results inline or sending the user to Search.gov the view the results. This setting defaults to "inline" but can be changed by setting  searchgov: { inline: false }  in  gatsby-config.js . \n ✅  Digital Analytics Program (DAP)  integration - Once you have registered your site with DAP add your "agency" and optionally,  subagency  to  gatsby-config.js  and uncomment the appropriate lines. Ex. \n ```\ndap: {\n    // agency: \'your-agency\', \n // Optional\n// subagency: \'your-subagency\',\n \n },\n``` \n ✅  Google Analytics  integration - If you have a Google Analytics account to use, add your "ua" to  gatsby-config.js  and uncomment the appropriate lines. Ex. \n ga: {\n    // ua: \'your-ua\',\n}, \n ✅ Out-of-the-box performant caching strategy following  Gatsby recommended practices  via  federalist.json . See  Federalist Documentation  for more information on  federalist.json . \n Getting Started \n Easy mode \n From Federalist \n This will create a copy of this repo in a Github repository of your choice and add it to your Federalist dashboard. \n \n From  Federalist  click the "+ Add Site" button. \n Click the "Use this template" button for the appropriate template \n Follow the instructions \n \n From Github \n This will create a copy of this repo in a Github repository of your choice but you will need to add it your  Federalist dashbord . \n \n Click the "Use this template" button above or  here . \n Follow the instructions \n Return to  Federalist  and add the repository. \n \n Hard mode \n With  npx  (requires node) \n $ npx degit https://github.com/18F/federalist-uswds-gatsby#main <destination-folder>\n$ cd <destination-folder>\n \n Push to your Github repository \n \n Create a new Github repository . \n Follow the instructions form Github or\n $ git init\n    $ git symbolic-ref HEAD refs/heads/main\n    $ git add . && git commit -m \'Initial commit\'\n    $ git remote add origin git@github.com:<your-org>/<your-repo>.git\n    (Make sure to replace `<your-org>` and `<your-repo>` above with the correct values)\n    $ git push -u origin main \n \n Installation for development \n $ git clone https://github.com/18F/federalist-uswds-gatsby\n$ cd federalist-uswds-gatsby\n \n Running the application \n With locally installed  node \n $ npm install\n$ npm run develop\n \n To build but not serve the site, run  npm run build . \n With Docker \n $ docker-compose run node npm install\n$ docker-compose up\n \n To build but not serve the site, run:\n docker-compose run node npm run build \n. \n Note that when built by Federalist,  npm run federalist  is used instead of\n npm run build . \n Open your web browser to  localhost:8000  to view your\nsite. \n Note: You\'ll also see a second link:  http://localhost:8000/___graphql . This is a tool you can use to experiment with querying your data. Learn more about using this tool in the  Gatsby tutorial . \n Note that when built by Federalist,  npm run federalist  is used instead of the\n build  script. \n Technologies you should be familiarize yourself with \n \n Gatsby  - The framework that does all the magic \n React  - The view library \n GraphQL  - The data layer used by Gatsby \n node  - Javascript runtime \n npm  - Javascript dependency manager \n U.S. Web Design System v 2.0 \n \n 🎓 Learning Gatsby \n Looking for more guidance? Full documentation for Gatsby lives  on the website . Here are some places to start: \n \n \n For most developers, we recommend starting with our  in-depth tutorial for creating a site with Gatsby .  It starts with zero assumptions about your level of ability and walks through every step of the process. \n \n \n To dive straight into code samples, head  to our documentation .  In particular, check out the  Guides ,  API Reference , and  Advanced Tutorials  sections in the sidebar. \n \n \n Things to Note \n \n The Federalist script runs the Gatsby build with the  --prefix-paths  flag. This is necessary to make sure all internal internal links point to the correct path for preview deployments. \n Always use the  Link  component provided by Gatsby for internal links, see previous note. \n Importing USWDS images can be done straight from their local location in  node_modules . See  Banner.js  for an example. \n This is built from the default  Gatsby default starter , you can view the documentation there to see more of what is included. \n \n Contributing \n See  CONTRIBUTING  for additional information. \n Public domain \n This project is in the worldwide  public domain . As stated in  CONTRIBUTING : \n \n This project is in the public domain within the United States, and copyright\nand related rights in the work worldwide are waived through the  CC0 1.0\nUniversal public domain dedication . \n All contributions to this project will be released under the CC0 dedication.\nBy submitting a pull request, you are agreeing to comply with this waiver of\ncopyright interest. \n']
coryf,['homebrew \n Personal homebrew formula', 'Sudoku Curses']
jrodrigomg,['SERVIDOR NODEJS PROYECTO 1, SISTEMAS OPERATIVOS 1', 'enGeoy \n Aplicación Front End ', 'ejemplo-angular \n Ejemplo básico de angular js para exposición seminario', 'restify-example', 'Protostar Challenges \n Mis soluciones para los retos de Protostar.', 'My first implementation of tensorflow js to predict the contrast of a background color. \n Using p5.js and tensorflow.js to get the more easily to read a text with the random background color. \n DEMO: \n https://calm-basin-67709.herokuapp.com/contrast \n LICENSE: MIT', 'puzzleLearning \n Implementation of neural network to play a puzzle game \n Using tensorflow and p5 to select the squares in ascending or descending order. \n LICENSE: MIT \n DEMO \n https://calm-basin-67709.herokuapp.com/puzzle', 'trex-volution \n Using NeuroEvolution.js to learning how to play t-rex game \n References \n This was made thanks to the following projects:\n*  FlappyLearning  Build the neural network(NeuroEvolution.js)  https://github.com/xviniette/FlappyLearning\n*  t-rex-runner  The game of T-rex https://github.com/wayou/t-rex-runner\n*  T-Rex :To stablish some ideas to interact with the program https://github.com/amaneureka/T-Rex  \n Demo \n \n https://calm-basin-67709.herokuapp.com/trex \n \n Preview \n', 'My First Alexa Skill \n This is my first implementation of alexa skill. \n I made this example thanks to this reference:\n* https://www.youtube.com/watch?v=1pvR4aqwGhg&index=3&list=LLuicraBjEqukSJItEoTJabQ&t=0s\n* https://www.youtube.com/watch?v=_SUeL-HSuIo&index=4&list=LLuicraBjEqukSJItEoTJabQ&t=0s\n* https://www.youtube.com/watch?v=cFzAIhsldbs&index=2&list=LLuicraBjEqukSJItEoTJabQ&t=0s', 'first-linear-regression \n LInear regression model with Tensorflow.js \n The dataset is from:\nhttps://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/mlr/frames/frame.html \n This code is thanks to this references: \n \n https://stackoverflow.com/a/43899449 \n https://js.tensorflow.org/tutorials/fit-curve.html \n https://medium.com/all-of-us-are-belong-to-machines/gentlest-intro-to-tensorflow-part-3-matrices-multi-feature-linear-regression-30a81ebaaa6c \n https://www.youtube.com/watch?v=NZR-N_dhK2M \n', 'Analizame \n Analize sentiment, keywords and relevance from user tweets  \n REQUIREMENTS \n Submit this credentials in Config.json: \n \n \n Python 3 \n \n \n IBM Watson Service (NLP)\nYou need credentials from NLP of IBM\n    https://www.ibm.com/watson/services/natural-language-understanding/ \n \n \n Twitter CONSUMER and ACCESS (Keys and Secrets)\nhttps://apps.twitter.com/ \n \n \n python-twitter library:\nhttps://github.com/bear/python-twitter \n \n', 'black-ai \n Playing Black Jack with Reinforcement Learning \n Requisites: \n \n Python3 \n Virtualenv \n \n Instalation \n Cloning:\n bash\n$ git clone https://github.com/jrodrigomg/black-ai\n$ cd black-ai/ai \nCreating a virtual environment:\n bash\n$ virtualenv -p python3.5 env \n Load environment, this is an example for fish shell:\n bash\n$ source env/bin/activate.fish \nInstall requirments:\n bash\n$ pip install -r requirements.txt \n Run the application server \n Run the main file:\n bash\n$ python main.py \n Running the game \n In the main directory go to the game carpet:\n bash\n$ cd game \n Run a simple http server with python(Not necesary the enviroment here)\n bash\n$ python -m SimpleHTTPServer 8080 \n Try \n \n Once running both application we need to configure wherever the server ask \n go http://localhost:8080 in the browser \n If just appear "Epoch 1" then refresh the browser \n \n Credits \n \n Thanks to adventuresinML for the incredible post of RL  (https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/) \n Thanks to Kusnierewicz for the game of blackjack in javascript (https://github.com/Kusnierewicz/Blackjack-game-in-JS) \n For the structure of the enviroment i used some part of Vincent Dutordoir\'s repo (https://github.com/vdutor/TF-rex) \n', 'simple-max-ga \n A simple function maximization with genetic algorithm \n Credits \n Special thanks to CodingTrain for the awesome playlist of Genetic Algorithms.\nhttps://www.youtube.com/watch?v=9zfeTw-uFCw', 'ACRONYM API \n Requerimientos \n Debes tener actualmente instalado:\n* nodejs\n* mongodb\n* npm\n* pm2 \n Instalación \n \n Instala las dependencias locales del proyecto de node: \n \n npm install \n2. Debes configurar las variables de ecosystem.config.js en un ambiente como lo siguiente: \n ```\n    //General\n    NODE_ENV: \'development\',\n    "TZ": "America/Guatemala", \n PORT:               3002, //Puerto del node\n//Mongo\nURLMONGO:           \'mongodb://127.0.0.1:27017/\',\nUSER_MONGO:         \'USERMONGO\',\nPASS_MONGO:         \'PASSWORDMONGO\',\nBD_MONGO:           \'DBMONGO\',\n\n//JWT\nsecret_jwt_key:     \'MISECRETTOKEN\',\nexpiredTime:        50000\n \n ``` \n \n \n Crear una base de datos, usuario donde éste usuario tenga los permisos necesarios \n \n \n Iniciar el servidor con: \n \n \n ```\n    pm2 start \n ```\n5. El servidor está arriba! Debería de desplegar un mensaje en http://localhost:3002 \n Nota \n Para probar los métodos PUT y DELETE se creó un endpoint para crear los JSON Web Token. \nEn la carpeta de testing se tiene automatizado el envío del header Authorization para \nno tener que hacerlo manual, sólo se tendría que ver la url correcta. \n El esquema de la base de datos es  name  donde éste corresponde al acrónimo y  description \ncorresponde a la definición. Para el método POST podría testearse con  curl \ncon el siguiente comando: \n ```\ncurl -X POST -H "Content-Type:application/json" -d \'{"name":"TOMATE", "description":"Ve a traer tomates"}\' "http://localhost:3002/acronym" \n ```']
vascofernandes,['CustomersVRPF \n The project consists of two main parts:\n- backend based on ASP.NET WebAPI\n- frontend based on AngularJS in the form of a Single Page Application. \n The WebAPI is defined in Controllers/CustomersController.cs\nA general description of the API is as follows: \n GET    api/customers      - Get all customers\nGET    api/customers/{id} - Get one customer by ID\nPOST   api/customers      - Insert a new customer\nPUT    api/customers/{id} - Update a customer\nDELETE api/customers/{id} - delete a customer. \n A customer is defined in Models/Customers.cs.\nThe operation are handled by the class Datastores/Datastore.cs which implements Interfaces/IUnitOfWork.cs.\nIn Datastore.cs the methods Customers returns an instance of Repositories/CustomerRepository.cs which implements Interfaces/ICustomerRepository.cs.\nCustomerRepository.cs handles all the data related tasks, such as insertion, update, and deletion of customers.\nIn this case, the implementation is simple a non-persistent List with some sample data.\nThis can be easily replaced with database functionality.\nWith this structure one can easily extend to API to handle for example products or invoices for each customer. \n When relevant I used some basic data mapping from domain objects, such as Customer.cs, to data transfer objects, to send over the network.\nFor a more robust solution one could make use of libraries such as Json.NET (http://james.newtonking.com/json)\nand Automapper (http://automapper.org/), both available on NuGet. \n On the front end the applications uses angularjs ui-router to handle the three views of the applications:\nCustomer List/Add New, Detailed Customer, and Edit Customer. \n The frontend communicates with the backend using and service based on AngularJS Resources defined in app/services/customerResource.js.\nThere controllers, one for each view, are defined in app/customers and make use of the customerResoure to communicate with the API.\nThe front end is organized on a feature basis.\nIf one would need to implement product management, a folder app/products would contain controllers and views related with product management. \n The applications communicates correctly with the backend, but since there is no persistence,\nthe customers information does not change between requests.', 'angular-powers', 'angularjs-reduced \n Install node.js modules:\n  ```bash \n npm install\n \n ``` \n Install DefinitelyTyped Typescript deffinition files:\n  ```bash \n npm run tsd install\n \n ``` \n Build\n  ```bash \n npm run build\n \n ```', '"# android-natve-to-webview-example" ', '"# Reaction-Diffusion"  \n Quick and dirty way to put together a C++ and OpenGL reaction diffusion experiment. \n http://scitation.aip.org/content/aip/journal/jcp/116/13/10.1063/1.1453964 \n http://science.sciencemag.org/content/293/5535/1635 \n', '"# SoftEngine" ', 'angularjs-typescript-components \n http://jsbin.com/koyulexiro/1/edit?html,js,console,output', 'Publish and run the pubished app']
seba-1511,['Arduino-Chrono \n An Arduino-based chronometer for Tooski \n Warning: This project is still in Beta, so some things may or may not work. If you find a problem, bug or have an idea don\'t hesitate to send me a message. It always makes people happy, right ? \n Presentation \n Here are all the files I used to build my own chronometer. It was initialy an idea for the website Tooski.ch, so made for skiers, and that\'s why the text on the screen is in French. However it is really easy to modify it and to understand the code, as the comments are in English. \n It is based on Arduino, uses Xbee for wireless communication (~300m) and the detectors are using IR beams to detect when something goes through them. By the way I take this occasion to thank Ken Shirriff for his library that helped a lot ! If you go throught the folders, you will find the part list, the schematics and of course the commented Arduino code. Not to mention that there is also the instructions on how to use it once you are done building it. \n Once again critics, comments and suggestions are really welcome, and keep in mind I may do errors, so point them out to me. Also if you modify or improve my code, let me know because I\'ll be happy to see what you\'ve done. \n You can see a video of the prototype here: http://www.youtube.com/watch?v=61YS39FGn74 \n Futur implementations: \n \n \n Storing the times into an SD card and make a table out of them. \n \n \n Allow multiple skiers at the same time. \n \n \n Implementing a camera that takes a photo when the chrono starts. \n \n \n Having a camera that films you only from when the chrono starts to when it ends. \n \n \n License \n Copyright (c) 2012 Sébastien Arnold \n Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: \n The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. \n THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.', 'Booter \n The Booter Project: A predefined structure to kick your projects. This includes:\n* CodeIgniter 2.1.3 (With some default configuration. Ex: Autoload, Cookies, etc...)\n* jQuery 1.9 and jQuery 2.0 to best match your targeted visitors\n* jQuery UI 1.10.3 not themed\n* jQuery Mobile 1.3.1\n* Bootstrap 2.3.1 JS and CSS\n* Less 1.3.3\n* HandleBarsJS 1.0.0\n* Glyphish Pro 3.0 Icons\n* Custom Scripts \n Notes on Custom Scripts \n CodeIgniter\'s Layout Library \n As we don\'t want to always load the same Headers, Footers, etc... of the page, we implemented a library to do this for us. The principle is that you add views, that will be stored in the class. Then by simply calling the printPage function with\n  php\n $this->layout->printPage();  \n the whole page will be printed. Note that you can also add a JS, CSS and set the title by simply calling the librarie\'s methods. See an example in the welcome controler. \n Also, you can set your own default layouts by creating them in the views/layout folder. By default, there are only 4 of them:\n *  default  Just an empty HTML5 layout\n *  basic  Only main JavScripts and CSS are imported\n *  quicksite  The starting point for a project, with some elements of Bootstrap\n *  mobile  A simple web app with header and footer \n IMPORTANT: The Layout Library is imported by default, so remove it in autoload if you don\'t use it !   \n CodeIgniter CRUD Model \n The file application/core/MY_CRUD.php implements a basic CRUD implementation. (With count option) The easiest way to understand it is to read the file. We highly recommend to use it as a core function for the different models you\'ll build, by doing so:\n ```php\n class User_model extends MY_CRUD\n{\n    protected $table = \'users\';\n} \n ``` \n Booter JS \n Booter has its own JavaScript Class, found in the file assets/js/booter.utils.js. This file has some functions that I found usefull, and that can be used in about any projects. To use it, just call \n javascript\nbooter.theNameOfThePublicMethod(); \n License \n Copyright (C) 2013 Sébastien Arnold \n Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: \n The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. \n THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.', 'YoBooter \n A mix of Booter and Yeoman, for even faster web app developpment', "PersHo \n A personal homepage for everybody \n Requirements: \n \n Installable with a simple git clone \n Links collection like ethz1.goat.ch - Public \n Personal Link collection to my other services - Public \n Quick Notes, twitter-like - Hidden but shareable \n Todo's - Private \n Uses Django 1.5 for backend \n Uses AngularJS for front-end \n Local SQLite to store everything. \n \n Structure of Database: \n \n Links : Name | Link | Date | Id \n Personal : Name | Link | Date | Id \n Notes : Title | Content | Date | Id \n Todo's : State | Content | Date | Id \n", 'mosgapPhonegap', 'mosgapJSensors', 'mobileNotes \n A mobile app for taking notes', 'meritocracy \n A Meritocracy-based nodeGame', 'tooski_mobile \n The mobile application for Tooski', 'js-speech-api-test \n Test the HTML5 Speech recognition and TTS API.', "planktonChallenge \n The Kaggle challenge for plankton recognition \n To make it work \n Please download the data from the kaggle website and unzip the folders into the 'data' folder on the root of the project.", 'fluster_backend \n \nThe backend of the application for Fluster \n Deployement Link: http://uwsgi-docs.readthedocs.org/en/latest/tutorials/Django_and_nginx.html', 'fluster_frontend \n \nThe front end of the website for Fluster', 'Quantum \n This repository contains my code and experiments for my research on quantum computing as a student at ISI, under the advisement of Dr. Itay Hen. It contains the following: \n simulatedAnnealing/ \n Several implementations of the simulated annealing, some of them with a basic grid search procedure. A version is based on the schedule found by Dr. Albash. The goal was to solve planted instances as fast as possible.  \n quantumAnnealing/ \n The implementation of a quantum annealer, to solve the same kind of problems as with simulated annealing.', 'ReadMe \n Some code for my GSoC 15 demo project. Also, allows me to test the Keras library.', 'Colloquium 310 \n This repository will contain all summaries produced for the second part of lecture  Software Engineering , CSCI 310. \n List of Lectures \n The list of covered by those summaries, as well as who worked on them. \n Lecture Title               | Lecture n° | Authors                             | Link\n----------------------------|------------|-------------------------------------|------\nSoftware Process Models     | 12         | Jenny (S)                           |  link \nAgile Development           | 13         | Matthew (W)                         |  link \nCleanroom Dev Process       | 14         | ?                          |  link \nBlack-box Testing           | 15         | Kelsey (T)                          |  link   \nWhite-box Testing           | 16         | Anthony (Th)                        |  link \nRegression Testing          | 17         | Pavel(F)                              |  link \nFault Localization          | 18         | Justine (Th)                         |  link \nArchitectural Styles        | 19         | Seb (S)                             |  link \nDesign Patterns             | 20         | Seb (T)                             |  link \nSecure Software Engineering | 21         | Daler (F)                               |  link \n Presentation Guidelines \n In the span of one hour, we should be able to be able to summarize 2 weeks of lectures. This means about 30 minutes per lecture and thus we should focus on important or advanced concepts as student will be familiar with the basic ones. The idea is that people presenting are experts in their respective lecture and share their knowledge. To have a good idea of what the presentation should be about, check the last slide of every lecture. \n Suggested structure: \n \n \n Important Concept 1: \n \n What it is about ? \n How it fits in the  \n Why it is important. \n How does it differ from other concepts we’ve seen ? \n \n \n \n Repeat for every concept \n \n \n The way I worked through the Design Pattern lecture was to first decide if a slide was important or not, and if it was then imagine what were the possible questions about it. \n Other Resources \n A summary built by other team members of the class:  Available here', 'Specialists \n Report on an experiment with the generalist-specialist framework. \n This repository is far from being clean, and the experiments were carried out with an older version of neon. You can read the obtained results in  paper/paper.pdf .', "conjugate-gradient-newton \n Comparison of Gradient Descent, Newton's Method, and Conjugate Directions Algorithm", 'CSCI-544-Final-Project \n Final Project for CSCI-544 Spring 2016 \n Instructions \n To run the whole experiment run: (approx. 20min on a recent NVIDIA GPU) \n make all \n It should download the dataset, unzip, train and serialize the core components, and finally train and benchmark the augmented, multimodal model. \n A typical output should be:  \n ``` \n \n python experiment/data.py\n./SaudiNewsNet/SaudiNewsNet-master/dataset/2015-08-09.zip\n./SaudiNewsNet/SaudiNewsNet-master/dataset/2015-08-10.zip\n./SaudiNewsNet/SaudiNewsNet-master/dataset/2015-07-25.zip\n./SaudiNewsNet/SaudiNewsNet-master/dataset/2015-07-24.zip\n./SaudiNewsNet/SaudiNewsNet-master/dataset/2015-08-08.zip\n./SaudiNewsNet/SaudiNewsNet-master/dataset/2015-08-01.zip\n./SaudiNewsNet/SaudiNewsNet-master/dataset/2015-07-22.zip\n./SaudiNewsNet/SaudiNewsNet-master/dataset/2015-08-04.zip\n./SaudiNewsNet/SaudiNewsNet-master/dataset/2015-08-11.zip\n./SaudiNewsNet/SaudiNewsNet-master/dataset/2015-08-06.zip\n./SaudiNewsNet/SaudiNewsNet-master/dataset/2015-07-27.zip\n./SaudiNewsNet/SaudiNewsNet-master/dataset/2015-07-26.zip\n./SaudiNewsNet/SaudiNewsNet-master/dataset/2015-07-21.zip\n./SaudiNewsNet/SaudiNewsNet-master/dataset/2015-07-23.zip\n./SaudiNewsNet/SaudiNewsNet-master/dataset/2015-08-02.zip\n./SaudiNewsNet/SaudiNewsNet-master/dataset/2015-08-07.zip\n./SaudiNewsNet/SaudiNewsNet-master/dataset/2015-08-03.zip\n./SaudiNewsNet/SaudiNewsNet-master/dataset/2015-07-31.zip\nvocabulary size -  343172 \n of samples -  31030 \n of classes 14 \n class distribution -  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] [2080 2947 2964 3690 4852 2279 2090 3065  133 2846 1411 2252   52  369]\nsentence length -  1527 [   0    1    2 ..., 4480 5160 6200] [354  42 104 ...,   1   1   1] \n of train - 24740, # of valid - 6290 \n vocabulary from dataset is saved into labeledTrainData.tsv.vocab\npython experiment/train_core.py -e 2\n2016-04-26 23:11:08,573 - neon.backends - WARNING - deterministic_update and deterministic args are deprecated in favor of specifying random seed\nEpoch 0   [Train |████████████████████|  194/194  batches, 2.17 cost, 65.13s]\nEpoch 1   [Train |████████████████████|  193/193  batches, 1.16 cost, 63.86s]\nTrain accuracy:  [ 0.90553761]\nTest accuracy:  [ 0.49904612]\npython experiment/train_augmented.py -e 7 -i 1\n2016-04-26 23:14:20,195 - neon.backends - WARNING - deterministic_update and deterministic args are deprecated in favor of specifying random seed\nEpoch 0   [Train |████████████████████|  194/194  batches, 0.71 cost, 124.93s]\nEpoch 1   [Train |████████████████████|  193/193  batches, 0.45 cost, 124.21s]\nEpoch 2   [Train |████████████████████|  193/193  batches, 0.34 cost, 124.83s]\nEpoch 3   [Train |████████████████████|  194/194  batches, 0.30 cost, 127.64s]\nEpoch 4   [Train |████████████████████|  193/193  batches, 0.25 cost, 126.71s]\nEpoch 5   [Train |████████████████████|  193/193  batches, 0.23 cost, 126.60s]\nEpoch 6   [Train |████████████████████|  193/193  batches, 0.24 cost, 126.76s]\nTrain accuracy:  [ 0.96604687]\nTest accuracy:  [ 0.49252781]\n``` \n \n License \n The content of this repository is exclusive to the purpose of CSCI 544 class at\nUSC. Copy or redistribution of the code is strictly forbidden without prior\nauthorization of all the authors.', 'dtrpo', "\n \n \n \n randopt is a Python package for machine learning experiment management, hyper-parameter optimization, and results visualization. Some of its features include: \n \n result logging and management, \n human-readable format, \n support for parallelism / distributed / asynchronous experiments, \n command-line and programmatic API, \n shareable, flexible Web visualization, \n automatic hyper-parameter search, and \n pure Python - no dependencies ! \n \n Installation \n shell\npip install randopt \n Usage \n ```python\nimport randopt as ro \n def loss(x):\n    return x**2 \n e = ro.Experiment('myexp', {\n        'alpha': ro.Gaussian(mean=0.0, std=1.0, dtype='float'),\n    }) \n Sampling parameters \n for i in xrange(100):\n    e.sample('alpha')\n    res = loss(e.alpha)\n    print('Result: ', res)\n    e.add_result(res) \n Manually setting parameters \n e.alpha = 0.00001\nres = loss(e.alpha)\ne.add_result(res) \n Search over all experiments results, including ones from previous runs \n opt = e.minimum()\nprint('Best result: ', opt.result, ' with params: ', opt.params)\n``` \n Results Visualization \n Once you obtained some results, run  roviz.py path/to/experiment/folder  to visualize them in your web browser. \n For more info on visualization and  roviz.py , refer to the  Visualizing Results  tutorial. \n Hyper-Parameter Optimization \n To generate results and search for good hyper-parameters you can either user  ropt.py  or write your own optimizaiton script using the  Evolutionary  and  GridSearch  classes. \n For more info on hyper-parameter optimization, refer to the  Optimizing Hyperparams  tutorial. \n Documentation \n For more examples, tutorials, and documentation refer to the  wiki . \n Contributing \n To contribute to Randopt, it is recommended to follow the  contribution guidelines . \n Acknowledgements \n Randopt is maintained by  Séb Arnold , with numerous contributions from the following persons. \n \n Noel Trivedi \n Cyrus Jia \n Daler Asrorov \n \n License \n Randopt is released under the Apache 2 License. For more information, refer to the  LICENSE file . \n I would love to hear how your use Randopt. Feel free to  drop me a line  !", 'dist_blog \n Distributed Optimization Blog Post', 'cdn \n Personal CDN', "Shapechanger \n Formerly knows as mj_transfer. \n Additional transfer learning environments for OpenAI Gym. \n Install Instructions \n \n Clone this folder. \n Simply execute  python setup.py develop  in this folder. \n You can now  import mj_transfer  to register them, and  gym.make('BigAnt-v1')  if you want to use the BigAnt env. \n \n MuJoCo Tutorial \n You can find a short tutorial for that explains the basics of MuJoCo there:  seba-1511.github.io/mj_transfer \n Available Environments \n AmputedAnt-v1 \n Like Ant-v1, but with one-quarter leg missing.  \n BigAnt-v1 \n Like Ant-v1, but with long legs.  \n ExtendedAnt-v1 \n Like Ant-v1, but with an additional joint on each limb. \n SmallInvertedPendulum-v1 \n Like InvertedPendulum, but with an arm half the size.  \n BigInvertedPendulum-v1 \n Like InvertedPendulum, but with an arm twice the size.  \n Finger-v1 \n A realistic tendon-driven finger. \n         Environments                                  |          &nbsp;\n    -------------------                               |     ----------------\n \n   |   \n                    |   \n               |  ", 'mpi4pycuda \n A wrapper to enable GPU-GPU communication with pyCUDA and mpi4py. \n Install \n Example', 'nnexp \n A template for neural network experiments \n Goals \n \n Define your PyTorch Dataset, your NN, and your randopt variables. Call  learn(...) , and off you go. \n Automatic splitting (if desired) into train/valid/test data. \n Choose to use CUDA (automatically distributed) or not. \n Use sensible - but overrideable - defaults. (for optimizer, data pre-processing, hyper-params, etc...) \n Provide Mnist example. \n train function should only do 1 epoch. \n Sensible command-line arguments support. \n Easy network definition. \n', 'dist_rl \n Distributed Implementation of RL Algorithms', "lstms.pth \n Implementation of LSTM variants, in PyTorch.  \n For now, they only support a sequence size of 1, and meant for RL use-cases. \nBesides that, they are a stripped-down version of PyTorch's RNN layers. \n(no bidirectional, no num_layers, no batch_first) \n Base Modules: \n \n SlowLSTM: a (mostly useless) pedagogic example. \n LayerNorm: Layer Normalization as in  Ba & al. :  Layer Normalization . \n \n Dropout Modules: \n \n LSTM: the original. \n GalLSTM: using dropout as in  Gal & Ghahramami :  A Theoretically Grounded Application of Dropout in RNNs . \n MoonLSTM: using dropout as in  Moon & al :  RNNDrop: A Novel Dropout for RNNs in ASR . \n SemeniutaLSTM: using dropout as in  Semeniuta & al :  Recurrent Dropout without Memory Loss . \n \n Normalization + Dropout Modules: \n \n LayerNormLSTM: Dropout + Layer Normalization. \n LayerNormGalLSTM: Gal Dropout + Layer Normalization. \n LayerNormMoonLSTM: Moon Dropout + Layer Normalization. \n LayerNormSemeniutaLSTM: Semeniuta Dropout + Layer Normalization. \n \n Container Modules: \n \n MultiLayerLSTM: helper class to build multiple layers LSTMs. \n \n Convention:  If applicable, the activations are computed first, and  then  the nodes are droped. (dropout on the output, not the input, just like PyTorch) \n Install \n pip install -e . \n Usage \n You can find a good example of how to use the layers in  test/test_speed.py . \n All Dropout models share the same signature: \n python\nLSTM(self, input_size, hidden_size, bias=True, dropout=0.0, dropout_method='pytorch') \n All Normalization + Dropout models share the same signature: \n python\nLayerNormLSTM(self, input_size, hidden_size, bias=True, dropout=0.0, \n             dropout_method='pytorch', ln_preact=True, learnable=True): \n And all models use the same  out, hidden = model.forward(x, hidden) signature as the official PyTorch LSTM layers. They also all provide a  model.sample_mask()  method, which needs to be called in order to sample a new Dropout mask. (e.g, when processing a new sequence) \n Note:   LayerNorm  is not an LSTM layer, and thus uses  out = model.forward(x) . \n Containers \n This package provides a helper class,  MultiLayerLSTM , which can be use to stack multiple LSTMs together. \n python\nlstm = MultiLayerLSTM(input_size=256, layer_type=LayerNormSemeniutaLSTM,\n                      layer_sizes=(64, 64, 16), dropout=0.7, ln_preact=False)\nhiddens = lstm.create_hiddens(bsz=batch_size)\nx = Variable(th.rand(1, 1, 256))\nfor _ in range(10):\n    out, hiddens = lstm(x, hiddens) \n Note that  hiddens  doesn't match the PyTorch specification. It is the list of  (h_i, c_i)  for each LSTM layer. Instead, the  LSTM  layers in PyTorch return a single tuple of  (h_n, c_n) , where  h_n  and  c_n  have sizes (num_layers * num_directions, batch, hidden_size). \n Capacity Benchmarks \n Warning: This is an artificial memory benchmark, not necessarily representative of each method's capacity. \n Note: nn.LSTM and SlowLSTM do not have dropout in these experiments. \n Info: dropout =  0.9 , SEQ_LEN =  10 , dataset size =  100  layer size =  256 \n model          |         error\n ---------------|--------------\n nn.LSTM        | 3.515\n SlowLSTM       | 4.171\n LSTM           | 4.160\n GalLSTM        | 4.456\n MoonLSTM       | 4.442\n SemeniutaLSTM  | 3.762\n GalLSTM        | 4.456\n MoonLSTM       | 4.442\n SemeniutaLSTM  | 3.762 \n Speed Benchmarks \n Available by running  make speed . \n Warning: Inference timings only, and on a single sequence of length 1000  with  dropout =  0.5 . \n SlowLSTM  Benchmark \n size   | nn.LSTM   |  SlowLSTM  | Speedup\n-------|-----------|------------|--------\n128    | 0.628     | 0.666      | 0.943\n256    | 0.676     | 0.759      | 0.890\n512    | 0.709     | 1.026      | 0.690\n1024    | 2.364     | 2.867      | 0.824\n2048    | 6.161     | 8.261      | 0.746 \n LSTM  Benchmark \n size   | nn.LSTM   |  LSTM  | Speedup\n-------|-----------|--------|--------\n128    | 0.568     | 0.387  | 1.466\n256    | 0.668     | 0.419  | 1.594\n512    | 0.803     | 0.769  | 1.045\n1024    | 2.966     | 2.002  | 1.482\n2048    | 6.291     | 6.393  | 0.984 \n GalLSTM  Benchmark \n size   | nn.LSTM   |  GalLSTM  | Speedup\n-------|-----------|-----------|--------\n128    | 0.557     | 0.488     | 1.142\n256    | 0.683     | 0.446     | 1.530\n512    | 0.966     | 0.548     | 1.763\n1024    | 2.524     | 2.587     | 0.975\n2048    | 6.618     | 6.099     | 1.085 \n MoonLSTM  Benchmark \n size   | nn.LSTM   |  MoonLSTM  | Speedup\n-------|-----------|------------|--------\n128    | 0.667     | 0.445      | 1.499\n256    | 0.818     | 0.535      | 1.530\n512    | 0.908     | 0.695      | 1.306\n1024    | 2.517     | 2.553      | 0.986\n2048    | 6.475     | 6.779      | 0.955 \n SemeniutaLSTM  Benchmark \n size   | nn.LSTM   |  SemeniutaLSTM  | Speedup\n-------|-----------|-----------------|--------\n128    | 0.692     | 0.513           | 1.348\n256    | 0.685     | 0.697           | 0.983\n512    | 0.717     | 0.701           | 1.022\n1024    | 2.639     | 2.751           | 0.959\n2048    | 7.294     | 6.122           | 1.191 \n LayerNormLSTM  Benchmark \n size   | nn.LSTM   |  LayerNormLSTM  | Speedup\n-------|-----------|-----------------|--------\n128    | 0.646     | 1.656           | 0.390\n256    | 0.583     | 1.800           | 0.324\n512    | 0.770     | 1.989           | 0.387\n1024    | 2.623     | 3.844           | 0.682\n2048    | 6.573     | 9.592           | 0.685 \n LayerNormGalLSTM  Benchmark \n size   | nn.LSTM   |  LayerNormGalLSTM  | Speedup\n-------|-----------|--------------------|--------\n128    | 0.566     | 0.486              | 1.163\n256    | 0.592     | 0.350              | 1.693\n512    | 0.920     | 0.606              | 1.517\n1024    | 2.508     | 2.427              | 1.034\n2048    | 7.356     | 10.268              | 0.716 \n LayerNormMoonLSTM  Benchmark \n size   | nn.LSTM   |  LayerNormMoonLSTM  | Speedup\n-------|-----------|---------------------|--------\n128    | 0.507     | 0.389               | 1.305\n256    | 0.685     | 0.511               | 1.342\n512    | 0.762     | 0.685               | 1.111\n1024    | 2.661     | 2.261               | 1.177\n2048    | 8.904     | 9.710               | 0.917 \n LayerNormSemeniutaLSTM  Benchmark \n size   | nn.LSTM   |  LayerNormSemeniutaLSTM  | Speedup\n-------|-----------|--------------------------|--------\n128    | 0.492     | 0.388                    | 1.267\n256    | 0.583     | 0.360                    | 1.616\n512    | 0.760     | 0.578                    | 1.316\n1024    | 2.586     | 2.328                    | 1.111\n2048    | 6.970     | 10.725                    | 0.650", 'cluster_setup.rpi \n Custom setup scripts for the RPi Cluster', 'cervix.kaggle \n Intel Cervix Kaggle Competition  \n Getting Started \n Install pytorch following the instructions on  pytorch.org . \n Then run:  pip install -r requirements.txt  to install the dependencies.', 'dist_tuto.pth \n PyTorch Distributed Tutorial', 'Plotify \n Make beautiful plots, fast. \n Plotify is in active development. It is an intuitive wrapper around Matplotlib, Moviepy, and others that allows to easily create plots and animations. \n Checkout the scripts in  tests  and the results in  outputs . \n Python 3.3+.', "Sobolev Training with Pytorch \n Small scale replication of Sobolev Training for NNs. \n Overview \n You can use the code by importing  SobolevLoss  from  sobolev.py . In order to use it, checkout the example in  main.py . The general guideline for distillation is: \n ```python\nfrom sobolev import SobolevLoss \n teacher = Net()\nstudent = Net()\nloss = SobolevLoss(loss=nn.MSELoss(), weight=1.0, order=2) \n compute the gradients of teacher and student \n sobolev = loss(student.parameters(), teacher.parameters()) \n At this point, the parameters' gradients of student look like: \n s.grad = s.original_grad + s.grad.grad \n where s.grad.grad comes from the Sobolov loss \n ``` \n Remarks:  \n \n Make sure that your teacher is well-trained. \n It works well towards the end of distillation. \n Instead of  student.parameters()  and  teacher.parameters()  you can pass an iterable of parameters whose nth order gradients have been computed. \n Theoretically should work for higher order, but I didn't test it. \n \n Benchmark results \n The results obtained by distilling a LeNet-teacher (converged) into a LeNet-student with the same random architecture. The results are in the form  train / test  at the 100th epoch of training. \n Metric        |  Vanilla    | Sobolev\n--------------|-------------|------------\nDistill. Loss |  1.2 / 1.19 | 0.56 / 0.64\nStudent Loss  |  0.94 / 0.9 | 0.8 / 0.82\nTeacher Loss  |  0.7 / 0.72 | 0.7 / 0.72\nSobolev Loss  |  n / a      | 2e-4 / 4e-4", "Randopt Plugins \n A bunch of plugins for randopt \n To install all: \n ~~~shell\npip install -r requirements.txt\npip install .\n~~~ \n Live Wrapper \n The live wrapper allows to print / plot experimental metrics live. \n Install \n ~~~shell\npip install matploblib terminaltables\n~~~ \n Usage \n ~~~python\nimport randopt as ro\nfrom randopt_plugins.live import Live \n exp = ro.Experiment('live_example', params={\n    'x': ro.Gaussian(),\n    'y': ro.Gaussian()\n})\nlive = Live(exp, metrics=['square', 'norm', 'xminusy', 'time']) \n start = time()\nfor i in range(100):\n    live.sample_all_params()\n    live.update('square', live.x**2)\n    live.update({\n        'norm': abs(exp.y),\n        'xminusy': exp.x - exp.y,\n        'time': time() - start\n    })\n    print(live.table_metrics())\n    live.plot_metrics()\n    sleep(1) \n live.add_result(exp.x - exp.y)\nlive.add_result(exp.x - exp.y, {'useless': [0, 0, 0, 0]})\nlive.add_result(exp.x - exp.y, data={'useless': [0, 0, 0, 0]})\n~~~ \n Screenshots \n Live Plotting \n \n Live Table \n", 'Implicit Gradient Transport in PyTorch \n This repository is the original code for the experiments in:  https://arxiv.org/abs/1906.03532 . \n For more information, please refer to our  paper  or our  website . \n Citation \n You can cite the algorithms implemented in this repository by citing the following paper. \n ~~~\n@misc{arnold2019reducing,\n    title={Reducing the variance in online optimization by transporting\n           past gradients},\n    author={Sebastien M. R. Arnold,\n            Pierre-Antoine Manzagol,\n            Reza Babanezhad,\n            Ioannis Mitliagkas,\n            Nicolas Le Roux},\n    year={2019},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG}\n}\n~~~ \n Installation \n pip install -e . \n Usage \n See  tests/  folder for more examples. \n ```python\nimport torch.optim as optim\nfrom torch_igt import IGTransporter \n opt = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nopt = IGTransporter(model.parameters(), opt) \n Compute a single optimization step \n opt.train()  # Ensures parameters are set to the transported ones\nloss = L(model(X_train), y_train)\nopt.zero_grad()\nloss.backward()\nopt.step() \n Reverts parameters to the true ones \n opt.eval()\nloss = L(model(X_test), y_test)\n``` \n Note \n The ITA family of algorithms (such as  Heavyball-ITA  in the paper) are implemented as  torch_igt.ITA(params, opt) .', "\n \n \n Cherry is a reinforcement learning framework for researchers built on top of PyTorch. \n Unlike other reinforcement learning implementations, cherry doesn't implement a single monolithic  interface to existing algorithms.\nInstead, it provides you with low-level, common tools to write your own algorithms.\nDrawing from the UNIX philosophy, each tool strives to be as independent from the rest of the framework as possible.\nSo if you don't like a specific tool, you don’t need to use it. \n Features \n \n Pythonic and low-level interface  à la  Pytorch. \n Support for tabular (!) and function approximation algorithms. \n Various OpenAI Gym environment wrappers. \n Helper functions for popular algorithms. (e.g. A2C, DDPG, TRPO, PPO, SAC) \n Logging, visualization, and debugging tools. \n Painless and efficient distributed training on CPUs and GPUs. \n Unit, integration, and regression tested, continuously integrated. \n \n To learn more about the tools and philosophy behind cherry, check out our  Getting Started tutorial . \n Example \n The following snippet showcases some of the tools offered by cherry. \n ~~~python\nimport cherry as ch \n Wrap environments \n env = gym.make('CartPole-v0')\nenv = ch.envs.Logger(env, interval=1000)\nenv = ch.envs.Torch(env) \n policy = PolicyNet()\noptimizer = optim.Adam(policy.parameters(), lr=1e-2)\nreplay = ch.ExperienceReplay()  # Manage transitions \n for step in range(1000):\n    state = env.reset()\n    while True:\n        mass = Categorical(policy(state))\n        action = mass.sample()\n        log_prob = mass.log_prob(action)\n        next_state, reward, done, _ = env.step(action) \n     # Build the ExperienceReplay\n    replay.append(state, action, reward, next_state, done, log_prob=log_prob)\n    if done:\n        break\n    else:\n        state = next_state\n\n# Discounting and normalizing rewards\nrewards = ch.td.discount(0.99, replay.reward(), replay.done())\nrewards = ch.normalize(rewards)\n\nloss = -th.sum(replay.log_prob() * rewards)\noptimizer.zero_grad()\nloss.backward()\noptimizer.step()\nreplay.empty()\n \n ~~~ \n Many more high-quality examples are available in the  examples/  folder. \n Installation \n Note  Cherry is considered in early alpha release. Stuff might break. \n pip install cherry-rl \n Changelog \n A human-readable changelog is available in the  CHANGELOG.md  file. \n Documentation \n Documentation and tutorials are available on cherry’s website:  http://cherry-rl.net . \n Contributing \n First, thanks for your consideration in contributing to cherry.\nHere are a couple of guidelines we strive to follow. \n \n It's always a good idea to open an issue first, where we can discuss how to best proceed. \n If you want to contribute a new example using cherry, it would preferably stand in a single file. \n If you would like to contribute a new feature to the core library, we suggest to first implement an example showcasing your new functionality. Doing so is quite useful: \n it allows for automatic testing, \n it ensures that the functionality is correctly implemented, \n it shows users how to use your functionality, and \n it gives a concrete example when discussing the best way to merge your implementation. \n \n \n \n We don't have forums, but are happy to discuss with you on slack.\nMake sure to send an email to  smr.arnold@gmail.com  to get an invite. \n Acknowledgements \n Cherry draws inspiration from many reinforcement learning implementations, including \n \n OpenAI Baselines , \n John Schulman's  implementations \n Ilya Kostrikov's  implementations , \n Shangtong Zhang's  implementations , \n Dave Abel's  implementations , \n Vitchyr Pong's  implementations , \n Kai Arulkumaran's  implementations , \n RLLab  /  Garage . \n \n Why 'cherry' ? \n Because it's the sweetest part of  the cake .", '\n \n \n learn2learn is a PyTorch library for meta-learning implementations. \n The goal of meta-learning is to enable agents to  learn how to learn .\nThat is, we would like our agents to become better learners as they solve more and more tasks.\nFor example, the animation below shows an agent that learns to run after a only one parameter update. \n \n Features \n learn2learn provides high- and low-level utilities for meta-learning.\nThe high-level utilities allow arbitrary users to take advantage of exisiting meta-learning algorithms.\nThe low-level utilities enable researchers to develop new and better meta-learning algorithms. \n Some features of learn2learn include: \n \n Modular API: implement your own training loops with our low-level utilities. \n Provides various meta-learning algorithms (e.g. MAML, FOMAML, MetaSGD, ProtoNets, DiCE) \n Task generator with unified API, compatible with torchvision, torchtext, torchaudio, and cherry. \n Provides standardized meta-learning tasks for vision (Omniglot, mini-ImageNet), reinforcement learning (Particles, Mujoco), and even text (news classification). \n 100% compatible with PyTorch -- use your own modules, datasets, or libraries! \n \n Installation \n ~~~bash\npip install learn2learn\n~~~ \n API Demo \n The following is an example of using the high-level MAML implementation on MNIST.\nFor more algorithms and lower-level utilities, please refer to the  documentation  or the  examples . \n ~~~python\nimport learn2learn as l2l \n mnist = torchvision.datasets.MNIST(root="/tmp/mnist", train=True) \n mnist = l2l.data.MetaDataset(mnist)\ntask_generator = l2l.data.TaskGenerator(mnist,\n                                        ways=3,\n                                        classes=[0, 1, 4, 6, 8, 9],\n                                        tasks=10)\nmodel = Net()\nmaml = l2l.algorithms.MAML(model, lr=1e-3, first_order=False)\nopt = optim.Adam(maml.parameters(), lr=4e-3) \n for iteration in range(num_iterations):\n    learner = maml.clone()  # Creates a clone of model\n    adaptation_task = task_generator.sample(shots=1) \n # Fast adapt\nfor step in range(adaptation_steps):\n    error = compute_loss(adaptation_task)\n    learner.adapt(error)\n\n# Compute evaluation loss\nevaluation_task = task_generator.sample(shots=1,\n                                        task=adaptation_task.sampled_task)\nevaluation_error = compute_loss(evaluation_task)\n\n# Meta-update the model parameters\nopt.zero_grad()\nevaluation_error.backward()\nopt.step()\n \n ~~~ \n Citation \n To cite the  learn2learn  repository in your academic publications, please use the following reference. \n \n Sébastien M.R. Arnold, Praateek Mahajan, Debajyoti Datta, Ian Bunner.  "learn2learn" .  https://github.com/learnables/learn2learn , 2019. \n \n You can also use the following Bibtex entry. \n ~~~bib\n@misc{learn2learn2019,\n    author       = {Sébastien M.R. Arnold, Praateek Mahajan, Debajyoti Datta, Ian Bunner},\n    title        = {learn2learn},\n    month        = sep,\n    year         = 2019,\n    url          = {https://github.com/learnables/learn2learn}\n    }\n~~~ \n Acknowledgements & Friends \n \n The RL environments are adapted from Tristan Deleu\'s  implementations  and from the ProMP  repository . Both shared with permission, under the MIT License. \n TorchMeta  is similar library, with a focus on supervised meta-learning. If learn2learn were missing a particular functionality, we would go check if TorchMeta has it. But we would also open an issue ;) \n higher  is a PyTorch library that also enables differentiating through optimization inner-loops. Their approach is different from learn2learn in that they monkey-patch nn.Module to be stateless. For more information, refer to  their ArXiv paper . \n', 'Embedding Adaptation is Still Needed for Few-Shot Learning \n \n Code Release for "Embedding Adaptation is Still Needed for Few-Shot Learning" \n This code provides: \n \n Re-implementation of the ATG algorithm in  examples/atg.py . \n Loaders for the dataset splits introduced in the paper. \n Demonstration code for training the algorithms, borrowed from  learn2learn . \n \n Resources \n \n Website:  seba1511.net/projects/atg \n Preprint:  arxiv.org/abs/2101.XXXXX \n Code:  github.com/Sha-Lab/atg \n \n Citation \n Please cite this work as follows: \n \n "Embedding Adaptation is Still Needed for Few-Shot Learning", Sébastien M. R. Arnold and Fei Sha \n \n or with the following BibTex entry: \n ~~~bibtex\n@article{arnold2021embedding,\n    title={Embedding Adaptation is Still Needed for Few-Shot Learning},\n    author={Sebastien M. R. Arnold, Fei Sha},\n    year={2021},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG}\n}\n~~~ \n Usage \n Dependencies include the following Python packages: \n \n PyTorch>=1.3.0 \n torchvision>=0.5.0 \n scikit-learn>=0.19.2 \n tqdm>=4.48.2 \n learn2learn on the master branch \n \n Running ATG \n A standalone re-implementation of ATG is provided in  examples/atg.py . To run it on a synthetic dataset: \n bash\npython examples/atg.py \n Training on ATG Partitions \n ~~~bash\npython examples/train.py --algorithm=\'protonet\' --dataset=\'mini-imagenet\' --taskset=\'original\'\n~~~ \n where \n \n taskset  takes values  easy ,  medium-easy ,  medium-hard ,  hard  or  randomX  where  X  is the seed to reproduce random splits. \n dataset  takes values  mini-imagenet ,  tiered-imagenet ,  emnist ,  lfw10 ,  cifar100 . \n algorithm  takes values  protonet ,  maml ,  anil . \n \n For more help on the interface, run:  python examples/train.py --help .', 'pycyju \n Benchmarking Python wrapping other languages', 'Policy Learning and Evaluation with Randomized Quasi-Monte Carlo \n \n Code release for "Policy Learning and Evaluation with Randomized Quasi-Monte Carlo", AISTATS 2022. \n This code provides a re-implementation of SAC combined with RQMC. It is based on the PyTorch implementation of SAC in  spinning-up . \n Resources \n \n Website:  seba1511.net/projects/qrl \n Preprint:  arxiv.org/abs/2202.07808 \n Code:  github.com/seba-1511/qrl \n \n Citation \n Please cite this work as follows: \n \n S. M. R. Arnold, P. L\'Ecuyer, L. Chen, Y. Chen, F. Sha,  Policy Learning and Evaluation with Randomized Quasi-Monte Carlo . AISTATS 2022. \n \n or with the following BibTex entry: \n ~~~bibtex\n@inproceedings{Arnold2022qrl,\n    title={Policy Learning and Evaluation with Randomized Quasi-Monte Carlo},\n    author={Arnold, S\\\'ebastien M. R. and L\'Ecuyer, Pierre and Chen, Liyu and Chen, Yi-fan and Sha, Fei},\n    year={2022},\n    booktitle={Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},\n    volume={131},\n    series={Proceedings of Machine Learning Research},\n    publisher={PMLR},\n}\n~~~ \n Usage \n Policy learning experiments can be run with the following command: \n ~~~shell\npython qsac.py --env HalfCheetah-v2 --rqmc --multi_actions 4\n~~~', 'sync-paperpile-notion \n Sync changes in Paperpile to a Notion database. \n Setup \n On Notion \n \n \n Create a new database (e.g. "Papers") with the columns named exactly: \n \n Title  of type title. \n Authors  of type text. \n Year  of type text. \n Link  of type url. \n Reference ID  of type text. \n \n \n \n Get the  database identifier  from the database page. If your database url is: \n https://www.notion.so/my_workspace/aaaabbbbccccddddeeeeffffgggghhhh \n Then the database identifier is:  aaaabbbbccccddddeeeeffffgggghhhh . \n \n \n Create a new integration on  https://www.notion.so/my-integrations/ . \n \n Name: Paperpile to Notion \n Associated Workspace: Workspace of the database. \n Content Capabilities: Read Content, Update Content, Insert Content. \n User Capabilities: Read user information, including email addresses. \n Press "Submit" and copy the  Internal Integration Token . \n \n \n \n On the database page, click "Share" (top right) and add "Paperpile to Notion" with edit access. \n \n \n On GitHub \n \n Fork this repository with the green "Use this template" button. \n On you fork, go to: "Settings -> Secrets -> Actions". \n \n Create 2 new repository secrets named exactly: \n \n NOTION_TOKEN : Your integration\'s internal integration token, from step 3.5 above. \n DATABASE_IDENTIFIER : Your database identifier, from step 2 above. \n \n \n \n On Paperpile \n \n Click on the top-right gear, go to "Workflows and Integrations". \n \n Follow the instructions to add a new "BibTex Export", choosing: \n \n Your GitHub repository fork as the repository. \n references.bib  as the export path. \n \n \n \n The first sync should start as soon as the Paperpile workflow is created, and subsequent syncs are triggered whenever papers are added or updated in your Paperpile. \n Note \nThe first sync might take some time as Notion limits the API rate to ~ 3 requests / second; so if you have 1,000 papers it\'ll take ~ 6 minutes before they are all available in Notion.']
xiaozhuchacha,['Code for CVPR 2016 Oral Paper \n Inferring Forces and Learning Human Utilities From Videos \n See  Wiki  for instructions \n Project website: http://www.yzhu.io/projects/cvpr16_chair/index.html \n Bibtex:\n bash\n@InProceedings{cvpr2016chair,\n    author = {Zhu, Yixin and Jiang, Chenfanfu and Zhao, Yibiao and Terzopoulos, Demetri and Zhu, Song-Chun},\n    title = {Inferring Forces and Learning Human Utilities From Videos},\n    booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n    year = {2016}}', 'Kinect2 Toolbox \n Code for CVPR 2015 Paper \n Understanding Tools: Task-Oriented Object Modeling, Learning and Recognition \n Project website: http://www.yzhu.io/projects/cvpr15_tool/ \n Bibtex:\n bash\n@InProceedings{zhu2015tool,\n    title={Understanding Tools: Task-Oriented Object Modeling, Learning and Recognition},\n    author={Zhu, Yixin and Zhao, Yibiao and Zhu, Song-Chun},\n    booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n    year={2015}\n} \n Kinect v2 Toolbox Manual \n Contact:  Yixin Zhu  (yixin.zhu@ucla.edu) \n Last revised: October 2016 \n Prerequisite \n \n Kinect SDK  Download Link . \n Windows 8.1/10. \n Visual Studio 2015 installed or Runtime Libraries installed. \n MATLAB R2014b or newer version for visualization (optional). \n \n Usage \n The toolbox includes two types of code:  Recorders  and  Converters .  Recorders  dump the Kinect raw data into RGB images, depth images and skeleton data.  Converters  post-process the data collected by Recorders. \n Recorders  include two sets: \n Set 1: Everything will be converted to depth space later.\n* ColorRecorder: dump raw RGB images to hard drive\n* DepthRecorder: dump raw depth images to hard drive\n* ColorDepthRecorder: dump both raw depth and RGB images to hard drive\n* SkeletonRecorder: dump skeleton data to hard drive\n* ColorDepthSkeletonRecorder: dump RGB, depth and skeleton simultaneously to hard drive. Two versions are provided. The  Mask  version marks the skeleton area in the depth image. The mapper produced in this code will be used to  convert rgb to depth space. . \n Set 2: Everything will be converted to color space later.\n* ColorDepthSkeletonRecorder2: dump RGB, depth and skeleton simultaneously to hard drive. The mapper produced in this code will be used to  convert depth to rgb space. . \n Converters  include two sets: \n Set 1: Everything will be converted to depth space later.\n* RGBDAlign: align RGB images to the depth images\n* Depth2PC: convert raw depth images to point cloud .ply files (no color)\n* ColorDepth2PC: convert aligned RGB-D images to point cloud .ply files \n Set 2: Everything will be converted to color space later.\n* RGBDAlign2: align depth images to the RGB images \n We recommend to use the binary files located inside Pipeline forder if you do not need to change anything in the source code. \n Record Kinect Raw Data with Kinect Studio \n You will need Kinect v2 to physically connect to your PC. Please refer to  this page  to check whether your machine is compatible with Kinect v2. The most important component is a USB 3.0 port. \n Open Kinect Studio, and switch to  Record  tab. Click on  Connect  button, and check all  11 streams . Once you finish recording, a .xef file will be generated. \n \n Kinect v2 consumes lots of memory. If you need to record long clips, remember to edit the buffer size before recording. Also, in order to achieve high frame rate when dumping the data into hard drive, it is recommended to have more than 16GB memory for Kinect Studio. \n \n Recorders \n In this section, NO Kinect v2 needed to physically connect to PC. Following instructions assume no connection. \n \n Load the recorded Kinect data (.xef file) that you want to dump using Kinect Studio. \n In PLAY tab, click Connected button. \n Check all 11 streams. \n \n \n \n Hit Play button, and you should be able to see the replayed videos. You can change the stream (RGB, depth, skeleton, etc) in two viewers by clicking on the gear-shape buttons. \n \n \n \n Hit Pause button at the beginning of the data that you want to dump. \n Open any  Recorder  mentioned in previous section. You should see a  stilled  skeleton in a pop-up windows. In this manual, we will use  ColorDepthSkeletonRecorder  as the example, since it is the most comprehensive  Recorder  that records RGB, depth and skeleton data simultaneously. \n \n \n \n Hit Play button to resume playing. The skeleton in the pop-up windows should start to move together with the videos in Kinect Studio. Data starts to dump into the  data folder. \n Hit Pause button again to stop dumping (before the video runs out), and close the pop-up window. \n \n Converters \n All the  converters  load the data from  data  folder, and write the new data into the same  data  folder. \n The inputs and outputs of  converters  are defined as \n Converters | Inputs | Outputs | Physically plugin Kinect?\n-----------|--------|---------|--------------------------\nDepth2PC | raw_depth | point cloud file (.ply) | Yes\nRGBDAlign | depth2rgb_mapper, raw_depth, raw_rgb | aligned_rgb | No\nColorDepth2PC | aligned_rgb, raw_depth | point cloud file (.ply) | Yes\nRGBDAlign2 | depth2rgb_mapper2, raw_depth, raw_rgb | aligned_depth | No \n MATLAB Visualization (optional) \n This part of toolbox is designed for visualization only. You can visualize the data easily in other programming languages, e.g. Python, C++, etc. \n \n Copy all the files from previous  data  folder into the subfolder  data  located in the folder  matlab_viz . \n Run  viz_result.m . It should show point cloud with human skeleton. The function  showPointCloud  is only supported in MATLAB 2014b or newer version, but can be easily replaced by function  plot3  in older versions. \n \n', 'OpenBottle \n Code for the paper "Feeling the Force: Integrating Force and Pose for Fluent Discovery through Imitation Learning to Open Medicine Bottles" \n Instructions:\nhttps://github.com/xiaozhuchacha/OpenBottle/wiki \n Please refer for the wiki for instructions on how to use this code. \n If you find this code useful, please cite our work with the following bibtex:\n @inproceedings{edmonds2017feeling,\n    title={Feeling the Force: Integrating Force and Pose for Fluent Discovery through Imitation Learning to Open Medicine Bottles},\n    author={Mark Edmonds, Feng Gao, Xu Xie, Hangxin Liu, Siyuan Qi, Yixin Zhu, Brandon Rothrock, Song-Chun Zhu},\n    booktitle={International Conference on Intelligent Robots and Systems (IROS)},\n    year={2017},\n    organization={IEEE}                                                         \n}', 'VCLATactileGlove \n Code for IROS 2017 Paper: A Glove-based System for Studying Hand-Object Manipulation via Joint Pose and Force Sensing \n Please refer for the wiki for instructions on how to use this code. \n If you find this code useful, please cite our work with the following bibtex:\n @inproceedings{liu2017glove,\n    title={A Glove-based System for Studying Hand-Object Manipulation via Joint Pose and Force Sensing},\n    author={Liu, Hangxin and Xie, Xu and Millar, Matt and Edmonds, Mark and Gao, Feng and Zhu, Yixin and Santos, Veronica J and Rothrock, Brandon and Zhu, Song-Chun},\n    booktitle={International Conference on Intelligent Robots and Systems (IROS)},\n    year={2017}\n}', 'AOG_AR \n Code for ICRA 2018 paper - Interactive Robot Knowledge Patching using Augmented Reality \n ROS Package : send_to_hololens/ \n Library Used \n \n tacopie \n OpenBottle/robot_control  for controlling a rethink baxter robot \n \n source files \n TCPPackageConstants.h \n Defines constants for the format of the packets transmitted between ROS and Hololens. \n send_to_hololens.cpp \n Establishes TCP connection with Hololens. Listens to different types of ROS messages (TF, image etc.) , and does some processing specific to the robot(Baxter) and gripper used in this application. Send updates on robot states to Hololens, and also listens to commands and updates sent by user from Hololens. \n Unity Application : Assets/      ProjectSettings/ \n Development Platform \n \n Microsoft Hololens \n Unity 5.6 \n Visual Studio 2017 \n \n Library Used \n \n MixedRealityToolkit-Unity \n HoloLensARToolKit \n \n Overview \n This application receives real-time updates on robot states(e.g. TF, kinect images, gripper force) from ROS, and displays those information as hololens to users through Microsoft Hololens platform. The user can also see the knowledge base of the robot represented as an and-or graph, and interactively patch the knowledge structure.  \n Scripts \n TCPManager.cs \n Listens for a TCP connection first. Once connection established, try to fetch data every frame, and stores them in the buffer for other scripts to access. \n SensorDisplay.cs \n Controls the visibility and position of the holograms and the buttons. Convert data from the frame of the robot to world frame of the Hololens application. \n Node.cs \n Class to represent a node in the and-or graph. \n SimpleTree.cs \n Class to represent the and-or graph structure. Provide function to go to the next end node(action) from any end node. \n SlotPanel.cs, Slot.cs, NodeChange.cs, DragHandler.cs, Inventory.cs \n Implements drag and drop on canvas to enable and-or graph patching. Parse the and-or graph to generate action sequence. \n OnScreenKeyboard.cs \n Helper class to use TouchScreenKeyboard class to edit new action name. (Requires building in XMAL instead of D3D) \n HandDraggableCustom.cs, GripperNewPoseControl.cs, EditAction.cs \n Allow user to edit an end node, and set new pose for the action by dragging a gripper hologram with hand gesture. \n Wiki \n https://github.com/xiaozhuchacha/AOG_AR/wiki']
bossjones,['vagrant box add scarlettpi_v1 /Users/malcolm/dev/basebox-packer/virtualbox/ubuntu1204-desktop-provisionerless.box \n vagrant plugin install vagrant-salt', 'bossjones-pocketsphinx \n Clone of SVN Pocketsphinx 0.8 with optimizations for Scarlett.', "bossjones-fabric \n Just a github repo for my fabric tasks. Will continue to refactor over time \n For my purposes, I always use a Jumpserver to access all of my remote servers. \n because of this you need to add the following to your  .bash_profile : \n For bossjones fabric tasks \n ```\nexport FABRIC_KEY_FILENAME='/path/to/.ssh/id_rsa' \n export FABRIC_JUMPSERVER='127.0.0.1' \n export FABRIC_USER='blacktonystarkoflife'\n``` \n Yes I know I can use a user wide  ~/.fabricrc  file as well. \n Known errors \n \n \n If you get the following error  NetworkError: Error reading SSH protocol banner  make sure you note which servers this error was thrown on, and simply ssh into it. This will add it to your  known_hosts  file, and should prevent this issue from coming up. \n \n \n If you're using a jumpserver with  nc , seems as though it caps out at 10 concurrent connections. Researching how to increase this limit \n \n", 'Overview \n This is a demo of Vagrant using a  SaltStack  provisioner. It can be used as a template for a new webapp project. Following the steps below will set you up with a Ubuntu 12.04 (precise32) Vagrant VM running unconfigured nginx, supervisord, memcached, postgresql and rabbitmq. Feel free to fork and extend this example. \n After  downloading and installing Vagrant , you need to set up your Vagrant configuration with some additional elements.  You only have to do these things once on your development box. \n vagrant gem install vagrant-salt\nvagrant gem install vagrant-vbguest \n The first installs the SaltStack provisioner for Vagrant. The second installs the Vagrant "auto-update VirtualBox extensions" module, which is optional, but handy. \n Next, clone this repo and run the following commands: \n git clone https://github.com/jaddison/vagrant-salt-demo.git\ncd vagrant-salt-demo\ngit submodule init\ngit submodule update\nvagrant up \n Your vagrant will now be provisioned and running - try browsing to  http://localhost:9000/ . \n The  git submodule used is here . It is a simple set of Salt states and roles, which I plan to improve upon - pull requests are welcome. Feel free to use them in other projects. \n TODO \n \n set up a  virtualenv  to host a Python webapp (ie. Django). \n set up database user, db, etc \n use Salt\'s templating to set up configuration files \n include multiple Vagrantfiles for different scenarios (ie. multi-VMs that communicate between themselves) \n', 'bossjones-chef \n The bossjones chef setup \n links: \n ```\n- http://www.markholmberg.com/articles/installing-ruby-2-system-wide-on-ubuntu-12-04-using-rbenv \n \n \n https://github.com/fesplugas/rbenv-installer \n \n \n https://www.digitalocean.com/community/articles/how-to-install-chef-and-ruby-with-rvm-on-a-ubuntu-vps \n \n \n NOTE, Make sure the following ENV variables are set in your  .bash_profile \n ``` \n ``` \n export GIT_ACCESS_TOKEN="gdfsiuhdsiughfdsiughfdiughdfsiughsdfiugh"\nexport KITCHEN_GLOBAL_YAML="~/path/to/chef/config/kitchen.yml" \n ``` \n Also, don\'t forget to install  knife-block', 'bossjones-chef-scarlett \n Provision a scarlett dev server with chef-client or provision an existing server with chef_solo', 'Sigintwrapper \n This is a simple wrapper which runs the provided program and\nforwards SIGTERM to SIGINT.  This is because uWSGI listens for\nSIGINT for restarts while runit sends SIGTERM. \n Usage \n After installing via: \n pip install sigintwrapper\n \n Use via: \n sigintwrapper /absolute/path/to/program [with arguments]\n', 'bossjones-packer \n Create Packer images using Rake tasks \n ```\n± |master ✓| → be rake -T\nrake build:digitalocean         # Build a base image from chef cookbooks\nrake check:build_vars[service]  # Check that ENV variables are set\nrake check:validate[service]    # Validate all the packer templates in service directory\nrake spec                       # Run the specs \n |1.9.3-p448|  malcolm-MacBookPro1 in ~/dev/bossjones/bossjones-packer\n± |master ✓| →\n```', 'opencv-spotify-hackathon \n Using opencv to do face detection and play a creepy sound every-time a face is detected \n install pygame in virtualenv \n brew install libvorbis sdl sdl_image sdl_mixer sdl_ttf portmidi\npip install hg+http://bitbucket.org/pygame/pygame', '\n \n docker-php \n Provides basic building blocks for PHP web applications, available on  Docker Hub \nAdd’s PHP-FPM, mods, and specific backend configuration to Behance’s  docker-nginx \n Three variants are available:\n- (default) Ubuntu-based, PHP 7.0 \n- (slim) Alpine-based, PHP 7.0, tagged as  -alpine \n- (beta) Ubuntu-based, PHP 7.1, tagged as  -beta \n- (legacy) Ubuntu-based, PHP 5.6, tagged as  -legacy   \n Includes \n \n \n Nginx \n PHP/PHP-FPM (7.0, 7.1, 5.6) \n S6: for PID 1 zombie reaping, startup coordination, shutdown signal transferal \n Goss: for serverspec-like testing. Run  goss -g /tests/php-fpm/{variant_name}.goss.yaml  to validate any configuration updates \n Extra PHP Modules: \n \n *   - not available on Alpine variant \n ^   - not available on Beta tag \n ~   - disabled by default (use  phpenmod  to enable on Ubuntu-based variants, uncomment .ini file otherwise)\n  - apc ^ (only visible for backwards compatibility) \n  - apcu^\n  - calendar\n  - bz2\n  - ctype\n  - curl\n  - date\n  - dom\n  - exif\n  - fpm\n  - gd\n  - gearman \n  - iconv\n  - igbinary \n  - intl\n  - json\n  - mbstring\n  - mcrypt\n  - memcache ^\n  - memcached \n  - mysqli\n  - mysqlnd\n  - newrelic~ (activates with env variables)\n  - opcache\n  - openssl\n  - pcntl\n  - pdo\n  - pdo_mysql\n  - pdo_pgsql~\n  - pgsql~\n  - phar\n  - posix\n  - redis~\n  - shmop\n  - SimpleXML\n  - sockets\n  - sysvmsg\n  - sysvsem\n  - sysvshm\n  - xdebug~\n  - xml\n  - xmlreader\n  - xmlwriter\n  - yaml ~\n  - zip\n  - zlib \n Expectations \n \n Applications that leverage  bryanlatten/docker-php  as their container parent are expected to copy their application into  /app , for example:\n COPY ./ /app/ \n Inside the copied directory, there must be a directory named  public  -- this will be automatically assigned as the webroot for the web server, which expects\na front controller called  index.php . \n Production Mode: an immutable container (without file updates) should set  CFG_APP_DEBUG=0  for max PHP performance   \n NOTE: Nginx is exposed and bound to an unprivileged port,  8080   \n Monitoring \n \n \n NewRelic APM: automatically enabled by adding providing environment variables  REPLACE_NEWRELIC_APP  and  REPLACE_NEWRELIC_LICENSE \n PHP-FPM Status: available  only  inside container at  /__status . Application healthcheck can pull PHP-FPM statistics from  http://127.0.0.1/__status?json . To open to more clients than local, add more  allow  statements in  __status  location block in  $CONF_NGINX_SITE ( /etc/nginx/sites-available/default ) \n Nginx Status: available  only  inside container at  /__nginx_status . Application healthcheck can pull nginx statistics from  http://127.0.0.1/__nginx_status . To open to more clients than local, add more  allow  statements in  __nginx_status  location block in $CONF_NGINX_SITE ( /etc/nginx/sites-available/default )  \n \n Downstream Configuration \n \n Several environment variables can be used to configure various PHP FPM paramaters, as well as a few Nginx configurations.\nas such. These can be used to drive the configuration of the downstream PHP application in any way necessary, but there are a few environment variables that  bryanlatter/docker-php  will process along the way... \n See parent  docker-nginx  for additional configuration   \n Variable | Example | Default | Description\n--- | --- | --- | ---\n *  |  DATABASE_HOST=master.rds.aws.com  | - | PHP has access to environment variables by default\n CFG_APP_DEBUG  |  CFG_APP_DEBUG=1  | 1 | Set to  1  or  true  will cue the Opcache to watch for file changes. Set to 0 for  production mode , which provides a sizeable performance boost, though manually updating a file will not be seen unless the opcache is reset.\n SERVER_MAX_BODY_SIZE  |  SERVER_MAX_BODY_SIZE=4M  | 1M | Allows the downstream application to specify a non-default  client_max_body_size  configuration for the  server -level directive in  /etc/nginx/sites-available/default \n SERVER_FASTCGI_BUFFERS  |  SERVER_FASTCGI_BUFFERS=‘512 32k’  | 256 16k |  docs ,  tweaking \n SERVER_FASTCGI_BUFFER_SIZE  |  SERVER_FASTCGI_BUFFER_SIZE=‘256k’  | 128k |  docs ,  tweaking \n SERVER_FASTCGI_BUSY_BUFFERS_SIZE  |  SERVER_FASTCGI_BUSY_BUFFERS_SIZE=‘1024k’  | 256k |  docs \n REPLACE_NEWRELIC_APP  |  REPLACE_NEWRELIC_APP=prod-server-abc  | - | Sets application name for newrelic\n REPLACE_NEWRELIC_LICENSE  |  REPLACE_NEWRELIC_LICENSE=abcdefg  | - | Sets license for newrelic, when combined with above, will enable newrelic reporting\n PHP_FPM_MEMORY_LIMIT  |  PHP_FPM_MEMORY_LIMIT=256M  | 192MB | Sets memory limit for FPM instances of PHP\n PHP_FPM_MAX_EXECUTION_TIME  |  PHP_FPM_MAX_EXECUTION_TIME=30  | 60 | Sets time limit for FPM workers\n PHP_FPM_UPLOAD_MAX_FILESIZE  |  PHP_FPM_UPLOAD_MAX_FILESIZE=100M  | 1M | Sets both upload_max_filesize and post_max_size\n PHP_FPM_MAX_CHILDREN  |  PHP_FPM_MAX_CHILDREN=15  | 4096 |  docs \n PHP_FPM_START_SERVERS  |  PHP_FPM_START_SERVERS=40  | 20 |  docs \n PHP_FPM_MAX_REQUESTS  |  PHP_FPM_MAX_REQUESTS=100  | 1024 |  docs  How many requests an individual FPM worker will process before recycling\n PHP_FPM_MIN_SPARE_SERVERS  |  PHP_FPM_MIN_SPARE_SERVERS=10  | 5 |  docs \n PHP_FPM_MAX_SPARE_SERVERS  |  PHP_FPM_MAX_SPARE_SERVERS=64  | 128 |  docs', 'dockerfiles \n Bunch of Dockfiles for me to mess around w/ different services/applications', "pentest-linux-privileges \n borrowed via chousensha \n Linux privilege checker \n Searches for privileges, service settings, sensitive information, running\nprocesses, installed programs, network statistics, logs and  various other information about the system. \n Inspired from g0tm1lk's cheatsheet, available at http://blog.g0tmi1k.com/2011/08/basic-linux-privilege-escalation.html \n NOTES \n Full output may be excessive, I tried to break it down in specific functions to make it easy to check for relevant information only by commenting out the unnecessary bits. \n It's using the subprocess module for running commands. \n Running time may vary, with all checks enabled it took ~1min on a freshly installed Debian and ~4min on a custom Kali system. \n DEMO \n See text file for output. \n Collection of network related scripts. \n DEMO \n \n dnsquery \n Perform DNS lookups using Python. \n \n ./dnsquery.py twitter.com A\nAddress for twitter.com is 199.16.156.38\nAddress for twitter.com is 199.16.156.70\nAddress for twitter.com is 199.16.156.102\nQuery completed at Tue Mar 25 12:40:28 2014 \n ./dnsquery.py wikipedia.com AAAA\nIPv6 address for wikipedia.com is 2620:0:861:ed1a::1\nQuery completed at Tue Mar 25 12:49:28 2014 \n ./dnsquery.py wikipedia.com MX\nMail server at lists.wikimedia.org. preference: 50\nMail server at mchenry.wikimedia.org. preference: 10\nQuery completed at Tue Mar 25 12:50:29 2014 \n ./dnsquery.py yahoo.com NS\nDNS server for yahoo.com domain: ns3.yahoo.com.\nDNS server for yahoo.com domain: ns4.yahoo.com.\nDNS server for yahoo.com domain: ns2.yahoo.com.\nDNS server for yahoo.com domain: ns1.yahoo.com.\nDNS server for yahoo.com domain: ns6.yahoo.com.\nDNS server for yahoo.com domain: ns5.yahoo.com.\nQuery completed at Tue Mar 25 12:51:06 2014 \n \n honeypot \n Simple honeypot server using the socket module. \n Server listens on a port and displays a banner read from a file to every client that\nconnects to it. Data received from clients is logged to a file \n \n ./honeypot.py -f banner.txt -p 5555\nListening on port 5555 on 2014-04-10 10:21\n[2014-04-10 10:21] Connection received from 192.168.127.133:52379\nReceived from 192.168.127.133: I'm the first client \n [2014-04-10 10:21] Connection received from 192.168.127.130:59013\nReceived from 192.168.127.130: And I'm the second \n Received from 192.168.127.130: blabla \n Received from 192.168.127.130: bye", 'Clean Blog by Start Bootstrap - Jekyll Version \n The official Jekyll version of the Clean Blog theme by  Start Bootstrap . \n View Live Demo → \n Before You Begin \n In the _config.yml file, the base URL is set to /startbootstrap-clean-blog-jekyll which is this themes gh-pages preview. It\'s recommended that you remove the base URL before working with this theme locally! \n It should look like this:\n baseurl: "" \n What\'s Included \n A full Jekyll environment is included with this theme. If you have Jekyll installed, simply run  jekyll serve  in your command line and preview the build in your browser. You can use  jekyll serve --watch  to watch for changes in the source files as well. \n A Grunt environment is also included. There are a number of tasks it performs like minification of the JavaScript, compiling of the LESS files, adding banners to keep the Apache 2.0 license intact, and watching for changes. Run the grunt default task by entering  grunt  into your command line which will build the files. You can use  grunt watch  if you are working on the JavaScript or the LESS. \n You can run  jekyll serve --watch  and  grunt watch  at the same time to watch for changes and then build them all at once. \n Support \n Visit Clean Blog\'s template overview page on Start Bootstrap at http://startbootstrap.com/template-overviews/clean-blog/ and leave a comment, email feedback@startbootstrap.com, or open an issue here on GitHub for support. \n TONYDARK.IO \n How to get setup \n If you don\'t already have Jekyll, run:  gem install jekyll \n If you don\'t already have Bundler, run:  gem install bundler \n If you don\'t already have the site locally, fork and clone. \n To get dependencies run from the site directory:  bundle install \n How to run Jekyll locally \n serve only \n bundle exec jekyll serve \n serve and watch for changes \n bundle exec jekyll serve --watch \n serve drafts and watch for changes \n bundle exec jekyll serve --watch --drafts \n How to contribute content \n \n If you are not already in the authors file, add yourself \n In a local fork, preferably in a branch off of master, write in a markdown file ( .md ) in the  _drafts  folder, optionally committing the work to manage "revisions" and push to your remote if you\'d like to keep your work "backed up" \n When ready for review (final or incremental), push the post upstream and open a pull request \n note: if you are looking for comments but not ready to publish, keep the post in the  _drafts  directory; if you are ready to publish, move the file to the  _posts  directory and prefix the filename with the date ( 2015-09-27-my-fence-post.md ) before opening the pull request \n Colleagues can then view and comment on the post (potentially with a "diff" of a previous version, if one exists in the "upstream") \n Once you and the reviewer(s) are satisfied, merge the PR. If the content was moved to the  _posts  directory in the PR, then you are now "published." \n \n References \n \n Jekyll docs \n Markdown docs \n Github Flow guide \n', 'spotify-leap-hackathon \n Repo for Synthesis & Samples Music Hackathon. \n brew requirements \n brew install liblo libsndfile portaudio portmidi \n On the Mac, it is very simple to build pyo from source with the Homebrew package mananger: * \n ``` \n source: https://code.google.com/p/pyo/wiki/Installation \n brew install python liblo libsndfile portaudio portmidi\ncd /tmp\nsvn checkout http://pyo.googlecode.com/svn/trunk/ pyo\ncd pyo\npython setup.py install --use-coreaudio --use-double --universal\n``` \n A short gist explaining how to compile pyo - the powerful Audio/DSP library for Python, on Mac OS X. The instructions are specifically for installing pyo with a brewed Python install - not with Mac OS X system Python. \n ``` \n source: https://gist.github.com/pwalsh/5691534 \n using brew installed Python \n brew install portaudio portmidi libsndfile liblo jack \n brew link portaudio portmidi libsndfile liblo \n cd ~/Sites/tmp \n svn checkout http://pyo.googlecode.com/svn/trunk/ pyo-read-only \n cd pyo-read-only \n either jump into a virtualenv now or create one \n CHOOSE: \n workon spotify-leap-hackathon \n mkvirtualenv spotify-leap-hackathon \n python setup.py install --use-coreaudio --use-double \n cd ../ \n sudo rm -r pyo-read-only\n``` \n  Fix for LeapPython.so and using homebrew python  \n install_name_tool -change /Library/Frameworks/Python.framework/Versions/2.7/Python \\\n/usr/local/Cellar/python/2.7.9/Frameworks/Python.framework/Versions/2.7/lib/libpython2.7.dylib \\\nLeapPython.so \n ``` \n Info on static libs installed \n Python-pyo (version 0.7.5) \n System requirements : OS X 10.6 to 10.10 \n This package installs all the required components to run pyo inside your current Python installation. Python 2.6 or 2.7 (32/64 bit) must be already installed on your system. \n This package is divided into two separate installers. If you do not require one of them, please unselect the package in custom installation mode. \n \n pyo extension:\nThe following components will be installed in the site-packages folder of the current Python Framework: \n \n _pyo.so\n_pyo64.so\npyo.py\npyo64.py\npyolib (folder) \n \n Support libraries (i386 and x86_64):\nThis component will install a number of dynamic libraries on which pyo depends. If you already have these, then you can skip this installation. \n \n Warning: this installation will overwrite any previously installed libraries. These are the libraries that will be installed in your /usr/local/lib directory: \n liblo.7.dylib\nlibportaudio.2.dylib\nlibportmidi.dylib\nlibsndfile.1.dylib\nlibFLAC.8.dylib\nlibvorbisenc.2.dylib\nlibvorbis.0.dylib\nlibogg.0.dylib \n Olivier Bélanger, 2015\n```` \n Important for keyboard interaction etc and useful bootstrap steps \n ``` \n source: https://github.com/openstenoproject/plover/blob/master/osx/DevReadme.txt \n ``` \n ```\nTest that pyo is installed correctly etc \n from pyo import * \n s = Server().boot()\ns.start()\nm = Input(chnl=1, mul=2)\npva = PVAnal(m, size=1024)\npvt = PVTranspose(pva, transpo=1.5)\npvs = PVSynth(pvt).out()\ndry = Delay(m, delay=1024./s.getSamplingRate(), mul=.7).out(1) \n ```', 'sublime3-user-settings \n My user settings for sublime text 3', 'bossjones-sphinxbase \n Clone of SVN Sphinxbase 0.8 with additions for Scarlett.', 'bossjones-cthehardway \n Learning some C!', 'bossjones-gst-plugins-espeak \n Clone of gst-plugins-espeak to work with gst-0.10. v0.3.5', 'drywall \n Python wrapper script of common bash commands to find out info on a linux system', 'dogknife \n (n) A sweet integration between knife and datadog\n \n Datadog  already instruments your  Chef  runs.\nNow, with  dogknife , instrument your knife commands to know  who  ran  what  and  when . \n Getting Started \n \n If you are not a Datadog user yet,  sign up \n Get your  API key  and your Datadog handle. \n Configure them as  datadog_api_key  and  datadog_user  in your knife configuration file (usually in  ~/.chef/knife.rb ) \n Install  dogknife  with  gem install dogknife \n Use  dogknife  where you would use  knife \n', 'golang-helloworld', "bossjones-dotfiles \n My dotfiles and other cool shit \n Fonts \n Enabling the Powerline look \n Powerline originated as a status-line plugin for Vim. Its popular eye-catching\nlook is based on the use of special symbols:  \n To make use of these symbols, there are several options: \n \n use a font that already bundles those: this is e.g. the case of the\n   2.030R-ro/1.050R-it version  of the Source Code Pro] font \n use a  pre-patched font \n use your preferred font along with the  Powerline font  (that\n  only contains the Powerline symbols):  this highly depends on your operating\n  system and your terminal emulator \n patch your preferred font  by adding the missing\n  Powerline symbols: this is the most difficult way and is no more documented in\n  the  Powerline manual \n \n Please see the  Powerline manual  for further details. \n Then edit the  ~/.tmux.conf.local  file ( <prefix> e ) and adjust the following\nvariables: \n ```\ntmux_conf_theme_left_separator_main='\ue0b0'\ntmux_conf_theme_left_separator_sub='\ue0b1'\ntmux_conf_theme_right_separator_main='\ue0b2'\ntmux_conf_theme_right_separator_sub='\ue0b3' \n Dotfiles to checkout \n \n https://github.com/giftofjehovah/dotfiles/blob/8b61dfa30df615667eae7d236076a17729de5604/settings/macos.sh \n https://github.com/rafeca/dotfiles/blob/816ffb81dc9ef37a16ce1037dbe121038ac86222/osx/apps/iterm2.sh \n https://github.com/vbdjames/dotfiles/blob/853731feaad7c8486f74ee089f32a3674e0b349c/iterm/install.sh \n https://github.com/khoad/dotfiles/blob/c80db09659a590070c16d5942fcbfe8ccabd8e5c/setupNewRig.sh \n https://github.com/davejacobs/dotfiles/blob/a028a6e9b1f116035dd81ab6acdeee75751eb3f4/bin/setup-envs/osx \n", 'gobot.io-examples', 'opencv-python-examples \n Install OpenCV via: http://www.pyimagesearch.com/2015/06/15/install-opencv-3-0-and-python-2-7-on-osx/', 'scarlett-dbus-poc \n Scarlett Dbus Listener Service implementation Proof-Of-Concept. \n Source: https://www.reddit.com/r/gnome/comments/3owhp6/python_help_critique_my_application_design_home/ \n How to test scarlett_player \n Open ipython and type the following: \n ```\nIn [1]: import scarlett_player \n In [2]: scarlett_player.ScarlettPlayer("pi-listening").run()\n``` \n Simple python dbus example \n Source: https://github.com/stylesuxx/python-dbus-examples \n \n \n Run dbus monitor and grep for the interesting output: \n dbus-monitor | grep /tld/domain/sub \n \n \n Run the receiver \n \n \n Then you can either\n* Run the invoker, which will call the proxxied receivers Test object methods.\n* run the emitter, which will emit a test signal and a quit signal. \n After running either of them, the receiver should be stopped. \n Vagrant packaging \n package the box \n vagrant package --base scarlett-dbus-poc_default_1449512156423_85046 --output ~/ubuntu_14_04_base_w_gst_pulseaudio_guestadd2.box \n NOTE: WE SHOULD TRY RUNNING PACKAGE LIKE THIS \n vagrant package --base scarlett-dbus-poc_default_1449512156423_85046 --output ~/ubuntu_14_04_base_w_gst_pulseaudio_guestadd3.box --vagrantfile ~/dev/bossjones/scarlett-dbus-poc/Vagrantfile \n add the box \n vagrant box add --name "scarlettpi-base-ubuntu-14-04-pulse" /Users/malcolm/ubuntu_14_04_base_w_gst_pulseaudio_guestadd3.box \n reinstalling unity to make desktop work correctly again \n http://askubuntu.com/questions/502224/dash-wont-launch-applications-after-upgrade-to-14-04 \n new way to install \n https://github.com/cmusphinx/pocketsphinx\nhttps://github.com/cmusphinx/cmudict\nhttps://github.com/cmusphinx/pocketsphinx-python\nhttps://github.com/cmusphinx/sphinxbase\nhttps://github.com/cmusphinx/sphinxtrain\nhttps://github.com/cmusphinx/cmudict-tools\nhttps://github.com/cmusphinx/kaldi \n dbus.con.emit_signal() example and breakdown \n ``` \n Important reading:   https://wiki.gnome.org/action/show/Projects/PyGObject/IntrospectionPorting?action=show&redirect=PyGObject/IntrospectionPorting \n devhelp command on ubuntu 15.10 \n Assume c code looks like: \n gboolean\ng_dbus_connection_emit_signal (GDBusConnection  connection,\n                               const gchar  destination_bus_name,\n                               const gchar  object_path,\n                               const gchar  interface_name,\n                               const gchar  signal_name,\n                               GVariant  parameters,\n                               GError **error); \n ... where ... \n Parameters:\n  connection\n    a GDBusConnection \n destination_bus_name\n    the unique bus name for the destination for the signal or NULL to emit to all listeners. \n object_path\n    path of remote object \n interface_name\n    D-Bus interface to emit a signal on \n signal_name\n    the name of the signal to emit \n parameters\n    a GVariant tuple with parameters for the signal or NULL if not passing parameters. \n error\n    Return location for error or NULL \n Return value\n    TRUE unless error is set \n Then the python version looks like: \n listener_rdy_status = GLib.Variant("(ss)", (message, scarlett_sound))\nbus.emit_signal(None,\n                \'/org/scarlett/Listener\',\n                \'org.scarlett.Listener\',\n                \'ListenerReadySignal\',\n                listener_rdy_status)\n``` \n Client -> Service issue: \n ``` \n Currently getting the following issue when proxy tries to connet: \n method call time=1455504495.425644 sender=:1.49 -> destination=org.freedesktop.DBus serial=1 path=/org/freedesktop/DBus; interface=org.freedesktop.DBus; member=Hello\nmethod return time=1455504495.425676 sender=org.freedesktop.DBus -> destination=:1.49 serial=1 reply_serial=1\n   string ":1.49"\nsignal time=1455504495.425695 sender=org.freedesktop.DBus -> destination=(null destination) serial=203 path=/org/freedesktop/DBus; interface=org.freedesktop.DBus; member=NameOwnerChanged\n   string ":1.49"\n   string ""\n   string ":1.49"\nsignal time=1455504495.425726 sender=org.freedesktop.DBus -> destination=:1.49 serial=2 path=/org/freedesktop/DBus; interface=org.freedesktop.DBus; member=NameAcquired\n   string ":1.49"\nmethod call time=1455504495.426215 sender=:1.4 -> destination=org.freedesktop.DBus serial=165 path=/org/freedesktop/DBus; interface=org.freedesktop.DBus; member=GetConnectionUnixProcessID\n   string ":1.49"\nmethod return time=1455504495.426264 sender=org.freedesktop.DBus -> destination=:1.4 serial=204 reply_serial=165\n   uint32 27637\nmethod call time=1455504495.428239 sender=:1.49 -> destination=org.freedesktop.DBus serial=2 path=/org/freedesktop/DBus; interface=org.freedesktop.DBus; member=AddMatch\n   string "type=\'signal\',sender=\'org.freedesktop.DBus\',interface=\'org.freedesktop.DBus\',member=\'NameOwnerChanged\',path=\'/org/freedesktop/DBus\',arg0=\'org.scarlett.Listener.SttFailedSignal\'"\nmethod return time=1455504495.428292 sender=org.freedesktop.DBus -> destination=:1.49 serial=3 reply_serial=2\nmethod call time=1455504495.428302 sender=:1.49 -> destination=org.freedesktop.DBus serial=3 path=/org/freedesktop/DBus; interface=org.freedesktop.DBus; member=GetNameOwner\n   string "org.scarlett.Listener.SttFailedSignal"\nerror time=1455504495.428320 sender=org.freedesktop.DBus -> destination=:1.49 error_name=org.freedesktop.DBus.Error.NameHasNoOwner reply_serial=3\n   string "Could not get owner of name \'org.scarlett.Listener.SttFailedSignal\': no such name"\nmethod call time=1455504495.428336 sender=:1.49 -> destination=org.scarlett serial=4 path=/org/scarlett; interface=org.freedesktop.DBus.Introspectable; member=Introspect\nmethod return time=1455504495.428346 sender=:1.48 -> destination=:1.49 serial=12 reply_serial=4\n   string "<!DOCTYPE node PUBLIC "-//freedesktop//DTD D-BUS Object Introspection 1.0//EN"\n                      "http://www.freedesktop.org/standards/dbus/1.0/introspect.dtd"> \n  GDBus 2.46.2  \n \n \n \n" \n THIS IS A PROPER SIGNAL EMIT \n signal time=1455504580.676030 sender=:1.48 -> destination=(null destination) serial=13 path=/org/scarlett/Listener; interface=org.scarlett.Listener.SttFailedSignal; member=SttFailedSignal\n   string "  ScarlettListener hit Max STT failures"\n   string "pi-response2"\n``` \n FYI \n ```\nIn [38]: ss = bus.get("org.scarlett", object_path=\'/org/scarlett/Listener\') \n In [39]: ss.\nss.Get                          ss.emitCommandRecognizedSignal\nss.GetAll                       ss.emitConnectedToListener\nss.GetMachineId                 ss.emitKeywordRecognizedSignal\nss.Introspect                   ss.emitListenerCancelSignal\nss.Message                      ss.emitListenerReadySignal\nss.Ping                         ss.emitSttFailedSignal\nss.Set \n In [39]: ss.\nss.Get                          ss.emitCommandRecognizedSignal\nss.GetAll                       ss.emitConnectedToListener\nss.GetMachineId                 ss.emitKeywordRecognizedSignal\nss.Introspect                   ss.emitListenerCancelSignal\nss.Message                      ss.emitListenerReadySignal\nss.Ping                         ss.emitSttFailedSignal\nss.Set \n In [39]: exit\n``` \n Use this to figure out why foo ... which is suppose to just return a string, returns a ("string",) instead: \n ```\nIn [38]: foo_client = bus.get(\'net.lvht\', object_path=\'/net/lvht/Foo\') \n In [39]: foo_client\nOut[39]:  \n In [40]: foo_client.\nfoo_client.Get           foo_client.GetAll        foo_client.GetMachineId  foo_client.HelloWorld    foo_client.Introspect    foo_client.Ping          foo_client.Set \n In [40]: foo_client.\nfoo_client.Get           foo_client.GetAll        foo_client.GetMachineId  foo_client.HelloWorld    foo_client.Introspect    foo_client.Ping          foo_client.Set \n In [40]: foo_client.HelloWorld\nOut[40]:  > \n In [41]: foo_client.HelloWorld(\'hello\',1)\nOut[41]: (\'hello1\',) \n In [42]:\n``` \n Looks like the in/out variables look incorrect: \n ```\n(MainThread) DEBUG    Inside self.method_inargs and self.method_outargs\n{   \'Message\': (),\n    \'emitCommandRecognizedSignal\': (\'s\',),\n    \'emitConnectedToListener\': (\'s\',),\n    \'emitKeywordRecognizedSignal\': (),\n    \'emitListenerCancelSignal\': (),\n    \'emitListenerReadySignal\': (),\n    \'emitSttFailedSignal\': ()}\n{   \'Message\': \'(s)\',\n    \'emitCommandRecognizedSignal\': \'(s)\',\n    \'emitConnectedToListener\': \'((s))\',\n    \'emitKeywordRecognizedSignal\': \'(s)\',\n    \'emitListenerCancelSignal\': \'(s)\',\n    \'emitListenerReadySignal\': \'(s)\',\n    \'emitSttFailedSignal\': \'(s)\'} \n with xml \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n``` \n ``` \n compared to foo.py \n (MainThread) DEBUG    Inside self.method_inargs and self.method_outargs\n(MainThread) DEBUG    Inside self.method_inargs\n{   \'HelloWorld\': (\'a\', \'b\')}\n(MainThread) DEBUG    Inside self.method_outargs\n{   \'HelloWorld\': \'(s)\'} \n with XML that looks like \n \n \n \n \n \n \n \n \n \n \n``` \n we are finally closer to getting signals read in and acted upon!! \n ```\n\'GStreamer 1.6.0\' \n \n (MainThread) DEBUG    ss PrettyPrinter:\n \n(MainThread) DEBUG    player_cb PrettyPrinter:\n(    ,\n    \':1.88\',\n    \'/org/scarlett/Listener\',\n    \'org.scarlett.Listener\',\n    \'SttFailedSignal\',\n    GLib.Variant(\'(ss)\', (\'  ScarlettListener hit Max STT failures\', \'pi-response2\'))) \n \n ValueError                                Traceback (most recent call last)\n/home/pi/dev/bossjones-github/scarlett-dbus-poc/test_gdbus_proxy_service.py in player_cb( args=( , \':1.88\', \'/org/scarlett/Listener\', \'org.scarlett.Listener\', \'SttFailedSignal\', GLib.Variant(\'(ss)\', (\'  ScarlettListener hit Max STT failures\', \'pi-response2\'))),  *kwargs={})\n     99     pp = pprint.PrettyPrinter(indent=4)\n    100     pp.pprint(args)\n--> 101     msg, scarlett_sound = args\n        msg = undefined\n        scarlett_sound = undefined\n        args = ( , \':1.88\', \'/org/scarlett/Listener\', \'org.scarlett.Listener\', \'SttFailedSignal\', GLib.Variant(\'(ss)\', (\'  ScarlettListener hit Max STT failures\', \'pi-response2\')))\n    102     logger.warning(" msg: {}".format(msg))\n    103     logger.warning(" scarlett_sound: {}".format(scarlett_sound)) \n ValueError: too many values to unpack \n \n /home/pi/dev/bossjones-github/scarlett-dbus-poc/test_gdbus_proxy_service.py(101)player_cb()\n     99     pp = pprint.PrettyPrinter(indent=4)\n    100     pp.pprint(args)\n--> 101     msg, scarlett_sound = args\n    102     logger.warning(" msg: {}".format(msg))\n    103     logger.warning(" scarlett_sound: {}".format(scarlett_sound)) \n \n ipdb>\n``` \n Got signal emit + callback working. player_cb / command_cb \n ```\nService: python test_gdbus_simple_service.py \n Tasker: python test_gdbus_proxy_service.py\n``` \n Got full emit + subscribe [02/21/2016] / No Threads + No Player / Speaker yet \n ```\nService: python test_gdbus_service.py \n Tasker: python test_gdbus_proxy_service.py\n``` \n Beginning threading tasker fixes 3/6/2016 \n Got Proper Player working in generator_player.py [04/17/2016] / Properly using Threading RLocks + Semaphores to open wav files, play, stop, then clean up \n CMD: python generator_player.py \n Beginning work on generator_speaker.py and generator_listener.py [04/17/2016] Threading RLocks + Semaphores \n TODO: \n \n [X] Listener. Good-to-go. \n [ ] Tasker \n [ ] Threading \n [ ] Calls to Player \n [ ] Calls to Speaker \n [ ] Calls to Features \n [ ] Player \n [ ] Speaker \n [ ] Brain \n [ ] Features \n [ ] Forecast \n [ ] GstUtils \n [ ] Timer \n [ ] Wordnik \n [ ] Stocks \n [ ] News \n [ ] Lights \n [ ] Music \n [ ] TV \n [ ] Sound \n [ ] Blinds \n \n Investigating Generators and No-Op functions in pygobject threading - 3/13/2016 \n ```\nService: python test_gdbus_service.py \n Tasker: python FooThreadDbus.py\n```', 'scarlett-ansible \n Audio on ubuntu 16.04 links \n https://help.ubuntu.com/community/SoundTroubleshootingProcedure\nhttp://askubuntu.com/questions/774458/installed-lubuntu-16-04-version-no-audio-now\nhttps://musescore.org/en/node/107601\nhttps://ubuntuforums.org/showthread.php?t=2321631 \n Description \n The purpose of this repository is to provision a server with all required Scarlett dependencies via ansible. When finished, we can then verify that everything is in place correctly by using serverspec as a integration testing framework. This repo will also serve as a brain dump for any installation issues we come across while attempting to get GTK+3, Gstreamer 1.0, PyGi, Pocketsphinx 5prealpha, PulseAudio, Alsa and Python to play nicely together. As you can see from the list of dependices, there are A LOT of things to take into consideration when configuring this system properly ... the fact that I did not create this earlier baffles my mind! \n Hardware Requirements \n \n Microphone: 4 of Kinobo - USB 2.0 Mini Microphone "Makio" Mic for Laptop/Desktop PCs - Skype / VOIP / Voice Recognition Software \n \n ``` \n NOTE this particular mic is not required, it\'s just the one i\'m using, some values are HARDCODED to this. EG, My mic\'s name is "USB PnP Sound Device" \n NOTE THIS IS WRONG \n ± |master U:3 ?:6 ✗| → cat /proc/asound/cards\n 0 [I82801AAICH    ]: ICH - Intel 82801AA-ICH\n                      Intel 82801AA-ICH with AD1980 at irq 21\n 1 [Device         ]: USB-Audio - USB PnP Sound Device\n                      C-Media Electronics Inc. USB PnP Sound Device at usb-0000:00:1f.4-1, full speed \n ``` \n Software Requirements \n NOTE: Everything is currently being tested on a Ubuntu 14.04.3 LTS Virtualbox Machine on my Macbook Pro. \nOn Macbook Pro:\n- Homebrew\n- Ruby ( via rbenv )\n- Ansible (v1.9.4)\n- fswatch ( installed via homebrew ). We will use this to sync files we\'re working on locally. \n Setup serverspec \n Just like how Python recomends the use of virtual environments, we don\'t want to install any rubygems system wide.  serverspec , a ruby gem, can be installed locally in a  .vendor  directory. Simply do the following: \n ``` \n install bundler \n gem install bundler \n install serverspec and other dependencies \n bundle install --path .vendor \n run serverspec tests via rake, similar to make but for ruby \n bundle exec rake\n``` \n How to Provision a Vagrant box \n ``` \n NOTE: I wrapped commands w/ Make mainly due to some weird things I did with this setup. Will clean this up later. \n bring up virtualbox \n make vagrant-up \n provision runs ansible \n make vagrant-provision \n ssh on to server \n make vagrant-ssh \n stop server \n make vagrant-halt\n``` \n How to Provision Scarlett on another machine \n Coming soon... \n Helpful links \n \n http://cmusphinx.sourceforge.net/wiki/installingpythonstuff \n https://github.com/feanil/raspi-video-sync \n https://gist.github.com/GiorgioNatili/ce5647a79ce88cf73a8a \n http://cmusphinx.sourceforge.net/wiki/#advanced_user_documentation \n http://sourceforge.net/p/cmusphinx/discussion/search/?q=Pocketsphinx+listen+for+grammar \n http://sourceforge.net/p/cmusphinx/discussion/help/thread/a7822390/?limit=25#8f38/f5b8/bc40 \n https://wolfpaulus.com/journal/embedded/raspberrypi2-sr/ \n http://blog.scphillips.com/posts/2013/01/getting-gstreamer-to-work-on-a-raspberry-pi/ \n \n https://wiki.debian.org/PulseAudio \n \n \n https://github.com/spk121/burro-engine \n \n http://cmusphinx.sourceforge.net/wiki/download \n http://cmusphinx.sourceforge.net/wiki/faq#qwhy_my_accuracy_is_poor \n https://github.com/cmusphinx/pocketsphinx/issues/25 \n https://blogs.gnome.org/uraeus/2011/10/04/tutorial-for-python-gstreamer-and-gtk-3/ \n https://developer.ridgerun.com/wiki/index.php/GStreamer_Debugging \n https://developer.ridgerun.com/wiki/index.php?title=Embedded_GStreamer_Performance_Tuning \n http://docs.gstreamer.com/display/GstSDK/Basic+tutorial+11%3A+Debugging+tools \n http://wiki.buzztrax.org/index.php/Debugging \n http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer/html/gst-running.html \n http://gstreamer.freedesktop.org/data/doc/gstreamer/head/manual/html/section-checklist-debug.html \n https://gramps-project.org/wiki/index.php?title=GEPS_029:_GTK3-GObject_introspection_Conversion \n https://wiki.archlinux.org/index.php/PulseAudio/Examples \n https://wiki.archlinux.org/index.php/PulseAudio/Configuration \n http://trac.gateworks.com/wiki/Yocto/gstreamer/audio \n http://www.alsa-project.org/alsa-doc/alsa-lib/pcm.html \n https://www.raspberrypi.org/forums/viewtopic.php?f=38&t=37873  set alsa and save settings \n https://github.com/ozzyjohnson/ansible-ffmpeg-build/blob/master/build-ffmpeg.yml  good example of how to compile from source in ansible \n http://cmusphinx.sourceforge.net/wiki/raspberrypi  getting pocketsphinx working on raspbian ( need to add ansible commands for that ) \n https://wolfpaulus.com/journal/embedded/raspberrypi2-sr/  NOTE: lets update steps... see following: \n \n Vagrant commands \n start vagrant server \n VAGRANT_VAGRANTFILE=Vagrantfile-ansible-test vagrant up \n provision server \n VAGRANT_VAGRANTFILE=Vagrantfile-ansible-test vagrant provision \n Make commands \n help\nlist\nserverspec\nserverspec-diff\nserverspec-install\nvagrant-destroy\nvagrant-halt\nvagrant-provision\nvagrant-up \n Speech Recognition w/ pocketsphinx \n Corpus:  See file  ./language_files/base_scarlett.corpus , which contains a list of words/sentences seperated by new lines that we want recognized by pocketsphinx. \n ``` \n Example Corpus \n SCARLETT\nTURN ON THE LIGHTS\nTURN OFF THE LIGHTS\nCHANGE THE LIGHTS BLUE\nCHANGE THE LIGHTS GREEN\nCHANGE THE LIGHTS RED\nCHANGE THE LIGHTS WHITE\nWHATS THE WEATHER\nWHAT TIME IS IT\nCHANGE THE TV TO MTV\nCHANGE THE TV TO HBO\nTURN THE LIGHTS BRIGHTER\nTURN THE LIGHTS DARKER\nSEXY TIME\nCANCEL\nNEGATIVE\nSTOP\nGO\nLEFT\nRIGHT\nUP\nDOWN\nFORWARD\nBACKWARD\nFRIZZY\nTV\nAPPLE\nTOSHIBA\nUP\nDOWN\nLEFT\nRIGHT\nMENU\nPAUSE\nPLAY\nCIRCLE BUTTON\nCHANNEL UP\nCHANNEL DOWN\nVOLUME UP\nVOLUME DOWN\nMUTE\nRECALL\nINPUT\nSELECT UP\nSELECT LEFT\nSELECT RIGHT\nSELECT ENTER\nONE\nTWO\nTHREE\nFOUR\nFIVE\nSIX\nSEVEN\nEIGHT\nNINE\nZERO\nPOWER\nGET LIGHT NAMES\nGET LIGHT IDS\nWHAT ARE THE NAMES OF MY LIGHTS\nGIVE ME LIGHT NAMES\nCLOSE THE SHADES\nTURN ON BATHROOM LIGHTS\nTURN ON WINDOW LIGHTS\nTURN ON FAMILY ROOM LIGHTS\nTURN ON LIGHTS IN BATHROOM\nTURN ON LIGHTS IN FAMILY ROOM\nTURN ON LIGHTS IN HALLWAY\nTURN ON ALL LIGHTS \n ``` \n Generating new .lm and .dict files from your base corpus \n MANUALLY: \nUpload the text file here: http://www.speech.cs.cmu.edu/tools/lmtool-new.html\nand then download the generated Pronunciation Dictionary and Language Model. \n AUTOMATED: \n Simply run  ./generate_language_pocketsphinx.sh . This will automatically upload your corpus to http://www.speech.cs.cmu.edu/tools/lmtool-new.html and download the  .lm  and  .dict  files that get created into  ./language_files/lang/ . Their format will look something like  1473.dic  and  1473.lm . When you finish running the command, you will need to update your  .scarlett  config file. It will also write a file locally called  LANGUAGE_VERSION.txt  which will have an up-to-date # generated by lmtool, eg  1473 . \n For the the text file mentioned above, this is what the tool generates: \n 1473.lm: \n ALL AO L\nAPPLE AE P AH L\nARE AA R\nARE(2)  ER\nBACKWARD  B AE K W ER D\nBATHROOM  B AE TH R UW M\nBLUE  B L UW\nBRIGHTER  B R AY T ER\nBUTTON  B AH T AH N\nCANCEL  K AE N S AH L\nCHANGE  CH EY N JH\nCHANNEL CH AE N AH L\nCIRCLE  S ER K AH L\nCLOSE K L OW S\nCLOSE(2)  K L OW Z\nDARKER  D AA R K ER\nDOWN  D AW N\nEIGHT EY T\nENTER EH N T ER\nENTER(2)  EH N ER\nFAMILY  F AE M AH L IY\nFAMILY(2) F AE M L IY\nFIVE  F AY V\nFORWARD F AO R W ER D\nFOUR  F AO R\nFRIZZY  F R IH Z IY\nGET G EH T\nGET(2)  G IH T\nGIVE  G IH V\nGO  G OW\nGREEN G R IY N\nHALLWAY HH AO L W EY\nHBO EY CH B IY OW\nIDS AY D IY Z\nIDS(2)  IH D Z\nIN  IH N\nINPUT IH N P UH T\nIS  IH Z\nIT  IH T\nLEFT  L EH F T\nLIGHT L AY T\nLIGHTS  L AY T S\nME  M IY\nMENU  M EH N Y UW\nMTV EH M T IY V IY\nMUTE  M Y UW T\nMY  M AY\nNAMES N EY M Z\nNEGATIVE  N EH G AH T IH V\nNINE  N AY N\nOF  AH V\nOFF AO F\nON  AA N\nON(2) AO N\nONE W AH N\nONE(2)  HH W AH N\nPAUSE P AO Z\nPLAY  P L EY\nPOWER P AW ER\nRECALL  R IY K AO L\nRECALL(2) R IH K AO L\nRED R EH D\nRIGHT R AY T\nROOM  R UW M\nSCARLETT  S K AA R L IH T\nSELECT  S AH L EH K T\nSEVEN S EH V AH N\nSEXY  S EH K S IY\nSHADES  SH EY D Z\nSIX S IH K S\nSTOP  S T AA P\nTHE DH AH\nTHE(2)  DH IY\nTHREE TH R IY\nTIME  T AY M\nTO  T UW\nTO(2) T IH\nTO(3) T AH\nTOSHIBA T OW SH IY B AH\nTURN  T ER N\nTV  T IY V IY\nTV(2) T EH L AH V IH ZH AH N\nTWO T UW\nUP  AH P\nVOLUME  V AA L Y UW M\nWEATHER W EH DH ER\nWHAT  W AH T\nWHAT(2) HH W AH T\nWHATS W AH T S\nWHATS(2)  HH W AH T S\nWHITE W AY T\nWHITE(2)  HH W AY T\nWINDOW  W IH N D OW\nZERO  Z IY R OW \n language model: \n ```\nLanguage model created by QuickLM on Sat Jan  2 12:51:32 EST 2016\nCopyright (c) 1996-2010 Carnegie Mellon University and Alexander I. Rudnicky \n The model is in standard ARPA format, designed by Doug Paul while he was at MITRE. \n The code that was used to produce this language model is available in Open Source.\nPlease visit http://www.speech.cs.cmu.edu/tools/ for more information \n The (fixed) discount mass is 0.5. The backoffs are computed using the ratio method.\nThis model based on a corpus of 70 sentences and 80 words \n \\data\\\nngram 1=80\nngram 2=155\nngram 3=128 \n \\1-grams:\n-0.9301  -0.3010\n-0.9301   -0.2393\n-2.7752 ALL -0.2892\n-2.7752 APPLE -0.2468\n-2.7752 ARE -0.2915\n-2.7752 BACKWARD -0.2468\n-2.4742 BATHROOM -0.2334\n-2.7752 BLUE -0.2468\n-2.7752 BRIGHTER -0.2468\n-2.7752 BUTTON -0.2468\n-2.7752 CANCEL -0.2468\n-1.9971 CHANGE -0.2915\n-2.4742 CHANNEL -0.2944\n-2.7752 CIRCLE -0.3003\n-2.7752 CLOSE -0.2915\n-2.7752 DARKER -0.2468\n-2.1732 DOWN -0.2468\n-2.7752 EIGHT -0.2468\n-2.7752 ENTER -0.2468\n-2.4742 FAMILY -0.2996\n-2.7752 FIVE -0.2468\n-2.7752 FORWARD -0.2468\n-2.7752 FOUR -0.2468\n-2.7752 FRIZZY -0.2468\n-2.4742 GET -0.2988\n-2.7752 GIVE -0.3003\n-2.7752 GO -0.2468\n-2.7752 GREEN -0.2468\n-2.7752 HALLWAY -0.2468\n-2.7752 HBO -0.2468\n-2.7752 IDS -0.2468\n-2.2981 IN -0.2974\n-2.7752 INPUT -0.2468\n-2.7752 IS -0.3003\n-2.7752 IT -0.2468\n-2.2981 LEFT -0.2468\n-2.2981 LIGHT -0.2981\n-1.5711 LIGHTS -0.2393\n-2.7752 ME -0.2988\n-2.7752 MENU -0.2468\n-2.7752 MTV -0.2468\n-2.7752 MUTE -0.2468\n-2.7752 MY -0.2892\n-2.2981 NAMES -0.2459\n-2.7752 NEGATIVE -0.2468\n-2.7752 NINE -0.2468\n-2.7752 OF -0.3003\n-2.7752 OFF -0.2915\n-1.8722 ON -0.2747\n-2.7752 ONE -0.2468\n-2.7752 PAUSE -0.2468\n-2.7752 PLAY -0.2468\n-2.7752 POWER -0.2468\n-2.7752 RECALL -0.2468\n-2.7752 RED -0.2468\n-2.2981 RIGHT -0.2468\n-2.4742 ROOM -0.2334\n-2.7752 SCARLETT -0.2468\n-2.1732 SELECT -0.2922\n-2.7752 SEVEN -0.2468\n-2.7752 SEXY -0.2996\n-2.7752 SHADES -0.2468\n-2.7752 SIX -0.2468\n-2.7752 STOP -0.2468\n-1.6613 THE -0.2832\n-2.7752 THREE -0.2468\n-2.4742 TIME -0.2459\n-2.4742 TO -0.2996\n-2.7752 TOSHIBA -0.2468\n-1.7339 TURN -0.2847\n-2.2981 TV -0.2451\n-2.7752 TWO -0.2468\n-2.0763 UP -0.2468\n-2.4742 VOLUME -0.2944\n-2.7752 WEATHER -0.2468\n-2.4742 WHAT -0.2988\n-2.7752 WHATS -0.2915\n-2.7752 WHITE -0.2468\n-2.7752 WINDOW -0.2892\n-2.7752 ZERO -0.2468 \n \\2-grams:\n-2.1461   APPLE 0.0000\n-2.1461   BACKWARD 0.0000\n-2.1461   CANCEL 0.0000\n-1.3680   CHANGE 0.0000\n-1.8451   CHANNEL 0.0000\n-2.1461   CIRCLE 0.0000\n-2.1461   CLOSE 0.0000\n-1.8451   DOWN 0.0000\n-2.1461   EIGHT 0.0000\n-2.1461   FIVE 0.0000\n-2.1461   FORWARD 0.0000\n-2.1461   FOUR 0.0000\n-2.1461   FRIZZY 0.0000\n-1.8451   GET 0.0000\n-2.1461   GIVE 0.0000\n-2.1461   GO 0.0000\n-2.1461   INPUT 0.0000\n-1.8451   LEFT 0.0000\n-2.1461   MENU 0.0000\n-2.1461   MUTE 0.0000\n-2.1461   NEGATIVE 0.0000\n-2.1461   NINE 0.0000\n-2.1461   ONE 0.0000\n-2.1461   PAUSE 0.0000\n-2.1461   PLAY 0.0000\n-2.1461   POWER 0.0000\n-2.1461   RECALL 0.0000\n-1.8451   RIGHT 0.0000\n-2.1461   SCARLETT 0.0000\n-1.5441   SELECT 0.0000\n-2.1461   SEVEN 0.0000\n-2.1461   SEXY 0.0000\n-2.1461   SIX 0.0000\n-2.1461   STOP 0.0000\n-2.1461   THREE 0.0000\n-2.1461   TOSHIBA 0.0000\n-1.1047   TURN 0.0000\n-2.1461   TV -0.2218\n-2.1461   TWO 0.0000\n-1.8451   UP 0.0000\n-1.8451   VOLUME 0.0000\n-1.8451   WHAT 0.0000\n-2.1461   WHATS 0.0000\n-2.1461   ZERO 0.0000\n-0.3010 ALL LIGHTS -0.1938\n-0.3010 APPLE   -0.3010\n-0.3010 ARE THE -0.2840\n-0.3010 BACKWARD   -0.3010\n-0.6021 BATHROOM   -0.3010\n-0.6021 BATHROOM LIGHTS -0.1938\n-0.3010 BLUE   -0.3010\n-0.3010 BRIGHTER   -0.3010\n-0.3010 BUTTON   -0.3010\n-0.3010 CANCEL   -0.3010\n-0.3010 CHANGE THE -0.0902\n-0.6021 CHANNEL DOWN 0.0000\n-0.6021 CHANNEL UP 0.0000\n-0.3010 CIRCLE BUTTON 0.0000\n-0.3010 CLOSE THE -0.2840\n-0.3010 DARKER   -0.3010\n-0.3010 DOWN   -0.3010\n-0.3010 EIGHT   -0.3010\n-0.3010 ENTER   -0.3010\n-0.3010 FAMILY ROOM 0.0000\n-0.3010 FIVE   -0.3010\n-0.3010 FORWARD   -0.3010\n-0.3010 FOUR   -0.3010\n-0.3010 FRIZZY   -0.3010\n-0.3010 GET LIGHT 0.0000\n-0.3010 GIVE ME 0.0000\n-0.3010 GO   -0.3010\n-0.3010 GREEN   -0.3010\n-0.3010 HALLWAY   -0.3010\n-0.3010 HBO   -0.3010\n-0.3010 IDS   -0.3010\n-0.7782 IN BATHROOM -0.1761\n-0.7782 IN FAMILY 0.0000\n-0.7782 IN HALLWAY 0.0000\n-0.3010 INPUT   -0.3010\n-0.3010 IS IT 0.0000\n-0.3010 IT   -0.3010\n-0.3010 LEFT   -0.3010\n-0.7782 LIGHT IDS 0.0000\n-0.4771 LIGHT NAMES -0.1249\n-0.6601 LIGHTS   -0.3010\n-1.5051 LIGHTS BLUE 0.0000\n-1.5051 LIGHTS BRIGHTER 0.0000\n-1.5051 LIGHTS DARKER 0.0000\n-1.5051 LIGHTS GREEN 0.0000\n-1.0280 LIGHTS IN 0.0000\n-1.5051 LIGHTS RED 0.0000\n-1.5051 LIGHTS WHITE 0.0000\n-0.3010 ME LIGHT -0.1249\n-0.3010 MENU   -0.3010\n-0.3010 MTV   -0.3010\n-0.3010 MUTE   -0.3010\n-0.3010 MY LIGHTS -0.1938\n-0.4771 NAMES   -0.3010\n-0.7782 NAMES OF 0.0000\n-0.3010 NEGATIVE   -0.3010\n-0.3010 NINE   -0.3010\n-0.3010 OF MY 0.0000\n-0.3010 OFF THE -0.1413\n-1.2041 ON ALL 0.0000\n-1.2041 ON BATHROOM -0.1761\n-1.2041 ON FAMILY 0.0000\n-0.7270 ON LIGHTS -0.2583\n-1.2041 ON THE -0.1413\n-1.2041 ON WINDOW 0.0000\n-0.3010 ONE   -0.3010\n-0.3010 PAUSE   -0.3010\n-0.3010 PLAY   -0.3010\n-0.3010 POWER   -0.3010\n-0.3010 RECALL   -0.3010\n-0.3010 RED   -0.3010\n-0.3010 RIGHT   -0.3010\n-0.6021 ROOM   -0.3010\n-0.6021 ROOM LIGHTS -0.1938\n-0.3010 SCARLETT   -0.3010\n-0.9031 SELECT ENTER 0.0000\n-0.9031 SELECT LEFT 0.0000\n-0.9031 SELECT RIGHT 0.0000\n-0.9031 SELECT UP 0.0000\n-0.3010 SEVEN   -0.3010\n-0.3010 SEXY TIME -0.1761\n-0.3010 SHADES   -0.3010\n-0.3010 SIX   -0.3010\n-0.3010 STOP   -0.3010\n-0.5119 THE LIGHTS -0.0746\n-1.4150 THE NAMES -0.2218\n-1.4150 THE SHADES 0.0000\n-1.1139 THE TV -0.1249\n-1.4150 THE WEATHER 0.0000\n-0.3010 THREE   -0.3010\n-0.6021 TIME  -0.3010\n-0.6021 TIME IS 0.0000\n-0.6021 TO HBO 0.0000\n-0.6021 TO MTV 0.0000\n-0.3010 TOSHIBA  -0.3010\n-1.3424 TURN OFF 0.0000\n-0.4393 TURN ON 0.0000\n-1.0414 TURN THE -0.1413\n-0.7782 TV  -0.3010\n-0.4771 TV TO 0.0000\n-0.3010 TWO  -0.3010\n-0.3010 UP  -0.3010\n-0.6021 VOLUME DOWN 0.0000\n-0.6021 VOLUME UP 0.0000\n-0.3010 WEATHER  -0.3010\n-0.6021 WHAT ARE 0.0000\n-0.6021 WHAT TIME -0.1761\n-0.3010 WHATS THE -0.2840\n-0.3010 WHITE  -0.3010\n-0.3010 WINDOW LIGHTS -0.1938\n-0.3010 ZERO  -0.3010 \n \\3-grams:\n-0.3010   APPLE  \n-0.3010   BACKWARD  \n-0.3010   CANCEL  \n-0.3010   CHANGE THE\n-0.6021   CHANNEL DOWN\n-0.6021   CHANNEL UP\n-0.3010   CIRCLE BUTTON\n-0.3010   CLOSE THE\n-0.3010   DOWN  \n-0.3010   EIGHT  \n-0.3010   FIVE  \n-0.3010   FORWARD  \n-0.3010   FOUR  \n-0.3010   FRIZZY  \n-0.3010   GET LIGHT\n-0.3010   GIVE ME\n-0.3010   GO  \n-0.3010   INPUT  \n-0.3010   LEFT  \n-0.3010   MENU  \n-0.3010   MUTE  \n-0.3010   NEGATIVE  \n-0.3010   NINE  \n-0.3010   ONE  \n-0.3010   PAUSE  \n-0.3010   PLAY  \n-0.3010   POWER  \n-0.3010   RECALL  \n-0.3010   RIGHT  \n-0.3010   SCARLETT  \n-0.9031   SELECT ENTER\n-0.9031   SELECT LEFT\n-0.9031   SELECT RIGHT\n-0.9031   SELECT UP\n-0.3010   SEVEN  \n-0.3010   SEXY TIME\n-0.3010   SIX  \n-0.3010   STOP  \n-0.3010   THREE  \n-0.3010   TOSHIBA  \n-1.3424   TURN OFF\n-0.4393   TURN ON\n-1.0414   TURN THE\n-0.3010   TV  \n-0.3010   TWO  \n-0.3010   UP  \n-0.6021   VOLUME DOWN\n-0.6021   VOLUME UP\n-0.6021   WHAT ARE\n-0.6021   WHAT TIME\n-0.3010   WHATS THE\n-0.3010   ZERO  \n-0.3010 ALL LIGHTS  \n-0.3010 ARE THE NAMES\n-0.3010 BATHROOM LIGHTS  \n-0.4771 CHANGE THE LIGHTS\n-0.7782 CHANGE THE TV\n-0.3010 CHANNEL DOWN  \n-0.3010 CHANNEL UP  \n-0.3010 CIRCLE BUTTON  \n-0.3010 CLOSE THE SHADES\n-0.6021 FAMILY ROOM  \n-0.6021 FAMILY ROOM LIGHTS\n-0.6021 GET LIGHT IDS\n-0.6021 GET LIGHT NAMES\n-0.3010 GIVE ME LIGHT\n-0.3010 IN BATHROOM  \n-0.3010 IN FAMILY ROOM\n-0.3010 IN HALLWAY  \n-0.3010 IS IT  \n-0.3010 LIGHT IDS  \n-0.3010 LIGHT NAMES  \n-0.3010 LIGHTS BLUE  \n-0.3010 LIGHTS BRIGHTER  \n-0.3010 LIGHTS DARKER  \n-0.3010 LIGHTS GREEN  \n-0.7782 LIGHTS IN BATHROOM\n-0.7782 LIGHTS IN FAMILY\n-0.7782 LIGHTS IN HALLWAY\n-0.3010 LIGHTS RED  \n-0.3010 LIGHTS WHITE  \n-0.3010 ME LIGHT NAMES\n-0.3010 MY LIGHTS  \n-0.3010 NAMES OF MY\n-0.3010 OF MY LIGHTS\n-0.3010 OFF THE LIGHTS\n-0.3010 ON ALL LIGHTS\n-0.3010 ON BATHROOM LIGHTS\n-0.3010 ON FAMILY ROOM\n-0.3010 ON LIGHTS IN\n-0.3010 ON THE LIGHTS\n-0.3010 ON WINDOW LIGHTS\n-0.3010 ROOM LIGHTS  \n-0.3010 SELECT ENTER  \n-0.3010 SELECT LEFT \n-0.3010 SELECT RIGHT \n-0.3010 SELECT UP \n-0.3010 SEXY TIME \n-0.9031 THE LIGHTS \n-1.2041 THE LIGHTS BLUE\n-1.2041 THE LIGHTS BRIGHTER\n-1.2041 THE LIGHTS DARKER\n-1.2041 THE LIGHTS GREEN\n-1.2041 THE LIGHTS RED\n-1.2041 THE LIGHTS WHITE\n-0.3010 THE NAMES OF\n-0.3010 THE SHADES \n-0.3010 THE TV TO\n-0.3010 THE WEATHER \n-0.3010 TIME IS IT\n-0.3010 TO HBO \n-0.3010 TO MTV \n-0.3010 TURN OFF THE\n-1.2041 TURN ON ALL\n-1.2041 TURN ON BATHROOM\n-1.2041 TURN ON FAMILY\n-0.7270 TURN ON LIGHTS\n-1.2041 TURN ON THE\n-1.2041 TURN ON WINDOW\n-0.3010 TURN THE LIGHTS\n-0.6021 TV TO HBO\n-0.6021 TV TO MTV\n-0.3010 VOLUME DOWN \n-0.3010 VOLUME UP \n-0.3010 WHAT ARE THE\n-0.3010 WHAT TIME IS\n-0.3010 WHATS THE WEATHER\n-0.3010 WINDOW LIGHTS  \n \\end\\\n``` \n Test out pocketsphinx_continuous after compiling \n This video shows the recognizer running in keyword spotting mode, using the dictionary and model mentioned above: \n pocketsphinx_continuous -lm /home/pi/dev/bossjones-github/scarlett/static/speech/lm/1602.lm -dict /home/pi/dev/bossjones-github/scarlett/static/speech/dict/1602.dic -keyphrase "SCARLETT" -kws_threshold 1e-20 -inmic yes \n The purpose is to provide some indication of the recognition speed that can be expected, running PocketSphinx on the Raspberry Pi 2. \n Good question from ColinW re: Alsa \n ColinW  on November 14, 2015 at 8:56 pm\nGreat article! Question on how to change the card index on the new Raspbian Jessie.\nIn Jessie, there is no longer “alsa-base.conf” to set “options snd-usb-audio index=0”.\nAs best as I can figure, the new config file is now at “/usr/share/alsa/alsa.conf”. Do you know how I would be able to do it there?\nAlternatively, is there a way to make PocketSphinx use the mic at index 1? \n Is this bug still valid? \n- https://bugs.tizen.org/jira/browse/TC-664 \n NOTE: \n If there is ever concern about env variables messing up how scarlett runs, make sure these are either set/unset, which are defined in postactivate and postdeactivate: \n export GSTREAMER=1.0\nexport PI_HOME=/home/pi\nexport MAIN_DIR=$PI_HOME/dev/bossjones-github/scarlett-dbus-poc\nexport VIRT_ROOT=$PI_HOME/.virtualenvs/scarlett-dbus-poc\nexport PKG_CONFIG_PATH=$VIRT_ROOT/lib/pkgconfig\nexport SCARLETT_CONFIG=$MAIN_DIR/tests/fixtures/.scarlett\nexport SCARLETT_HMM=$MAIN_DIR/tests/fixtures/model/hmm/en_US/hub4wsj_sc_8k\nexport SCARLETT_LM=$MAIN_DIR/tests/fixtures/lm/1602.lm\nexport SCARLETT_DICT=$MAIN_DIR/tests/fixtures/dict/1602.dic \n LINKS TO HELP WITH TESTING YOUR MICROPHONE \n \n https://www.onlinemictest.com/ \n http://kinobo.co.uk/support/index.php?sid=126171&lang=en&action=artikel&cat=3&id=12&artlang=en \n http://kinobo.co.uk/support/index.php?action=artikel&cat=4&id=18&artlang=en \n http://kinobo.co.uk/support/index.php?action=artikel&cat=4&id=14&artlang=en \n http://www.kinobo.co.uk/# \n https://wiki.archlinux.org/index.php/PulseAudio/Configuration \n http://apple.stackexchange.com/questions/170105/list-usb-devices-on-osx-command-line \n http://www.alsa-project.org/main/index.php/Asoundrc \n \n VIRTUALBOX make sure guest can see usb mics ( MAC OS X ONLY) \n Run this on your mac:  ioreg -p IOUSB -w0 -l \n This will produce something like: \n ○ → ioreg -p IOUSB -w0 | sed \'s/[^o]*o //; s/@.*$//\' | grep -v \'^Root.*\'\nXHCI Root Hub SS Simulation\nXHCI Root Hub USB 2.0 Simulation\nUSB PnP Sound Device\nEHCI Root Hub Simulation\nHubDevice\nFaceTime HD Camera (Built-in)\nEHCI Root Hub Simulation\nHubDevice\nHubDevice\nApple Internal Keyboard / Trackpad\nBRCM20702 Hub\nBluetooth USB Host Controller \n DEBUGGING Gstreamer and PulseAudio \n Figure out if Gstreamer is configued to use pulseaudio: \n```\n± |master U:3 ?:3 ✗| → gconftool-2 --recursive-list /system/gstreamer\n /system/gstreamer/0.10:\n  /system/gstreamer/0.10/default:\n   musicaudiosink_description = Default\n   audiosrc = autoaudiosrc\n   audiosrc_description = Default\n   chataudiosink_description = Default\n   musicaudiosink = autoaudiosink\n   audiosink_description = Default\n   visualization = goom\n   videosrc = v4l2src\n   audiosink = autoaudiosink\n   chataudiosink = autoaudiosink\n   videosink = autovideosink\n /system/gstreamer/1.0:\n  /system/gstreamer/1.0/default:\n   audiosink = pulsesink\n   audiosrc = pulsesrc\n  /system/gstreamer/1.0/audio:\n   /system/gstreamer/1.0/audio/profiles:\n    /system/gstreamer/1.0/audio/profiles/mp3:\n     name = CD Quality, MP3\n     extension = mp3\n     pipeline = audio/x-raw,rate=44100,channels=2 ! lamemp3enc name=enc target=0 quality=6 ! xingmux ! id3v2mux\n     description = Used for converting to CD-quality audio, but with the lossy MP3 codec. Use this for preparing files for copying to devices that only support the MP3 codec. Note that using this format may be illegal in your jurisdiction; contact your lawyer for advice.\n     active = true\n    /system/gstreamer/1.0/audio/profiles/cdlossless:\n     name = CD Quality, Lossless\n     extension = flac\n     pipeline = audio/x-raw,rate=44100,channels=2 ! flacenc name=enc\n     description = Used for converting to CD-quality audio, but with a lossless compression codec. Use this if you later want to edit the file or burn it to CD.\n     active = true\n    /system/gstreamer/1.0/audio/profiles/mp2:\n     name = CD Quality, MP2\n     extension = mp2\n     pipeline = audio/x-raw,rate=44100,channels=2 ! twolame name=enc mode=0 bitrate=192 ! id3v2mux\n     description = Used for converting to CD-quality audio, but with the lossy MP2 codec. Use this for preparing files for copying to devices that only support the MP2 codec. Note that using this format may be illegal in your jurisdiction; contact your lawyer for advice.\n     active = true\n    /system/gstreamer/1.0/audio/profiles/aac:\n     name = CD Quality, AAC\n     extension = m4a\n     pipeline = audio/x-raw,rate=44100,channels=2 ! faac profile=2 ! ffmux_mp4\n     description = Used for converting to CD-quality audio, but with the lossy AAC codec. Use this for preparing files for copying to devices that only support the AAC codec. Note that using this format may be illegal in your jurisdiction; contact your lawyer for advice.\n     active = true\n    /system/gstreamer/1.0/audio/profiles/voicelossless:\n     name = Voice, Lossless\n     extension = wav\n     pipeline = audio/x-raw,rate=22050,channels=1 ! wavenc name=enc\n     description = Used for converting to lossless voice-quality audio. Use this for recording and editing speech.\n     active = true\n    /system/gstreamer/1.0/audio/profiles/voicelossy:\n     name = Voice, Lossy\n     extension = spx\n     pipeline = audio/x-raw,rate=32000,channels=1 ! speexenc name=enc ! oggmux\n     description = Used for converting to lossy voice-quality audio. Use this for recording speech that doesn\'t need to be edited.\n     active = true\n    /system/gstreamer/1.0/audio/profiles/cdlossy:\n     name = CD Quality, Lossy\n     extension = ogg\n     pipeline = audio/x-raw,rate=44100,channels=2 ! vorbisenc name=enc quality=0.5 ! oggmux\n     description = Used for converting to CD-quality audio, but with a lossy compression codec. Use this for CD extraction and radio recordings.\n     active = true\n   /system/gstreamer/1.0/audio/global:\n    profile_list = [cdlossy,cdlossless,aac,mp2,mp3,voicelossy,voicelossless] \n using virtualenv: scarlett-dbus-poc scarlett-ansible in ~/dev/bossjones-github/scarlett-gstreamer-pocketsphinx-demo\n± |master U:3 ?:3 ✗| →\n``` \n source:  https://twitter.com/andolamin/status/661050200614502400 \n gconftool-2 -t string --set /system/gstreamer/1.0/default/audiosink pulsesink \n ``` \n modified with 1.0 \n gconftool-2 -t string --set /system/gstreamer/1.0/default/audiosink pulsesink\ngconftool-2 -t string --set /system/gstreamer/1.0/default/audiosrc pulsesrc\ngconftool-2 -t string --set /system/gstreamer/1.0/default/musicaudiosink pulsesink \n source: http://blog.scphillips.com/posts/2013/01/getting-gstreamer-to-work-on-a-raspberry-pi/ \n check the keys are there: \n ± |master U:3 ?:3 ✗| → gconftool-2 -a /system/gstreamer/1.0/default\n musicaudiosink = pulsesink\n audiosink = pulsesink\n audiosrc = pulsesrc\n``` \n ``` \n OLD WAY OF DOING THIS gstreamer 0.10 \n gconftool -t string --set /system/gstreamer/0.10/default/audiosink pulsesink\ngconftool -t string --set /system/gstreamer/0.10/default/audiosrc pulsesrc\n``` \n Source: \n- http://www.alsa-project.org/main/index.php/Asoundrc \n The numbers after hw: stand for the soundcard number and device number. This can get confusing as some sound "cards" are better represented by calling them sound "devices", for example USB sounddevices. However they are still "cards" in the sense that they have a specific driver controlling a specific piece of hardware. They also correspond to the index shown in\n/proc/asound/cards\nAs with most arrays the first item usually starts at 0 not 1. This is true for the way pcm devices (physical I/O channels) are represented in ALSA. Starting at pcm0c (capture), pcm0p (playback).\nWe use subdevices mainly for hardware which can mix several streams together. It is impractical to have 32 devices with exactly the same capabilities. The subdevices can be opened without a specific address, so the first free subdevice is opened. Also, we temporarily use subdevices for hardware with a lot of streams (I/O connectors) — for example MIDI. There are several limits given by used minor numbers (8 PCM devices per card, 8 MIDI devices per card etc.).\nFor example, to access the first device on the first soundcard/device, you would use\nhw:0,0\nto access the first device on the second soundcard/device, you would use\nhw:1,0\nto access the second device on the third soundcard/device, you would use\nhw:2,1 \n Generic bash magic: \n echo "libpulse-dev\nlibpulse-mainloop-glib0\nlibpulse-mainloop-glib0-dbg\nlibpulse0\nlibpulse0-dbg\nlibsox-fmt-pulse\npaman\npaprefs\npavucontrol\npavumeter\npulseaudio\npulseaudio-dbg\npulseaudio-esound-compat\npulseaudio-esound-compat-dbg\npulseaudio-module-bluetooth\npulseaudio-module-gconf\npulseaudio-module-jack\npulseaudio-module-lirc\npulseaudio-module-lirc-dbg\npulseaudio-module-x11\npulseaudio-module-zeroconf\npulseaudio-module-zeroconf-dbg\npulseaudio-utils\npavucontrol\navahi-daemon\nlibtheora-dev\nlibogg-dev\nlibvorbis-dev\nlibasound2-dev\nlibjack-dev" | sed \'s,^,",g\' | sed \'s,$,",g\' \n ``` \n COMMANDS THAT FINALLY LET ME RECORD W/ PULSE AUDIO FROM COMMANDLINE \n $ pacmd "load-module module-alsa-source source_name=input device=hw:1" \n $ pacmd "set-default-source input" \n $ pactl stat \n Currently in use: 1 blocks containing 63.9 KiB bytes total. \n Allocated during whole lifetime: 46233 blocks containing 74.5 MiB bytes total. \n Sample cache size: 0 B \n Server String: /run/user/1000/pulse/native \n Library Protocol Version: 28 \n Server Protocol Version: 28 \n Is Local: yes \n Client Index: 30 \n Tile Size: 65472 \n User Name: pi \n Host Name: scarlett-ansible \n Server Name: pulseaudio \n Server Version: 4.0 \n Default Sample Specification: s16le 2ch 44100Hz \n Default Channel Map: front-left,front-right \n Default Sink: alsa_output.pci-0000_00_05.0.analog-stereo \n Default Source: input \n Cookie: 008c:872c \n $ parecord -d input outfile.wav \n $ aplay outfile.wav \n IMPORTANT NOTE: IF DEFAULT SOURCE ISNT SET TO HW:1, THIS DOES NOT WORK! \n ``` \n blog post notes \n ```\nUPGRADING FROM POCKETSPHINX 0.8 -> 5prealpha \n what is alsa \n what is pulse audio \n why do we need to configure both \n setting up alsa \n setting up pulseaudio \n other sound frameworks ( Gstreamer ) + how to configure it \n testing alsa works correctly w/ arecord and aplay \n testing pulseaduio works with parecord and paplay \n pocketsphinx install \n finally running pocketsphinx_continuous \n TERMS \n PCM - Pulse-code modulation (PCM) is a method used to digitally represent sampled analog signals. It is the standard form of digital audio in computers, Compact Discs, digital telephony and other digital audio applications.\n``` \n set usb to default sound device \n ``` \n \n \n To configure the adapter to be card 0, I changed the following line in /etc/modprobe.d/alsa-base.conf from\noptions snd-usb-audio index=-2\nto\noptions snd-usb-audio index=0 \n \n \n REBOOT \n \n \n with /proc/asound/cards now showing the adapter as the default (card 0), I\'m able to record audio from the mic input with "arecord -d 5 -r 48000 test.wav" and play it back through the headset output with "aplay test.wav." I used alsamixer to adjust the Speaker output level and the Capture input level. \n \n \n NOTE THIS IS WRONG \n ± |master U:3 ?:6 ✗| → cat /proc/asound/cards\n 0 [I82801AAICH    ]: ICH - Intel 82801AA-ICH\n                      Intel 82801AA-ICH with AD1980 at irq 21\n 1 [Device         ]: USB-Audio - USB PnP Sound Device\n                      C-Media Electronics Inc. USB PnP Sound Device at usb-0000:00:1f.4-1, full speed \n NOTE THIS IS RIGHT \n ± |master U:3 ?:6 ✗| → cat /proc/asound/cards\n 0 [Device         ]: USB-Audio - USB PnP Sound Device\n                      C-Media Electronics Inc. USB PnP Sound Device at usb-0000:00:1f.4-1, full speed\n 1 [I82801AAICH    ]: ICH - Intel 82801AA-ICH\n                      Intel 82801AA-ICH with AD1980 at irq 21\n``` \n GSTREAMER 1.0 + POCKETSPHINX 5prealpha WORKING IN a gst-launch-1.0 pipeline \n ```\n  using virtualenv: scarlett-dbus-poc scarlett-ansible in ~/dev/bossjones-github/scarlett-gstreamer-pocketsphinx-demo\n± |master U:3 ?:12 ✗| → gst-launch-1.0 -m alsasrc device=plughw:CARD=Device,DEV=0 ! \\ \n \n queue leaky=2 ! \\\naudioconvert ! \\\naudioresample ! \\\n"audio/x-raw,format=S16LE,channels=1,layout=interleaved" ! \\\npocketsphinx name=asr \\\nbestpath=0 \\\nhmm=/home/pi/.virtualenvs/scarlett-dbus-poc/share/pocketsphinx/model/en-us/en-us \\\nlm=~/dev/bossjones-github/scarlett-gstreamer-pocketsphinx-demo/1473.lm \\\ndict=~/dev/bossjones-github/scarlett-gstreamer-pocketsphinx-demo/1473.dic ! \\\nqueue leaky=2 ! \\\nfakesink\n \n Setting pipeline to PAUSED ...\nCurrent configuration:\n[NAME]      [DEFLT]   [VALUE]\n-agc      none    none\n-agcthresh    2.0   2.000000e+00\n-allphone\n-allphone_ci    no    no\n-alpha      0.97    9.700000e-01\n-ascale     20.0    2.000000e+01\n-aw     1   1\n-backtrace    no    no\n-beam     1e-48   1.000000e-48\n-bestpath   yes   no\n-bestpathlw   9.5   9.500000e+00\n-ceplen     13    13\n-cmn      current   current\n-cmninit    8.0   40,3,-1\n-compallsen   no    no\n-debug          0\n-dict         /home/pi/dev/bossjones-github/scarlett-gstreamer-pocketsphinx-demo/1473.dic\n-dictcase   no    no\n-dither     no    no\n-doublebw   no    no\n-ds     1   1\n-fdict\n-feat     1s_c_d_dd 1s_c_d_dd\n-featparams\n-fillprob   1e-8    1.000000e-08\n-frate      100   100\n-fsg\n-fsgusealtpron    yes   yes\n-fsgusefiller   yes   yes\n-fwdflat    yes   yes\n-fwdflatbeam    1e-64   1.000000e-64\n-fwdflatefwid   4   4\n-fwdflatlw    8.5   8.500000e+00\n-fwdflatsfwin   25    25\n-fwdflatwbeam   7e-29   7.000000e-29\n-fwdtree    yes   yes\n-hmm          /home/pi/.virtualenvs/scarlett-dbus-poc/share/pocketsphinx/model/en-us/en-us\n-input_endian   little    little\n-jsgf\n-keyphrase\n-kws\n-kws_delay    10    10\n-kws_plp    1e-1    1.000000e-01\n-kws_threshold    1   1.000000e+00\n-latsize    5000    5000\n-lda\n-ldadim     0   0\n-lifter     0   22\n-lm         /home/pi/dev/bossjones-github/scarlett-gstreamer-pocketsphinx-demo/1473.lm\n-lmctl\n-lmname\n-logbase    1.0001    1.000100e+00\n-logfn\n-logspec    no    no\n-lowerf     133.33334 1.300000e+02\n-lpbeam     1e-40   1.000000e-40\n-lponlybeam   7e-29   7.000000e-29\n-lw     6.5   6.500000e+00\n-maxhmmpf   30000   30000\n-maxwpf     -1    -1\n-mdef\n-mean\n-mfclogdir\n-min_endfr    0   0\n-mixw\n-mixwfloor    0.0000001 1.000000e-07\n-mllr\n-mmap     yes   yes\n-ncep     13    13\n-nfft     512   512\n-nfilt      40    25\n-nwpen      1.0   1.000000e+00\n-pbeam      1e-48   1.000000e-48\n-pip      1.0   1.000000e+00\n-pl_beam    1e-10   1.000000e-10\n-pl_pbeam   1e-10   1.000000e-10\n-pl_pip     1.0   1.000000e+00\n-pl_weight    3.0   3.000000e+00\n-pl_window    5   5\n-rawlogdir\n-remove_dc    no    no\n-remove_noise   yes   yes\n-remove_silence   yes   yes\n-round_filters    yes   yes\n-samprate   16000   1.600000e+04\n-seed     -1    -1\n-sendump\n-senlogdir\n-senmgau\n-silprob    0.005   5.000000e-03\n-smoothspec   no    no\n-svspec         0-12/13-25/26-38\n-tmat\n-tmatfloor    0.0001    1.000000e-04\n-topn     4   4\n-topn_beam    0   0\n-toprule\n-transform    legacy    dct\n-unit_area    yes   yes\n-upperf     6855.4976 6.800000e+03\n-uw     1.0   1.000000e+00\n-vad_postspeech   50    50\n-vad_prespeech    20    20\n-vad_startspeech  10    10\n-vad_threshold    2.0   2.000000e+00\n-var\n-varfloor   0.0001    1.000000e-04\n-varnorm    no    no\n-verbose    no    no\n-warp_params\n-warp_type    inverse_linear  inverse_linear\n-wbeam      7e-29   7.000000e-29\n-wip      0.65    6.500000e-01\n-wlen     0.025625  2.562500e-02 \n \n Pipeline is live and does not need PREROLL ...\nGot message #23 from element "fakesink0" (state-changed): GstMessageStateChanged, old-state=(GstState)GST_STATE_NULL, new-state=(GstState)GST_STATE_READY, pending-state=(GstState)GST_STATE_VOID_PENDING;\nGot message #24 from element "queue1" (state-changed): GstMessageStateChanged, old-state=(GstState)GST_STATE_NULL, new-state=(GstState)GST_STATE_READY, pending-state=(GstState)GST_STATE_VOID_PENDING;\nGot message #25 from element "asr" (state-changed): GstMessageStateChanged, old-state=(GstState)GST_STATE_NULL, new-state=(GstState)GST_STATE_READY, pending-state=(GstState)GST_STATE_VOID_PENDING;\nGot message #26 from element "capsfilter0" (state-changed): GstMessageStateChanged, old-state=(GstState)GST_STATE_NULL, new-state=(GstState)GST_STATE_READY, pending-state=(GstState)GST_STATE_VOID_PENDING;\nGot message #27 from element "audioresample0" (state-changed): GstMessageStateChanged, old-state=(GstState)GST_STATE_NULL, new-state=(GstState)GST_STATE_READY, pending-state=(GstState)GST_STATE_VOID_PENDING;\nGot message #28 from element "audioconvert0" (state-changed): GstMessageStateChanged, old-state=(GstState)GST_STATE_NULL, new-state=(GstState)GST_STATE_READY, pending-state=(GstState)GST_STATE_VOID_PENDING;\nGot message #29 from element "queue0" (state-changed): GstMessageStateChanged, old-state=(GstState)GST_STATE_NULL, new-state=(GstState)GST_STATE_READY, pending-state=(GstState)GST_STATE_VOID_PENDING;\nGot message #30 from element "alsasrc0" (state-changed): GstMessageStateChanged, old-state=(GstState)GST_STATE_NULL, new-state=(GstState)GST_STATE_READY, pending-state=(GstState)GST_STATE_VOID_PENDING;\nGot message #31 from element "pipeline0" (state-changed): GstMessageStateChanged, old-state=(GstState)GST_STATE_NULL, new-state=(GstState)GST_STATE_READY, pending-state=(GstState)GST_STATE_PAUSED;\nGot message #33 from pad "queue1:src" (stream-status): GstMessageStreamStatus, type=(GstStreamStatusType)GST_STREAM_STATUS_TYPE_CREATE, owner=(GstElement)"(GstQueue)\\ queue1", object=(GstTask)"(GstTask)\\ queue1:src";\nGot message #34 from element "queue1" (state-changed): GstMessageStateChanged, old-state=(GstState)GST_STATE_READY, new-state=(GstState)GST_STATE_PAUSED, pending-state=(GstState)GST_STATE_VOID_PENDING;\nGot message #35 from element "asr" (state-changed): GstMessageStateChanged, old-state=(GstState)GST_STATE_READY, new-state=(GstState)GST_STATE_PAUSED, pending-state=(GstState)GST_STATE_VOID_PENDING;\nGot message #36 from element "capsfilter0" (state-changed): GstMessageStateChanged, old-state=(GstState)GST_STATE_READY, new-state=(GstState)GST_STATE_PAUSED, pending-state=(GstState)GST_STATE_VOID_PENDING;\nGot message #37 from element "audioresample0" (state-changed): GstMessageStateChanged, old-state=(GstState)GST_STATE_READY, new-state=(GstState)GST_STATE_PAUSED, pending-state=(GstState)GST_STATE_VOID_PENDING;\nGot message #38 from element "audioconvert0" (state-changed): GstMessageStateChanged, old-state=(GstState)GST_STATE_READY, new-state=(GstState)GST_STATE_PAUSED, pending-state=(GstState)GST_STATE_VOID_PENDING;\nGot message #39 from pad "queue0:src" (stream-status): GstMessageStreamStatus, type=(GstStreamStatusType)GST_STREAM_STATUS_TYPE_CREATE, owner=(GstElement)"(GstQueue)\\ queue0", object=(GstTask)"(GstTask)\\ queue0:src";\nGot message #40 from element "queue0" (state-changed): GstMessageStateChanged, old-state=(GstState)GST_STATE_READY, new-state=(GstState)GST_STATE_PAUSED, pending-state=(GstState)GST_STATE_VOID_PENDING;\nGot message #41 from pad "queue1:src" (stream-status): GstMessageStreamStatus, type=(GstStreamStatusType)GST_STREAM_STATUS_TYPE_ENTER, owner=(GstElement)"(GstQueue)\\ queue1", object=(GstTask)"(GstTask)\\ queue1:src";\nGot message #42 from pad "queue0:src" (stream-status): GstMessageStreamStatus, type=(GstStreamStatusType)GST_STREAM_STATUS_TYPE_ENTER, owner=(GstElement)"(GstQueue)\\ queue0", object=(GstTask)"(GstTask)\\ queue0:src";\nGot message #46 from pad "alsasrc0:src" (stream-status): GstMessageStreamStatus, type=(GstStreamStatusType)GST_STREAM_STATUS_TYPE_CREATE, owner=(GstElement)"(GstAlsaSrc)\\ alsasrc0", object=(GstTask)"(GstTask)\\ alsasrc0:src";\nGot message #47 from element "alsasrc0" (state-changed): GstMessageStateChanged, old-state=(GstState)GST_STATE_READY, new-state=(GstState)GST_STATE_PAUSED, pending-state=(GstState)GST_STATE_VOID_PENDING;\nGot message #48 from element "pipeline0" (state-changed): GstMessageStateChanged, old-state=(GstState)GST_STATE_READY, new-state=(GstState)GST_STATE_PAUSED, pending-state=(GstState)GST_STATE_VOID_PENDING;\nGot message #49 from pad "alsasrc0:src" (stream-status): GstMessageStreamStatus, type=(GstStreamStatusType)GST_STREAM_STATUS_TYPE_ENTER, owner=(GstElement)"(GstAlsaSrc)\\ alsasrc0", object=(GstTask)"(GstTask)\\ alsasrc0:src";\nGot message #50 from element "pipeline0" (stream-start): GstMessageStreamStart, group-id=(uint)0;\nSetting pipeline to PLAYING ...\nGot message #53 from element "pipeline0" (new-clock): GstMessageNewClock, clock=(GstClock)"(GstAudioClock)\\ GstAudioSrcClock";\nNew clock: GstAudioSrcClock\nGot message #55 from element "queue1" (state-changed): GstMessageStateChanged, old-state=(GstState)GST_STATE_PAUSED, new-state=(GstState)GST_STATE_PLAYING, pending-state=(GstState)GST_STATE_VOID_PENDING;\nGot message #56 from element "asr" (state-changed): GstMessageStateChanged, old-state=(GstState)GST_STATE_PAUSED, new-state=(GstState)GST_STATE_PLAYING, pending-state=(GstState)GST_STATE_VOID_PENDING;\nGot message #57 from element "capsfilter0" (state-changed): GstMessageStateChanged, old-state=(GstState)GST_STATE_PAUSED, new-state=(GstState)GST_STATE_PLAYING, pending-state=(GstState)GST_STATE_VOID_PENDING;\nGot message #58 from element "audioresample0" (state-changed): GstMessageStateChanged, old-state=(GstState)GST_STATE_PAUSED, new-state=(GstState)GST_STATE_PLAYING, pending-state=(GstState)GST_STATE_VOID_PENDING;\nGot message #59 from element "audioconvert0" (state-changed): GstMessageStateChanged, old-state=(GstState)GST_STATE_PAUSED, new-state=(GstState)GST_STATE_PLAYING, pending-state=(GstState)GST_STATE_VOID_PENDING;\nGot message #60 from element "queue0" (state-changed): GstMessageStateChanged, old-state=(GstState)GST_STATE_PAUSED, new-state=(GstState)GST_STATE_PLAYING, pending-state=(GstState)GST_STATE_VOID_PENDING;\nGot message #61 from element "alsasrc0" (state-changed): GstMessageStateChanged, old-state=(GstState)GST_STATE_PAUSED, new-state=(GstState)GST_STATE_PLAYING, pending-state=(GstState)GST_STATE_VOID_PENDING;\nGot message #62 from object "audiosrcringbuffer0" (stream-status): GstMessageStreamStatus, type=(GstStreamStatusType)GST_STREAM_STATUS_TYPE_ENTER, owner=(GstElement)"(GstAlsaSrc)\\ alsasrc0", object=(GThread)NULL;\nGot message #69 from element "asr" (element): pocketsphinx, timestamp=(guint64)940000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)SIX;\nGot message #70 from element "asr" (element): pocketsphinx, timestamp=(guint64)980000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)TO;\nGot message #71 from element "asr" (element): pocketsphinx, timestamp=(guint64)1030000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)TURN;\nGot message #72 from element "asr" (element): pocketsphinx, timestamp=(guint64)1270000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)"TURN\\ IS";\nGot message #73 from element "asr" (element): pocketsphinx, timestamp=(guint64)1400000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)"TURN\\ IS\\ IT";\nGot message #74 from element "asr" (element): pocketsphinx, timestamp=(guint64)1500000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)"TURN\\ IS\\ IT\\ GO";\nGot message #75 from element "asr" (element): pocketsphinx, timestamp=(guint64)1640000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)"TURN\\ IS\\ IT\\ TURN";\nGot message #76 from element "asr" (element): pocketsphinx, timestamp=(guint64)18446744073709551615, final=(boolean)true, confidence=(glong)0, hypothesis=(string)"TURN\\ IS\\ IT\\ TURN";\nGot message #77 from element "fakesink0" (state-changed): GstMessageStateChanged, old-state=(GstState)GST_STATE_READY, new-state=(GstState)GST_STATE_PAUSED, pending-state=(GstState)GST_STATE_VOID_PENDING;\nGot message #79 from element "pipeline0" (async-done): GstMessageAsyncDone, running-time=(guint64)18446744073709551615;\nGot message #81 from element "fakesink0" (state-changed): GstMessageStateChanged, old-state=(GstState)GST_STATE_PAUSED, new-state=(GstState)GST_STATE_PLAYING, pending-state=(GstState)GST_STATE_VOID_PENDING;\nGot message #82 from element "pipeline0" (state-changed): GstMessageStateChanged, old-state=(GstState)GST_STATE_PAUSED, new-state=(GstState)GST_STATE_PLAYING, pending-state=(GstState)GST_STATE_VOID_PENDING;\nGot message #83 from element "asr" (element): pocketsphinx, timestamp=(guint64)5320000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)EIGHT;\nGot message #84 from element "asr" (element): pocketsphinx, timestamp=(guint64)5580000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)THE;\nGot message #85 from element "asr" (element): pocketsphinx, timestamp=(guint64)6270000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)PAUSE;\nGot message #86 from element "asr" (element): pocketsphinx, timestamp=(guint64)6290000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)ON;\nGot message #87 from element "asr" (element): pocketsphinx, timestamp=(guint64)6730000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)"ON\\ GET";\nGot message #88 from element "asr" (element): pocketsphinx, timestamp=(guint64)18446744073709551615, final=(boolean)true, confidence=(glong)0, hypothesis=(string)GET;\nGot message #89 from element "asr" (element): pocketsphinx, timestamp=(guint64)8870000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)GET;\nGot message #90 from element "asr" (element): pocketsphinx, timestamp=(guint64)9150000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)ONE;\nGot message #91 from element "asr" (element): pocketsphinx, timestamp=(guint64)9450000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)WHAT;\nGot message #92 from element "asr" (element): pocketsphinx, timestamp=(guint64)18446744073709551615, final=(boolean)true, confidence=(glong)0, hypothesis=(string)WHAT;\nGot message #93 from element "asr" (element): pocketsphinx, timestamp=(guint64)11830000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)THE;\nGot message #94 from element "asr" (element): pocketsphinx, timestamp=(guint64)11870000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)TV;\nGot message #95 from element "asr" (element): pocketsphinx, timestamp=(guint64)12070000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)THE;\nGot message #96 from element "asr" (element): pocketsphinx, timestamp=(guint64)12080000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)TV;\nGot message #97 from element "asr" (element): pocketsphinx, timestamp=(guint64)12470000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)THE;\nGot message #98 from element "asr" (element): pocketsphinx, timestamp=(guint64)12650000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)"THE\\ WHAT";\nGot message #99 from element "asr" (element): pocketsphinx, timestamp=(guint64)12690000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)"THE\\ WHITE";\nGot message #100 from element "asr" (element): pocketsphinx, timestamp=(guint64)12750000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)"THE\\ PLAY";\nGot message #101 from element "asr" (element): pocketsphinx, timestamp=(guint64)12770000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)"THE\\ WHITE";\nGot message #102 from element "asr" (element): pocketsphinx, timestamp=(guint64)18446744073709551615, final=(boolean)true, confidence=(glong)0, hypothesis=(string)"THE\\ WHITE";\nGot message #103 from element "asr" (element): pocketsphinx, timestamp=(guint64)13980000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)EIGHT;\nGot message #104 from element "asr" (element): pocketsphinx, timestamp=(guint64)14040000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)TO;\nGot message #105 from element "asr" (element): pocketsphinx, timestamp=(guint64)14090000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)TURN;\nGot message #106 from element "asr" (element): pocketsphinx, timestamp=(guint64)14120000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)ZERO;\nGot message #107 from element "asr" (element): pocketsphinx, timestamp=(guint64)14140000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)TURN;\nGot message #108 from element "asr" (element): pocketsphinx, timestamp=(guint64)14170000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)ZERO;\nGot message #109 from element "asr" (element): pocketsphinx, timestamp=(guint64)14320000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)SCARLETT;\nGot message #110 from element "asr" (element): pocketsphinx, timestamp=(guint64)18446744073709551615, final=(boolean)true, confidence=(glong)0, hypothesis=(string)SCARLETT;\nGot message #111 from element "asr" (element): pocketsphinx, timestamp=(guint64)19840000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)FOUR;\nGot message #112 from element "asr" (element): pocketsphinx, timestamp=(guint64)19870000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)WHAT;\nGot message #113 from element "asr" (element): pocketsphinx, timestamp=(guint64)19900000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)ONE;\nGot message #114 from element "asr" (element): pocketsphinx, timestamp=(guint64)19910000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)WHAT;\nGot message #115 from element "asr" (element): pocketsphinx, timestamp=(guint64)20220000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)"WHAT\\ TIME";\nGot message #116 from element "asr" (element): pocketsphinx, timestamp=(guint64)20230000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)"WHAT\\ TIME\\ IS";\nGot message #117 from element "asr" (element): pocketsphinx, timestamp=(guint64)20370000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)"WHAT\\ TIME\\ IS\\ IT";\nGot message #118 from element "asr" (element): pocketsphinx, timestamp=(guint64)18446744073709551615, final=(boolean)true, confidence=(glong)0, hypothesis=(string)"WHAT\\ TIME\\ IS\\ IT";\nGot message #119 from element "asr" (element): pocketsphinx, timestamp=(guint64)22940000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)TURN;\nGot message #120 from element "asr" (element): pocketsphinx, timestamp=(guint64)22950000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)FRIZZY;\nGot message #121 from element "asr" (element): pocketsphinx, timestamp=(guint64)22980000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)TURN;\nGot message #122 from element "asr" (element): pocketsphinx, timestamp=(guint64)23050000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)ENTER;\nGot message #123 from element "asr" (element): pocketsphinx, timestamp=(guint64)23070000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)"TURN\\ ON";\nGot message #124 from element "asr" (element): pocketsphinx, timestamp=(guint64)23120000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)NINE;\nGot message #125 from element "asr" (element): pocketsphinx, timestamp=(guint64)23480000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)"TURN\\ ON\\ THE";\nGot message #126 from element "asr" (element): pocketsphinx, timestamp=(guint64)23670000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)"TURN\\ ON\\ THE\\ GET";\nGot message #127 from element "asr" (element): pocketsphinx, timestamp=(guint64)24190000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)"TURN\\ ON\\ THE\\ GET\\ ENTER";\nGot message #128 from element "asr" (element): pocketsphinx, timestamp=(guint64)18446744073709551615, final=(boolean)true, confidence=(glong)0, hypothesis=(string)"TURN\\ ON\\ THE\\ GET";\nGot message #129 from element "asr" (element): pocketsphinx, timestamp=(guint64)27970000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)ARE;\nGot message #130 from element "asr" (element): pocketsphinx, timestamp=(guint64)28100000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)PAUSE;\nGot message #131 from element "asr" (element): pocketsphinx, timestamp=(guint64)28110000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)ARE;\nGot message #132 from element "asr" (element): pocketsphinx, timestamp=(guint64)28220000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)PAUSE;\nGot message #133 from element "asr" (element): pocketsphinx, timestamp=(guint64)18446744073709551615, final=(boolean)true, confidence=(glong)0, hypothesis=(string)ARE;\nGot message #134 from element "asr" (element): pocketsphinx, timestamp=(guint64)29150000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)MY;\nGot message #135 from element "asr" (element): pocketsphinx, timestamp=(guint64)29400000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)TURN;\nGot message #136 from element "asr" (element): pocketsphinx, timestamp=(guint64)29510000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)APPLE;\nGot message #137 from element "asr" (element): pocketsphinx, timestamp=(guint64)29880000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)"TURN\\ THE\\ ON";\nGot message #138 from element "asr" (element): pocketsphinx, timestamp=(guint64)29890000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)"TURN\\ THE\\ PAUSE";\nGot message #139 from element "asr" (element): pocketsphinx, timestamp=(guint64)29940000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)"TURN\\ THE\\ ON";\nGot message #140 from element "asr" (element): pocketsphinx, timestamp=(guint64)30020000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)"TURN\\ THE\\ OFF\\ THE";\nGot message #141 from element "asr" (element): pocketsphinx, timestamp=(guint64)30320000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)"TURN\\ THE\\ OFF\\ THE\\ GO";\nGot message #142 from element "asr" (element): pocketsphinx, timestamp=(guint64)30360000000, final=(boolean)false, confidence=(glong)0, hypothesis=(string)"TURN\\ THE\\ OFF\\ THE\\ FOUR";\nGot message #143 from element "asr" (element): pocketsphinx, timestamp=(guint64)18446744073709551615, final=(boolean)true, confidence=(glong)0, hypothesis=(string)"TURN\\ OFF\\ THE\\ FOUR";\n^Chandling interrupt.\nGot message #144 from element "pipeline0" (application): GstLaunchInterrupt, message=(string)"Pipeline\\ interrupted";\nInterrupt: Stopping pipeline ...\nExecution ended after 0:00:33.286680180\nSetting pipeline to PAUSED ...\nSetting pipeline to READY ...\nSetting pipeline to NULL ...\nFreeing pipeline ...\n``` \n vagrant package --base "f5fba22c-929b-4a55-bb1c-c5a3d0f26b8d" --output ubuntu_1604_desktop_base.box \n |2.1.7|  using virtualenv: packer-ubuntu-1604  Malcolms-MBP-3 in ~\n○ → vagrant package --base "f5fba22c-929b-4a55-bb1c-c5a3d0f26b8d" --output ubuntu_1604_desktop_base.box\n==> f5fba22c-929b-4a55-bb1c-c5a3d0f26b8d: Exporting VM...\n==> f5fba22c-929b-4a55-bb1c-c5a3d0f26b8d: Compressing package to: /Users/malcolm/ubuntu_1604_desktop_base.box \n Environment variables for resize script \n ```\n± |featutre-1604 U:2 ?:1 ✗| → env | grep CD_TO \n |2.1.7|   Malcolms-MBP-3 in ~/dev/bossjones/scarlett-ansible\n± |featutre-1604 U:2 ?:1 ✗| → export CD_TO=~/dev/bossjones/scarlett-ansible \n |2.1.7|   Malcolms-MBP-3 in ~/dev/bossjones/scarlett-ansible\n± |featutre-1604 U:2 ?:1 ✗| → export USERDIR=$PWD \n |2.1.7|   Malcolms-MBP-3 in ~/dev/bossjones/scarlett-ansible\n± |featutre-1604 U:2 ?:1 ✗| → export VDMK_FILE=packer-ubuntu-16.04-amd64-disk1.vmdk \n |2.1.7|   Malcolms-MBP-3 in ~/dev/bossjones/scarlett-ansible\n± |featutre-1604 U:2 ?:1 ✗| → export VAGRANT_VMNAME=scarlett-base-16-04 \n |2.1.7|   Malcolms-MBP-3 in ~/dev/bossjones/scarlett-ansible\n± |featutre-1604 U:2 ?:1 ✗| → export VDI_FILENAME=scarlett_50gb.vdi \n |2.1.7|   Malcolms-MBP-3 in ~/dev/bossjones/scarlett-ansible\n± |featutre-1604 U:2 ?:1 ✗| → env | grep CD_TO\nCD_TO=/Users/malcolm/dev/bossjones/scarlett-ansible \n |2.1.7|   Malcolms-MBP-3 in ~/dev/bossjones/scarlett-ansible\n± |featutre-1604 U:2 ?:1 ✗| → env | grep USERDIR\nUSERDIR=/Users/malcolm/dev/bossjones/scarlett-ansible \n |2.1.7|   Malcolms-MBP-3 in ~/dev/bossjones/scarlett-ansible\n± |featutre-1604 U:2 ?:1 ✗| → env | grep VDMK_FILE\nVDMK_FILE=packer-ubuntu-16.04-amd64-disk1.vmdk \n |2.1.7|   Malcolms-MBP-3 in ~/dev/bossjones/scarlett-ansible\n± |featutre-1604 U:2 ?:1 ✗| → env | grep VAGRANT_VMNAME\nVAGRANT_VMNAME=scarlett-base-16-04 \n |2.1.7|   Malcolms-MBP-3 in ~/dev/bossjones/scarlett-ansible\n± |featutre-1604 U:2 ?:1 ✗| → env | grep VDI_FILENAME\nVDI_FILENAME=scarlett_50gb.vdi \n |2.1.7|   Malcolms-MBP-3 in ~/dev/bossjones/scarlett-ansible\n± |featutre-1604 U:2 ?:1 ✗| → env | grep VMNAME\nVAGRANT_VMNAME=scarlett-base-16-04 \n |2.1.7|   Malcolms-MBP-3 in ~/dev/bossjones/scarlett-ansible\n± |featutre-1604 U:2 ?:1 ✗| → cd $CD_TO \n |2.1.7|   Malcolms-MBP-3 in ~/dev/bossjones/scarlett-ansible\n± |featutre-1604 U:2 ?:1 ✗| → export VMNAME=$(grep "  vb.name" Vagrantfile | cut -d\\\' -f2) \n |2.1.7|   Malcolms-MBP-3 in ~/dev/bossjones/scarlett-ansible\n± |featutre-1604 U:2 ?:1 ✗| → env | grep VMNAME\nVMNAME=scarlett-ansible-1604-packer2\nVAGRANT_VMNAME=scarlett-base-16-04 \n |2.1.7|   Malcolms-MBP-3 in ~/dev/bossjones/scarlett-ansible\n± |featutre-1604 U:2 ?:1 ✗| → env | grep VM_PATH\nNVM_PATH=/Users/malcolm/.nvm/versions/node/v5.9.0/lib/node \n |2.1.7|   Malcolms-MBP-3 in ~/dev/bossjones/scarlett-ansible\n± |featutre-1604 U:2 ?:1 ✗| → export VM_PATH="$HOME/VirtualBox VMs/$VMNAME" \n |2.1.7|   Malcolms-MBP-3 in ~/dev/bossjones/scarlett-ansible\n± |featutre-1604 U:2 ?:1 ✗| → env | grep VM_PATH\nNVM_PATH=/Users/malcolm/.nvm/versions/node/v5.9.0/lib/node\nVM_PATH=/Users/malcolm/VirtualBox VMs/scarlett-ansible-1604-packer2 \n |2.1.7|   Malcolms-MBP-3 in ~/dev/bossjones/scarlett-ansible\n± |featutre-1604 U:2 ?:1 ✗| → \n |2.1.7|   Malcolms-MBP-3 in ~/dev/bossjones/scarlett-ansible\n± |featutre-1604 U:2 ?:1 ✗| → vagrant up --no-provision\nBringing machine \'scarlett-1604-packer2\' up with \'virtualbox\' provider...\n==> scarlett-1604-packer2: Importing base box \'bossjones/scarlett-1604-packer\'...\n==> scarlett-1604-packer2: Matching MAC address for NAT networking...\n==> scarlett-1604-packer2: Setting the name of the VM: scarlett-ansible-1604-packer2\n==> scarlett-1604-packer2: Clearing any previously set network interfaces...\n==> scarlett-1604-packer2: Preparing network interfaces based on configuration...\n    scarlett-1604-packer2: Adapter 1: nat\n    scarlett-1604-packer2: Adapter 2: bridged\n==> scarlett-1604-packer2: Forwarding ports...\n    scarlett-1604-packer2: 2376 (guest) => 2376 (host) (adapter 1)\n    scarlett-1604-packer2: 22 (guest) => 2222 (host) (adapter 1)\n==> scarlett-1604-packer2: Running \'pre-boot\' VM customizations...\n==> scarlett-1604-packer2: Booting VM...\n==> scarlett-1604-packer2: Waiting for machine to boot. This may take a few minutes...\n    scarlett-1604-packer2: SSH address: 127.0.0.1:2222\n    scarlett-1604-packer2: SSH username: pi\n    scarlett-1604-packer2: SSH auth method: private key\n==> scarlett-1604-packer2: Machine booted and ready!\n==> scarlett-1604-packer2: Checking for guest additions in VM...\n==> scarlett-1604-packer2: Setting hostname...\n==> scarlett-1604-packer2: Configuring and enabling network interfaces...\nThe following SSH command responded with a non-zero exit status.\nVagrant assumes that this means the command failed! \n /sbin/ifdown eth1 2> /dev/null \n Stdout from the command: \n Stderr from the command: \n mesg: ttyname failed: Inappropriate ioctl for device \n |2.1.7|   Malcolms-MBP-3 in ~/dev/bossjones/scarlett-ansible\n± |featutre-1604 U:3 ?:1 ✗| → vagrant halt\n==> scarlett-1604-packer2: Attempting graceful shutdown of VM... \n |2.1.7|   Malcolms-MBP-3 in ~/dev/bossjones/scarlett-ansible\n± |featutre-1604 U:3 ?:1 ✗| → cd "$VM_PATH" \n |2.1.7|   Malcolms-MBP-3 in ~/VirtualBox VMs/scarlett-ansible-1604-packer2\n○ → ls -lta\ntotal 11968688\ndrwx------   6 malcolm  staff         204 Sep 10 14:17 .\n-rw-------   1 malcolm  staff  6127943680 Sep 10 14:17 packer-ubuntu-16.04-amd64-disk1.vmdk\n-rw-------   1 malcolm  staff        8560 Sep 10 14:17 scarlett-ansible-1604-packer2.vbox\n-rw-------   1 malcolm  staff        8560 Sep 10 14:17 scarlett-ansible-1604-packer2.vbox-prev\ndrwx------   3 malcolm  staff         102 Sep 10 14:14 Logs\ndrwx------  12 malcolm  staff         408 Sep 10 14:14 .. \n |2.1.7|   Malcolms-MBP-3 in ~/VirtualBox VMs/scarlett-ansible-1604-packer2\n○ → \n UUID:           b5bb12c1-4e9d-4d4f-9dea-e5a88601e3ad\nParent UUID:    base\nState:          inaccessible\nType:           normal (base)\nLocation:       /Users/malcolm/VirtualBox VMs/scarlett-ansible-1604-packer2/out.vdi\nStorage format: VDI\nCapacity:       56320 MBytes\nEncryption:     disabled \n UUID:           2a845830-55d1-4181-8b0f-ba274b524db1\nParent UUID:    base\nState:          inaccessible\nType:           normal (base)\nLocation:       /Users/malcolm/VirtualBox VMs/scarlett-ansible-1604-packer2/box-disk1_50gb.vmdk\nStorage format: VMDK\nCapacity:       56320 MBytes\nEncryption:     disabled \n UUID:           c33810ce-a153-44d9-bd51-f66af6a2fc82\nParent UUID:    base\nState:          inaccessible\nType:           normal (base)\nLocation:       /Users/malcolm/VirtualBox VMs/scarlett-ansible-1604-packer2/scarlett_50gb.vdi\nStorage format: VDI\nCapacity:       56320 MBytes\nEncryption:     disabled \n UUID:           1683a1b1-e82c-4f8a-bcd6-1b706565179e\nParent UUID:    base\nState:          created\nType:           normal (base)\nLocation:       /Users/malcolm/VirtualBox VMs/scarlett-ansible-1604-packer2/packer-ubuntu-16.04-amd64-disk1.vmdk\nStorage format: VMDK\nCapacity:       20000 MBytes\nEncryption:     disabled\n``` \n The trick to fixing this was here: \n https://coderwall.com/p/8m--dq/purge-deleted-hard-disks-from-virtual-box \n ```\n |2.1.7|   Malcolms-MBP-3 in ~/dev/bossjones/scarlett-ansible\n± |featutre-1604 U:1 ✗| → vagrant halt\n==> scarlett-1604-packer2: Attempting graceful shutdown of VM... \n |2.1.7|   Malcolms-MBP-3 in ~/dev/bossjones/scarlett-ansible\n± |featutre-1604 U:1 ✗| → cd "$VM_PATH" \n |2.1.7|   Malcolms-MBP-3 in ~/VirtualBox VMs/scarlett-ansible-1604-packer2\n○ → ll\ntotal 11979568\ndrwx------   6 malcolm  staff         204 Sep 10 14:41 .\ndrwx------  12 malcolm  staff         408 Sep 10 14:40 ..\ndrwx------   3 malcolm  staff         102 Sep 10 14:40 Logs\n-rw-------   1 malcolm  staff  6133514240 Sep 10 14:41 packer-ubuntu-16.04-amd64-disk1.vmdk\n-rw-------   1 malcolm  staff        8560 Sep 10 14:41 scarlett-ansible-1604-packer2.vbox\n-rw-------   1 malcolm  staff        8560 Sep 10 14:41 scarlett-ansible-1604-packer2.vbox-prev \n |2.1.7|   Malcolms-MBP-3 in ~/VirtualBox VMs/scarlett-ansible-1604-packer2\n○ → VBoxManage clonehd $VDMK_FILE ${VDI_FILENAME} --format VDI\n0%...10%...20%...30%...40%...50%...60%...70%...80%...90%...100%\nClone medium created in format \'VDI\'. UUID: a392e58b-04fe-48ea-9ed4-e199654b503e \n |2.1.7|   Malcolms-MBP-3 in ~/VirtualBox VMs/scarlett-ansible-1604-packer2\n○ → VBoxManage modifyhd ${VDI_FILENAME} --resizebyte 59055800320\n0%...10%...20%...30%...40%...50%...60%...70%...80%...90%...100% \n |2.1.7|   Malcolms-MBP-3 in ~/VirtualBox VMs/scarlett-ansible-1604-packer2\n○ → VBoxManage clonehd ${VDI_FILENAME} box-disk1_50gb.vmdk --format VMDK\n0%...10%...20%...30%...40%...50%...60%...70%...80%...90%...100%\nClone medium created in format \'VMDK\'. UUID: 0158b89a-f0c9-4f1f-b097-c207e034a9bb \n |2.1.7|   Malcolms-MBP-3 in ~/VirtualBox VMs/scarlett-ansible-1604-packer2\n○ → \n VBoxManage showvminfo "scarlett-ansible-1604-packer2" \n ○ → VBoxManage showvminfo "scarlett-ansible-1604-packer2"\nName:            scarlett-ansible-1604-packer2\nGroups:          /\nGuest OS:        Ubuntu (64-bit)\nUUID:            5d9fbcf5-2022-4e82-88dc-a93c4688b52a\nConfig file:     /Users/malcolm/VirtualBox VMs/scarlett-ansible-1604-packer2/scarlett-ansible-1604-packer2.vbox\nSnapshot folder: /Users/malcolm/VirtualBox VMs/scarlett-ansible-1604-packer2/Snapshots\nLog folder:      /Users/malcolm/VirtualBox VMs/scarlett-ansible-1604-packer2/Logs\nHardware UUID:   5d9fbcf5-2022-4e82-88dc-a93c4688b52a\nMemory size:     2048MB\nPage Fusion:     off\nVRAM size:       8MB\nCPU exec cap:    100%\nHPET:            off\nChipset:         piix3\nFirmware:        BIOS\nNumber of CPUs:  2\nPAE:             on\nLong Mode:       on\nCPUID Portability Level: 0\nCPUID overrides: None\nBoot menu mode:  message and menu\nBoot Device (1): HardDisk\nBoot Device (2): DVD\nBoot Device (3): Not Assigned\nBoot Device (4): Not Assigned\nACPI:            on\nIOAPIC:          on\nTime offset:     0ms\nRTC:             UTC\nHardw. virt.ext: on\nNested Paging:   on\nLarge Pages:     on\nVT-x VPID:       on\nVT-x unr. exec.: on\nParavirt. Provider: Default\nState:           powered off (since 2016-09-10T18:41:35.261000000)\nMonitor count:   1\n3D Acceleration: off\n2D Video Acceleration: off\nTeleporter Enabled: off\nTeleporter Port: 0\nTeleporter Address:\nTeleporter Password:\nTracing Enabled: off\nAllow Tracing to Access VM: off\nTracing Configuration:\nAutostart Enabled: off\nAutostart Delay: 0\nDefault Frontend:\nStorage Controller Name (0):            IDE Controller\nStorage Controller Type (0):            PIIX4\nStorage Controller Instance Number (0): 0\nStorage Controller Max Port Count (0):  2\nStorage Controller Port Count (0):      2\nStorage Controller Bootable (0):        on\nIDE Controller (0, 0): /Users/malcolm/VirtualBox VMs/scarlett-ansible-1604-packer2/packer-ubuntu-16.04-amd64-disk1.vmdk (UUID: c36a7d63-db73-461c-b05d-093917e19e19)\nNIC 1:           MAC: 0800279C7307, Attachment: NAT, Cable connected: on, Trace: off (file: none), Type: 82540EM, Reported speed: 0 Mbps, Boot priority: 0, Promisc Policy: deny, Bandwidth group: none\nNIC 1 Settings:  MTU: 0, Socket (send: 64, receive: 64), TCP Window (send:64, receive: 64)\nNIC 1 Rule(0):   name = ssh, protocol = tcp, host ip = 127.0.0.1, host port = 2222, guest ip = , guest port = 22\nNIC 1 Rule(1):   name = tcp2376, protocol = tcp, host ip = 127.0.0.1, host port = 2376, guest ip = , guest port = 2376\nNIC 2:           MAC: 080027CF00AC, Attachment: Bridged Interface \'en0: Wi-Fi (AirPort)\', Cable connected: on, Trace: off (file: none), Type: 82540EM, Reported speed: 0 Mbps, Boot priority: 0, Promisc Policy: deny, Bandwidth group: none\nNIC 3:           disabled\nNIC 4:           disabled\nNIC 5:           disabled\nNIC 6:           disabled\nNIC 7:           disabled\nNIC 8:           disabled\nPointing Device: PS/2 Mouse\nKeyboard Device: PS/2 Keyboard\nUART 1:          disabled\nUART 2:          disabled\nLPT 1:           disabled\nLPT 2:           disabled\nAudio:           enabled (Driver: CoreAudio, Controller: AC97, Codec: STAC9700)\nClipboard Mode:  disabled\nDrag and drop Mode: disabled\nVRDE:            enabled (Address 127.0.0.1, Ports 5993, MultiConn: off, ReuseSingleConn: off, Authentication type: null)\nVideo redirection: disabled\nVRDE property: TCP/Ports  = "5993"\nVRDE property: TCP/Address = "127.0.0.1"\nVRDE property: VideoChannel/Enabled =  \nVRDE property: VideoChannel/Quality =  \nVRDE property: VideoChannel/DownscaleProtection =  \nVRDE property: Client/DisableDisplay =  \nVRDE property: Client/DisableInput =  \nVRDE property: Client/DisableAudio =  \nVRDE property: Client/DisableUSB =  \nVRDE property: Client/DisableClipboard =  \nVRDE property: Client/DisableUpstreamAudio =  \nVRDE property: Client/DisableRDPDR =  \nVRDE property: H3DRedirect/Enabled =  \nVRDE property: Security/Method =  \nVRDE property: Security/ServerCertificate =  \nVRDE property: Security/ServerPrivateKey =  \nVRDE property: Security/CACertificate =  \nVRDE property: Audio/RateCorrectionMode =  \nVRDE property: Audio/LogPath =  \nUSB:             enabled\nEHCI:            disabled\nXHCI:            disabled \n USB Device Filters: \n \n Bandwidth groups:   \n Shared folders: \n Name: \'vagrant\', Host path: \'/Users/malcolm/dev/bossjones/scarlett-ansible\' (machine mapping), writable \n Video capturing:    not active\nCapture screens:    0\nCapture file:       /Users/malcolm/VirtualBox VMs/scarlett-ansible-1604-packer2/scarlett-ansible-1604-packer2.webm\nCapture dimensions: 1024x768\nCapture rate:       512 kbps\nCapture FPS:        25 \n Guest: \n Configured memory balloon size:      0 MB\n \n○ → VBoxManage showvminfo "$VMNAME"\nName:            scarlett-ansible-1604-packer2\nGroups:          /\nGuest OS:        Ubuntu (64-bit)\nUUID:            5d9fbcf5-2022-4e82-88dc-a93c4688b52a\nConfig file:     /Users/malcolm/VirtualBox VMs/scarlett-ansible-1604-packer2/scarlett-ansible-1604-packer2.vbox\nSnapshot folder: /Users/malcolm/VirtualBox VMs/scarlett-ansible-1604-packer2/Snapshots\nLog folder:      /Users/malcolm/VirtualBox VMs/scarlett-ansible-1604-packer2/Logs\nHardware UUID:   5d9fbcf5-2022-4e82-88dc-a93c4688b52a\nMemory size:     2048MB\nPage Fusion:     off\nVRAM size:       8MB\nCPU exec cap:    100%\nHPET:            off\nChipset:         piix3\nFirmware:        BIOS\nNumber of CPUs:  2\nPAE:             on\nLong Mode:       on\nCPUID Portability Level: 0\nCPUID overrides: None\nBoot menu mode:  message and menu\nBoot Device (1): HardDisk\nBoot Device (2): DVD\nBoot Device (3): Not Assigned\nBoot Device (4): Not Assigned\nACPI:            on\nIOAPIC:          on\nTime offset:     0ms\nRTC:             UTC\nHardw. virt.ext: on\nNested Paging:   on\nLarge Pages:     on\nVT-x VPID:       on\nVT-x unr. exec.: on\nParavirt. Provider: Default\nState:           powered off (since 2016-09-10T18:41:35.261000000)\nMonitor count:   1\n3D Acceleration: off\n2D Video Acceleration: off\nTeleporter Enabled: off\nTeleporter Port: 0\nTeleporter Address:\nTeleporter Password:\nTracing Enabled: off\nAllow Tracing to Access VM: off\nTracing Configuration:\nAutostart Enabled: off\nAutostart Delay: 0\nDefault Frontend:\nStorage Controller Name (0):            IDE Controller\nStorage Controller Type (0):            PIIX4\nStorage Controller Instance Number (0): 0\nStorage Controller Max Port Count (0):  2\nStorage Controller Port Count (0):      2\nStorage Controller Bootable (0):        on\nIDE Controller (0, 0): /Users/malcolm/VirtualBox VMs/scarlett-ansible-1604-packer2/packer-ubuntu-16.04-amd64-disk1.vmdk (UUID: c36a7d63-db73-461c-b05d-093917e19e19)\nNIC 1:           MAC: 0800279C7307, Attachment: NAT, Cable connected: on, Trace: off (file: none), Type: 82540EM, Reported speed: 0 Mbps, Boot priority: 0, Promisc Policy: deny, Bandwidth group: none\nNIC 1 Settings:  MTU: 0, Socket (send: 64, receive: 64), TCP Window (send:64, receive: 64)\nNIC 1 Rule(0):   name = ssh, protocol = tcp, host ip = 127.0.0.1, host port = 2222, guest ip = , guest port = 22\nNIC 1 Rule(1):   name = tcp2376, protocol = tcp, host ip = 127.0.0.1, host port = 2376, guest ip = , guest port = 2376\nNIC 2:           MAC: 080027CF00AC, Attachment: Bridged Interface \'en0: Wi-Fi (AirPort)\', Cable connected: on, Trace: off (file: none), Type: 82540EM, Reported speed: 0 Mbps, Boot priority: 0, Promisc Policy: deny, Bandwidth group: none\nNIC 3:           disabled\nNIC 4:           disabled\nNIC 5:           disabled\nNIC 6:           disabled\nNIC 7:           disabled\nNIC 8:           disabled\nPointing Device: PS/2 Mouse\nKeyboard Device: PS/2 Keyboard\nUART 1:          disabled\nUART 2:          disabled\nLPT 1:           disabled\nLPT 2:           disabled\nAudio:           enabled (Driver: CoreAudio, Controller: AC97, Codec: STAC9700)\nClipboard Mode:  disabled\nDrag and drop Mode: disabled\nVRDE:            enabled (Address 127.0.0.1, Ports 5993, MultiConn: off, ReuseSingleConn: off, Authentication type: null)\nVideo redirection: disabled\nVRDE property: TCP/Ports  = "5993"\nVRDE property: TCP/Address = "127.0.0.1"\nVRDE property: VideoChannel/Enabled =  \nVRDE property: VideoChannel/Quality =  \nVRDE property: VideoChannel/DownscaleProtection =  \nVRDE property: Client/DisableDisplay =  \nVRDE property: Client/DisableInput =  \nVRDE property: Client/DisableAudio =  \nVRDE property: Client/DisableUSB =  \nVRDE property: Client/DisableClipboard =  \nVRDE property: Client/DisableUpstreamAudio =  \nVRDE property: Client/DisableRDPDR =  \nVRDE property: H3DRedirect/Enabled =  \nVRDE property: Security/Method =  \nVRDE property: Security/ServerCertificate =  \nVRDE property: Security/ServerPrivateKey =  \nVRDE property: Security/CACertificate =  \nVRDE property: Audio/RateCorrectionMode =  \nVRDE property: Audio/LogPath =  \nUSB:             enabled\nEHCI:            disabled\nXHCI:            disabled \n USB Device Filters: \n \n Bandwidth groups:   \n Shared folders: \n Name: \'vagrant\', Host path: \'/Users/malcolm/dev/bossjones/scarlett-ansible\' (machine mapping), writable \n Video capturing:    not active\nCapture screens:    0\nCapture file:       /Users/malcolm/VirtualBox VMs/scarlett-ansible-1604-packer2/scarlett-ansible-1604-packer2.webm\nCapture dimensions: 1024x768\nCapture rate:       512 kbps\nCapture FPS:        25 \n Guest: \n Configured memory balloon size:      0 MB\n```', 'scarlett-gstreamer-pocketsphinx-demo \n Basic demo to make sure all gstreamer + pocketsphinx dependencies were installed correctly, and STT works w/ pocketphinx gst plugin for scarlett \n if recording isnt working \n http://askubuntu.com/questions/61289/how-to-verify-if-my-microphone-input-is-dead-or-ubuntu-not-detected-it-yet-i-ca\nhttp://www.linux.org/threads/beats-audio-on-linux.4443/ \n IMPORTANT NOTE: \n Currently assumes  hmm  folder is located at  ./hmm  FYI. Will need to figure out if this is still required in newer versions of pocketpshinx or not. \n NOTE: \n This demo uses  autoenv  (see: https://github.com/kennethreitz/autoenv ) to set environment variables to find pocketphinx static libs, eg: \n using virtualenv: scarlett-dbus-poc scarlett-ansible in ~\n○ → ls -lta /home/pi/.virtualenvs/scarlett-dbus-poc/lib/\ntotal 6324\ndrwxrwxr-x 2 pi pi    4096 Dec 29 19:08 pkgconfig\ndrwxrwxr-x 2 pi pi    4096 Dec 29 19:08 gstreamer-1.0\ndrwxrwxr-x 5 pi pi    4096 Dec 29 19:08 .\n-rw-r--r-- 1 pi pi 1941766 Dec 29 19:08 libpocketsphinx.a\n-rwxr-xr-x 1 pi pi    1393 Dec 29 19:08 libpocketsphinx.la\nlrwxrwxrwx 1 pi pi      24 Dec 29 19:08 libpocketsphinx.so -> libpocketsphinx.so.3.0.0\nlrwxrwxrwx 1 pi pi      24 Dec 29 19:08 libpocketsphinx.so.3 -> libpocketsphinx.so.3.0.0\n-rwxr-xr-x 1 pi pi 1147601 Dec 29 19:08 libpocketsphinx.so.3.0.0\n-rw-r--r-- 1 pi pi   18938 Dec 29 18:59 libsphinxad.a\n-rwxr-xr-x 1 pi pi    1096 Dec 29 18:59 libsphinxad.la\nlrwxrwxrwx 1 pi pi      20 Dec 29 18:59 libsphinxad.so -> libsphinxad.so.3.0.0\nlrwxrwxrwx 1 pi pi      20 Dec 29 18:59 libsphinxad.so.3 -> libsphinxad.so.3.0.0\n-rwxr-xr-x 1 pi pi   21677 Dec 29 18:59 libsphinxad.so.3.0.0\n-rw-r--r-- 1 pi pi 2074892 Dec 29 18:59 libsphinxbase.a\n-rwxr-xr-x 1 pi pi    1049 Dec 29 18:59 libsphinxbase.la\nlrwxrwxrwx 1 pi pi      22 Dec 29 18:59 libsphinxbase.so -> libsphinxbase.so.3.0.0\nlrwxrwxrwx 1 pi pi      22 Dec 29 18:59 libsphinxbase.so.3 -> libsphinxbase.so.3.0.0\n-rwxr-xr-x 1 pi pi 1222864 Dec 29 18:59 libsphinxbase.so.3.0.0\ndrwxrwxr-x 8 pi pi    4096 Dec 24 12:09 ..\ndrwxrwxr-x 4 pi pi    4096 Dec 23 16:25 python2.7 \n If you don\'t have  autoenv  installed, simply run  source /path/to/scarlett-gstreamer-pocketsphinx-demo/.env \n gst-inspect-1.0 pocketsphinx default values: \n ```\n± |master ✓| → gst-inspect-1.0 pocketsphinx\nCurrent configuration:\n[NAME]          [DEFLT]     [VALUE]\n-agc            none        none\n-agcthresh      2.0     2.000000e+00\n-allphone\n-allphone_ci        no      no\n-alpha          0.97        9.700000e-01\n-ascale         20.0        2.000000e+01\n-aw         1       1\n-backtrace      no      no\n-beam           1e-48       1.000000e-48\n-bestpath       yes     yes\n-bestpathlw     9.5     9.500000e+00\n-ceplen         13      13\n-cmn            current     current\n-cmninit        8.0     40,3,-1\n-compallsen     no      no\n-debug                  0\n-dict                   /home/pi/.virtualenvs/scarlett-dbus-poc/share/pocketsphinx/model/en-us/cmudict-en-us.dict\n-dictcase       no      no\n-dither         no      no\n-doublebw       no      no\n-ds         1       1\n-fdict                  /home/pi/.virtualenvs/scarlett-dbus-poc/share/pocketsphinx/model/en-us/en-us/noisedict\n-feat           1s_c_d_dd   1s_c_d_dd\n-featparams             /home/pi/.virtualenvs/scarlett-dbus-poc/share/pocketsphinx/model/en-us/en-us/feat.params\n-fillprob       1e-8        1.000000e-08\n-frate          100     100\n-fsg\n-fsgusealtpron      yes     yes\n-fsgusefiller       yes     yes\n-fwdflat        yes     yes\n-fwdflatbeam        1e-64       1.000000e-64\n-fwdflatefwid       4       4\n-fwdflatlw      8.5     8.500000e+00\n-fwdflatsfwin       25      25\n-fwdflatwbeam       7e-29       7.000000e-29\n-fwdtree        yes     yes\n-hmm                    /home/pi/.virtualenvs/scarlett-dbus-poc/share/pocketsphinx/model/en-us/en-us\n-input_endian       little      little\n-jsgf\n-keyphrase\n-kws\n-kws_delay      10      10\n-kws_plp        1e-1        1.000000e-01\n-kws_threshold      1       1.000000e+00\n-latsize        5000        5000\n-lda\n-ldadim         0       0\n-lifter         0       22\n-lm                 /home/pi/.virtualenvs/scarlett-dbus-poc/share/pocketsphinx/model/en-us/en-us.lm.bin\n-lmctl\n-lmname\n-logbase        1.0001      1.000100e+00\n-logfn\n-logspec        no      no\n-lowerf         133.33334   1.300000e+02\n-lpbeam         1e-40       1.000000e-40\n-lponlybeam     7e-29       7.000000e-29\n-lw         6.5     6.500000e+00\n-maxhmmpf       30000       30000\n-maxwpf         -1      -1\n-mdef                   /home/pi/.virtualenvs/scarlett-dbus-poc/share/pocketsphinx/model/en-us/en-us/mdef\n-mean                   /home/pi/.virtualenvs/scarlett-dbus-poc/share/pocketsphinx/model/en-us/en-us/means\n-mfclogdir\n-min_endfr      0       0\n-mixw\n-mixwfloor      0.0000001   1.000000e-07\n-mllr\n-mmap           yes     yes\n-ncep           13      13\n-nfft           512     512\n-nfilt          40      25\n-nwpen          1.0     1.000000e+00\n-pbeam          1e-48       1.000000e-48\n-pip            1.0     1.000000e+00\n-pl_beam        1e-10       1.000000e-10\n-pl_pbeam       1e-10       1.000000e-10\n-pl_pip         1.0     1.000000e+00\n-pl_weight      3.0     3.000000e+00\n-pl_window      5       5\n-rawlogdir\n-remove_dc      no      no\n-remove_noise       yes     yes\n-remove_silence     yes     yes\n-round_filters      yes     yes\n-samprate       16000       1.600000e+04\n-seed           -1      -1\n-sendump                /home/pi/.virtualenvs/scarlett-dbus-poc/share/pocketsphinx/model/en-us/en-us/sendump\n-senlogdir\n-senmgau\n-silprob        0.005       5.000000e-03\n-smoothspec     no      no\n-svspec                 0-12/13-25/26-38\n-tmat                   /home/pi/.virtualenvs/scarlett-dbus-poc/share/pocketsphinx/model/en-us/en-us/transition_matrices\n-tmatfloor      0.0001      1.000000e-04\n-topn           4       4\n-topn_beam      0       0\n-toprule\n-transform      legacy      dct\n-unit_area      yes     yes\n-upperf         6855.4976   6.800000e+03\n-uw         1.0     1.000000e+00\n-vad_postspeech     50      50\n-vad_prespeech      20      20\n-vad_startspeech    10      10\n-vad_threshold      2.0     2.000000e+00\n-var                    /home/pi/.virtualenvs/scarlett-dbus-poc/share/pocketsphinx/model/en-us/en-us/variances\n-varfloor       0.0001      1.000000e-04\n-varnorm        no      no\n-verbose        no      no\n-warp_params\n-warp_type      inverse_linear  inverse_linear\n-wbeam          7e-29       7.000000e-29\n-wip            0.65        6.500000e-01\n-wlen           0.025625    2.562500e-02 \n Factory Details:\n  Rank                     none (0)\n  Long-name                PocketSphinx\n  Klass                    Filter/Audio\n  Description              Convert speech to text\n  Author                   CMUSphinx-devel  cmusphinx-devel@lists.sourceforge.net \n Plugin Details:\n  Name                     pocketsphinx\n  Description              PocketSphinx plugin\n  Filename                 /home/pi/.virtualenvs/scarlett-dbus-poc/lib/gstreamer-1.0/libgstpocketsphinx.so\n  Version                  5prealpha\n  License                  BSD\n  Source module            pocketsphinx\n  Binary package           PocketSphinx\n  Origin URL               http://cmusphinx.sourceforge.net/ \n GObject\n +----GInitiallyUnowned\n       +----GstObject\n             +----GstElement\n                   +----GstPocketSphinx \n Pad Templates:\n  SINK template: \'sink\'\n    Availability: Always\n    Capabilities:\n      audio/x-raw\n                 format: { S16LE }\n               channels: 1\n                   rate: 16000 \n SRC template: \'src\'\n    Availability: Always\n    Capabilities:\n      text/plain \n Element Flags:\n  no flags set \n Element Implementation:\n  Has change_state() function: gst_element_change_state_func \n Element has no clocking capabilities.\nElement has no indexing capabilities.\nElement has no URI handling capabilities. \n Pads:\n  SINK: \'sink\'\n    Implementation:\n      Has chainfunc(): 0x7f8017bad1c0\n      Has custom eventfunc(): 0x7f8017bad160\n      Has custom queryfunc(): gst_pad_query_default\n      Has custom iterintlinkfunc(): gst_pad_iterate_internal_links_default\n    Pad Template: \'sink\'\n  SRC: \'src\'\n    Implementation:\n      Has custom eventfunc(): gst_pad_event_default\n      Has custom queryfunc(): gst_pad_query_default\n      Has custom iterintlinkfunc(): gst_pad_iterate_internal_links_default\n    Pad Template: \'src\' \n Element Properties:\n  name                : The name of the object\n                        flags: readable, writable\n                        String. Default: "pocketsphinx0"\n  parent              : The parent of the object\n                        flags: readable, writable\n                        Object of type "GstObject"\n  hmm                 : Directory containing acoustic model parameters\n                        flags: readable, writable\n                        String. Default: "/home/pi/.virtualenvs/scarlett-dbus-poc/share/pocketsphinx/model/en-us/en-us"\n  lm                  : Language model file\n                        flags: readable, writable\n                        String. Default: "/home/pi/.virtualenvs/scarlett-dbus-poc/share/pocketsphinx/model/en-us/en-us.lm.bin"\n  lmctl               : Language model control file (for class LMs)\n                        flags: readable, writable\n                        String. Default: null\n  lmname              : Language model name (to select LMs from lmctl)\n                        flags: readable, writable\n                        String. Default: null\n  dict                : Dictionary File\n                        flags: readable, writable\n                        String. Default: "/home/pi/.virtualenvs/scarlett-dbus-poc/share/pocketsphinx/model/en-us/cmudict-en-us.dict"\n  fsg                 : Finite state grammar file\n                        flags: readable, writable\n                        String. Default: null\n  fsg-model           : Finite state grammar object (fsg_model_t *)\n                        flags: writable\n                        Pointer. Write only\n  fwdflat             : Enable Flat Lexicon Search\n                        flags: readable, writable\n                        Boolean. Default: true\n  bestpath            : Enable Graph Search\n                        flags: readable, writable\n                        Boolean. Default: true\n  maxhmmpf            : Maximum number of HMMs searched per frame\n                        flags: readable, writable\n                        Integer. Range: 1 - 100000 Default: 30000\n  maxwpf              : Maximum number of words searched per frame\n                        flags: readable, writable\n                        Integer. Range: 1 - 100000 Default: -1\n  beam                : Beam width applied to every frame in Viterbi search\n                        flags: readable, writable\n                        Double. Range:              -1 -               1 Default:           1e-48\n  wbeam               : Beam width applied to phone transitions\n                        flags: readable, writable\n                        Double. Range:              -1 -               1 Default:           7e-29\n  pbeam               : Beam width applied to phone transitions\n                        flags: readable, writable\n                        Double. Range:              -1 -               1 Default:           1e-48\n  dsratio             : Evaluate acoustic model every N frames\n                        flags: readable, writable\n                        Integer. Range: 1 - 10 Default: 1\n  latdir              : Output Directory for Lattices\n                        flags: readable, writable\n                        String. Default: null\n  decoder             : The underlying decoder\n                        flags: readable\n                        Boxed pointer of type "PSDecoder"\n  configured          : Set this to finalize configuration\n                        flags: readable, writable\n                        Boolean. Default: true\n``` \n commandline test \n pocketsphinx_continuous -hmm /home/pi/.virtualenvs/scarlett-dbus-poc/share/pocketsphinx/model/en-us/en-us -lm 1602.lm -dict 1602.dic -samprate 16000/8000/48000 -inmic yes', 'karlandkeke-locustio \n Some basic load test scenarios for karlandkeke using locust.io', 'hubot-docker-slack \n Hubot Docker Slack Proof of Concept', 'ScarlettOS \n S.C.A.R.L.E.T.T is Tony Darks artificially programmed intelligent computer. She is programmed to speak with a female voice in a British accent. \n \n \n \n \n \n \n \n ScarlettOS Docker Image ( Dockerfile ) \n \n \n \n \n \n Free software: Apache 2.0 \n Documentation:  https://scarlett-os.readthedocs.io . \n \n # Features \n \n TODO \n \n Credits \n This package was created with  Cookiecutter  and the  audreyr/cookiecutter-pypackage  project template. \n INSTALL \n \n Fill this out soon \n \n Development \n Interactive bash:  docker exec -it scarlettos_master_1 bash -l \n |2.2.3|  using virtualenv: scarlett-os-venv2  Malcolms-MBP-3 in ~/dev/bossjones/scarlett_os\n± |feature-dev-container {5} U:1 ✗| → docker exec -it scarlettos_master_1 bash -l\npi  ⎇  feature-dev-container {5} U:1  ~/dev/bossjones-github/scarlett_os \n Debugging in VSCode \n Source: https://github.com/mikemcgowan/django-cms-plus/blob/57e3fa8ec35d73cdd937baac25f5201ec78bbdb9/README.md \n VSCode debug launch configuration: \n json\n{\n    "name": "Attach (Remote Debug)",\n    "type": "python",\n    "request": "attach",\n    "localRoot": "${workspaceRoot}",\n    "remoteRoot": "",\n    "port": 2222,\n    "secret": "my_secret",\n    "host": "localhost"\n} \n Ensure the following exists in  web/manage.py  immediately before  execute_from_command_line() : \n ```python \n https://stackoverflow.com/questions/41201438/debug-python-application-running-in-docker \n try:\n    import ptvsd\n    ptvsd.enable_attach(secret=\'my_secret\', address=(\'0.0.0.0\', 2222))\nexcept (OSError, ImportError):\n    pass\n``` \n Gstreamer Environment Variables \n Official docs: https://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer/html/gst-running.html \n Variable | Example | Description\n--- | --- | ---\n GST_PLUGIN_SYSTEM_PATH  |  GST_PLUGIN_SYSTEM_PATH=/usr/local/lib/gstreamer-1.0  | GStreamer will scan these paths for GStreamer plug-ins. These plug-ins will be loaded after the plug-ins in the GST_PLUGIN_PATH variable below.\n GST_DEBUG  |  GST_DEBUG=GST_AUTOPLUG:6,GST_ELEMENT_*:4  | This variable can be set to a list of debug options, which cause GStreamer to print out different types of debugging information to stderr. The variable takes a comma-separated list of "category_name:level" pairs to set specific levels for the individual categories. The level value ranges from 0 (nothing) to 9 .\n GST_DEBUG_DUMP_DOT_DIR  |  GST_DEBUG_DUMP_DOT_DIR=/tmp  | Set this environment variable to a path to turn on all #GST_DEBUG_BIN_TO_DOT_FILE or #GST_DEBUG_BIN_TO_DOT_FILE_WITH_TS calls and have the dot files in that location. This will only work if the application in question makes these calls in strategic places (like when the pipeline state changes or an error occurs)\n GST_REGISTRY_UPDATE  |  GST_REGISTRY_UPDATE=no  | Set this environment variable to "no" to prevent GStreamer from updating the plugin registry. This is useful for embedded device which is not updating the plugins frequently, it will save time when doing gst_init().\n G_DEBUG  |  G_DEBUG=fatal_warnings  | Useful GLib environment variable. Set G_DEBUG=fatal_warnings to make GStreamer programs abort when a critical warning such as an assertion failure occurs. This is useful if you want to find out which part of the code caused that warning to be triggered and under what circumstances. Simply set G_DEBUG as mentioned above and run the program in gdb (or let it core dump). Then get a stack trace in the usual way.', 'Cabbie \n Prerequisites \n \n python (>=2.7) \n pip \n git \n \n Installation \n To set up a virtual environment, run following commands in order: \n ```bash\nsudo pip install virtualenv\nsudo pip install virtualenvwrapper \n export WORKON_HOME=$HOME/.virtualenvs \n /usr/bin/ or /usr/local/bin/ depending on the system \n source /usr/bin/virtualenvwrapper.sh \n mkvirtualenv --distribute --no-site-package cabbie \n pip install -r requirements.txt\n``` \n To compile SASS: \n bash\ngem install sass\ngem install compass\nsass --watch cabbie/static/sass/:cabbie/static/css/ --compass \n To create a local development environment: \n bash\ncp cabbie/local_settings.py.template cabbie/local_settings.py \n To create a local testing environment with Ionic: \n bash\ncd ionic/cabbie-driver # or ionic/cabbie-passenger\nnpm install\nionic serve\ngulp watch \n Todo \n \n Add  is_accepted  field to the  Driver  model \n Dispatch estimate information to passenger and driver \n Permission control for REST API \n', 'osx-dotfiles', 'AWS create/destroy ELB  \n Manage AWS ELB. \n Requirements \n \n Ansible 2.0.1 or higher. \n Tested on Ubuntu 14.04 and Amazon 7 \n \n Role Variables \n | parameter             | required | default | choices | comments |\n| --------------------- | -------- | ------- | -------- |-------- |\n| aws_elb_subnets | yes| | | A list of VPC subnets to use when creating ELB. Zones should be empty if using this. |\n| aws_elb_scheme| no| internet-facing | internet-facing, internal|The scheme to use when creating the ELB. For a private VPC-visible ELB use \'internal\'. |\n| aws_elb_security_group_ids| | | |A list of security groups to apply to the elb |\n| aws_elb_instance_listeners| | | | List of ports/protocols for this ELB to listen on (see  vars | \n| aws_elb_ping_path| no | / | |The destination for the HTTP or HTTPS request. | \n| aws_elb_healthcheck_response_timeout|yes | | | The amount of time to wait when receiving a response from the health check, in seconds.| \n| aws_elb_ping_protocol | no| http| | The protocol to use to connect with the instance. Ping protocols: TCP, HTTP, HTTPS, and SSL|\n| aws_elb_ping_port | no| http| | The port to use to connect with the instance, as a protocol:port pair. If the load balancer fails to connect with the instance at the specified port within the configured response timeout period, the instance is considered unhealthy.|\n| aws_elb_healthcheck_interval| no | 10 | |The amount of time between health checks of an individual instance, in seconds. | \n| aws_elb_unhealthy_threshold| no | 5 | |The number of consecutive failed health checks that must occur before declaring an EC2 instance unhealthy. | \n| aws_elb_healthy_threshold | | no | 3| The number of consecutive successful health checks that must occur before declaring an EC2 instance healthy.| \n| aws_elb_cross_az_load_balancing| no | yes|yes, no | Distribute load across all configured Availability Zones | \n| aws_elb_draining_timeout| no |20 | | Wait a specified timeout allowing connections to drain before terminating an instance| \n| aws_resource_tags  | yes  |   | | a hash/dictionary of tags to add to the new instance or for starting/stopping instance by tag; \'{"key":"value"}\' and \'{"VREnv":"PROD","VRProject":"sample","VRTeam":"infra", "Name":"instance_name"}\' |\n| aws_wait_timeout | no | 600 | |  how long before wait gives up, in seconds | \n| state |  no |  present |present, absent| create or destroy elb  |\n| region |  yes |   || The AWS region to use. Must be specified if ec2_url is not used. If not specified then the value of the EC2_REGION environment variable, if any, is used. See http://docs.aws.amazon.com/general/latest/gr/rande.html#ec2_region  |\n| vivareal_project_build | yes | | | elb name | \n Ansible modules \n ec2_elb_lb \n Output variables \n ec2_load_balancer: created elb name \nhealthcheck_dns_name: dns name of elb\n \n Example Playbook \n - hosts: localhost\n  vars:\n    aws_resource_tags: {\n     \'Name\': \'my-elb-name\',\n     \'VREnv\': \'PROD\',\n     \'VRProject\': \'infra-ansible\',\n     \'VRTeam\': \'infra\'}\n    region: us-east-1\n    aws_elb_instance_listeners: [{protocol: \'http\',load_balancer_port: \'80\',instance_protocol: \'http\',instance_port: \'80\'}]\n    aws_elb_subnets: [\'subnet-0959b37f\']\n    aws_elb_security_group_ids: [\'sg-a25ea6db\']\n    aws_elb_ping_protocol: "tcp"\n    vivareal_project_name: my-elb-name\n    vivareal_build_version: 1\n    vivareal_project_build: "{{ vivareal_project_name }}-{{ vivareal_build_version}}\n  roles:\n    - { role: aws-elb }\n \n Destroy stack \n - hosts: localhost\n  vars:\n    vivareal_project_name: my-elb-name\n  roles:\n    - { role: aws-elb, state: absent }\n \n License \n BSD \n Author: \n Giancarlo Rubio ( gianrubio@gmail.com )', "scarlett-packer \n Packer templates for creating scarlett vagrant images, and potentially raspberry pi preseed.cfg's later. \n Packer  templates for  Vagrant  base boxes. \n I borrowed quite a bit of logic from https://github.com/kaorimatz/packer-templates ( Thank you for figuring out all the very difficult stuff ). His work allowed me to focus more on the desktop side of things. \n Usage \n Clone the repository: \n $ git clone https://github.com/bossjones/scarlett-packer && cd scarlett-packer\n \n Build a machine image from the template in the repository: \n $ packer build -only=virtualbox-iso ubuntu-16.04-amd64.json\n \n Add the built box to Vagrant: \n $ vagrant box add scarlett-1604 ubuntu-16.04-amd64-virtualbox.box\n \n Configuration \n You can configure each template to match your requirements by setting the following  user variables . \n User Variable       | Default Value | Description\n---------------------|---------------|----------------------------------------------------------------------------------------\n  compression_level  | 6             |  Documentation \n cpus               | 1             | Number of CPUs\n  disk_size          | 40000         |  Documentation \n headless           | 0             |  Documentation \n memory             | 512           | Memory size in MB\n  mirror             |               | A URL of the mirror where the ISO image is available \n Example \n Build an uncompressed Arch Linux vagrant box with a 4GB hard disk using the VirtualBox provider: \n $ packer build -only=virtualbox-iso -var compression_level=0 -var disk_size=4000 ubuntu-16.04-amd64.json\n \n Pre-built Boxes \n You can also use the pre-built boxes hosted on  Atlas . \n $ vagrant box add bossjones/scarlett-1604\n", 'boss-toolbox \n \n Bunch of tools I like to use for debugging shit \n FAQ \n Q: I tried to run perf and I got this. What do I do?: \n perf stat ls\nError:\nYou may not have permission to collect stats.\nConsider tweaking /proc/sys/kernel/perf_event_paranoid:\n -1 - Not paranoid at all\n  0 - Disallow raw tracepoint access for unpriv\n  1 - Disallow cpu events for unpriv\n  2 - Disallow kernel profiling for unpriv \n A: On your host ( READ: Not docker container ), run the following: \n sudo sh -c \'echo 1 >/proc/sys/kernel/perf_event_paranoid\' \n or \n sysctl -w kernel.perf_event_paranoid="1" \n PERSIST REBOOTS: \n sudo sh -c \'echo kernel.perf_event_paranoid=1 > /etc/sysctl.d/00-local.conf\' \n Q: What does any of that stuff mean? \n source: https://unix.stackexchange.com/questions/14227/do-i-need-root-admin-permissions-to-run-userspace-perf-tool-perf-events-ar \n What you can do with perf without being root depends on the  kernel.perf_event_paranoid  sysctl setting. \n kernel.perf_event_paranoid  = 2: you can\'t take any measurements. The perf utility might still be useful to analyse existing records with perf ls, perf report, perf timechart or perf trace. \n kernel.perf_event_paranoid  = 1: you can trace a command with perf stat or perf record, and get kernel profiling data. \n kernel.perf_event_paranoid  = 0: you can trace a command with perf stat or perf record, and get CPU event data. \n kernel.perf_event_paranoid  = -1: you get raw access to kernel tracepoints (specifically, you can  mmap  the file created by  perf_event_open , I don\'t know what the implications are). \n Q: Can you explain why you chose those docker run settings? \n ```\n--ipc=host \n By default, all containers have the IPC namespace enabled. \n IPC (POSIX/SysV IPC) namespace provides separation of named shared memory segments, semaphores and message queues. \n Shared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or through the network stack. Shared memory is commonly used by databases and custom-built (typically C/OpenMPI, C++/using boost libraries) high performance applications for scientific computing and financial services industries. If these types of applications are broken into multiple containers, you might need to share the IPC mechanisms of the containers.\n``` \n ```\n--net=host \n --network="bridge" : Connect a container to a network\n                      \'bridge\': create a network stack on the default Docker bridge\n                      \'none\': no networking\n                      \'container: \': reuse another container\'s network stack\n                      \'host\': use the Docker host network stack\n                      \' | \': connect to a user-defined network\n``` \n ```\n--pid=host \n By default, all containers have the PID namespace enabled. \n PID namespace provides separation of processes. The PID Namespace removes the view of the system processes, and allows process ids to be reused including pid 1. \n In certain cases you want your container to share the host’s process namespace, basically allowing processes within the container to see all of the processes on the system. For example, you could build a container with debugging tools like strace or gdb, but want to use these tools when debugging processes within the container.\n``` \n Additionally, the operator can set any environment variable in the container by using one or more -e flags, even overriding those mentioned above, or already defined by the developer with a Dockerfile ENV. If the operator names an environment variable without specifying a value, then the current value of the named variable is propagated into the container’s environment: \n https://www.projectatomic.io/blog/2015/09/introducing-the-fedora-tools-image-for-fedora-atomic-host/ \n https://fedoraproject.org/wiki/StackTraces', 'boss-docker-python3 \n Bossjones repo for Ubuntu 16.04 w/ Python 3.5.2 compiled from source. \n Build \n docker build -t bossjones/boss-docker-python3 .', "boss-docker-gnome-pygobject-gtk3-gst-cmusphinx-jhbuild \n Gnome x Jhbuild x PyGObject x Cmusphinx x Gtk+3 in 🐳 \n \n \n \n \n \n \n Jhbuild PyGObject3 Docker Image ( Dockerfile ) \n \n \n \n \n NOTE: This is a prereq for  scarlett_os . It makes some strong assumptions about how you plan on running jhbuild, and should mainly just run on CI systems. \n Docker container that installs an jhbuild environment that has the following: \n \n Python3 \n Jhbuild \n Glib \n Gobject-introspection \n Gstreamer \n Gst-Espeak-Plugin \n Gtk3 \n Pocketsphinx/Sphinxbase \n \n Compiling jhbuild and deps ( Prerequisites ) \n Building from tarball requires: \n \n gcc or clang ( for compiling ) \n libxft ( for font rendering ) \n libxinerama ( for XINERAMA support ) \n gdk-pixbuf2 ( for pixmap rendering ) \n libxrandr ( for XRANDR support ) \n libsndfile ( for sound support ) \n libsm ( for X11R6 session management ) \n fribidi ( for i18n text rendering ) \n \n Building from git also requires: \n \n git ( for cloning the git repository and updating changelogs ) \n autoconf/automake or cmake toolchain ( for build scripts ) \n xorg-mkfontdir ( for installing themes ) \n asciidoctor or asciidoc ( for creating html documentation ) \n markdown ( for building release package ) \n \n Build \n docker build -t docker-gnome-pygobject-gtk3-gst-cmusphinx-jhbuild . \n Links \n \n https://github.com/search?q=execlineb+sshd&type=Code&utf8=%E2%9C%93 \n \n Order of operations \n jhbuild_pygobject3_1  | [init] no run.d scripts\njhbuild_pygobject3_1  | [run] starting process manager\njhbuild_pygobject3_1  | [s6-init] making user provided files available at /var/run/s6/etc...exited 0.\njhbuild_pygobject3_1  | [s6-init] ensuring user provided files have correct perms...exited 0.\njhbuild_pygobject3_1  | [fix-attrs.d] applying ownership & permissions fixes...\njhbuild_pygobject3_1  | [fix-attrs.d] done.\njhbuild_pygobject3_1  | [cont-init.d] executing container initialization scripts...\njhbuild_pygobject3_1  | [cont-init.d] 00-init-ssh: executing...\njhbuild_pygobject3_1  | [cont-init.d] 00-init-ssh: exited 0.\njhbuild_pygobject3_1  | [cont-init.d] done.\njhbuild_pygobject3_1  | [services.d] starting services\njhbuild_pygobject3_1  | [services.d] done. \n Environment Variables \n Variable | Example | Description\n--- | --- | ---\n S6_KILL_FINISH_MAXTIME  |  S6_KILL_FINISH_MAXTIME=1  | Wait time (in ms) for zombie reaping before sending a kill signal\n S6_KILL_GRACETIME  |  S6_KILL_GRACETIME=1  | Wait time (in ms) for S6 finish scripts before sending kill signal\n SERVER_LOG_MINIMAL  |  SERVER_LOG_MINIMAL=1  | Wait time (in ms) for S6 finish scripts before sending kill signal\n SERVER_APP_NAME  |  SERVER_APP_NAME=jhbuild-compile  | Set application name for stdout logging info\n COMPOSE_PROJECT_NAME  |  COMPOSE_PROJECT_NAME=jhbuild-compile  | The default project name is the basename of the project directory. You can set a custom project name by using the -p command line option or the this environment variable.\n SCARLETT_ENABLE_SSHD  |  SCARLETT_ENABLE_SSHD=1  | When set to 0, openssh-server will be enabled for development use w/ VSCode or Sublime\n SCARLETT_ENABLE_DBUS  |  SCARLETT_ENABLE_DBUS='true'  | When set, a session dbus service will be started\n SCARLETT_BUILD_GNOME  |  SCARLETT_BUILD_GNOME='true'  | When set, jhbuild and deps will be compiled\n TRAVIS_CI  |  TRAVIS_CI='true'  | Signal s6 to stop when finished all run.d scripts. Important for CI builds. \n \n with-contenv  tool, which is used to expose environment variables across scripts, has a limitation that it cannot read beyond 4k characters for environment variable values. To work around this issue, use the script  /scripts/with-bigcontenv  instead of  with-contenv . You'll need to remove the  with-contenv  from the shebang line, and add   source /scripts/with-bigcontenv  in the next line after the shebang line. \n \n Startup/Runtime Modification \n To inject changes just before runtime, shell scripts may be placed into the\n /etc/cont-init.d  folder.\nAs part of the process manager, these scripts are run in advance of the supervised processes. @see https://github.com/just-containers/s6-overlay#executing-initialization-andor-finalization-tasks \n Optional Arguments \n Variable | Example | Description\n--- | --- | ---\n SCARLETT_ENABLE_SSHD  |  SCARLETT_ENABLE_SSHD=0  | When set to 0, openssh-server will be enabled for development use w/ VSCode or Sublime\n SCARLETT_ENABLE_DBUS  |  SCARLETT_ENABLE_DBUS='true'  | When set, a session dbus service will be started\n SCARLETT_BUILD_GNOME  |  SCARLETT_BUILD_GNOME='true'  | When set, jhbuild and deps will be compiled\n TRAVIS_CI  |  TRAVIS_CI='true'  | Signal s6 to stop when finished all run.d scripts. Important for CI builds. \n ```", "boss-docker-base-gtk3-deps \n Docker container that installs a bunch of gtk3 packages in preparation for usage with jhbuild. All of these are sytem packages installed by  apt . This is a prereq for  jhbuild  environments. \n boss-docker-base-gtk3-deps \n Gnome x Jhbuild x PyGObject x Cmusphinx x Gtk+3 in 🐳 \n \n \n \n \n \n \n \n Base GTK3 Deps Docker Image ( Dockerfile ) \n \n \n \n \n NOTE: This is a prereq for  scarlett_os  and  boss-docker-jhbuild-pygobject3 . It makes some strong assumptions about how you plan on running jhbuild, and should mainly just run on CI systems. \n Docker container that installs the following apt dependencies an jhbuild environment that has the following: \n \n Python3 \n Glib \n Gobject-introspection \n Gstreamer \n Gst-Espeak-Plugin \n Gtk3 \n Pocketsphinx/Sphinxbase \n \n Build \n docker build -t bossjones/boss-docker-base-gkt3-deps . \n Links \n \n https://github.com/search?q=execlineb+sshd&type=Code&utf8=%E2%9C%93 \n \n Order of operations \n jhbuild_pygobject3_1  | [init] no run.d scripts\njhbuild_pygobject3_1  | [run] starting process manager\njhbuild_pygobject3_1  | [s6-init] making user provided files available at /var/run/s6/etc...exited 0.\njhbuild_pygobject3_1  | [s6-init] ensuring user provided files have correct perms...exited 0.\njhbuild_pygobject3_1  | [fix-attrs.d] applying ownership & permissions fixes...\njhbuild_pygobject3_1  | [fix-attrs.d] done.\njhbuild_pygobject3_1  | [cont-init.d] executing container initialization scripts...\njhbuild_pygobject3_1  | [cont-init.d] 00-init-ssh: executing...\njhbuild_pygobject3_1  | [cont-init.d] 00-init-ssh: exited 0.\njhbuild_pygobject3_1  | [cont-init.d] done.\njhbuild_pygobject3_1  | [services.d] starting services\njhbuild_pygobject3_1  | [services.d] done. \n Environment Variables \n Variable | Example | Description\n--- | --- | ---\n S6_KILL_FINISH_MAXTIME  |  S6_KILL_FINISH_MAXTIME=1  | Wait time (in ms) for zombie reaping before sending a kill signal\n S6_KILL_GRACETIME  |  S6_KILL_GRACETIME=1  | Wait time (in ms) for S6 finish scripts before sending kill signal\n SERVER_LOG_MINIMAL  |  SERVER_LOG_MINIMAL=1  | Wait time (in ms) for S6 finish scripts before sending kill signal\n SERVER_APP_NAME  |  SERVER_APP_NAME=jhbuild-compile  | Set application name for stdout logging info\n COMPOSE_PROJECT_NAME  |  COMPOSE_PROJECT_NAME=jhbuild-compile  | The default project name is the basename of the project directory. You can set a custom project name by using the -p command line option or the this environment variable.\n SCARLETT_ENABLE_SSHD  |  SCARLETT_ENABLE_SSHD=1  | When set to 0, openssh-server will be enabled for development use w/ VSCode or Sublime\n SCARLETT_ENABLE_DBUS  |  SCARLETT_ENABLE_DBUS='true'  | When set, a session dbus service will be started\n SCARLETT_BUILD_GNOME  |  SCARLETT_BUILD_GNOME='true'  | When set, jhbuild and deps will be compiled\n TRAVIS_CI  |  TRAVIS_CI='true'  | Signal s6 to stop when finished all run.d scripts. Important for CI builds. \n \n with-contenv  tool, which is used to expose environment variables across scripts, has a limitation that it cannot read beyond 4k characters for environment variable values. To work around this issue, use the script  /scripts/with-bigcontenv  instead of  with-contenv . You'll need to remove the  with-contenv  from the shebang line, and add   source /scripts/with-bigcontenv  in the next line after the shebang line. \n \n Startup/Runtime Modification \n To inject changes just before runtime, shell scripts may be placed into the\n /etc/cont-init.d  folder.\nAs part of the process manager, these scripts are run in advance of the supervised processes. @see https://github.com/just-containers/s6-overlay#executing-initialization-andor-finalization-tasks \n Optional Arguments \n Variable | Example | Description\n--- | --- | ---\n SCARLETT_ENABLE_SSHD  |  SCARLETT_ENABLE_SSHD=0  | When set to 0, openssh-server will be enabled for development use w/ VSCode or Sublime\n SCARLETT_ENABLE_DBUS  |  SCARLETT_ENABLE_DBUS='true'  | When set, a session dbus service will be started\n SCARLETT_BUILD_GNOME  |  SCARLETT_BUILD_GNOME='true'  | When set, jhbuild and deps will be compiled\n TRAVIS_CI  |  TRAVIS_CI='true'  | Signal s6 to stop when finished all run.d scripts. Important for CI builds. \n ```", '\n gosa \n Test', 'NOTE: \n This readme, and most likely a lot of the work in this repo were probably borrowed from the awesome work that  docent-net/fedora-desktop-ansible  did. \n oh-my-fedora24 \n This is my provisioner for the Fedora 24 Gnome3 / Wayland\ngeneric desktop. Feel free to use it, modify, contribute etc.\nAlways interested in your comments - write me! \n This repository has submodule dependency. All Ansible roles are placed in  roles  directory\nwhich is a git submodule. \n Usage: \n \n Clone this repo \n Edit  group_vars/all.yml \n Install prerequisites on destination box:  dnf -y install ansible sudo python-dnf libselinux-python \n Create entry for your user in  /etc/sudoers.d/user \n Enter proper  ansible_ssh_host  and  ansible_ssh_user  in  hosts  file \n Make sure it works:  ansible -m ping <your_host> \n Rollout:  ansible-playbook playbook.yml \n \n Of course you may install just a specified part of the\nwhole installation by using tags. List available tags\nwith: \n $ ansible-playbook playbook.yml --list-tags \n And install specified part with: \n $ ansible-playbook playbook.yml --tags dropbox,virtualbox \n If you want to only update packages (like  dnf update -y  simply run: \n $ ansible-playbook playbook.yml --tags pkgs,update --skip-tags install \n Contributing / committing changes \n \n In order to modify those roles a bit to match your needs simply fork and work on your forked repo \n If you feel like your change should be added to thise repo - create Pull Request (thanks btw!) \n \n Managing multiple desktops \n I use this repository in order to manage 5+ laptops. There are two methods to achieve this\nmulti - desktop configuration: \n \n First is to keep every workstation configuration in different branch. This way every workstation\n   has its own  playbook.yml  and  hosts  file. Downside is the need for keeping branches in sync\n   (which is a basic merge so it\'s not a rocket science). \n Second is to keep every workstation provisioner in different playbook (e.g. in  plays  directory).\n   In this scenario you keep all hosts configuration in one  hosts  file. \n \n Malcolm Jones\n@bossjones \n https://github.com/geerlingguy/ansible-role-solr/issues/64 \n Known Problems \n ansible \n https://github.com/geerlingguy/ansible-role-solr/issues/64\nhttps://github.com/ansible/ansible/issues/23358 \n ```\nTASK [ksylvan.docker : fedora_repo]  * * * * * * **\nok: [hyena_org] => {\n    "msg": {\n        "age": "215",\n        "changed": false,\n        "connection": "close",\n        "content": "<!DOCTYPE html>\\n\\n Index of /repo/main/fedora/ \\n\\n Index of /repo/main/fedora/ \\n ../ \\n 20/                                          01-Dec-2016 19:36  -\\n 21/                                          01-Dec-2016 19:36  -\\n 22/                                          01-Dec-2016 19:36  -\\n 23/                                          10-Jan-2017 23:53  -\\n 24/                                          05-Apr-2017 23:12  -\\n 25/                                          05-Apr-2017 23:12  -\\n \\n",\n        "content_length": "687",\n        "content_type": "text/html",\n        "date": "Thu, 06 Apr 2017 01:34:13 GMT",\n        "etag": "\\"8cb7fa660761d3b6a8a824f2bf396dc0\\"",\n        "last_modified": "Thu, 06 Apr 2017 00:39:19 GMT",\n        "msg": "OK (687 bytes)",\n        "redirected": false,\n        "server": "AmazonS3",\n        "status": 200,\n        "url": "https://yum.dockerproject.org/repo/main/fedora/",\n        "via": "1.1 9aaf336897fdd8a2dfd1b375c61d8b0b.cloudfront.net (CloudFront)",\n        "x_amz_cf_id": "B9bF3GbU36hAYJhtIDXoQGXu9EiYcUUSXEWDDzyxMPzEakpPchZELw==",\n        "x_amz_meta_s3cmd_attrs": "uid:0/gname:root/uname:root/gid:0/mode:33261/mtime:1491434020/atime:1491434299/md5:8cb7fa660761d3b6a8a824f2bf396dc0/ctime:1491434020",\n        "x_amz_version_id": "hCdPIsuNZF5gW6GfubBq2gcvxNtxCAkw",\n        "x_cache": "Hit from cloudfront"\n    }\n}\n TASK [ksylvan.docker : ansible_distribution_version]  * * * * *\nok: [hyena_org] => {\n    "msg": "24"\n} \n TASK [ksylvan.docker : Look for the exact version]  * * * * *\nfatal: [hyena_org]: FAILED! => {"failed": true, "msg": "The conditional check \'>{{ansible_distribution_version}}/< in {{fedora_repo.content}}\' failed. The error was: Invalid conditional detected: invalid syntax ( , line 1)\\n\\nThe error appears to have been in \'/Users/malcolm/dev/bossjones/oh-my-fedora24/roles/ksylvan.docker/tasks/main.yml\': line 27, column 3, but may\\nbe elsewhere in the file depending on the exact syntax problem.\\n\\nThe offending line appears to be:\\n\\n\\n- name: Look for the exact version\\n  ^ here\\n"}\n        to retry, use: --limit @/Users/malcolm/dev/bossjones/oh-my-fedora24/playbook.retry \n PLAY RECAP  * * * * * * * * * * *\nhyena_org                  : ok=13   changed=0    unreachable=0    failed=1 \n Ansible failed to complete successfully. Any error output should be\nvisible above. Please fix these errors and try again. \n ``` \n https://jedi.readthedocs.io/en/latest/docs/usage.html#tab-completion-in-the-python-shell \n ``` \n ~/.pythonrc.py \n try:\n    from jedi.utils import setup_readline\n    setup_readline()\nexcept ImportError:\n    # Fallback to the stdlib readline completer if it is installed.\n    # Taken from http://docs.python.org/2/library/rlcompleter.html\n    print("Jedi is not installed, falling back to readline")\n    try:\n        import readline\n        import rlcompleter\n        readline.parse_and_bind("tab: complete")\n    except ImportError:\n        print("Readline is not installed either. No tab completion is enabled.")\n``` \n dnf -y copr enable dperson/neovim\ndnf -y install neovim\ndnf -y install python3-neovim python3-neovim-gui \n ```\nWARNING: IPv4 forwarding is disabled\nWARNING: bridge-nf-call-iptables is disabled\nWARNING: bridge-nf-call-ip6tables is disabled \n net.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1 \n net.ipv6.conf.default.router_solicitations = 0\nnet.ipv6.conf.default.accept_ra_rtr_pref = 0\nnet.ipv6.conf.default.accept_ra_pinfo = 0\nnet.ipv6.conf.default.accept_ra_defrtr = 0\nnet.ipv6.conf.default.autoconf = 0\nnet.ipv6.conf.default.dad_transmits = 0\nnet.ipv6.conf.default.max_addresses = 1 \n /sbin/sysctl -w kernel.domainname="example.com" \n /sbin/sysctl -w net.bridge.bridge-nf-call-iptables="1"\n``` \n ```\nWARNING: IPv4 forwarding is disabled\nWARNING: bridge-nf-call-iptables is disabled\nWARNING: bridge-nf-call-ip6tables is disabled \n net.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1 \n net.ipv6.conf.default.router_solicitations = 0\nnet.ipv6.conf.default.accept_ra_rtr_pref = 0\nnet.ipv6.conf.default.accept_ra_pinfo = 0\nnet.ipv6.conf.default.accept_ra_defrtr = 0\nnet.ipv6.conf.default.autoconf = 0\nnet.ipv6.conf.default.dad_transmits = 0\nnet.ipv6.conf.default.max_addresses = 1 \n /sbin/sysctl -w kernel.domainname="example.com" \n /sbin/sysctl -w net.bridge.bridge-nf-call-iptables="1"\n``` \n Fix: Re-associate vagrant with vm \n vmdk gets misplaced \n https://github.com/hashicorp/vagrant/issues/1755 \n Example fix ( for above ) \n VBoxManage list vms | grep hyena | cut -d" " -f2 | sed \'s,{,,g\' | sed \'s,},,g\' > ~/dev/bossjones/oh-my-fedora24/.vagrant/machines/hyena_org/virtualbox/id \n Folder structure example \n |2.2.3|    hyenatop in ~/dev/user/scarlett-ansible/.vagrant/machines/default/virtualbox\n± |featutre-1604 ?:1 ✗| → ls\naction_provision action_set_name  creator_uid      id               index_uuid       synced_folders', 'reproduce_pytest_mock_issue_84 \n reproduce_pytest_mock_issue_84 \n Disclaimer \n I put this repo together pretty quickly, in between the work week in an effort to repoduce the issues I saw while working w/  pytest  and  pytest-mock  for my personal project,  scarlett_os . I left a lot of the modules that I originally used in this repo, along w/ some other things that might be a bit unecessary. The purpose of that was to create an environment that was as close as possible to what I\'m actually using ... just minus some of the complexity. \n Requirements \n \n Docker For Mac ( Also tested on fedora 24 in virtualbox ) \n \n Specifically i\'m using: \n ```\n$ docker --version\nDocker version 1.12.5, build 7392c3b \n $ docker-compose --version\ndocker-compose version 1.9.0, build 2585387 \n |2.2.3|   Malcolms-MBP-3 in ~/dev\n○ →\n``` \n Setup \n 1. Start docker container via docker-compose \n make docker-compose \n 2. Exec into the container using bash \n make docker-exec \n 3. Once in the container, run bootstrap command to install dependencies \n ``` \n from inside container \n make bootstrap\n``` \n 4. Enable virtualenv and run tests \n A. How to repoduce error \n ``` \n from inside container \n workon repoduce_pytest_mock_issue_84\nmake test\n``` \n B. Run tests w/ mocker.stopall() to fix "leak" \n When you set this environment variable, mocker.stopall() runs at the beginning and end of each test case. \n ``` \n from inside container \n workon repoduce_pytest_mock_issue_84\nENABLE_STOPALL=1 make test\n``` ', 'docker-java-jmx-newrelic-demo \n Just a demo application to visualize the type of statistics we can get from running jmx or newrelic in java', 'docker-scala-jmx-newrelic-demo \n Just a demo application to visualize the type of statistics we can get from running jmx or newrelic in scala', 'Gnome Builder (from master branch) Image based on Fedora Rawhide \n What do I need? \n \n local ip address \n How do I get it? \n ifconfig en0 | grep "inet "| awk \'{print $2}\' \n    or  \n npm install -g my-local-ip \n \n \n \n \n socat \n How do I get it? \n brew install socat \n <package-manager> install socat \n \n \n \n \n XQuartz \n How do I get it? \n brew cask install xquartz \n \n \n \n \n \n Helpful aliases \n alias docker_x11_socat=\'socat TCP-LISTEN:6000,reuseaddr,fork UNIX-CLIENT:\\"$DISPLAY\\"\'\nalias docker_x11_jess_geary=\'docker run --rm --name geary -e DISPLAY=`ifconfig en0 | grep "inet "  | cut -d" " -f2`:0 jess/geary\'\nalias my_ip=\'ifconfig en0 | grep "inet "  | cut -d" " -f2\'\nalias docker_eclipse=\'docker run -it -e DISPLAY=`ifconfig en0 | grep "inet "  | cut -d" " -f2`:0 batmat/docker-eclipse\'\nalias docker_xquartz=\'open -a XQuartz\' \n Prereq \n - Start Up socat: `alias docker_x11_socat=\'socat TCP-LISTEN:6000,reuseaddr,fork UNIX-CLIENT:\\"$DISPLAY\\"\'`\n \n I am not a GNOME developer, but found no easy way to try Builder on my machine.\nSo I decided to create this image. Simple like this. \n To execute it: \n $ docker run --rm -it -e DISPLAY=$DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix leandrosansilva/gnome-builder \n $ MY_IP=$(ifconfig en0 | grep "inet "| awk \'{print $2}\') \\\n  docker run --rm -e DISPLAY=$MY_IP:0 \\\n  -i -t \\\n  bossjones/gnome-builder-docker-image bash \n MY_IP=$(ifconfig en0 | grep "inet "| awk \'{print $2}\') \\\n  docker run --rm -e DISPLAY=192.168.0.3:0 \\\n  -i -t \\\n  -v /Users/timlinux:/home/timlinux \\\n    kartoza/qgis-desktop qgis \n docker run --rm -it -e DISPLAY=192.168.1.129:0 \n docker build -t kartoza/qgis-desktop git://github.com/kartoza/docker-qgis-desktop \n docker build -t bossjones/gnome-builder . \n PS: This image is  huge , about 2GB in size.', 's6-lab \n Series of scripts to test out different s6-overlay execlineb scripts. Dev purposes only.', 'Scarlett-Devpi-Docker \n Inspired by: \n \n https://github.com/scrapinghub/docker-devpi \n https://gist.github.com/wassname/18895c4d62ed842fba1f \n https://github.com/muccg/docker-devpi \n \n This repository contains  Dockerfile  of  Devpi  for  Docker \'s  trusted build  published to the public  Docker Registry . \n Dependencies \n \n dockerfile/ubuntu \n \n Installation \n \n \n Install  Docker . \n \n \n Download  trusted build  from public  Docker Registry :  docker pull scrapinghub/devpi \n \n \n (alternatively, you can build an image from Dockerfile:  docker build -t="scrapinghub/devpi" github.com/scrapinghub/docker-devpi ) \n Usage \n Run  devpi-server \n docker run -d --name devpi -p 3141:3141 scrapinghub/devpi\n \n Devpi creates a user named  root  by default, its password can be set with\n DEVPI_PASSWORD  environment variable.', 'boss-cheatsheets \n cheat  allows you to create and view interactive cheatsheets on the\ncommand-line. It was designed to help remind *nix system administrators of\noptions for commands that they use frequently, but not frequently enough to\nremember. \n This repo contains a number of commands that bossjones likes to use when debugging things, feel free to contribute to the list by making a PR! \n Install custom cheatsheets \n ```sh \n clone this repo somewhere on your laptop \n git clone https://github.com/bossjones/boss-cheatsheets ~/dev/behance/devops-cheatsheets \n Edit the cheat yaml config to reference the location of your cheatsheets \n ~/dev/bossjones/boss-cheatsheets feature-git-search-history\n❯ cat ~/.config/cheat/conf.yml \n \n The editor to use with \'cheat -e  \'. Defaults to $EDITOR or $VISUAL. \n editor: vim \n Should \'cheat\' always colorize output? \n colorize: true \n Which \'chroma\' colorscheme should be applied to the output? \n Options are available here: \n https://github.com/alecthomas/chroma/tree/master/styles \n style: monokai \n Which \'chroma\' "formatter" should be applied? \n One of: "terminal", "terminal256", "terminal16m" \n formatter: terminal16m \n Through which pager should output be piped? (Unset this key for no pager.) \n pager: less -FRX \n The paths at which cheatsheets are available. Tags associated with a cheatpath \n are automatically attached to all cheatsheets residing on that path. \n \n Whenever cheatsheets share the same title (like \'tar\'), the most local \n cheatsheets (those which come later in this file) take precedent over the \n less local sheets. This allows you to create your own "overides" for \n "upstream" cheatsheets. \n \n But what if you want to view the "upstream" cheatsheets instead of your own? \n Cheatsheets may be filtered via \'cheat -t  \' in combination with other \n commands. So, if you want to view the \'tar\' cheatsheet that is tagged as \n \'community\' rather than your own, you can use: cheat tar -t community \n cheatpaths: \n # Paths that come earlier are considered to be the most "global", and will\n  # thus be overridden by more local cheatsheets. That being the case, you\n  # should probably list community cheatsheets first.\n  #\n  # Note that the paths and tags listed below are placeholders. You may freely\n  # change them to suit your needs.\n  #\n  # Community cheatsheets must be installed separately, though you may have\n  # downloaded them automatically when installing \'cheat\'. If not, you may\n  # download them here:\n  #\n  # https://github.com/cheat/cheatsheets\n  #\n  # Once downloaded, ensure that \'path\' below points to the location at which\n  # you downloaded the community cheatsheets. \n \n name: behance                   # a name for the cheatpath\n    path: /Users/malcolm/dev/behance/devops-cheatsheets # the path\'s location on the filesystem\n    tags: [ behance ]               # these tags will be applied to all sheets on the path\n    readonly: false                    # if true,  cheat  will not create new cheatsheets here \n \n # - name: personal\n  #   path: /Users/malcolm/documents/cheat/personal  # this is a separate directory and repository than above\n  #   tags: [ personal ]\n  #   readonly: false                   # new sheets may be written here \n \n name: community\n    path: /Users/malcolm/.config/cheat/cheatsheets/community\n    tags: [ community ]\n    readonly: true \n \n # # If you have personalized cheatsheets, list them last. They will take\n  # # precedence over the more global cheatsheets.\n  # - name: personal\n  #   path: /Users/malcolm/.config/cheat/cheatsheets/personal\n  #   tags: [ personal ]\n  #   readonly: false \n # While it requires no configuration here, it\'s also worth noting that\n  # \'cheat\' will automatically append directories named \'.cheat\' within the\n  # current working directory to the \'cheatpath\'. This can be very useful if\n  # you\'d like to closely associate cheatsheets with, for example, a directory\n  # containing source code.\n  #\n  # Such "directory-scoped" cheatsheets will be treated as the most "local"\n  # cheatsheets, and will override less "local" cheatsheets. Likewise,\n  # directory-scoped cheatsheets will always be editable (\'readonly: false\'). \n ``` \n Update custom cheatsheets \n ```sh \n update cheatsheets \n pushd ~/.cheat\ngit pull\npopd \n View cheatsheets \n cheat -l\n``` \n Example \n Forgot all of your go-to commands for sysdig? No problem, just run: \n sh\ncheat sysdig \n You will be presented with a cheatsheet resembling: \n ```sh \n Basic Command List \n Capture all the events from the live system and print them to screen \n sysdig \n Capture all the events from the live system and save them to disk \n sysdig -qw dumpfile.scap \n Read events from a file and print them to screen \n sysdig -r dumpfile.scap \n Print all the open system calls invoked by cat \n sysdig proc.name=cat and evt.type=open \n Print the name of the files opened by cat \n ./sysdig -p"%evt.arg.name" proc.name=cat and evt.type=open \n List the available chisels \n ./sysdig -cl \n Run the spy_ip chisel for the 192.168.1.157 IP address: \n sysdig -c spy_ip 192.168.1.157 \n Output Format \n By default, sysdig prints the information for each captured event on a single line, with the following format: \n             \n where: evt.time is the event timestamp evt.cpu is the CPU number where the event was captured proc.name is the name of the process that generated the event thread.tid id the TID that generated the event, which corresponds to the PID for single thread processes evt.dir is the event direction, > for enter events and < for exit events evt.type is the name of the event, e.g. \'open\' or \'read\' evt.args is the list of event arguments. \n The output format can be customized with the -p switch, using any of the fields listed by \'sysdig -l\'. \n Filtering \n sysdig filters are specified at the end of the command line. The simplest filter is a simple field-value check: \n sysdig proc.name=cat \n The list of available fields can be obtained with \'sysdig -l\'. Checks can use one of these comparison operators: =, !=, <, <=, >, >= and contains. e.g. \n sysdig fd.name contains /etc \n Multiple checks can be combined through parentheses and the following boolean operators: and, or, not. e.g. \n sysdig "not(fd.name contains /proc or fd.name contains /dev)" \n Chisels \n Sysdig’s chisels are little scripts that analyze the sysdig event stream to perform useful actions. To get the list of available chisels, type \n sysdig –cl \n For each chisel, you get the description and the list of arguments it expects. To run one of the chisels, you use the –c flag, e.g.: \n sysdig –c topfiles_bytes \n If a chisel needs arguments, you specify them after the chisel name: \n sysdig –c spy_ip 192.168.1.157 \n Chiesls can be combined with filters: \n sysdig -c topfiles_bytes "not fd.name contains /dev" \n View the list of containers running on the machine and their resource usage \n csysdig -vcontainers \n View the list of processes with container context \n csysdig -pc \n See all the GET HTTP requests made by the machine \n sysdig -s 2000 -A -c echo_fds fd.port=80 and evt.buffer contains GET \n See all the SQL select queries made by the machine \n sysdig -s 2000 -A -c echo_fds evt.buffer contains SELECT \n See queries made via apache to an external MySQL server happening in real time \n sysdig -s 2000 -A -c echo_fds fd.sip=192.168.30.5 and proc.name=apache2 and evt.buffer contains SELECT \n To get the list of available chisels, type \n sysdig –cl \n list process by top CPU inside container @ name \n sysdig -pc -c topprocs_cpu container.name=mesos-ad8a3341-0f7c-4a28-ac47-744c6767990e-S18.4b5c695e-f885-47f3-9a77-7b024ffa7aa \n Show all the interactive commands executed inside the container \n sysdig -pc -c spy_users container.name=mesos-ad8a3341-0f7c-4a28-ac47-744c6767990e-S18.4b5c695e-f885-47f3-9a77-7b024ffa7aa1\n``` \n To see what cheatsheets are available, run  cheat -l . \n Note that, while  cheat  was designed primarily for *nix system administrators,\nit is agnostic as to what content it stores. If you would like to use  cheat \nto store notes on your favorite cookie recipes, feel free. \n Installing Cheat CLI \n It is recommended to install  cheat  with  pip : \n Global Install cheat \n sh\n[sudo] pip install cheat \n Virtualenv Install cheat \n sh\nmkvirtualenv --python=python2 cheat2\npip install cheat \n Thanks \n Borrowed some of the java cheats from https://github.com/chhsiao90/cheatsheets-java', 'reproduce_travisci_docker_permissions_issue \n Simple environment to reproduce docker permissions issues, specifically when uid/gid of user running container does not match user in container. ', 'ansible-role-bossjones-cli-tools \n ansible role to install bossjones cli favorites. Only works on fedora 24 currently.', 'Role Name \n A brief description of the role goes here. \n Requirements \n Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required. \n Role Variables \n A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well. \n Dependencies \n A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles. \n Example Playbook \n Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too: \n - hosts: servers\n  roles:\n     - { role: bossjones.ipv6, x: 42 }\n \n License \n BSD \n Author Information \n An optional section for the role authors to include contact information, or a website (HTML is not allowed).', 'docker-swarm-vbox-lab \n lab to try out using docker swarm on virtualized environment using vagrant \n Lots of influence from \n \n https://github.com/deviantony/docker-elk \n \n Stack Overview( All using docker-swarm ) \n \n Prometheus Server \n Prometheus Alertmanager \n Node Exporter \n Portainer \n Grafana \n Nginx reverse proxy with SSL/TLS  Let\'s encrypt certificate \n ELK stack ( Elasticsearch, Logstash, Kibana ) \n \n Future ( Fun things to try to get working ) \n \n StackStorm  Event-driven automation \n Bro IDS \n Redis/Zmq/RabbitMq ( Some sort of queue solution ) \n zq  Kafka-based Job Queue for Python  \n etcd:  Distributed reliable key-value store for the most critical data of a distributed system \n faas:  Functions as a Service (OpenFaaS) \n opentracing:  distributed tracing and context propagation \n Zipkin \n Jaegar \n locustio \n Loki:  Simple, Distributed Tracing \n Vizceral:  WebGL visualization for displaying animated traffic graphs \n Vizceral Example \n Jenkins:  For automation of tasks \n Vault:  hashicorp secret management \n Augmented Traffic Control: \n Facebook Osquery:  SQL powered operating system instrumentation, monitoring, and analytics. \n Hubble:  Security compliance framework \n doorman:  osquery fleet manager \n ChaosMonkey:  resiliency tool that helps applications tolerate random instance failures \n git-server-docker:  Git Server in Docker \n gitlab-ce:  GitLab Community Edition docker image based on the Omnibus package \n \n Research: \n \n https://www-public.tem-tsp.eu/~berger_o/docker/install-docker-machine-virtualbox.html \n https://unix.stackexchange.com/questions/269912/send-command-to-the-shell-via-makefile \n https://docs.docker.com/get-started/part3/ \n https://stackoverflow.com/questions/7897549/make-ignores-my-python-bash-alias \n https://botleg.com/stories/monitoring-docker-swarm-with-cadvisor-influxdb-and-grafana/ \n https://github.com/bvis/docker-prometheus-swarm/blob/master/docker-compose.logging.yml \n \n Recover after swarm reboot \n \n https://forums.docker.com/t/docker-worker-nodes-shown-as-down-after-re-start/22329/2 \n https://github.com/moby/moby/issues/23828 \n https://docs.docker.com/engine/swarm/admin_guide/#monitor-swarm-health \n https://docs.docker.com/engine/swarm/admin_guide/#run-manager-only-nodes \n \n \n export discotoken=<some-token> \n How to fix \'Error response from daemon: 404 page not found\' \n Source:  DOCKER SWARM MODE SETUP WITH DOCKER MACHINE  - http://perica.zivkovic.nl/blog/setup-docker-swarm-with-docker-machine-do/ \n Example output from creating a docker swarm \n ```\n |2.2.3|   Malcolms-MBP-3 in ~/dev/bossjones/docker-swarm-vbox-lab\n± |master U:2 ✗| → ./bin/docker-machine-x86_64 env local\nexport DOCKER_TLS_VERIFY="1"\nexport DOCKER_HOST="tcp://192.168.99.100:2376"\nexport DOCKER_CERT_PATH="/Users/malcolm/.docker/machine/machines/local"\nexport DOCKER_MACHINE_NAME="local" \n Run this command to configure your shell: \n eval $(./bin/docker-machine-x86_64 env local) \n |2.2.3|   Malcolms-MBP-3 in ~/dev/bossjones/docker-swarm-vbox-lab\n± |master U:2 ✗| → eval $(./bin/docker-machine-x86_64 env local) \n |2.2.3|   Malcolms-MBP-3 in ~/dev/bossjones/docker-swarm-vbox-lab\n± |master U:2 ✗| → make dm-ls\n./bin/docker-machine-x86_64 ls\nNAME                   ACTIVE   DRIVER         STATE     URL                         SWARM   DOCKER        ERRORS\ndev                    -        vmwarefusion   Stopped                                       Unknown\nlocal                  *        virtualbox     Running   tcp://192.168.99.100:2376           v17.06.2-ce\nscarlett-1604-packer   -        generic        Stopped                                       Unknown \n |2.2.3|   Malcolms-MBP-3 in ~/dev/bossjones/docker-swarm-vbox-lab\n± |master U:2 ✗| → make bootstrap-swarm\n./bin/docker-machine-x86_64 create -d virtualbox swarm-manager\nRunning pre-create checks...\nCreating machine...\n(swarm-manager) Copying /Users/malcolm/.docker/machine/cache/boot2docker.iso to /Users/malcolm/.docker/machine/machines/swarm-manager/boot2docker.iso...\n(swarm-manager) Creating VirtualBox VM...\n(swarm-manager) Creating SSH key...\n(swarm-manager) Starting the VM...\n(swarm-manager) Check network to re-create if needed...\n(swarm-manager) Waiting for an IP...\nWaiting for machine to be running, this may take a few minutes...\nDetecting operating system of created instance...\nWaiting for SSH to be available...\nDetecting the provisioner...\nProvisioning with boot2docker...\nCopying certs to the local machine directory...\nCopying certs to the remote machine...\nSetting Docker configuration on the remote daemon...\nChecking connection to Docker...\nDocker is up and running!\nTo see how to connect your Docker Client to the Docker Engine running on this virtual machine, run: ./bin/docker-machine-x86_64 env swarm-manager\n./bin/docker-machine-x86_64 create -d virtualbox node-01\nRunning pre-create checks...\nCreating machine...\n(node-01) Copying /Users/malcolm/.docker/machine/cache/boot2docker.iso to /Users/malcolm/.docker/machine/machines/node-01/boot2docker.iso...\n(node-01) Creating VirtualBox VM...\n(node-01) Creating SSH key...\n(node-01) Starting the VM...\n(node-01) Check network to re-create if needed...\n(node-01) Waiting for an IP...\nWaiting for machine to be running, this may take a few minutes...\nDetecting operating system of created instance...\nWaiting for SSH to be available...\nDetecting the provisioner...\nProvisioning with boot2docker...\nCopying certs to the local machine directory...\nCopying certs to the remote machine...\nSetting Docker configuration on the remote daemon...\nChecking connection to Docker...\nDocker is up and running!\nTo see how to connect your Docker Client to the Docker Engine running on this virtual machine, run: ./bin/docker-machine-x86_64 env node-01\n./bin/docker-machine-x86_64 create -d virtualbox node-02\nRunning pre-create checks...\nCreating machine...\n(node-02) Copying /Users/malcolm/.docker/machine/cache/boot2docker.iso to /Users/malcolm/.docker/machine/machines/node-02/boot2docker.iso...\n(node-02) Creating VirtualBox VM...\n(node-02) Creating SSH key...\n(node-02) Starting the VM...\n(node-02) Check network to re-create if needed...\n(node-02) Waiting for an IP...\nWaiting for machine to be running, this may take a few minutes...\nDetecting operating system of created instance...\nWaiting for SSH to be available...\nDetecting the provisioner...\nProvisioning with boot2docker...\nCopying certs to the local machine directory...\nCopying certs to the remote machine...\nSetting Docker configuration on the remote daemon...\nChecking connection to Docker...\nDocker is up and running!\nTo see how to connect your Docker Client to the Docker Engine running on this virtual machine, run: ./bin/docker-machine-x86_64 env node-02 \n ./bin/docker-machine-x86_64 create -d virtualbox node-03 \n |2.2.3|   Malcolms-MBP-3 in ~/dev/bossjones/docker-swarm-vbox-lab\n± |master U:2 ✗| → MANAGER_IP=$(docker-machine ip swarm-manager) \n |2.2.3|   Malcolms-MBP-3 in ~/dev/bossjones/docker-swarm-vbox-lab\n± |master U:2 ✗| → docker swarm --help \n Usage:  docker swarm COMMAND \n Manage Swarm \n Options:\n      --help   Print usage \n Commands:\n  init        Initialize a swarm\n  join        Join a swarm as a node and/or manager\n  join-token  Manage join tokens\n  leave       Leave the swarm\n  unlock      Unlock swarm\n  unlock-key  Manage the unlock key\n  update      Update the swarm \n Run \'docker swarm COMMAND --help\' for more information on a command. \n |2.2.3|   Malcolms-MBP-3 in ~/dev/bossjones/docker-swarm-vbox-lab\n± |master U:2 ✗| → docker swarm join-token --help \n Usage:  docker swarm join-token [OPTIONS] (worker|manager) \n Manage join tokens \n Options:\n      --help     Print usage\n  -q, --quiet    Only display token\n      --rotate   Rotate join token \n |2.2.3|   Malcolms-MBP-3 in ~/dev/bossjones/docker-swarm-vbox-lab\n± |master U:2 ✗| → MANAGER_IP=$(./bin/docker-machine-x86_64 ip swarm-manager) \n |2.2.3|   Malcolms-MBP-3 in ~/dev/bossjones/docker-swarm-vbox-lab\n± |master U:2 ✗| → echo ${MANAGER_IP}\n192.168.99.104 \n |2.2.3|   Malcolms-MBP-3 in ~/dev/bossjones/docker-swarm-vbox-lab\n± |master U:2 ✗| → ./bin/docker-machine-x86_64 ssh swarm-manager docker swarm init --advertise-addr ${MANAGER_IP}\nSwarm initialized: current node (9r8nhpx908lk2msh9odlge39v) is now a manager. \n To add a worker to this swarm, run the following command: \n docker swarm join --token SWMTKN-1-3qdkv66g9isfqftixhz34vxkawosw32vwjebgu0yprpxah5vms-0u8wxklm8ms8wcwxsrbu0rpla 192.168.99.104:2377\n \n To add a manager to this swarm, run \'docker swarm join-token manager\' and follow the instructions. \n |2.2.3|   Malcolms-MBP-3 in ~/dev/bossjones/docker-swarm-vbox-lab\n± |master U:2 ✗| → WORKER_TOKEN=$(./bin/docker-machine-x86_64 ssh swarm-manager docker swarm join-token --quiet worker) \n |2.2.3|   Malcolms-MBP-3 in ~/dev/bossjones/docker-swarm-vbox-lab\n± |master U:2 ✗| → echo ${WORKER_TOKEN}}\nSWMTKN-1-3qdkv66g9isfqftixhz34vxkawosw32vwjebgu0yprpxah5vms-0u8wxklm8ms8wcwxsrbu0rpla} \n |2.2.3|   Malcolms-MBP-3 in ~/dev/bossjones/docker-swarm-vbox-lab\n± |master U:2 ✗| → ./bin/docker-machine-x86_64 ssh node-01 docker swarm join --token ${WORKER_TOKEN} ${MANAGER_IP}:2377\nThis node joined a swarm as a worker. \n |2.2.3|   Malcolms-MBP-3 in ~/dev/bossjones/docker-swarm-vbox-lab\n± |master U:2 ✗| → ./bin/docker-machine-x86_64 ssh node-02 docker swarm join --token ${WORKER_TOKEN} ${MANAGER_IP}:2377\nThis node joined a swarm as a worker. \n |2.2.3|   Malcolms-MBP-3 in ~/dev/bossjones/docker-swarm-vbox-lab\n± |master U:2 ✗| → eval "$(./bin/docker-machine-x86_64 env swarm-manager)" \n |2.2.3|   Malcolms-MBP-3 in ~/dev/bossjones/docker-swarm-vbox-lab\n± |master U:2 ✗| → docker node ls\nID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS\n9r8nhpx908lk2msh9odlge39v *   swarm-manager       Ready               Active              Leader\nlcar141vjpbmp8robql4sy6za     node-02             Ready               Active\nlgmpnbaga2yiowzzwecblreqo     node-01             Ready               Active \n |2.2.3|   Malcolms-MBP-3 in ~/dev/bossjones/docker-swarm-vbox-lab\n± |master U:2 ✗| → echo ${MANAGER_IP}\n192.168.99.104 \n |2.2.3|   Malcolms-MBP-3 in ~/dev/bossjones/docker-swarm-vbox-lab\n± |master U:2 ✗| → docker service create \\ \n \n -d \\\n--name portainer \\\n--publish 9000:9000 \\\nportainer/portainer \\\n-H tcp://${MANAGER_IP}:2376\nk6ue1in8da665mraodqmnid7a \n \n |2.2.3|   Malcolms-MBP-3 in ~/dev/bossjones/docker-swarm-vbox-lab\n± |master U:2 ✗| → docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES \n |2.2.3|   Malcolms-MBP-3 in ~/dev/bossjones/docker-swarm-vbox-lab\n± |master U:2 ✗| → docker ps -a\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES \n |2.2.3|   Malcolms-MBP-3 in ~/dev/bossjones/docker-swarm-vbox-lab\n± |master U:2 ✗| → docker node ls\nID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS\n9r8nhpx908lk2msh9odlge39v *   swarm-manager       Ready               Active              Leader\nlcar141vjpbmp8robql4sy6za     node-02             Ready               Active\nlgmpnbaga2yiowzzwecblreqo     node-01             Ready               Active \n |2.2.3|   Malcolms-MBP-3 in ~/dev/bossjones/docker-swarm-vbox-lab\n± |master U:2 ✗| → docker service --help \n Usage:  docker service COMMAND \n Manage services \n Options:\n      --help   Print usage \n Commands:\n  create      Create a new service\n  inspect     Display detailed information on one or more services\n  logs        Fetch the logs of a service or task\n  ls          List services\n  ps          List the tasks of one or more services\n  rm          Remove one or more services\n  scale       Scale one or multiple replicated services\n  update      Update a service \n Run \'docker service COMMAND --help\' for more information on a command. \n |2.2.3|   Malcolms-MBP-3 in ~/dev/bossjones/docker-swarm-vbox-lab\n± |master U:2 ✗| → docker service ls\nID                  NAME                MODE                REPLICAS            IMAGE                 PORTS\nk6ue1in8da66        portainer           replicated          1/1                 portainer/portainer   *:9000->9000/tcp \n |2.2.3|   Malcolms-MBP-3 in ~/dev/bossjones/docker-swarm-vbox-lab\n± |master U:2 ✗| → gs\nOn branch master\nYour branch is up-to-date with \'origin/master\'. \n Changes not staged for commit:\n  (use "git add  ..." to update what will be committed)\n  (use "git checkout --  ..." to discard changes in working directory) \n     modified:   Makefile\n    modified:   README.md\n \n no changes added to commit (use "git add" and/or "git commit -a") \n |2.2.3|   Malcolms-MBP-3 in ~/dev/bossjones/docker-swarm-vbox-lab\n± |master U:2 ✗| → git add . \n |2.2.3|   Malcolms-MBP-3 in ~/dev/bossjones/docker-swarm-vbox-lab\n± |master S:2 ✗| → git commit -m "Chg: portainer"\n[master 4609578] Chg: portainer\n 2 files changed, 44 insertions(+) \n |2.2.3|   Malcolms-MBP-3 in ~/dev/bossjones/docker-swarm-vbox-lab\n± |master ↑1 ✓| → gp\nCounting objects: 4, done.\nDelta compression using up to 4 threads.\nCompressing objects: 100% (4/4), done.\nWriting objects: 100% (4/4), 1.20 KiB | 1.20 MiB/s, done.\nTotal 4 (delta 3), reused 0 (delta 0)\nremote: Resolving deltas: 100% (3/3), completed with 3 local objects.\nTo github.com:bossjones/docker-swarm-vbox-lab.git\n   6e63286..4609578  master -> master \n |2.2.3|   Malcolms-MBP-3 in ~/dev/bossjones/docker-swarm-vbox-lab\n± |master ✓| →\n``` \n Dashboard for Grafana \n Borrowed from https://github.com/botleg/swarm-monitoring ! \n Source: https://github.com/botleg/swarm-monitoring/blob/master/dashboard.json \n Prometheus \n Borrowed from https://github.com/vegasbrianc/prometheus \n SSL configuration example \n \n https://github.com/danguita/prometheus-monitoring-stack \n \n SMTP server for alertmanager \n \n https://hub.docker.com/r/marvambass/versatile-postfix/ \n \n networking external example \n \n https://github.com/bvis/docker-prometheus-swarm/blob/master/docker-compose.logging.yml \n \n networking on swarm classic \n \n Multi-host networking with standalone swarms \n \n Install Grafana Plugins ( grafana-cli admin ) \n \n http://docs.grafana.org/plugins/installation/ \n http://docs.grafana.org/administration/cli/ \n \n Consul setup and config \n \n https://medium.com/zendesk-engineering/making-docker-and-consul-get-along-5fceda1d52b9 \n https://www.consul.io/docs/guides/consul-containers.html \n https://blog.octo.com/en/how-does-it-work-docker-part-1-swarm-general-architecture/ \n \n Networking problems \n \n https://github.com/docker/compose/issues/2908 \n https://docs.docker.com/v17.06/compose/compose-file/#external-1 \n \n Example: In the example below, proxy is the gateway to the outside world. Instead of attempting to create a network called [projectname]_outside, Compose will look for an existing network simply called outside and connect the proxy service’s containers to it. \n ```\nversion: \'2\' \n services:\n  proxy:\n    build: ./proxy\n    networks:\n      - outside\n      - default\n  app:\n    build: ./app\n    networks:\n      - default \n networks:\n  outside:\n    external: true\n``` \n \n \n https://blog.octo.com/en/how-does-it-work-docker-part-3-load-balancing-service-discovery-and-security/ \n \n \n LOOK AT ME: Service is not DNS resolvable from another one if containers run on different nodes https://github.com/docker/swarmkit/issues/1429 \n \n \n https://github.com/docker/swarmkit/issues/1716 \n \n https://github.com/nlandolfi/mixer/blob/75ecdd0ad2959b0088ea75810ed1755b83e74490/deploy/kube/conf/import_dashboard.sh \n \n IOT Solution \n \n https://medium.com/@DazWilkin/docker-swarm-and-prometheus-fd19462f1bf8 \n \n Node file Service Discovery \n \n https://github.com/SphericalElephant/ansible-role-prometheus-node-exporter/blob/master/defaults/main.yml \n \n -collectors.enabled=conntrack,diskstats,entropy,filefd,filesystem,loadavg,mdadm,meminfo,netdev,netstat,sockstat,stat,textfile,time,uname,vmstat \n This is Ansible FYI: \n prometheus_node_exporter_parameters:\n  - "-collectors.enabled={{ prometheus_node_exporter_collectors_enable | join(\',\') }}"\n  - "-web.listen-address={{ prometheus_node_exporter_web_listen_address }}"\n  - \'-log.level=info\'\n  - \'-collector.diskstats.ignored-devices=^(ram|loop|fd)\\d+$\'\n  - \'-collector.filesystem.ignored-mount-points=^/(sys|proc|dev|run)($|/)\'\n  - \'-collector.netdev.ignored-devices="{{ prometheus_node_exporter_collector_netdev_ignored_devices }}"\'\n  - \'-collector.textfile.directory=/var/lib/prometheus/node-exporter\' \n Monitoring a Docker Swarm Cluster with Prometheus \n \n https://chmod666.org/2017/08/monitoring-a-docker-swarm-cluster-with-prometheus \n \n elasticsearch env vars example \n ELASTICSEARCH_URL=http://elasticsearch:9200\nXPACK_GRAPH_ENABLED=false\nXPACK_ML_ENABLED=false\nXPACK_MONITORING_ENABLED=false\nXPACK_REPORTING_ENABLED=false\nXPACK_SECURITY_ENABLED=false \n FIXME: Env vars ( elasticsearch, kibana, logstash ) \n ```\nFor compatibility with container orchestration systems, these environment variables are written in all capitals, with underscores as word separators. The helper translates these names to valid Kibana setting names. \n example \n services:\n  kibana:\n    image: docker.elastic.co/kibana/kibana:5.6.2\n    environment:\n      SERVER_NAME: kibana.example.org\n      ELASTICSEARCH_URL: http://elasticsearch.example.org \n docker defaults \n Docker defaultsedit\nThe following settings have different default values when using the Docker image: \n server.host \n "0" \n elasticsearch.url \n http://elasticsearch:9200 \n elasticsearch.username \n elastic \n elasticsearch.password \n changeme \n xpack.monitoring.ui.container.elasticsearch.enabled \n true \n one more example \n source: https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html \n version: \'2\'\nservices:\n  elasticsearch1:\n    image: docker.elastic.co/elasticsearch/elasticsearch:5.6.2\n    container_name: elasticsearch1\n    environment:\n      - cluster.name=docker-cluster\n      - bootstrap.memory_lock=true\n      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    mem_limit: 1g\n    volumes:\n      - esdata1:/usr/share/elasticsearch/data\n    ports:\n      - 9200:9200\n    networks:\n      - esnet\n  elasticsearch2:\n    image: docker.elastic.co/elasticsearch/elasticsearch:5.6.2\n    environment:\n      - cluster.name=docker-cluster\n      - bootstrap.memory_lock=true\n      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"\n      - "discovery.zen.ping.unicast.hosts=elasticsearch1"\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    mem_limit: 1g\n    volumes:\n      - esdata2:/usr/share/elasticsearch/data\n    networks:\n      - esnet \n volumes:\n  esdata1:\n    driver: local\n  esdata2:\n    driver: local \n networks:\n  esnet:\n``` \n See: https://www.elastic.co/guide/en/kibana/current/_configuring_kibana_on_docker.html', 'swarm-mode-init-script', 'This stack deploys all the components needed to monitor a Docker Swarm cluster:\n  - prometheus\n  - alertmanager\n  - grafana\n  - blackbox-exporter\n  - docker-exporter\n  - node-exporter\n  - cAdvisor\n  - fregate (free mobile sms api \n If you want to expose some components through Traefik reverse proxy you first have to have an overlay network with Traefik running in it. In my compose file this overlay network is called "traefik-net" \n To deploy the stack run the following command:\n``` \n docker stack deploy --compose-file prometheus.yml \n prometheus\n``` \n This stack uses configs and secrets. (compose "v3.3") \n \n Modify the pushover credentials in configs/alertmanager.yml \n Modify the Traefik basic auths and url in promtheus.yml \n If using the free sms api put you username and password in: \n secrets/fregate_user \n secrets/fregate_password \n \n \n', 'local-mesos-cluster \n Quick setup to allow anyone to bring up a local mesos/marathon environment using docker-compose. \n config \n ~/.local-mesos-cluster/config.json \n "local-mesos-cluster": {\n    "docker-machines": {\n        "instances": {\n            "local-mesos-cluster": {\n                "docker": {\n                    "tls_verify": "1",\n                    "host": "tcp://192.168.99.101:2376",\n                    "cert_path": "/Users/user/.docker/machine/machines/local-mesos-cluster",\n                    "machine_name": "local-mesos-cluster"\n                }\n            }\n        }\n    }\n}', 'pyspark-lab \n Scripts to try out pyspark with. Use with https://github.com/jupyter/docker-stacks/tree/master/pyspark-notebook \n Spark-practice \n \n https://github.com/XD-DENG/Spark-practice \n https://github.com/mahmoudparsian/pyspark-tutorial \n https://github.com/svenkreiss/pysparkling \n https://github.com/ksindi/kafka-pyspark-demo \n https://github.com/confluentinc/cp-docker-images/wiki/Getting-Started \n \n https://docs.confluent.io/current/connect/managing.html \n \n \n http://activisiongamescience.github.io/2016/06/15/Kafka-Client-Benchmarking/ \n \n \n kafkacat \n https://github.com/edenhill/kafkacat \n ``` \n see info about your image \n docker run --rm kafkacat \n produce stuff (Ctrl+C to exit) \n echo "msg 1" | docker run -i --rm --net=host kafkacat -b mybroker -t logs -P \n consume stuff (Ctrl+C to exit) \n docker run --rm -t --net=host kafkacat -b mybroker -t logs -C \n produce from file or command inside the container \n echo 1 > example.log\ndocker run --name test-producer -d -v $(pwd)/example.log:/logs/example.log --entrypoint /bin/bash --net=host kafkacat \\\n  -c \'tail -f /logs/example.log | kafkacat -b mybroker -t logs -P\'\necho 2 >> example.log \n the last example runs in background so clean up \n docker kill test-producer && docker rm test-producer\n``` \n remote-jmx-with-docker \n https://ptmccarthy.github.io/2014/07/24/remote-jmx-with-docker/ \n An important Docker-related note about the Tomcat configuration above is that the -Djava.rmi.server.hostname must be set to the externally accessible IP address of the Tomcat server. You want to use the address of the Docker host, not the Docker-assigned internal IP address. \n troubleshooting host networking on zookeeper ( Should see something like this ) \n [2017-11-24 03:31:32,185] INFO Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.server.ZooKeeperServer)\n[2017-11-24 03:31:32,185] INFO Server environment:java.io.tmpdir=/tmp (org.apache.zookeeper.server.ZooKeeperServer)\n[2017-11-24 03:31:32,185] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)\n[2017-11-24 03:31:32,185] INFO Server environment:os.name=Linux (org.apache.zookeeper.server.ZooKeeperServer)\n[2017-11-24 03:31:32,185] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)\n[2017-11-24 03:31:32,185] INFO Server environment:os.version=4.4.17-boot2docker (org.apache.zookeeper.server.ZooKeeperServer)\n[2017-11-24 03:31:32,185] INFO Server environment:user.name=root (org.apache.zookeeper.server.ZooKeeperServer)\n[2017-11-24 03:31:32,185] INFO Server environment:user.home=/root (org.apache.zookeeper.server.ZooKeeperServer)\n[2017-11-24 03:31:32,185] INFO Server environment:user.dir=/ (org.apache.zookeeper.server.ZooKeeperServer)\n[2017-11-24 03:31:32,195] INFO tickTime set to 2000 (org.apache.zookeeper.server.ZooKeeperServer)\n[2017-11-24 03:31:32,195] INFO minSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)\n[2017-11-24 03:31:32,195] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)\n[2017-11-24 03:31:32,206] INFO binding to port 0.0.0.0/0.0.0.0:32181 (org.apache.zookeeper.server.NIOServerCnxnFactory) \n Monitoring (pyspark) \n \n https://github.com/JasonMWhite/spark-datadog-relay \n https://github.com/jvutukur/Data-Visualization \n https://github.com/wangcunxin/wespark/blob/58805b88bb56c27d4116f5bf3e1efdd861798f1d/bblink/graphite/kafka_graphite_streaming.py \n https://github.com/hopshadoop/hops-yarn-ML/blob/7e54f12bafeaae62fde40eb6a18ebcac4a6f5e9e/yarn_machine_learning.py \n https://tlfvincent.github.io/2016/09/25/kafka-spark-pipeline-part-1/ \n https://github.com/divolte/divolte-spark \n \n jmxtrans other examples \n https://github.com/jmxtrans/jmxtrans/wiki/MoreExamples \n FIX for: Spark Twitter Streaming exception : (org.apache.spark.Logging) classnotfound \n https://stackoverflow.com/questions/38893655/spark-twitter-streaming-exception-org-apache-spark-logging-classnotfound', 'docker-swarm-marvel \n ansible playbooks, scripts, and docker-compose files based on marvel comics locations. Use with digital ocean. Based on https://github.com/bossjones/docker-swarm-vbox-lab \n pip install --ignore-installed --pre "https://github.com/pradyunsg/pip/archive/hotfix/9.0.2.zip#egg=pip" \\\n    && pip install --upgrade setuptools==36.0.1 wheel==0.29.0 \n Regarding firewalls, remember this!!! \n ``` \n icmp no PORT DEFINED \n --outbound-rules "protocol:icmp,address:0.0.0.0/0,address:::/0 protocol:tcp,ports:all,address:0.0.0.0/0,address:::/0 protocol:udp,ports:all,address:0.0.0.0/0,address:::/0"\n```', 'boss-golang-labs \n Repo so I can learn some golang. Starting off with version 1.9.2 \n Tutorials \n Sentdex -  Go Language Programming Practical Basics', "boss-grafana-graphite \n FOR LEARNING PURPOSES. Docker image with StatsD, Graphite, Grafana 2 and a Kamon Dashboard. Based strongly o:\n- https://github.com/kamon-io/docker-grafana-graphite\n- http://mintbeans.com/jvm-monitoring-docker/ \n StatsD + Graphite + Grafana 4 + Kamon Dashboards \n This image contains a sensible default configuration of StatsD, Graphite and Grafana, and comes bundled with a example\ndashboard that gives you the basic metrics currently collected by Kamon for both Actors and Traces. There are two ways\nfor using this image: \n Using the Docker Index \n This image is published under  Kamon's repository on the Docker Hub  and all you\nneed as a prerequisite is having  docker ,  docker-compose , and  make  installed on your machine. The container exposes the following ports: \n \n 80 : the Grafana web interface. \n 81 : the Graphite web port \n 2003 : the Graphite data port \n 8125 : the StatsD port. \n 8126 : the StatsD administrative port. \n \n To start a container with this image you just need to run the following command: \n bash\n$ make up \n To stop the container\n bash\n$ make down \n To run container's shell\n bash\n$ make shell \n To view the container log\n bash\n$ make tail \n If you already have services running on your host that are using any of these ports, you may wish to map the container\nports to whatever you want by changing left side number in the  --publish  parameters. You can omit ports you do not plan to use. Find more details about mapping ports in the Docker documentation on  Binding container ports to the host  and  Legacy container links . \n Building the image yourself \n The Dockerfile and supporting configuration files are available in our  Github repository .\nThis comes specially handy if you want to change any of the StatsD, Graphite or Grafana settings, or simply if you want\nto know how the image was built. \n Using the Dashboards \n Once your container is running all you need to do is: \n \n open your browser pointing to http://localhost:80 (or another port if you changed it) \n Docker with VirtualBox on macOS: use  docker-machine ip  instead of  localhost \n login with the default username (admin) and password (admin) \n open existing dashboard (or create a new one) and select 'Local Graphite' datasource \n play with the dashboard at your wish... \n \n Persisted Data \n When running  make up , directories are created on your host and mounted into the Docker container, allowing graphite and grafana to persist data and settings between runs of the container. \n Now go explore! \n We hope that you have a lot of fun with this image and that it serves it's\npurpose of making your life easier. This should give you an idea of how the dashboard looks like when receiving data\nfrom one of our toy applications: \n \n", 'go-chatbot-lab \n Development only. I\'m trying to learnsome golang! First project, building a chatbot! \n Getting started \n This project requires Go to be installed. On OS X with Homebrew you can just run  brew install go . \n Running it then should be as simple as: \n console\n$ make\n$ ./bin/go-chatbot-lab \n Testing \n make test \n Thanks \n Cookiecutter! \n cookiecutter-golang \n Example Projects Using Same Cookiecutter \n iothub \n Problems \n \n Editor not looking at vendor folder it is ln -s from other path VSCode \n \n Basic Golang gotyas \n You go code needs to exist in  $GOPATH/src \n Eg. \n ``` \n |2.2.3|    dev7-behance-1484 in ~/dev/go_workspace/src/github.com/bossjones\n○ → code go-chatbot-lab/\n``` \n mockgen \n ``` \n !/bin/bash -e \n source: https://github.com/rafrombrc/gomock/blob/master/update_mocks.sh \n mockgen github.com/rafrombrc/gomock/gomock Matcher \\ \n \n gomock/mock_matcher/mock_matcher.go\nmockgen github.com/rafrombrc/gomock/sample Index,Embed,Embedded \\\nsample/mock_user/mock_user.go\ngofmt -w gomock/mock_matcher/mock_matcher.go sample/mock_user/mock_user.go \n \n echo >&2 "OK"\n``` \n A getting started guide for Go newcomers \n https://github.com/alco/gostart \n Server code borrowed from coolspeed/century project \n It\'s just a simple chatbot i\'m building to help teach me how golang works. Actual server code borrowed from coolspeed/century project! Will add more on top of that. \n Feature \n \n High throughput \n High concurrency \n (Automatic) High scalability, especially on many-core computers. (Think of 64-core computers, as much as 4-core ones.) \n \n Detailed Information \n You can find an even simpler chat server on: \n https://gist.github.com/drewolson/3950226 \n (In fact I started my scratch from that.) \n Build and Run \n 1) First, you need to install  golang , of course: \n Download it from  https://golang.org/dl/ , or install go via your package management tool: \n For Ubuntu: \n sudo apt-get install golang \n For Mac OS X: \n brew install go \n 2) Now, just build. \n cd  into the repo directory. \n To build the server, execute: \n make build \n 3) Run \n 3.1 Run the chat server: \n ./bin/go-chatbot-lab \n 3.2 Run the chat client: \n Client : You can use  telnet  as the client: \n telnet localhost 6666 \n type anything.', 'vagrant-cluster-ubuntu1404 \n A ubuntu1404 cluster via Vagrant', 'ansible-role-skeleton \n   \n We try to open source as much as possible of our infrastructure and services. This skeleton contains all basic information we use to develop and test  Ansible  roles. You can easily use it to start developing new roles or add testing to old ones. \n Usage \n # init a new role\n./init-role.sh /path/to/new-role\n\n# update an existing role\n./update-role.sh /path/to/role\n \n Testing \n Make sure your user is in the  docker  group. To only test your current setup, do \n molecule test\n \n To test different versions of ansible, do \n tox\n \n If your role depends on other roles from  Ansible Galaxy , uncomment the dependency lines in  molecule.yml  and add the dependencies in  tests/requirements.yml . \n License \n GPL \n Author Information \n https://www.systemli.org', 'linux-dotfiles \n linux dotfiles setup ( currently tested with ubuntu, near future fedora, coreOS ) \n Thanks \n \n https://github.com/jessfraz/dotfiles \n https://github.com/jessfraz/dockerfiles \n https://github.com/blacktop/dotfiles \n https://dotfiles.github.io/ \n https://github.com/bndabbs/dotfiles \n https://github.com/webpro/dotfiles \n \n LOOK AT THIS ONE \n \n https://github.com/webpro/dotfiles \n \n Test in docker container \n ```\ndocker build --force-rm --no-cache -t dotfiles . \n docker run -it dotfiles:latest bash\n``` \n Install? \n ``` \n 1  cd ~/.dotfiles/\n2  git pull\n3  clear\n4  git pull\n5  clear\n6  source ~/.dotfiles/install.sh\n7  source ~/.nvm/nvm.sh\n8  nvm ls\n9  npm search bats\n10  histor\n11  history \n ``` \n wip ~/.bashrc \n ```\nsource ~malcolm/.pyenv/.pyenvrc\nsource ~malcolm/.pyenv/completions/pyenv.bash \n tabtab source for serverless package \n uninstall by removing these lines or running  tabtab uninstall serverless \n [ -f /Users/malcolm/.nvm/versions/node/v10.3.0/lib/node_modules/serverless/node_modules/tabtab/.completions/serverless.bash ] && . /Users/malcolm/.nvm/versions/node/v10.3.0/lib/node_modules/serverless/node_modules/tabtab/.completions/serverless.bash \n tabtab source for sls package \n uninstall by removing these lines or running  tabtab uninstall sls \n [ -f /Users/malcolm/.nvm/versions/node/v10.3.0/lib/node_modules/serverless/node_modules/tabtab/.completions/sls.bash ] && . /Users/malcolm/.nvm/versions/node/v10.3.0/lib/node_modules/serverless/node_modules/tabtab/.completions/sls.bash\n``` \n .profile \n ``` \n export PATH="$HOME/.cargo/bin:$PATH" \n export PATH="/usr/local/bin:$PATH"\nexport PATH="/usr/local/sbin:$PATH" \n export PYENV_ROOT="$HOME/.pyenv"\nexport PATH="$PYENV_ROOT/bin:$PATH"\nexport WORKON_HOME=~/.virtualenvs\nexport PROJECT_HOME=~/dev\neval "$(pyenv init -)"\npyenv virtualenvwrapper_lazy \n # System-wide .profile for sh(1) \n \n if [ -x /usr/libexec/path_helper ]; then \n eval  /usr/libexec/path_helper -s \n fi \n \n if [ "${BASH-no}" != "no" ]; then \n [ -r /etc/bashrc ] && . /etc/bashrc \n fi \n ``` \n default ~/.bashrc \n ``` \n [ -f ~/.fzf.bash ] && source ~/.fzf.bash \n \n # added by travis gem \n [ -f /Users/malcolm/.travis/travis.sh ] && source /Users/malcolm/.travis/travis.sh \n System-wide .bashrc file for interactive bash(1) shells. \n if [ -z "$PS1" ]; then\n   return\nfi \n PS1=\'\\h:\\W \\u\\$ \' \n Make bash check its window size after a process completes \n shopt -s checkwinsize \n [ -r "/etc/bashrc_$TERM_PROGRAM" ] && . "/etc/bashrc_$TERM_PROGRAM" \n export PATH="/usr/local/bin:$PATH"\nexport PATH="/usr/local/sbin:$PATH" \n export PYENV_ROOT="$HOME/.pyenv"\nexport PATH="$PYENV_ROOT/bin:$PATH"\nexport WORKON_HOME=~/.pyenv/versions\nexport PROJECT_HOME=~/dev\neval "$(pyenv init -)"\npyenv virtualenvwrapper_lazy \n ``` \n Environment Variables \n \n SKIP_DOTFILES_PROVISION=1 \n SKIP_DOTFILES_PREREQ=1 : skip prereq-linux.sh \n SKIP_DOTFILES_FORCE_INSTALL_HOMEBREW=1 : skip prereq-osx.sh \n', 'Role Name \n A brief description of the role goes here. \n Requirements \n Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required. \n Role Variables \n A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well. \n Dependencies \n A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles. \n Example Playbook \n Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too: \n - hosts: servers\n  roles:\n     - { role: bossjones.ansible, x: 42 }\n \n License \n Apache \n Author Information \n An optional section for the role authors to include contact information, or a website (HTML is not allowed).', 'boss-emulate-networking-vagrant \n Boss-emulate-networking-vagrant. Based on guide: http://www.brianlinkletter.com/how-to-use-virtualbox-to-emulate-a-network/', 'Role Name \n A brief description of the role goes here. \n Requirements \n Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required. \n Role Variables \n A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well. \n Dependencies \n A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles. \n Example Playbook \n Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too: \n - hosts: servers\n  roles:\n     - { role: bossjones.common, x: 42 }\n \n License \n Apache \n Author Information \n An optional section for the role authors to include contact information, or a website (HTML is not allowed).', 'Role Name \n A brief description of the role goes here. \n Requirements \n Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required. \n Role Variables \n A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well. \n Dependencies \n A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles. \n Example Playbook \n Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too: \n - hosts: servers\n  roles:\n     - { role: bossjones.java, x: 42 }\n \n License \n Apache \n Author Information \n An optional section for the role authors to include contact information, or a website (HTML is not allowed).', 'Role Name \n A brief description of the role goes here. \n Requirements \n Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required. \n Role Variables \n A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well. \n Dependencies \n A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles. \n Example Playbook \n Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too: \n - hosts: servers\n  roles:\n     - { role: bossjones.maven, x: 42 }\n \n License \n Apache \n Author Information \n An optional section for the role authors to include contact information, or a website (HTML is not allowed).', 'Role Name \n A brief description of the role goes here. \n Requirements \n Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required. \n Role Variables \n A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well. \n Dependencies \n A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles. \n Example Playbook \n Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too: \n - hosts: servers\n  roles:\n     - { role: bossjones.swapfile, x: 42 }\n \n License \n Apache \n Author Information \n An optional section for the role authors to include contact information, or a website (HTML is not allowed).', 'Role Name \n A brief description of the role goes here. \n Requirements \n Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required. \n Role Variables \n A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well. \n Dependencies \n A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles. \n Example Playbook \n Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too: \n - hosts: servers\n  roles:\n     - { role: bossjones.pysyslog, x: 42 }\n \n License \n Apache \n Author Information \n An optional section for the role authors to include contact information, or a website (HTML is not allowed).', 'Role Name \n A brief description of the role goes here. \n Requirements \n Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required. \n Role Variables \n A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well. \n Dependencies \n A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles. \n Example Playbook \n Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too: \n - hosts: servers\n  roles:\n     - { role: bossjones.tuning, x: 42 }\n \n License \n Apache \n Author Information \n An optional section for the role authors to include contact information, or a website (HTML is not allowed).', "boss-ansible-role-update-hosts \n Inspired entirely by https://github.com/bertvv/ansible-role-hosts \n An Ansible role for managing the hosts file ( /etc/hosts ). Specifically, the responsibilities of this role are to: \n \n Add the default localhost entry; \n Add an entry for the host name bound to the host's default external IPv4 address (optional); \n Add entries for basic IPv6 addresses, e.g. ip6-localnet (optional); \n Add entries for Ansible managed hosts (optional); \n Add entries specified in Yaml (optional, see below); \n Add entries specified in text files (optional). \n \n Requirements \n No specific requirements \n Role Variables \n None of the variables below are required. When not set, the default setting is applied. \n | Variable                                 | Default                              | Comments                                                                                                          |\n| :---                                     | :---                                 | :---                                                                                                              |\n|  hosts_add_default_ipv4                  | true                                 | If true, an entry for the host name is added, bound to the host's default IPv4 address.                           |\n|  hosts_add_basic_ipv6                    | false                                | If true, basic IPv6 entries are added (e.g. localhost6, ip6-localnet, etc.)                                       |\n|  hosts_add_ansible_managed_hosts         | false                                | If true, an entry for hosts managed by Ansible is added. (†)                                                      |\n|  hosts_add_ansible_managed_hosts_groups  | ['all']                              | Control which host entries are created when using  hosts_add_ansible_managed_hosts  |\n|  hosts_entries                           | []                                   | A list of dicts with custom entries to be added to the hosts file. See below for an example.                      |\n|  hosts_file_snippets                     | []                                   | A list of files containing host file snippets to be added to the hosts file verbatim.                             |\n|  hosts_ip_protocol                       |  ipv4                                | When adding Ansible managed hosts, this specifies the IP protocol ( ipv4  or  ipv6 )                              |\n|  hosts_network_interface                 |  {{ansible_default_ipv4.interface}}  | When adding Ansible managed hosts, this specifies the network interface for which the IP address should be added. |\n|  hosts_file_backup                       | no                                   | If yes, backup of host file is created with timestamp                                                             |\n|                                          |                                      |                                                                                                                   | \n (†) When setting  hosts_add_ansible_managed_hosts , an entry for the current host will also be added. Consequently,  hosts_add_default_ipv4  doesn't need to be set. \n Individual hosts file entries can be added with  hosts_entries , a list of dicts with keys  name ,  ip  and (optional)  aliases . Example: \n Yaml\nhosts_entries:\n  - name: slashdot\n    ip: 216.34.181.45\n  - name: gns1\n    ip: 8.8.8.8\n    aliases:\n      - googledns1\n      - googlens1\n  - name: gns2\n    ip: 8.8.4.4\n    aliases:\n      - googledns2\n      - googlens2 \n Dependencies \n No dependencies. \n Example Playbook \n See the  test playbook \n Tests for this role are provided in the form of a Vagrant environment that is kept in a separate branch,  tests . I use  git-worktree(1)  to include the test code into the working directory. Instructions for running the tests: \n \n Fetch the tests branch:  git fetch origin tests \n Create a Git worktree for the test code:  git worktree add tests tests  (remark: this requires at least Git v2.5.0). This will create a directory  tests/ . \n cd tests/ \n vagrant up  will then create a VM and apply a test playbook ( test.yml ). \n \n You may want to change the base box into one that you like. The current one,  bertvv/centos72  was generated using a Packer template from the  Boxcutter project  with a few modifications. \n Contributing \n Issues, feature requests, ideas are appreciated and can be posted in the Issues section. Pull requests are also very welcome. Preferably, create a topic branch and when submitting, squash your commits into one (with a descriptive message). \n Example /etc/hosts before refactor \n ```\nroot@rsyslogd-master-01:~# cat /etc/hosts\n127.0.0.1       localhost \n The following lines are desirable for IPv6 capable hosts \n ::1     ip6-localhost   ip6-loopback\nfe00::0 ip6-localnet\nff00::0 ip6-mcastprefix\nff02::1 ip6-allnodes\nff02::2 ip6-allrouters\nff02::3 ip6-allhosts\n127.0.1.1       rsyslogd-master-01 \n 192.168.50.108 rsyslogd-master-01 rsyslogd-master-01\n192.168.50.109 rsyslogd-worker-01 rsyslogd-worker-01\nroot@rsyslogd-master-01:~#\n``` \n ipv6 notes \n SOURCE: https://unix.stackexchange.com/questions/234412/whats-the-use-for-the-special-ipv6-addresses-in-etc-hosts \n ::1: This is the loopback address, whose IPv4-equivalent is 127.0.0.1.\nfe00::0: Can be compared to the Class E address space in IPv4, therefore it's in the reserved scope; reserved for future use.\nff02::1: The group of all IPv6 nodes (including the routers) in the Link-local scope (similar to a broadcast address of the subnet in IPv4: 192.168.x.255).\nff02::2: The group of all IPv6 routers in the Link-local scope (also similar to a broadcast in IPv4, but only refering the routers).\nff02::3: This exists no longer an is unassigned at the moment. Earlier it stood for the group of all hosts (excluding the routers) in the Link-local scope.", 'Role Name \n A brief description of the role goes here. \n Requirements \n Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required. \n Role Variables \n A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well. \n Dependencies \n A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles. \n Example Playbook \n Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too: \n - hosts: servers\n  roles:\n     - { role: boss-ansible-role-bootstrap, x: 42 }\n \n License \n Apache \n Author Information \n An optional section for the role authors to include contact information, or a website (HTML is not allowed). \n Influence \n Strong strong influence on this repo from the debops.bootstrap ansible role!!! THIS REPO IS ONLY FOR LEARNING PURPOSES!!! \n https://github.com/debops/ansible-bootstrap/', 'Role Name \n Dump all variables used by Ansible during playbook run to a file for inspection. This role is not active during normal playbook operation and should be used for development only. ( Based on DebOps role!!!!! ) \n Requirements \n Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required. \n Role Variables \n A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well. \n Dependencies \n A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles. \n Example Playbook \n Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too: \n - hosts: servers\n  roles:\n     - { role: boss-ansible-role-debug, x: 42 }\n \n License \n Apache \n Author Information \n An optional section for the role authors to include contact information, or a website (HTML is not allowed).', 'Role Name \n A brief description of the role goes here. \n Requirements \n Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required. \n Role Variables \n A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well. \n Dependencies \n A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles. \n Example Playbook \n Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too: \n - hosts: servers\n  roles:\n     - { role: boss-ansible-role-kernel-tuning, x: 42 }\n \n License \n Apache \n Author Information \n An optional section for the role authors to include contact information, or a website (HTML is not allowed).', 'Role Name \n A brief description of the role goes here. \n Requirements \n Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required. \n Role Variables \n A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well. \n Dependencies \n A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles. \n Example Playbook \n Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too: \n - hosts: servers\n  roles:\n     - { role: boss-ansible-role-ulimit, x: 42 }\n \n License \n Apache \n Author Information \n An optional section for the role authors to include contact information, or a website (HTML is not allowed). \n INSPIRATION \n https://github.com/picotrading/ansible-ulimit \n FOR LEARNING PURPOSES ONLY!', 'Role Name \n A brief description of the role goes here. \n Requirements \n Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required. \n Role Variables \n A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well. \n Dependencies \n A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles. \n Example Playbook \n Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too: \n - hosts: servers\n  roles:\n     - { role: boss-ansible-role-users, x: 42 }\n \n License \n Apache \n Author Information \n An optional section for the role authors to include contact information, or a website (HTML is not allowed). \n Influence \n https://github.com/singleplatform-eng/ansible-users/tree/master/tasks \n THIS IS FOR LEARNING PURPOSES ONLY!!!! \n Example: \n ---\nusers:\n  - username: foo\n    name: Foo Barrington\n    groups: [\'wheel\',\'systemd-journal\']\n    uid: 1001\n    home: /local/home/foo\n    profile: |\n      alias ll=\'ls -lah\'\n    ssh_key:\n      - "ssh-rsa AAAAA.... foo@machine"\n      - "ssh-rsa AAAAB.... foo2@machine"\ngroups_to_create:\n  - name: developers\n    gid: 10000\nusers_deleted:\n  - username: bar\n    name: Bar User\n    uid: 1002\n \n Deleting users \n The  users_deleted  variable contains a list of users who should no longer be\nin the system, and these will be removed on the next ansible run. The format\nis the same as for users to add, but the only required field is  username .\nHowever, it is recommended that you also keep the  uid  field for reference so\nthat numeric user ids are not accidentally reused. \n You can optionally choose to remove the user\'s home directory and mail spool with\nthe  remove  parameter, and force removal of files with the  force  parameter. \n users_deleted:\n  - username: bar\n    uid: 1002\n    remove: yes\n    force: yes\n', 'Role Name \n A brief description of the role goes here. \n Requirements \n Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required. \n Role Variables \n A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well. \n Dependencies \n A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles. \n Example Playbook \n Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too: \n - hosts: servers\n  roles:\n     - { role: boss-ansible-core, x: 42 }\n \n License \n Apache \n Author Information \n An optional section for the role authors to include contact information, or a website (HTML is not allowed). \n Influence \n This is heavily influenced by the Debops project!!!!!!!!!!!!!! Using this for learning purposes only!!!!!!!!!!!!', 'Role Name \n A brief description of the role goes here. \n Requirements \n Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required. \n Role Variables \n A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well. \n Dependencies \n A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles. \n Example Playbook \n Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too: \n - hosts: servers\n  roles:\n     - { role: boss-ansible-fact, x: 42 }\n \n License \n Apache \n Author Information \n An optional section for the role authors to include contact information, or a website (HTML is not allowed).', 'Role Name \n A brief description of the role goes here. \n Requirements \n Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required. \n Role Variables \n A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well. \n Dependencies \n A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles. \n Example Playbook \n Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too: \n - hosts: servers\n  roles:\n     - { role: boss-ansible-role-environment, x: 42 }\n \n License \n Apache \n Author Information \n An optional section for the role authors to include contact information, or a website (HTML is not allowed).', 'Role Name \n A brief description of the role goes here. \n Requirements \n Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required. \n Role Variables \n A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well. \n Dependencies \n A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles. \n Example Playbook \n Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too: \n - hosts: servers\n  roles:\n     - { role: boss-ansible-role-ubuntu-devtop, x: 42 }\n \n License \n Apache \n Author Information \n An optional section for the role authors to include contact information, or a website (HTML is not allowed).', 'Role Name \n A brief description of the role goes here. \n Requirements \n Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required. \n Role Variables \n A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well. \n Dependencies \n A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles. \n Example Playbook \n Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too: \n - hosts: servers\n  roles:\n     - { role: boss-ansible-role-nvm, x: 42 }\n \n License \n Apache \n Author Information \n An optional section for the role authors to include contact information, or a website (HTML is not allowed). \n INSPIRATION: \n Completely inspired by https://github.com/pablocrivella/ansible-role-nvm !!!', 'ubuntu-devtop-vm \n Just a devtop vm for me to try random shit in', 'Role Name \n A brief description of the role goes here. \n Requirements \n Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required. \n Role Variables \n A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well. \n Dependencies \n A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles. \n Example Playbook \n Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too: \n - hosts: servers\n  roles:\n     - { role: boss-ansible-role-bash_it, x: 42 }\n \n License \n Apache \n Author Information \n An optional section for the role authors to include contact information, or a website (HTML is not allowed).', 'Role Name \n A brief description of the role goes here. \n Requirements \n Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required. \n Role Variables \n A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well. \n Dependencies \n A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles. \n Example Playbook \n Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too: \n - hosts: servers\n  roles:\n     - { role: boss-ansible-role-apt, x: 42 }\n \n License \n Apache \n Author Information \n An optional section for the role authors to include contact information, or a website (HTML is not allowed). \n FOR LEARNING PURPOSES ONLY. THIS IS HEAVILY INFLUENCED BY THE DEBOPS PROJECT!!!', 'Role Name \n A brief description of the role goes here. \n Requirements \n Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required. \n Role Variables \n A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well. \n Dependencies \n A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles. \n Example Playbook \n Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too: \n - hosts: servers\n  roles:\n     - { role: boss-ansible-role-pki, x: 42 }\n \n License \n Apache \n Author Information \n An optional section for the role authors to include contact information, or a website (HTML is not allowed).', 'Role Name \n A brief description of the role goes here. \n Requirements \n Any pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required. \n Role Variables \n A description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well. \n Dependencies \n A list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles. \n Example Playbook \n Including an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too: \n - hosts: servers\n  roles:\n     - { role: boss-ansible-role-secret, x: 42 }\n \n License \n Apache \n Author Information \n An optional section for the role authors to include contact information, or a website (HTML is not allowed).']
nik0spapp,['authid  — The attached MATLAB code implements Moshe Koppel\'s et al. improved method introduced in the paper indicated below. The code was developed during my master studies in Information Management at the University of the Aegean under \nthe scope of a semester project for the Machine Learning and Knowledge Discovery course. \n Koppel, Moshe, Jonathan Schler, and Shlomo Argamon. "Authorship attribution in the wild." Language Resources and Evaluation 45.1 (2011): 83-94.\n URL : http://link.springer.com/article/10.1007%2Fs10579-009-9111-2?LI=true \n The detect_author.m is the main file needed for execution and the rest of the files are dependencies for the former. \n Input parameters: \n \n Variable Description Example \n known_text Array with text vectors [t_{1}, t_{2}, ..., t_{n}] \n snippets Array with unknown text vectors [s_{1}, s_{2}, ..., s_{n}] \n authors Array with author names [\'Author 1\', \'Author 2\', ..., \'Author n\'] \n s_authors Array with author snippets [s_{1}, s_{2}, ..., s_{n}] \n k1 Number of iterations 10 \n sigma Final decision threshold (0-100) 80 \n distance_function The desired distance function cosine \n \n where t_{i} and s_{i} are n-dimensional float vectors (e.g. [f_{1}, f_{2}, ..., f_{n}]). \n Output: \n \n Variable Description \n snippet_results Array with author per snippet \n precision Precision scores \n recall Recall scores \n confusion_matrix Table with confusion matrix \n', 'icrawler  — In this repository you can find material related to the opinion mining and retrieval system \nwhich was described in the following paper:  \n @incollection{pappas13c,\n location = {Samos, Greece},\n year = {2013},\n isbn = {978-3-642-37255-1},\n booktitle = {Computational Linguistics and Intelligent Text Processing},\n volume = {7817},\n title = {Distinguishing the Popularity between Topics: A System for Up-to-Date Opinion Retrieval and Mining in the Web},\n author = {Pappas, Nikolaos and Katsimpras, Georgios and Stamatatos, Efstathios},\n pages = {197-209}\n} \n Please check the available code of the individual components that are available: \n- SD algorithm: web page segmentation and noise removal.  \n https://github.com/nik0spapp/webpage_segmentation \n \n Unsupervised sentiment classification using bootstrapping procedures for subjectivity and polarity classification.  \n https://github.com/nik0spapp/unsupervised_sentiment \n \n Contact: \n nik0spapp@gmail.com', 'usent  — The attached code is a Python implementation of a dictionary-based sentiment classification procedure which combines two different bootstrapping procedures, namely for subjectivity and polarity detection (as in [ 3 ,  4 ] respectively). The rule-based polarity classifier is an extension of the one that was presented in [ 5 ]. Moreover,  /TED_comment_annotations folder contains the files of the human study we conducted on TED comment sentiment classification (with 6 human annotators).   \n \n E. Riloff and J. Wiebe. Learning extraction patterns for subjective expressions.\nIn Proceedings of the 2003 conference on Empirical methods in natural language\nprocessing, 2003.   \n D. K. M Wiegand. Bootstrapping supervised machine-learning polarity classifiers with rule-based classification. \nIn Proceedings of the ECAI-Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, 2009.   \n T. Wilson, J. Wiebe, and P. Hoffmann. Recognizing contextual polarity in phrase-level sentiment \nanalysis. In Proceedings of the conference on Human Language Technology and Empirical Methods in \nNatural Language Processing, 2005.  \n \n The code was used for an opinion mining and retrieval system presented at CICLing 2013 [ 1 ],  and for improving one-class collaborative filtering [ 2 ].  \n ```\n@incollection{pappas13c,\n location = {Samos, Greece},\n year = {2013},\n booktitle = {Computational Linguistics and Intelligent Text Processing},\n volume = {7817},\n doi = {10.1007/978-3-642-37256-8_17},\n title = {Distinguishing the Popularity between Topics: A System for Up-to-Date Opinion Retrieval and Mining in the Web},\n author = {Pappas, Nikolaos and Katsimpras, Georgios and Stamatatos, Efstathios},\n pages = {197-209}\n} \n @inproceedings{pappas13a,\n author = {Pappas, Nikolaos and Popescu-Belis, Andrei},\n title = {Sentiment Analysis of User Comments for One-Class Collaborative Filtering Over {TED} Talks},\n booktitle = {Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval},\n series = {SIGIR \'13},\n year = {2013},\n isbn = {978-1-4503-2034-4},\n location = {Dublin, Ireland},\n pages = {773--776},\n numpages = {4},\n} \n``` \n Installing dependencies \n The available code for unsupervised sentiment classification requires Python programming \nlanguage and pip package manager to run. For detailed installing instructions please refer to \nthe following links:  \nhttp://www.python.org/getit/  \nhttp://www.pip-installer.org/en/latest/ \n After installing them, you should be able to install the following packages:  \n bash\n$ pip install nltk  \n$ pip install stemmer \n$ pip install numpy\n$ pip install pickle \n After you install nltk you will need some corpora to train the sequential POS tagger (pos.py) and the nltk tokenizer.\n bash\n$ python \n python\nimport nltk \nnltk.download() \nThe issue of the above command will load a graphical interface that lets you manage several corpora\nrelated to nltk library. From the list select and download the following corpora: \n tokenizers/punkt/english ,  wordnet ,  brown ,  conll2000  and  treebank .  \n Lastly, pyml library is needed for the SVM classifier that is used currently in our code.  \nDownload http://pyml.sourceforge.net/ and then issue:  \n bash \n $ tar zxvf PyML-0.7.11.tar.gz\n $ cd PyML-0.7.11\n $ python setup.py build\n $ python setup.py install \n Processing pipeline \n The current pipline that is implemented in sentiment.py is depicted in the following diagram. Initially,\nthe input text is split into sentences and each sentence is fed to a high precision subjectivity classifier.\nIf the sentence is classified as subjective then syntactic patterns are learned from this instance. In case \nthat the sentence is not detected as such then it is fed to the pattern-based classifier. The pattern-based\nclassifier outputs the class of the sentence based on the learned patterns so far. If the instance is subjective\nthen again more patterns are learned from it, otherwise it is fed to a high precision objectivity classifier.\nIf the sentence is classified as objective, then it is ignored, otherwise it is fed to the polarity classifier.\nFinally, the polarity classifier estimates the numerical sentiment and normalized sentiment values and outputs\nthe result. The instances with high confidence from the polarity classifier can be further used to train an SVM \nclassifier to improve further the classification performance (see paper for further details). At the current version\nthis option is disabled, but you can easily enable it. Similarly, you can remove some of the components from the \npipeline according to your needs (e.g. skip subjectivity classification). \n \n \n \n Examples \n To estimate the total sentiment and total normalized sentiment (as described in the papers), \nyou can simply execute the sentiment.py file and give the desired block of text as an argument.\nMake sure that you escape symbols such as \'"\' and \'!\'. Apart from the command line execution you \ncan integrate the library to your code and use directly the returned results. Below you can \nfind two simple examples for demonstrating purposes: \n bash\n$ python sentiment.py "I have to give much love and respect to Rony. Your work is Amazing\\\\!" \n```\n[+] Loaded existing UBT tagger!\n[+] Loaded existing pattern knowledge! \n [*] Checking block of text:\n[1] I have to give much love and respect to Rony.\n[2] Your work is Amazing! \n [*] Overall sentiment analysis: \n Parts:  2\n Sentiments:  [\'positive\', \'positive\']\n Scores:  [4, 6.0]\n Results:  {\'positive\': {\'count\': 2, \'score\': 10.0, \'nscore\': 1.9},\n        \'neutral\': {\'count\': 0, \'score\': 0, \'nscore\': 0},\n        \'negative\': {\'count\': 0, \'score\': 0, \'nscore\': 0}} \n subjective-----> 100.00%\n objective------> 0.00% \n positive-------> 100.00%\n neutral--------> 0.00%\n negative-------> 0.00% \n [x] positive (10.00, 1.90)\n``` \n bash\n$ python sentiment.py "I was blown away by some of the comments here posted by people who is either \nuneducated, ignorant, self-righteous or al-of-the-above. I\'m irritated and saddened as I read these \nfinger-pointing \\"i\'m right and you\'re wrong\\" type of posts\\!" \n```\n[+] Loaded existing UBT tagger!\n[+] Loaded existing pattern knowledge! \n [*] Checking block of text:\n[1] I was blown away by some of the comments here posted by people who is either uneducated, ignorant, self-righteous or al-of-the-above.\n[2] I\'m irritated and saddened as I read these finger-pointing "i\'m right and you\'re wrong" type of posts! \n [*] Overall sentiment analysis: \n Parts:  2\n Sentiments:  [\'negative\', \'negative\']\n Scores:  [-4, -4.0]\n Results:  {\'positive\': {\'count\': 0, \'score\': 0, \'nscore\': 0},\n        \'neutral\': {\'count\': 0, \'score\': 0, \'nscore\': 0},\n        \'negative\': {\'count\': 2, \'score\': -8.0, \'nscore\': -0.3722943722943723}} \n subjective-----> 100.00%\n objective------> 0.00% \n positive-------> 0.00%\n neutral--------> 0.00%\n negative-------> 100.00% \n [x] negative (-8.00, -0.37)\n```', 'sdalg  — The attached code is a Python version of the style-density tree based algorithm, which was described in the following paper. The SD algorithm performs web page segmentation and noise removal and then returns the identified web page type (Article, Article with Comments and Multiple areas) along with the region annotations per type. \n @inproceedings{pappas12,\n author = {Pappas, Nikolaos and Katsimpras, Georgios and Stamatatos, Efstathios},\n title = {Extracting Informative Textual Parts from Web Pages Containing User-generated Content},\n booktitle = {Proceedings of the 12th International Conference on Knowledge Management and Knowledge Technologies},\n series = {i-KNOW \'12},\n year = {2012},\n isbn = {978-1-4503-1242-4},\n location = {Graz, Austria},\n pages = {4:1--4:8},\n articleno = {4},\n numpages = {8},\n doi = {10.1145/2362456.2362462}, \n}   \n Installing dependencies \n The available code for webpage segmentation requires Python programming \nlanguage and pip package manager to run. For detailed installing instructions please refer to \nthe following links:  \nhttp://www.python.org/getit/  \nhttp://www.pip-installer.org/en/latest/ \n After installing them, you should be able to install the following packages:  \n bash\n$ pip install nltk  \n$ pip install urllib \n$ pip install lxml \n Examples \n To run the SD algorithm simply execute the sd_algorithm.py file and give as parameter \nthe URL of your preference. Make sure that you use double quotes in case of weird parameters\non the URL, check examples below. Lastly, the algorithm relies on two thresholds that have\nto be tuned on a subset of your target documents (see the related paper), otherwise the \nsegmentation may not be as expected.  \n ```\n$ python sd_algorithm.py http://www.bbc.co.uk/news/world-africa-12328506\n[ ] Create DOM tree...\n[ ] Calculating initial groups...\n[ ] Merging groups...\n[ ] Creating regions...\n[ ] Calculating distances from max region...\n[ ] Printing regions... \n [*] Validating candidate comment group based on its content... \n [INFO:] Article detected!\nArticle class: \n[x] \'story-body\'\nArticle title: \nEgypt protest: \'Carnival atmosphere\' among demonstrators\nArticle text: \nFor many in Tahrir Square in central Cairo, the days are starting to take on a familiar pattern. After \nnearly a week of demonstrations, many people now sleep here. There are a few tents and pieces of cardboard \nthat serve as beds on a small patch of grass in front of a government building, the Mugamma. "We get just \nfour hours sleep or so and then we wake up to start the protest again," said Samah al-Dweik, who has not been \nto her home in Maadi, just outside the city, since Friday.  "We do not know how long we will have to continue. \nOnly if Mubarak goes, will we go home. \n(...)\nThey declare: "I\'m free" and "Game over" but also demand policy changes from Western countries that have \nsupported the Mubarak government. "US: we hate your hypocrisy" read one banner, referring to the disparity \nbetween American calls for human rights and democracy and its support of their president. "Listen to the Egyptian \npeople," another demanded. Despite an official curfew, the numbers in the square swell in early evening and the \nchants increase in volume. Protesters are only too aware of the government\'s hope that by delaying its response \nto their demands it will drain their energy.  But they say they are determined to prove otherwise.  \n [INFO:] No comments found.\n``` \n ```\n$ python sd_algorithm.py http://www.care2.com/greenliving/chocolate-may-reduce-risk-of-heart-failure.html\n[ ] Create DOM tree...\n[ ] Calculating initial groups...\n[ ] Merging groups...\n[ ] Creating regions...\n[ ] Calculating distances from max region...\n[ ] Printing regions... \n [*] Validating candidate comment group based on its content... \n [INFO:] Article with comments detected!\n[INFO:] Article detected!\nArticle class: \n[x] \'article_content\'\nArticle title: \nChocolate May Reduce Risk of Heart Failure\nArticle text: \nForget what you’ve heard about death by chocolate.\xa0 A new Harvard study shows that chocolate may be good \nfor your heart. It\'s a great day for chocolate lovers everywhere. Murray Mittleman and his colleagues \nat Harvard Medical School studied data on 31,823 middle-aged and elderly Swedish women to assess the \nrelationship between chocolate and heart failure.\xa0 The women who consumed an average of one to two servings \n(that’s a fairly small amount) of high-quality, cocoa-rich chocolate per week had a 32 percent lower risk \nof experiencing heart failure. Those women who ate 1 to 3 servings a month had a 26 percent lower risk of \nheart failure. The scientists noted that the high concentration of phytonutrients called flavonoids in \ndark chocolate are potent antioxidants that are likely responsible for the results.\xa0 The flavonoids are \nbelieved to lower blood pressure and reducing inflammation linked with heart failure. Keep in mind that \nnot just any chocolate will do.\xa0 Forget the vast majority of candy bars on the market.The study results \nwere achieved with high-quality, cocoa-rich chocolate.\xa0 Read DARK chocolate.\xa0 The darker the better.\xa0 \nAnd, be sure the one you choose is low in sugar, has no trans or hydrogenated fats, and no artificial \ncolors, flavors, or other synthetic ingredients. Related:Easy Greening: Chocolate 101Chocolate: Fact vs. \nFictionDark Chocolate Definitely Eases Emotional StressChocolate Tantric Pie Subscribe to my free \ne-newsletter World\'s Healthiest News for more cutting-edge health news, tips, recipes, and more.  \n Comment class:\n[x] comment_text contain_floats\nComments: \n I love chocolates and I\'m very pleased on how nutritious it is. I gained a lot of information about \nchocolates on chocolarious.com and now I\'m a certified chocoholic. lol.Also, try checking out \nmilkdelight.com, coffeefashion.com, everything-cake.com, and zcocktails.com. => \n The darker the better - so says the author. However, keep in mind that something is better than nothing \n- like the bournvita, cocoa and other drinks. BTW, the article is a reminder to me to have today\'s \nquota of my chocolate! \n I love chocolate. I can go to bed with chocolate in my mouth. Not good for my teeth or weight, but I \ndont have a teeth or weight problem. I guess I\'m just lucky. I would have to eat at least 4oz\'s of \nchocolate a day. Addicted to the great stuff. Thanks for the assurance I\'m doing the right thing by \neating so much of it. :) \n Thank you for sharing.\n``` \n ```\n$ python sd_algorithm.py "http://www.lonelyplanet.com/thorntree/forum.jspa;jsessionid=57DA8CB66960A9D820CAB16BB221094D.app01?forumID=34&errorMsg=The%20thread%20requested%20is%20not%20currently%20available"\n[ ] Create DOM tree...\n[ ] Calculating initial groups...\n[ ] Merging groups...\n[ ] Creating regions...\n[ ] Calculating distances from max region...\n[ ] Printing regions... \n [INFO:] Multiple similar regions detected!\n[x]\nTexts:  \n Hi All,Am going for a six month trip to Central Asia, Nepal and China.I need to sort our my \nconnectivity needs...I will have a mac and a smartphone with me (samsung Note 2, unlocked). I would \nlike to find the most convenient way to get on the internet and use the phone locally (no real need \nfor voice long distance, and i can always use skype for that).What options do i have considering \nthat. - barring nepal - i will not be staying long in any country (2-3 weeks max).The main priority \nis data really... I guess that when i have the ability to downoad data i can always work out my local \ncalls through skype...Is it better to buy a local sim card with data/voice capability in every country \n(ie uzbekistan, tajikistan, kyrgyzstan) or should i go for an international sim card (which one?)? Or \nmaybe a combination of the two?Ideas gratefully received...With thanks,Str...more » \n Hello everybodyMy boyfriend, who is a carpenter and cabinet maker, is going to work for a few months \nto help fix up an old 17th century farmhouse in a rural area of the Hérault, France. There is no \ninternet connection for miles, so in order to communicate more effectively we were thinking of buying \na tablet that holds a SIM card to Skype or internet. When I was there I noticed Bouygues Telecom was \nthe main carrier and they have a good deal for a prepaid SIM card.Formule 24/24, la recharge 20€ est \nvalable 1 moisPour 20€ rechargés, vous bénéficiez pendant 1 mois :- d\'appels et de SMS illimités 24/24+- \nde 250 Mo d\'Internet 3G +- de 4€ de crédit de réserve offertThat sorted, we would love some advice on \nthe tablet. Could you please recommend any that:\nhas a Windows operating system (cause that\'s wha... \n Not quite sure this thread belong here, but oh well.Originally from Australia and I am beginning to \ntravel the world by van from the 28th march onwards, starting in NZ and AUS.Now I know whats required \nfor car registration in Australia, but I have absolutely no idea what I do with registration whilst \noverseas(planning to be overseas for 12+ years).Do you just register in the country you are a citizen? \nIs there a world registration group?Just what do I have to do to keep on the right side of the laws \naround the globe?Thanks for your timeand happy travels. \n Does anyone know how easy it is to draw a route on Google Earth and then export it into a SatNav app \non a mobile phone, to use off-line?I\'ve moved travelled routes from my Garmin GPS device into Google \nEarth before, could probably do it the other way if necessary, I guess (but probably not enough storage \nspace for what I\'m thinking of). Thinking of getting a phone that has GPS in it (moving into the 21st \ncentury - haven\'t actually ever owned a mobile phone yet!). \n Flikr is not the only place to display photos nowadaze... Tumblr and Twitter will accept photos \nuploaded directly from your Flikr account...with just one click of the mouse... Although both Tumblr \nand Twitter have photo upload capabilities...Tumblr allows many more ways to display your photos on \nthe Net...And Twitter has a media sidebar feature for photos...which can appear as a slide show...\nright next to your Tweets...You don\'t have to use their twitpic feature if you have a Flikr account...\nhttp://vasenka.tumblr.com/ \n I am looking for a good MS Windows based program that can keep track of places I have visited. I \nwould like to be able to view it hopefully by continent, country, states/provinces/regions/etc, cities/\ntowns/etc. I currently do this on a MS Excel spreadsheet, but would like to be able to use it to view \nmaps indicating somehow the locations I have visited, as well as lists.I would also like to add details \nof my trips to each location, including possibly photos.I live in the U.S.A., and am currently working \non visiting all 50 states (40 of them done, with 2 more to be added this year).That said, I have also \ntravelled internationally. To date I have visited 31 countires across 4 continents, and hope to do more \nin the future.Any thoughts? Anything out there that can do something like this? \n Hey everyone i recently took up photography as a hobby and a really enjoying it. i\'ll be leaving on my \nRTW trip in about a month and will be going through parts of europe first then the middle east, asia \nand south and then north america. yet i want to only carry essential gear for my photography yet still \nwant to take great shots. I\'m using a Sony A57 with a 18-35mm lens, still in the market for a larger \nand better lens yet i want to ask those here who have done travel photography what essential gear should \ni bring and what can i do without.i\'ll certainly be taking as compact a tripod as i can find yet i\'ll \nalso be taking filters and lens hoods. this gear is already starting to feel bulky in my mind but can \ni afford to do without these items in the least and is there any other ones i should consider. looking \nforward to all replies. \n Sorry, not sure which branch to put this under.I wanted to buy something online in a foreign currency. \nI have done this many times whilst abroad but just wanted to check with the company if it was okay. The \ncompany told me that they only accept £ UK pounds and euros. I wanted to pay in US$. The question is \n-If I pay in US$ for a euro transaction, does my bank send dollars for the equivalent amount and the \nreceiving bank does the exchange? Or, does my bank send euros? I am paying the exchange rate charges so \nI assume my bank sends euros. If that is the case then the receiving company gets euros, so I am a bit \nconfused why they would say that. It is a European company, the bank is in Europe, I am European, the \ndelivery and invoice address is European, so there are no issues with customs/tax.Thanks. \n Greetings, travelers!Heading to South Luangwa National Park in Zambia and then making way by bus to \nKruger in South Africa over a period of weeks.Wondering if anyone reading this has had experience with\nputting a sim in a Boostmobile or Sprint HTC Design Evo 4g? Were you able to get it working on data , \nand if so what speeds? Did the wi fi hotspot function work ?If not HTC, did anyone use a usb modem mi \nfi wi fihotspot in South Africa, Botswana, or Zambia?Here\'s and example of the device I am talking \nabout :Huwaei BROADBAND MIFI WIFI RouterOr did you purchase a local data modem stick and which company \ndid you use?Thanks \n My last few travel cameras have been from the Panasonic TZ series but the last 2 cameras that I\'ve \nhad have readily gotten what looks like dirt in the lens. Has anyone else experienced this problem? I \nwant to get a new camera to replace the one with dust in the lens (it is now no longer under warrenty \nso can\'t get it fixed cheaply) and am wondering whether to go for something else this time and what? \nI like the compact high zoom format of the Panasonic TZ series but maybe this is also the reason for \nthe problems? Any recommendations? \nThanks!\n``` ', "wmil  — The attached code is a Python implementation of the multiple-instance learning algorithm for aspect-based \nsentiment analysis which was proposed in the paper listed below. Moreoever, the features \nextracted from seven datasets are provided for research purposes.  \n @InProceedings{pappas14,\n  author    = {Pappas, Nikolaos  and  Popescu-Belis, Andrei},\n  title     = {Explaining the Stars: Weighted Multiple-Instance Learning for Aspect-Based Sentiment Analysis},\n  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},\n  month     = {October},\n  year      = {2014},\n  address   = {Doha, Qatar},\n  publisher = {Association for Computational Linguistics},\n  pages     = {455--466},\n  url       = {http://www.aclweb.org/anthology/D14-1052}\n} \n A more scalable version of the above algorithm based on stochastic gradient descent can be found here:   wmil-sgd .  \n Installing dependencies \n The available code requires Python programming language and pip package manager to run. \nFor detailed instructions on how to install it along with a package manager please refer \nto the following links: http://www.python.org/getit/ and http://www.pip-installer.org/en/latest/. \n Next, you should be able to install the following packages:  \n bash\n$ pip install numpy \n$ pip install scikit-learn\n$ pip install scipy \n Training and testing the model \n The code extends BaseEstimator class from scikit-learn package, so you should be able to use it as a common sklearn estimator (check more details on http://scikit-learn.org/stable/). For example:\n```bash\n$ python \n \n \n \n import pickle\nfrom wmil import APWeights\nfrom sklearn.metrics import mean_absolute_error\ndata = pickle.load(open('features/ted_comments.p'))\nsize = len(data['X'])\nk = int(size*0.5)\nx_train = data['X'][:k]\ny_train = data['Y'][:k]\nx_test = data['X'][k:]\ny_test = data['Y'][k:]\nmodel = APWeights(20, e1=1.0, e2=1.0, e3=1.0)\nmodel.fit(x_train, y_train)\n[+] Training...\n--/start\niteration 0 -> (MAE: 0.103437)\niteration 1 -> (MAE: 0.089629)\niteration 2 -> (MAE: 0.087793)\niteration 3 -> (MAE: 0.087565)\niteration 4 -> (MAE: 0.087523)\niteration 5 -> (MAE: 0.087515)\niteration 6 -> (MAE: 0.087510)\niteration 7 -> (MAE: 0.087511)\n--/end\nmean_absolute_error(model.predict(x_train),y_train) # training error\n0.096217463769192518\nmean_absolute_error(model.predict(x_test), y_test) # testing error\n0.16325402985689552\n``` \n \n \n \n Contact: \n npappas@idiap.ch", '']
ematvey,['NanoEngineer-1 \n CAD for design of molecular machines. \n What was done \n \n Applied a few patches for OS X compatibility. \n \n Contacts \n Original project:  nanoengineer-1.com \n Current source: https://github.com/kanzure/nanoengineer', "pybacktest \n Simple yet powerful backtesting framework in python/pandas. \n Currently I don't plan to continue working on this project. \n About \n It allows user to specify trading strategies using full power of pandas, at the same time hiding all boring things like manually calculating trades, equity, performance statistics and creating visualizations. Resulting strategy code is usable both in research and production setting. \n Strategies could be defined as simple this:\n python\nms = pandas.rolling_mean(ohlc.C, 50)\nml = pandas.rolling_mean(ohlc.C, 100)\nbuy = cover = (ms > ml) & (ms.shift() < ml.shift())\nsell = short = (ms < ml) & (ms.shift() > ml.shift()) \n And then tested like this:\n pybacktest.Backtest(locals()) \n We use it in our research and production operations. \n Installation \n pip install git+https://github.com/ematvey/pybacktest.git \nIf you don't install it in virtualenv, you might need to prepend last line with sudo. \n Tutorial \n Tutorials are provided as ipython notebooks in folder  examples . You run it from cloned repo or  watch via nbviewer . \n Status \n Single-security backtester is ready. Multi-security testing could be implemented by running single-sec backtests and then combining equity. Later we will add easier way.", 'pystuff \n Collection of useful Python codes', 'RedisPyObj \n Python-redis proxy read/write objects: dict, list, set. \n TODO:\n* package, setup.py\n* auto serialization', 'golearn \n Machine Learning with Go', 'etsys \n Toy trading simulator and infrastructure in Go', "ratelimiter \n Rate limiting library written in Go, using redis as it's backend.", 'pcache', 'GoNets \n Running Neural Networks in Go \n GoNets allows you to specify and run feed-forward neural networks. It does not support training, and I have no plans to implement that in immediate future. The goal of this package is to integrate nets trained elsewhere into golang projects. \n Specifying networks is as simple as:\n golang\nbiases := []float64{0.0, 0.1, -0.3}\nweights := [][]float64{\n    {0.3, 0.4, -0.1},\n    {-0.1, 0.1, 0.0},\n    {-0.3, 0.0, 0.0},\n}\nnet, err := SoftmaxLayer(InputLayer(3), biases, weights)\npredictions, err := net.GetOutput([]float64{3, 2, 1})', 'gostat \n Collection of statistical routines in golang \n Forked from code.google.com/p/gostat on 28.04.2015', 'Python Data Science Lab in Docker', 'Silk - Hierarchical Data Smoothing in Python \n Loosly related to the following statistical procedures: \n- https://en.wikipedia.org/wiki/Inverse_probability_weighting\n- https://en.wikipedia.org/wiki/Additive_smoothing', "seq2seq with TensorFlow \n Collection of unfinished tutorials. May be good for educational purposes. \n 1 -  simple sequence-to-sequence model with dynamic unrolling \n \n Deliberately slow-moving, explicit tutorial. I tried to thoroughly explain everything that I found in any way confusing. \n Implements simple seq2seq model described in  Sutskever at al., 2014  and tests it against toy memorization task. \n \n \n Picture from  Sutskever at al., 2014 \n 2 -  advanced dynamic seq2seq \n \n Encoder is bidirectional now. Decoder is implemented using  tf.nn.raw_rnn . It feeds previously generated tokens during training as inputs, instead of target sequence. \n \n \n Picture from  Deep Learning for Chatbots \n 3 -  Using  tf.contrib.seq2seq  (TF<=1.1) \n \n New dynamic seq2seq appeared in r1.0. Let's try it. \n \n UPDATE: that this tutorial doesn't work with tf version > 1.1, API. I recommend checking out new  official tutorial  instead to learn high-level seq2seq API.", "Deep Text Classifier \n Implementation of document classification model described in  Hierarchical Attention Networks for Document Classification (Yang et al., 2016) . \n How to run \n \n \n Create a virtual environment, activate it, and install requirements:\n python3 -m venv env\nsource env/bin/activate\npip install -r requirements.txt \n \n \n Download the English model for spaCy: \n \n \n python -m spacy download en \n \n Get  Yelp review dataset  and extract it in this directory.\n python3 yelp_prepare.py dataset/review.json\npython3 worker.py --mode=train --device=/gpu:0 --batch-size=30 \n \n Results \n I am getting 65% accuracy on a dev set (16% of data) after 3 epochs. Results reported in the paper are 71% on Yelp'15.\nNo systemic hyperparameter optimization was performed.", "AI copywriter \n Given a large corpus of correct English texts and a one-way mechanism to corrupt a sentence in such a way that will be similar to mistakes that non-native speaker makes, we can develop seq2seq model that acts like denoising autoencoder. It could be used by non-proficient English writers to write better texts. \n Main hypothesis is that corruption does not have to be very clever, just sufficiently diverse. Currently I use a set of simple heuristics for corruption, but later hope to develop a generative model for that. \n Status \n Trying to make it work.\nLSTMs seem to be less volatile then GRUs. Deeper nets (5 layers vs 3 layers) seem to converge to higher loss. \n Probably unnecessary warning: this is research project, not in any way suitable for production use. No API stability guarantee of any kind is given. If you want to derive from this work, copy-paste it! \n How to run \n Requires Python >=3.5 and TensorFlow r1.0.0. \n Download Wikipedia dump e.g. with  \n wget -c https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2 \n Install deps. \n Run preprocessor: \n python3 wiki_prepare.py path/to/wikidump \n It should take several hours. Corruption types that preprocessor is doing could be found in wiki_data_op.py. \n Run training: \n python3 worker.py --device=/gpu:0 --batch-size=30 \n You may want to increase/decrease batch size based on your GPU mem. I have GTX 970 with 4gb. \n Related work \n @atpaino  implemented similar model  on smaller corpus in Jan 2017. His work is based on tensorflow's  translate  example, which uses static RNN rollout with bucketizing. In this work I attempt to implement dynamic RNN rollout using tensorflow's lower-level blocks. \n License \n MIT", 'pytorch-rnn \n Collection of RNN research models in PyTorch \n Contents \n Seq2Seq with attention + toy copy task', 'general-ai \n Agent training (with all optional parameters):\n python reinforce_commai.py -t tasks/micro1.json -o "output-{}" --id 0 -l INFO \n Task schedulers \n Task scheduler  tasks/micro1.json  contains only micro1 task with infinite rotation. \n Task scheduler  ../Round1/src/tasks_config.challenge.json  is a scheduler proposed by organizers.', 'Carvana Semantic Segmentation - solution \n [ Challenge link ] \n This solution gets 0.9945 dice, which is within 0.25% from current top, but still places in the middle of the leaderboard (competition requires pixel-perfect segmentation).  \n This is mainly for-fun solution, without any special tricks (no ensembling, no data augmentation), with very little hyperparameter tuning. \n Architecture: slightly customized UNet (so that it can process images in native 1918x1280 resolution) \n Framework: PyTorch \n Running \n \n Extract competition data in  input/ \n Preprocess data with  python dataset.py  (20 min) \n Train to convergence with  python train.py  (12+ hours) \n If necessary, generate submission with  python make_submission.py \n \n License \n MIT']
keskarnitish,['', "OBA \n Author:  Nitish Shirish Keskar \n OBA  is a second-order method for convex L1-regularized optimization with active-set prediction. \nOBA belongs to the family of  Orthant-Based  methods (such as OWL) and uses a selective-corrective mechanism which brings about increased efficiency and robustness.  \n Features \n The OBA package \n \n allows for solving general convex L1-regularized problems including Logistic Regression and LASSO. \n is written in pure-MATLAB with minimal dependencies and emphasizes simplicity and cross-platform compatibility.  \n includes both Newton and quasi-Newton options for the proposed algorithm. \n \n Usage Guide \n The algorithm can be run using the syntax  \n x = OBA(funObj,lambda,[options]); \n Here,\n*  funObj  is an object with member functions for computing the function, gradient and Hessian-vector products at the iterates. Logistic Regression and LASSO classes are provided with the package. The file  funTemplate.m  can be used as a base for designing a custom function.\n*  lambda  is the positive scalar for inducing sparsity in the solution.\n*  options  is an optional argument for changing the default parameters used in OBA. For ease of use, the user can generate the default options struct using  options=GenOptions()  and change the parameters therein before passing it to OBA. \n The parameters and their default values are\n*       options.optol : termination tolerance\n          (default: 1e-6)\n*       options.qn : Quasi-Newton, 0 (Newton's Method), or 1 (quasi-Newton)\n          (default: 0)\n*       options.mem_size : quasi-Newton memory size\n          (default: 20)\n*       options.maxiter : max number of iterations\n          (default: 1000)\n*       options.printlev : print level, 0 (no printing) or 1\n          (default: 1)\n*       options.CGtol : CG termination tolerance (for Newton's Method)\n          (default: 1e-1)\n*       options.maxCGiter : max number of CG iterations (Newton's Method)\n          (default: 1000). \n For a detailed documentation of OBA and its associated functions, use  help OBA . \n Citation \n If you use OBA for your research, please cite the paper\n @article{OBA_Keskar2016,\nauthor = {N. Keskar and J. Nocedal and F. Öztoprak and A. Wächter},\ntitle = {A second-order method for convex -regularized optimization with active-set prediction},\njournal = {Optimization Methods and Software},\nvolume = {0},\nnumber = {0},\npages = {1-17},\nyear = {0},\ndoi = {10.1080/10556788.2016.1138222},\nURL = {http://dx.doi.org/10.1080/10556788.2016.1138222},\neprint = {http://dx.doi.org/10.1080/10556788.2016.1138222}\n}", 'Templates \n Templates for (Smooth) Nonlinear Optimization', 'minSQN : Stochastic Quasi-Newton Optimization in MATLAB \n Authors:  Nitish Shirish Keskar  and  Albert S. Berahas \n Please contact us if you have any questions, suggestions, requests or bug-reports. \n Introduction \n This is a package for solving an unconstrained minimization\nproblem of the form,\nmin f(x) = (1/n)*sum_i f_i(x). \n minSQN allows for the user to solve large-scale (sum-of-functions)\noptimization problems using one of 11 Stochastic Quasi-Newton methods. \n The following table summarizes all the methods that minSQN contains. The\nmethods are classified in terms of:\n- Hyperparameters\n- Length of LBFGS memory (limited/inf) [if inf, BFGS method used]\n- Powell damping\n- Hessian damping\n- Curvature pair update (y) \n +----------+-----------------+--------------+----------------+-----------------+------------------------+-----------------------------+\n|  Method  | Hyperparameters | LBFGS Memory | Powell Damping | Hessian Damping |     Curvature pair     |          Reference          |\n|          |                 | (finite/inf) |      (Y/N)     |      (Y/N)      |       (y) update       |                             |\n+----------+-----------------+--------------+----------------+-----------------+------------------------+-----------------------------+\n|    SQN   |     alpha, L    |    finite    |        N       |        N        | Hessian-vector product |     Byrd et. al. (2014)     |\n+----------+-----------------+--------------+----------------+-----------------+------------------------+-----------------------------+\n|   DSQN   |     alpha, L    |    finite    |        Y       |        N        | Hessian-vector product |              --             |\n+----------+-----------------+--------------+----------------+-----------------+------------------------+-----------------------------+\n|   oBFGS  |   alpha, delta  |      inf     |        N       |        Y        |  Gradient differencing |  Schraudolph et. al. (2007) |\n+----------+-----------------+--------------+----------------+-----------------+------------------------+-----------------------------+\n|  oLBFGS  |   alpha, delta  |    finite    |        N       |        N        |  Gradient differencing | Schraudolph et. al. (2007), |\n|          |                 |              |                |                 |                        |   Mokhtari et. al. (2014)   |\n+----------+-----------------+--------------+----------------+-----------------+------------------------+-----------------------------+\n|  D-oBFGS |   alpha, delta  |      inf     |        Y       |        Y        |  Gradient differencing |              --             |\n+----------+-----------------+--------------+----------------+-----------------+------------------------+-----------------------------+\n| D-oLBFGS |   alpha, delta  |    finite    |        Y       |        N        |  Gradient differencing |              --             |\n+----------+-----------------+--------------+----------------+-----------------+------------------------+-----------------------------+\n|    RES   |   alpha, delta  |      inf     |        N       |        Y        |  Gradient differencing |   Mokhtari et. al. (2014)   |\n+----------+-----------------+--------------+----------------+-----------------+------------------------+-----------------------------+\n|   L-RES  |   alpha, delta  |    finite    |        N       |        N        |  Gradient differencing |                             |\n+----------+-----------------+--------------+----------------+-----------------+------------------------+-----------------------------+\n|  SDBFGS  |   alpha, delta  |      inf     |        Y       |        Y        |  Gradient differencing |     Wang et. al. (2014)     |\n+----------+-----------------+--------------+----------------+-----------------+------------------------+-----------------------------+\n| L-SDBFGS |   alpha, delta  |    finite    |        Y       |        Y        |  Gradient differencing |              --             |\n+----------+-----------------+--------------+----------------+-----------------+------------------------+-----------------------------+\n|   adaQN  |     alpha, L    |    finite    |        N       |        N        | Hessian-vector product |    Keskar et. al. (2015)    |\n|          |                 |              |                |                 |   accumulated Fisher   |                             |\n+----------+-----------------+--------------+----------------+-----------------+------------------------+-----------------------------+ \n Features \n The minSQN package \n \n is written in pure-MATLAB with minimal dependencies and emphasizes simplicity, extendibility and cross-platform compatibility.  \n allows the user to run 11 different stochastic quasi-Newton methods which are able to solve a vast array of problems (both convex and non-convex).  \n comes with an automatic hyperparameter tuning mechanism thus obviating the need for manually tuning the parameters for any of the included methods.  \n \n Citation \n If you use minSQN for your research, please cite the Github repository: \n @misc{minSQN2016,\n   author = "Nitish Shirish Keskar and Albert S. Berahas",\n   title = "{minSQN}: {S}tochastic {Q}uasi-{N}ewton {O}ptimization in {MATLAB}",\n   year = "2016",\n   url = "https://github.com/keskarnitish/minSQN/",\n   note = "[Online]"\n } \n Usage Guide \n The algorithm can be run using the syntax  \n logger = minSQN(problem,options,[hyperparameters]); \nwhere,\n*  problem  is an object pertaining to a specific loss function and data set. \n*  options  is a struct containing the necessary parameters for use in the optimization algorithms. \n*  hyperparameters  is an array of hyperparameters necessary for the optimization algorithms such as the step-size, damping constants and aggregation lengths. This is an optional argument. If it is not specified, minSQN uses its inbuilt automatic tuner (which we describe next) to find hyperparameters.  \n Automatic Tuning: \n All method above have certain hyperparameters that need to be set or\ntuned. The second column in the table above indicates what\nhyperparameters are needed for each of the methods. \n In minSQN, we provide an automatic tuning mechanism that randomly samples hyperparameters (as in Bergstra et. al. (2012)) from a prespecified range of hyperparameter values, solves the problem several times, and returns the best optimization run and hyperparameter setting. The number of tuning steps is determined in the options (default is 10).   \n Example (no tuning): \n To solve a problem using minSQN, the user must follow 4 steps: \n \n Construct the problem class (Logistic Regression and Least Squares are included with the minSQN code. Others can be coded easily using our template) \n Generate default options using  GenOptions()  and over-write them as needed \n Set the hyperparameters necessary for the specific method (For instance, SGD requires the step-size, RES requires the step-size and the damping constant and SQN requires the step-size and the aggregation length) \n Run minSQN \n \n ```\nX = randn(5000,500);\ny = 2*(randn(5000,1) > 0.5) - 1;\nproblem = lossFunctions.LogReg(X,y); \n options = GenOptions();\noptions.method = \'SQN\';\nsqn_log_untuned = minSQN(problem,options,[5e-2,5]);\n``` \n The output of  minSQN  would be: \n ```\nsqn_log_untuned =  \n           fhist: [21x1 double]\nhyperparameters: [0.050000000000000 5]\n         w_star: [500x1 double]\n \n ``\nwhere fhist is the history of average loss function values over each epoch, hyperparameters returns the provided hyperparameters in the case when they are provided and returns their tuned values if the automatic tuning was chosen (see next example). w_star` is the value of the iterate at the end of the optimization.   \n Example (with tuner): \n The process for running the methods with automatic tuning is similar to above except no hyperparameters are passed as input (as in Step 3 above). \n```\nX = randn(5000,500);\ny = 2*(randn(5000,1) > 0.5) - 1;\nproblem = lossFunctions.LogReg(X,y); \n options = GenOptions();\noptions.method = \'SQN\';\nsqn_log_tuned = minSQN(problem,options);\n``` \n The output of  minSQN  in this example would be:\n```\nsqn_log_tuned =  \n           fhist: [21x1 double]\nhyperparameters: [0.006962319523931 26]\n         w_star: [500x1 double]\n \n `` fhist and w_star are the function values and final iterate as explained in the previous example. In this case, hyperparameters` is the value of the best hyperparameters as chosen by the automatic tuning mechanism.  \n Please refer to  demo.m  for a short demonstration of using  minSQN  for solving a small Logistic Regression problem using three different methods and plotting the results. For a detailed documentation of minSQN and its associated functions, use MATLAB\'s  help . For instance, to obtain details about the different options and their significance, use  help GenOptions . \n References: \n SGD:\n- Bottou, L., 1998. Online learning and stochastic approximations. On-line learning in neural networks, 17(9), p.142. \n SQN:\n- Byrd, R. H., Hansen, S. L., Nocedal, J., & Singer, Y. (2016). A stochastic quasi-Newton method for large-scale optimization. SIAM Journal on Optimization, 26(2), 1008-1031. \n oBFGS:\n- Schraudolph, N.N., Yu, J. and Günter, S., 2007. A stochastic\nquasi-Newton method for online convex optimization. In\nInternational Conference on Artificial Intelligence and Statistics\n(pp. 436-443). \n oLBFGS:\n- Schraudolph, N.N., Yu, J. and Günter, S., 2007. A stochastic\nquasi-Newton method for online convex optimization. In\nInternational Conference on Artificial Intelligence and Statistics\n(pp. 436-443).\n- Mokhtari, A., & Ribeiro, A. (2015). Global convergence of online limited memory bfgs. Journal of Machine Learning Research, 16, 3151-3181. \n RES:\n- Mokhtari, A. and Ribeiro, A., 2014. Res: Regularized stochastic\nbfgs algorithm. Signal Processing, IEEE Transactions on, 62(23),\npp.6089-6104. \n SDBFGS:\n- Wang, X., Ma, S., Goldfarb, D., & Liu, W. (2014). Stochastic Quasi-Newton Methods for Nonconvex Stochastic Optimization. arXiv preprint arXiv:1607.01231. \n adaQN:\n- Keskar, N. S., & Berahas, A. S. (2016). adaQN: An Adaptive Quasi-Newton Algorithm for Training RNNs. European Conference Machine Learning and Knowledge Discovery in Databases, (ECML PKDD 2016), Part I, Vol 9851, 1-16.', '', "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima \n by Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy and Peter Tang \n Paper link:  arXiv preprint \n Table of Contents \n \n Introduction \n Citation \n Disclaimer and Known Issues \n Usage \n \n Update (April 24) \n Our code was written in Keras 1.X, there have been a lot of API changes in Keras 2.X which have broken our code. We're working on updating our code to support Keras 2.X but in the meantime, provide a preliminary PyTorch implementation (refer to the PyTorch folder for details). As always, we welcome any questions, suggestions, requests or bug-reports.  \n Introduction \n This repository contains (Python) code needed to reproduce some of the figures in our  paper . The plots illustrate the relative  sharpness  of the minima obtained when trained using small-batch (SB) and large-batch (LB) methods. For ease of exposition, we use a Keras/Theano setup but owing to the simplicity of the code, translating the code into other frameworks should be easy. Please contact us if you have any questions, suggestions, requests or bug-reports. \n Citation \n If you use this code or our results in your research, please cite: \n @article{Keskar2016,\n    author = {Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy and Ping Tak Peter Tang},\n    title = {On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima},\n    journal = {arXiv preprint arXiv:1609.04836},\n    year = {2016}\n}\n \n Disclaimer and Known Issues \n \n In the included code, we use Theano/Keras to train the networks C1 - C4 using a batch size of 256 (for SB) and using 5000 (for LB). Depending on your hardware (especially if using GPUs), you may run into memory issues when training using larger batch sizes. If this happens, you can either train using a different setup (such as CPUs with large host-memory) or adapt our code to enable multi-GPU training.  \n The code for computing the  sharpness  of a minima (Metric 2.1) will be released soon. As is the case with the parametric plots, the code is quite straightforward. The code in Keras' pull-request  #3064  along with SciPy's  L-BFGS-B  optimizer can be used in conjunction to compute the values easily.  \n \n Usage \n To reproduce the parametric plots, you only need the two Python files ( plot_parametric_plot.py  and  network_zoo.py ). The latter contains the model configurations for the C1-C4 networks; the former trains the model imported from  network_zoo  using the SB and LB methods and plots the parametric plot connecting the two minimizers. The network is chosen using a command-line argument  -n  (or  --network ) and the generated plot is saved in PDF form. For instance, to plot for the C1 network, one can simply run: \n KERAS_BACKEND=theano python plot_parametric_plot.py -n C1\n \n with the necessary Theano flags depending on the setup. The figure in the  Figures/  folder should resemble: \n", "A Limited-Memory Quasi-Newton Algorithm for Bound-Constrained Nonsmooth Optimization \n by  Nitish Shirish Keskar  and  Andreas Waechter \n Paper link:  arXiv preprint \n Table of Contents \n \n Introduction \n Citation \n Disclaimer and Known Issues \n Usage \n \n Introduction \n The proposed method (NQN) is a limited-memory quasi-Newton method for bound-constrained nonsmooth optimization. It is an active-set method in that it operates iteratively in a two-phase approach of predicting the optimal active-set and computing steps in the identified subspace. The code is written in pure-Python and aims to mimic the calling syntax of  scipy.optimize.fmin_l_bfgs_b . In order to replace an existing call to  fmin_l_bfgs_b  with our code, simply import  NQN.py  and replace  scipy.optimize.fmin_l_bfgs_b  to  NQN.fmin_l_bfgs_b . In most cases, no changes to the calling arguments should be necessary. We include additional usage details at the end of this README. \n Please contact us if you have any questions, suggestions, requests or bug-reports. \n Citation \n If you use this code or our results in your research, please cite: \n @article{Keskar2016,\n    author = {Nitish Shirish Keskar and Andreas Waechter},\n    title = {A Limited-Memory Quasi-Newton Algorithm for Bound-Constrained Nonsmooth Optimization},\n    journal = {arXiv:1612.07350},\n    year = {2016}\n}\n \n Disclaimer and Known Issues \n \n Our code is written in pure-Python. Circumstantially, this can be slower than SciPy's L-BFGS-B implementation which is written in Fortran.  \n If you set the value of the gradient-sampling memory (M) to be greater than 1, you need the  CVXOPT  package for solving the QP. This can be easily installed via  pip . However, we do not recommend setting M to be greater than 1 unless you need extremely high-precision solutions. \n Please write to us if you need the Python code for the test problems, other solvers/wrappers or scripts for generating figures.  \n \n Usage \n In order to use our code, simply import the  NQN.py  file and call  NQN.fmin_l_bfgs_b  with the calling options similar to  scipy.optimize.fmin_l_bfgs_b . Concretely, the function signature is \n fmin_l_bfgs_b(funObj, x0, gradObj, bounds=None, m=20, M=1, pgtol=1e-5, iprint=-1, maxfun=15000, maxiter=15000, callback=None, factr=0.) \n where: \n funObj   => Function to minimize \nx0       => Initial/starting point\ngradObj  => Gradient of objective function. Unlike L-BFGS-B, for NQN, this *must* be provided \nbounds   => A tuple of bound constraints. Use (-numpy.inf, numpy.inf) if a a variable in unconstrained\nm        => L-BFGS memory\nM        => Gradient sampling memory\npgtol    => Scaled 2-norm tolerance for convergence\niprint   => Controls output; iprint>0 prints to STDOUT\nmaxfun   => Max. number of function evaluations\nmaxiter  => Max. number of iterations\ncallback => Any function, called before every iteration\nfactr    => The iteration stops when (f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= factr * 1E-16 \n The only additional option we employ over SciPy's L-BFGS-B implementation is the size of the gradient-sampling memory (M) which is initialized to 1. Further, the user can choose to employ a BFGS variant of our code (i.e., retaining BFGS matrices instead of limited-memory curvature pairs) by setting  m=numpy.inf . \n A sample usage of our code (on the popular Rosenbrock function) is:\n```\nimport scipy.optimize\nimport NQN\nimport numpy \n n = 100\nfunObj = scipy.optimize.rosen\ngradObj = func = scipy.optimize.rosen_der\nbounds = [(-0.5,0.5) for i in range(n)]\nx0 = numpy.zeros(n) \n scipy_output = scipy.optimize.fmin_l_bfgs_b(func,x0,fprime=grad,bounds=bounds)\nNQN_output = NQN.fmin_l_bfgs_b(func,x0,fprime=grad,bounds=bounds)\n``` \n Similar to the SciPy L-BFGS-B implementation, we return the final iterate, corresponding function value and an  information dictionary . We use different termination flags from SciPy's L-BFGS-B implementation. They are\n Termination Flag:\n0 => Converged\n1 => Reached Maximum Iterations\n2 => Reached Maximum Function Evaluations\n3 => Converged to Nonstationary Point\n4 => Abnormal Termination in Line Search\n5 => Iterate Has NaN values\n6 => Numerical Issues\n7 => FACTR Convergence", 'MyWebsite \n The files for my website']
jatrost,['Development of this project has moved to the Apache git repo and has mostly been picked up by Josh Elser: http://people.apache.org/~elserj/accumulo-pig/ \n This code will remain here because it is still being used in production by some. \n \n build the JAR (Note, you will need to download the accumulo src, build it, and install it into your maven repo before this will work) \n mvn package\n \n download the JARs needed by pig \n mvn dependency:copy-dependencies -DoutputDirectory=lib  -DincludeArtifactIds=zookeeper,libthrift,accumulo-core,cloudtrace\n \n print the register statements we will need in pig \n for JAR in lib/*.jar target/accumulo-pig-1.4.0.jar ; \ndo \n    echo register `pwd`/$JAR; \ndone\n \n Example output \n register /home/developer/workspace/accumulo-pig/lib/accumulo-core-1.4.0.jar\nregister /home/developer/workspace/accumulo-pig/lib/cloudtrace-1.4.0.jar\nregister /home/developer/workspace/accumulo-pig/lib/libthrift-0.6.1.jar\nregister /home/developer/workspace/accumulo-pig/lib/zookeeper-3.3.1.jar\nregister /home/developer/workspace/accumulo-pig/target/accumulo-pig-1.4.0.jar\n \n Run Pig, copy the register statements above and paste them into the pig terminal.  Then you can LOAD from and STORE into accumulo. \n $ pig\n2012-03-02 08:15:25,808 [main] INFO  org.apache.pig.Main - Logging error messages to: /home/developer/workspace/accumulo-pig/pig_1330694125807.log\n2012-03-02 08:15:25,937 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: hdfs://127.0.0.1/\n2012-03-02 08:15:26,032 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to map-reduce job tracker at: 127.0.0.1:9001\ngrunt> register /home/developer/workspace/accumulo-pig/lib/accumulo-core-1.4.0.jar\ngrunt> register /home/developer/workspace/accumulo-pig/lib/cloudtrace-1.4.0.jar\ngrunt> register /home/developer/workspace/accumulo-pig/lib/libthrift-0.6.1.jar\ngrunt> register /home/developer/workspace/accumulo-pig/lib/zookeeper-3.3.1.jar\ngrunt> register /home/developer/workspace/accumulo-pig/target/accumulo-pig-1.4.0.jar\ngrunt> \ngrunt> DATA = LOAD \'accumulo://webpage?instance=inst&user=root&password=secret&zookeepers=127.0.0.1:2181&columns=f:cnt\' \n>>    using org.apache.accumulo.pig.AccumuloStorage() AS (row, cf, cq, cv, ts, val);\ngrunt> \ngrunt> DATA2 = FOREACH DATA GENERATE row, cf, cq, cv, val;\ngrunt> \ngrunt> STORE DATA2 into \'accumulo://webpage_content?instance=inst&user=root&password=secret&zookeepers=127.0.0.1:2181\' using org.apache.accumulo.pig.AccumuloStorage();\n2012-03-02 08:18:44,090 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig features used in the script: UNKNOWN\n2012-03-02 08:18:44,093 [main] INFO  org.apache.pig.newplan.logical.rules.ColumnPruneVisitor - Columns pruned for DATA: $4\n2012-03-02 08:18:44,108 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler - File concatenation threshold: 100 optimistic? false\n2012-03-02 08:18:44,110 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: 1\n2012-03-02 08:18:44,110 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: 1\n2012-03-02 08:18:44,117 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig script settings are added to the job\n2012-03-02 08:18:44,118 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3\n2012-03-02 08:18:44,120 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - creating jar file Job7611629033341757288.jar\n2012-03-02 08:18:46,282 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - jar file Job7611629033341757288.jar created\n2012-03-02 08:18:46,286 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job\n2012-03-02 08:18:46,375 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map-reduce job(s) waiting for submission.\n2012-03-02 08:18:46,876 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete\n2012-03-02 08:18:46,878 [Thread-17] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths (combined) to process : 1\n2012-03-02 08:18:47,887 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_201203020643_0001\n2012-03-02 08:18:47,887 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - More information at: http://127.0.0.1:50030/jobdetails.jsp?jobid=job_201203020643_0001\n2012-03-02 08:18:54,434 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 50% complete\n2012-03-02 08:18:57,484 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete\n2012-03-02 08:18:57,485 [main] INFO  org.apache.pig.tools.pigstats.SimplePigStats - Script Statistics:\n\nHadoopVersion    PigVersion    UserId    StartedAt    FinishedAt    Features\n0.20.2    0.9.2    developer    2012-03-02 08:18:44    2012-03-02 08:18:57    UNKNOWN\n\nSuccess!\n\nJob Stats (time in seconds):\nJobId    Maps    Reduces    MaxMapTime    MinMapTIme    AvgMapTime    MaxReduceTime    MinReduceTime    AvgReduceTime    Alias    Feature    Outputs\njob_201203020643_0001    1    0    3    3    3    0    0    0    DATA,DATA2    MAP_ONLY    accumulo://webpage_content?instance=inst&user=root&password=secret&zookeepers=127.0.0.1:2181,\n\nInput(s):\nSuccessfully read 288 records from: "accumulo://webpage?instance=inst&user=root&password=secret&zookeepers=127.0.0.1:2181&columns=f:cnt"\n\nOutput(s):\nSuccessfully stored 288 records in: "accumulo://webpage_content?instance=inst&user=root&password=secret&zookeepers=127.0.0.1:2181"\n\nCounters:\nTotal records written : 288\nTotal bytes written : 0\nSpillable Memory Manager spill count : 0\nTotal bags proactively spilled: 0\nTotal records proactively spilled: 0\n\nJob DAG:\njob_201203020643_0001\n\n\n2012-03-02 08:18:57,492 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!\ngrunt>\n', 'I recently needed a quick way to analyze millions of small binary files (from 100K-19MB each) and\nI wanted a scalable way to repeatedly do this sort of analysis.  I chose Hadoop as the platform,\nand I built this little framework (really, a single MapReduce job) to do it.  This is very much a \nwork in progress, and feedback and pull requests are welcome. \n The main MapReduce job in this framework accepts a Sequence file of  <Text, BytesWritable>  where the \n Text  is a name and the  BytesWritable  is the contents of a file.  The framework unpacks the bytes of \nthe  BytesWritable  to the local filesystem of the mapper it is running on, allowing the mapper to run\narbitrary analysis tools that require local filesystem access.  The framework then captures stdout and stderr from the\nanalysis tool/script and stores it (how it stores it is pluggable, see  io.covert.binary.analysis.OutputParser ). \n Building: \n mvn package assembly:assembly\n \n Running: \n JAR=target/hadoop-binary-analysis-1.0-SNAPSHOT-job.jar\n\n# a local directory with files in it (directories are ignored for now)\nLOCAL_FILES=src/main/java/io/covert/binary/analysis/\nINPUT="dir-in-hdfs"\nOUTPUT="output-dir-in-hdfs"\n\n# convert a bunch of relatively small files into one sequence file (Text, BytesWritable)\nhadoop jar $JAR io.covert.binary.analysis.BuildSequenceFile $LOCAL_FILES $INPUT\n\n# Use the config properties in example.xml to basically run the wrapper.sh script on each file using Hadoop\n# as the platform for computation\nhadoop jar $JAR io.covert.binary.analysis.BinaryAnalysisJob -files wrapper.sh -conf example.xml $INPUT $OUTPUT\n \n From example.xml: \n   <property>\n    <name>binary.analysis.program</name>\n    <value>./wrapper.sh</value>\n  </property>\n\n  <property>\n    <name>binary.analysis.program.args</name>\n    <value>${file}</value>\n  </property>\n\n  <property>\n    <name>binary.analysis.program.args.delim</name>\n    <value>,</value>\n  </property>\n \n This block of example instructs the framework to run  wrapper.sh  using the args of  ${file}  (where  ${file} \nis replaced by the unpacked filename from the Sequence File.  If multiple command line args are required,\nthey can be specified by appending a delimiter and then each arg to the value of the  binary.analysis.program.args \nproperty \n FileFormatToConverterJob \n Useful for performing distributed file computation, mainly tailored for converting large binary files to a different format.  Example, converting a weird compressed file format to a normal one that can use standard Hadoop tools. \n hadoop fs -ls files | awk \'{print $8}\' > /tmp/all   \n# OR hadoop fs -lsr | grep -v \'^d\' | awk \'{print $8}\' > /tmp/all\n\nmkdir file-lists\ncd file-lists\nsplit -l 10 /tmp/all\ncd ..\nhadoop fs -put file-lists file-lists\n\nJAR=target/hadoop-binary-analysis-1.0-SNAPSHOT-job.jar\nhadoop jar $JAR io.covert.util.FileFormatToConverterJob -Dstream.process.command="/opt/decompress.sh" file-lists\n', 'hadoop-dns-mining \n This is a small framework for performing large amounts of DNS lookups using Hadoop. This is a work in progress, pull requests are welcome. \n Here are the steps for getting it working: \n Download, compile and install the Maxmind JAR into maven \n wget http://geolite.maxmind.com/download/geoip/api/java/GeoIPJava-1.2.5.zip\nunzip GeoIPJava-1.2.5.zip\ncd GeoIPJava-1.2.5/source/com/maxmind/geoip/\njavac *.java\ncd ../../../\nzip -r maxmind.jar com/\nmvn install:install-file -Dfile=maxmind.jar -DgroupId=com.maxmind -DartifactId=geo-ip -Dversion=1.2.5 -Dpackaging=jar\n \n Obtain the Maxmind IP Geo Database \n wget http://geolite.maxmind.com/download/geoip/database/GeoLiteCity.dat.gz\ngzip -d GeoLiteCity.dat.gz\n \n Obtain the Maxmind ASN Database \n wget http://www.maxmind.com/download/geoip/database/asnum/GeoIPASNum.dat.gz\ngzip -d GeoIPASNum.dat.gz\n \n Create/obtain large lists of domain names (e.g. domains.txt) and copy them into HDFS \n # you may want to split these domain files before placing in HDFS in order to use more mappers\nsplit -a 5 -d -l 100000  domains.txt domains_\nhadoop fs -put domains_* /data/domains/\n \n Download and build this project \n git clone https://jt6211@github.com/jt6211/hadoop-dns-mining.git\ncd hadoop-dns-mining\nmvn package assembly:assembly\n \n Run the various MapReduce jobs \n # These are the records that will be requested\nREC_TYPES=A,MX,NS,TXT\n\nJAR=target/hadoop-dns-mining-1.0-SNAPSHOT-job.jar\n\n# performs A record, MX record, and NS record lookups on each domain provided using 50 \n# resolving threads per Mapper using the nameserver of 8.8.8.8 and store the results in \n# HDFS in /data/dns-mining/01_raw\n# Note: choose the nameserver wisely, otherwise you may overload it.  In testing I mainly \n#  used a bind server deployed on each hadoop node so my nameserver was 127.0.0.1\ntime hadoop jar $JAR io.covert.dns.collection.CollectionJob \\\n    -D dns.collection.num.resolvers=50 \\\n    -D dns.collection.nameservers=8.8.8.8 \\\n    IN \\\n    "$REC_TYPES" \\\n    /data/domains/ \\\n    /data/dns-mining/01_raw\n\n# parse the raw responses into JSON (one record per RR in the DNS responses)\ntime hadoop jar $JAR io.covert.dns.parse.ParseJob \\\n    /data/dns-mining/01_raw \\\n    /data/dns-mining/02_parsed\n\n# lookup any IP addresses in the results in the maxmind DBs and enrich the records\ntime hadoop jar $JAR io.covert.dns.geo.GeoJob \\\n    -files /usr/local/lib/maxmind/GeoLiteCity.dat,/usr/local/lib/maxmind/GeoIPASNum.dat \\\n    GeoLiteCity.dat \\\n    GeoIPASNum.dat \\\n    /data/dns-mining/02_parsed  \\\n    /data/dns-mining/03_enriched\n\n# run a filter job for the rec types requested as well as for rec types that commonly occur in \n# the results as part of normal queries.  This will separate the various DNS records into their\n# own directories in HDFS\nfor X REC `echo "$REC_TYPES,SOA,NS,CNAME" | sed \'s/,/\\n/g\'| sort -u`; \ndo\n    time hadoop jar $JAR io.covert.dns.filtering.FilterJob \\\n        "type == \'$REC\'" \\\n        /data/dns-mining/03_enriched \\\n        /data/dns-mining/04_filtered-type=$REC; \ndone\n\n# This is a JEXL expression that filters out target fields that are IP addresses \n# and returns the target field lowercased\nTARGET_EXPR=\'if(target !~ "^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.$")return target.toLowerCase()\'\n\n# extract the \'target\' field from the MX records\ntime hadoop jar $JAR io.covert.dns.extract.ExtractorJob "$TARGET_EXPR" \\\n    /data/dns-mining/04_filtered-type=MX /data/dns-mining/05_extracted-mailservers\n\n# extract the \'target\' field from the NS records\ntime hadoop jar $JAR io.covert.dns.extract.ExtractorJob "$TARGET_EXPR" \\\n    /data/dns-mining/04_filtered-type=NS /data/dns-mining/05_extracted-nameservers\n\nHOST_EXPR=\'if(host !~ "^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.$")return host.toLowerCase()\'\n\n# extract the \'host\' field from the SOA records\ntime hadoop jar $JAR io.covert.dns.extract.ExtractorJob "$HOST_EXPR" \\\n    /data/dns-mining/04_filtered-type=SOA /data/dns-mining/05_extracted-nameservers-SOA\n', 'This is a modified version of peframe (https://code.google.com/p/peframe/) that produces JSON output \n Usage: \n python peframe.py FILE\n \n There are no command line options, it just dumps json as one line \n To install the project dependencies do this: \n pip install -r requirements.txt', 'Licensed under GNU/GPLv3', 'Modern Honey Network \n Multi-snort and honeypot sensor management, uses a network of VMs, small footprint SNORT installations, stealthy dionaeas, and a centralized server for management. \n For questions regarding installation please review the  MHN Troubleshooting Guide .  Search past questions on the  modern-honey-network Google Group .  Or send emails to  modern-honey-network@googlegroups.com . \n HONEYPOT \n Deployed sensors with intrusion detection software installed: Snort, Kippo, Conpot, and Dionaea.  \n MANAGEMENT SERVER \n Flask application that exposes an HTTP API that honeypots can use to:\n- Download a deploy script\n- Connect and register\n- Download snort rules\n- Send intrusion detection logs \n It also allows systems administrators to:\n- View a list of new attacks\n- Manage snort rules: enable, disable, download \n INSTALLING SERVER (tested Ubuntu 12.0.4.3 x86_64 and Centos 6.7) \n \n The MHN server is supported on Ubuntu 12, Ubuntu 14, and Centos 6.7.   \n Ubuntu 16 is not supported at this time.   \n Other flavors/versions of Linux may work, but are generally not tested or supported. \n \n Note: if you run into trouble during the install, please checkout the  troubleshooting guide  on the wiki.  If you only want to experiment with MHN on some virtual machines, please check out the  Getting up and Running with Vagrant  guide on the wiki. \n Install Git \n # on Debian or Ubuntu\n$ sudo apt-get install git -y\n\n# on Centos or RHEL\n$ sudo yum install -y git\n \n Install MHN \n $ cd /opt/\n$ sudo git clone https://github.com/threatstream/mhn.git\n$ cd mhn/\n \n Run the following script to complete the installation.  While this script runs, you will\nbe prompted for some configuration options.  See below for how this looks. \n $ sudo ./install.sh\n \n Configuration: \n ===========================================================\nMHN Configuration\n===========================================================\nDo you wish to run in Debug mode?: y/n n\nSuperuser email: YOUR_EMAIL@YOURSITE.COM\nSuperuser password: \nServer base url ["http://1.2.3.4"]: \nHoneymap url ["http://1.2.3.4:3000"]:\nMail server address ["localhost"]: \nMail server port [25]: \nUse TLS for email?: y/n n\nUse SSL for email?: y/n n\nMail server username [""]: \nMail server password [""]: \nMail default sender [""]: \nPath for log file ["mhn.log"]:\n \n Running \n If the installation scripts ran successfully, you should have a number of services running on your MHN server.  See below for checking these. \n user@precise64:/opt/mhn/scripts$ sudo /etc/init.d/nginx status\n * nginx is running\nuser@precise64:/opt/mhn/scripts$ sudo /etc/init.d/supervisor status\n is running\nuser@precise64:/opt/mhn/scripts$ sudo supervisorctl status\ngeoloc                           RUNNING    pid 31443, uptime 0:00:12\nhoneymap                         RUNNING    pid 30826, uptime 0:08:54\nhpfeeds-broker                   RUNNING    pid 10089, uptime 0:36:42\nmhn-celery-beat                  RUNNING    pid 29909, uptime 0:18:41\nmhn-celery-worker                RUNNING    pid 29910, uptime 0:18:41\nmhn-collector                    RUNNING    pid 7872,  uptime 0:18:41\nmhn-uwsgi                        RUNNING    pid 29911, uptime 0:18:41\nmnemosyne                        RUNNING    pid 28173, uptime 0:30:08\n \n Manual Password Reset \n If email based password resets are not working for you, here is another method. \n $ cd $MHN_HOME\n$ source env/bin/activate\n$ cd server\n$ python manual_password_reset.py \nEnter email address: YOUR_USER@YOUR_SITE.com\nEnter new password: \nEnter new password (again): \nuser found, updating password\n \n Deploying honeypots with MHN \n MHN was designed to make scalable deployment of honeypots easier.  Here are the steps for deploying a honeypot with MHN: \n \n Login to your MHN server web app. \n Click the "Deploy" link in the upper left hand corner. \n Select a type of honeypot from the drop down menu (e.g. "Ubuntu 12.04 Dionaea"). \n Copy the deployment command. \n Login to a honeypot server and run this command as root. \n That\'s it! \n \n Integration with Splunk and ArcSight \n hpfeeds-logger can be used to integrate MHN with Splunk and ArcSight.  Installation below. \n Splunk \n cd /opt/mhn/scripts/\nsudo ./install_hpfeeds-logger-splunk.sh\n \n This will log the events as key/value pairs to /var/log/mhn-splunk.log.  This log should be monitored by the SplunkUniveralForwarder. \n Arcsight \n cd /opt/mhn/scripts/\nsudo ./install_hpfeeds-logger-arcsight.sh\n \n This will log the events as CEF to /var/log/mhn-arcsight.log \n Data \n The MHN server reports anonymized attack data back to Anomali, Inc. (formerly known as ThreatStream).  If you are interested in this data please contact:  modern-honey-network@googlegroups.com .  This data reporting can be disabled by running the following command from the MHN server after completing the initial installation steps outlined above:  /opt/mhn/scripts/disable_collector.sh \n Support or Contact \n MHN is an open source project brought to you by the passionate folks at Anomali, Inc. Please check out our troubleshooting guide on the wiki. We will also lend a hand, if needed. Find us at:  modern-honey-network@googlegroups.com . \n Credit and Thanks \n MHN leverages and extends upon several awesome projects by the Honeynet project. Please show them your support by way of donation. \n LICENSE \n Modern Honeypot Network \n Copyright (C) 2014 - Anomali, Inc. \n This program free software; you can redistribute it and/or\nmodify it under the terms of the GNU Lesser General Public\nLicense as published by the Free Software Foundation; either\nversion 2.1 of the License, or (at your option) any later version. \n This program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\nLesser General Public License for more details. \n You should have received a copy of the GNU Lesser General Public\nLicense along with this program; if not, write to the Free Software\nFoundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA', 'files']
shayanb,['owght', 'mdd \n Model Driven Development Project - To Do List Manager', 'mdd \n Model Driven Development Project - To Do List Manager', 'Instagram-Realtime \n Instagram-PHP-MySQL-RealTime', 'Stide-ADS \n System Call based Anomaly Detection System\nPython - STIDE technique (sequence time-delay embedding) \n STIDE technique Readings: \n Forrest et al. employed a methodology motivated by immune systems. \nThis characterizes the problem as distinguishing ‘self’ from ‘non-self’ (normal and abnormal behaviors respectively). An event horizon is built from a sliding window applied to the sequence of system calls made by an application during normal use. The sequences formed by the sliding window are stored in a table that establishes the normal behavior model. During the deployment (detection) phase, if the pattern from the sliding window is not in the normal behavior database it is considered a mismatch.\nInput to the Stide detector takes the form of system call traces of an application for which the detector is trained. Specifically, Stide builds a “normal database” by segmenting the training data (of system call traces) into fixed length sequences . To do so, a sliding window of N is employed over the training dataset and the resulting system call patterns are stored in the “normal database”. During testing, the same sliding window size is employed on the data. Resulting patterns are compared against the “normal database” and if there is no match, a mismatch is recorded. Given a window size of N and system call trace length M, anomaly rate for the trace is calculated by dividing the number of mismatches by the number of sliding window patterns (i.e. M – N + 1).\n[4] \n 1- S. Forrest, S. Hofmeyr, A. SoMayaji, and T. Longstaff, “A sense of self for Unix processes,” in Security and Privacy, 1996. Proceedings., 1996 IEEE Symposium on, May. 1996, pp. 120–128. \n 2- S. Forrest, S. A. Hofmeyr, and A. SoMayaji, “Computer immunology,” Commun. ACM, vol. 40, no. 10, pp. 88–96, Oct. 1997.[Online]. Available: http://doi.acm.org/10.1145/262793.262811  \n 3- S. A. Hofmeyr, S. Forrest, and A. SoMayaji, “Intrusion detection using sequences of system calls.” Journal of Computer Security, vol. 6, no. 3, p. 151, 1998. [Online]. Available: http://search.ebscohost.com/login.aspx?\ndirect=true&db=tsh&AN=1531432&site=ehost- live \n 4- Kayacık, H. G., & Zincir-Heywood, A. N. (2008). Mimicry Attacks Demystified: What Can Attackers Do To Evade Detection? (A. N. Zincir-Heywood, Ed.), 1–11.\nDESCRIPTION OF FILES\n==================== \n -+ Presentation and Paper \n ------ Presentation.pdf \n ------ FInal Report.pdf \n --> Problem description and details regarding the dataset and methods. \n -+ Sample Data \n -----+ Anubis-good \n -----+ Malware \n -->Sample files from the dataset used in this project. \n for further information you can check these links:\n\n    http://anubis.iseclab.org/\n\n    http://anubis.iseclab.org/?action=publications\n \n -+ Sample outputs \n --> Sample output files from the python folder. \n -+ Python files \n ------ compare_STIDE.py \n ->STIDE technique implementation with Python \n ------ SysCallExtract.py \n -> Extract system call number sequences from the Anubis Dataset \n you can change the window size and the shift size by changing the W and K values \n LICENSE \n The MIT License (MIT) \n Copyright (c) 2013 Shayan Eskandari \n Permission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the "Software"), to deal in\nthe Software without restriction, including without limitation the rights to\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\nthe Software, and to permit persons to whom the Software is furnished to do so,\nsubject to the following conditions: \n The above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software. \n THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\nFOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\nCOPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\nIN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.', 'TAF \n Trace Analysis Framework (Version 0.2) \n \n \n Parser and tools to extract system call sequences from strace trace of an application. \n \n \n exports CSV of system calls: (Process ID, System Call Name/Number, Arguments, Return Values) \n \n \n windows the system call sequence to W sized windows (default=6) \n \n \n Use STIDE technique to check two databases (Normal, Malicious) to flag the anomaly system call sequence windows in the final file \n \n \n outputs flagged system call sequence windows and converts the anomalies to system call name sequences \n \n \n (Support for default switches with following the forks: strace -o trace.txt -C -f file) \n \n for more information see the readme in /docs/ \n TODO \n \n \n Add support for adding fields in the trace (time, relative time, ...) \n \n \n Automate the whole process (not sure if this is needed, cause personally I needed every output of each python file so that would be one run for all the outputs.) \n \n \n STIDE technique Readings: \n Forrest et al. employed a methodology motivated by immune systems. \nThis characterizes the problem as distinguishing ‘self’ from ‘non-self’ (normal and abnormal behaviors respectively). An event horizon is built from a sliding window applied to the sequence of system calls made by an application during normal use. The sequences formed by the sliding window are stored in a table that establishes the normal behavior model. During the deployment (detection) phase, if the pattern from the sliding window is not in the normal behavior database it is considered a mismatch.\nInput to the Stide detector takes the form of system call traces of an application for which the detector is trained. Specifically, Stide builds a “normal database” by segmenting the training data (of system call traces) into fixed length sequences . To do so, a sliding window of N is employed over the training dataset and the resulting system call patterns are stored in the “normal database”. During testing, the same sliding window size is employed on the data. Resulting patterns are compared against the “normal database” and if there is no match, a mismatch is recorded. Given a window size of N and system call trace length M, anomaly rate for the trace is calculated by dividing the number of mismatches by the number of sliding window patterns (i.e. M – N + 1).\n[4] \n 1- S. Forrest, S. Hofmeyr, A. SoMayaji, and T. Longstaff, “A sense of self for Unix processes,” in Security and Privacy, 1996. Proceedings., 1996 IEEE Symposium on, May. 1996, pp. 120–128. \n 2- S. Forrest, S. A. Hofmeyr, and A. SoMayaji, “Computer immunology,” Commun. ACM, vol. 40, no. 10, pp. 88–96, Oct. 1997.[Online]. Available: http://doi.acm.org/10.1145/262793.262811  \n 3- S. A. Hofmeyr, S. Forrest, and A. SoMayaji, “Intrusion detection using sequences of system calls.” Journal of Computer Security, vol. 6, no. 3, p. 151, 1998. [Online]. Available: http://search.ebscohost.com/login.aspx?\ndirect=true&db=tsh&AN=1531432&site=ehost- live \n 4- Kayacık, H. G., & Zincir-Heywood, A. N. (2008). Mimicry Attacks Demystified: What Can Attackers Do To Evade Detection? (A. N. Zincir-Heywood, Ed.), 1–11. \n The MIT License (MIT) \n Copyright (c) [2013] [Shayan Eskandari] [Shayan [ a t ] theshayan.com] \n Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the "Software"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions: \n The above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software. \n THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,#OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.', 'btc-papers \n Collaborate on the writings', "Open-Wallet-Format \n Bitcoin Wallet Convertor (for now) 1.0 \n OWF.py reads the keys from Bitcoin-QT wallet (e.g wallet.dat) and exports them to MultiBit wallet format (e.g multibit.key) but also applicable for Blockchain.info imports \n I used jackjack-jj fork of Pywallet.py (https://github.com/jackjack-jj/pywallet) to read wallet.dat and it is included in this repository.\nso you need to install the pywallet dependencies prior to the use of this. \n Why should I use this? \n To Convert your wallet.dat (or exported wallet from Bitcoin-QT) to:\n* MultiBit Wallet\n* Blockchain Importable format\n* Backup your private keys in a format that would not be as complicated as wallet.dat and less possible to be corrupted \n WARNING: the output is not encrypted so take good care of the file!  \n Usage: \n Usage: OWF.py [options] \n Options: \n --version             show program's version number and exit \n -h, --help            show this help message and exit \n --wallet=WALLET       wallet.dat or the exported wallet from Bitcoin-QT to\n                        be converted \n --newwallet=NWWALLET  New wallet name (multibit format) (in the\n                        same directory as the --wallet \n Example \n ./OWF.py --wallet ./wallet.dat \n ./OWF.py --wallet ./wallet.dat --newwallet ./newwallet.key \n TODO \n \n Less dependencies to pywallet.py \n Add the other way convertion (Using Import Private keys of Pywallet.py) \n Add --passphrase to read encrypted wallet.dat \n Nicer code! for now it works fine but it could be implemented in less hacky-code style \n Should be tested on windows too (tested on Mac and Linux so far) \n Define a standard for wallet format as .OWF \n \n Support \n Please support the work by either writing code or donate to 1owfJHTsWrrCpgaaYjC1vbJevuQzYRTYn \n It would be greatly appreciated. ", 'GoPro Black 3 Plus Firmware Analysis \n Analysis and findings on GoPro Black 3 Plus Firmware', 'SEC.gov-form-retriever \n Bulk download forms in .txt format from SEC.gov', 'mygithubpage', 'PHP Bitcoin Point of Sale \n Customized for  Cafe Aunja , Montreal \n Some features:\n* Easy to install (PHP and MySQL)\n* Real-time BTC/CAD(/USD) conversion\n* Uses Blockchain.info and Blockexplorer API (No need for Bitcoind)\n* Generates a new address for each transaction (Privacy Preserved)\n* RSA Encrypted private keys\n* Report page for the merchant, with showing the sales price and the realtime price of bitcoin amount\n* Admin page to decrypt and export the private keys associated with the addresses that holds a balance in MultiBit.key format \n \n Open source projects used:\n*  Bitcoin SCI  : Bitcoin Shopping Card Interface (0.5.4 (beta))\n    -    phpseclib \n    -    PHP Elliptic Curve library \n*  Sweet Alert  : A Beautiful replacement for javascript\'s "Alert"\n*  bitcoin-prices  : Display bitcoin prices in human-friendly manner in fiat currency using bitcoinaverage.com market data \n \n Installation \n \n Download this repo and upload it to your webserver \n Create the MySQL database with the same schema as the one in DB_SCHEMA.sql and assign the DB user/password \n Edit sci/config.php and sci/dbconnet.php \n Make sure to set admin/report/superadmin password and also Database credentials \n Security String can be ANY 16 characters or more. \n Leave PUBLIC RSA Key alone at this point. (We will come back to it) \n Save and upload config.php and dbconnect.php \n visit URL/sci/admin.php \n Login with your  admin  password from config.php \n Click RSA KeyGen. Save the private key offline in a safe place, put the public key in config.php \n (Optional) Add your logo to /sci/img/logo.jpg (or change the refrence in /css/main.css)\nDone! \n \n \n Some clarifications on the admins: \n sci/admin.php \nto be used on the first time to generate the RSA keys, and also in case you want to decrypt or check balance any specific address \n sci/report.php \nA simple report page that shows all the confirmed transactions in the database \n \n sci/superadmin.php \nAlmost same as report.php but has the option to rescan the whole database to check the balances or just checks the temporary table to see if there was any transactions that has not been added to the final table, also you can  extract all the bitcoin private keys assosiated with the addresses that has balance in them  with MultiBit.key style.\nYou can easily save the output in a .key file and import it in  MultiBit  or import it in blockchain.info. \n \n Most of the core functionality is from Bitcoin SCI by Jacob Bruce.\nSome notes by bitfreak group: \n \n The Bitcoin Shopping Cart Interface package is a set of libraries and tools that     enable you to process bitcoin tansactions with only PHP. You can have your own Instant Payment Notification system without the need for a middleman. If you\'ve been wondering how to handle customer payment since MyBitcoin went down, look no further, because this is the safest solution. \n An elliptic curve library written in PHP is used to achieve server side generation of FRESH bitcoin addresses for each customer. The script monitors the status of a payment by making use of the data supplied by blockexplorer.com. As such, there is no need to install a heavy duty service such as bitcoind on your server. The only limitation with this PHP package is that you can\'t make outgoing payments. \n The bitcoin private keys are now encrypted using RSA public-key cryptography technology. This means that the bitcoins keys are encrypted with a public RSA key, but they can only be decrypted with a private RSA key. So even if a hacker gains access to your bitcoin keys, they wont be able to decrypt that data unless they have your private RSA key. You can manage your keys by visiting the sci/admin.php script. \n The SCI package comes with a simple example to give you an idea about how to generate new keys and initiate a new payment through the Bitcoin Payment Gateway. This is NOT full shopping cart software, you would typically use this script to offer Bitcoins as one method of payment. The sci/config.php file needs to be modified to work properly on your website. You may also need to customize the following files: \n sci/process-order.php and \nsci/ipn-control.php \n Note:  PHP 5.3 or later (earlier versions of PHP should work but will not support alt-coins)\nNOTE: if you do not have 5.3 installed and wish to use BitcoinSCI, open up lib/bitcoin.lib.php and change line 38 and 42 from return static:: to return self::\nPHPExtension BCMath must be installed (most webhosts have it enabled by default) \n \n \n Screenshots \n \n \n \n \n This project was done to meet client\'s requirments, most of the funtionalities have the potencial to be a lot more complete or have another model for implementation (such as admin/report view) \n Contributions are more than welcome. \n 1ARH4G6BCKM8xoFucEtaKP3Vq5Ahr7dqcv \n Todo\'s \n \n Fast Confirmation , check blockchain.info API with Z0ro confirmation (who is going to do a successful double spend for a coffee?) \n One complete admin panel, preferebly with a seperate report page \n Nicer User Interface \n BIP32  for address generation \n \n License \n GNU General Public License v2 (GPL-2) \n You may copy, distribute and modify the software as long as you track changes/dates of in source files and keep all modifications under GPL. You can distribute your application using a GPL library commercially, but you must also disclose the source code.', "Plague  Social Platform Python API \n \n All Plague users are connected to each other right from the start.\nInfection starts at the source of the information…\nand spreads to the nearest users like a virus.\nInfected users spread the information further by infecting the users closest to them.\nPlague allows you to incubate information epidemics of any size.\nThe possibilities are endless. \n -plague.io \n \n Available Functions:\n* login(user,password)\n* vote_repost(post_id)\n* vote_skip(post_id)\n* send_text(text)\n* comment(post_id, text)\n* photo_post(file,text) #Not yet working\n* post_link(media_link, media_link_preview, text)\n* post_delete(post_id)\n* signup(name,email,password,lat,lon)\n* get_posts(uid,token)\n* get_infections_nearby(uid, token) \n to get UserId and Token run this: \n \n python ./plague_python_api.py -e 'name@mailinator.com' -p 'PAs$w0R1)' \n \n to post a text plague: \n \n python ./plague_python_api.py -u UserId -T Token -t 'Text goes here' \n \n to Spread (Vote up) a plague: \n \n python ./plague_python_api.py -u UserId -T Token -p Post_Id \n \n to comment on a plague: \n \n python ./plague_python_api.py -u UserId -T Token -p Post_Id -c 'Comment Text' \n \n for more options: \n \n python ./plague_python_api.py -h \n \n Also you can change the longitude and latitude to any of your choosing.\n token = 'TOKEN'\nu_id = 'USERID'\nlon = '90.0000'\nlat = '0.0000' \n \n Disclaimer \n This is for personal and research use only. No one likes spammers. \n Keywords: Plague, Plague Netwrok, social, platform, Python, API, plague.io, reverse engineering, RE", 'SaruTobi31 - Play SaruTobi 24/7 \n \n As the new update of  SaruTobi  is now more secure and prevents this type of attack, I\'m releasing the code. \n \n "SaruTobi is literally Japanese for \'Monkey Fly\', and this is pretty much the premise of the game. The user ... flings him across an 8-bit jungle collecting floating bitcoin along the way."\n- Christian Moss \n \n What is SariTubi31 \n Before version 1.31, you could simulate game play and get bitcoin rewards without playing the game, hence without generating revenue for the developer. \n How \n On the first run sarutobi31 will generate required bitcoin addresses and on the second run forward it will simulate the game play\n-  genwallet.key  holds the private keys and can be imported in MultiBit or Blockchain.info \n-  pubkeys.txt  holds the public keys and is for the use of the script \n result \n ...\nlaravel_session=COOKIE\n3\nDonation Address: 3MXxfNZoifLYdS8wJTpvfeDNPt9ZWuMAaN Distance: 5946 Passphrase: XXX\n{\n    "thestatus": "sent",\n    "amount": "0.0001"\n}\nlaravel_session=COOKIE2\n4\nDonation Address: 3MXxfNZoifLYdS8wJTpvfeDNPt9ZWuMAaN Distance: 5418 Passphrase: XXX\n{\n    "thestatus": "sent",\n    "amount": "0.0001"\n}\n... \n Am I rich? \n I donated back half of what I earned from this script to Sarutobi, and will keep the rest to tip people in reddit like I usually do. \n However the concern I have regarding this type of games is to overpollute the blockchain... Other than having too many dust transactions, when I was trying to send a transaction of 0.1 BTC from these addresses (Gathered by 1000 tips of 0.0001 BTC), MultiBit was not able to broadcast the transaction and had to rebuild it\'s blockchain database many times to show the right balance. \n Why 31? \n \n Disclaimer \n I\'m not responsible for anything, anywhere, anytime', 'For Support Related Rapidleech Visit - www.rapidleech.com \n Rapid Leech is a free server transfer script for use on various popular upload/download sites such as megaupload.com, Rapidshare.com and more than 45 others. The famous Rapidleech script transfers files from Rapidshare, Megaupload, Depositfiles.com, Easy-share.com, etc, via your fast servers connection speed and dumps the file on your server. You may then download these files from your server anytime later. \n Rapidleech script has being used by more than 5 million users worldwide and has being installed on more than 2000 servers.\nFor webmasters, if you have not tried the script before, download and install now and you will see how convenient the script can be. You may also generate income by offering your Rapidleech sites to end-users and earn income from advertising programs. Some webmasters are earning hundreds per day on the advertising program(Google and yahoo Ads) from their Rapidleech sites. Script installation is extremely easy and does not require any database. \n For end-users, you may search on our forum for readily available installed scripts on servers worldwide. You may use them but please support these sites by visiting their sponsors or donate in order to keep these sites available.', 'Testnet-blocksize-fork-data-Raw \n For some tests I use Blocktrail testnet webhooks, \nHere are all the blocks (in structured format) that my service rejected in last ~2 weeks for various reasons,\nsome are just timeout issues that took more time than the next block to reach my server and considered an older block,\nand the later ones are regarding the forks on testnet blockchain for blocksize proposals (BIP101, 8MB vote, ... ) \n Some info about the data : \n Starting block : \n   * height: 580259\n\n  * arrival_time: 2015-10-27T12:40:55+0000\n \n End Block: \n   * height: 601430\n\n  * arrival_time: 2015-11-11T07:13:27+0000\n \n I may not have the time to make some sense out of this data, but I know someone will \nAlthough there might be better ways to get this data (including data for the main branch), It might be challenging for some. \n Keep me posted if you did something cool with it, \n p.s some interesting things to look at https://twitter.com/lopp/status/664099176234532864 \n e.g. \n   * "height": 601529\n  * "block_time": "2015-11-11T08:20:02+0000"\n  * "arrival_time":"2015-11-11T06:33:13+0000"\n', 'two1 library - OTB \n 21212121212121212121212121212\n   2121212121212121212121212121212\n  21212121212121212121212121212121\n  21212121                  212121\n  2121212       121212      121212\n  212121      21212121      212121\n  21212121212121212121      212121\n  212121212      12121      212121\n  212121         21212     1212121\n  212121     212121212121212121212\n  212121     212121212121   212121\n  212121                    212121\n  212121                    212121\n  21212121212121212121212121212121\n  2121212121212121212121212121212 \n  21212121212121212121212121212 \nTwo1 Python3 library + other files ~~1.0.0~~ ~~2.0.1~~ ~~2.3.1~~ 2.3.6 (April 21 2016) \n Disclaimer  (Original Package metadata) \n Metadata-Version: 1.1\nName: two1\nVersion: 1.0.0\nSummary: Buy and sell anything on the internet with Bitcoin.\nHome-page: https://github.com/21dotco/two1\nAuthor: 21\nAuthor-email: 21@21.co\nLicense: MIT\nDescription: ``two1``: buy/sell anything on the internet with Bitcoin. \n Package: two1\nVersion: 2.0.1-1\nArchitecture: all\nMaintainer: 21 <21@21.co>\nInstalled-Size: 840\nDepends: python3, python3-arrow, python3-click, python3-pytest, python3-requests, python3-responses, python3-simplejson, python3:any (>= 3.3.2-2~), libpam-systemd, python3-pyaes, python3-base58, python3-jsonrpcclient, python3-jsonrpcserver, python3-path.py, python3-tabulate, python3-sha256, python3-mnemonic, python3-protobuf, python3-funcsigs\nSection: python\nPriority: optional\nDescription: Buy and sell anything on the internet with Bitcoin.\n ``two1``: buy/sell anything on the internet with Bitcoin. \n Package: two1\nVersion: 2.3.6-1\nArchitecture: all\nMaintainer: 21 <21@21.co>\nInstalled-Size: 1040\nDepends: python3, python3-arrow, python3-click, python3-pytest, python3-requests, python3-responses, python3-simplejson, python3-yaml, python3:any (>= 3.3.2-2~), libpam-systemd, rng-tools, minerd:armhf (= 0.3.4-1), python3-base58:armhf (= 0.2.2-1), python3-click:armhf (= 4.1-1), python3-funcsigs:armhf (= 0.4-1), python3-jsonrpcclient:armhf (= 2.0.1-1), python3-jsonrpcserver:armhf (= 3.1.1-1), python3-mnemonic:armhf (= 0.13-1), python3-path.py:armhf (= 8.1.2-1), python3-protobuf:armhf (= 3.0.0a3-1), python3-pyaes:armhf (= 1.3.0-1), python3-sha256:armhf (= 0.1-1), python3-tabulate:armhf (= 0.7.5-1), zerotier-one:armhf (= 1.1.4-1)\nSection: python\nPriority: optional\nDescription: Buy and sell anything on the internet with Bitcoin.', 'Russian Roulette \n Summary:  Randomly sends a file from the server file system, one of which is wifi config file (/etc/wpa_supplicant/wpa_supplicant.conf) which contains SSID and the PASSWORD \n Running the server \n $ python3 russianroulette-server.py\n \n Running the client (and pull the trigger) \n wget https://goo.gl/2cEQ1e -O russianroulette-client.py ; python3 russianroulette-client.py shoot\n \n API; \n 1. Get info \n HTTP URI: / \n Params: None \n Result: \n   |^\\                      _________________/\\_\n |~~~|--------------~~~~~~~~~~~~~~~~,xx.~~~~~~~~\\\n|___|-------++++==|___|~~~~~|_____(x@x),;\'//  ||\n                  |~~~||    |~~~~~~~~~~~ //   ||\n                   ~\\(_(=)~~ ,-~-\\       \\  __/\n                      ~~~~~\\[  \\ ]\\       \\/\n                            `:  |\'()       \\\n                              ~~~~\\ \\       \\\n                                   \\ \\       \\\n                                    \\ \\       \\\n                                     \\ \\       \\\n                                      \\ \\       ||\n                                       | \\       ||\n                                       |  \\_  ___||\n                                       \\____( )-=~\n\n{\n    "description": "I can lose my wifi ssid and password",\n    "name": "Russian Roulette",\n    "pricing": {\n        "/shoot": {\n            "minimum": 1313\n        }\n    },\n    "version": 101\n}\n \n Pricing: \n Free\n \n 2. Pull the Trigger \n HTTP URI: /shoot \n Params: None \n Result: \n [Content of the chosen file]\n \n Pricing: \n 1313\n \n TODO: \n \n add more random stuff \n reboot the machine if shot? \n reboot the client if shot? \n', 'Blockchain 411 \n Blockchain Analytics \n Lookup Bitcoin addresses based on their cluster and the clusters they\'ve had transactions with, and some other basic information \n Running the server \n $ python3 411-server.py\n \n Running the client \n 21 buy --maxprice 100 url http://10.151.47.208:21411/lookup?address=[BITCOIN_ADDRESS]\n \n or for a nice command line interface: \n  wget https://goo.gl/JMBFev -O 411_client.py ; python3 411_client.py\n \n API; \n 1. Get info \n HTTP URI: / \n Params: None \n Result: \n ````\n$$\\   $$\\         $$\\           $$\\\n$$ |  $$ |      $$$$ |        $$$$ |\n$$ |  $$ |      _$$ |        _$$ |\n$$$$$$$$ |        $$ |          $$ |\n_ _$$ |        $$ |          $$ |\n      $$ |        $$ |          $$ |\n      $$ |      $$$$$$\\       $$$$$$\\\n      _ |      _ |      _ __| \n Name: Blockchain 411     Version: 104\nDescription: If I know, I would tell on you...\nEndpoint: /lookup?address=[address]      Minimum Price: 100 \n Commandline usage: python3 ./411_client.py lookup [BITCOIN_ADDRESS] \n Do you want to lookup a Bitcoin address? (Y/n)\n```` \n Pricing: \n Free\n \n 2. Lookup and address \n HTTP URI: /lookup?address=[address] \n Params: bitcoin address \n Result: \n {\n   "key": "1AAXXXXXXXXXXXXXXXXXXXXXXXXNSfRs",\n   "balance": {\n      "confirmed": 0,\n      "sent": 11.78123559,\n      "received": 11.78123559\n   },\n   "identity": {\n      "cluster": {\n         "name": "NucleusMarket",\n         "type": "darkweb",\n         "neighbours": 75301,\n         "id": "XJZNdo"\n      },\n      "web": {\n         "count": 0,\n         "detail": []\n      }\n   },\n   "count": {\n      "transactions": {\n         "total": 12,\n         "sent": 6,\n         "received": 6\n      }\n   },\n   "relationships": {\n      "rel_clusters_in": [\n         {\n            "name": "AgoraMarket",\n            "type": "darkweb",\n            "id": "x5BRmk",\n            "level": 1\n         }\n      ],\n      "tagged_addresses": []\n   }\n}\n \n Pricing: \n 100\n \n TODO: \n \n add more APIs \n', 'Start Bootstrap  -  Creative \n Creative  is a one page creative theme for  Bootstrap  created by  Start Bootstrap . \n Getting Started \n To use this theme, choose one of the following options to get started:\n* Download the latest release on Start Bootstrap\n* Fork this repository on GitHub \n Bugs and Issues \n Have a bug or an issue with this theme?  Open a new issue  here on GitHub or leave a comment on the  template overview page at Start Bootstrap . \n Creator \n Start Bootstrap was created by and is maintained by  David Miller , Managing Partner at  Iron Summit Media Strategies . \n \n https://twitter.com/davidmillerskt \n https://github.com/davidtmiller \n \n Start Bootstrap is based on the  Bootstrap  framework created by  Mark Otto  and  Jacob Thorton . \n Copyright and License \n Copyright 2013-2015 Iron Summit Media Strategies, LLC. Code released under the  Apache 2.0  license.', 'Solidity-Logging-Library \n An expensive mid-level library to do loggings in smart contracts . \n example \n ```javascript\nimport \'logging.sol\'; \n contract MyContract is logging { \n function MyContract() {\n    logging.log("Initiated");\n  } \n function logTest(string test) public{\n    logging.log(test); //Gas Used By calling this function to write "test" = 65877\n  }\n} \n ``` \n view logs \n Logging would log in two places for now, one the variable called lastLog and the other event log \n javascript\nMyContract.lastLog.call()\n[1467913219, "Initiated"] \n ```\n(event log)\n00000000000000000000000000000000000000000000000000000000577e9299\n0000000000000000000000000000000000000000000000000000000000000040\n0000000000000000000000000000000000000000000000000000000000000009\n496e697469617465640000000000000000000000000000000000000000000000 \n timestamp -> 577e9299 -> GMT: Thu, 07 Jul 2016 17:34:17 GMT  // http://www.epochconverter.com/hex\nmessage -> 496e697469617465640000000000000000000000000000000000000000000000 -> "Initiated"\n``` \n TODO \n \n Log Stack \n string[] public logStack; ? \n mapping (uint => Log[]) public logStack; ? \n Optimize gas usage \n Support more datatypes (address, bytes, uint) \n maybe using a toBytes converter to write logs and a printLog() function to print logStack ?  \n', 'FirstBlood Crowdsale Page  \n /app  mainApp  \n /ethereum  tokenApp  \n Quick start \n \n git clone  clone this repo  \n npm i  and  sudo npm install \n lastly  npm start  and you are ready!  \n', 'keystamp-crypto \n need heroku project access\nInside project directory run \n locally:\n export NOTARIZE_PRV=MAIN_NOTARIZER_PRIVATE_KEY\nexport blockcypher_api_key=blockcypher_api_key\npython manage.py runserver 0.0.0.0:5000 \n Deply:\n```\ngit push heroku master\nheroku config:set NOTARIZE_PRV=MAIN_NOTARIZER_PRIVATE_KEY\nheroku config:set blockcypher_api_key=blockcypher_api_key \n ``` \n Test local server:  python test_client.py local \n Test heroku:  python test_client.py \n API \n BASE_URL = https://reghackto.herokuapp.com \n Endpoints: \n Hashing \n ``` \n Hash file with SHA256 \n /hashme HTTP POST {"file_url" : URL}\nresponse: {\'status\': \'success\', \'hash\': \'dbfdad915a13827c1684b39ff9875b24efaebd239f815f54e2263fbb217ad5d2\'}\n \n Hash string with SHA256 \n /hashme_string HTTP POST {"text" : "lorem test text"}\nresponse: {\'status\': \'success\', \'hash\': \'dbfdad915a13827c1684b39ff9875b24efaebd239f815f54e2263fbb217ad5d2\'}\n``` \n Key generation \n ``` \n Generate master seed for OSC (Note that this end point might take longer to response, as it generates entropy for address generation) \n /generate_master_seed HTTP GET/POST\nresponse {\'status\': \'success\', \'xpub\': \'xpub661MyMwAqRbcEkr1KVuZG4s7BXbGkoSjMGEGtPjFU976HPotfmmZMsssB9q2Gt9j6d4aNAVF2vgD3GB6fcufLxSWHz7TFkjgWmEsWMyE9PF\', \'xprv\': \'xprv9s21ZrQH143K2GmYDUNYtvvNdVknMLisz3Jg61Kduoa7QbUk8ETJp5ZPKrHPgNTgR2uCYgeXqVFgKCZpDsPjgXQM19A7j6vKaXncY58JLi2\'}\n \n generate firm key using master (OSC) seed and firm_id (5 digit int) \n /generate_firm_key HTTP POST {"osc_key": "OSC_XPRV" ,"firm_id": 12345}\nresponse  {\'status\': \'success\', "xpub": xpub, "xprv": xprv,\'path\' : \'path\'}\n \n generate advisor key using firm key and advisor_id (5 digit int) \n /generate_advisor_key HTTP POST {"firm_key": "FIRM_XPRV" ,"advisor_id": 12345}\nresponse  {\'status\': \'success\', "xpub": xpub, "xprv": xprv,\'path\' : \'path\'}\n``` \n Notarization \n ``` \n notarize and save the text to the Bitcoin blockchain. text is limited to 80 characters \n /notarizeme HTTP POST {\'text\':\'FINAL_HASH_TO_BE_SAVED_TO_BC\'}\nresponse {"status":"success","txid":\'THE_TRANSACTION_ID_OF_THE_TRANSACTION_CONTAINING_THE_HASH\'}\n``` \n Validation \n ``` \n retreive hash stored in txid \n /get_hash_from_bc HTTP POST {\'txid\': \'THE_TRANSACTION_ID_OF_THE_TRANSACTION_CONTAINING_THE_HASH\'}\nresponse {"status":"success","hash": \'THE_HASH_OF_THE_SAVED_DOCUMENT_IN_BLOCKCHAIN\'}\n``` \n ``` \n verify if a file (file_url) and a txid has the same hash. (Note: Verified is the final flag that should be checked) \n /validate_file_url HTTP POST {\'file_url\': "http://site.com/image.jpg", \'txid\': \'915a13827c1684b39ff9875b24efaebd239f815f54e2263fbb217ad5d\'}\nresponse {\'status\':success\',  \'verified\':True, \'txid_hash\':\'THE_HASH_THAT_WAS_SAVED_TO_BC\',\'file_url_hash\':\'HASH_OF_THE_FILE\' ,\'file_url\': "http://site.com/image.jpg", \'txid\': \'915a13827c1684b39ff9875b24efaebd239f815f54e2263fbb217ad5d\'}\n ``\nDid we just cryptographically proved THE_HASH_THAT_WAS_SAVED_TO_BC = HASH_OF_THE_FILE` ? return verified # :) \n make it do this for you \n ``` \n gets the final puzzle pieces and puts them together \n /notarize_this HTTP POST\n{\'file_url\':\'http://site.com/contract.pdf\', \'advisor_signature\':\'SIGNATURE_OF_DOCUMENTHASH_USER_RECEIPT_WITH_ADVISORS_PRIVATE_KEY\', \'client_authorization\':\'TWILLIO_CODE_OR_ANY_OTHER_AUTHORIZATION_RECEIPT\'}\nnote that you can send  file_hash  instead of  file_url \nresponse: {"status": "success", "keystamp":"HASH_OF_FINAL_KEY_THAT_IS_SAVED_ON_BC", "txid": tx_hash, "final_key": \'FILE_HASH:POST:TWILLIO_CODE_OR_ANY_OTHER_AUTHORIZATION_RECEIPT\'.encode(\'utf-8\') }\n```', 'snapp-python-api \n Snapp Python API', 'Toshiba-tv-RE \n Toshiba TV Reverse Engineering \n Firmware \n Series : L621U \n Model : 49L621U \n File name: UPDATE_CASTTV_UHD_01.03.40_INIT_CLEAR..zip \n File size: 136 MB \n Last Modified: 16.12.2016 \n Download and more info: http://tvna.compal-toshiba.com/us/en/download/?series=529&model=544', 'A first look at browser-based Cryptojacking \n This is the repository for the paper and presentation material presented at IEEE SECURITY & PRIVACY ON THE BLOCKCHAIN ( IEEE S&B ) \n \n Paper:  A first look at browser-based Cryptojacking ,   arxiv.org \n Presentation:  IEEE S&B Slides \n', 'CryptoReceipts', 'shayan.es \n my personal web site \n \n Forked From:\n -  Leonids  is a clean Jekyll theme perfect for powering your GitHub hosted blog.', 'smart-contract-sanctuary \n 🐦🌴🌴🌴🦕 A home for ethereum smart contracts. 🏠 \n This repo autosubmits contracts to  4byte.directory . Feel free to contribute sources. \n | Folder       | Description   |\n| ------------ | ------------- |\n| contracts    | folder structure of dumped solidity contract sources |\n| utils        | utilities for dumping smart contracts from public sources | \n Contracts \n The folder structure contains the solidity sources.  contracts.json  is more or less an index with some metadata of that day the contract was dumped. \n Utils \n Scripts for dumping smart contracts from public sources (etherscan.io, etherchain.com) \n requires:   pip install pyetherchain \n Update \n To use  List of Verified Contract addresses with an OpenSource license , you can download the csv file, add it to the util folder, and run  parse_download_contracts_etherscan_io.py  (with your etherscan API). This will add the new contracts to the appropriate folder \n Contribute \n Feel free to contribute smart contract sources, scripts for dumping sources or your analysis results with us. \n Want \n \n deduplication script (link instead of duplicate) \n statistics \n scripts to dump more sources \n code-hash (without comments; maybe compile and hash bytecode to dedup sources) \n', 'libsubmarine \n _\n                    | \\\n                     \'.|\n     _-   _-    _-  _-||    _-    _-  _-   _-    _-    _-\n       _-    _-   - __||___    _-       _-    _-    _-\n    _-   _-    _-  |   _   |       _-   _-    _-\n      _-    _-    /_) (_) (_\\        _-    _-       _-\n              _.-\'           `-._      ________       _-\n        _..--`                   `-..\'       .\'\n    _.-\'  o/o                     o/o`-..__.\'        ~  ~\n .-\'      o|o                     o|o      `.._.  // ~  ~\n `-._     o|o                     o|o        |||<|||~  ~\n     `-.__o\\o                     o|o       .\'-\'  \\\\ ~  ~\nLGB       `-.______________________\\_...-``\'.       ~  ~\n                                    `._______\'. \n \n A work-in-progress implementation of better submarine sends for Ethereum. \n Authors \n libsumarine\'s development was started by the  Submarines  group at the  2018 IC3 Ethereum bootcamp :\n- Lorenz Breidenbach\n- Tyler Kell\n- Alex Manuskin\n- Casey Detrio\n- Derek Chin\n- Shayan Eskandari\n- Stephane Gosselin\n- Yael Doweck \n Submarine Spec \n \n A  User \n B  Commit Address (No Private key) \n C  Libsubmarine \n D  Dapp, Application (e.g Auction) \n MPT  Verify Merkle Proof (This probably will be included in  C ) \n \n Steps: \n ```\n     TXcommit (1)\nA +-------------------> B\n+                       +\n|                       | TXunlock (3)\n|                       |\n|                       v\n+---------------------> C < - - - - - - - - - - - - -+ D\n    TXreveal (2)        ^       (pull) isFinalize\n                        |\n                        v\n                        MPT \n A +--------------> D\n      finalize(4)\n``` \n Order of Transactions \n \n Commit ( A  -->  B ) \n Reveal ( A  -->  C ) \n Unlock ( B  -->  C ) \n Finalize ( A  -->  D ) \n \n Generate  TXunlock \n A  chooses a (e.g. 256-bit) witness  w  uniformly at random and computes\n commit = Keccak256(addr(A) | addr(C) | $value | d | w | gasPrice | gasLimit) . \n commit  also used as  sessionId  in the other\n A  then generates a transaction  TXunlock  committing to data  d . \n javascript\nto: C\nvalue: $value\nnonce: 0\ndata: commit\ngasPrice: $gp\ngasLimit: gl\nr: Keccak256(commit | 0)\ns: Keccak256(commit | 1)\nv: 27 // This makes TXunlock replayable across chains ¯\\_(ツ)_/¯ \n Note that  TXunlock  is replay-able across chains because we use the pre-EIP155  v = 27 . We don\'t really care because  B  needs to be funded explicitly anyways, greatly reducing the potential for replay attacks. \n A  then computes  ECRECOVER(Txunlock)  which outputs either  B  or ⊥ (Invalid signature). If the output is ⊥,  A  picks a new  w  and repeats this step. Otherwise,  A  now knows  B . \n Done in  generate_commitment \n Commit \n A  generates  TXcommit , sending  $value + $unlockgas  to  B , where  $unlockgas = $gp * gl .\nLet  commitBlock  be the block in which  TXcommit  was mined. Let  commitIndex  be the index at which  TXcommit  was included in the block. \n Reveal \n A  sends  TXreveal  to  C , containing  $value ,  d ,  w ,  commitBlock , and  commitIndex .\n C  Checks for the  sessionId  and save the variables in  Sessions[sessionId] . validates the reveal and performs application specific logic.\nReveal transaction should include a deposit  revealDeposit  which will be refunded later if  A  is honest. This is to prevent DoS and cover the Cheat/Fraud Proof gas fees. \n C  also records the blockHash of  commitBlock  to process future fraud proofs. \n Unlock \n A  (or any other party) broadcasts  TXunlock . Upon receiving  TXunlock ,  C  will "finalize" the status of the session for  A  (assuming that  A  performed a valid reveal). \n C  can associate  TXunlock  with  A  thanks to  commit  data contained in it(a.k.a  sessionId ). \n Finalize \n A  calls the Dapp  D  to check for the finalization state. if:\n- Not finalized, returns  False \n- finalized, return  True ,  unlockAmount ,  d \n \n ```console\n[07-25 01:17:47] (master) ~/Documents/Github/submarines\n🍺  tree\n.\n├── README.md // That\'d be me\n├── contract\n│   ├── LibSubmarine.sol // Registry Contract\n│   ├── MerklePatriciaVerifier.sol // MP Verifier Contract\n│   ├── RLP.sol // RLPReader\n│   └── SafeMath.sol\n├── generate_commitment // Generate AddressB and UnlockTx\n│   ├── Go\n│   │   └── make_transaction.go //Go Implementation, NOT DONE\n│   ├── README.md // Docs\n│   ├──  init .py\n│   ├── generate_submarine_commit.py //Python Implementation, VERIFIED for 0.1.0\n│   └── requirements.txt\n├── generate_merkle //Generate Merkle Proof\n│   ├──  init .py\n│   ├── generateProof.js\n│   ├── generator.py\n│   └── test_generator.py\n├── requirements.txt\n└── test // Tests, duh!\n   ├── test_MerklePatriciaVerifier.py\n   ├── test_ReceiverContract.py\n   └── test_utils.py \n 5 directories, 18 files\n(=ↀωↀ=)\n``` \n \n LibSubmarine.sol \n LibSubmarine Registry. \n Constructor \n javascript\nconstructor(uint256 _revealDeposit, uint256 _challengePeriod) \n-  uint256 revealDeposit  : Minimum deposit require for Reveal(). This is to cover costs for challenge() //Cheat/Fraud Proof. \n \n uint256 challengePeriod  : Number of blocks to wait for possible challenge. Sessions would not be finalized in the challenge period. \n \n Reveal \n Reveal the commit transaction details\n javascript\nreveal(uint256 _commitBlock, uint256 _commitIndex, address _dappAddress, uint256 _unlockAmount,\\\n   bytes _data, bytes32 _witness, uint256 _gasPrice, uint256 _gasLimit) \n-  uint256 _commitBlock : The block number transaction () A  ->  B ) was confirmed in\n-  uint256 _commitIndex : The index of the transaction within the block (a.k.a  Position )\n-  address _dappAddress : The address of the DApp using the libsubmarine registry. The funds will be transferred to this address after finalization\n-  uint256 _unlockAmount : unlockAmount included in the unlock transaction  TXunlock  ( B  ->  C )\n-  bytes _data : DApp specific Data included in the  TXunlock \n-  bytes32 _witness : Random bytes (witness) included in  TXunlock \n-  uint256 _gasPrice : Gas Price specified for  TXunlock \n-  uint256 _gasLimit : gasLimit specified  TXunlock \n Unlock \n Receives the  TXunlock  ( B  ->  C ) and changes the state of the session accordingly \n javascript\nunlock(bytes32 _sessionId) \n-  bytes32 _sessionId : sessionId which is the commit message:  sessionId = Keccak256(addr(A) | addr(C) | $value | d | w | gasPrice | gasLimit) \n Finalize \n Finalizes the state of the session(sessionId) and releases the funds \n javascript\nfinalize(bytes32 _sessionId) \n-  bytes32 _sessionId : sessionId \n isFinalizable \n View function to show if the state is finalizable. \n \n isFinalizable(bytes32 _sessionId) \n \n Returns:\n```javascript\n(true, unlockAmount, DAppData) \n OR \n (false, 0, "")\n``` \n Challenge \n Anyone can challenge a reveal to prove  A  cheated and the commit transaction has not happened they way it was revealed. \n If proven right ( A  has cheated), unLockAmount will be transferred to the user reporting the fraud. \n javascript\nchallenge(bytes32 _sessionId, bytes _proofBlob, bytes _unsignedCommitTx) \n-  bytes32 _sessionId : sessionId\n-  bytes _proofBlob : //TODO\n-  bytes _unsignedCommitTx : // TODO \n \n Tests \n Install Solc ( Installation guide ) \n Install requirements \n pip3 install -r requirements.txt \n run the tests:\n python3 test/test_ReceiverContract.py \n \n TODO \n \n Explain fraud proofs \n Explain how to deal with reordering attacks (e.g. maliciously inserting  TXunlock  in front of  TXreveal ). Statemachine with {locked, unlocked} x {unknown, revealed}. \n Prove that you are correct (Proof of Correctness) and no need to wait for the challenge period to end. \n Better start/end blocks for states (reveal/challenge) \n Move test_generator.py to test folder \n clean up the Tests \n \n \n Disclaimer \n This project is a Work in Progress. \n IC3 Ethereum Bootcamp <3 2018', 'hethfinder \n Installation \n pip install -r requirements.txt', 'Pyfaast \n This is a python client for  Faa.st <http://faa.st> _ API. \n API Documentaion:  api.faa.st \n Please see the  example.py  for more information on how to use. \n Installation \n BASH\npython setup.py install \n Usage \n ```PYTHON\nfrom pyfaast import Faast \n faastObj = Faast() ### Faast(TESTNET = TRUE)  for testnet \n ``` \n Get Supported Currencies \n PYTHON\nfaastObj.get_supported_currencies() \n Response: \n ```JSON\n[\n...,\n{"cmcID": "ethereum",\n "decimals": 18,\n "deposit": True,\n "iconUrl": "https://testapi.faa.st/api/v1/public/static/img/coins/icon_ETH.png",\n "infoUrl": "https://ethereum.org",\n "name": "Ethereum",\n "receive": True,\n "symbol": "ETH",\n "walletUrl": "https://faa.st"},\n...\n] \n Number of supported tokens (pairs):     391 # Sep 13 2018 \n ``` \n Get pair price \n PYTHON\nfaastObj.get_pair_price(pair = "BTC_ETH") \n Response:\n```JSON\n{\n    "pair": "BTC_ETH",\n    "price": 0.03002374,\n    "deposit_amount": 0,\n    "minimum_deposit": 0.00017925\n} \n ``` \n Trading \n Create a Swap \n PYTHON\nswapObj = faastObj.create_a_swap(swap_pair = "BTC_ETH",\n                      withdrawal_address = "0x08d62881d04f62a02ee80f45abf454f418c60e99",\n                      refund_address = None,\n                      deposit_amount = 0,\n                      user_id = "",\n                      affiliate_margin = 5,\n                      affiliate_payment_address = "0x08d62881d04f62a02ee80f45abf454f418c60e99"\n                    )\nOnly swap_pair and withdrawal_address are mandatory \n Response: \n ```JSON\n{"affiliate_margin": 5,\n "affiliate_payment_address": "1fs4Vz12WGBgPe6LmE2TDnGeuAjFhws6k",\n "created_at": "2018-09-13T08:27:19.865Z",\n "deposit_address": "2N44At9pemAX3mXwXV86fdXsPRaoKYt5jqS",\n "deposit_currency": "BTC",\n "refund_address": "0x08d62881d04f62a02ee80f45abf454f418c60e99",\n "status": "awaiting deposit",\n "swap_id": "df53cda2-ac44-4ed9-9439-1b7586bb3879",\n "user_id": "",\n "withdrawal_address": "0x08d62881d04f62a02ee80f45abf454f418c60e99",\n "withdrawal_currency": "ETH"} \n ``` \n TODO: \n \n implement: \n QUERY SWAPS BY WITHDRAWAL_ADDRESS OR USER_ID \n FETCH/REFRESH A SWAP \n \n \n \n Testing \n TESTS NOT IMPLEMENTED YET \n install the needed test deps:: \n $ pip install vcrpy nose coverage\n \n Now we we can run the tests:: \n $ nosetests pyfaast --with-coverage --cover-package pyfaast\n', 'Simple Auction \n This is a template to run a simple auction on Ethereum Blockchain with web interface on github.io', "Crop Insurance on Blockchain \n A 2019  initc3  bootcamp project, built on Ethereum.  \n Slides : https://docs.google.com/presentation/d/1esDRIJ6rhfc185wXPz6Q08MYKb9XoIUcYrCvQlrSdNI/edit?usp=sharing \n Live Demo : https://shayanb.github.io/indemnity/index.html \n Overview \n The following documentation outlines an MVP for a peer to peer crop insurance ecosystem, powered by Ethereum. The objective of this MVP is to drive a holistically enhanced customer experience by enabling direct connectivity between demand-side (Insurance Buyers) and supply-side (Insurance Providers) participants and ensuring guaranteed, timely payouts of premiums and policies when applciable.   \n Additionally, this MVP serves to demonstrate the coexistence of a robust, seamless secondary trading marketplace for digital insurance contracts, and the benefits of facilitating that marketplace on the same rails utilized for the primary creation process.  \n Scope of MVP: \n \n \n Policy Creation: An Insurance Buyer submits an initial request for a new insurance policy that includes all desired terms; Insurance Providers view the requests and determine if they choose to accept the requests.  \n \n \n Claim Submission: Insurance Buyers submit off-chain claims to the chosen Oracle, requesting a determination that total crop failure has occurred prior to policy expiry. Oracles either reject the claim, or provide the Insurance Buyer with a signed message, indicating that total crop failure has occured.  \n \n \n Policy Expiry:  \n \n \n Trading:  \n \n \n This MVP does not yet address: plot and land registries, bidding, matching, partial crop failures, premium & payout reinvestment options, market-making, among other functionalities. See below for further discussion. Payments are currently made in ETH; however, a future-state version may require the use of a more stable currency, or at least the ability to specify alternatives.  \n Actors \n \n \n Insurance Buyers:  Farmers and landowners looking to purchase crop insurance to protect against the event of failed harvests \n \n Note: farmers could independently insure their crops through this market, or pool assets to unlock lower premiums  \n \n \n \n Insurance Providers:  Independent actuaries and smaller insurance companies who can provide coverage to Insurance Buyers in the event of crop failure and generate revenue from premiums in the absence of crop failure. Their decision to engage in the market is typically based on an assesment of risk factors, an analysis process that's typically unique to each provider.  \n \n \n Oracles:  trusted data sources that can verify the outcome of whether or not a crop has failed \n \n Note: Oracles are not required to directly participate in the network within this MVP \n Potential oracles:  \n Satellite data \n Trusted evaluation firms \n Sensors \n \n \n \n \n \n Insurance Traders:  Insurance Providers can be Insurance Traders \n \n Motivations for trading can be varied, most notably including: diversification of risk, definite profit opportunities   \n \n \n \n \n Workflow \n 1. Policy Creation \n Actors: Insurance Buyer, Insurance Trader \n Conditions:  need to fill in \n Process:   \n A Farmer will create a proposal for insuring a plot of their land, then submit to the Insurance Provider marketplace. The proposal contains the following terms:  \n \n Plot ID - one plot ID per proposal (Specified within plot registry: Total Size (km2) - total area covered by all plots included within contract) \n Start Date - when the policy would start \n End Date - when the policy would expire \n Premium (ETH / km2) - amount per km2 paid by the farmer for the insurance protection in the event that crop failure does not occur  \n Payout (ETH / km2) - amount per km2 of protection that the farmer seeks and would be guaranteed by an insurance provider in the event of crop failure  \n Approved Oracle(s) - approved data provider(s) who will be ultimately responsible for verifying any claims of failure up till and including expiry; corresponds to a whitelist of approved and trusted public keys  \n \n At submission, the farmer also transfers the total premium amount (ETH) to the proposal. \n Before the submitted proposal is confirmed, there is a check to ensure that the submitter (the Farmer) is the listed owner of that plot \n The premium sits within the proposal and away from the farmer, but remains unlocked (farmer is able to withdraw), and the terms of the proposal can be modified accordingly, until the proposal is accepted by an Insurance Provider.  \n An Insurance Provider will be able to view the proposal and choose to accept. At acceptance, the Insurance Provider also commits (sends) the total payout amount ($) to the proposal.  \n Once accepted, and the funds from the Insurance Provider are deposited, the proposal is complete and is a fully formed contract or policy.  \n 2. Claim Submission \n Actors: Insurance Buyer, Oracle \n Conditions:  \n Need to complete \n- Insurance contract is still valid\n- Current date must be equal to or less than date of expiry  \n Process: \n The Farmer will contact the Oracle, indicating they are raising an insurance claim, and request for a verification of the state of the Plot(s). This communication will take place off-chain.  \n The Oracle will address the following question: has total crop failure occurred for the total plot within this contract? This determination (i.e. analysis of data) will take place off-chain.  \n \n \n If NO (total crop failure has not occurred): the workflow is complete.  \n \n \n If YES (total crop failure has occurred): The Oracle will then sign a transaction indicating Yes (total crop failure has occurred) with their trusted private key. This transaction will be submitted to the network by the farmer. \n        - There will be an automated verification process, confirming that the signature provided by the Oracle does, indeed, match one of the pre-specified trusted public keys (a determination made originally within the Farmer’s proposal). This will occur on-chain. \n        - Total payout and total premium are released to the Farmer\n        - The Insurance Policy is no longer valid  \n \n \n 3. Expiry \n Actors: Insurance Provider \n Need to complete \n Conditions: \n- Current date is after expiry date\n- Policy ID exists \n- Requester is a listed insurance provider with stake in the policy; their stake still exists\n- No claims have been made on this policy\n- Message sender has not previously withdrawn funds from this contract \n After expiry date, if no claims have been made and both payout and premium exist within the contract, the Insurance Provider will be able to withdraw funds from the contract. The Insurance Provider will submit a transaction requesting this withdrawal, and if all conditions are met, both premium and payout is sent to the Insurance Provider, in accordance with their ownership stake in thep policy.   \n 4. Trading \n Actors: Insurance Provider, Insurance Trader \n Need to edit \nConditions: \nInsurance contract is still valid\nCurrent date must be equal to or less than date of expiry \n Workflow:\nThe Insurance Provider (Existing Holder) seeks to exit their total position or percentage of their position held within an Insurance Policy.  \n The Insurance Provider (Existing Holder) will create a request to sell, specifying: \n- Change in Payout (ETH) - This is effectively the price of the trade. Trade settlement completes once the Insurance Trader deposits this amount into the Insurance Policy, and the same amount is withdrawn and transferred to the Insurance Provider, decreasing their liability position within the policy in the event of crop failure. This number must be equal to or less than the total payout amount of the policy. \n \n Change in Premium (ETH) - This amount corresponds to the change in ownership stake that the Insurance Provider has in the total premium. After trade settlement, this amount would be owed to the Insurance Trader instead of the Insurance Provider in the event crop failure.  \n \n The Insurance Provider would then submit the request to the marketplace.  \n The Insurance Trader (another Insurance provider) who seeks to enter into or expand position within the same Insurance Policy would accept the terms of the trade. In doing so, the Insurance Trader must send the total amount corresponding to the price of the transfer (ETH) to the Insurance Contract. That same amount is then released from the contract and sent to the Insurance Provider. The ownership positions are updated accordingly. The Insurance Trader is now a listed Insurance Provider within the policy.  \n Future-State \n \n Plot Registries \n Bidding \n Insurance Provider proposals + Matching engine \n Partial crop failures \n Exchange and market-making tools \n Premium and payout reinvestment options \n More flexibility with Oracles (i.e. approval rules - k of N Oracle approvals, etc.)  \n \n Security Consideration \n \n Oracles  \n Double Indemnity \n Payout safety \n \n API Design \n Data Structures \n struct plot {\n    uint plotId;\n    address owner;\n} \n struct insuranceRequest {\n    uint plotId;\n    uint startDate;\n    uint endDate;\n    uint premium;\n    unit coverRequired;\n} \n struct policy {\n    uint policyId;\n    mapping(address => uint) collateralLiabilities;\n    mapping(address => uint) premiumDividends;\n    mapping(address => boolean) premiumDividendPayouts;\n    uint startDate;\n    uint endDate;\n} \n struct tranche {\n    address seller;\n    uint collateralLiabilityChange;\n    boolean sold;\n} \n public plot[] plots;\npublic insuranceRequest[] insuranceRequests;\npublic policy[] policies;\npublic tranche[] tranches; \n Functions \n \n submitInsuranceRequest \n \n plotId, startDate, endDate, premium, coverRequired \n \n require(plots[plotId] != 0) \n require(msg.sender == plots[plotId].owner) \n require(msg.value == premium) \n require(premium < coverRequired) \n require(startDate > now) \n require(endDate > startDate) \n require(endDate - startDate < 365 days) \n \n \n \n cancelInsuranceRequest \n \n \n insuranceRequestId \n \n require(insuranceRequests[insuranceRequestId] != 0) \n require(policies[insuranceRequestId] == 0) \n require(msg.sender == plots[plotId].owner) \n \n \n \n submitClaim \n \n \n policyId, claimAmount \n \n require(policies[policyId] != 0) \n require(msg.sender == policies[policyId].insuredParty) \n require(policies[policyId].endDate > now) \n require(verifiers[ecrecover(hash, v, r, s)] != 0) \n require(policies[policyId].claimAmount <= policies[policyId].totalCollateral) \n \n \n \n provideCover \n \n \n insuranceRequestId \n \n require(insuranceRequests[insuranceRequestId] != 0) \n require(msg.value == insuranceRequests[insuranceRequestId].coverRequired) \n require(msg.sender != insuranceRequests[insuranceRequestId].insuredParty) \n \n \n \n sellPolicyPremiumDividend \n \n \n collateralLiabilityChange, premiumDividendChange, policyId \n \n require(policies[policyId] != 0) \n require(policies[policyId].collateralLiabilities[msg.sender] != 0) \n require(policies[policyId].premiumDividends[msg.sender] != 0) \n require(policies[policyId].collateralLiabilities[msg.sender] > collateralLiabilityChange) \n require(policies[policyId].premiumDividends[msg.sender] > premiumDividendChange) \n \n \n \n cancelPolicyPremiumDividendSale \n \n \n trancheId \n \n require(msg.sender == tranches[trancheId].seller) \n require(tranches[trancheId].sold != true) \n \n \n \n buyPolicyPremiumDividend \n \n \n tracheId \n \n require(tranches[trancheId] != 0) \n require(tranches[trancheId].sold != true) \n require(msg.value == tranches[tranchId].collateralLiabilityChange) \n require(msg.sender != tranches[trancheId].seller) \n \n \n \n requestPolicyPremiumPayout \n \n \n policyId \n \n require(policies[policyId] != 0) \n require(policies[policyId].endDate < now) \n require(policies[policyId].collateralLiabilities[msg.sender] != 0) \n require(policies[policyId].premiumDividends[msg.sender] != 0) \n require(policies[policyId].premiumDividendPayouts[msg.sender] != true) \n \n \n \n Diagrams \n \n \n \n \n", '', 'This is the web site at  https://chia.network/ . Pull requests accepted! \n If you have ideas for the FAQ, check out  https://github.com/Chia-Network/website/tree/master/webroot/faq \n You can see this web site locally with python 2 as follows: \n bash\n    $ cd webroot\n    $ python -m SimpleHTTPServer \n or with python 3: \n bash\n    $ cd webroot\n    $ python3 -m http.server \n Then browse to http://127.0.0.1:8000/ \n Translations \n This site uses  l20n.js  v5.0 to translate static html content. Please take a moment to familiarize yourself\nwith its rules and syntax before adding content or translations. \n Guidelines for new content in existing files \n \n All text section elements must include the  data-l10n-id="text-id"  attribute: \n \n html\n    <span data-l10n-id="new-text-section">This is a new section of text</span> \n \n \n Before adding new translation entries, make sure one does not already exist by checking the  app.en-US.ftl \n \n \n Add new text entries and english translations to  app.en-US.ftl \n \n \n Add the same entries to the remaining  translation files \n \n \n Guidelines for new html files \n \n All html files must include the following in the  head  section: \n \n html\n    <meta name="defaultLanguage" content="en-US">\n    <meta name="availableLanguages" content="en-US, fr, ja, nl, de, es, sr, pt-BR, tr">\n    <link rel="localization" href="./locales/app.{locale}.ftl">\n    <script defer src="./js/l20n.min.js"></script> \n Guidelines for adding languages \n For html \n \n Add the two character locale code to the following meta element in all existing html files \n \n html\n    <meta name="availableLanguages" content="en-US, fr, ja, nl, de, es, sr, pt-BR, tr"> \n \n Add the translated name of the language to the dictionary at the top of  locales.js \n \n javascript\nvar names = {\n   "en-US": "English",\n   "fr": "Français",\n   "ja": "日本語",\n   "nl": "Nederlands",\n   "de": "Deutsch",\n   "es": "Español",\n   "sr": "Srpski",\n   "pt-BR": "Português",\n   "tr": "Türk"\n}; \n \n Copy  locales/app.en-US.ftl  and rename it with the locale code being added. \n Translate away! \n \n For the FAQ \n \n Translations of the FAQ are kept in markdown, and don\'t use l20n \n Copy  faq.md  to the  translated markdown files  folder, changing its name to  faq.<2 letter locale code>.md \n Translate away! \n', 'DlAGRAMS \n A collection of icons to be used in Draw.io (http://app.diagrams.net) containing: \n \n Popular Cryptocurrencies Icons \n Wallets & Applications \n Decentralized Finance (DeFi) \n \n Usage \n \n \n Use in the Browser:  draw.io \n \n \n Import in your Browser Instance: Go to  file  -->  Open Library From  -->  URL , and paste this address  https://dlagrams.net/DeFi.xml \n \n \n On native apps, Clone this repo, and then import {Library_name.xml} (e.g.  defi.xml ) by going to  file  -->  Open Library \n \n \n Still need help?  Work with custom shape libraries \n Screenshot \n', "███████╗████████╗██╗  ██╗ █████╗ ███████╗\n██╔════╝╚══██╔══╝██║  ██║██╔══██╗██╔════╝\n█████╗     ██║   ███████║╚██████║███████╗\n██╔══╝     ██║   ██╔══██║ ╚═══██║╚════██║\n███████╗   ██║   ██║  ██║ █████╔╝███████║\n╚══════╝   ╚═╝   ╚═╝  ╚═╝ ╚════╝ ╚══════╝   \n \n \n \n \n \n Instant retro UI for calling any contract function you want \n See it online @  eth95.dev  here's an example with DAI: \n https://eth95.dev/?network=1&address=0x6b175474e89094c44da98b954eedeac495271d0f \n Direct Link \n Features \n \n 🤙 Call any contract function as long as you have the ABI \n 🔌 Connect via localhost:8545, MetaMask, or a custom node URL \n ⚡ Watches your artifacts folder and automatically updates the UI \n 🔢 Encode your calls for a proxy to call on your behalf \n ⚙️ Set a custom signer or a custom contract address \n 📜 Built-in log for easy visibility \n \n Install \n Works on any dapp project (Truffle, Buidler, etc.) as long as you point it to the JSON artifacts. And even if you don't have a project, you can run it by itself and manually add ABIs and artifacts. \n \n \n Install \n shell\nnpm install -g eth95 \n \n \n Run with path to your artifacts folder: \n shell\neth95 ./build/contracts \n \n \n How it works \n When  eth95  is run, an Express server is fired up and a frontend (packaged by  Parcel ) is served at  localhost:3000  (you can define the port with a flag  -p 1234 ). The server will watch the directory you passed in for any changes to your artifacts and send those changes down to the frontend via Websockets. \n Note that you can also add any contracts you want if you have the ABI or Artifact (there is an Add Contract button). \n Contact \n If you have any questions or comments, please file an issue."]
SimonVilleneuve,['simonvilleneuve.github.io']
nicholas,['```\npip install tensorflowjs==1.7.4 \n tensorflowjs_converter \\\n  --input_format=tf_saved_model \\\n  --output_format=tfjs_graph_model \\\n  --weight_shard_size_bytes=10485760 \\\n  --skip_op_check \\\n  assets/exported_container \\\n  assets/converted_tfjs\n```']
devottam,['Redditext \n Extract text from URLs for reading purposes. Right now, it only gets the text from  Reddit . Future plan is to extend it to be more friendlier to any URLs given. \n Plans \n \n [ ] Extend it such that it can take any URL \n [ ] If the article is good and want to send to kindle, have the interface for that. \n [ ] ... \n']
skytreader,['This is just a test GitHub project for me. Trying to learn both CodeIgniter\nand git. \n I intend to use this personally and, as such, my branch of development will\nnot focus on things which an actual library might need such as keeping tab\non library members and borrowed books. For the most part, it is just an\ninterface to a book database I designed.', "PyGame Objects \n \n Want a sample on how to work with this framework? Check out the docs in the\n wiki , particularly the\n Walkthrough \n \n The most awesome framework for PyGame you will ever encounter!\n \n \n But  it's not there yet. Right now, it's just a simple abstraction for some\ncommmon code patterns I find while using PyGame. \n Built on:  see  .travis.yml  as well as  requirements.txt . Needs Python 3.7+\nas this relies on language features only available from there. \n Development \n Aside from  .travis.yml , the Dockerfile is provided for development. The\nDockerfile takes an argument  userid  which should be a user id outside the\ncontainer that has access to  /tmp/.X11-unix . If you are in a graphical desktop\nenvironment, it would suffice to pass the  $UID  environment variable like so, \n docker build -t pygame-objects --build-arg userid=$UID .\n \n You can also pull the image via \n docker run skytreader/pygame-objects:latest\n \n At this point, you only have the image. To develop and run games with it, you\nshould use the provided  duckrunner  script. It takes the package path to the\nscript that invokes your game loop. For example, to run the included snake game,\ncall  duckrunner demo.snake.game . \n Current Status \n Right now, I'm working on adding extra game-making functionalities; stuff that\nwill maybe come useful if you do an RPG, platformer, arcade, etc., type of game.\nI'm also working on adding new native drawing functionalities. \n File Organization \n components  houses the main framework.  sample_sprites  contains the sprites I\nused for the test and demo files.  tests  contains, well, tests. \n Since I tried to follow PEP 328, it may not be that straightforward to run the\ntests. For convenience, navigate to the  runscripts  directory, pick the test\n(as directories) you would like to run, and execute the runscripts from there. \n The tests were written after every feature I finished. The tests are a mix of demos\nand unit tests. Every now and then, I also write some mini-games to compile\ndemonstrations of some feature I'm working on. \n Git Organization \n Documentation is at the wiki. My todo list I made as issues. I am currently\nworking to make the documentation comprehensive (no more peeking at the framework\ncode!). However, something I was not able to forsee is that, the documentation\nfor Milestone 1 and for the code at the repo head has been mixed at the wiki.\nFortunately, they are not yet that different from each other and when and where\nthey differ, it is easy to note. In the near future, I plan to include markdown\nfiles of the docs along with the code. \n License \n All code in master (except when noted) are licensed under MIT. \n All sprites under  sample_sprites/tiles  are licensed under a\n Creative Commons Attribution-ShareAlike 4.0 \nlicense.", 'About \n A little app for automatically generating exchange gift pairings. The gift-giving graph produced will be acyclic (i.e., no two persons ever will be assigned to each other). Not yet very abstracted; it expects that an "admin" will enter the names of those invited into the database. \n Future \n I intend this to be a Facebook app (create an event, run the app, get your pairings). But while I am not yet very proficient with the Graph API, we have this. ;D', "The Large Hadron Colliders \n presents \n xkcd hats dragons \n My (unfinished) work at the first ever Python Philippines' Hackathon for Creative Pythonistas. \n Dodge the dragons. Get to the Large Hadron Colliders. Summon the God Particle and survive. \n (No event handling yet though. Awww.)"]
squiddy,['Gerrard - Styleguide generator \n Generates a styleguide from your stylesheets. Documentation format is based on\n Knyle Style Sheets , but adds a section to\nspecify a HTML template to use when generating the guide.', 'louie - gamepad browser control \n Think Steam "Big Picture"-mode control in your browser. \n Why? I liked the idea behind the controls. Doesn\'t matter if it ends up being\nuseful or not. Take this as a proof-of-concept, it won\'t be able to hold up\nagainst Steams optimized browser. \n Features (WIP) \n \n Chrome/Chromium - Support \n Zoom \n Moving around page \n Highlighting and clicking links \n basic Gamepad API support \n \n TODO \n Client \n \n implement daisywheel \n vimium link navigation as alternative to clicking links \n kinetic scrolling \n request fullscreen \n keep zoom-level consistent across pages \n \n Browser-Extension \n \n enable/disable globally \n', 'Vice City MP3 Control \n Mod for GTA Vice City to take control of the mp3 radio station.  This project is unmaintained \n Really crappy video without sound \n', "git-rs \n \n A git reader implemented in Rust. The primary goal here is to learn Rust, so\nthis won't even be near feature complete. \n Development \n Running tests \n cargo test \n Run clippy checks \n rustup run nightly cargo clippy \n License \n git-rs is released under the  MIT License .", 'vscode-xcolor \n \n \n Pick any color on the screen and have the hexadecimal representation insert\ninto the currently active editor. This is a thin wrapper around the\n xcolor  program. \n Features \n \n Usage \n Once you have an editor open, you can trigger the  Pick screen color \ncommand. You should see a small preview window attached to your cursor, once\nyou selected a color, the hexcode will be inserted at the current position of\nthe cursor. \n Requirements \n This extension requires you to have  xcolor  installed globally. See\nhttps://github.com/Soft/xcolor for installation instructions. \n Note that  xcolor  only supports linux systems running X11.', "gitlab-ci-status: a live overview of CI pipelines \n \n Note: This is a pet project of mine, so I might try different things from time to time. \n \n Motivation \n When working together with multiple colleagues, sharing a single gitlab\nrunner (tool that runs your pipeline jobs) quickly led to some problems: \n \n my build is stuck, what is the runner doing? \n is my build done? \n \n Especially stuck builds were annoying because we have to navigate through all\nCI enabled projects on gitlab and check the pipeline status. \n Some problems go away after you add concurrency / more runners, nevertheless\nhaving a overview what's happening right now is useful sometimes. \n How does it work? \n For each project you want to see in the overview, a new webhook has to be set\nup that points to the instance of gitlab-ci-status. Updates are send for\npipeline and job events. A server persists the state on disk and the client\nperiodically fetches the state and renders it using React. \n Installation \n TODO \n Ideas \n \n utilize SSE (server-side events) to only send updates to clients, not the\n  whole state \n", "advent-of-code-2018 \n My solutions to puzzles in this year's Advent Of Code: https://adventofcode.com/2018 \n This year I'm probably going with at least Go, Nim and Rust. Let's see how that works out. :)", 'runmap \n Create a map of runs from GPX files. Used on my homepage. \n \n Usage \n go build -o runmap\n./runmap --imageSize 1000 path/to/directory/with/gpx/files/ \n Output will be written as  output.png  in the current working directory.', 'polar-flow-exporter \n Downloads GPX data of (optionally filtered) trainings from Polar Flow.', 'grafana-image-panel \n Shows and refreshes an image. That\'s it. \n We have a slackbot running at work that accepts image URLs and we show the\ncurrent image on one of our grafana dashboards. \n Right now it\'s about 5 lines of plain JavaScript to embed the image and\nrefresh it from time to time. This repository is an attempt to complicate\nthings... or rather learn a thing or two about grafana plugins. :-) \n Developed against Grafana 6.0.x \n Screenshots \n \n \n Setup \n Install dependencies: \n $ yarn \n Make a production build: \n $ yarn build \n Continously build the plugin: \n $ yarn watch \n Development \n I\'m using docker to run grafana and point it\'s plugin directory to this\nrepository: \n $ docker run -p 3000:3000 -v $(pwd):/plugins/ -e GF_PATHS_PLUGINS="/plugins/" grafana/grafana:6.0.2 \n Open  localhost:3000  in your browser and add the image panel.', "Homer  \n Homer is a  statsd -compatible stats\naggregator written in Rust. I'm basing this implementation on github's statsd\nimplementation  brubeck . \n Why \n To write something in Rust. I've been reading statsd related code lately,\nespecially a C-implementation, and I'd like to see whether I can make it\nshorter and easier to understand for my usecase. \n What's working \n \n [x] configuration loading \n [ ] statsd \n [x] receive packets via UDP \n [x] basic packet parsing (no support for sampling rates) \n [ ] performance (recvmmsg, multi thread) \n \n \n [ ] aggregation \n [x] counter \n [ ] gauge \n [ ] timer \n [ ] sets (maybe later) \n \n \n [ ] carbon \n [x] plain text protocol \n [x] periodic flushing \n [ ] pickle protocol (maybe later) \n \n \n [ ] logging \n [ ] proper error handling \n \n Setup \n Get the nightly rust compiler in version 1.36.0 and run  cargo run .\nConfiguration is possible by editing the  config.toml  in the working\ndirectory.", 'wcolor \n \n WORK IN PROGRESS! \n Simple color picker for Wayland. Pick any color on the screen using the mouse. This is basically the Wayland aquivalent to  xcolor . \n Usage \n Run  wcolor  and select a color. Its hexadecimal RGB representation will be\nprinted to standard output. \n Screenshot \n \n Setup \n \n zig build \n zig build run \n \n References \n \n https://github.com/ifreund/zig-wayland \n https://github.com/Soft/xcolor (the X11 version) \n https://github.com/emersion/grim \n https://github.com/emersion/slurp (consulting the source of grim and slurp helped a lot in understanding Wayland) \n', 'Annotations for grafana from gitlab deployments \n \n \n Provides a grafana datasource to get annotations for each deployment in\ngitlab. Useful to see whether code you deployed had any impact. \n \n Usage \n shell\n$ go build\n$ export GITLAB_URL=https://gitlab.example.com\n$ export GITLAB_TOKEN=apitoken \n$ export HTTP_ADDRESS=:8080\n$ ./gitlab-deployment-annotations \n Log into grafana ( http://localhost:3000 ) and add a SimpleJSON datasource\nand point it to your datasource, e.g.  http://localhost:8080 \n Add annotations to a dashboard using your datasource and configure the query\n(which accepts JSON), which allows you to select the project you want to\ndisplay, for example: \n json\n{"project_id": 16, "environment": "Live"} \n Development \n \n Build a grafana container image with the simple-json-datasource plugin\npreinstalled using this Dockerfile: \n \n Dockerfile\nFROM grafana/grafana\nRUN grafana-cli --pluginsDir "$GF_PATHS_PLUGINS" plugins install grafana-simple-json-datasource \n $ docker build -t gf . \n \n Run a local instance of grafana \n \n $ docker run --net host gf \n \n Run and configure the datasource (see usage) \n', 'Personal CLI for GitLab \n \n Provides some shortcuts and workflows when working with GitLab at work.\nTotally not inspired by https://github.com/cli/cli.', 'libcst_easy_matcher \n An experiment to create matchers from source code when working with\n LibCST . A matcher enables you to check\nwhether a certain node in the tree generated by LibCST matches a certain\npattern. \n Status \n python\na = 3\na = __\n__ = 3\nb = bar(x=3)\nb = bar(x=__)\nb = __(x=3) \n See  tests/test_libcst_easy_matcher.py  for all examples. \n Motivation \n If I want to check that a statement matches  \n python\nclient["cookies"] = get_cookie(request) \n I\'d write something like this: \n python\nm.Assign(\n    targets=[\n        m.AssignTarget(\n            target=m.Subscript(\n                value=m.Attribute(\n                    value=m.Name("client"),\n                    attr=m.Name("cookies"),\n                )\n            )\n        )\n    ],\n    value=m.Call(\n        func=m.Name(\n            value=\'get_cookie\'\n        ),\n        args=[\n            m.Arg(\n                value=m.Name(\n                    value=\'request\'\n                )\n            )\n        ]\n    )\n) \n I\'m experimenting here to instead do it like this:  \n python\ncreate_matcher("client[\'cookies\'] = get_cookie(request)") \n Placeholders \n Just matching against concrete values only get\'s you so far, so I\'m\nevaluating the use of placeholders like this: \n python\n__ = get_cookie(request)\nclient[__] = get_cookie(request)\nclient[\'cookies\'] = __(request)\nclient[\'cookies\'] = get_cookie(__) \n All of which should match the example code.  __  might not be the best choice\nin the long run, but I needed a valid identifier to be able to still parse\nthe code with LibCST. \n License \n MIT', "swaylock-configurator \n Visualize changes to  swaylock's  configuration. \n \n Does not support all configuration options (mostly text related settings are missing). \n Live Demo \n Screenshot \n", 'pytest-overwatch \n \n pytest-overwatch  is a Jest inspired interactive test runner plugin for\npytest. It reruns tests whenever files change and allows you to select a\nsubset of tests to run. \n Work in progress! \n Features \n \n [x] re-run tests on file change \n [x] select subset of tests based on filename \n [ ] select subset of tests based on test name \n [x] support dropping into debugger on test failure \n \n In action \n \n Motivation \n pytest  is my go-to test runner for python projects and I use it heavily at\nwork. I usually use the  --looponfailure  feature of the\n pytest-xdist  plugin on the side, however having worked quite some time\nwith Jest in the javascript world, I was missing two things: \n \n ability to rerun all the selected tests constantly - instead of just the\n  failed ones - to discover potential new failures \n running a subset of all tests easily \n \n Related projects \n \n https://github.com/pytest-dev/pytest-xdist \n https://github.com/joeyespo/pytest-watch \n https://github.com/facebook/jest \n', "oog \n Simple android app showing the front camera in full screen. I'm using this in\ncombination with  scrcpy  as a webcam. \n Don't read this is you ever wrote an android app, I'm probably violating\n all  best practices here, but I kept it simple to understand it.\nSimilar example apps introduced things like fragments and navigation right away,\nand I don't understand those yet. \n This is also really hardcoded to work with my smartphone.", 'gitlab-to-sqlite \n \n Save data from GitLab to a SQLite database. \n Attribution \n The overall structure and CLI is taken from\nhttps://github.com/dogsheep/github-to-sqlite/. \n \n How to install \n Authentication \n Using custom gitlab instance \n Fetching projects \n Fetching pipelines \n \n How to install \n $ pip install gitlab-to-sqlite\n \n Authentication \n Create a GitLab personal access token: https://gitlab.com/-/profile/personal_access_tokens \n Run this command and paste in your new token: \n $ gitlab-to-sqlite auth\n \n This will create a file called auth.json in your current directory containing\nthe required value. To save the file at a different path or filename, use the\n--auth=myauth.json option. \n As an alternative to using an auth.json file you can add your access token to an\nenvironment variable called GITLAB_TOKEN. \n Using custom gitlab instance \n When running  auth  you may specify an optional  --host  parameter pointing\nto a custom instance. \n $ gitlab-to-sqlite auth --host gitlab.internal\n \n Fetching projects \n The  projects  command retrieves a single project. \n $ gitlab-to-sqlite projects gitlab.db group/project-name\n \n Fetching pipelines \n The  pipelines  command retrieves updated or created pipelines with their\ncorresponding jobs. \n $ gitlab-to-sqlite pipelines gitlab.db group/project-name\n \n This command can be run regularly. Based on the most recent created or updated\npipeline it only fetches changes that happened afterwards.', 'swc-plugin-glob-import']
tboquet,['configs \n dotfiles configurations', "okeydockey \n Chains of Dockerfiles for ML and stats research. \n Requirements and installation \n \n nvidia-docker \n one or more NVIDIA gpu(s) with cuda compute capabilities > 3.0 \n CUDA driver installed on the Host OS \n \n For more information about the nvidia-docker tool, please take a look at the  requirement  and the  installation  steps in the  nvidia-docker wiki . \n Specific example: \n If you want to use your GPU 0 and GPU 1 (as listed by nvidia-smi), be able to serve an ipython notebook via the port 8888 and mount a volume where some notebooks are located you could use:\n NV_GPU='0,1' nvidia-docker run -it -p 8888:8888 -v ~/notebooks:/notebooks tboquet/nameoftherepo", 'presentations \n My presentation repo', 'keras-contrib : Keras community contributions \n \n This library is the official extension repository for the python deep learning library  Keras . It contains additional layers, activations, loss functions, optimizers, etc. which are not yet available within Keras itself. All of these additional modules can be used in conjunction with core Keras models and modules. \n As the community contributions in Keras-Contrib are tested, used, validated, and their utility proven, they may be integrated into the Keras core repository. In the interest of keeping Keras succinct, clean, and powerfully simple, only the most useful contributions make it into Keras. This contribution repository is both the proving ground for new functionality, and the archive for functionality that (while useful) may not fit well into the Keras paradigm. \n \n Installation \n For instructions on how to install Keras, see https://keras.io/#installation \n shell\ngit clone https://www.github.com/farizrahman4u/keras-contrib.git\ncd keras-contrib\npython setup.py install \n Alternatively, using pip: \n shell\nsudo pip install git+https://www.github.com/farizrahman4u/keras-contrib.git \n For contributor guidelines see  CONTRIBUTING.md \n \n Example Usage \n Modules from the Keras-Contrib library are used in the same way as modules within Keras itself. \n ```python\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport numpy as np \n I wish Keras had the Parametric Exponential Linear activation.. \n Oh, wait..! \n from keras_contrib.layers.advanced_activations import PELU \n Create the Keras model, including the PELU advanced activation \n model = Sequential()\nmodel.add(Dense(100, input_shape=(10,)))\nmodel.add(PELU()) \n Compile and fit on random data \n model.compile(loss=\'mse\', optimizer=\'adam\')\nmodel.fit(x=np.random.random((100, 10)), y=np.random.random((100, 100)), nb_epoch=5, verbose=0) \n Save our model \n model.save(\'example.h5\')\n python\nfrom keras.models import load_model\nfrom keras_contrib.layers.advanced_activations import PELU \n Load our model \n model = load_model(\'example.h5\')\n``` \n A Common "Gotcha" \n As Keras-Contrib is external to the Keras core, loading a model requires a bit more work. While a pure Keras model is loadable with nothing more than an import of  keras.models.load_model , a model which contains a contributed module requires an additional import of  keras_contrib : \n ```python \n Required, as usual \n from keras.models import load_model \n Recommended method; requires knowledge of the underlying architecture of the model \n from keras_contrib.layers.advanced_activations import PELU \n Not recommended; however this will correctly find the necessary contrib modules \n from keras_contrib import * \n Load our model \n model = load_model(\'example.h5\')\n```', 'deep-learning-models \n A repository holding weights of trained models and entire models.']
cameronbriar,["Quotes \n A place to store my quotes. \n Information \n It's currently done in PHP with MySQL. \n My goal is to redo this project in other fun ways. \n Demo \n http://cloudedbox.com/quote/", 'FALT \n Fresno Audiovisual Lexicon Tool \n FALT is a tool to help investigate the similarities between audio-alone communication and visual-alone communication.  \n \n To install (Debian): \n # Install required Python modules\n\npip install django==1.4.2\npip install python-Levenshtein\n\n# Get the FALT project\ngit clone https://github.com/cameronbriar/FALT.git\ncd FALT\n\n# Fix `UPDATE_ROOT_DIRECTORY` variable in `FALT/settings.py` based on project directory location (e.g. `/home/user/FALT`)\n\n# Run the server\npython manage.py runserver 0.0.0.0:8000\n\n# Visit http://localhost:8000 in a browser\n# Click "About" to learn more\n', "the way it's put together", 'House Issue Tracker \n My new house\'s todo.txt \n CLI Interface \n Create \n To create an issue from the command line, simply: \n curl --user "cameronbriar:password" --request POST --data \'{"title" : "Shampoo the carpets", "body" : "Rent a RugDoctor and clean the filthy, blue carpets", "labels" : ["interior", "contingent"] }\' https://api.github.com/repos/cameronbriar/house/issues\n \n Request \n To retrieve an issue from the command line, simply: \n curl --user "cameronbriar:password" --request GET https://api.github.com/repos/cameronbriar/house/issues\n', 'Python and Curses \n import curses\n', 'Details \n iPython Notebook', '```\n< ANSIBLE > \n \n    \\ (__)\n     (oo)\n \n /------\\/\n  / |    ||\n *  /---/\\\n    ~~   ~~\n```', 'Javascript for Beginners @ Geekwise (2016) \n Outline \n ├── week_1      Intro to JavaScript, the tools, and the DOM\n│\xa0\xa0 ├── day_1\n│\xa0\xa0 └── day_2\n├── week_2      Syntax, Variables, Conditions, Functions, and For Loops\n│\xa0\xa0 ├── day_3\n│\xa0\xa0 └── day_4\n├── week_3      Functions, For Loops, Events, and "The Grid"\n│\xa0\xa0 ├── day_5\n│\xa0\xa0 └── day_6\n├── week_4      TBD\n│\xa0\xa0 ├── day_7\n│\xa0\xa0 └── day_8\n├── week_5      TBD\n│\xa0\xa0 ├── day_10\n│\xa0\xa0 └── day_9\n└── week_6      TBD\n    ├── day_11\n    └── day_12\n \n Instructors \n @thomas\n@cameron\n \n 2016', '', 'Hello', '  \n ReviewNinja Slack Bot \n A simple Slack Bot for  ReviewNinja . Intended to be deployed along side ReviewNinja to faciliate notification via  Slack . \n Credits \n \n![SAP](https://raw.githubusercontent.com/reviewninja/review.ninja/master/sap_logo.png)\n\n \n:heart: from the github team @ sap', 'Available Soon \n My Online Résumé', 'Adventures w/Angular 2 \n \n Angular 2 (ng2) \n AWS \n Docker \n Continuous Delivery \n etc... \n']
andyone,['.dotfiles \n A set of zsh, git, tig, and tmux configuration files. \n Installation \n bash <(curl -fsSL https://andy.one/dotfiles/install.sh) \n Extra commands \n | Name | Description |\n|------|-------------|\n|  tx  | Start or attach to tmux session |\n|  sshk  |  ssh  command without checking and saving host key |\n|  scpk  |  scp  command without checking and saving host key |\n|  g  |  grep  shortcut |\n|  e  | Editor shortcut |\n|  d  | Docker shortcut |\n|  dr  | Docker  run  shortcut |\n|  de  | Docker  exec  shortcut |\n|  hf  |  grep  over zsh history |\n|  txc  | Rename current tmux window to short path to current directory |\n|  goc  | Create HTML coverage report for Go sources |\n|  gcl  | Clone repository with Go sources |\n|  bkp  | Create backup for file or directory |\n|  git release {version}  | Add signed version tag for the latest commit to the master branch |\n|  git tag-delete {tag}  | Delete tag everywhere |\n|  git tag-update {tag}  | Update tag to the latest commit |\n|  git pr {pr}  | Fetch PR with given ID from GitHub |\n|  git undo  | Undo previous commit | \n Git aliases \n | Alias | Original   |\n|-------|------------|\n|  st   |  status    |\n|  ci   |  commit    |\n|  br   |  branch    |\n|  co   |  checkout  |\n|  df   |  diff      |\n|  dfi  |  icdiff    |\n|  lg   |  log       | \n Tmux cheatsheet \n | Shortcut | Action |\n|----------|--------|\n|  CTRL+B  | Prefix key |\n|  CTRL+B  →  T  | Show current time |\n|  CTRL+B  →  С  | Create new window |\n|  CTRL+B  →  R  | Rearrage windows |\n|  CTRL+B  →  W  | List windows |\n|  CTRL+B  →  S  | Show current tmux sessions |\n|  CTRL+B  →  \\|  | Split window vertically |\n|  CTRL+B  →  -  | Split window horizontaly |\n|  CTRL+B  →  ,  | Set window name |\n|  CTRL+B  →  N  | Next window |\n|  CTRL+B  →  P  | Previous window |\n|  CTRL+B  →  A  | Toggle panes syncing |\n|  CTRL + ←  | Move current window to the left ( reorder windows ) |\n|  CTRL + →  | Move current window to the right ( reorder windows ) |\n|  CTRL+B  →  Q  | Show pane numbers |\n|  CTRL+B  →  X  | Kill pane |\n|  ALT + ←  | Select pane on the left |\n|  ALT + →  | Select pane on the right |\n|  ALT + ↑  | Select upper pane |\n|  ALT + ↓  | Select bottom pane |\n|  CTRL+B  →  Space  | Set panes layout |\n|  CTRL+B  →  PgUp  | Scroll up |\n|  CTRL+B  →  PgDn  | Scroll down |\n|  F1  | Select window #1 |\n|  F2  | Select window #2 |\n|  F3  | Select window #3 |\n|  F4  | Select window #4 |\n|  F5  | Select window #5 |\n|  F6  | Select window #6 |\n|  F7  | Select window #7 |\n|  F8  | Select window #8 |\n|  F9  | Select window #9 |\n|  F10  | Select window #10 |\n|  F11  | Select window #11 |\n|  F12  | Kill current window | \n For function keys support in XShell 5+ you should use custom  mappings file .', '\n \n \n \n \n \n \n \n \n \n Installation  •  Usage example  •  Build Status  •  License \n \n redy  is a tiny Redis client based on  radix.v2  code base. \n Installation \n Make sure you have a working Go 1.17+ workspace ( instructions ), then: \n go get github.com/essentialkaos/redy/v4 \n For update to latest stable release, do: \n go get -u github.com/essentialkaos/redy/v4 \n Usage example \n ```go\npackage main \n import (\n  "fmt"\n  "time" \n "github.com/essentialkaos/redy/v4"\n) \n func main() {\n  rc := redy.Client{\n    Network:     "tcp",\n    Addr:        "127.0.0.1:6379",\n    DialTimeout: 15 * time.Second,\n  } \n err := rc.Connect() \n if err != nil {\n    fmt.Printf("Connection error: %v\\n", err)\n    return\n  } \n r := rc.Cmd("SET", "ABC", 1) \n if r.Err != nil {\n    fmt.Printf("Command error: %v\\n", r.Err)\n    return\n  } \n r = rc.Cmd("GET", "ABC") \n if r.Err != nil {\n    fmt.Printf("Command error: %v\\n", r.Err)\n    return\n  } \n val, err := r.Int() \n if err != nil {\n    fmt.Printf("Parsing error: %v\\n", err)\n    return\n  } \n fmt.Printf("ABC -> %d\\n", val)\n}\n``` \n Build Status \n | Branch     | Status |\n|------------|--------|\n|  master  |   |\n|  develop  |   | \n License \n MIT \n', '👋 Hi! My name is Anton, and I do some technical stuff. I develop tools and services with Go, C, Python, and Bash. Also, I have good experience in writing RPM specs and building packages.  \n \n Some of my projects: \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n More projects made by me can be found on the  @essentialkaos  organization page. \n \n Twitter  •  Instagram  •  Dribble  •  Flickr \n', 'sublime-settings \n This repository contains configuration files for  Sublime Text 4 . \n \n win/preferences.json  - Main Sublime Text configuration for Windows \n win/LSP.sublime-settings  -  LSP  configuration for Go on Windows \n mac/preferences.json  - Main Sublime Text configuration for macOS \n mac/LSP.sublime-settings  -  LSP  configuration for Go on macOS \n', 'action-info \n This is a very simple project for printing info about the GitHub Actions runners environment.']
