{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "# Libraries #\n",
    "#############\n",
    "import json\n",
    "import wget\n",
    "import time\n",
    "import csv\n",
    "import requests\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#############\n",
    "# Constants #\n",
    "#############\n",
    "\n",
    "URL = \"https://api.github.com/search/repositories?q=\"  # The basic URL to use the GitHub API\n",
    "QUERY = \"user:MustafaYasin\"  # The personalized query (for instance, to get repositories from user 'rsain')\n",
    "SUB_QUERIES = [\"+created%3A<%3D2021-03-31\",\n",
    "              \"+created%3A>%3D2014-01-01\"]  # Different sub-queries if you need to collect more than 1000 elements\n",
    "PARAMETERS = \"&per_page=100\"  # Additional parameters for the query (by default 100 items per page)\n",
    "DELAY_BETWEEN_QUERIES = 10  # The time to wait between different queries to GitHub (to avoid be banned)\n",
    "OUTPUT_FOLDER = \"../user_data_csv/\"  # Folder where ZIP files will be stored\n",
    "OUTPUT_CSV_FILE = \"../user_data_csv/repositories.csv\"  # Path to the CSV file generated as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "# Functions #\n",
    "#############\n",
    "\n",
    "def getUrl(url):\n",
    "    \"\"\" Given a URL it returns its body \"\"\"\n",
    "    response = requests.get(url)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/your/folder/GitHub-Crawler/repositories.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/myasin/Documents/thesis/project/backend/recommender_system/readme.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/myasin/Documents/thesis/project/backend/recommender_system/readme.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m countOfRepositories \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/myasin/Documents/thesis/project/backend/recommender_system/readme.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Output CSV file which will contain information about repositories\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/myasin/Documents/thesis/project/backend/recommender_system/readme.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m csv_file \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(OUTPUT_CSV_FILE, \u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/myasin/Documents/thesis/project/backend/recommender_system/readme.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m repositories \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mwriter(csv_file, delimiter\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/myasin/Documents/thesis/project/backend/recommender_system/readme.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Run queries to get information in json format and download ZIP file for each repository\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/your/folder/GitHub-Crawler/repositories.csv'"
     ]
    }
   ],
   "source": [
    "########\n",
    "# MAIN #\n",
    "########\n",
    "\n",
    "# To save the number of repositories processed\n",
    "countOfRepositories = 0\n",
    "\n",
    "# Output CSV file which will contain information about repositories\n",
    "csv_file = open(OUTPUT_CSV_FILE, 'w')\n",
    "repositories = csv.writer(csv_file, delimiter=',')\n",
    "\n",
    "# Run queries to get information in json format and download ZIP file for each repository\n",
    "for subquery in range(1, len(SUB_QUERIES) + 1):\n",
    "    print(\"Processing subquery \" + str(subquery) + \" of \" + str(len(SUB_QUERIES)) + \" ...\")\n",
    "    # Obtain the number of pages for the current subquery (by default each page contains 100 items)\n",
    "    url = URL + QUERY + str(SUB_QUERIES[subquery - 1]) + PARAMETERS\n",
    "    data = json.loads(json.dumps(getUrl(url)))\n",
    "    numberOfPages = int(math.ceil(data['total_count'] / 100.0))\n",
    "    print(\"No. of pages = \" + str(numberOfPages))\n",
    "    print(\"No. of pages = \" + str(numberOfPages))\n",
    "\n",
    "    # Results are in different pages\n",
    "    for currentPage in range(1, numberOfPages + 1):\n",
    "        print(\"Processing page \" + str(currentPage) + \" of \" + str(numberOfPages) + \" ...\")\n",
    "        url = URL + QUERY + str(SUB_QUERIES[subquery - 1]) + PARAMETERS + \"&page=\" + str(currentPage)\n",
    "        data = json.loads(json.dumps(getUrl(url)))\n",
    "        # Iteration over all the repositories in the current json content page\n",
    "        for item in data['items']:\n",
    "            # Obtain user and repository names\n",
    "            user = item['owner']['login']\n",
    "            repository = item['name']\n",
    "            # Download the zip file of the current project\n",
    "            print(\"Downloading repository '%s' from user '%s' ...\" % (repository, user))\n",
    "            url = item['clone_url']\n",
    "            fileToDownload = url[0:len(url) - 4] + \"/archive/refs/heads/master.zip\"\n",
    "            fileName = item['full_name'].replace(\"/\", \"#\") + \".zip\"\n",
    "            try:\n",
    "                wget.download(fileToDownload, out=OUTPUT_FOLDER + fileName)\n",
    "                repositories.writerow([user, repository, url, \"downloaded\"])\n",
    "            except Exception as e:\n",
    "                print(\"Could not download file {}\".format(fileToDownload))\n",
    "                print(e)\n",
    "                repositories.writerow([user, repository, url, \"error when downloading\"])\n",
    "            # Update repositories counter\n",
    "            countOfRepositories = countOfRepositories + 1\n",
    "\n",
    "    # A delay between different sub-queries\n",
    "    if subquery < len(SUB_QUERIES):\n",
    "        print(\"Sleeping \" + str(DELAY_BETWEEN_QUERIES) + \" seconds before the new query ...\")\n",
    "        time.sleep(DELAY_BETWEEN_QUERIES)\n",
    "\n",
    "print(\"DONE! \" + str(countOfRepositories) + \" repositories have been processed.\")\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "474cabef28ef5f39850ae0283db27a2b2b64f747313c5bcb210a7b2692fe8216"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
