,USERNAMES,READMES
0,alexbw,Sampling Inference for Bayesian HSMMs and HMMs   This is a Python library for approximate unsupervised sampling inference in Bayesian Hidden Markov Models  HMMs  and explicit duration Hidden semi Markov Models  HSMMs   focusing on the Bayesian Nonparametric extensions  the HDP HMM and HDP HSMM  via the weak limit approximation    There are also some plugins to extend the functionality      factorial models   autoregressive models   collapsed HDP sampling inference       The inference can be run in parallel over multiple cores and or multiple machines  even on EC2   using  ipython  s excellent  ipython parallel  module  Someday I might even document how to do it    Installing   You can clone this library and its dependencies with   git clone   recursive git   github com mattjj pyhsmm git   The library depends on  numpy    scipy   and  for visualization   matplotlib     Disabling assertions may speed things up  to disable assertions  run your CPython interpreter with the   O  flag    A Simple Demonstration   Here s how to draw from the HDP HSMM posterior over HSMMs given a sequence of observations   The same example  along with the code to generate the synthetic data loaded in this example  can be found in  examples basic py      Let s say we have some 2D data in a data txt file    bash  head  5 data txt  3 711962552600095444e 02 1 456401745267922598e 01 7 553818775915704942e 02 2 457422192223903679e 01  2 465977987699214502e 00 5 537627981813508793e 01  7 031638516485749779e 01 1 536468304146855757e 01  9 224669847039665971e 01 3 680035337673161489e 01   In Python  we can plot the data in a 2D plot  collapsing out the time dimension       python import numpy as np from matplotlib import pyplot as plt   data   np loadtxt  data txt   plt plot data   0  data   1   kx           We can also make a plot of time versus the first principal component    python from pyhsmm util plot import pca project data plt plot pca project data data 1       To learn an HSMM  we ll use  pyhsmm  to create an  HSMM  instance using some reasonable hyperparameters  We ll ask this model to infer the number of states as well  since an HDP HSMM is instantiated by default   so we ll give it an  Nmax  parameter       python import pyhsmm import pyhsmm basic distributions as distributions   obs dim   2 Nmax   25   obs hypparams     mu 0  np zeros obs dim                    sigma 0  np eye obs dim                    kappa 0  0 3                   nu 0  obs dim 5  dur hypparams     alpha 0  2 30                    beta 0  2    obs distns    distributions Gaussian  obs hypparams  for state in range Nmax   dur distns    distributions PoissonDuration  dur hypparams  for state in range Nmax     posteriormodel   pyhsmm models HSMM          alpha 6  gamma 6     better to sample over these  see concentration resampling py         init state concentration 6     pretty inconsequential         obs distns obs distns          dur distns dur distns          trunc 60    duration truncation speeds things up when it s possible        The first two arguments set the  new table  proportionality constant for the meta Chinese Restaurant Process and the other CRPs  respectively  in the HDP prior on transition matrices  For this example  they really don t matter at all  but on real data it s much better to infer these parameters  as in  examples concentration resampling py      The  trunc  parameter is an optional argument that can speed up inference  it sets a truncation limit on the maximum duration for any state  If you don t pass in the  trunc  argument  no truncation is used and all possible state duration lengths are considered    Then  we add the data we want to condition on    python posteriormodel add data data     If we had multiple observation sequences to learn from  we could add them to the model just by calling  add data    for each observation sequence     Now we run a resampling loop  For each iteration of the loop  all the latent variables of the model will be resampled by Gibbs sampling steps  including the transition matrix  the observation means and covariances  the duration parameters  and the hidden state sequence  We ll also copy some samples so that we can plot them    python models      for idx in progprint xrange 150       posteriormodel resample model       if  idx 1    10    0          models append copy deepcopy posteriormodel     Now we can plot our saved samples    python fig   plt figure   for idx  model in enumerate models       plt clf       model plot       plt gcf   suptitle  HDP HSMM sampled after  d iterations     10  idx 1        plt savefig  iter   3d png     10  idx 1        I generated these data from an HSMM that looked like this      So the posterior samples look pretty good    Speed   HSMMs constitute a much more powerful model class than plain old HMMs  and that enhanced power comes with a computational price  each sampling iteration for an HSMM is much slower than that of an HMM  But that price is often worthwhile if you want to place priors on state durations or have the model learn duration structure present in the data   In the example  strong duration structure is what made the inference algorithm latch onto the correct explanation so easily   In addition  the increased cost of each iteration often pays for itself  since HSMM samplers empirically seem to take fewer iterations to converge than comparable HMM samplers    Using my nothing special i7 920 desktop machine and a NumPy SciPy built against Intel s MKL BLAS  which generally outperforms ATLAS for vectorized operations  along with the Eigen backed classes  here s how long the demo iterations took      python examples hsmm py                               25 100     0 05sec avg     3 95sec ETA                                 50 100     0 05sec avg     2 64sec ETA                                 75 100     0 05sec avg     1 34sec ETA                                100 100     0 05sec avg     0 05sec ETA      0 05sec avg     5 21sec total   Extending the Code   To add your own observation or duration distributions  implement the interfaces defined in  basic abstractions py   Also see the plugins  To get a flavor of the style  see  pybasicbayes     Contributors   Contributions by Chia ying Lee    References       Matthew J  Johnson and Alan S  Willsky   Bayesian Nonparametric Hidden   Semi Markov Models     Journal of Machine Learning Research  JMLR   14 673 701  February 2013        Matthew J  Johnson and Alan S  Willsky   The Hierarchical Dirichlet Process   Hidden Semi Markov Model   26th   Conference on Uncertainty in Artificial Intelligence  UAI 2010   Avalon    California  July 2010        bibtex  article johnson2013hdphsmm      title  Bayesian Nonparametric Hidden Semi Markov Models       author  Johnson  Matthew J  and Willsky  Alan S        journal  Journal of Machine Learning Research       pages  673  701       volume  14       month  February       year  2013     An analgesic for high performance audio on iOS and OSX    Really fast audio in iOS and Mac OS X using Audio Units is hard  and will leave you scarred and bloody  What used to take days can now be done with just a few lines of code    Getting Audio   objective c Novocaine  audioManager    Novocaine audioManager    audioManager setInputBlock   float  newAudio  UInt32 numSamples  UInt32 numChannels           Now you re getting audio from the microphone every 20 milliseconds or so  How s that for easy         Audio comes in interleaved  so         if numChannels   2  newAudio 0  is channel 1  newAudio 1  is channel 2  newAudio 2  is channel 1  etc       audioManager play     Playing Audio   objective c Novocaine  audioManager    Novocaine audioManager    audioManager setOutputBlock   float  audioToPlay  UInt32 numSamples  UInt32 numChannels           All you have to do is put your audio into  audioToPlay        audioManager play     Does anybody actually use it    Yep  Novocaine is result of three years of work on the audio engine of  Octave    Fourier  and  oScope   a powerful suite of audio analysis apps  Please do check them out    A thing to note    The RingBuffer class is written in C   to make things extra zippy  so the classes that use it will have to be Objective C    Change all the files that use RingBuffer from MyClass m to MyClass mm    Want some examples    Inside of ViewController mm are a bunch of tiny little examples I wrote  Uncomment one and see how it sounds   Do note  however  for examples involving play through  that you should be using headphones  Having the  mic and speaker close to each other will produce some gnarly feedback      Want to learn the nitty gritty of Core Audio    If you want to get down and dirty  if you want to get brave and get close to the hardware  I can only point you to the places where I learned how to do this stuff  Chris Adamson and Michael Tyson are two giants in the field of iOS audio  and they each wrote indispensable blog posts   this is Chris s    this is Michael s    Also  Chris Adamson now has a  whole gosh darned BOOK on Core Audio   I would have done unspeakable things to get my hands on this when I was first starting    My Code for the Netflix Prize   I m not aware of folks having published their code for the Netflix Prize  Here s mine   Under the team name  Hi    I competed alone in college  I did it mostly for fun  and to learn modern machine learning techniques  It was an incredibly valuable  but strenuous  time  Well worth it on all fronts  though    I peaked out at  45 or so  and then dropped out to work on my senior thesis  and came in  145 or so     What I learned in the process was that smarter wasn t always better    make an algorithm  and then scale it up  and then make a dozen tweaks to it  and then average all of the results together  That s how you climbed the leaderboard       Anyhoo  I haven t touched this code in awhile  but perhaps it ll be useful to folks interested in competitive data mining   Specifically  the lessons I learned      Get the raw data into a saved and manageable format  fast   The easier it is to load your data in and start mutating it  the better    If doing simple pivots on your data is hard  and slows you down from visualizing whats in your data  spend time making data structures which make that easy      Generalize  Iterate  If you have a method you think will work  but it has a lot of knobs  and you don t know the best way to set those knobs  make it easy for you to try  every possible iteration   There is often not a good way to figure out what the  best  approach is  You will have to try many of them in order to build up an intuition  Specifically  that means  for me  a pluggable architecture  If there s ten ways to try a particular step  make sure you write your overarching algorithm so that it takes a function that you can pass to it  as opposed to having a method hardwired in the code  That way  you can hotswap all your ideas      Speed is a feature  Of course you make sure it works first  But your goal is to see if something works  If an algorithm takes a day to run  but you can spend five hours making it run in 1 3 of a day  do it  You ll be running it over and over again  and you ll learn more if you can iterate       As for the technical nitty gritty  everything that s speed sensitive is written in Cython  which was the best balance of speed and convenience in 2009  If I were to do it al again  I would use  Numba  http   github com numba numba       The original data is gone  I believe  but I might have it stored somewhere  I ll look for that    NURBS   Non Uniform Rational B Splines    This python module is a revival and update of Runar Tenfjord s NURBS toolbox  which itself is a port of Mark Spink s SCILAB MATLAB tolbox to python with help of numpy    Dependency   Python 2 0 or above  NumPy  Dislin    optional but recomended  PyOpenGL    optional     Installation   Just run   python setup py install     License   Runar Tenfjord originally relased this package under the GPL Version 2 license    so I suppose that means it has to stay that way       Originally by  Runar Tenfjord  runten netcom no  Minor updates to work with NumPy by Alex Wiltschko   cuda tests   Gotta learn some CUDA stuff pypatent   Scrape patents from the USPTO airruler   A ruler  Made of air   paralleltools   A summary of parallelizing moderate amounts of work in Python nsgt   Non stationary Gabor transforms  GitHub mirror of http   grrrr org research software nsgt   IDA   This code accompanies the publication from the Whitesides lab to Lab on a Chip  concerning automated analysis of red blood cell health using affordable field tests  Specifically  this code implements the automated extraction of scanned AMPS tubes from flatbed scanner images  the distillation of those images into 1D data traces  and then the automated identification of the anemic disease state of the blood in those 1D data traces  as well as the prediction of continuously varying red blood cell  RBC  parameters    Installation   This code requires Python  as well as some extra 3rd party libraries  All routines have only been tested on Mac OS  but should work on Linux  No guarantees for Windows  1  First  we recommend using the  Anaconda  Python distribution  specifically the Python 2 7 version   Download and install here   1  With anaconda installed  you will need one extra library  to read TIFF files   pip install imageio  1  You should be able to run the notebooks now  Inside this code repository  start up an IPython notebook   jupyter notebook  1  You should now be able to browser around the  extraction  and  analysis  folders  which contain the relevant code    Running extraction   The first step required will be to prepare the raw data of TIFF file scans of 4x3 12 tubes from a flatbed scanner  We will also have to associate metadata for each patient that the blood in a tube was drawn from  Our end goal will be a 1D array for each tube  along with the corresponding patient data  anemic state   RBC parameters     The extraction algorithm is explained step by step in the notebook  extraction Explaining the Extraction Algorithm ipynb   The implementation we use in the paper  which also fuses patient metadata with AMPS image data  can be found in  extraction Data Extraction Pipeline ipynb   Most of the code in this notebook is specific to the particular Excel file format that we used to record patient metadata  and may have to be largely rewritten for reuse    Running analysis   There are two sets of analyses done in the paper  classification and regression    Running classification analysis   The goal of this analysis is to predict disease state only from the 1D data trace extracted from each AMPS tube  We use logistic regression  a linear classifier  and transform the 1D data representation using PCA to remove redundancy and constrain the input dimensionality of the problem  We also used Bayesian Optimization  bayesopt  to tune the hyperparameters of the problem  including the specific output dimension of PCA  the regularization parameter of logistic regression  and some image preprocessing parameters  Unfortunately  the service we used for bayesopt is no longer available  If you would like to automatically tune these parameters  we recommend either using random search  a surprisingly effective hueristic   or the open source library  Spearmint   upon which the now defunct service we used was based  or products from the company SigOpt  which also implements bayesopt  The file we used to automatically tune these hypers is  classify py   The best set of hyperparameters is stored in the notebook  Analyzing best classification ipynb   This notebook examines ROC performance of the classifier for discriminating different anemia types  as well as the effect of centrifugation time of the tube on IDA classification AUC performance    Running regression analysis    Similar to above  we used a defunct bayesopt service to automatically tune some parameters of this algorithm  The original file is in  regress py   The original methodology will work well  even without automated tuning  The best set of hyperparameters is stored in the notebook  Analyzing best regression ipynb     License   See the LICENSE file  It is under a GPL license  and this code may only be used for academic purposes  adabayes   To do       x  Find candidate last layer features  DeCAF  Overfeat     x  Find AlexNet code  5 convolutional layers with max pooling  then three fully connected layers     X   Find download script for MNIST dataset    X   Find download script for CIFAR10 dataset    x  Fork torch dist repo    x  Update torch dist repo for OS X 10 10 install    x  Add new required submodules to distro  nnx  iTorch  ccn2  cunnx  cudnn  sdl2  cutorch     x  Figure out model serialization and loading   Built in model serialization   Loading  and  saving     There is also facebook s  Thrift library   which I haven t seen many examples for            Get the data out of the  DataSource  models that we re using       Train and Whetlab a net on MNIST  to get the whole workflow going        Train and Whetlab a net on CIFAR10       Train and Whetlab a net on STL10       Build dumb ensemble on CIFAR10       Build dumb ensemble on STL10       Grok boosting criterion       Whetlab a net with progressive ensembling on CIFAR10       Whetlab a net with progressive ensembling on STL10       Whetlab a net with progressive ensembling on the last layer features of ImageNet       Whetlab a net with progressive ensembling and tuned class weights on CIFAR10       Whetlab a net with progressive ensembling and tuned class weights on STL10       Extract last layer ImageNet features       Host last layer ImageNet features       Build download script for last layer ImageNet features       Train and Whetlab a net on last layer features on ImageNet       Build a dumb ensemble on ImageNet       Whetlab a net with progressive ensembling and tuned class weights on the last layer features of ImageNet     Collecting some resources   AlexNet  https   github com soumith convnet benchmarks blob master torch7 imagenet winners alexnet lua   I think this is NiN  https   github com soumith convnet benchmarks blob master torch7 imagenet winners googlenet lua   OverFeat  https   github com facebook fbcunn blob master examples imagenet models overfeat cunn lua   Multiclass criterion  https   github com torch nn blob master doc criterion md classnllcriterion   Some other interesting nets  https   github com culurciello profiling   Bunch of demos  not all nets  https   github com torch demos   Overfeat features  http   cilvr nyu edu doku php id software overfeat start   Decaf ImageNet submission  https   github com UCB ICSI Vision Group decaf release wiki imagenet   Some other net implementations  https   github com eladhoffer ImageNet Training Conda recipes for installing packages from the Torch ecosystem    NOTE  No longer maintained    To install packages         Install anaconda if you don t have it  instructions here for OS X    wget http   repo continuum io miniconda Miniconda latest MacOSX x86 64 sh sh Miniconda latest MacOSX x86 64 sh  b  p  HOME anaconda   Add anaconda to your  PATH   export PATH  HOME anaconda bin  PATH   Install Lua   Torch   conda install lua 5 2 lua science  c alexbw   Available versions of Lua  2 0  5 1  5 2  5 3   2 0 is LuaJIT         To build packages         Install anaconda if you don t have it  instructions here for OS X    wget http   repo continuum io miniconda Miniconda latest MacOSX x86 64 sh sh Miniconda latest MacOSX x86 64 sh  b  p  HOME anaconda   Add anaconda to your  PATH   export PATH  HOME anaconda bin  PATH   Get the newest version of conda  as well as some conda build tools   conda update conda  y conda install conda build anaconda client  y   Build all packages   sh build all sh   Ideally  all you have to do to install everything is this   conda install lua 5 2 lua science       TODO     https   github com AlexMili torch dataframe    https   github com twitter torch ipc    https   github com twitter torch distlearn    https   github com twitter torch dataset   Resources        Making packages relocatable  LuaJIT hard codes path        Build instructions for luarocks       Build instructions for Lua       Patching files with git diffs is finicky       Linking against readline   need a few extra flags  and link against  lncursesw  not  lncurses        Upgrading old Debian    Also this         Misc notes   metadata ns cfg     defines for YAML directives main build    defines version numbers  environ get lua include dir     uses version numbers to locate the include directory config Config  get lua   uses version numbers to locate the binary This is where the linked package name is converted into what is used validata   Continuous integration for your data   We do continuous integration on code  Why not data  Validata is a small package to run basic sanity checks on your data  I haven t found anything that aggregates all of these checks and tricks in one place    There is only one method which is exposed   check data labels    optionally taking data or labels   If any data check fails  it throws a well named error  as well as hints for how you might fix the problem    data covariance matrix ill conditioned  Try whitening    Initially  this will be a Python NumPy only package running basic checks  but hopefully it becomes a resource of data sanity and sanitation checks  Still very much a work in progress    Examples  some implemented  some not  include      If your labels are one hot  are you using all slots    Is the covariance matrix of your data ill conditioned    Do you have any constant variables    Can you train a classifier to distinguish train and test data  using whether they are in train or test as a label  Indicates different data distributions    If you re using integer labels  are the unique labels contiguous    Do you have just one unique label    Is the data under different labels statistically separable    If you have an old dataset and a new dataset  or two halves of the same dataset   is the distribution of each dimension stationary  Check for divergence with a KS test    What else  I end up applying these tricks in a very ad hoc fashion  whenever a subtle bug pops up  and not rigorously before each project I tackle  I d like to stuff all these tricks in one place  and run them like a unit test  or a continuous integration test  on data that I start working with      Should probably also think about engarde bayarea dl summerschool   Torch notebooks and slides for the Bay Area Deep Learning Summer School   Installation Instructions   Install anaconda if you don t have it  instructions here for OS X    wget http   repo continuum io miniconda Miniconda latest MacOSX x86 64 sh sh Miniconda latest MacOSX x86 64 sh  b  p  HOME anaconda   Add anaconda to your  PATH   export PATH  HOME anaconda bin  PATH   Install Lua   Torch       conda install lua 5 2 lua science  c alexbw   Although  you could install other Lua versions like 2 0  LuaJIT   5 1  5 2 and 5 3         Clone this repository and start the notebook server       git clone https   github com alexbw bayarea dl summerschool git cd bayarea dl summerschool itorch notebook   Will open a browser tab  then you can navigate to the notebooks
1,iassael,DEARanking   Proposing a hybrid DEA Polynomial Interpolation  DEA PI  algorithm for the raking of protected areas  An application in Greece Find The Word            Cheat    Find the greek word from the given letters  Application to cheat and solve games like             and                csoxcal   Oxford University  Computer Science Calendar Filter for Google Calendar use A Hybrid Parallel Implementation of the Aho Corasick and Wu Manber Algorithms Using NVIDIA CUDA and MPI Evaluated on a Biological Sequence Database    Charalampos S  Kouzinopoulos  Yannis M  Assael  Themistoklis K  Pyrgiotis  Konstantinos G  Margaritis   Multiple matching algorithms are used to locate the occurrences of patterns from a finite pattern set in a large input string  Aho Corasick and Wu Manber  two of the most well known algorithms for multiple matching require an increased computing power  particularly in cases where large size datasets must be processed  as is common in computational biology applications  Over the past years  Graphics Processing Units  GPUs  have evolved to powerful parallel processors outperforming Central Processing Units  CPUs  in scientific calculations  Moreover  multiple GPUs can be used in parallel  forming hybrid computer cluster configurations to achieve an even higher processing throughput  This paper evaluates the speedup of the parallel implementation of the Aho Corasick and Wu Manber algorithms on a hybrid GPU cluster  when used to process a snapshot of the Expressed Sequence Tags of the human genome and for different problem parameters    Links   arXiv pre print   Bibtex    article kouzinopoulos2015hybrid    title  A Hybrid Parallel Implementation of the Aho Corasick and Wu Manber Algorithms Using NVIDIA CUDA and MPI Evaluated on a Biological Sequence Database     author  Kouzinopoulos  Charalampos S  and Assael  Yannis M  and Pyrgiotis  Themistoklis K  and Margaritis  Konstantinos G      journal  International Journal on Artificial Intelligence Tools     volume  24     number  1     pages  1540001     year  2015     publisher  World Scientific      License   Code licensed under the GNU General Public License v3 0  RKHS Function   Description   Synthetic heteroscedastic 1D function generated from 2 Squared Exponential kernels for Bayesian Optimization method evaluation tasks    Input Space   x    0  1    Global Maximum   x 0 89235  f x  5 73839   Authors   Copyright  C  2014 Ziyu Wang  John Alexander Assael  Nando de Freitas published under GPLv3 license   Screenshot   Decomposition module for Torch7       Principal Component Analysis  PCA        Whitened Principal Component Analysis  W PCA        Linear Discriminant Analysis  LDA        Locality Preserving Projections  LPP        Neighbourhood Preserving Projections  NPP        Fast Independent Component Analysis  FastICA        by John Alexander Assael   http   www johnassael com   https   github com iassael torch7 decomposition   Installation   Clone this repository or download the source code    Usage   Call  decomposition   require  decomposition   and then any of the following        decomposition pca x          decomposition lda x  y          decomposition lpp x          decomposition npp x          decomposition fastica x          Alternativly  you can use iTorch notebook and open  decomposition ipynb     Contributing     Fork it    Create your feature branch   git checkout  b my new feature   Commit your changes   git commit  am  Add some feature    Push to the branch   git push origin my new feature   Submit a pull request     Notes   The implementations were developed in terms of learning and may not be optimal    License   Copyright  C  2015 John Alexander Assael  www johnassael com  https   github com iassael torch7 decomposition   The MIT License  MIT    Permission is hereby granted  free of charge  to any person obtaining a copy of this software and associated documentation files  the  Software    to deal in the Software without restriction  including without limitation the rights to use  copy  modify  merge  publish  distribute  sublicense  and or sell copies of the Software  and to permit persons to whom the Software is furnished to do so  subject to the following conditions    The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software    THE SOFTWARE IS PROVIDED  AS IS   WITHOUT WARRANTY OF ANY KIND  EXPRESS OR IMPLIED  INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT  IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM  DAMAGES OR OTHER LIABILITY  WHETHER IN AN ACTION OF CONTRACT  TORT OR OTHERWISE  ARISING FROM  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE  Torch7 impementation of    Embed to Control  A Locally Linear Latent Dynamics Model for Control from Raw Images   by Manuel Watter  Jost Tobias Springenberg  Joschka Boedecker  Martin Riedmiller  http   arxiv org abs 1506 07365    Implemented by John Alexander M  Assael  iassael gmail com  and Marc P  Deisenroth    The MIT License  MIT   Permission is hereby granted  free of charge  to any person obtaining a copy of this software and associated documentation files  the  Software    to deal in the Software without restriction  including without limitation the rights to use  copy  modify  merge  publish  distribute  sublicense  and or sell copies of the Software  and to permit persons to whom the Software is furnished to do so  subject to the following conditions   The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software   THE SOFTWARE IS PROVIDED  AS IS   WITHOUT WARRANTY OF ANY KIND  EXPRESS OR IMPLIED  INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT  IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM  DAMAGES OR OTHER LIABILITY  WHETHER IN AN ACTION OF CONTRACT  TORT OR OTHERWISE  ARISING FROM  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE   Regularization of Neural Networks using DropConnect   Li Wan  Matthew Zeiler  Sixin Zhang  Yann LeCun  Rob Fergus   Dept  of Computer Science  Courant Institute of Mathematical Science  New York University   Torch7 implementation by John Alexander M  Assael Fast and Accurate Deep Network Learning by Exponential Linear Units  ELUs    Djork Arn  Clevert  Thomas Unterthiner  Sepp Hochreiter   Torch7 implementation by John Alexander M  Assael Deep Exploration via Bootstrapped DQN   Ian Osband  Charles Blundell  Alexander Pritzel  Benjamin Van Roy   Usage   nn Bootstrap nn Linear size in  size out   10  0 08    Implemented by Yannis M  Assael  www yannisassael com  Recurrent Batch Normalization   Batch Normalized LSTMs   Tim Cooijmans  Nicolas Ballas  C sar Laurent   a lar G l ehre  Aaron Courville   http   arxiv org abs 1603 09025   Usage   local rnn   LSTM input size  rnn size  n  dropout  bn    n   number of layers  1 N    dropout   probability of dropping a neuron  0 1    bn   batch normalization  true  false    Example   https   github com iassael char rnn   Performance   Validation scores on char rnn with default options     Implemented in Torch by Yannis M  Assael  www yannisassael com  Grid World DQN using torch7   This is a naive DQN example implemented in torch7 to help future research    The environment is based on  rlenvs  of  Kaixhin  and the model makes use of  Increasing the Action Gap    http   arxiv org abs 1512 04860      Implemented by Yannis M  Assael   yannisassael com   Single pendulum Deterministic Policy Gradient example using torch7   Continuous Control with Deep Reinforcement Learning   Timothy P  Lillicrap  Jonathan J  Hunt  Alexander Pritzel  Nicolas Heess  Tom Erez  Yuval Tassa  David Silver  Daan Wierstra   http   arxiv org abs 1509 02971   Dependecies   luarocks install Math RungeKutta luarocks install csvigo luarocks install image luarocks install hdf5   Implemented by Yannis M  Assael   yannisassael com   Using deep Q learning to understand the tax evasion behavior of risk averse firms   Links     arXiv preprint   Expert Systems with Applications     Execution       Requirements  torch   th tax dqn lua   Bibtex    article goumagias2018using    title  Using deep Q learning to understand the tax evasion behavior of risk averse firms     author  Goumagias  Nikolaos D and Hristu Varsakelis  Dimitrios and Assael  Yannis M     journal  Expert Systems with Applications     volume  101     pages  258  270     year  2018     publisher  Elsevier       License   Code licensed under the Apache License v2 0 Restoring ancient text using deep learning   A case study on Greek epigraphy   Yannis Assael     Thea Sommerschield     Jonathan Prag     Ancient History relies on disciplines such as Epigraphy  the study of ancient inscribed texts  for evidence of the recorded past  However  these texts   inscriptions   are often damaged over the centuries  and illegible parts of the text must be restored by specialists  known as epigraphists  This work presents a novel assistive method for providing text restorations using deep neural networks  To the best of our knowledge  Pythia is the first ancient text restoration model that recovers missing characters from a damaged text input  Its architecture is carefully designed to handle long term context information  and deal efficiently with missing or corrupted character and word representations   To train it  we wrote a non trivial pipeline to convert PHI  the largest digital corpus of ancient Greek inscriptions  to machine actionable text  which we call PHI ML  On PHI ML  Pythia s predictions achieve a 30 1  character error rate  compared to the 57 3  of human epigraphists  Moreover  in 73 5  of cases the ground truth sequence was among the Top 20 hypotheses of Pythia  which effectively demonstrates the impact of such an assistive method on the field of digital epigraphy  and sets the state of the art in ancient text restoration       Pythia Bi Word processing the phrase             m d n  gan   nothing in excess   a fabled maxim inscribed on Apollo s temple in Delphi  The letters      are missing  and annotated with      Since word      contains missing characters  its embedding is treated as unknown   unk    The decoder outputs correctly          References     arXiv pre print   EMNLP IJCNLP 2019   Digital Classicist Wiki   DeepMind research blog   University of Oxford news blog     When using any of this project s source code  please cite    inproceedings assael2019restoring    title  Restoring ancient text using deep learning  a case study on  Greek  epigraphy     author  Assael  Yannis and Sommerschield  Thea and Prag  Jonathan     booktitle  Empirical Methods in Natural Language Processing     pages  6369  6376     year  2019      Pythia online   To aid further research in the field we created an online interactive python notebook  where researchers can query one of our models to get text restorations and visualise the attention weights      Google Colab     Pythia offline   The following snippets provide references for regenerating PHI ML and training new models offline    Dependencies   pip install  r requirements txt      python  m nltk downloader punkt   PHI ML dataset generation         Download PHI  this will take a while    python  c  import pythia data phi download  pythia data phi download main      Process and generate PHI ML   python  c  import pythia data phi process  pythia data phi process main        Preprocessed PHI ML uploaded by  Holger Danske800   link   Training   python  c  import pythia train  pythia train main      Evaluation   python  c  import pythia test  pythia test main      load checkpoint  your model path     Docker execution     build sh   run sh  GPU ID  python  c  import pythia train  pythia train main      License   Apache License  Version 2 0      Damaged inscription  a decree concerning the Acropolis of Athens  485 4 BCE    IG  I 3  4B   CC BY SA 3 0  WikiMedia   Vivechrom RGB color matcher   Yannis Assael   www assael gr     Description   This colaboratory notebook is used to find the closest vivechrom gr paint code given an RGB color    Instructions     Open this  Google Colab Notebook     To edit and run the code  you need to click  Open in Playground Mode  if you can see it at the top of the page  or save a local copy somewhere on your computer    The cells need to be run one by one by pressing shift enter or by using the play button    Fill the  target color rgb  value with your target RGB color    Run the cell to get suggestions ordered by color similarity      License       Copyright 2020 Yannis Assael   Licensed under the Apache License  Version 2 0  the  License    you may not use this file except in compliance with the License  You may obtain a copy of the License at   http   www apache org licenses LICENSE 2 0    Unless required by applicable law or agreed to in writing  software distributed under the License is distributed on an  AS IS  BASIS  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND  either express or implied  See the License for the specific language governing permissions and limitations under the License            Restoring and attributing ancient texts using deep neural networks   Yannis Assael 1     Thea Sommerschield 2 3     Brendan Shillingford 1   Mahyar Bordbar 1   John Pavlopoulos 4   Marita Chatzipanagiotou 4   Ion Androutsopoulos 4   Jonathan Prag 3   Nando de Freitas 1   1  DeepMind  United Kingdom   2  Ca  Foscari University of Venice  Italy   3  University of Oxford  United Kingdom   4  Athens University of Economics and Business  Greece      Authors contributed equally to this work       Ancient History relies on disciplines such as Epigraphy  the study of inscribed texts known as  inscriptions   for evidence of the thought  language  society and history of past civilizations  However  over the centuries many inscriptions have been damaged to the point of illegibility  transported far from their original location  and their date of writing is steeped in uncertainty  We present Ithaca  the first Deep Neural Network for the textual restoration  geographical and chronological attribution of ancient Greek inscriptions  Ithaca is designed to assist and expand the historian s workflow  its architecture focuses on collaboration  decision support  and interpretability        Restoration of damaged inscription  this inscription   IG  I 3  4B  records a decree concerning the Acropolis of Athens and dates 485 4 BCE   CC BY SA 3 0  WikiMedia      While Ithaca alone achieves 62  accuracy when restoring damaged texts  as soon as historians use Ithaca their performance leaps from 25  to 72   confirming this synergistic research aid s impact  Ithaca can attribute inscriptions to their original location with 71  accuracy and can date them with a distance of less than 30 years from ground truth ranges  redating key texts of Classical Athens and contributing to topical debates in Ancient History  This work shows how models like Ithaca can unlock the cooperative potential between AI and historians  transformationally impacting the way we study and write about one of the most significant periods in human history        Ithaca s architecture processing the phrase                      the people of Athens    The first 3 characters of the phrase were hidden and their restoration is proposed  In tandem  Ithaca also predicts the inscription s region and date      References     Nature article   DeepMind blog     When using any of this project s source code  please cite     article asssome2022restoring    title    Restoring and attributing ancient texts using deep neural networks     author    Assael   Yannis and Sommerschield   Thea and Shillingford  Brendan and Bordbar  Mahyar and Pavlopoulos  John and Chatzipanagiotou  Marita and Androutsopoulos  Ion and Prag  Jonathan and de Freitas  Nando     doi    10 1038 s41586 022 04448 z     journal    Nature     year    2022      Ithaca inference online   To aid further research in the field we created an online interactive python notebook  where researchers can query one of our trained models to get text restorations  visualise attention weights  and more      Ithaca Interactive Interface   Google Colab for using Ithaca for your research     Ithaca inference offline   Advanced users who want to perform inference using the trained model may want to do so manually using the  ithaca  library directly    First  to install the  ithaca  library and its dependencies  run   sh pip install     Then  download the model via  sh curl   output checkpoint pkl https   storage googleapis com ithaca resources models checkpoint v1 pkl   An example of using the library can be run via  sh python inference example py   input file example input txt  which will run restoration and attribution on the text in  example input txt     To run it with different input text  run    sh python inference example py   input         or using text in a UTF 8 encoded text file    python inference example py   input file some other input file txt       The restoration or attribution JSON can be saved to a file   sh python inference example py       input file example input txt       attribute json attribute json       restore json restore json   For full help  run   sh python inference example py   help   Dataset generation   Ithaca was trained on The Packard Humanities Institute s   Searchable Greek Inscriptions   public dataset  The processing workflow for generating the machine actionable text and metadata  as well as further details on the train  validation and test splits are available at  I PHI dataset     Training Ithaca   See  train README md  for instructions    License   Apache License  Version 2 0
2,bshillingford,id3tag fix codehackathon2014   Team members  Brendan Shillingford Daisy Shih Kevin Lim   http   data gc ca eng canadian open data experience code pycmdline template   Template for making your one off Python utilities more user friendly    Add   arg  and   cmdline  decorators to a main function as below  this provides an easier interface to the  logging  and  optparse  modules    Examples   Example 1  simple program   This program accepts 2 arguments  one a flag passed as    count COUNT   or   n COUNT   another is a position argument  listed without a dash prefixed flag name before it  and a default help flag    help    h  printing a usage page    The arguments to the decorators are the same as in  optparse        python     usr bin env python   from simpleargs import cmdline  arg    arg  argument   help  pass ARG to program   metavar  ARG     simple positional argument  arg    count     n   help  number of bars to foo   type int  default 1   cmdline description  This program foos bars a given number of times    def main argument  count     print  Hello there     print  Your Argument   s    argument   for i in range count       print  Fooing bar   d    i  print  Fooed  d bars     count     if  name       main        main         Example 2  simple program   logging           usr bin env python   import logging as L from simpleargs import cmdline  arg  loglevel    arg  argument   help  pass ARG to program   metavar  ARG     simple positional argument  arg    count     n   type int  default 1   loglevel                                                     if on  log level DEBUG  else INFO  cmdline description  This program foos bars a given number of times    def main argument  count     print  Hello there    L info  You passed an argument    L debug  Your Argument   s    argument   for i in range count       L debug  Fooing bar   d    i  L info  Fooed  d bars     count     if  name       main        main       Starbucks WiFi autologin   Automatically accept ToS when connecting    Barely tested    This tool is purely for educational purposes only  and should  not be used in a real situation    Dependencies      Python 2   requests   BeautifulSoup     Installation    Can just execute starbuckswifi py directly  but install if you want    Install via        python2 setup py install   When you connect to an accesspoint  run    python2  m starbuckswifi     REST and MQTT for IFTTT   What s IFTTT    IFTTT is a service that enables users to  connect different web applications   e g   Facebook  Evernote  Weather  Dropbox  etc   together through simple conditional statements known as  Recipes    Source  Wikipedia    A recipe is a  trigger  action  pair  However  IFTTT doesn t support fully custom actions  However  it does support publishing to any Wordpress blog  This app emulates Wordpress s XML RPC API so that you can send a  GET   POST   PUT  REST request  or publish a MQTT message  all by choosing the  create post  action in IFTTT    Install   Simply clone this repository and upload as a Heroku app  or host it however you want    For local testing  you can use  flaskrun py   If desired  use a  virtualenv   and install all the packages in  requirements txt     Usage   Set the  title  and  body  of the post  Everything else is ignored    Examples      title    GET http   httpbin org get some stuff here   title    POST http   example com something   body    whatever data you want to post   similarly for  PUT     title    MQTT PUB mqtt   iot eclipse org topic name goes here qos 1 retain T   body    the payload of your request     Notice the URI format used to encode MQTT settings    MQTT PUB syntax   The URI is parsed as follows   mqtt    hostname    port    topic name    settings     The post s body is used as the payload and may be empty  but  note IFTTT may remove newlines or mess with whitespace   Hence  I suggest using JSON if possible    Settings must be URI encoded as usual  the following are valid     qos     0  or  1  or  2   default is  0   see MQTT specifications for semantics     retain     true  or  false   default is  false    1    T    t   and  tRUe  are all accepted as true     protocol     MQTTv31  or  MQTTv311   latter is default autobw   autobw  is a simple library for automatically performing  a backwards pass  given only a forwards pass   in Torch    A major advantage of this is that the neural network s  structure need not be fixed before runtime   This allows for easy implementation of structures such as recurrent networks  See the example below    Backpropagation is often described as a method for propagating gradients through a computational graph  One way to implement it for graphs is to explicitly construct a graph given by the user  then evaluate the computational nodes in the order specified in the forward pass  then again but in reverse for the backward pass     Install   luarocks install https   raw githubusercontent com bshillingford autobw torch master autobw scm 1 rockspec   Details   A method that s closer to how one may reason about a neural network is to explicitly write down a forward pass while recording the statements as they are being executed  then execute the statements  derivative computations  aka adjoint  in reverse  This is equivalent to specifying a computation graph  but more explicit  and allows the user to use  control flow such as for loops and conditionals     This is similar to the approach taken by implementations of reverse mode automatic differentiation  see e g   http   arxiv org abs 1502 05767     Examples    A simple example of computing  linear x1    x2   sigmoid x3    but  randomly  replacing  sigmoid x3   with  x3  sometimes     lua lin   nn Linear 5 5  add   nn CAddTable   mul   nn CMulTable   sigm   nn Sigmoid     tape   autobw Tape                    START OF FORWARD PASS                   records the sequence of operations tape begin   coin flip   torch rand 1  1  val1   lin forward x1    if coin flip   0 5 then   maybe sigmoid   sigm forward x3  else   maybe sigmoid   x3 end   result   add forward val1  mul forward x2  maybe sigmoid   tape stop                  END OF FORWARD PASS                     Play it back in reverse  tape backward        Now  the gradients are in the four nn Module objects as usual        Note  I don t actually use the gradients at all here  and I don t set them to zero first  just to keep the example simple  See also  our nngraph practical  for the equivalent in  nngraph     RNN Example   See the  examples folder  for a  fully functional rnn example  with toy data    LSTM example   The LSTM example  https   github com oxford cs ml 2015 practical6  can easily be shortened by using this  We delete the backward pass  and simply play it back from the recorded forward pass     lua    setup autodiff tape   Tape      TODO  local      do fwd bwd and return loss  grad params function feval x      if x    params then         params copy x      end     grad params zero                        get minibatch                     local x  y   loader next batch                        forward pass                     tape begin                     local embeddings                    input embeddings local lstm c     0  initstate c     internal cell states of LSTM local lstm h     0  initstate h     output values of LSTM local predictions                   softmax outputs local loss   0  for t 1 opt seq length do     embeddings t    clones embed t  forward x      t            we re feeding the  correct  things in here  alternatively        we could sample from the previous timestep and embed that  but that s        more commonly done for LSTM encoder decoder models     lstm c t   lstm h t    unpack clones lstm t  forward embeddings t   lstm c t 1   lstm h t 1         predictions t    clones softmax t  forward lstm h t       loss   loss   clones criterion t  forward predictions t   y      t    end tape stop                                         backward pass                     tape backward                             misc                           transfer final state to initial state  BPTT  initstate c copy lstm c  lstm c   initstate h copy lstm h  lstm h       clip gradient element wise grad params clamp  5  5   return loss  grad params    end     gzfile   Conveniently  and with decent performance  read and write data from  gzip   files directly  Useful for text or very large files    Implemented as an FFI wrapper for zlib  including wrappers for C  FILE   functions like  fscanf  and  fwrite   More can be easily added    Install   luarocks install https   raw githubusercontent com bshillingford lua gzipfile master gzfile scm 1 rockspec   Example    Read 200 floats from a gzipped file directly into a torch tensor     lua require  torch  local GZFile   require  gzfile GZFile    local tensor   torch FloatTensor 200  local f   GZFile  floats gz    rb   f readbuf tensor data    200 4      sizeof float  4  read 200 4 bytes f close      now do stuff with the tensor     Note  compressing floats can be useful for neural nets  since similar values at similar orders of magnitude will often results in repeated byte patterns    To access the underlying  FILE   handle  use the  handle  property of  GZFile     Functions implemented      Constructor   file   GZFile filename  mode       Opens the file for reading or writing using the given mode      See zlib gzopen   documentation for supported modes     close    Closes the file  Later operations will fail     write str       Writes a lua string  Doesn t allocate memory  just casts using ffi      Calls fwrite and returns its ret val  i e  number of bytes written     read nbytes       Reads to a buffer then turns into a lua string      Allocates memory on each call  so slightly inefficient if you do many     reads     writebuf ptr  nbytes       Writes data from the given buffer to the file      Returns number of bytes written     readbuf ptr  nbytes       Reads into the given location in memory      Returns number of bytes read     flush      peek      tell        Returns the position in the file using ftell     seek offset  origin       Seek using fseek  relative to beginning of file      Note the argument order matches C s rather than Lua s io      Returns new position from ftell     getc      scanf fmt  typestring       Calls fscanf  but only for reading a single field e g    s       Allocates memory automatically  this memory will be gc d by lua      Typestring examples    float 1       uint8 t 1       char 16       Class visualizer for lua torch   Class hierarchy visualizer for torch classes  and classic classes  see github com deepmind classic    Currently generates a d3 visualization  easily adapted to graphviz    Example of viewing the classes in  nn   th generate lua htmld3tree nn   output html  Then open  output html  in your browser    Usage    sh th   lclassic  generate lua output mode package1  package2  package3         where  output mode  is  htmld3force  or  htmld3tree   See e g   htmld3tree lua   more can be added      Output is to stdout  redirect it to a file like  output html     Use   lclassic  to monitor classic classes instead   see  https   github com deepmind classic       Useful extensions TODOs      Output to graphviz   Parse libraries for documentation and include it in the visualization    At the least  actually do something useful when clicking on the classes      Internals   For  torch class  classes  it simply monkeypatches this function to monitor created classes when the packages get  require d  For classic classes  it uses a central  private  registry storing  the classes and their parents  nnquery   query large neural network graph structures in Torch   NN modules in Torch are often complex graph structures  like  nn Container s and its subclasses and  nn gModules    nngraph    arbitrarily nested  This makes it tedious to extract nn modules when debugging  monitoring training progress  or testing    nnquery  provides a facility to query these arbitrarily complex DAGs  XPath and CSS are designed to handle trees  whereas this library supports querying DAGs like neural nets  The API is loosely inspired by a mix of XPath  CSS queries  and  NET s LINQ    See below for a simple example  and a more complete example of extracting things from an LSTM    Installation   Install  nnquery    luarocks install https   raw githubusercontent com bshillingford nnquery master rocks nnquery scm 1 rockspec  Totem is optional  and used for unit tests    Usage   There are two important base classes that nearly everything is derived from      Element   full name   nnquery Element     ElementList     Every object you wish to query is wrapped in an  Element   and sequences collections of these are represented using  ElementList s    To wrap an object in an  Element  so you can query it     lua local nnq   require  nnquery  local seq   nn Sequential        add nn Tanh         add nn ReLU      local tanh   nnq seq  children   first       On the last line      nnq seq   wraps  seq  into an  Element      children    returns an  ElementList  of two  Elements  for  seq  s children     first    returns the first  Element  in the  ElementList       Realistic example with an LSTM    This is an example of using various functions in  Element  and  ElementList      lua require  nn  require  nngraph  local nnq   require  nnquery       nngraph implementation of LSTM timestep  from Oxford course s practical  6 function create lstm opt    local x   nn Identity       local prev c   nn Identity       local prev h   nn Identity       function new input sum          transforms input     local i2h              nn Linear opt rnn size  opt rnn size  x         transforms previous timestep s output     local h2h              nn Linear opt rnn size  opt rnn size  prev h      return nn CAddTable    i2h  h2h     end   local in gate            nn Sigmoid   new input sum      local forget gate        nn Sigmoid   new input sum      local out gate           nn Sigmoid   new input sum      local in transform       nn Tanh   new input sum      local next c             nn CAddTable           nn CMulTable    forget gate  prev c          nn CMulTable    in gate      in transform          local next h             nn CMulTable    out gate  nn Tanh   next c      nngraph annotateNodes     local mod   nn gModule  x  prev c  prev h    next c  next h     mod name    LSTM    return mod end      Example network local foo   nn Sequential        add nn Module         add create lstm rnn size 3        add nn ReLU         add nn ReLU         add nn Linear 3  4        Find the LSTM in a few different ways  local lstm   nnq foo       Wrap the module in an Element object using the default context                            which allows querying nn containers and nngraph s gmodules       descendants           Get all descendants below this node in the graph      where function e      Filter Elements by the given predicate       return e classIs nnq NNGraphGModuleElement      end       only                  Returns the first element in the returned sequence  and                            asserts that it is the only element in the sequence                              shortcut for list first   and assert list count      1   local lstm2   nnq foo       children              Returns the contained modules of the nn Sequential object as an                            ElementList      nth 2                 Grabs the 2nd child of the nn Sequential                             alternate shorthand syntax  nnq foo  children   2   local lstm3   nnq foo       descendants                  attr name  LSTM       Get only the objects with a name attribute set to  LSTM                              where it ll check both raw attributes and attempt to call                            the function assuming it s a getter method  i e  check                             module name       LSTM        only   assert lstm val      lstm2 val   and lstm2 val      lstm3 val         they should all return the same LSTM gmodule        Get the output nodes of the nngraph gmodule as an ElementList  local outputs   lstm outputs      Two ways to get the count for an ElementList  print  The LSTM gmodule has    outputs count      outputs  they are   outputs  print  The LSTM gmodule has     outputs    outputs  they are    outputs  assert outputs first   name       next c        name   is available on NNGraphNodeElements                                                 as a shortcut for  assert outputs first   val   data annotations name     next c         Let s find the forget gate  local forget gate   lstm descendants   attr name  forget gate   only   print forget gate     But it s the sigmoid  not the gate s pre activations  so let s get the sum  local input sum   forget gate parent      This is an alias for  parents   only                                              Note  nngraph nodes can have multiple parents  i e                                            inputs  assert torch isTypeOf input sum val   data module  nn CAddTable   assert torch isTypeOf input sum module    nn CAddTable      alias for  val   data module       Further details    Wrapping objects into elements and similar operations only make sense relative to a  context   an instance of  nnquery Context   which contains a list of  Element  types and conditions on which to instantiate depending on what type is provided to it  Additionally  the context caches  Element s  so that wrapping the same object twice returns the same instance of the  Element  subclass   nnquery init lua  contains the construction of a default context  accessible as  nnquery default   that contains all the implemented  Element  types  similarly to this   lua local ctx   nnq Context   ctx reg nnq NNGraphGModuleElement  nnq NNGraphGModuleElement isGmodule  ctx reg nnq NNGraphNodeElement  nnq NNGraphNodeElement isNode  ctx reg nnq ContainerElement  nnq ContainerElement isContainer     after since gModule IS A Container ctx default nnq ChildlessElement    Note that there is no true  root  node  unlike an XML HTML document  the root is simply the place where the query begins  Therefore  one cannot    search for the root s parents  even if the root module is contained in  for example  a container        Usually  Unless an element s parents are pre populated from a previous query    Documentation   Further documentation can be found in doc comment style before class definitions and method definitions in the code itself    TODO  extract these into markdown format and put links here   Developing   Extending   You may have your own  nn  modules that are not handled by the existing handlers  In this case  you can implement your own  Element  object  see the existing ones for examples   and create your own context that adds a handler for this  Element   See the default context  see above  for details    Contributing   Bug reports are appreciated  preferably with a pull request for a test that breaks existing code and a patch that fixes it  If you do  please adhere to the  informal  code style in the existing code where appropriate  Torch serialization reader for Python       Mostly direct port of the torch7 Lua and C serialization implementation to  Python  depending only on  numpy   and the standard library   array   and  struct    Sharing of objects including  torch Tensor s is preserved    python import torchfile stuff   torchfile load  a bunch of stuff t7     Installation    Install from  PyPI    sh pip install torchfile  or clone this repository  then   sh python setup py install   Supports Python 2 7  3 4  3 5  3 6  Probably others too    More examples    Write from torch  read from Python    Lua   lua  th  torch save   tmp test t7    hello 123  world torch rand 1 2 3     Python   python In  3   o   torchfile load   tmp test t7   In  4   print o  world   shape  1  2  3  In  5   o Out 5      hello   123   world   array     0 52291083   0 29261517   0 11113465              0 01017287   0 21466237   0 26572137        Arbitary torch classes supported       python In  1   import torchfile   In  2   o   torchfile load  testfiles x86 64 gmodule with linear identity t7     In  3   o forwardnodes 3  data module Out 3   TorchObject nn Identity    output   array     dtype float64    gradInput   array     dtype float64      In  4   for node in o forwardnodes  print repr node data module                                                                                                              None None None TorchObject nn Identity    output   array     dtype float64    gradInput   array     dtype float64    None TorchObject nn Identity    output   array     dtype float64    gradInput   array     dtype float64    TorchObject nn Linear    weight   array    0 0248373             0 17503954      gradInput   array     dtype float64    gradWeight   array     1 22317168e 312             1 22317168e 312      bias   array   0 05159848   0 25367146     gradBias   array    1 22317168e 312    1 22317168e 312     output   array     dtype float64    TorchObject nn CAddTable    output   array     dtype float64    gradInput        None   In  5   o forwardnodes 6  data module weight Out 5    array    0 0248373             0 17503954      In  6   o forwardnodes 6  data module bias Out 6   array   0 05159848   0 25367146         More complex writing from torch    Lua   lua  th  f   torch DiskFile   tmp test t7    w   binary    th  f writeBool false   th  f writeObject  hello 123    th  f writeInt 456   th  f close    Python   python In  1   import torchfile In  2   with open   tmp test t7   rb   as f              r   torchfile T7Reader f              print r read boolean                print r read obj                print r read int             False   hello   123  456   Supported types      nil  to Python  None   numbers to Python floats  or by default a heuristic changes them to ints or    longs if they are integral   booleans   strings  read as byte strings  Python 3  or normal strings  Python 2   like    lua strings which don t support unicode  and that can contain null chars   tables converted to a special dict      if they are list like  i e  have    numeric keys from 1 through n  they become a python list by default   Torch classes  supports Tensors and Storages  and most classes such as     modules  Trivially extensible much like the Torch serialization code     Trivial torch classes like most  nn Module  subclasses become      TorchObject s  The  torch readers  dict contains the mapping from class    names to reading functions    functions  loaded into the  LuaFunction   namedtuple      which simply wraps the raw serialized data  i e  upvalues and code     These are mostly useless  but exist so you can deserialize anything    tds Hash  tds Vec         Since Lua allows you to index a table with a table but Python does not  we      replace dicts with a subclass that is hashable  and change its     equality comparison behaviour to compare by reference      See  hashable uniq dict     Test files demonstrating various features       python In  1   import torchfile   In  2   torchfile load  testfiles x86 64 list table t7   Out 2     hello    world    third item   123    In  3   torchfile load  testfiles x86 64 doubletensor t7   Out 3    array    1     2     3              4     5     6 9         also other files demonstrating various types          The example  t7  files will work on any modern Intel or AMD 64 bit CPU  but the code will use the native byte ordering etc  Currently  the implementation  assumes the system dependent binary Torch format  but minor refactoring can  give support for the ascii format as well  Linux Spotify ad muter   Listens to track changes with dbus  Assumes pulseaudio  and mutes master during ads  i e  not just spotify s stream   1      License  BSD  Based on  https   muffinresearch co uk linux spotify track notifier with added d bus love       1  TODO  only mute spotify stream  read pavucontrol source code to figure out how  Currently mutes pulseaudio using  amixer  D pulse sset Master  off on    Fork of  fb debugger   dependency free   fb debugger  is an excellent debugger for torch and lua in general  and can be found at  https   github com facebook fblualib   However   fblualib  and its dependencies are quite heavy but you may just want the debugger  This repository is a fork of  fb debugger  with dependencies integrated and or removed     To run a script and drop into the debugger on an error  simply do   sh fbdbg run script name lua your arg1 your arg2       Install    luarocks install https   raw githubusercontent com bshillingford fbdebugger minimal master fbdebugger standalone 1 rockspec   Dependencies      penlight    1 3 1   libedit  installed in your system   libedit so  anywhere in the library search path         Original README    fb debugger  A source level Lua debugger   This package implements a source level Lua debugger    Usage   You may enter the debugger in two different ways    explicitly  at the point of interest  do  lua local debugger   require  fb debugger   debugger enter      and you will be dropped in the debugger   automatically when you hit an  uncaught  error  if using    fb trepl   you may set the environment variable    LUA DEBUG ON ERROR  to  1   and you ll be dropped in the debugger   whenever your code raises an uncaught error    Debugger commands   help  will give you a list of commands  inspired by  gdb   The following commands exist and behave similarly to their gdb counterparts     help  displays help    where     backtrace     bt  displays the current stack trace  with a   marker for the currently selected frame     frame  selects a given frame    up     down  moves the currently selected frame up   down one    b     break  sets a breakpoint at a given location  specified either as     file   line number   or   function name    the function name is looked up   in the scope of the current frame     info breakpoints  lists breakpoints    enable    disable    delete  enable  disable  and delete a breakpoint    respectively    next     n  single steps one line  skipping over function calls    step     s  single steps one line  descending into function calls    finish  continues execution until the function in the currently selected   frame returns    continue     c  continues program execution until the next breakpoint    or until the next time the debugger is reentered  via  debugger enter    or   automatically in case of error     locals     vlocals  shows locals in scope in the current frame   vlocals    also shows values  verbose     globals     vglobals  shows all globals    upvalues     vupvalues  shows the current function s upvalues    exec     e  executes code in the scope of the current frame    print     p  evaluates an expression in the scope of the current frame and   prints the result    list     l  lists source code  if available   by default it lists the   function in the current frame  but it accepts a location argument just like    break   just like gdb  repeating  l  without arguments continues listing   the same file    quit     q  quits the debugger  the program is resumed    Note that  locals    globals   or  upvalues  will occasionally show a synthetic name for a variable  such as   dbgl tmp 4    These indicate variables that have been shadowed in the current scope  and so their original name now refers to something else  or internal Lua temporaries  modifying those is ill advised   LipNet  End to End Sentence level Lipreading    Yannis M  Assael  Brendan Shillingford  Shimon Whiteson  Nando de Freitas   Links      arXiv pre print   Bibtex    article assael2016lipnet    title  LipNet  End to End Sentence level Lipreading     author  Assael  Yannis M and Shillingford  Brendan and Whiteson  Shimon and de Freitas  Nando     journal  GPU Technology Conference     year  2017       License   Code licensed under the Apache License v2 0  num2word   Line for line Lua port of  http   stackoverflow com questions 25150316 convert numbers to english strings   Python version fixed a  zero thousand  bug  Reasonably high quality for numbers smaller than a billion  but a few odd spellings left uncorrected e g  18    Test of equivalent implementation   bash luajit test num2word lua   lua txt python test num2word py   py txt diff lua txt py txt    echo  they are different  Wrapper for CUDA profiler start stop API functions  Zero dependencies    Example     python import cudaprofile   cudaprofile start         do expensive cuda stuff       cudaprofile stop       and run the script from nvprof or nvvp     You may want to use  nvprof  with    profile from start off  and only call  start    when desired  wifi locate  Python    Locates the Wi Fi enabled machine using nearby Wi Fi access points  relative signal strengths  Uses Google s API    To use  call  linux scan  or  osx scan   then give the result to  locate  which returns   accuracy   lat lng       Pretty useful for  xflux  or fetching weather    Quick start    Install   bash pip install git https   github com bshillingford wifi locate   Example   python from wifilocate import locate  linux scan accuracy  latlng   locate linux scan device  wlan0   iwlist path   sbin iwlist    print accuracy  latlng     e g  25   50 1234567   1 234567    Details   Calls Google s API  most likely used in Firefox  based on the URL   The module supports Python 2 and 3  and only depends on  requests   If you don t yet have requests  consider my dependency on it a favour  It s great    In Linux this uses  iwlist   and in OS X it uses a little known but built in utility called  airport   This is a fork of the excellent extension Better Google Tasks      https   chrome google com webstore detail better google tasks denjcdefjebbmlihdoojnebochnkgcin hl en GB   http   richwells me blog better google tasks      I added functionality for handling URLs to a specific task list  like this   https   mail google com tasks canvas List Name Here  and changing the default task list to open via the extension s options dialog  sharearray   Have you worried about creating large identical numpy arrays across processes due to RAM wastage  e g  datasets that are big enough to fit in RAM but large enough to cause concern when running multiple jobs using the same data   sharearray  efficiently caches numpy arrays in RAM  using shared memory in   dev shm   no root needed  locally on a machine    Usage is simple  using the  cache  function or  decorator  decorator  A first call saves the result of the call into the built in RAM disk  and returns a read only memory mapped view into it  Since it s in RAM  there s no performance penalty  Any subsequent calls with the same ID will return an identical read only memory mapped view  even across processes  The IDs are  global     Installation   pip install git https   github com bshillingford python sharearray  or  git clone https   github com bshillingford python sharearray python setup py install   Usage   Using  decorator        python  sharearray decorator  some unique id   verbose False  def get training data          create largeish   expensive to generate data     return my array   some instance of np ndarray   first call  across all processes  creates the array   arr view   get training data     all further calls are cached memoized  we return a view into memory   arr view 2   get training data         Using the  cache  function       python import sharearray import numpy as np arr   sharearray cache  my global id   lambda  create large array      or    arr   sharearray cache  my global id   lambda  create large array        where  for instance  create large array  returns a large training set  potentially performing expensive feature transformations or data augmentations first    By default  the file is at   dev shm sharearray my global id npy   and to avoid concurrency issues when first generating the array  and to avoid duplicated computation     For futher details  read the docstrings  You may be interested in the  timeout    verbose   and  log func  arguments  to either  cache  or  decorator      PyTorch   Since PyTorch does not yet support memmapped files  at time of writing   we can instead just create torch Tensors that point to the memory mapped by numpy   python data numpy   get training data              numpy ndarray data torch   torch from numpy data numpy    torch Tensor   Notes   TODO  support returning multiple arrays  e g  as a tuple or dict  from the callback   decorated function   There exist similar libraries in Python already  but this just makes it easier to do as a memoization style API  Also  this module is a single file  and does not write anything in C
4,clementfarabet,sys   Has moved to a more community friendly  repo   xlua   Has moved to a more community friendly  repo   image   Has moved to a more community friendly  repo   nnx  experimental  nn  components   The original neural network from Torch7   nn   contains stable and widely used modules   nnx  contains more experimental  unproven modules  and optimizations  Modules that become stable and which are proven useful make  their way into  nn   some already have     Library Documentation   This section includes documentation for the following objects      SoftMaxTree    a hierarchical log softmax Module    TreeNLLCriterion    a negative log likelihood Criterion for the SoftMaxTree    CTCCriterion    a Connectionist Temporal Classification Criterion based on  warp ctc     PushTable  and PullTable     extracts a table element and inserts it later in the network    MultiSoftMax    performs a softmax over the last dimension of a 2D or 3D input    SpatialReSampling    performs bilinear resampling of a 3D or 4D input image     QDRiemaNNLinear    nnx QDRiemaNNLinear    quasi diagonal reduction for Riemannian gradient descent   Recurrent    a generalized recurrent neural network container        SoftMaxTree   A hierarchy of parameterized log softmaxes  Used for computing the likelihood of a leaf class   This Module should be used in conjunction with the  TreeNLLCriterion    Using this for large vocabularies  100 000 and more  greatly accelerates training and evaluation  of neural network language models  NNLM    A vocabulary hierarchy is provided via the  dp  package s  BillionWords   DataSource     The constructor takes 2 mandatory and 4 optional arguments        inputSize    the number of units in the input embedding representation      hierarchy    a Tensor mapping one  parent id  to many  child id   a tree       rootId    a number identifying the root node in the hierarchy  Defaults to   1       accUpdate    when the intent is to use  backwardUpdate  or  accUpdateGradParameters   set this to true to save memory  Defaults to false      static    when true  the defualt   returns parameters with keys that don t change from batch to batch      verbose    prints some additional information concerning the hierarchy during construction    The  forward  method returns an  output  Tensor of size 1D  while   backward  returns a table   gradInput  gradTarget    The second  variable is just a Tensor of zeros   such that the  targets  can be  propagated through  Containers   like  ParallelTable        lua     input   torch randn 5 10  target   torch IntTensor 20 24 27 10 12  gradOutput   torch randn 5  root id   29 input size   10   hierarchy          29  torch IntTensor 30 1 2    1  torch IntTensor 3 4 5        2  torch IntTensor 6 7 8    3  torch IntTensor 9 10 11       4  torch IntTensor 12 13 14    5  torch IntTensor 15 16 17       6  torch IntTensor 18 19 20    7  torch IntTensor 21 22 23       8  torch IntTensor 24 25 26 27 28    smt   nn SoftMaxTree input size  hierarchy  root id  smt forward input  target   3 5186  3 8950  3 7433  3 3071  3 0522  torch DoubleTensor of dimension 5  smt backward  input  target   gradOutput      1   DoubleTensor   size  5x10   2   IntTensor   size  5                 TreeNLLCriterion   Measures the Negative log likelihood  NLL  for  SoftMaxTrees    Used for maximizing the likelihood of SoftMaxTree outputs  The SoftMaxTree Module outputs a column Tensor representing the log likelihood of each target in the batch  Thus SoftMaxTree requires the targets  So this Criterion only computes the negative of those outputs  as  well as its corresponding gradients        PushTable  and PullTable    PushTable and PullTable work together  The first can be put earlier in a digraph of Modules such that it can communicate with a  PullTable located later in the graph   PushTable forward input    for an  input  table of Tensors to the output  excluding one  the index of which  is specified by the  index  argument in the  PushTable index   constructor  The Tensor identified by this  index  is communicated to one or many  PullTables created via the  PushTable pull index   factory method   These can be inserted later in the digraph such that  a call to  PushTable forward input    where  input  is a table or a Tensor   will output a table with the previously  pushed  Tensor inserted  at index  index     An example utilizing the above  SoftMaxTree  Module and a Linear Module demonstrates how the PushTable can be used to  forward the  target  Tensor without any other   Table Modules      lua     mlp   nn Sequential   linear   nn Linear 50 100  push   nn PushTable 2  pull   push pull 2  mlp add push  mlp add nn SelectTable 1   mlp add linear  mlp add pull  mlp add smt    smt is a SoftMaxTree instance mlp forward input  target     input and target are defined above  3 5186  3 8950  3 7433  3 3071  3 0522  torch DoubleTensor of dimension 5  mlp backward  input  target   gradOutput     so is gradOutput     1   DoubleTensor   size  5x10   2   IntTensor   size  5    The above code is equivalent to the following  lua mlp2   nn Sequential   para   nn ParallelTable   para add linear  para add nn Identity    mlp2 add para  mlp2 add smt  mlp2 forward input  target   3 5186  3 8950  3 7433  3 3071  3 0522  torch DoubleTensor of dimension 5  mlp2 backward  input  target   gradOutput      1   DoubleTensor   size  5x10   2   IntTensor   size  5       In some cases  this can simplify the digraph of Modules  Note that  a PushTable can be associated to many PullTables  but each PullTable  is associated to only one PushTable        CTCCriterion   criterion   nn CTCCriterion    Creates a Criterion based on Baidus   warp ctc  implementation  This Module measures the loss between a 3D output of  batch x time x inputdim  and a target without needing alignment of inputs and labels  Must have installed warp ctc which can be installed via luarocks   luarocks install http   raw githubusercontent com baidu research warp ctc master torch binding rocks warp ctc scm 1 rockspec  Supports cuda via   criterion   nn CTCCriterion   cuda    Example      output   torch Tensor    1 2 3 4 5   6 7 8 9 10        Tensor of size 1x1x5  batch x time x inputdim   label     1 3   sizes   torch Tensor  2      Size of each sequence  sequence length  in the batch as a tensor ctcCriterion   nn CTCCriterion     err   ctcCriterion forward output label sizes  gradOut   ctcCriterion backward output label  print      CPU       print  Error        err  print  Gradients     print gradOut    ctcCriterion   ctcCriterion cuda      Switch to cuda implementation  output   output cuda     err   ctcCriterion forward output label sizes  gradOut   ctcCriterion backward output label  print      GPU       print  Error        err  print  Gradients     print gradOut        gives the output          CPU      Error   4 9038286209106  Gradients     1           0 0117  0 9683  0 0861  0 2341  0 6364   0 0117  0 0317  0 0861  0 7659  0 6364  torch FloatTensor of size 1x2x5        GPU      Error   4 9038290977478  Gradients     1           0 0117  0 9683  0 0861  0 2341  0 6364   0 0117  0 0317  0 0861  0 7659  0 6364  torch CudaTensor of size 1x2x5         MultiSoftMax   This Module takes 2D or 3D input and performs a softmax over the last dimension   It uses the existing  SoftMax   CUDA C code to do so such that the Module can be used on both GPU and CPU   This can be useful for  keypoint detection       SpatialReSampling   Applies a 2D re sampling over an input image composed of several input planes  or channels  colors   The input tensor in  forward input   is  expected to be a 3D or 4D tensor of size     batchSize x  nInputPlane x width x height    The number of output planes will be the same as the number of input planes    The re sampling is done using  bilinear interpolation    For a simple nearest neihbor upsampling  use  nn SpatialUpSampling     and for a simple average based down sampling  use   nn SpatialDownSampling       If the input image is a 3D tensor of size  nInputPlane x height x width   the output image size will be  nInputPlane x oheight x owidth  where  owidth  and  oheight  are given to the constructor    Instead of  owidth  and  oheight   one can provide  rwidth  and  rheight    such that  owidth   iwidth rwidth  and  oheight   iheight rheight     As an example  we can run the following code on the famous Lenna image   lua require  image                                                             require  nnx  input   image loadPNG  doc image Lenna png   l   nn SpatialReSampling owidth 150 oheight 150  output   l forward input  image save  doc image Lenna 150x150 bilinear png   output    The input        The re sampled output          QDRiemaNNLinear   The Quasi Diagonal Riemannian Neural Network Linear  QDRiemaNNLinear  module is an implementation of the quasi diagonal reduction of metrics  used for Riemannian gradient descent  The algorithm is defined in Riemannian metrics for neural networks I  feedforward networks by Yann Ollivier  http   arxiv org abs 1303 0818  and an efficient implementation is described in Practical Riemannian Neural Networks by Yann Ollivier and Gaetan Marceau Caron  http   arxiv org abs 1602 08007   To use this module  simply replace  nn Linear ninput noutput   with  nnx QDRiemaNNLinear ninput noutput    As always  the step size must be chosen accordingly  Two additional arguments are also possible    gamma  default 0 01   determine the update rate of the metric for a minibatch setting  i e    1 gamma    oldMetric   gamma newMetric  Smaller minibatches require a smaller gamma  A default value depending on the size of the minibatches is  gamma   1    torch pow 1  1  nTraining miniBatchSize   where  nTraining  is the number of training examples of the dataset and  miniBatchSize  is the number of training examples per minibatch     qdFlag  default true   Whether to use the quasi diagonal reduction  true  or only the diagonal  false   The former should be better    This module is a straightforward implementation of the outer product gradient descent    Requirements     Torch7  www torch ch      Installation     Install Torch7  refer to its own documentation     clone this project into dev directory of Torch7    Rebuild torch  it will include new projects too      Use the library   First run torch  and load nnx    sh   torch         lua     require  nnx          Once loaded  tab completion will help you navigate through the library  note that most function are added directly to nn         lua     nnx    TAB     nn    TAB         In particular  it s good to verify that all modules provided pass their tests        lua     nnx test all   nnx test omp             Recurrent   DEPRECATED July 6th  2015  Use  rnn  instead  This repo is a container for all my Torch7 packages    Note  all these packages used to be distributed into a big messy repo  called XLearn    Retrieve all packages   This repo is empty  and only contains references to other GIT repos  You can retrieve all of them like this    sh   git submodule init   git submodule update   Install   1  Torch7 and dependencies    On Linux  Ubuntu   9 04     sh   apt get install gcc g   git libreadline5 dev cmake wget libqt4 core libqt4 gui libqt4 dev   On Mac OS  Leopard  or more   using  Homebrew     sh   brew install git readline cmake wget qt   Then on both platforms    sh   git clone https   github com andresy torch   cd torch   mkdir build  cd build   cmake      make    sudo  make install   2  Packages    Once Torch7 is installed  it comes with a package manager that you can use to either install packages from the web    sh   torch pkg install pkg name   torch pkg   help   or build them locally  if you are planning to work on the  sources    sh   cd pkg name   torch pkg deploy   Use Torch7   First run torch  and load a package    sh   torch         lua     require  imgraph          Once loaded  tab completion will help you navigate through the library  note  tab completion will only work if you have Qt4 and readline         lua     imgraph    TAB imgraph colorize            imgraph connectcomponents   imgraph graph               imgraph histpooling         imgraph segmentmst          imgraph testme              imgraph watershed           imgraph gradient          Most packages then provide a testme   function to quickly see what it does        lua     imgraph testme           Checkout the demos   tutorials   sh   cd demos     this repo contains demos  and tutorials to get started  Looking at the code is the best way to get there    Developers   If you would like to develop one of the submodules you should check out the master branch of that module     sh   cd nnx   git checkout master   git pull   This puts you at the head of development for that submodule  and in the proper branch to commit any changes you make to the git repository for that module   To check out all the submodules in developer mode we have added the script     sh     gitall sh    a simple command to repeat a git command to all subdirectories   syntax      gitall sh    eg      switch all submodules to the master branch     gitall sh checkout master   pull updates for all submodules     gitall sh pull   other useful     gitall sh status     gitall sh diff     WARNING  will blindly send command s  to git in each directory imgraph  a package to create manipulate graphs on images   This package provides standard functions to create and manipulate edge weighted graphs  of images  create a graph  segment it   compute its watershed  or its connected components      Install   1  Torch7 is required    Dependencies  on Linux  Ubuntu   9 04     sh   apt get install gcc g   git libreadline5 dev cmake wget libqt4 core libqt4 gui libqt4 dev libboost all dev   Dependencies  on Mac OS  Leopard  or more   using  Homebrew     sh   brew install git readline cmake wget qt   Then on both platforms    sh   git clone https   github com andresy torch   cd torch   mkdir build  cd build   cmake      make    sudo  make install   2  Once Torch7 is available  install this package    sh    sudo  torch rocks install imgraph   Use the library   First run torch  and load imgraph    sh   torch         lua     require  imgraph          Once loaded  tab completion will help you navigate through the library        lua     imgraph    TAB imgraph colorize            imgraph connectcomponents   imgraph graph               imgraph histpooling         imgraph segmentmst          imgraph testme              imgraph watershed           imgraph gradient          To get quickly started  run the testme   function        lua     imgraph testme           which computes a few things on the famous image of Lena    neuFlow   neuFlow  is dataflow architecture optimized for large array tensor transforms  and especially image processing operations   More info about the architecture  hardware and applications can be found  here     this package   This package is a compiler toolkit for neuFlow  It is entirely written in  Lua   and relies on  Torch7  to represent N dimensional arrays efficiently  It also interfaces Torch7 s neural network package natively    how to install   Torch7 must be install first  a task most easily accomplished using the single line  install script     or alternatively to install Torch7 and the neuFlow package by hand  you will need to install a few dependencies    On Linux  Ubuntu     sh   apt get install gcc g   git libreadline5 dev cmake wget   apt get install libqt4 core libqt4 gui libqt4 dev   apt get install ffmpeg gnuplot   On Mac OS X    10 5   get  Homebrew  and then    sh   brew install git readline cmake wget   brew install qt   brew install ffmpeg gnuplot   You re ready to install Torch7  www torch ch   The most up to date instructions can be found at the  Torch7 github page         sh   git clone git   github com andresy torch git   cd torch   mkdir build   cd build     cmake    OR   cmake     DCMAKE INSTALL PREFIX  my install path       Or if you already have a previous Torch7 installed    sh   luarocks install torch WITH LUA JIT 1   Torch7  an efficient numeric library for Lua   You will also need additional packages    sh   luarocks install image          an image library for Torch7   luarocks install nnx            lots of extra neural net modules   luarocks install camera         a camera interface for Linux MacOS   luarocks install ffmpeg         a video decoder for most formats   luarocks install inline c       inline C capability   Now that Torch7 has been installed the neuflow package can be installed  Installing the neuflow package requires you to download the source code repository  It ll give you access to some demos  to get started    sh   git clone https   github com clementfarabet neuflow git   cd neuflow   luarocks make   how to run code on neuFlow   Demos are located in demos   To get started  you ll need a standard Xilinx dev board for the Virtex 6   the ML605 Kit   http   www xilinx com products devkits EK V6 ML605 G htm   We provide an image of neuFlow that s pre synthesized mapped routed for the Virtex6 VLX240T on this platform    To run any of the demos  follow these instructions  tested on Ubuntu 9 04  10 04 and Mac OS X 10 5  10 6 and 10 7         sh   git clone https   github com clementfarabet neuflow git   cd neuflow   make Xilinx tools available  it implies you have them   installed somewhere         source  XILINX INSTALL PATH settings   sh   turn on the ML605  plug the JTAG cable then load one of   our pre built bitfiles        cd scripts     get latest neuflow image     load bitfile neuFlow ml605 bit   at this points  you just have wait 2 seconds that the Ethernet   LEDs are back on  out of reset    run the simplest demo  a loopback client  to verify your setup         cd    demos   sudo torch loopback lua   on Linux or     loopback lua   on OSX   before loading a new demo  you have to reset neuFlow  for   now it is done by pressing the SW10 button  cpu rst    then you can run a typical convnet based program  a face detector      sudo torch face detector lua   on Linux or     face detector lua   on OSX           the load bitfile script assumes that you have properly installed Xilinx s USB cable driver  On RedHat and derivatives it works out of the box when installing Xilinx ISE  but on Ubuntu you ll have to follow these instructions  http   rmdir de  michael xilinx    This is not doable on Mac OS X unfortunately  I usually flash the ML605 board using Ubuntu  even a virtual box version works   and then run all the demos under Mac OS X         you need to have admin privileges on your machine  sudo  to be able to interact with neuFlow  as we re using a custom low level Ethernet framing protocol  UNSUP   A package for unsupervised learning in Torch    Provides modules that are compatible with  nn    LinearPsd    ConvPsd    AutoEncoder         and self contained algorithms   k means    PCA      Requirements   Basic dependencies      Torch7  github com andresy torch    kex     github com koraykv tools    optim   github cim koraykv optim      To run the demo scripts  you also need the following      image  github com clementfarabet lua   image    sys    github com clementfarabet lua   sys    xlua   github com clementfarabet lua   xlua      Installation   Build Install      Install Torch7  refer to its own documentation     clone all other repos  including this one  into dev directory of Torch7    Rebuild torch  it will include all these projects too      Alternatively  you can use torch s package manager  Once Torch is installed  you can install  unsup      torch pkg install unsup   para  el  a  simple  parallel computing framework for Torch   This package provides a simple mechanism to dispatch and run Torch Lua code as independant processes and communicate via ZeroMQ sockets  Processes can be forked locally or on remote machines    Install   Install ZeroMQ 3     bash sudo apt get install libzmq3 dev libzmq3   Install Torch7 per instructions at http   torch ch      Download and compile this package using luarocks    bash  sudo  luarocks install parallel   or    bash git clone https   github com clementfarabet lua   parallel git cd lua   parallel luarocks make   Use the library   API  in very short    Load start up package    lua require  parallel    Fork a new process  or N new processes  locally    lua parallel fork   parallel nfork 4    Fork remote processes  In that following code  we fork 4 processes on myserver org  and 6 processes on myserver2 org    lua parallel nfork   4  ip  myserver org   protocol  ssh   lua   path to remote torch                     6  ip  myserver2 org   protocol  ssh   lua   path to remote torch       Even more flexible  a list of machines can be established first  so that  a call to sfork    smart fork  can automatically distribute the forked processes onto the available machines        lua parallel addremote   ip  server1 org   cores 8  lua   path to torch   protocol  ssh  Y                         ip  server2 org   cores 16  lua   path to torch   protocol  ssh  Y                         ip  server3 org   cores 4  lua   path to torch   protocol  ssh  Y     parallel sfork 16       in this example  the 16 processes will be distributed over the 3 machines     server1 org  6 processes    server2 org  6 processes    server3 org  4 processes       In the spirit of  really  abstracting where the jobs are executed  calibrate   can be called to estimate the compute power of each machine  so that you can distribute your load accordingly    lua parallel addremote      parallel calibrate   forked   parallel sfork parallel remotes cores      fork as many processes as cores available for   forked in ipairs forked  do    print  id       forked id       speed        forked speed  end    the speed of each process is a number  0  1   A coef of 1 means that it is the    fastest process available  and 0 5 for example would mean that the process is 2x    slower   Once processes have been forked  they all exist in a table  parallel children  and all methods  exec send receive join  work either on individual processes  or on groups of processes    The first thing to do is to load these new processes with code  The code given can either be a function  with no arguments  it won t have any env when executing in the new process   or a string  Whether it is a string or a function  both get serialized into strings  and reloaded on the process side  using loadstring          lua    define process  code  code   function         arbitrary code contained here    require  torch     t   torch Tensor 10     print t       any process can access its id  its parent s id  and children s id     print parallel id     print parallel parent id     if parallel children 1  then print parallel children 1  id  end      if arguments were passed  they re found in the regular     table           args               print args 1   end      execute code in given process es   with optional arguments  parallel children exec code       this is equivalent to  for   child in ipairs parallel child  do     child exec code  end       parallel implements a simple yield join mechanism to allow a parent to sync and affect the behavior of its children        lua    child code  code   function      while true do       print  something         parallel yield      end end c   parallel fork   c exec code       parent code for i   1 10 do     c join   end      each time join   is called  it waits for the child to yield  and vice versa     in that example   something  only gets printed when the parent joins its child       Slightly more complex things can be implemented with yield join  join   can take a string as an argument  which is returned by the corresponding yield    This is useful to control branching in your children        lua    child code  code   function      while true do       print  something         m   parallel yield         if m     break  then break end    end end c   parallel fork   c exec code       parent code c join  break         Sometimes you might want to wait for a process to actually terminate  die   so that you can start new ones  The proper way to do this is to use the sync   function   which waits for the PID of that process to fully disappear from the OS  It also clears the child from the parallel children list  and decrement parallel nchildren    lua code   function           do nothing and die end parallel nfork 1                  fork one process parallel children exec code       execute dummy code print parallel nchildren          prints  1 parallel children sync            wait for all children  here only 1  to die print parallel nchildren          prints  0 parallel nfork 2                  fork 2 processes print parallel nchildren          prints  2 print parallel children 1         prints  nil print parallel children 2         prints  table     current running processes always print parallel children 3         prints  table     exist in children process id    When creating a child  parallel fork   a connection is established to transfer data between the two processes  Two functions send   and receive   can be used to  efficiently  transfer data between these processes  Any Lua type   and all Torch7 type  tensor  storage       can be transferred this way  The transmission is efficient for numeric data  as serialization merely involves a binary copy and some extra headers for book keeping  see serialization in Torch7 s manual         lua    define some code for children somecode   function      while true do          in an infinite loop  receive objects from parent        local obj   parallel parent receive            print       parallel print  received object    obj     end end      dispatch two processes  parallel nfork 2  parallel children exec somecode       and send them some data  t     a table   entry2  with arbitrary entries   tensor torch Tensor 100 100   while true do     parallel children 1  send t            send the whole table to child 1     parallel children 2  send t entry2     just send an entry to child 2 end       A convenient print function that prepends the process ID issuing the print        lua     parallel print  something         something       Last  but not least  always run your parent code in a protected call  to catch potential errors  Ctrl C  and the likes  and terminate nicely  By terminating nicely  I mean  killing all remote processes that you forked    If you don t do so  you leave you remote machines  and potentially yours  with hanging  processes that are just waiting to receive data  and will not hesitate to get back in business the next time you run your parent code           lua worker   function             some worker code end   parent   function             some parent code end   ok err   pcall parent  if not ok then    print err     parallel close        this is the key call  doing this will insure leaving a clean                          state  whatever the error was  ctrl c  internal error       end       A simple complete example        lua    required libs require  parallel       define code for workers  function worker         a worker starts with a blank stack  we need to reload       our libraries    require  sys     require  torch       print from worker     parallel print  Im a worker  my ID is       parallel id      and my IP       parallel ip       define a storage to receive data from top process    while true do          yield   allow parent to terminate me       m   parallel yield         if m     break  then break end        receive data   local t   parallel parent receive     parallel print  received object with norm     t data norm          send some data back   parallel parent send  this is my response      end end      define code for parent  function parent         print from top process    parallel print  Im the parent  my ID is       parallel id       fork N processes    parallel nfork 4       exec worker code in each process    parallel children exec worker       create a complex object to send to workers    t    name  my variable   data torch randn 100 100        transmit object to each worker    parallel print  transmitting object with norm     t data norm       for i   1 1000 do       parallel children join         parallel children send t        replies   parallel children receive      end    parallel print  transmitted data to all children        sync terminate when all workers are done    parallel children join  break      parallel print  all processes terminated   end      protected execution  ok err   pcall parent  if not ok then print err  parallel close   end       License   Copyright  c  2011 Clement Farabet  Marco Scoffier   Permission is hereby granted  free of charge  to any person obtaining a copy of this software and associated documentation files  the  Software    to deal in the Software without restriction  including without limitation the rights to use  copy  modify  merge  publish  distribute  sublicense  and or sell copies of the Software  and to permit persons to whom the Software is furnished to do so  subject to the following conditions    The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software    THE SOFTWARE IS PROVIDED  AS IS   WITHOUT WARRANTY OF ANY KIND  EXPRESS OR IMPLIED  INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT  IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM  DAMAGES OR OTHER LIABILITY  WHETHER IN AN ACTION OF CONTRACT  TORT OR OTHERWISE  ARISING FROM  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE  LOpenGM  Lua bindings for OpenGM   OpenGM  is a C   library for graphical  modeling  and inference  The Lua bindings provide a simple way of describing graphs  from Lua  and then optimizing them with OpenGM    Note  this package is superseded by  gm   a more general and simple  pure Lua  package for graphical models    License   LOpenGM Copyright  c  2011 Clement Farabet  Lua Bindings    OpenGM  Copyright  c  2010 by Bjoern Andres and Joerg Hendrik Kappes    This software was developed by Bjoern Andres and Joerg Hendrik Kappes  Enquiries shall be directed to    bjoern andres iwr uni heidelberg de  kappes math uni heidelberg de   All advertising materials mentioning features or use of this software must display the following acknowledgement    This product includes the OpenGM library developed by Bjoern Andres and Joerg Hendrik Kappes  Please direct enquiries concerning OpenGM to bjoern andres iwr uni heidelberg de  kappes math uni heidelberg de      Redistribution and use in source and binary forms  with or without modification  are permitted provided that the following conditions are met      Redistributions of source code must retain the above copyright notice    this list of conditions and the following disclaimer    Redistributions in binary form must reproduce the above copyright notice    this list of conditions and the following disclaimer in the documentation   and or other materials provided with the distribution    All advertising materials mentioning features or use of this software must   display the following acknowledgement    This product includes the OpenGM   library developed by Bjoern Andres and Joerg Hendrik Kappes  Please direct   enquiries concerning OpenGM to bjoern andres iwr uni heidelberg de    kappes math uni heidelberg de      The names of the authors must not be used to endorse or promote products   derived from this software without specific prior written permission      THIS SOFTWARE IS PROVIDED BY THE AUTHORS   AS IS   AND ANY EXPRESS OR IMPLIED WARRANTIES  INCLUDING  BUT NOT LIMITED TO  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED  IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT  INDIRECT  INCIDENTAL  SPECIAL  EXEMPLARY  OR CONSEQUENTIAL DAMAGES  INCLUDING  BUT NOT LIMITED TO  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES  LOSS OF USE  DATA  OR PROFITS  OR BUSINESS INTERRUPTION  HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY  WHETHER IN CONTRACT  STRICT LIABILITY  OR TORT  INCLUDING NEGLIGENCE OR OTHERWISE  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE  EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE    Install   1  Torch7 is required    Dependencies  on Linux  Ubuntu   9 04     sh   apt get install gcc g   git libreadline5 dev cmake wget libqt4 core libqt4 gui libqt4 dev   Dependencies  on Mac OS  Leopard  or more   using  Homebrew     sh   brew install git readline cmake wget qt   Then on both platforms    sh   git clone https   github com andresy torch   cd torch   mkdir build  cd build   cmake      make    sudo  make install   2  Once Torch7 is available  install this package    sh    sudo  torch pkg install opengm   Use the library   API  in very short    Load start up package    lua require  opengm    Construct a graph    lua g   opengm Graph        Optimize a graph    lua g optimize     Display a graph  using Graphviz    lua g show     A simple complete example       lua    load opengm require  opengm       standard factors f   opengm factors      define variables variables     car    person    building    street    vehicle        define factors factors       unary factors  prior probabilities of each class               f prior 0 9      car                 f prior 0 01     person                 f prior 0 7      building                 f prior 0 8      street                 f prior 0 4      vehicle                   Potts factors  joint probabilities               f band 0         car         person                 f band 0         person      building                 f band 0         building    street                 f band 0         car         building                 f band 0         building    vehicle                 f band 0         street      vehicle                 f band 0         person      vehicle                 f bimplies 1     car         vehicle          create graph g   opengm Graph variables  factors       optimize graph g optimize method  a    verbose true       print graph print g        Running the script above outputs     opengm  optimizing     step 1  E 3 99758  c 0 step 2  E 3 63212  c 2 19722 step 3  E 3 63212  c 2 19722  opengm Graph      nb of variables  4     nb of factors  6     graph is acyclic     current  optimized  variable states         car  1        person  0        building  0        street  0        vehicle  1  Torch7 Library for iOS   Torch7 provides a Matlab like environment for state of the art machine learning algorithms  It is easy to use and provides a very efficient implementation  thanks to an easy and fast scripting language  Lua  and a underlying C implementation    This package has been modified  or just hacked  to fully compile Torch7 for iOS  iPad iPhone  for all architectures  armv7  armv7a  arm64  i386  simulator   x86 64  simulator     Requirements   Torch7 needs to be installed prior to building the iOS version   torch  needs to be available in the user s path    I recommend doing the easy install if you have not installed Torch7  http   torch ch docs getting started html   Building The Framework   Simply run      generate ios framework   This will build all torch s libraries as static libs  and export them in a single dir  framework   The dir is ready to be included in an iOS project  it includes an example class to load Torch from within your Objective C project    For examples full examples that utilize this class  Torch m  please see  the ios examples  folder  More examples to come soon    Running   When creating your Objective C project simply import the class Torch m  h  include all the libs to the linker  add Torch framework   Accelrate framework and add all the Lua files as resources  define YOUR FILE lua and add it as  a resource  Run YOUR FILE lua using the method defined in Torch h  m liuflow  a wrapper around C Liu s optical flow   Note  this bit of code is a simple wrapper around the optical flow algorithm developped published by C Liu    C  Liu  Beyond Pixels  Exploring New Representations and Applications for Motion Analysis  Doctoral Thesis  Massachusetts Institute of  Technology  May 2009    More at  http   people csail mit edu celiu OpticalFlow  videograph  a package to create manipulate graphs on videos   This package provides standard functions to create and manipulate edge weighted graphs  of videos  create a graph  segment it  get  its adjacency matrix        Install   1  Torch7 is required    Dependencies  on Linux  Ubuntu   9 04     sh   apt get install gcc g   git libreadline5 dev cmake wget libqt4 core libqt4 gui libqt4 dev   Dependencies  on Mac OS  Leopard  or more   using  Homebrew     sh   brew install git readline cmake wget qt   Then on both platforms    sh   git clone https   github com andresy torch   cd torch   mkdir build  cd build   cmake      make    sudo  make install   2  Once Torch7 is available  install this package    sh    sudo  torch pkg install videograph   Use the library   First run torch  and load videograph    sh   torch         lua     require  videograph              IPAM Graduate Summer School   On Deep Learning  Feature Learning July 9   27  2012   More info here      Day 1  Setup       Welcome to the Practical Sessions for the summer school       Objectives        implementation level understanding of supervised and unsupervised learning algorithms       Many algorithms are more similar than researchers in the field     might have you believe        a sense of hyper parameter sensitivities and run times for various   algorithms       appreciation for two approaches to programming deep learning experiments       Code fragments for interactive exploration       Full blown application       exposure to programming languages and software stacks        Python  NumPy  SciPy  Theano       Lua  Torch7           Schedule  1 hour on four days this first week       Monday 12PM   1PM   Lua Torch  Python Theano  logging in to EC2       Tuesday 4PM   5PM   Supervised Learning in Lua and Python       Wednesday 4PM   5PM   Unsupervised Learning in Lua and Python       Thursday 4PM   5PM   TBA           Session Structure       Time is short for these practical sessions        Each day will start with two walk throughs of things you can experiment with      we ll try to be quick  to give you time afterward         After the walk throughs you can log in to an Amazon EC2 node where we ve set     things up        For lack of time   you will have to choose whether to do the Lua thing or     the Python thing in the in classroom time each day        We will negotiate with the organizers to leave the EC2 node up after the sessions       We will be around all week   feel free to ask questions any time        We will be available by email after the first week            10 mins crash course in Python  numpy       intro to IPython notebook       10 mins crash course in Lua  Torch7       will review very basic Lua and Torch concepts  to get people started       Remaining time   getting people into groups and setting them up to run the sample code   on laptop or EC2  Once they get it running  they can go for lunch or stick   around and play with things        Day 2  Supervised Learning       Models  SVM  MLP  ConvNets   Logistic Regression         Data Sets  MNIST  CIFAR  Google Street View House Numbers  SVHN     SVHN is an interesting new data set  very few results are available at this time     and is more computer visionny that MNIST         Optimization Methods  SGD  ASGD  L BFGS  batch vs  mini batch vs  online       Day 3  Feature Learning       Python  Imprinting  K Means  Autoencoder  De noising Autoencoder  RBM     Sparse Coding         Torch  Linar Autencoder  Convolutional Autoencoder  Linear and    Convolutional PSD  Predictive Sparse Decomposition  Autoencoder       Day 4  To Be Decided       Persitant Contrastive Divergence        Theano        Recurrent Neural Networks        GPU Programming 101        Torch nn extensions  write your own modules     csvigo  a package to handle CSV files  read and write     Install    First install Torch7  www torch ch  then simply install this package using luarocks    luarocks install csvigo   Use    The library provides 2 high level functions  csvigo load and csvigo save  To get help on these functions  simply do            csvigo save   csvigo load           Loading a CSV file in  query  mode gives you a convenient query function that you can use to query subsets of your original CSV file  To get help on this query function  simply do            query   csvigo load path  somefile csv   mode  query   query  help      print some help all   query  all   subset   query  union    somevar someval  someothervar  val1  val2            Large CSV mode   CSVigo supports efficient loading of very large CSV files into memory  The loaded data structure is a read only table with efficiency hidden under the hood    Loading    lua m   csvigo load  path    my large csv   mode    large      Printing by default only prints first 10 and last 10 rows  lua print m    Individual element access  lua print m 32     Size of table   lua print  m    For loop over entries    Type 1   lua for i 1   m do     print m i      get element end   Type 2   lua for k v in ipairs m  do     print k      print v  end   Type 3   lua for k v in pairs m  do     print k      print v  end   Read only table  lua    read only table  will error here  m 13     a  json  a package to handle json  read and write     TAKEN From JSON4Lua  originally written for Lua 5 1    Wrapped for Torch7  torch rocks     JSON4Lua  JSON encoding   decoding support for the Lua language  json Module    Author  Craig Mason Jones Homepage  http   json luaforge net  Version  0 9 40 This module is released under the MIT License  MIT     Install    First install Torch7  www torch ch  then simply install this package using torch rocks    torch rocks install json   Usage    This module exposes 4 functions        json string   encode o     returns the table   string   boolean   number   nil   json null value as a JSON encoded string    o   decode json string     returns a Lua object populated with the data encoded in the JSON string json string    save json file  o     saves the table   string   boolean   number   nil   json null value as a JSON encoded file    o   load json file     returns a Lua object populated with the data encoded in the JSON file      Torch Web Terminal   This is a browser based terminal for  Torch7    The goal of this project is to supersed the Qt4 interface  and to  enable full graphics capabilities within the browser    This project is built around  Node js    a super lightweight asynchronous framework to build servers  In our case  the server is only use to connect clients  browser  terminals  to Torch7 kernels  For now  one server instance can support an arbitrary number of clients  but each client only has access to one Torch7 kernel    Dependencies   You will need to install a couple of dependencies to enable this web terminal        Node js  which can be found  here   and   should also be installable with your system s package manager       NPM  Node s package manager  sometimes comes with Node js        Three Node js packages   ejs    stripcolorcodes  and  express       version 2 x        For instance  on MacOS    bash   brew install nodejs   curl http   npmjs org install sh   sh   npm install express 2 x ejs stripcolorcodes   Installation   This project is bundled as a  torch pkg  project  and can be  easily installed like this    bash   torch pkg  local install webterm   Or  if you have downloaded this repository on your machine  and you are in its directory    bash   torch pkg  local deploy   Note1  you have to deploy webterm locally   local flag   as the node  packages are only available to the current user  This could probably be fixed  but I still don t know how    Note2  depending on the version of Node js  you might have to do the NPM install in the package directory  i e     bash   cd    torch usr share torch lua webterm    npm install express 2 x ejs stripcolorcodes   Running it   webterm  is a standard package  so you can either require it from a running torch instance  or start torch with it like that    bash   torch  lwebterm   This should produce the following output    text Try the IDE  torch  ide Type help   for more info Torch 7 0  Copyright  C  2001 2011 Idiap  NEC Labs  NYU     Torch server listening on port 8080     Open http   localhost 8080  in your browser       Torch instance started for  t7    At this stage  you just have to open a browser and go to   http   localhost 8080    The cool thing of course is that you can access this adress remotely  Beware though that this might open up serious security issues    Functions   In the broswer  you will see a terminal  which provides full history and live completion  Completed entries are shown on the left pane  and are actual hyperlinks to documentation      One cool thing about a browser based terminal is that all the plots and renderings you can generate during your session can be transparently piped to the console      The mechanism we use to do this is very simple  the image  or plot  is dumped as a png into the root of the Node js server  and we then simply print a string of that form    img src  dumped png     to the terminal    In fact  this mechanism is completely general  try doing this in the terminal    lua print   h1 Some title  h1   p a paragraph     a     Now even more powerful  you can really print arbitrary html there  so printing  something like    lua print   script  console log  this is javascript      script         will just work perfectly fine    Multiple Users   By default  the user is set to  t7   which is what you should see in the terminal  You can create a new user by appending the string   user bob  to the URL  That ll create a  completely  new Torch7 kernel  which only Bob sees    TO DO     completion is still buggy  it starts screwing up after too many nested parenthesis   inline help  triggered by the     symbol  is shitty  we should use the full html   help instead of the poor text based help   I d love to have notebook like capabilities  where we can load a markdown file into   the browser  using the URL would be ok for now   e g     file myscript md    and    the text part would get rendered as html  and all the code blocks will be transformed   in interpretable code blocks   ala  Mathematica IPython    that last point implies that we need more flexible code entries  where we can go   back and forth to edit the code    ctrl C  not working yet  It generates a INT signal  but it doesn t seem to   do much for now    Taken from  http   www steve org uk Software lua fs docs index html Torch  Easy  Install   This is just an easy install script for Torch7  Eventually  it will be folded into the main repo    The goal of this script is to enable one line installs  To install Torch on any machine  just do    curl  s https   raw github com clementfarabet torchinstall master install all   bash    Once installed  you should be able to run Torch with basic libraries    torch  lparallel  loptim  lpl  limage    This script has been tested on MacOS X 10 8  and Ubuntu 12 04  It should work on earlier  Ubuntus and MacOS Xs  but other platforms are not supported    On Ubuntu you ll need  sudo  privileges  as the default install is global   and the script needs to install dependencies    If you ve already installed the dependencies  and don t have root privileges  you  can use this command to just install Torch    curl  s https   raw github com clementfarabet torchinstall master install torch   bash    By default  it will install Torch in  usr local    you can override this default path by doing    curl  s https   raw github com clementfarabet torchinstall master install torch   PREFIX   local bash    Torch7 now ships wih Luarocks  bundlde into an executable called torch rocks  You can install new packages like this    torch rocks search lua cjson torch rocks install lua cjson    By default  torch rocks includes a link to our own Rocks repository  hosted  here   If you wish to publish your  packages as rocks for Torch  simply clone this repo  add your rocks  and make a pull request on Github    Updating from a previous version   Note that if you are coming from a previous version you are advise to clean up the old installation with the following commands   rm  rf    luarocks rm  rf  usr local lib luarocks  rm  rf  usr local lib lua  rm  rf  usr local share torch  rm  rf  usr local share lua  rm  rf  usr local lua  rm  rf  usr local etc luarocks  nn2   nn2 is the successor of nn  The main thing we re trying to achieve here is      better consistency across modalities  Volumetric  Spatial  Temporal    better performance by packing features in memory     TODO List    Spatial  modules need to invert their convention  by packing the features in memory  Modules affected      SpatialConvolution   SpatialConvolutionMap          SpatialMaxPooling   SpatialSubSampling   Spatial Normalization   SpatialLPPooling   SpatialZeroPadding      Volumetric  modules      VolumetricConvolution   Lunatic  Python in Lua   Run a python interpreter within Lua  Pass data between python and lua    Bug fixed fork of  lua   python  which itself is forked from  Lunatic Page     See  Lunatic Page  for original documentation    Install   Clone this repo locally     git clone git github com dylski lua   python git    Build and install     cd lua   python   luarocks make python scm 0 rockspec  torchffi   Has moved to a more community friendly  repo   XML    Lua   This was taken from  this site   Licensed under an MIT license    Simply repackaged their code for Torch  I also added a parse   function  which simplifies the xml  table cnoversion  GraphicsMagick   A simple Lua wrapper to  GraphicsMagick     Only tested on Mac OSX  with GraphicsMagick installed via Homebrew    gm convert   This is just a binding to the command line convert utility  images are not loaded into Lua s memory   Examples    lua gm   require  graphicsmagick  gm convert     input     path to image png      output     path to image jpg      size    128x128      quality   95     verbose   true     gm info   Similarly  gm info file  is a simple binding to the command line utility  It s handy to extra the geometry of an image  as well as its exif metadata  On top of it  if geolocation is found  the GPS location is nicely formatted    lua gm   require  graphicsmagick  info   gm info  some jpeg   print info       width   1024    height   768    date   2013 01 01 00 00 01    location                 longitude   W80 13        latitude   N25 79           format   JPEG    exif                  Make   Apple         FocalLength   413 100                        gm Image   This is a full C interface to GraphicsMagick s Wand API  We expose one Class  the Image class  which allows loading and saving images  transforming them  and importing exporting them from to torch Tensors    Load library    lua gm   require  graphicsmagick    First  we provide two high level functions to load save directly into form tensors    lua img   gm load   path to image png     type         type    float   default     double     byte  gm save   path to image jpg    quality             quality   0 to 100  for jpegs only    The following provide a more controlled flow for loading saving jpegs    Create an image  from a file    lua image   gm Image   path to image png      or image   gm Image   image load   path to image png     Create an image  from a file  with a hint about the max size to be used       lua image load   path to image png   width    height        this tells the image loader that we won t need larger images than    what s specified  This can speedup loading by factors of 5 to 10        Save an image to disk       lua image save  filename ext        where     ext must be a know image format  jpg  JPEG  PNG           GraphicsMagick supports tons of them        Create an image  from a Tensor       lua image   gm Image tensor colorSpace dimensions     or image   gm Image   image load tensor colorSpace dimensions       where     colorSpace is  a string made of these characters  R G B A C Y M K I                    for example   RGB    RGBA    I   or  BGRA                          R  red  G  green      I  intensity       dimensions is  a string made of these characters  D H W                    for example   DHW  or  HWD                     D  depth  H  height  W  width       Export an image to a Tensor       lua image   gm Image  path jpg   image toTensor type colorSpace dimensions       where     type    float    double   or  byte     colorSpace   same as above    dimensions   same as above       When exporting Tensors  we can specify the color space       lua lab   image toTensor  float    LAB      equivalent to  image colorspace  LAB   lab   image toTensor  float        color spaces available  for now      LAB    HSL    HWB  and  YUV        Images can also be read written from to Lua strings  or binary blobs  This is convenient for in memory manipulation  e g  when downloading images from the web  no need to write it to disk        lua blob size   image toBlob   image fromBlob blob size    str   image toString   image fromString str        In this library  we use a single function to read write parameters  instead of the more classical get set      Here s an example of a resize       lua    get dimensions  width height   image size        resize  image size 512 384       resize by only imposing the largest dimension  image size 512       resize by imposing the smallest dimension  image size nil 512        Some basic transformations    lua    flip or flop an image  image flip   image flop     Sharpen    lua    Sharpens the image whith radius 0  sigma 0 6 image sharpen 0  0 6    Show an image  this makes use of Tensors  and Torch s Qt backend     lua image show     One cool thing about this library is that all the functions can be cascaded  Here s an example    lua    Open  transform and save back  gm Image  input jpg   flip   size 128  save  thumb jpg   RestClient   A simple client for REST APIs  This package provides a few functions  to get and post from to restful APIs  LBFGS   LibLBFGS  C Lib  wrapper    This is an FFI interface to  LibLBFGS     Installation   Simply build and install  LibLBFGS    with no SSE2 support  for now I don t support aligned memory blocks     This package can be installed with Luarocks    Usage   The code in test lua demonstrates how to use the solver  Its interface is 100  compatible to the solvers in  optim   CURL   A simple interface to CURL    Provides two functions   get  and  post     get        lua    load lib  curl   require  curl       getting random pages  res   curl get  http   www google com        with query  res   curl get  http   www google com    safe  off   output  search   oq  test         complete API  res   curl get      host    http   blogname blogspot com       path     feeds posts default       query             alt    json             format    json     parses the output  json    Lua table        Getting an image  and decoding it  img   curl get  http   www webstandards org files acid2 reference png   require  graphicsmagick   Image   fromString img  show         post     lua    post has the same API  with a form parameter  instead of query  res   curl post      host    http   myserver com       path            form             username    bob           password    key           somefiletoupload      local path to file jpg          gfx js  a browser based graphics server   Originally forked from the amazing  tty js     The goal is to extend this project to support the creation of rich media windows  on top of the terminal windows    The idea is simple  the server watches a directory  and monitors the creation   modification of HTML files  upong modification   creation  it creates a new window on the client side  browser   which simply render the HTML     Clients are easy to develop  one simply needs to dump HTML into the watched directory to have it rendered by the browser    For now  I m focusing on one client  written in Lua  for   Torch7       Check out  tty js  for reference on the original project  Note  I m simply extending their project  not modifying any of the core structure  so it should remain compatible    Installation   You have to have Node js  important  Version    0 10 0   NPM  and Torch7 installed  With older versions of Node js  things won t be stable  You also need libgraphicsmagick dev to be installed       sh   OS X   brew install graphicsmagick   Ubuntu   apt get install libgraphicsmagick1 dev apt get install graphicsmagick       Then simply run    sh luarocks install https   raw github com clementfarabet gfx js master gfx js scm 0 rockspec   Or  if you cloned the repo locally    sh luarocks make   Execution   Once installed  you can start stop the server like this  I m assuming a LuaJIT based install     luajit  lgfx start luajit  lgfx stop   And then open up a tab in your browser  at  http   localhost 8000     The browser acts as a passive graphics backend  The server monitors the creation of new resources  charts  plots  videos        and lets the browser know it should render them    The framework is very flexible  resources can be rendered by a client  luajit  with no browser open  and even no server listening running  The resources generated will still be saved  and can be visualized later  very useful to generate resources charts on a server with no X session     You can optionally specify a different port as an env variable  if the default  8000  is not available    PORT 4321 luajit  lgfx start PORT 4321 luajit  lgfx stop   Also  we provide a useful PS script  which lists running servers    luajit  lgfx ps   On Mac OS  we also provide a shortcut to start the server in the background and automatically open the browser at the right location    luajit  lgfx go   Alternatively  you can do things step by step        luajit  lgfx start   starts a server      luajit   starts a Torch session         At the prompt  you can load the gfx js client  and render things    lua gfx   require  gfx js  gfx image image lena    gfx image      image lena      image lena      image lena      image lena      image lena      image lena      image lena      image lena       zoom 0 5  legends   Image 1    Image 2       This will produce this output      I ve also slowly started to integrate plots from  NVD3   and bind them to Torch  so that they can seamlessly be called from the Torch repl       lua gfx chart data       chart    line      or  bar  stacked  multibar  scatter    width   600     height   450          where data has the form  data                   key    Legend 1           color     0f0           values      x 0 y 0    x 1 y 1                               key    Legend 2           color     00f           values      x 0 y 0    x 1 y 1                        or  for a single dataset  data         key    Legend       values      x 0 y 0                 values can be provided in convenient ways  values      x 0 y 0  size 0          values      0 0 0         values   torch randn 100 2  values   torch randn 100 3      the 3rd dimension is the optional size  only used by certain charts values   torch randn 100     in this case  y is provided  x defaults to range 0 N 1       shortcuts are also provided for quick plots  gfx chart torch randn 100 2    chart  scatter    gfx chart torch randn 100    chart  line        y is provided  x will be a range 1 N  gfx chart   torch randn 100   torch randn 100         multiple datasets gfx chart    1 2 3 4 5 6 7 8 7 6 5 4 3 2 1   torch randn 100         multiple datasets  table format       As explained above  one can generate resources charts figures with no server listening  One can connect a server later on  and redraw the last resources generated  Here are a few useful commands for that    lua gfx   require  gfx js  ids   gfx list 10     will list the last 10 figures generated  each figure has a unique ID  print ids 1      will print something like  dom 1212817597132847893127489 gfx redraw ids 1      will redraw this resource gfx redraw 10     will redraw the last 10 resources available  sorted by descending time    Finally  the server gets slower as the number of resources charts images grows in the  watched directory  It s useful to sometimes clear this cache manually   gfx clear   PERSIST   A persisting table for Lua    Built using Redis  it s a simple abstraction that allows one to write read from a table that persists over sessions  the key vals are persisted in Redis        lua    load lib  p   require  persist          write a few things to it  p test    something  p test2         some    table       nested    is 1          Shut down  start again       lua    load lib  p   require  persist          still there  print p test  print p test2        The following options can be passed    lua p   require  persist        url    localhost      port   6379     verbose   false     this is not only used on startup    namespace    th       this is the namespace in Redis    clear   false     clear all the data    ASyNC   An async framework for Lua Torch  based on  LibUV   using Tim Caswell s  luv  library     This lib is heavily inspired on the Node js architecture  It s fun  elegant  and should be extremely efficient  a lot of testing is required     The examples in  tests   should provide enough documentation on the API    License   MIT License   Examples   Starting the event loop  At the end of any program  the event loop must be started  Nothing will be interpreted after this call  as it takes control of the runtime    lua async go     It s useful to have a REPL  interpreter  running asynchronously  for debugging and live control of your programs    lua async repl       fires up an asyncronous repl   lua async repl listen  host  0 0 0 0   port 8080        fires up an async repl through a TCP server async repl connect  host  0 0 0 0   port 8080       connects to a remote repl through a TCP client   Common JS like timer controls   lua async setInterval millis  function      print  printed every N millis   end  async setTimeout millis  function      print  printed once in N millis   end    CPU Info  Useful to know how many processors are available  This is a synchronous call    lua print async cpuInfo      A TCP server       lua async tcp listen  host  0 0 0 0   port 8080   function client        Receive     client ondata function chunk           Data        print  received       chunk         Reply    client write  thanks       end       Done     client onend function         print  client gone         end  end        A TCP client       lua async tcp connect  host  127 0 0 1   port 8080   function client        Write something    client write  something         i       Callbacks    client ondata function chunk        print  received       chunk        client close      end       Done     client onend function         print  connection closed         end  end        File I O  The low level interface is not complete yet  but the high level one is final    lua async fs readFile  LICENSE   function content     print content     async fs writeFile  LICENSE copy   content  function status  err        print      wrote file        status or err      end  end    A lower level interface is also available  for C level performance  The upside  no copy is done  the user callback gets the raw pointer to the network buffer  read  and writes tap directly into the raw buffer  provided by the user  The downside  the buffer returned by the  ondata  callback lives only for the scope of that callback  and must be copied by the user         lua    assuming a client handle    local b   require  buffer    client onrawdata function chunk        chunk is a Buffer object  https   github com clementfarabet buffer     print chunk       chunk will not be valid past this point  so its content must be copied        not just referenced       local safe   chunk clone         safe can be past around         the most common use is to copy that chunk into an existing storage        for instance a tensor         assuming tensor is a torch Tensor     local dst   b tensor      creates a destination buffer on the tensor  a view  no copy     dst copy src  end       write   also accepts buffers  client write  b this is a string saved in a buffer object         last  the sync   interface can be set up in raw mode  client syncraw   local buffer   client read                We also provide a simple async interface to CURL    Provides two functions   get  and  post     get        lua    simple URL  async curl get  http   www google com   function res      print res  end       complete API  async curl get       host    http   blogname blogspot com       path     feeds posts default       query             alt    json             format    json     parses the output  json    Lua table    function res     print res  end       Getting an image  and decoding it  curl get  http   www webstandards org files acid2 reference png   function res    local decoded   require  graphicsmagick   Image   fromString res  end        post        lua    post has the same API  with a form parameter  instead of query   async curl post       host    http   myserver com       path            form             username    bob           password    key           somefiletoupload      local path to file jpg           function res     print res  end       or a simple file upload  async curl post       host    http   myserver com       path            file      path to file png      function res     print res  end      Manifold   A package to manipulate manifolds  for Torch7    Install   sh luarocks install manifold   Dependencies   In order to be able to run the binaries  you need to install the package  libatlas3 base   On a Ubuntu machine you can execute the following commands    sudo apt get update sudo apt get install libatlas3 base   Use      lua    package  m   require  manifold       a dataset  t   torch randn 100 10     100 samples  10 dim each      basic functions  ns   m neighbors t     return the matrix of neighbors for all samples  sorted  ds   m distances t     return the matrix of distances  L2  ts   m removeDuplicates t     remove duplicates from dataset      embeddings  p   m embedding random t   dim 2       embed samples into a 2D plane  using random projections p   m embedding lle t   dim 2  neighbors 3       embed samples into a 2D plane  using 3 neighbor  LLE  p   m embedding tsne t   dim 2  perplexity 30       embed samples into a 2D plane  using tSNE       Demos   To run the demos  simply type the following commands    sh cd demos qlua demo swissroll lua qlua demo tsne lua   Below is an example of a t SNE map produced on 5 000 MNIST digits by the  demos demo tsne lua  demo    Buffer   A buffer object for LuaJIT  The goal  efficient  C speed  byte manipulation for LuaJIT    Also provides interfaces to Torch s tensors and storages  for easy serialization    Install   luarocks install buffer   Simple use cases   Load lib       lua     b   require  buffer          Create a buffer  from a string  with a size  or from another buffer       lua     buf   b some  print buf    buf   b 10  print buf    buf2   b buf  print buf2    buf 1    10 buf 2    20 print buf2            Creating buffers never makes copies  A buffer created from a string always references the content of the string  A buffer created from another buffer references the same buffer    Concatenating two buffers is done like it s done for strings       lua     a   b some     b thing  str   a toString   print str  something         The  toString  method simply returns a Lua string from the buffer   In this case  the string is a copy  which won t be affected by further changes of the buffer       lua     a 1    a 1    1 print str  something print a toString    tomething         A slicing operator is provided       lua     a   b testing  print a  1 4    test a  1 4      sing  a  1 4     b sing      both supported print a  singing         A buffer can be created from a list of buffers  which provides efficient concatenation       lua     a1   b test  a2   b test  a3   b again  a   b a1 a2 a3  print a toString    testtestagain b   b   a1 a2 a3    print b toString    testtestagain         Finally  cloning a buffer allows clean memory separation       lua     a   b test  c   a clone           More advanced constructors are also available  to mount buffers on arbitrary managed or unmanaged chunks of memory  See tests for examples    Last  if Torch is available  converters are available from buffers to tensors and back  This is especially handy for multithreaded   multimachine environments  where exchanging tensors must be done at optimal speed  i e  with no complex  serialization        lua     t   torch FloatTensor 10  normal   buf   b t     buf is now a view on t s underlying contiguous storage    buf could be transmitted over sockets   threads  as raw binary data  see async for use cases         from buf  new storages or tensors can be constructed like this      tt   buf toFloatStorage   tt   buf toFloatTensor   tt   buf toFloatTensor 2 5     these are all views on the original storage of t          License   Code was originally inspired from the Luvit folks    Copyright 2013 2014 Clement Farabet  MADBITS  Copyright 2012 The Luvit Authors  All Rights Reserved    Licensed under the Apache License  Version 2 0  the  License    you may not use this file except in compliance with the License  You may obtain a copy of the License at   http   www apache org licenses LICENSE 2 0    Unless required by applicable law or agreed to in writing  software distributed under the License is distributed on an  AS IS  BASIS  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND  either express or implied  See the License for the specific language governing permissions and limitations under the License  LuaForever   Run a Lua LuaJIT script forever    Example       bash luajit myscript lua arg1 arg2   error  script crashes      luaforever myscript lua arg1 arg2   error  script crashes      myscript lua restarts automatically   error  script crashes      myscript lua restarts automatically             THMAP   A simple distributed framework to map jobs work onto multiple workers    The framework provides two binaries   thmap  and  thnode    thmap  is a controller  that lets you mirror commands to multiple  thnode  instances  and absorb the output of all these  thnode  instances in       The general philosophy is that you run a bunch of  thnode  instances on multiple machines  then run  thmap  everytime you need to schedule run new scripts onto these nodes    thmap  lets you git pull  and reload scripts  so that you can easily update a  distributed code base    Install   sh luarocks install thmap   Use   On the compute machine s   run  thnode     sh thnode   Write a config file that lists all the available nodes   nodes lua      lua    example configuration  with two nodes  return       host  ip1   port 10001       host  ip2   port 10001       by default   thnode  listens to port 10001  then increments if not available     Then start  thmap  to start monitoring and dispatching jobs    sh thmap   nodes nodes lua ip1 10001  ip2 10002  spawn  th     test lua          The above shows you a double shell  you are connected to two  thnode  instances  and any command you issue will be mirrored to both    In its current version   thmap  supports the following commands      spawn a job              spawn  th     script lua    opt1    opt2          autorestart true     restart running jobs     restart     list running jobs        ps     exec git command         git  pull      git  status    git pull   restart       update     kill all zombies         zombies     Schedlua   Basic scheduler   Note  this package is provided for compatibility with public tutorials   It is not maintained anymore     GM  Graphical Models for Torch LuaJIT   This package provides standard functions to create  arbitrary   undirected graphical models  using adjacency matrices    A graph is described by an adjacency matrix  node potentials and edge potentials     Install   sh   git clone        sudo  luarocks make   Use   First run torch  and load gm    sh   th         lua     require  gm          Once loaded  see and run the examples        lua     gm examples simple   gm examples trainMRF   gm examples trainCRF         nnop   Parameter free   operation only Neural Network primitives for torch nn    Motivation  Goals   Sometimes  it s useful to treat parameters as regular states  to either impose certain constraints on them  or simply make weight sharing visible   straight forward    The original design of  nn  treats trainable parameters as special variables  This package   nnop   builds on  nn  and  nngraph   but separates parameters from operations    It introduces a new module   nn Parameters   which provides trainable parameters  but does not do any computation  Every other parameterized node   nn Linear    nn SpatialConvolution        needs to be wrapped in  nnop  to decouple trainable parameters  and become pure operation nodes    TODO     wrap remaining parametrized nodes   nn SpatialConvolution          simplify unify auto generated parameterNodes      Examples   Weight sharing   In this example  2 modules are connected to a same set of trainable parameters  This is weight sharing       lua    Create parameters  linearWeight   nnop Parameters 10 100    linearBias   nnop Parameters 10         Create multiple layers  all connected to these parameters  input1   nn Identity     input2   nn Identity     linear1   nnop Linear    input1  linearWeight  linearBias   linear2   nnop Linear    input2  linearWeight  linearBias        Graph  graph   nn gModule  input1 input2    linear1 linear2        Tests  res   graph forward  torch randn 100   torch randn 100    assert type res      table  and  res    2    input   torch randn 100  res   graph forward  input  input   assert res 1  dist  res 2       0        Penalty on a set of parameters   In this example  we add an L1 penalty on a set of weight    When parameters are provided to the nnop Linear constructor  parameter nodes are automatically created  and automatically connected in the graph    We use this in this example  this way we don t have to create the parameter nodes  but are still free to access them and add a penalty on them       lua    create base modules  linear1   nnop Linear 10 100  tanh1   nn Tanh   linear2   nnop Linear 100 2       bind them in a graph  input   nn Identity     layer1   linear1 input  layer2   tanh1 layer1  layer3   linear2 layer2       get weights and impose penalty  weight1   linear1 parameterNodes weightNode sparse1   nn L1Penalty  001  weight1       build final model  model   nn gModule  input    layer3        train the model  for i   1 10 do    input   torch rand 10     output   model forward input     gradOutput   torch rand 2     gradInput   model updateGradInput input  gradOutput     model accGradParameters input  gradOutput  end     nnfunc   Functionalize nn modules  the goal of this package is to make it easy to develop 3rd party frameworks  by re exposing nn modules as functions  Basically provide a functional API to nn    Every instantiated module becomes a simple state less function  input data and parameters must be provided as inputs to this function  same thing for gradients  For convenience and efficiency  the state of the underlying nn module is still relied on for caching  every function returned by nnfunc is a closure relying on an instantiated nn module     API   Expose packages   Any package that provides  nn Module  children can be exposed    lua nnfunc functionalize  nn       done by default by nnfunc nnfunc functionalize  nnx      bundle new package      Once called  every module in the source package is available to use as a function  see examples below    API  1   A single function that evaluates the module  and automatically computes gradients if  gradOutput  is provided       lua    this returns a function that can be used to eval this module and    its gradients  layer   nnfunc nn Linear 10 100       compute module s output  prediction   layer      input   torch randn 10      weight   torch randn 100 10   bias   torch randn 100         prediction looks like this             output   torch Tensor 100            output can be user provided  optionally  prediction   layer      input   torch randn 10      weight   torch randn 100 10   bias   torch randn 100      output   torch Tensor 100         output is now valid      compute gradients  backprop    this happens automatically    because gradOutput is provided  grads   layer      input   torch randn 10      weight   torch randn 100 10   bias   torch randn 100      gradOutput   torch randn 100         grads looks like this             gradInput   torch Tensor 10         gradWeight   torch Tensor 100 10         gradBias   torch Tensor 100             the user can also provide all the tensors for computed gradients     if her application requires that they be owned externally  grads   layer      input   torch randn 10      weight   torch randn 100 10   bias   torch randn 100      gradOutput   torch randn 100      gradWeight   torch zeros 100 10   bias   torch zeros 100      gradInput   torch zeros 10         user provided gradInput  gradWeight and gradBias are now    valid        API  2   Two separate functions  one for eval  one for gradients  This can be useful when separate function pointers need to be used to register gradients       lua    two separate functions  layer gradLayer   nnfunc nn Linear 10 100       compute module s output  same as API  1   prediction   layer      input   torch randn 10      weight   torch randn 100 10   bias   torch randn 100           compute gradients  backprop   separate function for grads   grads   gradLayer      input   torch randn 10      weight   torch randn 100 10   bias   torch randn 100      gradOutput   torch randn 100            A hash table is also maintained to retrieve gradients associated to any object created       lua    two separate functions  layer gradLayer   nnfunc nn Linear 10 100       gradLayer could be retrieve like this  gradLayer2   nnfunc gradsOf layer  assert gradLayer2    gradLayer      Something  Regress   A very simple regression test package       lua local test   require  regress    test      test1   function         test mustBeTrue a    b   a should    b      end    test2   function         test shouldBeTrue a    b   a should    b      end
5,ajabri,pytorch maml   This is a PyTorch implementation of the supervised learning experiments from the paper  Model Agnostic Meta Learning  MAML   https   arxiv org abs 1703 03400   Important   You will need the latest version of PyTorch  v 0 2 0 to run this code  otherwise you will get errors about  double backwards not being supported     Currently  only the Omniglot experiments have been replicated here  The hyper parameters are the same as those used in the original  Tensorflow implementation  except that only 1 random seed is used here    5 way 1 shot training  best performance 98 9      20 way 1 shot training  best performance 92      Note  the 20 way performance is slightly lower than that reported in the paper  they report 95 8    If you can see why this might be  please let me know  Also in this experiment  we can see evidence of overfitting to the meta training set    The 5 way results are achieved by simply meta testing the network trained on the 1 shot task on the 5 shot task  e g  for the 5 way 5 shot result  test the 5 way 1 shot trained network with 5 shots   Again the 20 way result is lower here than reported in the paper    This repo also contains code for running maml experiments on permuted MNIST  tasks are created by shuffling the labels   This is a nice sanity check task  carml Space Time Correspondence as a Contrastive Random Walk        https   github com ajabri videowalk raw master figs teaser animation gif           This is the repository for  Space Time Correspondence as a Contrastive Random Walk   published at NeurIPS 2020        Paper     Project Page     Slides     Poster     Talk      inproceedings jabri2020walk      Author    Allan Jabri and Andrew Owens and Alexei A  Efros       Title    Space Time Correspondence as a Contrastive Random Walk       Booktitle    Advances in Neural Information Processing Systems       Year    2020      Consider citing our work or acknowledging this repository if you found this code to be helpful      Requirements     pytorch   1 3    torchvision  0 6 0    cv2   matplotlib   skimage   imageio     For visualization     visualize      wandb   visdom   sklearn   Train   An example training command is   python  W ignore train py   data path  path to kinetics      frame aug grid   dropout 0 1   clip len 4   temp 0 05     model type scratch   workers 16   batch size 20      cache dataset   data parallel   visualize   lr 0 0001   This yields a model with performance on DAVIS as follows  see below for evaluation instructions   provided as  pretrained pth    J F Mean    J Mean  J Recall  J Decay    F Mean  F Recall   F Decay   0 67606  0 645902  0 758043   0 2031  0 706219   0 83221  0 246789   Arguments of interest        dropout   The rate of edge dropout  default  0 1        clip len   Length of video sequence      temp   Softmax temperature      model type   Type of encoder  Use  scratch  or  scratch zeropad  if training from scratch  Use  imagenet18  to load an Imagenet pretrained network  Use  scratch  with    resume  if reloading a checkpoint      batch size   I ve managed to train models with batch sizes between 6 and 24  If you have can afford a larger batch size  consider increasing the    lr  from 0 0001 to 0 0003      frame aug    grid  samples a grid of patches to get nodes   none  will just use a single image and use embeddings in the feature map as nodes      visualize   Log diagonistics to  wandb  and data visualizations to  visdom       Data   We use the official  torchvision datasets Kinetics400  class for training  You can find directions for downloading Kinetics  here   In particular  the code expects the path given for kinetics to contain a  train 256  subdirectory    You can also provide    data path  with a file with a list of directories of images  or a path to a directory of directory of images  In this case  clips are randomly subsampled from the directory    Visualization   By default  the training script will log diagnostics to  wandb  and data visualizations to  visdom     Pretrained Model   You can find the model resulting from the training command above at  pretrained pth   We are still training updated ablation models and will post them when ready      Evaluation  Label Propagation   The label propagation algorithm is described in  test py    The output of  test py   predicted label maps  must be post processed for evaluation    DAVIS   To evaluate a trained model on the DAVIS task  clone the  davis2017 evaluation  repository  and prepare the data by downloading the  2017 dataset  and modifying the paths provided in  eval davis vallist txt   Then  run    Label Propagation    python test py   filelist  path to davis vallist txt     model type scratch   resume    pretrained pth   save path  save path     topk 10   videoLen 20   radius 12    temperature 0 05    cropSize  1  Though  test py  expects a model file created with  train py   it can easily be modified to be used with other networks  Note that we simply use the same temperature used at training time    You can also run the ImageNet baseline with the command below   python test py   filelist  path to davis vallist txt     model type imagenet18   save path  save path     topk 10   videoLen 20   radius 12    temperature 0 05    cropSize  1   Post Process         Convert   python eval convert davis py   in folder  save path    out folder  converted path   dataset  davis path    Compute metrics   python  path to davis2017 evaluation evaluation method py     task semi supervised     results path  converted path   set val     davis path  path to davis        You can generate the above commands with the script below  where removing    dryrun  will actually run them in sequence   python eval run test py   model path  path to model   L 20   K 10    T 0 05   cropSize  1   dryrun   Test time Adaptation   To do
6,USERNAMES,READMES
7,coryf,homebrew   Personal homebrew formula Sudoku Curses
8,jrodrigomg,SERVIDOR NODEJS PROYECTO 1  SISTEMAS OPERATIVOS 1 enGeoy   Aplicaci n Front End  ejemplo angular   Ejemplo b sico de angular js para exposici n seminario restify example Protostar Challenges   Mis soluciones para los retos de Protostar  My first implementation of tensorflow js to predict the contrast of a background color    Using p5 js and tensorflow js to get the more easily to read a text with the random background color    DEMO    https   calm basin 67709 herokuapp com contrast   LICENSE  MIT puzzleLearning   Implementation of neural network to play a puzzle game   Using tensorflow and p5 to select the squares in ascending or descending order    LICENSE  MIT   DEMO   https   calm basin 67709 herokuapp com puzzle trex volution   Using NeuroEvolution js to learning how to play t rex game   References   This was made thanks to the following projects     FlappyLearning  Build the neural network NeuroEvolution js   https   github com xviniette FlappyLearning    t rex runner  The game of T rex https   github com wayou t rex runner    T Rex  To stablish some ideas to interact with the program https   github com amaneureka T Rex    Demo     https   calm basin 67709 herokuapp com trex     Preview   My First Alexa Skill   This is my first implementation of alexa skill    I made this example thanks to this reference    https   www youtube com watch v 1pvR4aqwGhg index 3 list LLuicraBjEqukSJItEoTJabQ t 0s   https   www youtube com watch v  SUeL HSuIo index 4 list LLuicraBjEqukSJItEoTJabQ t 0s   https   www youtube com watch v cFzAIhsldbs index 2 list LLuicraBjEqukSJItEoTJabQ t 0s first linear regression   LInear regression model with Tensorflow js   The dataset is from  https   college cengage com mathematics brase understandable statistics 7e students datasets mlr frames frame html   This code is thanks to this references      https   stackoverflow com a 43899449   https   js tensorflow org tutorials fit curve html   https   medium com all of us are belong to machines gentlest intro to tensorflow part 3 matrices multi feature linear regression 30a81ebaaa6c   https   www youtube com watch v NZR N dhK2M   Analizame   Analize sentiment  keywords and relevance from user tweets    REQUIREMENTS   Submit this credentials in Config json        Python 3       IBM Watson Service  NLP  You need credentials from NLP of IBM     https   www ibm com watson services natural language understanding        Twitter CONSUMER and ACCESS  Keys and Secrets  https   apps twitter com        python twitter library  https   github com bear python twitter     black ai   Playing Black Jack with Reinforcement Learning   Requisites      Python3   Virtualenv     Instalation   Cloning   bash   git clone https   github com jrodrigomg black ai   cd black ai ai  Creating a virtual environment   bash   virtualenv  p python3 5 env   Load environment  this is an example for fish shell   bash   source env bin activate fish  Install requirments   bash   pip install  r requirements txt   Run the application server   Run the main file   bash   python main py   Running the game   In the main directory go to the game carpet   bash   cd game   Run a simple http server with python Not necesary the enviroment here   bash   python  m SimpleHTTPServer 8080   Try     Once running both application we need to configure wherever the server ask   go http   localhost 8080 in the browser   If just appear  Epoch 1  then refresh the browser     Credits     Thanks to adventuresinML for the incredible post of RL   https   adventuresinmachinelearning com reinforcement learning tutorial python keras     Thanks to Kusnierewicz for the game of blackjack in javascript  https   github com Kusnierewicz Blackjack game in JS    For the structure of the enviroment i used some part of Vincent Dutordoir s repo  https   github com vdutor TF rex    simple max ga   A simple function maximization with genetic algorithm   Credits   Special thanks to CodingTrain for the awesome playlist of Genetic Algorithms  https   www youtube com watch v 9zfeTw uFCw ACRONYM API   Requerimientos   Debes tener actualmente instalado    nodejs   mongodb   npm   pm2   Instalaci n     Instala las dependencias locales del proyecto de node      npm install  2  Debes configurar las variables de ecosystem config js en un ambiente como lo siguiente              General     NODE ENV   development        TZ    America Guatemala     PORT                3002    Puerto del node   Mongo URLMONGO             mongodb   127 0 0 1 27017    USER MONGO           USERMONGO   PASS MONGO           PASSWORDMONGO   BD MONGO             DBMONGO      JWT secret jwt key       MISECRETTOKEN   expiredTime         50000              Crear una base de datos  usuario donde  ste usuario tenga los permisos necesarios       Iniciar el servidor con                pm2 start       5  El servidor est  arriba  Deber a de desplegar un mensaje en http   localhost 3002   Nota   Para probar los m todos PUT y DELETE se cre  un endpoint para crear los JSON Web Token   En la carpeta de testing se tiene automatizado el env o del header Authorization para  no tener que hacerlo manual  s lo se tendr a que ver la url correcta    El esquema de la base de datos es  name  donde  ste corresponde al acr nimo y  description  corresponde a la definici n  Para el m todo POST podr a testearse con  curl  con el siguiente comando        curl  X POST  H  Content Type application json   d    name   TOMATE    description   Ve a traer tomates     http   localhost 3002 acronym
9,vascofernandes,CustomersVRPF   The project consists of two main parts    backend based on ASP NET WebAPI   frontend based on AngularJS in the form of a Single Page Application    The WebAPI is defined in Controllers CustomersController cs A general description of the API is as follows    GET    api customers        Get all customers GET    api customers  id    Get one customer by ID POST   api customers        Insert a new customer PUT    api customers  id    Update a customer DELETE api customers  id    delete a customer    A customer is defined in Models Customers cs  The operation are handled by the class Datastores Datastore cs which implements Interfaces IUnitOfWork cs  In Datastore cs the methods Customers returns an instance of Repositories CustomerRepository cs which implements Interfaces ICustomerRepository cs  CustomerRepository cs handles all the data related tasks  such as insertion  update  and deletion of customers  In this case  the implementation is simple a non persistent List with some sample data  This can be easily replaced with database functionality  With this structure one can easily extend to API to handle for example products or invoices for each customer    When relevant I used some basic data mapping from domain objects  such as Customer cs  to data transfer objects  to send over the network  For a more robust solution one could make use of libraries such as Json NET  http   james newtonking com json  and Automapper  http   automapper org    both available on NuGet    On the front end the applications uses angularjs ui router to handle the three views of the applications  Customer List Add New  Detailed Customer  and Edit Customer    The frontend communicates with the backend using and service based on AngularJS Resources defined in app services customerResource js  There controllers  one for each view  are defined in app customers and make use of the customerResoure to communicate with the API  The front end is organized on a feature basis  If one would need to implement product management  a folder app products would contain controllers and views related with product management    The applications communicates correctly with the backend  but since there is no persistence  the customers information does not change between requests  angular powers angularjs reduced   Install node js modules       bash   npm install          Install DefinitelyTyped Typescript deffinition files       bash   npm run tsd install          Build      bash   npm run build           android natve to webview example      Reaction Diffusion     Quick and dirty way to put together a C   and OpenGL reaction diffusion experiment    http   scitation aip org content aip journal jcp 116 13 10 1063 1 1453964   http   science sciencemag org content 293 5535 1635      SoftEngine   angularjs typescript components   http   jsbin com koyulexiro 1 edit html js console output Publish and run the pubished app
10,seba-1511,Arduino Chrono   An Arduino based chronometer for Tooski   Warning  This project is still in Beta  so some things may or may not work  If you find a problem  bug or have an idea don t hesitate to send me a message  It always makes people happy  right     Presentation   Here are all the files I used to build my own chronometer  It was initialy an idea for the website Tooski ch  so made for skiers  and that s why the text on the screen is in French  However it is really easy to modify it and to understand the code  as the comments are in English    It is based on Arduino  uses Xbee for wireless communication   300m  and the detectors are using IR beams to detect when something goes through them  By the way I take this occasion to thank Ken Shirriff for his library that helped a lot   If you go throught the folders  you will find the part list  the schematics and of course the commented Arduino code  Not to mention that there is also the instructions on how to use it once you are done building it    Once again critics  comments and suggestions are really welcome  and keep in mind I may do errors  so point them out to me  Also if you modify or improve my code  let me know because I ll be happy to see what you ve done    You can see a video of the prototype here  http   www youtube com watch v 61YS39FGn74   Futur implementations        Storing the times into an SD card and make a table out of them        Allow multiple skiers at the same time        Implementing a camera that takes a photo when the chrono starts        Having a camera that films you only from when the chrono starts to when it ends        License   Copyright  c  2012 S bastien Arnold   Permission is hereby granted  free of charge  to any person obtaining a copy of this software and associated documentation files  the  Software    to deal in the Software without restriction  including without limitation the rights to use  copy  modify  merge  publish  distribute  sublicense  and or sell copies of the Software  and to permit persons to whom the Software is furnished to do so  subject to the following conditions    The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software    THE SOFTWARE IS PROVIDED  AS IS   WITHOUT WARRANTY OF ANY KIND  EXPRESS OR IMPLIED  INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT  IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM  DAMAGES OR OTHER LIABILITY  WHETHER IN AN ACTION OF CONTRACT  TORT OR OTHERWISE  ARISING FROM  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE  Booter   The Booter Project  A predefined structure to kick your projects  This includes    CodeIgniter 2 1 3  With some default configuration  Ex  Autoload  Cookies  etc       jQuery 1 9 and jQuery 2 0 to best match your targeted visitors   jQuery UI 1 10 3 not themed   jQuery Mobile 1 3 1   Bootstrap 2 3 1 JS and CSS   Less 1 3 3   HandleBarsJS 1 0 0   Glyphish Pro 3 0 Icons   Custom Scripts   Notes on Custom Scripts   CodeIgniter s Layout Library   As we don t want to always load the same Headers  Footers  etc    of the page  we implemented a library to do this for us  The principle is that you add views  that will be stored in the class  Then by simply calling the printPage function with   php   this  layout  printPage       the whole page will be printed  Note that you can also add a JS  CSS and set the title by simply calling the librarie s methods  See an example in the welcome controler    Also  you can set your own default layouts by creating them in the views layout folder  By default  there are only 4 of them      default  Just an empty HTML5 layout     basic  Only main JavScripts and CSS are imported     quicksite  The starting point for a project  with some elements of Bootstrap     mobile  A simple web app with header and footer   IMPORTANT  The Layout Library is imported by default  so remove it in autoload if you don t use it       CodeIgniter CRUD Model   The file application core MY CRUD php implements a basic CRUD implementation   With count option  The easiest way to understand it is to read the file  We highly recommend to use it as a core function for the different models you ll build  by doing so      php  class User model extends MY CRUD       protected  table    users             Booter JS   Booter has its own JavaScript Class  found in the file assets js booter utils js  This file has some functions that I found usefull  and that can be used in about any projects  To use it  just call   javascript booter theNameOfThePublicMethod      License   Copyright  C  2013 S bastien Arnold   Permission is hereby granted  free of charge  to any person obtaining a copy of this software and associated documentation files  the  Software    to deal in the Software without restriction  including without limitation the rights to use  copy  modify  merge  publish  distribute  sublicense  and or sell copies of the Software  and to permit persons to whom the Software is furnished to do so  subject to the following conditions    The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software    THE SOFTWARE IS PROVIDED  AS IS   WITHOUT WARRANTY OF ANY KIND  EXPRESS OR IMPLIED  INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT  IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM  DAMAGES OR OTHER LIABILITY  WHETHER IN AN ACTION OF CONTRACT  TORT OR OTHERWISE  ARISING FROM  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE  YoBooter   A mix of Booter and Yeoman  for even faster web app developpment PersHo   A personal homepage for everybody   Requirements      Installable with a simple git clone   Links collection like ethz1 goat ch   Public   Personal Link collection to my other services   Public   Quick Notes  twitter like   Hidden but shareable   Todo s   Private   Uses Django 1 5 for backend   Uses AngularJS for front end   Local SQLite to store everything      Structure of Database      Links   Name   Link   Date   Id   Personal   Name   Link   Date   Id   Notes   Title   Content   Date   Id   Todo s   State   Content   Date   Id   mosgapPhonegap mosgapJSensors mobileNotes   A mobile app for taking notes meritocracy   A Meritocracy based nodeGame tooski mobile   The mobile application for Tooski js speech api test   Test the HTML5 Speech recognition and TTS API  planktonChallenge   The Kaggle challenge for plankton recognition   To make it work   Please download the data from the kaggle website and unzip the folders into the  data  folder on the root of the project  fluster backend    The backend of the application for Fluster   Deployement Link  http   uwsgi docs readthedocs org en latest tutorials Django and nginx html fluster frontend    The front end of the website for Fluster Quantum   This repository contains my code and experiments for my research on quantum computing as a student at ISI  under the advisement of Dr  Itay Hen  It contains the following    simulatedAnnealing    Several implementations of the simulated annealing  some of them with a basic grid search procedure  A version is based on the schedule found by Dr  Albash  The goal was to solve planted instances as fast as possible     quantumAnnealing    The implementation of a quantum annealer  to solve the same kind of problems as with simulated annealing  ReadMe   Some code for my GSoC 15 demo project  Also  allows me to test the Keras library  Colloquium 310   This repository will contain all summaries produced for the second part of lecture  Software Engineering   CSCI 310    List of Lectures   The list of covered by those summaries  as well as who worked on them    Lecture Title                 Lecture n    Authors                               Link                                                                                        Software Process Models       12           Jenny  S                               link  Agile Development             13           Matthew  W                             link  Cleanroom Dev Process         14                                         link  Black box Testing             15           Kelsey  T                              link    White box Testing             16           Anthony  Th                            link  Regression Testing            17           Pavel F                                  link  Fault Localization            18           Justine  Th                             link  Architectural Styles          19           Seb  S                                 link  Design Patterns               20           Seb  T                                 link  Secure Software Engineering   21           Daler  F                                   link   Presentation Guidelines   In the span of one hour  we should be able to be able to summarize 2 weeks of lectures  This means about 30 minutes per lecture and thus we should focus on important or advanced concepts as student will be familiar with the basic ones  The idea is that people presenting are experts in their respective lecture and share their knowledge  To have a good idea of what the presentation should be about  check the last slide of every lecture    Suggested structure        Important Concept 1      What it is about     How it fits in the    Why it is important    How does it differ from other concepts we ve seen           Repeat for every concept       The way I worked through the Design Pattern lecture was to first decide if a slide was important or not  and if it was then imagine what were the possible questions about it    Other Resources   A summary built by other team members of the class   Available here Specialists   Report on an experiment with the generalist specialist framework    This repository is far from being clean  and the experiments were carried out with an older version of neon  You can read the obtained results in  paper paper pdf   conjugate gradient newton   Comparison of Gradient Descent  Newton s Method  and Conjugate Directions Algorithm CSCI 544 Final Project   Final Project for CSCI 544 Spring 2016   Instructions   To run the whole experiment run   approx  20min on a recent NVIDIA GPU    make all   It should download the dataset  unzip  train and serialize the core components  and finally train and benchmark the augmented  multimodal model    A typical output should be             python experiment data py   SaudiNewsNet SaudiNewsNet master dataset 2015 08 09 zip   SaudiNewsNet SaudiNewsNet master dataset 2015 08 10 zip   SaudiNewsNet SaudiNewsNet master dataset 2015 07 25 zip   SaudiNewsNet SaudiNewsNet master dataset 2015 07 24 zip   SaudiNewsNet SaudiNewsNet master dataset 2015 08 08 zip   SaudiNewsNet SaudiNewsNet master dataset 2015 08 01 zip   SaudiNewsNet SaudiNewsNet master dataset 2015 07 22 zip   SaudiNewsNet SaudiNewsNet master dataset 2015 08 04 zip   SaudiNewsNet SaudiNewsNet master dataset 2015 08 11 zip   SaudiNewsNet SaudiNewsNet master dataset 2015 08 06 zip   SaudiNewsNet SaudiNewsNet master dataset 2015 07 27 zip   SaudiNewsNet SaudiNewsNet master dataset 2015 07 26 zip   SaudiNewsNet SaudiNewsNet master dataset 2015 07 21 zip   SaudiNewsNet SaudiNewsNet master dataset 2015 07 23 zip   SaudiNewsNet SaudiNewsNet master dataset 2015 08 02 zip   SaudiNewsNet SaudiNewsNet master dataset 2015 08 07 zip   SaudiNewsNet SaudiNewsNet master dataset 2015 08 03 zip   SaudiNewsNet SaudiNewsNet master dataset 2015 07 31 zip vocabulary size    343172   of samples    31030   of classes 14   class distribution      0  1  2  3  4  5  6  7  8  9 10 11 12 13   2080 2947 2964 3690 4852 2279 2090 3065  133 2846 1411 2252   52  369  sentence length    1527     0    1    2      4480 5160 6200   354  42 104        1   1   1    of train   24740    of valid   6290   vocabulary from dataset is saved into labeledTrainData tsv vocab python experiment train core py  e 2 2016 04 26 23 11 08 573   neon backends   WARNING   deterministic update and deterministic args are deprecated in favor of specifying random seed Epoch 0    Train                         194 194  batches  2 17 cost  65 13s  Epoch 1    Train                         193 193  batches  1 16 cost  63 86s  Train accuracy     0 90553761  Test accuracy     0 49904612  python experiment train augmented py  e 7  i 1 2016 04 26 23 14 20 195   neon backends   WARNING   deterministic update and deterministic args are deprecated in favor of specifying random seed Epoch 0    Train                         194 194  batches  0 71 cost  124 93s  Epoch 1    Train                         193 193  batches  0 45 cost  124 21s  Epoch 2    Train                         193 193  batches  0 34 cost  124 83s  Epoch 3    Train                         194 194  batches  0 30 cost  127 64s  Epoch 4    Train                         193 193  batches  0 25 cost  126 71s  Epoch 5    Train                         193 193  batches  0 23 cost  126 60s  Epoch 6    Train                         193 193  batches  0 24 cost  126 76s  Train accuracy     0 96604687  Test accuracy     0 49252781          License   The content of this repository is exclusive to the purpose of CSCI 544 class at USC  Copy or redistribution of the code is strictly forbidden without prior authorization of all the authors  dtrpo         randopt is a Python package for machine learning experiment management  hyper parameter optimization  and results visualization  Some of its features include      result logging and management    human readable format    support for parallelism   distributed   asynchronous experiments    command line and programmatic API    shareable  flexible Web visualization    automatic hyper parameter search  and   pure Python   no dependencies       Installation   shell pip install randopt   Usage      python import randopt as ro   def loss x       return x  2   e   ro Experiment  myexp              alpha   ro Gaussian mean 0 0  std 1 0  dtype  float             Sampling parameters   for i in xrange 100       e sample  alpha       res   loss e alpha      print  Result     res      e add result res    Manually setting parameters   e alpha   0 00001 res   loss e alpha  e add result res    Search over all experiments results  including ones from previous runs   opt   e minimum   print  Best result     opt result    with params     opt params        Results Visualization   Once you obtained some results  run  roviz py path to experiment folder  to visualize them in your web browser    For more info on visualization and  roviz py   refer to the  Visualizing Results  tutorial    Hyper Parameter Optimization   To generate results and search for good hyper parameters you can either user  ropt py  or write your own optimizaiton script using the  Evolutionary  and  GridSearch  classes    For more info on hyper parameter optimization  refer to the  Optimizing Hyperparams  tutorial    Documentation   For more examples  tutorials  and documentation refer to the  wiki     Contributing   To contribute to Randopt  it is recommended to follow the  contribution guidelines     Acknowledgements   Randopt is maintained by  S b Arnold   with numerous contributions from the following persons      Noel Trivedi   Cyrus Jia   Daler Asrorov     License   Randopt is released under the Apache 2 License  For more information  refer to the  LICENSE file     I would love to hear how your use Randopt  Feel free to  drop me a line    dist blog   Distributed Optimization Blog Post cdn   Personal CDN Shapechanger   Formerly knows as mj transfer    Additional transfer learning environments for OpenAI Gym    Install Instructions     Clone this folder    Simply execute  python setup py develop  in this folder    You can now  import mj transfer  to register them  and  gym make  BigAnt v1    if you want to use the BigAnt env      MuJoCo Tutorial   You can find a short tutorial for that explains the basics of MuJoCo there   seba 1511 github io mj transfer   Available Environments   AmputedAnt v1   Like Ant v1  but with one quarter leg missing     BigAnt v1   Like Ant v1  but with long legs     ExtendedAnt v1   Like Ant v1  but with an additional joint on each limb    SmallInvertedPendulum v1   Like InvertedPendulum  but with an arm half the size     BigInvertedPendulum v1   Like InvertedPendulum  but with an arm twice the size     Finger v1   A realistic tendon driven finger            Environments                                              nbsp                                                                                                                                     mpi4pycuda   A wrapper to enable GPU GPU communication with pyCUDA and mpi4py    Install   Example nnexp   A template for neural network experiments   Goals     Define your PyTorch Dataset  your NN  and your randopt variables  Call  learn        and off you go    Automatic splitting  if desired  into train valid test data    Choose to use CUDA  automatically distributed  or not    Use sensible   but overrideable   defaults   for optimizer  data pre processing  hyper params  etc       Provide Mnist example    train function should only do 1 epoch    Sensible command line arguments support    Easy network definition    dist rl   Distributed Implementation of RL Algorithms lstms pth   Implementation of LSTM variants  in PyTorch     For now  they only support a sequence size of 1  and meant for RL use cases   Besides that  they are a stripped down version of PyTorch s RNN layers    no bidirectional  no num layers  no batch first    Base Modules      SlowLSTM  a  mostly useless  pedagogic example    LayerNorm  Layer Normalization as in  Ba   al     Layer Normalization       Dropout Modules      LSTM  the original    GalLSTM  using dropout as in  Gal   Ghahramami    A Theoretically Grounded Application of Dropout in RNNs     MoonLSTM  using dropout as in  Moon   al    RNNDrop  A Novel Dropout for RNNs in ASR     SemeniutaLSTM  using dropout as in  Semeniuta   al    Recurrent Dropout without Memory Loss       Normalization   Dropout Modules      LayerNormLSTM  Dropout   Layer Normalization    LayerNormGalLSTM  Gal Dropout   Layer Normalization    LayerNormMoonLSTM  Moon Dropout   Layer Normalization    LayerNormSemeniutaLSTM  Semeniuta Dropout   Layer Normalization      Container Modules      MultiLayerLSTM  helper class to build multiple layers LSTMs      Convention   If applicable  the activations are computed first  and  then  the nodes are droped   dropout on the output  not the input  just like PyTorch    Install   pip install  e     Usage   You can find a good example of how to use the layers in  test test speed py     All Dropout models share the same signature    python LSTM self  input size  hidden size  bias True  dropout 0 0  dropout method  pytorch     All Normalization   Dropout models share the same signature    python LayerNormLSTM self  input size  hidden size  bias True  dropout 0 0                dropout method  pytorch   ln preact True  learnable True     And all models use the same  out  hidden   model forward x  hidden  signature as the official PyTorch LSTM layers  They also all provide a  model sample mask    method  which needs to be called in order to sample a new Dropout mask   e g  when processing a new sequence    Note    LayerNorm  is not an LSTM layer  and thus uses  out   model forward x      Containers   This package provides a helper class   MultiLayerLSTM   which can be use to stack multiple LSTMs together    python lstm   MultiLayerLSTM input size 256  layer type LayerNormSemeniutaLSTM                        layer sizes  64  64  16   dropout 0 7  ln preact False  hiddens   lstm create hiddens bsz batch size  x   Variable th rand 1  1  256   for   in range 10       out  hiddens   lstm x  hiddens    Note that  hiddens  doesn t match the PyTorch specification  It is the list of   h i  c i   for each LSTM layer  Instead  the  LSTM  layers in PyTorch return a single tuple of   h n  c n    where  h n  and  c n  have sizes  num layers   num directions  batch  hidden size     Capacity Benchmarks   Warning  This is an artificial memory benchmark  not necessarily representative of each method s capacity    Note  nn LSTM and SlowLSTM do not have dropout in these experiments    Info  dropout    0 9   SEQ LEN    10   dataset size    100  layer size    256   model                    error                                  nn LSTM          3 515  SlowLSTM         4 171  LSTM             4 160  GalLSTM          4 456  MoonLSTM         4 442  SemeniutaLSTM    3 762  GalLSTM          4 456  MoonLSTM         4 442  SemeniutaLSTM    3 762   Speed Benchmarks   Available by running  make speed     Warning  Inference timings only  and on a single sequence of length 1000  with  dropout    0 5     SlowLSTM  Benchmark   size     nn LSTM      SlowLSTM    Speedup                                           128      0 628       0 666        0 943 256      0 676       0 759        0 890 512      0 709       1 026        0 690 1024      2 364       2 867        0 824 2048      6 161       8 261        0 746   LSTM  Benchmark   size     nn LSTM      LSTM    Speedup                                       128      0 568       0 387    1 466 256      0 668       0 419    1 594 512      0 803       0 769    1 045 1024      2 966       2 002    1 482 2048      6 291       6 393    0 984   GalLSTM  Benchmark   size     nn LSTM      GalLSTM    Speedup                                          128      0 557       0 488       1 142 256      0 683       0 446       1 530 512      0 966       0 548       1 763 1024      2 524       2 587       0 975 2048      6 618       6 099       1 085   MoonLSTM  Benchmark   size     nn LSTM      MoonLSTM    Speedup                                           128      0 667       0 445        1 499 256      0 818       0 535        1 530 512      0 908       0 695        1 306 1024      2 517       2 553        0 986 2048      6 475       6 779        0 955   SemeniutaLSTM  Benchmark   size     nn LSTM      SemeniutaLSTM    Speedup                                                128      0 692       0 513             1 348 256      0 685       0 697             0 983 512      0 717       0 701             1 022 1024      2 639       2 751             0 959 2048      7 294       6 122             1 191   LayerNormLSTM  Benchmark   size     nn LSTM      LayerNormLSTM    Speedup                                                128      0 646       1 656             0 390 256      0 583       1 800             0 324 512      0 770       1 989             0 387 1024      2 623       3 844             0 682 2048      6 573       9 592             0 685   LayerNormGalLSTM  Benchmark   size     nn LSTM      LayerNormGalLSTM    Speedup                                                   128      0 566       0 486                1 163 256      0 592       0 350                1 693 512      0 920       0 606                1 517 1024      2 508       2 427                1 034 2048      7 356       10 268                0 716   LayerNormMoonLSTM  Benchmark   size     nn LSTM      LayerNormMoonLSTM    Speedup                                                    128      0 507       0 389                 1 305 256      0 685       0 511                 1 342 512      0 762       0 685                 1 111 1024      2 661       2 261                 1 177 2048      8 904       9 710                 0 917   LayerNormSemeniutaLSTM  Benchmark   size     nn LSTM      LayerNormSemeniutaLSTM    Speedup                                                         128      0 492       0 388                      1 267 256      0 583       0 360                      1 616 512      0 760       0 578                      1 316 1024      2 586       2 328                      1 111 2048      6 970       10 725                      0 650 cluster setup rpi   Custom setup scripts for the RPi Cluster cervix kaggle   Intel Cervix Kaggle Competition    Getting Started   Install pytorch following the instructions on  pytorch org     Then run   pip install  r requirements txt  to install the dependencies  dist tuto pth   PyTorch Distributed Tutorial Plotify   Make beautiful plots  fast    Plotify is in active development  It is an intuitive wrapper around Matplotlib  Moviepy  and others that allows to easily create plots and animations    Checkout the scripts in  tests  and the results in  outputs     Python 3 3   Sobolev Training with Pytorch   Small scale replication of Sobolev Training for NNs    Overview   You can use the code by importing  SobolevLoss  from  sobolev py   In order to use it  checkout the example in  main py   The general guideline for distillation is       python from sobolev import SobolevLoss   teacher   Net   student   Net   loss   SobolevLoss loss nn MSELoss    weight 1 0  order 2    compute the gradients of teacher and student   sobolev   loss student parameters    teacher parameters      At this point  the parameters  gradients of student look like    s grad   s original grad   s grad grad   where s grad grad comes from the Sobolov loss         Remarks       Make sure that your teacher is well trained    It works well towards the end of distillation    Instead of  student parameters    and  teacher parameters    you can pass an iterable of parameters whose nth order gradients have been computed    Theoretically should work for higher order  but I didn t test it      Benchmark results   The results obtained by distilling a LeNet teacher  converged  into a LeNet student with the same random architecture  The results are in the form  train   test  at the 100th epoch of training    Metric           Vanilla      Sobolev                                           Distill  Loss    1 2   1 19   0 56   0 64 Student Loss     0 94   0 9   0 8   0 82 Teacher Loss     0 7   0 72   0 7   0 72 Sobolev Loss     n   a        2e 4   4e 4 Randopt Plugins   A bunch of plugins for randopt   To install all       shell pip install  r requirements txt pip install         Live Wrapper   The live wrapper allows to print   plot experimental metrics live    Install      shell pip install matploblib terminaltables       Usage      python import randopt as ro from randopt plugins live import Live   exp   ro Experiment  live example   params        x   ro Gaussian         y   ro Gaussian      live   Live exp  metrics   square    norm    xminusy    time      start   time   for i in range 100       live sample all params       live update  square   live x  2      live update            norm   abs exp y            xminusy   exp x   exp y           time   time     start            print live table metrics        live plot metrics       sleep 1    live add result exp x   exp y  live add result exp x   exp y    useless    0  0  0  0    live add result exp x   exp y  data   useless    0  0  0  0          Screenshots   Live Plotting     Live Table   Implicit Gradient Transport in PyTorch   This repository is the original code for the experiments in   https   arxiv org abs 1906 03532     For more information  please refer to our  paper  or our  website     Citation   You can cite the algorithms implemented in this repository by citing the following paper         misc arnold2019reducing      title  Reducing the variance in online optimization by transporting            past gradients       author  Sebastien M  R  Arnold              Pierre Antoine Manzagol              Reza Babanezhad              Ioannis Mitliagkas              Nicolas Le Roux       year  2019       archivePrefix  arXiv       primaryClass  cs LG          Installation   pip install  e     Usage   See  tests   folder for more examples       python import torch optim as optim from torch igt import IGTransporter   opt   optim SGD model parameters    lr 0 01  momentum 0 9  opt   IGTransporter model parameters    opt    Compute a single optimization step   opt train      Ensures parameters are set to the transported ones loss   L model X train   y train  opt zero grad   loss backward   opt step     Reverts parameters to the true ones   opt eval   loss   L model X test   y test        Note   The ITA family of algorithms  such as  Heavyball ITA  in the paper  are implemented as  torch igt ITA params  opt          Cherry is a reinforcement learning framework for researchers built on top of PyTorch    Unlike other reinforcement learning implementations  cherry doesn t implement a single monolithic  interface to existing algorithms  Instead  it provides you with low level  common tools to write your own algorithms  Drawing from the UNIX philosophy  each tool strives to be as independent from the rest of the framework as possible  So if you don t like a specific tool  you don t need to use it    Features     Pythonic and low level interface    la  Pytorch    Support for tabular     and function approximation algorithms    Various OpenAI Gym environment wrappers    Helper functions for popular algorithms   e g  A2C  DDPG  TRPO  PPO  SAC    Logging  visualization  and debugging tools    Painless and efficient distributed training on CPUs and GPUs    Unit  integration  and regression tested  continuously integrated      To learn more about the tools and philosophy behind cherry  check out our  Getting Started tutorial     Example   The following snippet showcases some of the tools offered by cherry       python import cherry as ch   Wrap environments   env   gym make  CartPole v0   env   ch envs Logger env  interval 1000  env   ch envs Torch env    policy   PolicyNet   optimizer   optim Adam policy parameters    lr 1e 2  replay   ch ExperienceReplay      Manage transitions   for step in range 1000       state   env reset       while True          mass   Categorical policy state           action   mass sample           log prob   mass log prob action          next state  reward  done      env step action          Build the ExperienceReplay     replay append state  action  reward  next state  done  log prob log prob      if done          break     else          state   next state    Discounting and normalizing rewards rewards   ch td discount 0 99  replay reward    replay done    rewards   ch normalize rewards   loss    th sum replay log prob     rewards  optimizer zero grad   loss backward   optimizer step   replay empty            Many more high quality examples are available in the  examples   folder    Installation   Note  Cherry is considered in early alpha release  Stuff might break    pip install cherry rl   Changelog   A human readable changelog is available in the  CHANGELOG md  file    Documentation   Documentation and tutorials are available on cherry s website   http   cherry rl net     Contributing   First  thanks for your consideration in contributing to cherry  Here are a couple of guidelines we strive to follow      It s always a good idea to open an issue first  where we can discuss how to best proceed    If you want to contribute a new example using cherry  it would preferably stand in a single file    If you would like to contribute a new feature to the core library  we suggest to first implement an example showcasing your new functionality  Doing so is quite useful    it allows for automatic testing    it ensures that the functionality is correctly implemented    it shows users how to use your functionality  and   it gives a concrete example when discussing the best way to merge your implementation          We don t have forums  but are happy to discuss with you on slack  Make sure to send an email to  smr arnold gmail com  to get an invite    Acknowledgements   Cherry draws inspiration from many reinforcement learning implementations  including     OpenAI Baselines     John Schulman s  implementations   Ilya Kostrikov s  implementations     Shangtong Zhang s  implementations     Dave Abel s  implementations     Vitchyr Pong s  implementations     Kai Arulkumaran s  implementations     RLLab     Garage       Why  cherry      Because it s the sweetest part of  the cake         learn2learn is a PyTorch library for meta learning implementations    The goal of meta learning is to enable agents to  learn how to learn   That is  we would like our agents to become better learners as they solve more and more tasks  For example  the animation below shows an agent that learns to run after a only one parameter update      Features   learn2learn provides high  and low level utilities for meta learning  The high level utilities allow arbitrary users to take advantage of exisiting meta learning algorithms  The low level utilities enable researchers to develop new and better meta learning algorithms    Some features of learn2learn include      Modular API  implement your own training loops with our low level utilities    Provides various meta learning algorithms  e g  MAML  FOMAML  MetaSGD  ProtoNets  DiCE    Task generator with unified API  compatible with torchvision  torchtext  torchaudio  and cherry    Provides standardized meta learning tasks for vision  Omniglot  mini ImageNet   reinforcement learning  Particles  Mujoco   and even text  news classification     100  compatible with PyTorch    use your own modules  datasets  or libraries      Installation      bash pip install learn2learn       API Demo   The following is an example of using the high level MAML implementation on MNIST  For more algorithms and lower level utilities  please refer to the  documentation  or the  examples        python import learn2learn as l2l   mnist   torchvision datasets MNIST root   tmp mnist   train True    mnist   l2l data MetaDataset mnist  task generator   l2l data TaskGenerator mnist                                          ways 3                                          classes  0  1  4  6  8  9                                           tasks 10  model   Net   maml   l2l algorithms MAML model  lr 1e 3  first order False  opt   optim Adam maml parameters    lr 4e 3    for iteration in range num iterations       learner   maml clone      Creates a clone of model     adaptation task   task generator sample shots 1      Fast adapt for step in range adaptation steps       error   compute loss adaptation task      learner adapt error     Compute evaluation loss evaluation task   task generator sample shots 1                                          task adaptation task sampled task  evaluation error   compute loss evaluation task     Meta update the model parameters opt zero grad   evaluation error backward   opt step            Citation   To cite the  learn2learn  repository in your academic publications  please use the following reference      S bastien M R  Arnold  Praateek Mahajan  Debajyoti Datta  Ian Bunner    learn2learn     https   github com learnables learn2learn   2019      You can also use the following Bibtex entry       bib  misc learn2learn2019      author          S bastien M R  Arnold  Praateek Mahajan  Debajyoti Datta  Ian Bunner       title           learn2learn       month          sep      year           2019      url             https   github com learnables learn2learn              Acknowledgements   Friends     The RL environments are adapted from Tristan Deleu s  implementations  and from the ProMP  repository   Both shared with permission  under the MIT License    TorchMeta  is similar library  with a focus on supervised meta learning  If learn2learn were missing a particular functionality  we would go check if TorchMeta has it  But we would also open an issue      higher  is a PyTorch library that also enables differentiating through optimization inner loops  Their approach is different from learn2learn in that they monkey patch nn Module to be stateless  For more information  refer to  their ArXiv paper     Embedding Adaptation is Still Needed for Few Shot Learning     Code Release for  Embedding Adaptation is Still Needed for Few Shot Learning    This code provides      Re implementation of the ATG algorithm in  examples atg py     Loaders for the dataset splits introduced in the paper    Demonstration code for training the algorithms  borrowed from  learn2learn       Resources     Website   seba1511 net projects atg   Preprint   arxiv org abs 2101 XXXXX   Code   github com Sha Lab atg     Citation   Please cite this work as follows       Embedding Adaptation is Still Needed for Few Shot Learning   S bastien M  R  Arnold and Fei Sha     or with the following BibTex entry       bibtex  article arnold2021embedding      title  Embedding Adaptation is Still Needed for Few Shot Learning       author  Sebastien M  R  Arnold  Fei Sha       year  2021       archivePrefix  arXiv       primaryClass  cs LG          Usage   Dependencies include the following Python packages      PyTorch  1 3 0   torchvision  0 5 0   scikit learn  0 19 2   tqdm  4 48 2   learn2learn on the master branch     Running ATG   A standalone re implementation of ATG is provided in  examples atg py   To run it on a synthetic dataset    bash python examples atg py   Training on ATG Partitions      bash python examples train py   algorithm  protonet    dataset  mini imagenet    taskset  original        where     taskset  takes values  easy    medium easy    medium hard    hard  or  randomX  where  X  is the seed to reproduce random splits    dataset  takes values  mini imagenet    tiered imagenet    emnist    lfw10    cifar100     algorithm  takes values  protonet    maml    anil       For more help on the interface  run   python examples train py   help   pycyju   Benchmarking Python wrapping other languages Policy Learning and Evaluation with Randomized Quasi Monte Carlo     Code release for  Policy Learning and Evaluation with Randomized Quasi Monte Carlo   AISTATS 2022    This code provides a re implementation of SAC combined with RQMC  It is based on the PyTorch implementation of SAC in  spinning up     Resources     Website   seba1511 net projects qrl   Preprint   arxiv org abs 2202 07808   Code   github com seba 1511 qrl     Citation   Please cite this work as follows      S  M  R  Arnold  P  L Ecuyer  L  Chen  Y  Chen  F  Sha   Policy Learning and Evaluation with Randomized Quasi Monte Carlo   AISTATS 2022      or with the following BibTex entry       bibtex  inproceedings Arnold2022qrl      title  Policy Learning and Evaluation with Randomized Quasi Monte Carlo       author  Arnold  S  ebastien M  R  and L Ecuyer  Pierre and Chen  Liyu and Chen  Yi fan and Sha  Fei       year  2022       booktitle  Proceedings of The 25th International Conference on Artificial Intelligence and Statistics       volume  131       series  Proceedings of Machine Learning Research       publisher  PMLR           Usage   Policy learning experiments can be run with the following command       shell python qsac py   env HalfCheetah v2   rqmc   multi actions 4     sync paperpile notion   Sync changes in Paperpile to a Notion database    Setup   On Notion       Create a new database  e g   Papers   with the columns named exactly      Title  of type title    Authors  of type text    Year  of type text    Link  of type url    Reference ID  of type text          Get the  database identifier  from the database page  If your database url is    https   www notion so my workspace aaaabbbbccccddddeeeeffffgggghhhh   Then the database identifier is   aaaabbbbccccddddeeeeffffgggghhhh         Create a new integration on  https   www notion so my integrations        Name  Paperpile to Notion   Associated Workspace  Workspace of the database    Content Capabilities  Read Content  Update Content  Insert Content    User Capabilities  Read user information  including email addresses    Press  Submit  and copy the  Internal Integration Token           On the database page  click  Share   top right  and add  Paperpile to Notion  with edit access        On GitHub     Fork this repository with the green  Use this template  button    On you fork  go to   Settings    Secrets    Actions       Create 2 new repository secrets named exactly      NOTION TOKEN   Your integration s internal integration token  from step 3 5 above    DATABASE IDENTIFIER   Your database identifier  from step 2 above          On Paperpile     Click on the top right gear  go to  Workflows and Integrations       Follow the instructions to add a new  BibTex Export   choosing      Your GitHub repository fork as the repository    references bib  as the export path          The first sync should start as soon as the Paperpile workflow is created  and subsequent syncs are triggered whenever papers are added or updated in your Paperpile    Note  The first sync might take some time as Notion limits the API rate to   3 requests   second  so if you have 1 000 papers it ll take   6 minutes before they are all available in Notion
11,xiaozhuchacha,Code for CVPR 2016 Oral Paper   Inferring Forces and Learning Human Utilities From Videos   See  Wiki  for instructions   Project website  http   www yzhu io projects cvpr16 chair index html   Bibtex   bash  InProceedings cvpr2016chair      author    Zhu  Yixin and Jiang  Chenfanfu and Zhao  Yibiao and Terzopoulos  Demetri and Zhu  Song Chun       title    Inferring Forces and Learning Human Utilities From Videos       booktitle    IEEE Conference on Computer Vision and Pattern Recognition  CVPR        year    2016   Kinect2 Toolbox   Code for CVPR 2015 Paper   Understanding Tools  Task Oriented Object Modeling  Learning and Recognition   Project website  http   www yzhu io projects cvpr15 tool    Bibtex   bash  InProceedings zhu2015tool      title  Understanding Tools  Task Oriented Object Modeling  Learning and Recognition       author  Zhu  Yixin and Zhao  Yibiao and Zhu  Song Chun       booktitle  IEEE Conference on Computer Vision and Pattern Recognition  CVPR        year  2015      Kinect v2 Toolbox Manual   Contact   Yixin Zhu   yixin zhu ucla edu    Last revised  October 2016   Prerequisite     Kinect SDK  Download Link     Windows 8 1 10    Visual Studio 2015 installed or Runtime Libraries installed    MATLAB R2014b or newer version for visualization  optional       Usage   The toolbox includes two types of code   Recorders  and  Converters    Recorders  dump the Kinect raw data into RGB images  depth images and skeleton data   Converters  post process the data collected by Recorders    Recorders  include two sets    Set 1  Everything will be converted to depth space later    ColorRecorder  dump raw RGB images to hard drive   DepthRecorder  dump raw depth images to hard drive   ColorDepthRecorder  dump both raw depth and RGB images to hard drive   SkeletonRecorder  dump skeleton data to hard drive   ColorDepthSkeletonRecorder  dump RGB  depth and skeleton simultaneously to hard drive  Two versions are provided  The  Mask  version marks the skeleton area in the depth image  The mapper produced in this code will be used to  convert rgb to depth space      Set 2  Everything will be converted to color space later    ColorDepthSkeletonRecorder2  dump RGB  depth and skeleton simultaneously to hard drive  The mapper produced in this code will be used to  convert depth to rgb space      Converters  include two sets    Set 1  Everything will be converted to depth space later    RGBDAlign  align RGB images to the depth images   Depth2PC  convert raw depth images to point cloud  ply files  no color    ColorDepth2PC  convert aligned RGB D images to point cloud  ply files   Set 2  Everything will be converted to color space later    RGBDAlign2  align depth images to the RGB images   We recommend to use the binary files located inside Pipeline forder if you do not need to change anything in the source code    Record Kinect Raw Data with Kinect Studio   You will need Kinect v2 to physically connect to your PC  Please refer to  this page  to check whether your machine is compatible with Kinect v2  The most important component is a USB 3 0 port    Open Kinect Studio  and switch to  Record  tab  Click on  Connect  button  and check all  11 streams   Once you finish recording  a  xef file will be generated      Kinect v2 consumes lots of memory  If you need to record long clips  remember to edit the buffer size before recording  Also  in order to achieve high frame rate when dumping the data into hard drive  it is recommended to have more than 16GB memory for Kinect Studio      Recorders   In this section  NO Kinect v2 needed to physically connect to PC  Following instructions assume no connection      Load the recorded Kinect data   xef file  that you want to dump using Kinect Studio    In PLAY tab  click Connected button    Check all 11 streams          Hit Play button  and you should be able to see the replayed videos  You can change the stream  RGB  depth  skeleton  etc  in two viewers by clicking on the gear shape buttons          Hit Pause button at the beginning of the data that you want to dump    Open any  Recorder  mentioned in previous section  You should see a  stilled  skeleton in a pop up windows  In this manual  we will use  ColorDepthSkeletonRecorder  as the example  since it is the most comprehensive  Recorder  that records RGB  depth and skeleton data simultaneously          Hit Play button to resume playing  The skeleton in the pop up windows should start to move together with the videos in Kinect Studio  Data starts to dump into the  data folder    Hit Pause button again to stop dumping  before the video runs out   and close the pop up window      Converters   All the  converters  load the data from  data  folder  and write the new data into the same  data  folder    The inputs and outputs of  converters  are defined as   Converters   Inputs   Outputs   Physically plugin Kinect                                                            Depth2PC   raw depth   point cloud file   ply    Yes RGBDAlign   depth2rgb mapper  raw depth  raw rgb   aligned rgb   No ColorDepth2PC   aligned rgb  raw depth   point cloud file   ply    Yes RGBDAlign2   depth2rgb mapper2  raw depth  raw rgb   aligned depth   No   MATLAB Visualization  optional    This part of toolbox is designed for visualization only  You can visualize the data easily in other programming languages  e g  Python  C    etc      Copy all the files from previous  data  folder into the subfolder  data  located in the folder  matlab viz     Run  viz result m   It should show point cloud with human skeleton  The function  showPointCloud  is only supported in MATLAB 2014b or newer version  but can be easily replaced by function  plot3  in older versions      OpenBottle   Code for the paper  Feeling the Force  Integrating Force and Pose for Fluent Discovery through Imitation Learning to Open Medicine Bottles    Instructions  https   github com xiaozhuchacha OpenBottle wiki   Please refer for the wiki for instructions on how to use this code    If you find this code useful  please cite our work with the following bibtex    inproceedings edmonds2017feeling      title  Feeling the Force  Integrating Force and Pose for Fluent Discovery through Imitation Learning to Open Medicine Bottles       author  Mark Edmonds  Feng Gao  Xu Xie  Hangxin Liu  Siyuan Qi  Yixin Zhu  Brandon Rothrock  Song Chun Zhu       booktitle  International Conference on Intelligent Robots and Systems  IROS        year  2017       organization  IEEE                                                             VCLATactileGlove   Code for IROS 2017 Paper  A Glove based System for Studying Hand Object Manipulation via Joint Pose and Force Sensing   Please refer for the wiki for instructions on how to use this code    If you find this code useful  please cite our work with the following bibtex    inproceedings liu2017glove      title  A Glove based System for Studying Hand Object Manipulation via Joint Pose and Force Sensing       author  Liu  Hangxin and Xie  Xu and Millar  Matt and Edmonds  Mark and Gao  Feng and Zhu  Yixin and Santos  Veronica J and Rothrock  Brandon and Zhu  Song Chun       booktitle  International Conference on Intelligent Robots and Systems  IROS        year  2017    AOG AR   Code for ICRA 2018 paper   Interactive Robot Knowledge Patching using Augmented Reality   ROS Package   send to hololens    Library Used     tacopie   OpenBottle robot control  for controlling a rethink baxter robot     source files   TCPPackageConstants h   Defines constants for the format of the packets transmitted between ROS and Hololens    send to hololens cpp   Establishes TCP connection with Hololens  Listens to different types of ROS messages  TF  image etc     and does some processing specific to the robot Baxter  and gripper used in this application  Send updates on robot states to Hololens  and also listens to commands and updates sent by user from Hololens    Unity Application   Assets       ProjectSettings    Development Platform     Microsoft Hololens   Unity 5 6   Visual Studio 2017     Library Used     MixedRealityToolkit Unity   HoloLensARToolKit     Overview   This application receives real time updates on robot states e g  TF  kinect images  gripper force  from ROS  and displays those information as hololens to users through Microsoft Hololens platform  The user can also see the knowledge base of the robot represented as an and or graph  and interactively patch the knowledge structure     Scripts   TCPManager cs   Listens for a TCP connection first  Once connection established  try to fetch data every frame  and stores them in the buffer for other scripts to access    SensorDisplay cs   Controls the visibility and position of the holograms and the buttons  Convert data from the frame of the robot to world frame of the Hololens application    Node cs   Class to represent a node in the and or graph    SimpleTree cs   Class to represent the and or graph structure  Provide function to go to the next end node action  from any end node    SlotPanel cs  Slot cs  NodeChange cs  DragHandler cs  Inventory cs   Implements drag and drop on canvas to enable and or graph patching  Parse the and or graph to generate action sequence    OnScreenKeyboard cs   Helper class to use TouchScreenKeyboard class to edit new action name   Requires building in XMAL instead of D3D    HandDraggableCustom cs  GripperNewPoseControl cs  EditAction cs   Allow user to edit an end node  and set new pose for the action by dragging a gripper hologram with hand gesture    Wiki   https   github com xiaozhuchacha AOG AR wiki
12,bossjones,vagrant box add scarlettpi v1  Users malcolm dev basebox packer virtualbox ubuntu1204 desktop provisionerless box   vagrant plugin install vagrant salt bossjones pocketsphinx   Clone of SVN Pocketsphinx 0 8 with optimizations for Scarlett  bossjones fabric   Just a github repo for my fabric tasks  Will continue to refactor over time   For my purposes  I always use a Jumpserver to access all of my remote servers    because of this you need to add the following to your   bash profile     For bossjones fabric tasks       export FABRIC KEY FILENAME   path to  ssh id rsa    export FABRIC JUMPSERVER  127 0 0 1    export FABRIC USER  blacktonystarkoflife        Yes I know I can use a user wide     fabricrc  file as well    Known errors       If you get the following error  NetworkError  Error reading SSH protocol banner  make sure you note which servers this error was thrown on  and simply ssh into it  This will add it to your  known hosts  file  and should prevent this issue from coming up        If you re using a jumpserver with  nc   seems as though it caps out at 10 concurrent connections  Researching how to increase this limit     Overview   This is a demo of Vagrant using a  SaltStack  provisioner  It can be used as a template for a new webapp project  Following the steps below will set you up with a Ubuntu 12 04  precise32  Vagrant VM running unconfigured nginx  supervisord  memcached  postgresql and rabbitmq  Feel free to fork and extend this example    After  downloading and installing Vagrant   you need to set up your Vagrant configuration with some additional elements   You only have to do these things once on your development box    vagrant gem install vagrant salt vagrant gem install vagrant vbguest   The first installs the SaltStack provisioner for Vagrant  The second installs the Vagrant  auto update VirtualBox extensions  module  which is optional  but handy    Next  clone this repo and run the following commands    git clone https   github com jaddison vagrant salt demo git cd vagrant salt demo git submodule init git submodule update vagrant up   Your vagrant will now be provisioned and running   try browsing to  http   localhost 9000      The  git submodule used is here   It is a simple set of Salt states and roles  which I plan to improve upon   pull requests are welcome  Feel free to use them in other projects    TODO     set up a  virtualenv  to host a Python webapp  ie  Django     set up database user  db  etc   use Salt s templating to set up configuration files   include multiple Vagrantfiles for different scenarios  ie  multi VMs that communicate between themselves    bossjones chef   The bossjones chef setup   links          http   www markholmberg com articles installing ruby 2 system wide on ubuntu 12 04 using rbenv       https   github com fesplugas rbenv installer       https   www digitalocean com community articles how to install chef and ruby with rvm on a ubuntu vps       NOTE  Make sure the following ENV variables are set in your   bash profile               export GIT ACCESS TOKEN  gdfsiuhdsiughfdsiughfdiughdfsiughsdfiugh  export KITCHEN GLOBAL YAML    path to chef config kitchen yml          Also  don t forget to install  knife block bossjones chef scarlett   Provision a scarlett dev server with chef client or provision an existing server with chef solo Sigintwrapper   This is a simple wrapper which runs the provided program and forwards SIGTERM to SIGINT   This is because uWSGI listens for SIGINT for restarts while runit sends SIGTERM    Usage   After installing via    pip install sigintwrapper    Use via    sigintwrapper  absolute path to program  with arguments   bossjones packer   Create Packer images using Rake tasks          master      be rake  T rake build digitalocean           Build a base image from chef cookbooks rake check build vars service     Check that ENV variables are set rake check validate service       Validate all the packer templates in service directory rake spec                         Run the specs    1 9 3 p448   malcolm MacBookPro1 in   dev bossjones bossjones packer    master          opencv spotify hackathon   Using opencv to do face detection and play a creepy sound every time a face is detected   install pygame in virtualenv   brew install libvorbis sdl sdl image sdl mixer sdl ttf portmidi pip install hg http   bitbucket org pygame pygame     docker php   Provides basic building blocks for PHP web applications  available on  Docker Hub  Add s PHP FPM  mods  and specific backend configuration to Behance s  docker nginx   Three variants are available     default  Ubuntu based  PHP 7 0     slim  Alpine based  PHP 7 0  tagged as   alpine     beta  Ubuntu based  PHP 7 1  tagged as   beta     legacy  Ubuntu based  PHP 5 6  tagged as   legacy     Includes       Nginx   PHP PHP FPM  7 0  7 1  5 6    S6  for PID 1 zombie reaping  startup coordination  shutdown signal transferal   Goss  for serverspec like testing  Run  goss  g  tests php fpm  variant name  goss yaml  to validate any configuration updates   Extra PHP Modules            not available on Alpine variant         not available on Beta tag         disabled by default  use  phpenmod  to enable on Ubuntu based variants  uncomment  ini file otherwise      apc    only visible for backwards compatibility       apcu      calendar     bz2     ctype     curl     date     dom     exif     fpm     gd     gearman      iconv     igbinary      intl     json     mbstring     mcrypt     memcache       memcached      mysqli     mysqlnd     newrelic   activates with env variables      opcache     openssl     pcntl     pdo     pdo mysql     pdo pgsql      pgsql      phar     posix     redis      shmop     SimpleXML     sockets     sysvmsg     sysvsem     sysvshm     xdebug      xml     xmlreader     xmlwriter     yaml       zip     zlib   Expectations     Applications that leverage  bryanlatten docker php  as their container parent are expected to copy their application into   app   for example   COPY     app    Inside the copied directory  there must be a directory named  public     this will be automatically assigned as the webroot for the web server  which expects a front controller called  index php     Production Mode  an immutable container  without file updates  should set  CFG APP DEBUG 0  for max PHP performance     NOTE  Nginx is exposed and bound to an unprivileged port   8080     Monitoring       NewRelic APM  automatically enabled by adding providing environment variables  REPLACE NEWRELIC APP  and  REPLACE NEWRELIC LICENSE   PHP FPM Status  available  only  inside container at     status   Application healthcheck can pull PHP FPM statistics from  http   127 0 0 1   status json   To open to more clients than local  add more  allow  statements in    status  location block in   CONF NGINX SITE    etc nginx sites available default     Nginx Status  available  only  inside container at     nginx status   Application healthcheck can pull nginx statistics from  http   127 0 0 1   nginx status   To open to more clients than local  add more  allow  statements in    nginx status  location block in  CONF NGINX SITE    etc nginx sites available default        Downstream Configuration     Several environment variables can be used to configure various PHP FPM paramaters  as well as a few Nginx configurations  as such  These can be used to drive the configuration of the downstream PHP application in any way necessary  but there are a few environment variables that  bryanlatter docker php  will process along the way      See parent  docker nginx  for additional configuration     Variable   Example   Default   Description                              DATABASE HOST master rds aws com        PHP has access to environment variables by default  CFG APP DEBUG     CFG APP DEBUG 1    1   Set to  1  or  true  will cue the Opcache to watch for file changes  Set to 0 for  production mode   which provides a sizeable performance boost  though manually updating a file will not be seen unless the opcache is reset   SERVER MAX BODY SIZE     SERVER MAX BODY SIZE 4M    1M   Allows the downstream application to specify a non default  client max body size  configuration for the  server  level directive in   etc nginx sites available default   SERVER FASTCGI BUFFERS     SERVER FASTCGI BUFFERS  512 32k     256 16k    docs    tweaking   SERVER FASTCGI BUFFER SIZE     SERVER FASTCGI BUFFER SIZE  256k     128k    docs    tweaking   SERVER FASTCGI BUSY BUFFERS SIZE     SERVER FASTCGI BUSY BUFFERS SIZE  1024k     256k    docs   REPLACE NEWRELIC APP     REPLACE NEWRELIC APP prod server abc        Sets application name for newrelic  REPLACE NEWRELIC LICENSE     REPLACE NEWRELIC LICENSE abcdefg        Sets license for newrelic  when combined with above  will enable newrelic reporting  PHP FPM MEMORY LIMIT     PHP FPM MEMORY LIMIT 256M    192MB   Sets memory limit for FPM instances of PHP  PHP FPM MAX EXECUTION TIME     PHP FPM MAX EXECUTION TIME 30    60   Sets time limit for FPM workers  PHP FPM UPLOAD MAX FILESIZE     PHP FPM UPLOAD MAX FILESIZE 100M    1M   Sets both upload max filesize and post max size  PHP FPM MAX CHILDREN     PHP FPM MAX CHILDREN 15    4096    docs   PHP FPM START SERVERS     PHP FPM START SERVERS 40    20    docs   PHP FPM MAX REQUESTS     PHP FPM MAX REQUESTS 100    1024    docs  How many requests an individual FPM worker will process before recycling  PHP FPM MIN SPARE SERVERS     PHP FPM MIN SPARE SERVERS 10    5    docs   PHP FPM MAX SPARE SERVERS     PHP FPM MAX SPARE SERVERS 64    128    docs dockerfiles   Bunch of Dockfiles for me to mess around w  different services applications pentest linux privileges   borrowed via chousensha   Linux privilege checker   Searches for privileges  service settings  sensitive information  running processes  installed programs  network statistics  logs and  various other information about the system    Inspired from g0tm1lk s cheatsheet  available at http   blog g0tmi1k com 2011 08 basic linux privilege escalation html   NOTES   Full output may be excessive  I tried to break it down in specific functions to make it easy to check for relevant information only by commenting out the unnecessary bits    It s using the subprocess module for running commands    Running time may vary  with all checks enabled it took  1min on a freshly installed Debian and  4min on a custom Kali system    DEMO   See text file for output    Collection of network related scripts    DEMO     dnsquery   Perform DNS lookups using Python        dnsquery py twitter com A Address for twitter com is 199 16 156 38 Address for twitter com is 199 16 156 70 Address for twitter com is 199 16 156 102 Query completed at Tue Mar 25 12 40 28 2014     dnsquery py wikipedia com AAAA IPv6 address for wikipedia com is 2620 0 861 ed1a  1 Query completed at Tue Mar 25 12 49 28 2014     dnsquery py wikipedia com MX Mail server at lists wikimedia org  preference  50 Mail server at mchenry wikimedia org  preference  10 Query completed at Tue Mar 25 12 50 29 2014     dnsquery py yahoo com NS DNS server for yahoo com domain  ns3 yahoo com  DNS server for yahoo com domain  ns4 yahoo com  DNS server for yahoo com domain  ns2 yahoo com  DNS server for yahoo com domain  ns1 yahoo com  DNS server for yahoo com domain  ns6 yahoo com  DNS server for yahoo com domain  ns5 yahoo com  Query completed at Tue Mar 25 12 51 06 2014     honeypot   Simple honeypot server using the socket module    Server listens on a port and displays a banner read from a file to every client that connects to it  Data received from clients is logged to a file       honeypot py  f banner txt  p 5555 Listening on port 5555 on 2014 04 10 10 21  2014 04 10 10 21  Connection received from 192 168 127 133 52379 Received from 192 168 127 133  I m the first client    2014 04 10 10 21  Connection received from 192 168 127 130 59013 Received from 192 168 127 130  And I m the second   Received from 192 168 127 130  blabla   Received from 192 168 127 130  bye Clean Blog by Start Bootstrap   Jekyll Version   The official Jekyll version of the Clean Blog theme by  Start Bootstrap     View Live Demo     Before You Begin   In the  config yml file  the base URL is set to  startbootstrap clean blog jekyll which is this themes gh pages preview  It s recommended that you remove the base URL before working with this theme locally    It should look like this   baseurl       What s Included   A full Jekyll environment is included with this theme  If you have Jekyll installed  simply run  jekyll serve  in your command line and preview the build in your browser  You can use  jekyll serve   watch  to watch for changes in the source files as well    A Grunt environment is also included  There are a number of tasks it performs like minification of the JavaScript  compiling of the LESS files  adding banners to keep the Apache 2 0 license intact  and watching for changes  Run the grunt default task by entering  grunt  into your command line which will build the files  You can use  grunt watch  if you are working on the JavaScript or the LESS    You can run  jekyll serve   watch  and  grunt watch  at the same time to watch for changes and then build them all at once    Support   Visit Clean Blog s template overview page on Start Bootstrap at http   startbootstrap com template overviews clean blog  and leave a comment  email feedback startbootstrap com  or open an issue here on GitHub for support    TONYDARK IO   How to get setup   If you don t already have Jekyll  run   gem install jekyll   If you don t already have Bundler  run   gem install bundler   If you don t already have the site locally  fork and clone    To get dependencies run from the site directory   bundle install   How to run Jekyll locally   serve only   bundle exec jekyll serve   serve and watch for changes   bundle exec jekyll serve   watch   serve drafts and watch for changes   bundle exec jekyll serve   watch   drafts   How to contribute content     If you are not already in the authors file  add yourself   In a local fork  preferably in a branch off of master  write in a markdown file    md   in the   drafts  folder  optionally committing the work to manage  revisions  and push to your remote if you d like to keep your work  backed up    When ready for review  final or incremental   push the post upstream and open a pull request   note  if you are looking for comments but not ready to publish  keep the post in the   drafts  directory  if you are ready to publish  move the file to the   posts  directory and prefix the filename with the date   2015 09 27 my fence post md   before opening the pull request   Colleagues can then view and comment on the post  potentially with a  diff  of a previous version  if one exists in the  upstream     Once you and the reviewer s  are satisfied  merge the PR  If the content was moved to the   posts  directory in the PR  then you are now  published       References     Jekyll docs   Markdown docs   Github Flow guide   spotify leap hackathon   Repo for Synthesis   Samples Music Hackathon    brew requirements   brew install liblo libsndfile portaudio portmidi   On the Mac  it is very simple to build pyo from source with the Homebrew package mananger            source  https   code google com p pyo wiki Installation   brew install python liblo libsndfile portaudio portmidi cd  tmp svn checkout http   pyo googlecode com svn trunk  pyo cd pyo python setup py install   use coreaudio   use double   universal       A short gist explaining how to compile pyo   the powerful Audio DSP library for Python  on Mac OS X  The instructions are specifically for installing pyo with a brewed Python install   not with Mac OS X system Python          source  https   gist github com pwalsh 5691534   using brew installed Python   brew install portaudio portmidi libsndfile liblo jack   brew link portaudio portmidi libsndfile liblo   cd   Sites tmp   svn checkout http   pyo googlecode com svn trunk  pyo read only   cd pyo read only   either jump into a virtualenv now or create one   CHOOSE    workon spotify leap hackathon   mkvirtualenv spotify leap hackathon   python setup py install   use coreaudio   use double   cd       sudo rm  r pyo read only        Fix for LeapPython so and using homebrew python    install name tool  change  Library Frameworks Python framework Versions 2 7 Python    usr local Cellar python 2 7 9 Frameworks Python framework Versions 2 7 lib libpython2 7 dylib   LeapPython so         Info on static libs installed   Python pyo  version 0 7 5    System requirements   OS X 10 6 to 10 10   This package installs all the required components to run pyo inside your current Python installation  Python 2 6 or 2 7  32 64 bit  must be already installed on your system    This package is divided into two separate installers  If you do not require one of them  please unselect the package in custom installation mode      pyo extension  The following components will be installed in the site packages folder of the current Python Framework       pyo so  pyo64 so pyo py pyo64 py pyolib  folder      Support libraries  i386 and x86 64   This component will install a number of dynamic libraries on which pyo depends  If you already have these  then you can skip this installation      Warning  this installation will overwrite any previously installed libraries  These are the libraries that will be installed in your  usr local lib directory    liblo 7 dylib libportaudio 2 dylib libportmidi dylib libsndfile 1 dylib libFLAC 8 dylib libvorbisenc 2 dylib libvorbis 0 dylib libogg 0 dylib   Olivier B langer  2015        Important for keyboard interaction etc and useful bootstrap steps         source  https   github com openstenoproject plover blob master osx DevReadme txt             Test that pyo is installed correctly etc   from pyo import     s   Server   boot   s start   m   Input chnl 1  mul 2  pva   PVAnal m  size 1024  pvt   PVTranspose pva  transpo 1 5  pvs   PVSynth pvt  out   dry   Delay m  delay 1024  s getSamplingRate    mul  7  out 1        sublime3 user settings   My user settings for sublime text 3 bossjones sphinxbase   Clone of SVN Sphinxbase 0 8 with additions for Scarlett  bossjones cthehardway   Learning some C  bossjones gst plugins espeak   Clone of gst plugins espeak to work with gst 0 10  v0 3 5 drywall   Python wrapper script of common bash commands to find out info on a linux system dogknife    n  A sweet integration between knife and datadog    Datadog  already instruments your  Chef  runs  Now  with  dogknife   instrument your knife commands to know  who  ran  what  and  when     Getting Started     If you are not a Datadog user yet   sign up   Get your  API key  and your Datadog handle    Configure them as  datadog api key  and  datadog user  in your knife configuration file  usually in     chef knife rb     Install  dogknife  with  gem install dogknife   Use  dogknife  where you would use  knife   golang helloworld bossjones dotfiles   My dotfiles and other cool shit   Fonts   Enabling the Powerline look   Powerline originated as a status line plugin for Vim  Its popular eye catching look is based on the use of special symbols     To make use of these symbols  there are several options      use a font that already bundles those  this is e g  the case of the    2 030R ro 1 050R it version  of the Source Code Pro  font   use a  pre patched font   use your preferred font along with the  Powerline font   that   only contains the Powerline symbols    this highly depends on your operating   system and your terminal emulator   patch your preferred font  by adding the missing   Powerline symbols  this is the most difficult way and is no more documented in   the  Powerline manual     Please see the  Powerline manual  for further details    Then edit the     tmux conf local  file    prefix  e   and adjust the following variables        tmux conf theme left separator main     tmux conf theme left separator sub     tmux conf theme right separator main     tmux conf theme right separator sub       Dotfiles to checkout     https   github com giftofjehovah dotfiles blob 8b61dfa30df615667eae7d236076a17729de5604 settings macos sh   https   github com rafeca dotfiles blob 816ffb81dc9ef37a16ce1037dbe121038ac86222 osx apps iterm2 sh   https   github com vbdjames dotfiles blob 853731feaad7c8486f74ee089f32a3674e0b349c iterm install sh   https   github com khoad dotfiles blob c80db09659a590070c16d5942fcbfe8ccabd8e5c setupNewRig sh   https   github com davejacobs dotfiles blob a028a6e9b1f116035dd81ab6acdeee75751eb3f4 bin setup envs osx   gobot io examples opencv python examples   Install OpenCV via  http   www pyimagesearch com 2015 06 15 install opencv 3 0 and python 2 7 on osx  scarlett dbus poc   Scarlett Dbus Listener Service implementation Proof Of Concept    Source  https   www reddit com r gnome comments 3owhp6 python help critique my application design home    How to test scarlett player   Open ipython and type the following        In  1   import scarlett player   In  2   scarlett player ScarlettPlayer  pi listening   run         Simple python dbus example   Source  https   github com stylesuxx python dbus examples       Run dbus monitor and grep for the interesting output    dbus monitor   grep  tld domain sub       Run the receiver       Then you can either   Run the invoker  which will call the proxxied receivers Test object methods    run the emitter  which will emit a test signal and a quit signal    After running either of them  the receiver should be stopped    Vagrant packaging   package the box   vagrant package   base scarlett dbus poc default 1449512156423 85046   output   ubuntu 14 04 base w gst pulseaudio guestadd2 box   NOTE  WE SHOULD TRY RUNNING PACKAGE LIKE THIS   vagrant package   base scarlett dbus poc default 1449512156423 85046   output   ubuntu 14 04 base w gst pulseaudio guestadd3 box   vagrantfile   dev bossjones scarlett dbus poc Vagrantfile   add the box   vagrant box add   name  scarlettpi base ubuntu 14 04 pulse   Users malcolm ubuntu 14 04 base w gst pulseaudio guestadd3 box   reinstalling unity to make desktop work correctly again   http   askubuntu com questions 502224 dash wont launch applications after upgrade to 14 04   new way to install   https   github com cmusphinx pocketsphinx https   github com cmusphinx cmudict https   github com cmusphinx pocketsphinx python https   github com cmusphinx sphinxbase https   github com cmusphinx sphinxtrain https   github com cmusphinx cmudict tools https   github com cmusphinx kaldi   dbus con emit signal   example and breakdown         Important reading    https   wiki gnome org action show Projects PyGObject IntrospectionPorting action show redirect PyGObject IntrospectionPorting   devhelp command on ubuntu 15 10   Assume c code looks like    gboolean g dbus connection emit signal  GDBusConnection  connection                                 const gchar  destination bus name                                 const gchar  object path                                 const gchar  interface name                                 const gchar  signal name                                 GVariant  parameters                                 GError   error         where       Parameters    connection     a GDBusConnection   destination bus name     the unique bus name for the destination for the signal or NULL to emit to all listeners    object path     path of remote object   interface name     D Bus interface to emit a signal on   signal name     the name of the signal to emit   parameters     a GVariant tuple with parameters for the signal or NULL if not passing parameters    error     Return location for error or NULL   Return value     TRUE unless error is set   Then the python version looks like    listener rdy status   GLib Variant   ss     message  scarlett sound   bus emit signal None                    org scarlett Listener                    org scarlett Listener                    ListenerReadySignal                   listener rdy status        Client    Service issue          Currently getting the following issue when proxy tries to connet    method call time 1455504495 425644 sender  1 49    destination org freedesktop DBus serial 1 path  org freedesktop DBus  interface org freedesktop DBus  member Hello method return time 1455504495 425676 sender org freedesktop DBus    destination  1 49 serial 1 reply serial 1    string   1 49  signal time 1455504495 425695 sender org freedesktop DBus    destination  null destination  serial 203 path  org freedesktop DBus  interface org freedesktop DBus  member NameOwnerChanged    string   1 49     string       string   1 49  signal time 1455504495 425726 sender org freedesktop DBus    destination  1 49 serial 2 path  org freedesktop DBus  interface org freedesktop DBus  member NameAcquired    string   1 49  method call time 1455504495 426215 sender  1 4    destination org freedesktop DBus serial 165 path  org freedesktop DBus  interface org freedesktop DBus  member GetConnectionUnixProcessID    string   1 49  method return time 1455504495 426264 sender org freedesktop DBus    destination  1 4 serial 204 reply serial 165    uint32 27637 method call time 1455504495 428239 sender  1 49    destination org freedesktop DBus serial 2 path  org freedesktop DBus  interface org freedesktop DBus  member AddMatch    string  type  signal  sender  org freedesktop DBus  interface  org freedesktop DBus  member  NameOwnerChanged  path   org freedesktop DBus  arg0  org scarlett Listener SttFailedSignal   method return time 1455504495 428292 sender org freedesktop DBus    destination  1 49 serial 3 reply serial 2 method call time 1455504495 428302 sender  1 49    destination org freedesktop DBus serial 3 path  org freedesktop DBus  interface org freedesktop DBus  member GetNameOwner    string  org scarlett Listener SttFailedSignal  error time 1455504495 428320 sender org freedesktop DBus    destination  1 49 error name org freedesktop DBus Error NameHasNoOwner reply serial 3    string  Could not get owner of name  org scarlett Listener SttFailedSignal   no such name  method call time 1455504495 428336 sender  1 49    destination org scarlett serial 4 path  org scarlett  interface org freedesktop DBus Introspectable  member Introspect method return time 1455504495 428346 sender  1 48    destination  1 49 serial 12 reply serial 4    string    DOCTYPE node PUBLIC     freedesktop  DTD D BUS Object Introspection 1 0  EN                         http   www freedesktop org standards dbus 1 0 introspect dtd      GDBus 2 46 2             THIS IS A PROPER SIGNAL EMIT   signal time 1455504580 676030 sender  1 48    destination  null destination  serial 13 path  org scarlett Listener  interface org scarlett Listener SttFailedSignal  member SttFailedSignal    string    ScarlettListener hit Max STT failures     string  pi response2        FYI       In  38   ss   bus get  org scarlett   object path   org scarlett Listener     In  39   ss  ss Get                          ss emitCommandRecognizedSignal ss GetAll                       ss emitConnectedToListener ss GetMachineId                 ss emitKeywordRecognizedSignal ss Introspect                   ss emitListenerCancelSignal ss Message                      ss emitListenerReadySignal ss Ping                         ss emitSttFailedSignal ss Set   In  39   ss  ss Get                          ss emitCommandRecognizedSignal ss GetAll                       ss emitConnectedToListener ss GetMachineId                 ss emitKeywordRecognizedSignal ss Introspect                   ss emitListenerCancelSignal ss Message                      ss emitListenerReadySignal ss Ping                         ss emitSttFailedSignal ss Set   In  39   exit       Use this to figure out why foo     which is suppose to just return a string  returns a   string    instead        In  38   foo client   bus get  net lvht   object path   net lvht Foo     In  39   foo client Out 39      In  40   foo client  foo client Get           foo client GetAll        foo client GetMachineId  foo client HelloWorld    foo client Introspect    foo client Ping          foo client Set   In  40   foo client  foo client Get           foo client GetAll        foo client GetMachineId  foo client HelloWorld    foo client Introspect    foo client Ping          foo client Set   In  40   foo client HelloWorld Out 40        In  41   foo client HelloWorld  hello  1  Out 41     hello1      In  42         Looks like the in out variables look incorrect         MainThread  DEBUG    Inside self method inargs and self method outargs      Message            emitCommandRecognizedSignal     s          emitConnectedToListener     s          emitKeywordRecognizedSignal            emitListenerCancelSignal            emitListenerReadySignal            emitSttFailedSignal            Message     s         emitCommandRecognizedSignal     s         emitConnectedToListener      s          emitKeywordRecognizedSignal     s         emitListenerCancelSignal     s         emitListenerReadySignal     s         emitSttFailedSignal     s      with xml                                                                                                        compared to foo py    MainThread  DEBUG    Inside self method inargs and self method outargs  MainThread  DEBUG    Inside self method inargs      HelloWorld     a    b     MainThread  DEBUG    Inside self method outargs      HelloWorld     s      with XML that looks like                            we are finally closer to getting signals read in and acted upon          GStreamer 1 6 0       MainThread  DEBUG    ss PrettyPrinter     MainThread  DEBUG    player cb PrettyPrinter               1 88         org scarlett Listener        org scarlett Listener        SttFailedSignal       GLib Variant   ss        ScarlettListener hit Max STT failures    pi response2         ValueError                                Traceback  most recent call last   home pi dev bossjones github scarlett dbus poc test gdbus proxy service py in player cb  args       1 88     org scarlett Listener    org scarlett Listener    SttFailedSignal   GLib Variant   ss        ScarlettListener hit Max STT failures    pi response2        kwargs          99     pp   pprint PrettyPrinter indent 4      100     pp pprint args      101     msg  scarlett sound   args         msg   undefined         scarlett sound   undefined         args         1 88     org scarlett Listener    org scarlett Listener    SttFailedSignal   GLib Variant   ss        ScarlettListener hit Max STT failures    pi response2         102     logger warning   msg      format msg       103     logger warning   scarlett sound      format scarlett sound     ValueError  too many values to unpack      home pi dev bossjones github scarlett dbus poc test gdbus proxy service py 101 player cb        99     pp   pprint PrettyPrinter indent 4      100     pp pprint args      101     msg  scarlett sound   args     102     logger warning   msg      format msg       103     logger warning   scarlett sound      format scarlett sound       ipdb        Got signal emit   callback working  player cb   command cb       Service  python test gdbus simple service py   Tasker  python test gdbus proxy service py       Got full emit   subscribe  02 21 2016    No Threads   No Player   Speaker yet       Service  python test gdbus service py   Tasker  python test gdbus proxy service py       Beginning threading tasker fixes 3 6 2016   Got Proper Player working in generator player py  04 17 2016    Properly using Threading RLocks   Semaphores to open wav files  play  stop  then clean up   CMD  python generator player py   Beginning work on generator speaker py and generator listener py  04 17 2016  Threading RLocks   Semaphores   TODO       X  Listener  Good to go        Tasker       Threading       Calls to Player       Calls to Speaker       Calls to Features       Player       Speaker       Brain       Features       Forecast       GstUtils       Timer       Wordnik       Stocks       News       Lights       Music       TV       Sound       Blinds     Investigating Generators and No Op functions in pygobject threading   3 13 2016       Service  python test gdbus service py   Tasker  python FooThreadDbus py     scarlett ansible   Audio on ubuntu 16 04 links   https   help ubuntu com community SoundTroubleshootingProcedure http   askubuntu com questions 774458 installed lubuntu 16 04 version no audio now https   musescore org en node 107601 https   ubuntuforums org showthread php t 2321631   Description   The purpose of this repository is to provision a server with all required Scarlett dependencies via ansible  When finished  we can then verify that everything is in place correctly by using serverspec as a integration testing framework  This repo will also serve as a brain dump for any installation issues we come across while attempting to get GTK 3  Gstreamer 1 0  PyGi  Pocketsphinx 5prealpha  PulseAudio  Alsa and Python to play nicely together  As you can see from the list of dependices  there are A LOT of things to take into consideration when configuring this system properly     the fact that I did not create this earlier baffles my mind    Hardware Requirements     Microphone  4 of Kinobo   USB 2 0 Mini Microphone  Makio  Mic for Laptop Desktop PCs   Skype   VOIP   Voice Recognition Software           NOTE this particular mic is not required  it s just the one i m using  some values are HARDCODED to this  EG  My mic s name is  USB PnP Sound Device    NOTE THIS IS WRONG      master U 3   6      cat  proc asound cards  0  I82801AAICH       ICH   Intel 82801AA ICH                       Intel 82801AA ICH with AD1980 at irq 21  1  Device            USB Audio   USB PnP Sound Device                       C Media Electronics Inc  USB PnP Sound Device at usb 0000 00 1f 4 1  full speed         Software Requirements   NOTE  Everything is currently being tested on a Ubuntu 14 04 3 LTS Virtualbox Machine on my Macbook Pro   On Macbook Pro    Homebrew   Ruby   via rbenv     Ansible  v1 9 4    fswatch   installed via homebrew    We will use this to sync files we re working on locally    Setup serverspec   Just like how Python recomends the use of virtual environments  we don t want to install any rubygems system wide   serverspec   a ruby gem  can be installed locally in a   vendor  directory  Simply do the following          install bundler   gem install bundler   install serverspec and other dependencies   bundle install   path  vendor   run serverspec tests via rake  similar to make but for ruby   bundle exec rake       How to Provision a Vagrant box         NOTE  I wrapped commands w  Make mainly due to some weird things I did with this setup  Will clean this up later    bring up virtualbox   make vagrant up   provision runs ansible   make vagrant provision   ssh on to server   make vagrant ssh   stop server   make vagrant halt       How to Provision Scarlett on another machine   Coming soon      Helpful links     http   cmusphinx sourceforge net wiki installingpythonstuff   https   github com feanil raspi video sync   https   gist github com GiorgioNatili ce5647a79ce88cf73a8a   http   cmusphinx sourceforge net wiki  advanced user documentation   http   sourceforge net p cmusphinx discussion search  q Pocketsphinx listen for grammar   http   sourceforge net p cmusphinx discussion help thread a7822390  limit 25 8f38 f5b8 bc40   https   wolfpaulus com journal embedded raspberrypi2 sr    http   blog scphillips com posts 2013 01 getting gstreamer to work on a raspberry pi      https   wiki debian org PulseAudio       https   github com spk121 burro engine     http   cmusphinx sourceforge net wiki download   http   cmusphinx sourceforge net wiki faq qwhy my accuracy is poor   https   github com cmusphinx pocketsphinx issues 25   https   blogs gnome org uraeus 2011 10 04 tutorial for python gstreamer and gtk 3    https   developer ridgerun com wiki index php GStreamer Debugging   https   developer ridgerun com wiki index php title Embedded GStreamer Performance Tuning   http   docs gstreamer com display GstSDK Basic tutorial 11 3A Debugging tools   http   wiki buzztrax org index php Debugging   http   gstreamer freedesktop org data doc gstreamer head gstreamer html gst running html   http   gstreamer freedesktop org data doc gstreamer head manual html section checklist debug html   https   gramps project org wiki index php title GEPS 029  GTK3 GObject introspection Conversion   https   wiki archlinux org index php PulseAudio Examples   https   wiki archlinux org index php PulseAudio Configuration   http   trac gateworks com wiki Yocto gstreamer audio   http   www alsa project org alsa doc alsa lib pcm html   https   www raspberrypi org forums viewtopic php f 38 t 37873  set alsa and save settings   https   github com ozzyjohnson ansible ffmpeg build blob master build ffmpeg yml  good example of how to compile from source in ansible   http   cmusphinx sourceforge net wiki raspberrypi  getting pocketsphinx working on raspbian   need to add ansible commands for that     https   wolfpaulus com journal embedded raspberrypi2 sr   NOTE  lets update steps    see following      Vagrant commands   start vagrant server   VAGRANT VAGRANTFILE Vagrantfile ansible test vagrant up   provision server   VAGRANT VAGRANTFILE Vagrantfile ansible test vagrant provision   Make commands   help list serverspec serverspec diff serverspec install vagrant destroy vagrant halt vagrant provision vagrant up   Speech Recognition w  pocketsphinx   Corpus   See file    language files base scarlett corpus   which contains a list of words sentences seperated by new lines that we want recognized by pocketsphinx          Example Corpus   SCARLETT TURN ON THE LIGHTS TURN OFF THE LIGHTS CHANGE THE LIGHTS BLUE CHANGE THE LIGHTS GREEN CHANGE THE LIGHTS RED CHANGE THE LIGHTS WHITE WHATS THE WEATHER WHAT TIME IS IT CHANGE THE TV TO MTV CHANGE THE TV TO HBO TURN THE LIGHTS BRIGHTER TURN THE LIGHTS DARKER SEXY TIME CANCEL NEGATIVE STOP GO LEFT RIGHT UP DOWN FORWARD BACKWARD FRIZZY TV APPLE TOSHIBA UP DOWN LEFT RIGHT MENU PAUSE PLAY CIRCLE BUTTON CHANNEL UP CHANNEL DOWN VOLUME UP VOLUME DOWN MUTE RECALL INPUT SELECT UP SELECT LEFT SELECT RIGHT SELECT ENTER ONE TWO THREE FOUR FIVE SIX SEVEN EIGHT NINE ZERO POWER GET LIGHT NAMES GET LIGHT IDS WHAT ARE THE NAMES OF MY LIGHTS GIVE ME LIGHT NAMES CLOSE THE SHADES TURN ON BATHROOM LIGHTS TURN ON WINDOW LIGHTS TURN ON FAMILY ROOM LIGHTS TURN ON LIGHTS IN BATHROOM TURN ON LIGHTS IN FAMILY ROOM TURN ON LIGHTS IN HALLWAY TURN ON ALL LIGHTS         Generating new  lm and  dict files from your base corpus   MANUALLY   Upload the text file here  http   www speech cs cmu edu tools lmtool new html and then download the generated Pronunciation Dictionary and Language Model    AUTOMATED    Simply run    generate language pocketsphinx sh   This will automatically upload your corpus to http   www speech cs cmu edu tools lmtool new html and download the   lm  and   dict  files that get created into    language files lang    Their format will look something like  1473 dic  and  1473 lm   When you finish running the command  you will need to update your   scarlett  config file  It will also write a file locally called  LANGUAGE VERSION txt  which will have an up to date   generated by lmtool  eg  1473     For the the text file mentioned above  this is what the tool generates    1473 lm    ALL AO L APPLE AE P AH L ARE AA R ARE 2   ER BACKWARD  B AE K W ER D BATHROOM  B AE TH R UW M BLUE  B L UW BRIGHTER  B R AY T ER BUTTON  B AH T AH N CANCEL  K AE N S AH L CHANGE  CH EY N JH CHANNEL CH AE N AH L CIRCLE  S ER K AH L CLOSE K L OW S CLOSE 2   K L OW Z DARKER  D AA R K ER DOWN  D AW N EIGHT EY T ENTER EH N T ER ENTER 2   EH N ER FAMILY  F AE M AH L IY FAMILY 2  F AE M L IY FIVE  F AY V FORWARD F AO R W ER D FOUR  F AO R FRIZZY  F R IH Z IY GET G EH T GET 2   G IH T GIVE  G IH V GO  G OW GREEN G R IY N HALLWAY HH AO L W EY HBO EY CH B IY OW IDS AY D IY Z IDS 2   IH D Z IN  IH N INPUT IH N P UH T IS  IH Z IT  IH T LEFT  L EH F T LIGHT L AY T LIGHTS  L AY T S ME  M IY MENU  M EH N Y UW MTV EH M T IY V IY MUTE  M Y UW T MY  M AY NAMES N EY M Z NEGATIVE  N EH G AH T IH V NINE  N AY N OF  AH V OFF AO F ON  AA N ON 2  AO N ONE W AH N ONE 2   HH W AH N PAUSE P AO Z PLAY  P L EY POWER P AW ER RECALL  R IY K AO L RECALL 2  R IH K AO L RED R EH D RIGHT R AY T ROOM  R UW M SCARLETT  S K AA R L IH T SELECT  S AH L EH K T SEVEN S EH V AH N SEXY  S EH K S IY SHADES  SH EY D Z SIX S IH K S STOP  S T AA P THE DH AH THE 2   DH IY THREE TH R IY TIME  T AY M TO  T UW TO 2  T IH TO 3  T AH TOSHIBA T OW SH IY B AH TURN  T ER N TV  T IY V IY TV 2  T EH L AH V IH ZH AH N TWO T UW UP  AH P VOLUME  V AA L Y UW M WEATHER W EH DH ER WHAT  W AH T WHAT 2  HH W AH T WHATS W AH T S WHATS 2   HH W AH T S WHITE W AY T WHITE 2   HH W AY T WINDOW  W IH N D OW ZERO  Z IY R OW   language model        Language model created by QuickLM on Sat Jan  2 12 51 32 EST 2016 Copyright  c  1996 2010 Carnegie Mellon University and Alexander I  Rudnicky   The model is in standard ARPA format  designed by Doug Paul while he was at MITRE    The code that was used to produce this language model is available in Open Source  Please visit http   www speech cs cmu edu tools  for more information   The  fixed  discount mass is 0 5  The backoffs are computed using the ratio method  This model based on a corpus of 70 sentences and 80 words    data  ngram 1 80 ngram 2 155 ngram 3 128    1 grams   0 9301   0 3010  0 9301    0 2393  2 7752 ALL  0 2892  2 7752 APPLE  0 2468  2 7752 ARE  0 2915  2 7752 BACKWARD  0 2468  2 4742 BATHROOM  0 2334  2 7752 BLUE  0 2468  2 7752 BRIGHTER  0 2468  2 7752 BUTTON  0 2468  2 7752 CANCEL  0 2468  1 9971 CHANGE  0 2915  2 4742 CHANNEL  0 2944  2 7752 CIRCLE  0 3003  2 7752 CLOSE  0 2915  2 7752 DARKER  0 2468  2 1732 DOWN  0 2468  2 7752 EIGHT  0 2468  2 7752 ENTER  0 2468  2 4742 FAMILY  0 2996  2 7752 FIVE  0 2468  2 7752 FORWARD  0 2468  2 7752 FOUR  0 2468  2 7752 FRIZZY  0 2468  2 4742 GET  0 2988  2 7752 GIVE  0 3003  2 7752 GO  0 2468  2 7752 GREEN  0 2468  2 7752 HALLWAY  0 2468  2 7752 HBO  0 2468  2 7752 IDS  0 2468  2 2981 IN  0 2974  2 7752 INPUT  0 2468  2 7752 IS  0 3003  2 7752 IT  0 2468  2 2981 LEFT  0 2468  2 2981 LIGHT  0 2981  1 5711 LIGHTS  0 2393  2 7752 ME  0 2988  2 7752 MENU  0 2468  2 7752 MTV  0 2468  2 7752 MUTE  0 2468  2 7752 MY  0 2892  2 2981 NAMES  0 2459  2 7752 NEGATIVE  0 2468  2 7752 NINE  0 2468  2 7752 OF  0 3003  2 7752 OFF  0 2915  1 8722 ON  0 2747  2 7752 ONE  0 2468  2 7752 PAUSE  0 2468  2 7752 PLAY  0 2468  2 7752 POWER  0 2468  2 7752 RECALL  0 2468  2 7752 RED  0 2468  2 2981 RIGHT  0 2468  2 4742 ROOM  0 2334  2 7752 SCARLETT  0 2468  2 1732 SELECT  0 2922  2 7752 SEVEN  0 2468  2 7752 SEXY  0 2996  2 7752 SHADES  0 2468  2 7752 SIX  0 2468  2 7752 STOP  0 2468  1 6613 THE  0 2832  2 7752 THREE  0 2468  2 4742 TIME  0 2459  2 4742 TO  0 2996  2 7752 TOSHIBA  0 2468  1 7339 TURN  0 2847  2 2981 TV  0 2451  2 7752 TWO  0 2468  2 0763 UP  0 2468  2 4742 VOLUME  0 2944  2 7752 WEATHER  0 2468  2 4742 WHAT  0 2988  2 7752 WHATS  0 2915  2 7752 WHITE  0 2468  2 7752 WINDOW  0 2892  2 7752 ZERO  0 2468    2 grams   2 1461   APPLE 0 0000  2 1461   BACKWARD 0 0000  2 1461   CANCEL 0 0000  1 3680   CHANGE 0 0000  1 8451   CHANNEL 0 0000  2 1461   CIRCLE 0 0000  2 1461   CLOSE 0 0000  1 8451   DOWN 0 0000  2 1461   EIGHT 0 0000  2 1461   FIVE 0 0000  2 1461   FORWARD 0 0000  2 1461   FOUR 0 0000  2 1461   FRIZZY 0 0000  1 8451   GET 0 0000  2 1461   GIVE 0 0000  2 1461   GO 0 0000  2 1461   INPUT 0 0000  1 8451   LEFT 0 0000  2 1461   MENU 0 0000  2 1461   MUTE 0 0000  2 1461   NEGATIVE 0 0000  2 1461   NINE 0 0000  2 1461   ONE 0 0000  2 1461   PAUSE 0 0000  2 1461   PLAY 0 0000  2 1461   POWER 0 0000  2 1461   RECALL 0 0000  1 8451   RIGHT 0 0000  2 1461   SCARLETT 0 0000  1 5441   SELECT 0 0000  2 1461   SEVEN 0 0000  2 1461   SEXY 0 0000  2 1461   SIX 0 0000  2 1461   STOP 0 0000  2 1461   THREE 0 0000  2 1461   TOSHIBA 0 0000  1 1047   TURN 0 0000  2 1461   TV  0 2218  2 1461   TWO 0 0000  1 8451   UP 0 0000  1 8451   VOLUME 0 0000  1 8451   WHAT 0 0000  2 1461   WHATS 0 0000  2 1461   ZERO 0 0000  0 3010 ALL LIGHTS  0 1938  0 3010 APPLE    0 3010  0 3010 ARE THE  0 2840  0 3010 BACKWARD    0 3010  0 6021 BATHROOM    0 3010  0 6021 BATHROOM LIGHTS  0 1938  0 3010 BLUE    0 3010  0 3010 BRIGHTER    0 3010  0 3010 BUTTON    0 3010  0 3010 CANCEL    0 3010  0 3010 CHANGE THE  0 0902  0 6021 CHANNEL DOWN 0 0000  0 6021 CHANNEL UP 0 0000  0 3010 CIRCLE BUTTON 0 0000  0 3010 CLOSE THE  0 2840  0 3010 DARKER    0 3010  0 3010 DOWN    0 3010  0 3010 EIGHT    0 3010  0 3010 ENTER    0 3010  0 3010 FAMILY ROOM 0 0000  0 3010 FIVE    0 3010  0 3010 FORWARD    0 3010  0 3010 FOUR    0 3010  0 3010 FRIZZY    0 3010  0 3010 GET LIGHT 0 0000  0 3010 GIVE ME 0 0000  0 3010 GO    0 3010  0 3010 GREEN    0 3010  0 3010 HALLWAY    0 3010  0 3010 HBO    0 3010  0 3010 IDS    0 3010  0 7782 IN BATHROOM  0 1761  0 7782 IN FAMILY 0 0000  0 7782 IN HALLWAY 0 0000  0 3010 INPUT    0 3010  0 3010 IS IT 0 0000  0 3010 IT    0 3010  0 3010 LEFT    0 3010  0 7782 LIGHT IDS 0 0000  0 4771 LIGHT NAMES  0 1249  0 6601 LIGHTS    0 3010  1 5051 LIGHTS BLUE 0 0000  1 5051 LIGHTS BRIGHTER 0 0000  1 5051 LIGHTS DARKER 0 0000  1 5051 LIGHTS GREEN 0 0000  1 0280 LIGHTS IN 0 0000  1 5051 LIGHTS RED 0 0000  1 5051 LIGHTS WHITE 0 0000  0 3010 ME LIGHT  0 1249  0 3010 MENU    0 3010  0 3010 MTV    0 3010  0 3010 MUTE    0 3010  0 3010 MY LIGHTS  0 1938  0 4771 NAMES    0 3010  0 7782 NAMES OF 0 0000  0 3010 NEGATIVE    0 3010  0 3010 NINE    0 3010  0 3010 OF MY 0 0000  0 3010 OFF THE  0 1413  1 2041 ON ALL 0 0000  1 2041 ON BATHROOM  0 1761  1 2041 ON FAMILY 0 0000  0 7270 ON LIGHTS  0 2583  1 2041 ON THE  0 1413  1 2041 ON WINDOW 0 0000  0 3010 ONE    0 3010  0 3010 PAUSE    0 3010  0 3010 PLAY    0 3010  0 3010 POWER    0 3010  0 3010 RECALL    0 3010  0 3010 RED    0 3010  0 3010 RIGHT    0 3010  0 6021 ROOM    0 3010  0 6021 ROOM LIGHTS  0 1938  0 3010 SCARLETT    0 3010  0 9031 SELECT ENTER 0 0000  0 9031 SELECT LEFT 0 0000  0 9031 SELECT RIGHT 0 0000  0 9031 SELECT UP 0 0000  0 3010 SEVEN    0 3010  0 3010 SEXY TIME  0 1761  0 3010 SHADES    0 3010  0 3010 SIX    0 3010  0 3010 STOP    0 3010  0 5119 THE LIGHTS  0 0746  1 4150 THE NAMES  0 2218  1 4150 THE SHADES 0 0000  1 1139 THE TV  0 1249  1 4150 THE WEATHER 0 0000  0 3010 THREE    0 3010  0 6021 TIME   0 3010  0 6021 TIME IS 0 0000  0 6021 TO HBO 0 0000  0 6021 TO MTV 0 0000  0 3010 TOSHIBA   0 3010  1 3424 TURN OFF 0 0000  0 4393 TURN ON 0 0000  1 0414 TURN THE  0 1413  0 7782 TV   0 3010  0 4771 TV TO 0 0000  0 3010 TWO   0 3010  0 3010 UP   0 3010  0 6021 VOLUME DOWN 0 0000  0 6021 VOLUME UP 0 0000  0 3010 WEATHER   0 3010  0 6021 WHAT ARE 0 0000  0 6021 WHAT TIME  0 1761  0 3010 WHATS THE  0 2840  0 3010 WHITE   0 3010  0 3010 WINDOW LIGHTS  0 1938  0 3010 ZERO   0 3010    3 grams   0 3010   APPLE    0 3010   BACKWARD    0 3010   CANCEL    0 3010   CHANGE THE  0 6021   CHANNEL DOWN  0 6021   CHANNEL UP  0 3010   CIRCLE BUTTON  0 3010   CLOSE THE  0 3010   DOWN    0 3010   EIGHT    0 3010   FIVE    0 3010   FORWARD    0 3010   FOUR    0 3010   FRIZZY    0 3010   GET LIGHT  0 3010   GIVE ME  0 3010   GO    0 3010   INPUT    0 3010   LEFT    0 3010   MENU    0 3010   MUTE    0 3010   NEGATIVE    0 3010   NINE    0 3010   ONE    0 3010   PAUSE    0 3010   PLAY    0 3010   POWER    0 3010   RECALL    0 3010   RIGHT    0 3010   SCARLETT    0 9031   SELECT ENTER  0 9031   SELECT LEFT  0 9031   SELECT RIGHT  0 9031   SELECT UP  0 3010   SEVEN    0 3010   SEXY TIME  0 3010   SIX    0 3010   STOP    0 3010   THREE    0 3010   TOSHIBA    1 3424   TURN OFF  0 4393   TURN ON  1 0414   TURN THE  0 3010   TV    0 3010   TWO    0 3010   UP    0 6021   VOLUME DOWN  0 6021   VOLUME UP  0 6021   WHAT ARE  0 6021   WHAT TIME  0 3010   WHATS THE  0 3010   ZERO    0 3010 ALL LIGHTS    0 3010 ARE THE NAMES  0 3010 BATHROOM LIGHTS    0 4771 CHANGE THE LIGHTS  0 7782 CHANGE THE TV  0 3010 CHANNEL DOWN    0 3010 CHANNEL UP    0 3010 CIRCLE BUTTON    0 3010 CLOSE THE SHADES  0 6021 FAMILY ROOM    0 6021 FAMILY ROOM LIGHTS  0 6021 GET LIGHT IDS  0 6021 GET LIGHT NAMES  0 3010 GIVE ME LIGHT  0 3010 IN BATHROOM    0 3010 IN FAMILY ROOM  0 3010 IN HALLWAY    0 3010 IS IT    0 3010 LIGHT IDS    0 3010 LIGHT NAMES    0 3010 LIGHTS BLUE    0 3010 LIGHTS BRIGHTER    0 3010 LIGHTS DARKER    0 3010 LIGHTS GREEN    0 7782 LIGHTS IN BATHROOM  0 7782 LIGHTS IN FAMILY  0 7782 LIGHTS IN HALLWAY  0 3010 LIGHTS RED    0 3010 LIGHTS WHITE    0 3010 ME LIGHT NAMES  0 3010 MY LIGHTS    0 3010 NAMES OF MY  0 3010 OF MY LIGHTS  0 3010 OFF THE LIGHTS  0 3010 ON ALL LIGHTS  0 3010 ON BATHROOM LIGHTS  0 3010 ON FAMILY ROOM  0 3010 ON LIGHTS IN  0 3010 ON THE LIGHTS  0 3010 ON WINDOW LIGHTS  0 3010 ROOM LIGHTS    0 3010 SELECT ENTER    0 3010 SELECT LEFT   0 3010 SELECT RIGHT   0 3010 SELECT UP   0 3010 SEXY TIME   0 9031 THE LIGHTS   1 2041 THE LIGHTS BLUE  1 2041 THE LIGHTS BRIGHTER  1 2041 THE LIGHTS DARKER  1 2041 THE LIGHTS GREEN  1 2041 THE LIGHTS RED  1 2041 THE LIGHTS WHITE  0 3010 THE NAMES OF  0 3010 THE SHADES   0 3010 THE TV TO  0 3010 THE WEATHER   0 3010 TIME IS IT  0 3010 TO HBO   0 3010 TO MTV   0 3010 TURN OFF THE  1 2041 TURN ON ALL  1 2041 TURN ON BATHROOM  1 2041 TURN ON FAMILY  0 7270 TURN ON LIGHTS  1 2041 TURN ON THE  1 2041 TURN ON WINDOW  0 3010 TURN THE LIGHTS  0 6021 TV TO HBO  0 6021 TV TO MTV  0 3010 VOLUME DOWN   0 3010 VOLUME UP   0 3010 WHAT ARE THE  0 3010 WHAT TIME IS  0 3010 WHATS THE WEATHER  0 3010 WINDOW LIGHTS     end        Test out pocketsphinx continuous after compiling   This video shows the recognizer running in keyword spotting mode  using the dictionary and model mentioned above    pocketsphinx continuous  lm  home pi dev bossjones github scarlett static speech lm 1602 lm  dict  home pi dev bossjones github scarlett static speech dict 1602 dic  keyphrase  SCARLETT   kws threshold 1e 20  inmic yes   The purpose is to provide some indication of the recognition speed that can be expected  running PocketSphinx on the Raspberry Pi 2    Good question from ColinW re  Alsa   ColinW  on November 14  2015 at 8 56 pm Great article  Question on how to change the card index on the new Raspbian Jessie  In Jessie  there is no longer  alsa base conf  to set  options snd usb audio index 0   As best as I can figure  the new config file is now at   usr share alsa alsa conf   Do you know how I would be able to do it there  Alternatively  is there a way to make PocketSphinx use the mic at index 1    Is this bug still valid     https   bugs tizen org jira browse TC 664   NOTE    If there is ever concern about env variables messing up how scarlett runs  make sure these are either set unset  which are defined in postactivate and postdeactivate    export GSTREAMER 1 0 export PI HOME  home pi export MAIN DIR  PI HOME dev bossjones github scarlett dbus poc export VIRT ROOT  PI HOME  virtualenvs scarlett dbus poc export PKG CONFIG PATH  VIRT ROOT lib pkgconfig export SCARLETT CONFIG  MAIN DIR tests fixtures  scarlett export SCARLETT HMM  MAIN DIR tests fixtures model hmm en US hub4wsj sc 8k export SCARLETT LM  MAIN DIR tests fixtures lm 1602 lm export SCARLETT DICT  MAIN DIR tests fixtures dict 1602 dic   LINKS TO HELP WITH TESTING YOUR MICROPHONE     https   www onlinemictest com    http   kinobo co uk support index php sid 126171 lang en action artikel cat 3 id 12 artlang en   http   kinobo co uk support index php action artikel cat 4 id 18 artlang en   http   kinobo co uk support index php action artikel cat 4 id 14 artlang en   http   www kinobo co uk     https   wiki archlinux org index php PulseAudio Configuration   http   apple stackexchange com questions 170105 list usb devices on osx command line   http   www alsa project org main index php Asoundrc     VIRTUALBOX make sure guest can see usb mics   MAC OS X ONLY    Run this on your mac   ioreg  p IOUSB  w0  l   This will produce something like        ioreg  p IOUSB  w0   sed  s   o  o     s           grep  v   Root    XHCI Root Hub SS Simulation XHCI Root Hub USB 2 0 Simulation USB PnP Sound Device EHCI Root Hub Simulation HubDevice FaceTime HD Camera  Built in  EHCI Root Hub Simulation HubDevice HubDevice Apple Internal Keyboard   Trackpad BRCM20702 Hub Bluetooth USB Host Controller   DEBUGGING Gstreamer and PulseAudio   Figure out if Gstreamer is configued to use pulseaudio          master U 3   3      gconftool 2   recursive list  system gstreamer   system gstreamer 0 10     system gstreamer 0 10 default     musicaudiosink description   Default    audiosrc   autoaudiosrc    audiosrc description   Default    chataudiosink description   Default    musicaudiosink   autoaudiosink    audiosink description   Default    visualization   goom    videosrc   v4l2src    audiosink   autoaudiosink    chataudiosink   autoaudiosink    videosink   autovideosink   system gstreamer 1 0     system gstreamer 1 0 default     audiosink   pulsesink    audiosrc   pulsesrc    system gstreamer 1 0 audio      system gstreamer 1 0 audio profiles       system gstreamer 1 0 audio profiles mp3       name   CD Quality  MP3      extension   mp3      pipeline   audio x raw rate 44100 channels 2   lamemp3enc name enc target 0 quality 6   xingmux   id3v2mux      description   Used for converting to CD quality audio  but with the lossy MP3 codec  Use this for preparing files for copying to devices that only support the MP3 codec  Note that using this format may be illegal in your jurisdiction  contact your lawyer for advice       active   true      system gstreamer 1 0 audio profiles cdlossless       name   CD Quality  Lossless      extension   flac      pipeline   audio x raw rate 44100 channels 2   flacenc name enc      description   Used for converting to CD quality audio  but with a lossless compression codec  Use this if you later want to edit the file or burn it to CD       active   true      system gstreamer 1 0 audio profiles mp2       name   CD Quality  MP2      extension   mp2      pipeline   audio x raw rate 44100 channels 2   twolame name enc mode 0 bitrate 192   id3v2mux      description   Used for converting to CD quality audio  but with the lossy MP2 codec  Use this for preparing files for copying to devices that only support the MP2 codec  Note that using this format may be illegal in your jurisdiction  contact your lawyer for advice       active   true      system gstreamer 1 0 audio profiles aac       name   CD Quality  AAC      extension   m4a      pipeline   audio x raw rate 44100 channels 2   faac profile 2   ffmux mp4      description   Used for converting to CD quality audio  but with the lossy AAC codec  Use this for preparing files for copying to devices that only support the AAC codec  Note that using this format may be illegal in your jurisdiction  contact your lawyer for advice       active   true      system gstreamer 1 0 audio profiles voicelossless       name   Voice  Lossless      extension   wav      pipeline   audio x raw rate 22050 channels 1   wavenc name enc      description   Used for converting to lossless voice quality audio  Use this for recording and editing speech       active   true      system gstreamer 1 0 audio profiles voicelossy       name   Voice  Lossy      extension   spx      pipeline   audio x raw rate 32000 channels 1   speexenc name enc   oggmux      description   Used for converting to lossy voice quality audio  Use this for recording speech that doesn t need to be edited       active   true      system gstreamer 1 0 audio profiles cdlossy       name   CD Quality  Lossy      extension   ogg      pipeline   audio x raw rate 44100 channels 2   vorbisenc name enc quality 0 5   oggmux      description   Used for converting to CD quality audio  but with a lossy compression codec  Use this for CD extraction and radio recordings       active   true     system gstreamer 1 0 audio global      profile list    cdlossy cdlossless aac mp2 mp3 voicelossy voicelossless    using virtualenv  scarlett dbus poc scarlett ansible in   dev bossjones github scarlett gstreamer pocketsphinx demo    master U 3   3            source   https   twitter com andolamin status 661050200614502400   gconftool 2  t string   set  system gstreamer 1 0 default audiosink pulsesink         modified with 1 0   gconftool 2  t string   set  system gstreamer 1 0 default audiosink pulsesink gconftool 2  t string   set  system gstreamer 1 0 default audiosrc pulsesrc gconftool 2  t string   set  system gstreamer 1 0 default musicaudiosink pulsesink   source  http   blog scphillips com posts 2013 01 getting gstreamer to work on a raspberry pi    check the keys are there       master U 3   3      gconftool 2  a  system gstreamer 1 0 default  musicaudiosink   pulsesink  audiosink   pulsesink  audiosrc   pulsesrc             OLD WAY OF DOING THIS gstreamer 0 10   gconftool  t string   set  system gstreamer 0 10 default audiosink pulsesink gconftool  t string   set  system gstreamer 0 10 default audiosrc pulsesrc       Source     http   www alsa project org main index php Asoundrc   The numbers after hw  stand for the soundcard number and device number  This can get confusing as some sound  cards  are better represented by calling them sound  devices   for example USB sounddevices  However they are still  cards  in the sense that they have a specific driver controlling a specific piece of hardware  They also correspond to the index shown in  proc asound cards As with most arrays the first item usually starts at 0 not 1  This is true for the way pcm devices  physical I O channels  are represented in ALSA  Starting at pcm0c  capture   pcm0p  playback   We use subdevices mainly for hardware which can mix several streams together  It is impractical to have 32 devices with exactly the same capabilities  The subdevices can be opened without a specific address  so the first free subdevice is opened  Also  we temporarily use subdevices for hardware with a lot of streams  I O connectors    for example MIDI  There are several limits given by used minor numbers  8 PCM devices per card  8 MIDI devices per card etc    For example  to access the first device on the first soundcard device  you would use hw 0 0 to access the first device on the second soundcard device  you would use hw 1 0 to access the second device on the third soundcard device  you would use hw 2 1   Generic bash magic    echo  libpulse dev libpulse mainloop glib0 libpulse mainloop glib0 dbg libpulse0 libpulse0 dbg libsox fmt pulse paman paprefs pavucontrol pavumeter pulseaudio pulseaudio dbg pulseaudio esound compat pulseaudio esound compat dbg pulseaudio module bluetooth pulseaudio module gconf pulseaudio module jack pulseaudio module lirc pulseaudio module lirc dbg pulseaudio module x11 pulseaudio module zeroconf pulseaudio module zeroconf dbg pulseaudio utils pavucontrol avahi daemon libtheora dev libogg dev libvorbis dev libasound2 dev libjack dev    sed  s     g    sed  s     g          COMMANDS THAT FINALLY LET ME RECORD W  PULSE AUDIO FROM COMMANDLINE     pacmd  load module module alsa source source name input device hw 1      pacmd  set default source input      pactl stat   Currently in use  1 blocks containing 63 9 KiB bytes total    Allocated during whole lifetime  46233 blocks containing 74 5 MiB bytes total    Sample cache size  0 B   Server String   run user 1000 pulse native   Library Protocol Version  28   Server Protocol Version  28   Is Local  yes   Client Index  30   Tile Size  65472   User Name  pi   Host Name  scarlett ansible   Server Name  pulseaudio   Server Version  4 0   Default Sample Specification  s16le 2ch 44100Hz   Default Channel Map  front left front right   Default Sink  alsa output pci 0000 00 05 0 analog stereo   Default Source  input   Cookie  008c 872c     parecord  d input outfile wav     aplay outfile wav   IMPORTANT NOTE  IF DEFAULT SOURCE ISNT SET TO HW 1  THIS DOES NOT WORK          blog post notes       UPGRADING FROM POCKETSPHINX 0 8    5prealpha   what is alsa   what is pulse audio   why do we need to configure both   setting up alsa   setting up pulseaudio   other sound frameworks   Gstreamer     how to configure it   testing alsa works correctly w  arecord and aplay   testing pulseaduio works with parecord and paplay   pocketsphinx install   finally running pocketsphinx continuous   TERMS   PCM   Pulse code modulation  PCM  is a method used to digitally represent sampled analog signals  It is the standard form of digital audio in computers  Compact Discs  digital telephony and other digital audio applications        set usb to default sound device             To configure the adapter to be card 0  I changed the following line in  etc modprobe d alsa base conf from options snd usb audio index  2 to options snd usb audio index 0       REBOOT       with  proc asound cards now showing the adapter as the default  card 0   I m able to record audio from the mic input with  arecord  d 5  r 48000 test wav  and play it back through the headset output with  aplay test wav   I used alsamixer to adjust the Speaker output level and the Capture input level        NOTE THIS IS WRONG      master U 3   6      cat  proc asound cards  0  I82801AAICH       ICH   Intel 82801AA ICH                       Intel 82801AA ICH with AD1980 at irq 21  1  Device            USB Audio   USB PnP Sound Device                       C Media Electronics Inc  USB PnP Sound Device at usb 0000 00 1f 4 1  full speed   NOTE THIS IS RIGHT      master U 3   6      cat  proc asound cards  0  Device            USB Audio   USB PnP Sound Device                       C Media Electronics Inc  USB PnP Sound Device at usb 0000 00 1f 4 1  full speed  1  I82801AAICH       ICH   Intel 82801AA ICH                       Intel 82801AA ICH with AD1980 at irq 21       GSTREAMER 1 0   POCKETSPHINX 5prealpha WORKING IN a gst launch 1 0 pipeline         using virtualenv  scarlett dbus poc scarlett ansible in   dev bossjones github scarlett gstreamer pocketsphinx demo    master U 3   12      gst launch 1 0  m alsasrc device plughw CARD Device DEV 0         queue leaky 2     audioconvert     audioresample      audio x raw format S16LE channels 1 layout interleaved      pocketsphinx name asr   bestpath 0   hmm  home pi  virtualenvs scarlett dbus poc share pocketsphinx model en us en us   lm   dev bossjones github scarlett gstreamer pocketsphinx demo 1473 lm   dict   dev bossjones github scarlett gstreamer pocketsphinx demo 1473 dic     queue leaky 2     fakesink    Setting pipeline to PAUSED     Current configuration   NAME        DEFLT     VALUE   agc      none    none  agcthresh    2 0   2 000000e 00  allphone  allphone ci    no    no  alpha      0 97    9 700000e 01  ascale     20 0    2 000000e 01  aw     1   1  backtrace    no    no  beam     1e 48   1 000000e 48  bestpath   yes   no  bestpathlw   9 5   9 500000e 00  ceplen     13    13  cmn      current   current  cmninit    8 0   40 3  1  compallsen   no    no  debug          0  dict          home pi dev bossjones github scarlett gstreamer pocketsphinx demo 1473 dic  dictcase   no    no  dither     no    no  doublebw   no    no  ds     1   1  fdict  feat     1s c d dd 1s c d dd  featparams  fillprob   1e 8    1 000000e 08  frate      100   100  fsg  fsgusealtpron    yes   yes  fsgusefiller   yes   yes  fwdflat    yes   yes  fwdflatbeam    1e 64   1 000000e 64  fwdflatefwid   4   4  fwdflatlw    8 5   8 500000e 00  fwdflatsfwin   25    25  fwdflatwbeam   7e 29   7 000000e 29  fwdtree    yes   yes  hmm           home pi  virtualenvs scarlett dbus poc share pocketsphinx model en us en us  input endian   little    little  jsgf  keyphrase  kws  kws delay    10    10  kws plp    1e 1    1 000000e 01  kws threshold    1   1 000000e 00  latsize    5000    5000  lda  ldadim     0   0  lifter     0   22  lm          home pi dev bossjones github scarlett gstreamer pocketsphinx demo 1473 lm  lmctl  lmname  logbase    1 0001    1 000100e 00  logfn  logspec    no    no  lowerf     133 33334 1 300000e 02  lpbeam     1e 40   1 000000e 40  lponlybeam   7e 29   7 000000e 29  lw     6 5   6 500000e 00  maxhmmpf   30000   30000  maxwpf      1     1  mdef  mean  mfclogdir  min endfr    0   0  mixw  mixwfloor    0 0000001 1 000000e 07  mllr  mmap     yes   yes  ncep     13    13  nfft     512   512  nfilt      40    25  nwpen      1 0   1 000000e 00  pbeam      1e 48   1 000000e 48  pip      1 0   1 000000e 00  pl beam    1e 10   1 000000e 10  pl pbeam   1e 10   1 000000e 10  pl pip     1 0   1 000000e 00  pl weight    3 0   3 000000e 00  pl window    5   5  rawlogdir  remove dc    no    no  remove noise   yes   yes  remove silence   yes   yes  round filters    yes   yes  samprate   16000   1 600000e 04  seed      1     1  sendump  senlogdir  senmgau  silprob    0 005   5 000000e 03  smoothspec   no    no  svspec         0 12 13 25 26 38  tmat  tmatfloor    0 0001    1 000000e 04  topn     4   4  topn beam    0   0  toprule  transform    legacy    dct  unit area    yes   yes  upperf     6855 4976 6 800000e 03  uw     1 0   1 000000e 00  vad postspeech   50    50  vad prespeech    20    20  vad startspeech  10    10  vad threshold    2 0   2 000000e 00  var  varfloor   0 0001    1 000000e 04  varnorm    no    no  verbose    no    no  warp params  warp type    inverse linear  inverse linear  wbeam      7e 29   7 000000e 29  wip      0 65    6 500000e 01  wlen     0 025625  2 562500e 02     Pipeline is live and does not need PREROLL     Got message  23 from element  fakesink0   state changed   GstMessageStateChanged  old state  GstState GST STATE NULL  new state  GstState GST STATE READY  pending state  GstState GST STATE VOID PENDING  Got message  24 from element  queue1   state changed   GstMessageStateChanged  old state  GstState GST STATE NULL  new state  GstState GST STATE READY  pending state  GstState GST STATE VOID PENDING  Got message  25 from element  asr   state changed   GstMessageStateChanged  old state  GstState GST STATE NULL  new state  GstState GST STATE READY  pending state  GstState GST STATE VOID PENDING  Got message  26 from element  capsfilter0   state changed   GstMessageStateChanged  old state  GstState GST STATE NULL  new state  GstState GST STATE READY  pending state  GstState GST STATE VOID PENDING  Got message  27 from element  audioresample0   state changed   GstMessageStateChanged  old state  GstState GST STATE NULL  new state  GstState GST STATE READY  pending state  GstState GST STATE VOID PENDING  Got message  28 from element  audioconvert0   state changed   GstMessageStateChanged  old state  GstState GST STATE NULL  new state  GstState GST STATE READY  pending state  GstState GST STATE VOID PENDING  Got message  29 from element  queue0   state changed   GstMessageStateChanged  old state  GstState GST STATE NULL  new state  GstState GST STATE READY  pending state  GstState GST STATE VOID PENDING  Got message  30 from element  alsasrc0   state changed   GstMessageStateChanged  old state  GstState GST STATE NULL  new state  GstState GST STATE READY  pending state  GstState GST STATE VOID PENDING  Got message  31 from element  pipeline0   state changed   GstMessageStateChanged  old state  GstState GST STATE NULL  new state  GstState GST STATE READY  pending state  GstState GST STATE PAUSED  Got message  33 from pad  queue1 src   stream status   GstMessageStreamStatus  type  GstStreamStatusType GST STREAM STATUS TYPE CREATE  owner  GstElement   GstQueue   queue1   object  GstTask   GstTask   queue1 src   Got message  34 from element  queue1   state changed   GstMessageStateChanged  old state  GstState GST STATE READY  new state  GstState GST STATE PAUSED  pending state  GstState GST STATE VOID PENDING  Got message  35 from element  asr   state changed   GstMessageStateChanged  old state  GstState GST STATE READY  new state  GstState GST STATE PAUSED  pending state  GstState GST STATE VOID PENDING  Got message  36 from element  capsfilter0   state changed   GstMessageStateChanged  old state  GstState GST STATE READY  new state  GstState GST STATE PAUSED  pending state  GstState GST STATE VOID PENDING  Got message  37 from element  audioresample0   state changed   GstMessageStateChanged  old state  GstState GST STATE READY  new state  GstState GST STATE PAUSED  pending state  GstState GST STATE VOID PENDING  Got message  38 from element  audioconvert0   state changed   GstMessageStateChanged  old state  GstState GST STATE READY  new state  GstState GST STATE PAUSED  pending state  GstState GST STATE VOID PENDING  Got message  39 from pad  queue0 src   stream status   GstMessageStreamStatus  type  GstStreamStatusType GST STREAM STATUS TYPE CREATE  owner  GstElement   GstQueue   queue0   object  GstTask   GstTask   queue0 src   Got message  40 from element  queue0   state changed   GstMessageStateChanged  old state  GstState GST STATE READY  new state  GstState GST STATE PAUSED  pending state  GstState GST STATE VOID PENDING  Got message  41 from pad  queue1 src   stream status   GstMessageStreamStatus  type  GstStreamStatusType GST STREAM STATUS TYPE ENTER  owner  GstElement   GstQueue   queue1   object  GstTask   GstTask   queue1 src   Got message  42 from pad  queue0 src   stream status   GstMessageStreamStatus  type  GstStreamStatusType GST STREAM STATUS TYPE ENTER  owner  GstElement   GstQueue   queue0   object  GstTask   GstTask   queue0 src   Got message  46 from pad  alsasrc0 src   stream status   GstMessageStreamStatus  type  GstStreamStatusType GST STREAM STATUS TYPE CREATE  owner  GstElement   GstAlsaSrc   alsasrc0   object  GstTask   GstTask   alsasrc0 src   Got message  47 from element  alsasrc0   state changed   GstMessageStateChanged  old state  GstState GST STATE READY  new state  GstState GST STATE PAUSED  pending state  GstState GST STATE VOID PENDING  Got message  48 from element  pipeline0   state changed   GstMessageStateChanged  old state  GstState GST STATE READY  new state  GstState GST STATE PAUSED  pending state  GstState GST STATE VOID PENDING  Got message  49 from pad  alsasrc0 src   stream status   GstMessageStreamStatus  type  GstStreamStatusType GST STREAM STATUS TYPE ENTER  owner  GstElement   GstAlsaSrc   alsasrc0   object  GstTask   GstTask   alsasrc0 src   Got message  50 from element  pipeline0   stream start   GstMessageStreamStart  group id  uint 0  Setting pipeline to PLAYING     Got message  53 from element  pipeline0   new clock   GstMessageNewClock  clock  GstClock   GstAudioClock   GstAudioSrcClock   New clock  GstAudioSrcClock Got message  55 from element  queue1   state changed   GstMessageStateChanged  old state  GstState GST STATE PAUSED  new state  GstState GST STATE PLAYING  pending state  GstState GST STATE VOID PENDING  Got message  56 from element  asr   state changed   GstMessageStateChanged  old state  GstState GST STATE PAUSED  new state  GstState GST STATE PLAYING  pending state  GstState GST STATE VOID PENDING  Got message  57 from element  capsfilter0   state changed   GstMessageStateChanged  old state  GstState GST STATE PAUSED  new state  GstState GST STATE PLAYING  pending state  GstState GST STATE VOID PENDING  Got message  58 from element  audioresample0   state changed   GstMessageStateChanged  old state  GstState GST STATE PAUSED  new state  GstState GST STATE PLAYING  pending state  GstState GST STATE VOID PENDING  Got message  59 from element  audioconvert0   state changed   GstMessageStateChanged  old state  GstState GST STATE PAUSED  new state  GstState GST STATE PLAYING  pending state  GstState GST STATE VOID PENDING  Got message  60 from element  queue0   state changed   GstMessageStateChanged  old state  GstState GST STATE PAUSED  new state  GstState GST STATE PLAYING  pending state  GstState GST STATE VOID PENDING  Got message  61 from element  alsasrc0   state changed   GstMessageStateChanged  old state  GstState GST STATE PAUSED  new state  GstState GST STATE PLAYING  pending state  GstState GST STATE VOID PENDING  Got message  62 from object  audiosrcringbuffer0   stream status   GstMessageStreamStatus  type  GstStreamStatusType GST STREAM STATUS TYPE ENTER  owner  GstElement   GstAlsaSrc   alsasrc0   object  GThread NULL  Got message  69 from element  asr   element   pocketsphinx  timestamp  guint64 940000000  final  boolean false  confidence  glong 0  hypothesis  string SIX  Got message  70 from element  asr   element   pocketsphinx  timestamp  guint64 980000000  final  boolean false  confidence  glong 0  hypothesis  string TO  Got message  71 from element  asr   element   pocketsphinx  timestamp  guint64 1030000000  final  boolean false  confidence  glong 0  hypothesis  string TURN  Got message  72 from element  asr   element   pocketsphinx  timestamp  guint64 1270000000  final  boolean false  confidence  glong 0  hypothesis  string  TURN  IS   Got message  73 from element  asr   element   pocketsphinx  timestamp  guint64 1400000000  final  boolean false  confidence  glong 0  hypothesis  string  TURN  IS  IT   Got message  74 from element  asr   element   pocketsphinx  timestamp  guint64 1500000000  final  boolean false  confidence  glong 0  hypothesis  string  TURN  IS  IT  GO   Got message  75 from element  asr   element   pocketsphinx  timestamp  guint64 1640000000  final  boolean false  confidence  glong 0  hypothesis  string  TURN  IS  IT  TURN   Got message  76 from element  asr   element   pocketsphinx  timestamp  guint64 18446744073709551615  final  boolean true  confidence  glong 0  hypothesis  string  TURN  IS  IT  TURN   Got message  77 from element  fakesink0   state changed   GstMessageStateChanged  old state  GstState GST STATE READY  new state  GstState GST STATE PAUSED  pending state  GstState GST STATE VOID PENDING  Got message  79 from element  pipeline0   async done   GstMessageAsyncDone  running time  guint64 18446744073709551615  Got message  81 from element  fakesink0   state changed   GstMessageStateChanged  old state  GstState GST STATE PAUSED  new state  GstState GST STATE PLAYING  pending state  GstState GST STATE VOID PENDING  Got message  82 from element  pipeline0   state changed   GstMessageStateChanged  old state  GstState GST STATE PAUSED  new state  GstState GST STATE PLAYING  pending state  GstState GST STATE VOID PENDING  Got message  83 from element  asr   element   pocketsphinx  timestamp  guint64 5320000000  final  boolean false  confidence  glong 0  hypothesis  string EIGHT  Got message  84 from element  asr   element   pocketsphinx  timestamp  guint64 5580000000  final  boolean false  confidence  glong 0  hypothesis  string THE  Got message  85 from element  asr   element   pocketsphinx  timestamp  guint64 6270000000  final  boolean false  confidence  glong 0  hypothesis  string PAUSE  Got message  86 from element  asr   element   pocketsphinx  timestamp  guint64 6290000000  final  boolean false  confidence  glong 0  hypothesis  string ON  Got message  87 from element  asr   element   pocketsphinx  timestamp  guint64 6730000000  final  boolean false  confidence  glong 0  hypothesis  string  ON  GET   Got message  88 from element  asr   element   pocketsphinx  timestamp  guint64 18446744073709551615  final  boolean true  confidence  glong 0  hypothesis  string GET  Got message  89 from element  asr   element   pocketsphinx  timestamp  guint64 8870000000  final  boolean false  confidence  glong 0  hypothesis  string GET  Got message  90 from element  asr   element   pocketsphinx  timestamp  guint64 9150000000  final  boolean false  confidence  glong 0  hypothesis  string ONE  Got message  91 from element  asr   element   pocketsphinx  timestamp  guint64 9450000000  final  boolean false  confidence  glong 0  hypothesis  string WHAT  Got message  92 from element  asr   element   pocketsphinx  timestamp  guint64 18446744073709551615  final  boolean true  confidence  glong 0  hypothesis  string WHAT  Got message  93 from element  asr   element   pocketsphinx  timestamp  guint64 11830000000  final  boolean false  confidence  glong 0  hypothesis  string THE  Got message  94 from element  asr   element   pocketsphinx  timestamp  guint64 11870000000  final  boolean false  confidence  glong 0  hypothesis  string TV  Got message  95 from element  asr   element   pocketsphinx  timestamp  guint64 12070000000  final  boolean false  confidence  glong 0  hypothesis  string THE  Got message  96 from element  asr   element   pocketsphinx  timestamp  guint64 12080000000  final  boolean false  confidence  glong 0  hypothesis  string TV  Got message  97 from element  asr   element   pocketsphinx  timestamp  guint64 12470000000  final  boolean false  confidence  glong 0  hypothesis  string THE  Got message  98 from element  asr   element   pocketsphinx  timestamp  guint64 12650000000  final  boolean false  confidence  glong 0  hypothesis  string  THE  WHAT   Got message  99 from element  asr   element   pocketsphinx  timestamp  guint64 12690000000  final  boolean false  confidence  glong 0  hypothesis  string  THE  WHITE   Got message  100 from element  asr   element   pocketsphinx  timestamp  guint64 12750000000  final  boolean false  confidence  glong 0  hypothesis  string  THE  PLAY   Got message  101 from element  asr   element   pocketsphinx  timestamp  guint64 12770000000  final  boolean false  confidence  glong 0  hypothesis  string  THE  WHITE   Got message  102 from element  asr   element   pocketsphinx  timestamp  guint64 18446744073709551615  final  boolean true  confidence  glong 0  hypothesis  string  THE  WHITE   Got message  103 from element  asr   element   pocketsphinx  timestamp  guint64 13980000000  final  boolean false  confidence  glong 0  hypothesis  string EIGHT  Got message  104 from element  asr   element   pocketsphinx  timestamp  guint64 14040000000  final  boolean false  confidence  glong 0  hypothesis  string TO  Got message  105 from element  asr   element   pocketsphinx  timestamp  guint64 14090000000  final  boolean false  confidence  glong 0  hypothesis  string TURN  Got message  106 from element  asr   element   pocketsphinx  timestamp  guint64 14120000000  final  boolean false  confidence  glong 0  hypothesis  string ZERO  Got message  107 from element  asr   element   pocketsphinx  timestamp  guint64 14140000000  final  boolean false  confidence  glong 0  hypothesis  string TURN  Got message  108 from element  asr   element   pocketsphinx  timestamp  guint64 14170000000  final  boolean false  confidence  glong 0  hypothesis  string ZERO  Got message  109 from element  asr   element   pocketsphinx  timestamp  guint64 14320000000  final  boolean false  confidence  glong 0  hypothesis  string SCARLETT  Got message  110 from element  asr   element   pocketsphinx  timestamp  guint64 18446744073709551615  final  boolean true  confidence  glong 0  hypothesis  string SCARLETT  Got message  111 from element  asr   element   pocketsphinx  timestamp  guint64 19840000000  final  boolean false  confidence  glong 0  hypothesis  string FOUR  Got message  112 from element  asr   element   pocketsphinx  timestamp  guint64 19870000000  final  boolean false  confidence  glong 0  hypothesis  string WHAT  Got message  113 from element  asr   element   pocketsphinx  timestamp  guint64 19900000000  final  boolean false  confidence  glong 0  hypothesis  string ONE  Got message  114 from element  asr   element   pocketsphinx  timestamp  guint64 19910000000  final  boolean false  confidence  glong 0  hypothesis  string WHAT  Got message  115 from element  asr   element   pocketsphinx  timestamp  guint64 20220000000  final  boolean false  confidence  glong 0  hypothesis  string  WHAT  TIME   Got message  116 from element  asr   element   pocketsphinx  timestamp  guint64 20230000000  final  boolean false  confidence  glong 0  hypothesis  string  WHAT  TIME  IS   Got message  117 from element  asr   element   pocketsphinx  timestamp  guint64 20370000000  final  boolean false  confidence  glong 0  hypothesis  string  WHAT  TIME  IS  IT   Got message  118 from element  asr   element   pocketsphinx  timestamp  guint64 18446744073709551615  final  boolean true  confidence  glong 0  hypothesis  string  WHAT  TIME  IS  IT   Got message  119 from element  asr   element   pocketsphinx  timestamp  guint64 22940000000  final  boolean false  confidence  glong 0  hypothesis  string TURN  Got message  120 from element  asr   element   pocketsphinx  timestamp  guint64 22950000000  final  boolean false  confidence  glong 0  hypothesis  string FRIZZY  Got message  121 from element  asr   element   pocketsphinx  timestamp  guint64 22980000000  final  boolean false  confidence  glong 0  hypothesis  string TURN  Got message  122 from element  asr   element   pocketsphinx  timestamp  guint64 23050000000  final  boolean false  confidence  glong 0  hypothesis  string ENTER  Got message  123 from element  asr   element   pocketsphinx  timestamp  guint64 23070000000  final  boolean false  confidence  glong 0  hypothesis  string  TURN  ON   Got message  124 from element  asr   element   pocketsphinx  timestamp  guint64 23120000000  final  boolean false  confidence  glong 0  hypothesis  string NINE  Got message  125 from element  asr   element   pocketsphinx  timestamp  guint64 23480000000  final  boolean false  confidence  glong 0  hypothesis  string  TURN  ON  THE   Got message  126 from element  asr   element   pocketsphinx  timestamp  guint64 23670000000  final  boolean false  confidence  glong 0  hypothesis  string  TURN  ON  THE  GET   Got message  127 from element  asr   element   pocketsphinx  timestamp  guint64 24190000000  final  boolean false  confidence  glong 0  hypothesis  string  TURN  ON  THE  GET  ENTER   Got message  128 from element  asr   element   pocketsphinx  timestamp  guint64 18446744073709551615  final  boolean true  confidence  glong 0  hypothesis  string  TURN  ON  THE  GET   Got message  129 from element  asr   element   pocketsphinx  timestamp  guint64 27970000000  final  boolean false  confidence  glong 0  hypothesis  string ARE  Got message  130 from element  asr   element   pocketsphinx  timestamp  guint64 28100000000  final  boolean false  confidence  glong 0  hypothesis  string PAUSE  Got message  131 from element  asr   element   pocketsphinx  timestamp  guint64 28110000000  final  boolean false  confidence  glong 0  hypothesis  string ARE  Got message  132 from element  asr   element   pocketsphinx  timestamp  guint64 28220000000  final  boolean false  confidence  glong 0  hypothesis  string PAUSE  Got message  133 from element  asr   element   pocketsphinx  timestamp  guint64 18446744073709551615  final  boolean true  confidence  glong 0  hypothesis  string ARE  Got message  134 from element  asr   element   pocketsphinx  timestamp  guint64 29150000000  final  boolean false  confidence  glong 0  hypothesis  string MY  Got message  135 from element  asr   element   pocketsphinx  timestamp  guint64 29400000000  final  boolean false  confidence  glong 0  hypothesis  string TURN  Got message  136 from element  asr   element   pocketsphinx  timestamp  guint64 29510000000  final  boolean false  confidence  glong 0  hypothesis  string APPLE  Got message  137 from element  asr   element   pocketsphinx  timestamp  guint64 29880000000  final  boolean false  confidence  glong 0  hypothesis  string  TURN  THE  ON   Got message  138 from element  asr   element   pocketsphinx  timestamp  guint64 29890000000  final  boolean false  confidence  glong 0  hypothesis  string  TURN  THE  PAUSE   Got message  139 from element  asr   element   pocketsphinx  timestamp  guint64 29940000000  final  boolean false  confidence  glong 0  hypothesis  string  TURN  THE  ON   Got message  140 from element  asr   element   pocketsphinx  timestamp  guint64 30020000000  final  boolean false  confidence  glong 0  hypothesis  string  TURN  THE  OFF  THE   Got message  141 from element  asr   element   pocketsphinx  timestamp  guint64 30320000000  final  boolean false  confidence  glong 0  hypothesis  string  TURN  THE  OFF  THE  GO   Got message  142 from element  asr   element   pocketsphinx  timestamp  guint64 30360000000  final  boolean false  confidence  glong 0  hypothesis  string  TURN  THE  OFF  THE  FOUR   Got message  143 from element  asr   element   pocketsphinx  timestamp  guint64 18446744073709551615  final  boolean true  confidence  glong 0  hypothesis  string  TURN  OFF  THE  FOUR    Chandling interrupt  Got message  144 from element  pipeline0   application   GstLaunchInterrupt  message  string  Pipeline  interrupted   Interrupt  Stopping pipeline     Execution ended after 0 00 33 286680180 Setting pipeline to PAUSED     Setting pipeline to READY     Setting pipeline to NULL     Freeing pipeline           vagrant package   base  f5fba22c 929b 4a55 bb1c c5a3d0f26b8d    output ubuntu 1604 desktop base box    2 1 7   using virtualenv  packer ubuntu 1604  Malcolms MBP 3 in       vagrant package   base  f5fba22c 929b 4a55 bb1c c5a3d0f26b8d    output ubuntu 1604 desktop base box     f5fba22c 929b 4a55 bb1c c5a3d0f26b8d  Exporting VM        f5fba22c 929b 4a55 bb1c c5a3d0f26b8d  Compressing package to   Users malcolm ubuntu 1604 desktop base box   Environment variables for resize script          featutre 1604 U 2   1      env   grep CD TO    2 1 7    Malcolms MBP 3 in   dev bossjones scarlett ansible    featutre 1604 U 2   1      export CD TO   dev bossjones scarlett ansible    2 1 7    Malcolms MBP 3 in   dev bossjones scarlett ansible    featutre 1604 U 2   1      export USERDIR  PWD    2 1 7    Malcolms MBP 3 in   dev bossjones scarlett ansible    featutre 1604 U 2   1      export VDMK FILE packer ubuntu 16 04 amd64 disk1 vmdk    2 1 7    Malcolms MBP 3 in   dev bossjones scarlett ansible    featutre 1604 U 2   1      export VAGRANT VMNAME scarlett base 16 04    2 1 7    Malcolms MBP 3 in   dev bossjones scarlett ansible    featutre 1604 U 2   1      export VDI FILENAME scarlett 50gb vdi    2 1 7    Malcolms MBP 3 in   dev bossjones scarlett ansible    featutre 1604 U 2   1      env   grep CD TO CD TO  Users malcolm dev bossjones scarlett ansible    2 1 7    Malcolms MBP 3 in   dev bossjones scarlett ansible    featutre 1604 U 2   1      env   grep USERDIR USERDIR  Users malcolm dev bossjones scarlett ansible    2 1 7    Malcolms MBP 3 in   dev bossjones scarlett ansible    featutre 1604 U 2   1      env   grep VDMK FILE VDMK FILE packer ubuntu 16 04 amd64 disk1 vmdk    2 1 7    Malcolms MBP 3 in   dev bossjones scarlett ansible    featutre 1604 U 2   1      env   grep VAGRANT VMNAME VAGRANT VMNAME scarlett base 16 04    2 1 7    Malcolms MBP 3 in   dev bossjones scarlett ansible    featutre 1604 U 2   1      env   grep VDI FILENAME VDI FILENAME scarlett 50gb vdi    2 1 7    Malcolms MBP 3 in   dev bossjones scarlett ansible    featutre 1604 U 2   1      env   grep VMNAME VAGRANT VMNAME scarlett base 16 04    2 1 7    Malcolms MBP 3 in   dev bossjones scarlett ansible    featutre 1604 U 2   1      cd  CD TO    2 1 7    Malcolms MBP 3 in   dev bossjones scarlett ansible    featutre 1604 U 2   1      export VMNAME   grep    vb name  Vagrantfile   cut  d    f2     2 1 7    Malcolms MBP 3 in   dev bossjones scarlett ansible    featutre 1604 U 2   1      env   grep VMNAME VMNAME scarlett ansible 1604 packer2 VAGRANT VMNAME scarlett base 16 04    2 1 7    Malcolms MBP 3 in   dev bossjones scarlett ansible    featutre 1604 U 2   1      env   grep VM PATH NVM PATH  Users malcolm  nvm versions node v5 9 0 lib node    2 1 7    Malcolms MBP 3 in   dev bossjones scarlett ansible    featutre 1604 U 2   1      export VM PATH   HOME VirtualBox VMs  VMNAME     2 1 7    Malcolms MBP 3 in   dev bossjones scarlett ansible    featutre 1604 U 2   1      env   grep VM PATH NVM PATH  Users malcolm  nvm versions node v5 9 0 lib node VM PATH  Users malcolm VirtualBox VMs scarlett ansible 1604 packer2    2 1 7    Malcolms MBP 3 in   dev bossjones scarlett ansible    featutre 1604 U 2   1         2 1 7    Malcolms MBP 3 in   dev bossjones scarlett ansible    featutre 1604 U 2   1      vagrant up   no provision Bringing machine  scarlett 1604 packer2  up with  virtualbox  provider        scarlett 1604 packer2  Importing base box  bossjones scarlett 1604 packer         scarlett 1604 packer2  Matching MAC address for NAT networking        scarlett 1604 packer2  Setting the name of the VM  scarlett ansible 1604 packer2     scarlett 1604 packer2  Clearing any previously set network interfaces        scarlett 1604 packer2  Preparing network interfaces based on configuration        scarlett 1604 packer2  Adapter 1  nat     scarlett 1604 packer2  Adapter 2  bridged     scarlett 1604 packer2  Forwarding ports        scarlett 1604 packer2  2376  guest     2376  host   adapter 1      scarlett 1604 packer2  22  guest     2222  host   adapter 1      scarlett 1604 packer2  Running  pre boot  VM customizations        scarlett 1604 packer2  Booting VM        scarlett 1604 packer2  Waiting for machine to boot  This may take a few minutes        scarlett 1604 packer2  SSH address  127 0 0 1 2222     scarlett 1604 packer2  SSH username  pi     scarlett 1604 packer2  SSH auth method  private key     scarlett 1604 packer2  Machine booted and ready      scarlett 1604 packer2  Checking for guest additions in VM        scarlett 1604 packer2  Setting hostname        scarlett 1604 packer2  Configuring and enabling network interfaces    The following SSH command responded with a non zero exit status  Vagrant assumes that this means the command failed     sbin ifdown eth1 2   dev null   Stdout from the command    Stderr from the command    mesg  ttyname failed  Inappropriate ioctl for device    2 1 7    Malcolms MBP 3 in   dev bossjones scarlett ansible    featutre 1604 U 3   1      vagrant halt     scarlett 1604 packer2  Attempting graceful shutdown of VM       2 1 7    Malcolms MBP 3 in   dev bossjones scarlett ansible    featutre 1604 U 3   1      cd   VM PATH     2 1 7    Malcolms MBP 3 in   VirtualBox VMs scarlett ansible 1604 packer2     ls  lta total 11968688 drwx         6 malcolm  staff         204 Sep 10 14 17    rw          1 malcolm  staff  6127943680 Sep 10 14 17 packer ubuntu 16 04 amd64 disk1 vmdk  rw          1 malcolm  staff        8560 Sep 10 14 17 scarlett ansible 1604 packer2 vbox  rw          1 malcolm  staff        8560 Sep 10 14 17 scarlett ansible 1604 packer2 vbox prev drwx         3 malcolm  staff         102 Sep 10 14 14 Logs drwx        12 malcolm  staff         408 Sep 10 14 14       2 1 7    Malcolms MBP 3 in   VirtualBox VMs scarlett ansible 1604 packer2       UUID            b5bb12c1 4e9d 4d4f 9dea e5a88601e3ad Parent UUID     base State           inaccessible Type            normal  base  Location         Users malcolm VirtualBox VMs scarlett ansible 1604 packer2 out vdi Storage format  VDI Capacity        56320 MBytes Encryption      disabled   UUID            2a845830 55d1 4181 8b0f ba274b524db1 Parent UUID     base State           inaccessible Type            normal  base  Location         Users malcolm VirtualBox VMs scarlett ansible 1604 packer2 box disk1 50gb vmdk Storage format  VMDK Capacity        56320 MBytes Encryption      disabled   UUID            c33810ce a153 44d9 bd51 f66af6a2fc82 Parent UUID     base State           inaccessible Type            normal  base  Location         Users malcolm VirtualBox VMs scarlett ansible 1604 packer2 scarlett 50gb vdi Storage format  VDI Capacity        56320 MBytes Encryption      disabled   UUID            1683a1b1 e82c 4f8a bcd6 1b706565179e Parent UUID     base State           created Type            normal  base  Location         Users malcolm VirtualBox VMs scarlett ansible 1604 packer2 packer ubuntu 16 04 amd64 disk1 vmdk Storage format  VMDK Capacity        20000 MBytes Encryption      disabled       The trick to fixing this was here    https   coderwall com p 8m  dq purge deleted hard disks from virtual box         2 1 7    Malcolms MBP 3 in   dev bossjones scarlett ansible    featutre 1604 U 1      vagrant halt     scarlett 1604 packer2  Attempting graceful shutdown of VM       2 1 7    Malcolms MBP 3 in   dev bossjones scarlett ansible    featutre 1604 U 1      cd   VM PATH     2 1 7    Malcolms MBP 3 in   VirtualBox VMs scarlett ansible 1604 packer2     ll total 11979568 drwx         6 malcolm  staff         204 Sep 10 14 41   drwx        12 malcolm  staff         408 Sep 10 14 40    drwx         3 malcolm  staff         102 Sep 10 14 40 Logs  rw          1 malcolm  staff  6133514240 Sep 10 14 41 packer ubuntu 16 04 amd64 disk1 vmdk  rw          1 malcolm  staff        8560 Sep 10 14 41 scarlett ansible 1604 packer2 vbox  rw          1 malcolm  staff        8560 Sep 10 14 41 scarlett ansible 1604 packer2 vbox prev    2 1 7    Malcolms MBP 3 in   VirtualBox VMs scarlett ansible 1604 packer2     VBoxManage clonehd  VDMK FILE   VDI FILENAME    format VDI 0    10    20    30    40    50    60    70    80    90    100  Clone medium created in format  VDI   UUID  a392e58b 04fe 48ea 9ed4 e199654b503e    2 1 7    Malcolms MBP 3 in   VirtualBox VMs scarlett ansible 1604 packer2     VBoxManage modifyhd   VDI FILENAME    resizebyte 59055800320 0    10    20    30    40    50    60    70    80    90    100     2 1 7    Malcolms MBP 3 in   VirtualBox VMs scarlett ansible 1604 packer2     VBoxManage clonehd   VDI FILENAME  box disk1 50gb vmdk   format VMDK 0    10    20    30    40    50    60    70    80    90    100  Clone medium created in format  VMDK   UUID  0158b89a f0c9 4f1f b097 c207e034a9bb    2 1 7    Malcolms MBP 3 in   VirtualBox VMs scarlett ansible 1604 packer2       VBoxManage showvminfo  scarlett ansible 1604 packer2        VBoxManage showvminfo  scarlett ansible 1604 packer2  Name             scarlett ansible 1604 packer2 Groups             Guest OS         Ubuntu  64 bit  UUID             5d9fbcf5 2022 4e82 88dc a93c4688b52a Config file       Users malcolm VirtualBox VMs scarlett ansible 1604 packer2 scarlett ansible 1604 packer2 vbox Snapshot folder   Users malcolm VirtualBox VMs scarlett ansible 1604 packer2 Snapshots Log folder        Users malcolm VirtualBox VMs scarlett ansible 1604 packer2 Logs Hardware UUID    5d9fbcf5 2022 4e82 88dc a93c4688b52a Memory size      2048MB Page Fusion      off VRAM size        8MB CPU exec cap     100  HPET             off Chipset          piix3 Firmware         BIOS Number of CPUs   2 PAE              on Long Mode        on CPUID Portability Level  0 CPUID overrides  None Boot menu mode   message and menu Boot Device  1   HardDisk Boot Device  2   DVD Boot Device  3   Not Assigned Boot Device  4   Not Assigned ACPI             on IOAPIC           on Time offset      0ms RTC              UTC Hardw  virt ext  on Nested Paging    on Large Pages      on VT x VPID        on VT x unr  exec   on Paravirt  Provider  Default State            powered off  since 2016 09 10T18 41 35 261000000  Monitor count    1 3D Acceleration  off 2D Video Acceleration  off Teleporter Enabled  off Teleporter Port  0 Teleporter Address  Teleporter Password  Tracing Enabled  off Allow Tracing to Access VM  off Tracing Configuration  Autostart Enabled  off Autostart Delay  0 Default Frontend  Storage Controller Name  0              IDE Controller Storage Controller Type  0              PIIX4 Storage Controller Instance Number  0   0 Storage Controller Max Port Count  0    2 Storage Controller Port Count  0        2 Storage Controller Bootable  0          on IDE Controller  0  0    Users malcolm VirtualBox VMs scarlett ansible 1604 packer2 packer ubuntu 16 04 amd64 disk1 vmdk  UUID  c36a7d63 db73 461c b05d 093917e19e19  NIC 1            MAC  0800279C7307  Attachment  NAT  Cable connected  on  Trace  off  file  none   Type  82540EM  Reported speed  0 Mbps  Boot priority  0  Promisc Policy  deny  Bandwidth group  none NIC 1 Settings   MTU  0  Socket  send  64  receive  64   TCP Window  send 64  receive  64  NIC 1 Rule 0     name   ssh  protocol   tcp  host ip   127 0 0 1  host port   2222  guest ip     guest port   22 NIC 1 Rule 1     name   tcp2376  protocol   tcp  host ip   127 0 0 1  host port   2376  guest ip     guest port   2376 NIC 2            MAC  080027CF00AC  Attachment  Bridged Interface  en0  Wi Fi  AirPort    Cable connected  on  Trace  off  file  none   Type  82540EM  Reported speed  0 Mbps  Boot priority  0  Promisc Policy  deny  Bandwidth group  none NIC 3            disabled NIC 4            disabled NIC 5            disabled NIC 6            disabled NIC 7            disabled NIC 8            disabled Pointing Device  PS 2 Mouse Keyboard Device  PS 2 Keyboard UART 1           disabled UART 2           disabled LPT 1            disabled LPT 2            disabled Audio            enabled  Driver  CoreAudio  Controller  AC97  Codec  STAC9700  Clipboard Mode   disabled Drag and drop Mode  disabled VRDE             enabled  Address 127 0 0 1  Ports 5993  MultiConn  off  ReuseSingleConn  off  Authentication type  null  Video redirection  disabled VRDE property  TCP Ports     5993  VRDE property  TCP Address    127 0 0 1  VRDE property  VideoChannel Enabled     VRDE property  VideoChannel Quality     VRDE property  VideoChannel DownscaleProtection     VRDE property  Client DisableDisplay     VRDE property  Client DisableInput     VRDE property  Client DisableAudio     VRDE property  Client DisableUSB     VRDE property  Client DisableClipboard     VRDE property  Client DisableUpstreamAudio     VRDE property  Client DisableRDPDR     VRDE property  H3DRedirect Enabled     VRDE property  Security Method     VRDE property  Security ServerCertificate     VRDE property  Security ServerPrivateKey     VRDE property  Security CACertificate     VRDE property  Audio RateCorrectionMode     VRDE property  Audio LogPath     USB              enabled EHCI             disabled XHCI             disabled   USB Device Filters      Bandwidth groups      Shared folders    Name   vagrant   Host path    Users malcolm dev bossjones scarlett ansible   machine mapping   writable   Video capturing     not active Capture screens     0 Capture file         Users malcolm VirtualBox VMs scarlett ansible 1604 packer2 scarlett ansible 1604 packer2 webm Capture dimensions  1024x768 Capture rate        512 kbps Capture FPS         25   Guest    Configured memory balloon size       0 MB       VBoxManage showvminfo   VMNAME  Name             scarlett ansible 1604 packer2 Groups             Guest OS         Ubuntu  64 bit  UUID             5d9fbcf5 2022 4e82 88dc a93c4688b52a Config file       Users malcolm VirtualBox VMs scarlett ansible 1604 packer2 scarlett ansible 1604 packer2 vbox Snapshot folder   Users malcolm VirtualBox VMs scarlett ansible 1604 packer2 Snapshots Log folder        Users malcolm VirtualBox VMs scarlett ansible 1604 packer2 Logs Hardware UUID    5d9fbcf5 2022 4e82 88dc a93c4688b52a Memory size      2048MB Page Fusion      off VRAM size        8MB CPU exec cap     100  HPET             off Chipset          piix3 Firmware         BIOS Number of CPUs   2 PAE              on Long Mode        on CPUID Portability Level  0 CPUID overrides  None Boot menu mode   message and menu Boot Device  1   HardDisk Boot Device  2   DVD Boot Device  3   Not Assigned Boot Device  4   Not Assigned ACPI             on IOAPIC           on Time offset      0ms RTC              UTC Hardw  virt ext  on Nested Paging    on Large Pages      on VT x VPID        on VT x unr  exec   on Paravirt  Provider  Default State            powered off  since 2016 09 10T18 41 35 261000000  Monitor count    1 3D Acceleration  off 2D Video Acceleration  off Teleporter Enabled  off Teleporter Port  0 Teleporter Address  Teleporter Password  Tracing Enabled  off Allow Tracing to Access VM  off Tracing Configuration  Autostart Enabled  off Autostart Delay  0 Default Frontend  Storage Controller Name  0              IDE Controller Storage Controller Type  0              PIIX4 Storage Controller Instance Number  0   0 Storage Controller Max Port Count  0    2 Storage Controller Port Count  0        2 Storage Controller Bootable  0          on IDE Controller  0  0    Users malcolm VirtualBox VMs scarlett ansible 1604 packer2 packer ubuntu 16 04 amd64 disk1 vmdk  UUID  c36a7d63 db73 461c b05d 093917e19e19  NIC 1            MAC  0800279C7307  Attachment  NAT  Cable connected  on  Trace  off  file  none   Type  82540EM  Reported speed  0 Mbps  Boot priority  0  Promisc Policy  deny  Bandwidth group  none NIC 1 Settings   MTU  0  Socket  send  64  receive  64   TCP Window  send 64  receive  64  NIC 1 Rule 0     name   ssh  protocol   tcp  host ip   127 0 0 1  host port   2222  guest ip     guest port   22 NIC 1 Rule 1     name   tcp2376  protocol   tcp  host ip   127 0 0 1  host port   2376  guest ip     guest port   2376 NIC 2            MAC  080027CF00AC  Attachment  Bridged Interface  en0  Wi Fi  AirPort    Cable connected  on  Trace  off  file  none   Type  82540EM  Reported speed  0 Mbps  Boot priority  0  Promisc Policy  deny  Bandwidth group  none NIC 3            disabled NIC 4            disabled NIC 5            disabled NIC 6            disabled NIC 7            disabled NIC 8            disabled Pointing Device  PS 2 Mouse Keyboard Device  PS 2 Keyboard UART 1           disabled UART 2           disabled LPT 1            disabled LPT 2            disabled Audio            enabled  Driver  CoreAudio  Controller  AC97  Codec  STAC9700  Clipboard Mode   disabled Drag and drop Mode  disabled VRDE             enabled  Address 127 0 0 1  Ports 5993  MultiConn  off  ReuseSingleConn  off  Authentication type  null  Video redirection  disabled VRDE property  TCP Ports     5993  VRDE property  TCP Address    127 0 0 1  VRDE property  VideoChannel Enabled     VRDE property  VideoChannel Quality     VRDE property  VideoChannel DownscaleProtection     VRDE property  Client DisableDisplay     VRDE property  Client DisableInput     VRDE property  Client DisableAudio     VRDE property  Client DisableUSB     VRDE property  Client DisableClipboard     VRDE property  Client DisableUpstreamAudio     VRDE property  Client DisableRDPDR     VRDE property  H3DRedirect Enabled     VRDE property  Security Method     VRDE property  Security ServerCertificate     VRDE property  Security ServerPrivateKey     VRDE property  Security CACertificate     VRDE property  Audio RateCorrectionMode     VRDE property  Audio LogPath     USB              enabled EHCI             disabled XHCI             disabled   USB Device Filters      Bandwidth groups      Shared folders    Name   vagrant   Host path    Users malcolm dev bossjones scarlett ansible   machine mapping   writable   Video capturing     not active Capture screens     0 Capture file         Users malcolm VirtualBox VMs scarlett ansible 1604 packer2 scarlett ansible 1604 packer2 webm Capture dimensions  1024x768 Capture rate        512 kbps Capture FPS         25   Guest    Configured memory balloon size       0 MB     scarlett gstreamer pocketsphinx demo   Basic demo to make sure all gstreamer   pocketsphinx dependencies were installed correctly  and STT works w  pocketphinx gst plugin for scarlett   if recording isnt working   http   askubuntu com questions 61289 how to verify if my microphone input is dead or ubuntu not detected it yet i ca http   www linux org threads beats audio on linux 4443    IMPORTANT NOTE    Currently assumes  hmm  folder is located at    hmm  FYI  Will need to figure out if this is still required in newer versions of pocketpshinx or not    NOTE    This demo uses  autoenv   see  https   github com kennethreitz autoenv   to set environment variables to find pocketphinx static libs  eg    using virtualenv  scarlett dbus poc scarlett ansible in       ls  lta  home pi  virtualenvs scarlett dbus poc lib  total 6324 drwxrwxr x 2 pi pi    4096 Dec 29 19 08 pkgconfig drwxrwxr x 2 pi pi    4096 Dec 29 19 08 gstreamer 1 0 drwxrwxr x 5 pi pi    4096 Dec 29 19 08    rw r  r   1 pi pi 1941766 Dec 29 19 08 libpocketsphinx a  rwxr xr x 1 pi pi    1393 Dec 29 19 08 libpocketsphinx la lrwxrwxrwx 1 pi pi      24 Dec 29 19 08 libpocketsphinx so    libpocketsphinx so 3 0 0 lrwxrwxrwx 1 pi pi      24 Dec 29 19 08 libpocketsphinx so 3    libpocketsphinx so 3 0 0  rwxr xr x 1 pi pi 1147601 Dec 29 19 08 libpocketsphinx so 3 0 0  rw r  r   1 pi pi   18938 Dec 29 18 59 libsphinxad a  rwxr xr x 1 pi pi    1096 Dec 29 18 59 libsphinxad la lrwxrwxrwx 1 pi pi      20 Dec 29 18 59 libsphinxad so    libsphinxad so 3 0 0 lrwxrwxrwx 1 pi pi      20 Dec 29 18 59 libsphinxad so 3    libsphinxad so 3 0 0  rwxr xr x 1 pi pi   21677 Dec 29 18 59 libsphinxad so 3 0 0  rw r  r   1 pi pi 2074892 Dec 29 18 59 libsphinxbase a  rwxr xr x 1 pi pi    1049 Dec 29 18 59 libsphinxbase la lrwxrwxrwx 1 pi pi      22 Dec 29 18 59 libsphinxbase so    libsphinxbase so 3 0 0 lrwxrwxrwx 1 pi pi      22 Dec 29 18 59 libsphinxbase so 3    libsphinxbase so 3 0 0  rwxr xr x 1 pi pi 1222864 Dec 29 18 59 libsphinxbase so 3 0 0 drwxrwxr x 8 pi pi    4096 Dec 24 12 09    drwxrwxr x 4 pi pi    4096 Dec 23 16 25 python2 7   If you don t have  autoenv  installed  simply run  source  path to scarlett gstreamer pocketsphinx demo  env   gst inspect 1 0 pocketsphinx default values           master      gst inspect 1 0 pocketsphinx Current configuration   NAME            DEFLT       VALUE   agc            none        none  agcthresh      2 0     2 000000e 00  allphone  allphone ci        no      no  alpha          0 97        9 700000e 01  ascale         20 0        2 000000e 01  aw         1       1  backtrace      no      no  beam           1e 48       1 000000e 48  bestpath       yes     yes  bestpathlw     9 5     9 500000e 00  ceplen         13      13  cmn            current     current  cmninit        8 0     40 3  1  compallsen     no      no  debug                  0  dict                    home pi  virtualenvs scarlett dbus poc share pocketsphinx model en us cmudict en us dict  dictcase       no      no  dither         no      no  doublebw       no      no  ds         1       1  fdict                   home pi  virtualenvs scarlett dbus poc share pocketsphinx model en us en us noisedict  feat           1s c d dd   1s c d dd  featparams              home pi  virtualenvs scarlett dbus poc share pocketsphinx model en us en us feat params  fillprob       1e 8        1 000000e 08  frate          100     100  fsg  fsgusealtpron      yes     yes  fsgusefiller       yes     yes  fwdflat        yes     yes  fwdflatbeam        1e 64       1 000000e 64  fwdflatefwid       4       4  fwdflatlw      8 5     8 500000e 00  fwdflatsfwin       25      25  fwdflatwbeam       7e 29       7 000000e 29  fwdtree        yes     yes  hmm                     home pi  virtualenvs scarlett dbus poc share pocketsphinx model en us en us  input endian       little      little  jsgf  keyphrase  kws  kws delay      10      10  kws plp        1e 1        1 000000e 01  kws threshold      1       1 000000e 00  latsize        5000        5000  lda  ldadim         0       0  lifter         0       22  lm                  home pi  virtualenvs scarlett dbus poc share pocketsphinx model en us en us lm bin  lmctl  lmname  logbase        1 0001      1 000100e 00  logfn  logspec        no      no  lowerf         133 33334   1 300000e 02  lpbeam         1e 40       1 000000e 40  lponlybeam     7e 29       7 000000e 29  lw         6 5     6 500000e 00  maxhmmpf       30000       30000  maxwpf          1       1  mdef                    home pi  virtualenvs scarlett dbus poc share pocketsphinx model en us en us mdef  mean                    home pi  virtualenvs scarlett dbus poc share pocketsphinx model en us en us means  mfclogdir  min endfr      0       0  mixw  mixwfloor      0 0000001   1 000000e 07  mllr  mmap           yes     yes  ncep           13      13  nfft           512     512  nfilt          40      25  nwpen          1 0     1 000000e 00  pbeam          1e 48       1 000000e 48  pip            1 0     1 000000e 00  pl beam        1e 10       1 000000e 10  pl pbeam       1e 10       1 000000e 10  pl pip         1 0     1 000000e 00  pl weight      3 0     3 000000e 00  pl window      5       5  rawlogdir  remove dc      no      no  remove noise       yes     yes  remove silence     yes     yes  round filters      yes     yes  samprate       16000       1 600000e 04  seed            1       1  sendump                 home pi  virtualenvs scarlett dbus poc share pocketsphinx model en us en us sendump  senlogdir  senmgau  silprob        0 005       5 000000e 03  smoothspec     no      no  svspec                 0 12 13 25 26 38  tmat                    home pi  virtualenvs scarlett dbus poc share pocketsphinx model en us en us transition matrices  tmatfloor      0 0001      1 000000e 04  topn           4       4  topn beam      0       0  toprule  transform      legacy      dct  unit area      yes     yes  upperf         6855 4976   6 800000e 03  uw         1 0     1 000000e 00  vad postspeech     50      50  vad prespeech      20      20  vad startspeech    10      10  vad threshold      2 0     2 000000e 00  var                     home pi  virtualenvs scarlett dbus poc share pocketsphinx model en us en us variances  varfloor       0 0001      1 000000e 04  varnorm        no      no  verbose        no      no  warp params  warp type      inverse linear  inverse linear  wbeam          7e 29       7 000000e 29  wip            0 65        6 500000e 01  wlen           0 025625    2 562500e 02   Factory Details    Rank                     none  0    Long name                PocketSphinx   Klass                    Filter Audio   Description              Convert speech to text   Author                   CMUSphinx devel  cmusphinx devel lists sourceforge net   Plugin Details    Name                     pocketsphinx   Description              PocketSphinx plugin   Filename                  home pi  virtualenvs scarlett dbus poc lib gstreamer 1 0 libgstpocketsphinx so   Version                  5prealpha   License                  BSD   Source module            pocketsphinx   Binary package           PocketSphinx   Origin URL               http   cmusphinx sourceforge net    GObject       GInitiallyUnowned             GstObject                   GstElement                         GstPocketSphinx   Pad Templates    SINK template   sink      Availability  Always     Capabilities        audio x raw                  format    S16LE                  channels  1                    rate  16000   SRC template   src      Availability  Always     Capabilities        text plain   Element Flags    no flags set   Element Implementation    Has change state   function  gst element change state func   Element has no clocking capabilities  Element has no indexing capabilities  Element has no URI handling capabilities    Pads    SINK   sink      Implementation        Has chainfunc    0x7f8017bad1c0       Has custom eventfunc    0x7f8017bad160       Has custom queryfunc    gst pad query default       Has custom iterintlinkfunc    gst pad iterate internal links default     Pad Template   sink    SRC   src      Implementation        Has custom eventfunc    gst pad event default       Has custom queryfunc    gst pad query default       Has custom iterintlinkfunc    gst pad iterate internal links default     Pad Template   src    Element Properties    name                  The name of the object                         flags  readable  writable                         String  Default   pocketsphinx0    parent                The parent of the object                         flags  readable  writable                         Object of type  GstObject    hmm                   Directory containing acoustic model parameters                         flags  readable  writable                         String  Default    home pi  virtualenvs scarlett dbus poc share pocketsphinx model en us en us    lm                    Language model file                         flags  readable  writable                         String  Default    home pi  virtualenvs scarlett dbus poc share pocketsphinx model en us en us lm bin    lmctl                 Language model control file  for class LMs                          flags  readable  writable                         String  Default  null   lmname                Language model name  to select LMs from lmctl                          flags  readable  writable                         String  Default  null   dict                  Dictionary File                         flags  readable  writable                         String  Default    home pi  virtualenvs scarlett dbus poc share pocketsphinx model en us cmudict en us dict    fsg                   Finite state grammar file                         flags  readable  writable                         String  Default  null   fsg model             Finite state grammar object  fsg model t                            flags  writable                         Pointer  Write only   fwdflat               Enable Flat Lexicon Search                         flags  readable  writable                         Boolean  Default  true   bestpath              Enable Graph Search                         flags  readable  writable                         Boolean  Default  true   maxhmmpf              Maximum number of HMMs searched per frame                         flags  readable  writable                         Integer  Range  1   100000 Default  30000   maxwpf                Maximum number of words searched per frame                         flags  readable  writable                         Integer  Range  1   100000 Default   1   beam                  Beam width applied to every frame in Viterbi search                         flags  readable  writable                         Double  Range                1                 1 Default            1e 48   wbeam                 Beam width applied to phone transitions                         flags  readable  writable                         Double  Range                1                 1 Default            7e 29   pbeam                 Beam width applied to phone transitions                         flags  readable  writable                         Double  Range                1                 1 Default            1e 48   dsratio               Evaluate acoustic model every N frames                         flags  readable  writable                         Integer  Range  1   10 Default  1   latdir                Output Directory for Lattices                         flags  readable  writable                         String  Default  null   decoder               The underlying decoder                         flags  readable                         Boxed pointer of type  PSDecoder    configured            Set this to finalize configuration                         flags  readable  writable                         Boolean  Default  true       commandline test   pocketsphinx continuous  hmm  home pi  virtualenvs scarlett dbus poc share pocketsphinx model en us en us  lm 1602 lm  dict 1602 dic  samprate 16000 8000 48000  inmic yes karlandkeke locustio   Some basic load test scenarios for karlandkeke using locust io hubot docker slack   Hubot Docker Slack Proof of Concept ScarlettOS   S C A R L E T T is Tony Darks artificially programmed intelligent computer  She is programmed to speak with a female voice in a British accent                  ScarlettOS Docker Image   Dockerfile               Free software  Apache 2 0   Documentation   https   scarlett os readthedocs io         Features     TODO     Credits   This package was created with  Cookiecutter  and the  audreyr cookiecutter pypackage  project template    INSTALL     Fill this out soon     Development   Interactive bash   docker exec  it scarlettos master 1 bash  l    2 2 3   using virtualenv  scarlett os venv2  Malcolms MBP 3 in   dev bossjones scarlett os    feature dev container  5  U 1      docker exec  it scarlettos master 1 bash  l pi     feature dev container  5  U 1    dev bossjones github scarlett os   Debugging in VSCode   Source  https   github com mikemcgowan django cms plus blob 57e3fa8ec35d73cdd937baac25f5201ec78bbdb9 README md   VSCode debug launch configuration    json        name    Attach  Remote Debug         type    python        request    attach        localRoot      workspaceRoot         remoteRoot            port   2222       secret    my secret        host    localhost      Ensure the following exists in  web manage py  immediately before  execute from command line          python   https   stackoverflow com questions 41201438 debug python application running in docker   try      import ptvsd     ptvsd enable attach secret  my secret   address   0 0 0 0   2222   except  OSError  ImportError       pass       Gstreamer Environment Variables   Official docs  https   gstreamer freedesktop org data doc gstreamer head gstreamer html gst running html   Variable   Example   Description                  GST PLUGIN SYSTEM PATH     GST PLUGIN SYSTEM PATH  usr local lib gstreamer 1 0    GStreamer will scan these paths for GStreamer plug ins  These plug ins will be loaded after the plug ins in the GST PLUGIN PATH variable below   GST DEBUG     GST DEBUG GST AUTOPLUG 6 GST ELEMENT   4    This variable can be set to a list of debug options  which cause GStreamer to print out different types of debugging information to stderr  The variable takes a comma separated list of  category name level  pairs to set specific levels for the individual categories  The level value ranges from 0  nothing  to 9    GST DEBUG DUMP DOT DIR     GST DEBUG DUMP DOT DIR  tmp    Set this environment variable to a path to turn on all  GST DEBUG BIN TO DOT FILE or  GST DEBUG BIN TO DOT FILE WITH TS calls and have the dot files in that location  This will only work if the application in question makes these calls in strategic places  like when the pipeline state changes or an error occurs   GST REGISTRY UPDATE     GST REGISTRY UPDATE no    Set this environment variable to  no  to prevent GStreamer from updating the plugin registry  This is useful for embedded device which is not updating the plugins frequently  it will save time when doing gst init     G DEBUG     G DEBUG fatal warnings    Useful GLib environment variable  Set G DEBUG fatal warnings to make GStreamer programs abort when a critical warning such as an assertion failure occurs  This is useful if you want to find out which part of the code caused that warning to be triggered and under what circumstances  Simply set G DEBUG as mentioned above and run the program in gdb  or let it core dump   Then get a stack trace in the usual way  Cabbie   Prerequisites     python    2 7    pip   git     Installation   To set up a virtual environment  run following commands in order       bash sudo pip install virtualenv sudo pip install virtualenvwrapper   export WORKON HOME  HOME  virtualenvs    usr bin  or  usr local bin  depending on the system   source  usr bin virtualenvwrapper sh   mkvirtualenv   distribute   no site package cabbie   pip install  r requirements txt       To compile SASS    bash gem install sass gem install compass sass   watch cabbie static sass  cabbie static css    compass   To create a local development environment    bash cp cabbie local settings py template cabbie local settings py   To create a local testing environment with Ionic    bash cd ionic cabbie driver   or ionic cabbie passenger npm install ionic serve gulp watch   Todo     Add  is accepted  field to the  Driver  model   Dispatch estimate information to passenger and driver   Permission control for REST API   osx dotfiles AWS create destroy ELB    Manage AWS ELB    Requirements     Ansible 2 0 1 or higher    Tested on Ubuntu 14 04 and Amazon 7     Role Variables     parameter               required   default   choices   comments                                                                         aws elb subnets   yes      A list of VPC subnets to use when creating ELB  Zones should be empty if using this      aws elb scheme  no  internet facing   internet facing  internal The scheme to use when creating the ELB  For a private VPC visible ELB use  internal       aws elb security group ids       A list of security groups to apply to the elb     aws elb instance listeners        List of ports protocols for this ELB to listen on  see  vars      aws elb ping path  no        The destination for the HTTP or HTTPS request       aws elb healthcheck response timeout yes       The amount of time to wait when receiving a response from the health check  in seconds      aws elb ping protocol   no  http    The protocol to use to connect with the instance  Ping protocols  TCP  HTTP  HTTPS  and SSL    aws elb ping port   no  http    The port to use to connect with the instance  as a protocol port pair  If the load balancer fails to connect with the instance at the specified port within the configured response timeout period  the instance is considered unhealthy     aws elb healthcheck interval  no   10    The amount of time between health checks of an individual instance  in seconds       aws elb unhealthy threshold  no   5    The number of consecutive failed health checks that must occur before declaring an EC2 instance unhealthy       aws elb healthy threshold     no   3  The number of consecutive successful health checks that must occur before declaring an EC2 instance healthy      aws elb cross az load balancing  no   yes yes  no   Distribute load across all configured Availability Zones      aws elb draining timeout  no  20     Wait a specified timeout allowing connections to drain before terminating an instance     aws resource tags    yes          a hash dictionary of tags to add to the new instance or for starting stopping instance by tag     key   value    and    VREnv   PROD   VRProject   sample   VRTeam   infra    Name   instance name        aws wait timeout   no   600      how long before wait gives up  in seconds      state    no    present  present  absent  create or destroy elb      region    yes        The AWS region to use  Must be specified if ec2 url is not used  If not specified then the value of the EC2 REGION environment variable  if any  is used  See http   docs aws amazon com general latest gr rande html ec2 region      vivareal project build   yes       elb name     Ansible modules   ec2 elb lb   Output variables   ec2 load balancer  created elb name  healthcheck dns name  dns name of elb    Example Playbook     hosts  localhost   vars      aws resource tags          Name    my elb name         VREnv    PROD         VRProject    infra ansible         VRTeam    infra       region  us east 1     aws elb instance listeners    protocol   http  load balancer port   80  instance protocol   http  instance port   80        aws elb subnets    subnet 0959b37f       aws elb security group ids    sg a25ea6db       aws elb ping protocol   tcp      vivareal project name  my elb name     vivareal build version  1     vivareal project build      vivareal project name       vivareal build version     roles          role  aws elb      Destroy stack     hosts  localhost   vars      vivareal project name  my elb name   roles          role  aws elb  state  absent      License   BSD   Author    Giancarlo Rubio   gianrubio gmail com   scarlett packer   Packer templates for creating scarlett vagrant images  and potentially raspberry pi preseed cfg s later    Packer  templates for  Vagrant  base boxes    I borrowed quite a bit of logic from https   github com kaorimatz packer templates   Thank you for figuring out all the very difficult stuff    His work allowed me to focus more on the desktop side of things    Usage   Clone the repository      git clone https   github com bossjones scarlett packer    cd scarlett packer    Build a machine image from the template in the repository      packer build  only virtualbox iso ubuntu 16 04 amd64 json    Add the built box to Vagrant      vagrant box add scarlett 1604 ubuntu 16 04 amd64 virtualbox box    Configuration   You can configure each template to match your requirements by setting the following  user variables     User Variable         Default Value   Description                                                                                                                                  compression level    6                Documentation   cpus                 1               Number of CPUs   disk size            40000            Documentation   headless             0                Documentation   memory               512             Memory size in MB   mirror                               A URL of the mirror where the ISO image is available   Example   Build an uncompressed Arch Linux vagrant box with a 4GB hard disk using the VirtualBox provider      packer build  only virtualbox iso  var compression level 0  var disk size 4000 ubuntu 16 04 amd64 json    Pre built Boxes   You can also use the pre built boxes hosted on  Atlas       vagrant box add bossjones scarlett 1604  boss toolbox     Bunch of tools I like to use for debugging shit   FAQ   Q  I tried to run perf and I got this  What do I do     perf stat ls Error  You may not have permission to collect stats  Consider tweaking  proc sys kernel perf event paranoid    1   Not paranoid at all   0   Disallow raw tracepoint access for unpriv   1   Disallow cpu events for unpriv   2   Disallow kernel profiling for unpriv   A  On your host   READ  Not docker container    run the following    sudo sh  c  echo 1   proc sys kernel perf event paranoid    or   sysctl  w kernel perf event paranoid  1    PERSIST REBOOTS    sudo sh  c  echo kernel perf event paranoid 1    etc sysctl d 00 local conf    Q  What does any of that stuff mean    source  https   unix stackexchange com questions 14227 do i need root admin permissions to run userspace perf tool perf events ar   What you can do with perf without being root depends on the  kernel perf event paranoid  sysctl setting    kernel perf event paranoid    2  you can t take any measurements  The perf utility might still be useful to analyse existing records with perf ls  perf report  perf timechart or perf trace    kernel perf event paranoid    1  you can trace a command with perf stat or perf record  and get kernel profiling data    kernel perf event paranoid    0  you can trace a command with perf stat or perf record  and get CPU event data    kernel perf event paranoid     1  you get raw access to kernel tracepoints  specifically  you can  mmap  the file created by  perf event open   I don t know what the implications are     Q  Can you explain why you chose those docker run settings          ipc host   By default  all containers have the IPC namespace enabled    IPC  POSIX SysV IPC  namespace provides separation of named shared memory segments  semaphores and message queues    Shared memory segments are used to accelerate inter process communication at memory speed  rather than through pipes or through the network stack  Shared memory is commonly used by databases and custom built  typically C OpenMPI  C   using boost libraries  high performance applications for scientific computing and financial services industries  If these types of applications are broken into multiple containers  you might need to share the IPC mechanisms of the containers              net host     network  bridge    Connect a container to a network                        bridge   create a network stack on the default Docker bridge                        none   no networking                        container     reuse another container s network stack                        host   use the Docker host network stack                              connect to a user defined network             pid host   By default  all containers have the PID namespace enabled    PID namespace provides separation of processes  The PID Namespace removes the view of the system processes  and allows process ids to be reused including pid 1    In certain cases you want your container to share the host s process namespace  basically allowing processes within the container to see all of the processes on the system  For example  you could build a container with debugging tools like strace or gdb  but want to use these tools when debugging processes within the container        Additionally  the operator can set any environment variable in the container by using one or more  e flags  even overriding those mentioned above  or already defined by the developer with a Dockerfile ENV  If the operator names an environment variable without specifying a value  then the current value of the named variable is propagated into the container s environment    https   www projectatomic io blog 2015 09 introducing the fedora tools image for fedora atomic host    https   fedoraproject org wiki StackTraces boss docker python3   Bossjones repo for Ubuntu 16 04 w  Python 3 5 2 compiled from source    Build   docker build  t bossjones boss docker python3   boss docker gnome pygobject gtk3 gst cmusphinx jhbuild   Gnome x Jhbuild x PyGObject x Cmusphinx x Gtk 3 in                 Jhbuild PyGObject3 Docker Image   Dockerfile             NOTE  This is a prereq for  scarlett os   It makes some strong assumptions about how you plan on running jhbuild  and should mainly just run on CI systems    Docker container that installs an jhbuild environment that has the following      Python3   Jhbuild   Glib   Gobject introspection   Gstreamer   Gst Espeak Plugin   Gtk3   Pocketsphinx Sphinxbase     Compiling jhbuild and deps   Prerequisites     Building from tarball requires      gcc or clang   for compiling     libxft   for font rendering     libxinerama   for XINERAMA support     gdk pixbuf2   for pixmap rendering     libxrandr   for XRANDR support     libsndfile   for sound support     libsm   for X11R6 session management     fribidi   for i18n text rendering       Building from git also requires      git   for cloning the git repository and updating changelogs     autoconf automake or cmake toolchain   for build scripts     xorg mkfontdir   for installing themes     asciidoctor or asciidoc   for creating html documentation     markdown   for building release package       Build   docker build  t docker gnome pygobject gtk3 gst cmusphinx jhbuild     Links     https   github com search q execlineb sshd type Code utf8  E2 9C 93     Order of operations   jhbuild pygobject3 1     init  no run d scripts jhbuild pygobject3 1     run  starting process manager jhbuild pygobject3 1     s6 init  making user provided files available at  var run s6 etc   exited 0  jhbuild pygobject3 1     s6 init  ensuring user provided files have correct perms   exited 0  jhbuild pygobject3 1     fix attrs d  applying ownership   permissions fixes    jhbuild pygobject3 1     fix attrs d  done  jhbuild pygobject3 1     cont init d  executing container initialization scripts    jhbuild pygobject3 1     cont init d  00 init ssh  executing    jhbuild pygobject3 1     cont init d  00 init ssh  exited 0  jhbuild pygobject3 1     cont init d  done  jhbuild pygobject3 1     services d  starting services jhbuild pygobject3 1     services d  done    Environment Variables   Variable   Example   Description                  S6 KILL FINISH MAXTIME     S6 KILL FINISH MAXTIME 1    Wait time  in ms  for zombie reaping before sending a kill signal  S6 KILL GRACETIME     S6 KILL GRACETIME 1    Wait time  in ms  for S6 finish scripts before sending kill signal  SERVER LOG MINIMAL     SERVER LOG MINIMAL 1    Wait time  in ms  for S6 finish scripts before sending kill signal  SERVER APP NAME     SERVER APP NAME jhbuild compile    Set application name for stdout logging info  COMPOSE PROJECT NAME     COMPOSE PROJECT NAME jhbuild compile    The default project name is the basename of the project directory  You can set a custom project name by using the  p command line option or the this environment variable   SCARLETT ENABLE SSHD     SCARLETT ENABLE SSHD 1    When set to 0  openssh server will be enabled for development use w  VSCode or Sublime  SCARLETT ENABLE DBUS     SCARLETT ENABLE DBUS  true     When set  a session dbus service will be started  SCARLETT BUILD GNOME     SCARLETT BUILD GNOME  true     When set  jhbuild and deps will be compiled  TRAVIS CI     TRAVIS CI  true     Signal s6 to stop when finished all run d scripts  Important for CI builds      with contenv  tool  which is used to expose environment variables across scripts  has a limitation that it cannot read beyond 4k characters for environment variable values  To work around this issue  use the script   scripts with bigcontenv  instead of  with contenv   You ll need to remove the  with contenv  from the shebang line  and add   source  scripts with bigcontenv  in the next line after the shebang line      Startup Runtime Modification   To inject changes just before runtime  shell scripts may be placed into the   etc cont init d  folder  As part of the process manager  these scripts are run in advance of the supervised processes   see https   github com just containers s6 overlay executing initialization andor finalization tasks   Optional Arguments   Variable   Example   Description                  SCARLETT ENABLE SSHD     SCARLETT ENABLE SSHD 0    When set to 0  openssh server will be enabled for development use w  VSCode or Sublime  SCARLETT ENABLE DBUS     SCARLETT ENABLE DBUS  true     When set  a session dbus service will be started  SCARLETT BUILD GNOME     SCARLETT BUILD GNOME  true     When set  jhbuild and deps will be compiled  TRAVIS CI     TRAVIS CI  true     Signal s6 to stop when finished all run d scripts  Important for CI builds        boss docker base gtk3 deps   Docker container that installs a bunch of gtk3 packages in preparation for usage with jhbuild  All of these are sytem packages installed by  apt   This is a prereq for  jhbuild  environments    boss docker base gtk3 deps   Gnome x Jhbuild x PyGObject x Cmusphinx x Gtk 3 in                   Base GTK3 Deps Docker Image   Dockerfile             NOTE  This is a prereq for  scarlett os  and  boss docker jhbuild pygobject3   It makes some strong assumptions about how you plan on running jhbuild  and should mainly just run on CI systems    Docker container that installs the following apt dependencies an jhbuild environment that has the following      Python3   Glib   Gobject introspection   Gstreamer   Gst Espeak Plugin   Gtk3   Pocketsphinx Sphinxbase     Build   docker build  t bossjones boss docker base gkt3 deps     Links     https   github com search q execlineb sshd type Code utf8  E2 9C 93     Order of operations   jhbuild pygobject3 1     init  no run d scripts jhbuild pygobject3 1     run  starting process manager jhbuild pygobject3 1     s6 init  making user provided files available at  var run s6 etc   exited 0  jhbuild pygobject3 1     s6 init  ensuring user provided files have correct perms   exited 0  jhbuild pygobject3 1     fix attrs d  applying ownership   permissions fixes    jhbuild pygobject3 1     fix attrs d  done  jhbuild pygobject3 1     cont init d  executing container initialization scripts    jhbuild pygobject3 1     cont init d  00 init ssh  executing    jhbuild pygobject3 1     cont init d  00 init ssh  exited 0  jhbuild pygobject3 1     cont init d  done  jhbuild pygobject3 1     services d  starting services jhbuild pygobject3 1     services d  done    Environment Variables   Variable   Example   Description                  S6 KILL FINISH MAXTIME     S6 KILL FINISH MAXTIME 1    Wait time  in ms  for zombie reaping before sending a kill signal  S6 KILL GRACETIME     S6 KILL GRACETIME 1    Wait time  in ms  for S6 finish scripts before sending kill signal  SERVER LOG MINIMAL     SERVER LOG MINIMAL 1    Wait time  in ms  for S6 finish scripts before sending kill signal  SERVER APP NAME     SERVER APP NAME jhbuild compile    Set application name for stdout logging info  COMPOSE PROJECT NAME     COMPOSE PROJECT NAME jhbuild compile    The default project name is the basename of the project directory  You can set a custom project name by using the  p command line option or the this environment variable   SCARLETT ENABLE SSHD     SCARLETT ENABLE SSHD 1    When set to 0  openssh server will be enabled for development use w  VSCode or Sublime  SCARLETT ENABLE DBUS     SCARLETT ENABLE DBUS  true     When set  a session dbus service will be started  SCARLETT BUILD GNOME     SCARLETT BUILD GNOME  true     When set  jhbuild and deps will be compiled  TRAVIS CI     TRAVIS CI  true     Signal s6 to stop when finished all run d scripts  Important for CI builds      with contenv  tool  which is used to expose environment variables across scripts  has a limitation that it cannot read beyond 4k characters for environment variable values  To work around this issue  use the script   scripts with bigcontenv  instead of  with contenv   You ll need to remove the  with contenv  from the shebang line  and add   source  scripts with bigcontenv  in the next line after the shebang line      Startup Runtime Modification   To inject changes just before runtime  shell scripts may be placed into the   etc cont init d  folder  As part of the process manager  these scripts are run in advance of the supervised processes   see https   github com just containers s6 overlay executing initialization andor finalization tasks   Optional Arguments   Variable   Example   Description                  SCARLETT ENABLE SSHD     SCARLETT ENABLE SSHD 0    When set to 0  openssh server will be enabled for development use w  VSCode or Sublime  SCARLETT ENABLE DBUS     SCARLETT ENABLE DBUS  true     When set  a session dbus service will be started  SCARLETT BUILD GNOME     SCARLETT BUILD GNOME  true     When set  jhbuild and deps will be compiled  TRAVIS CI     TRAVIS CI  true     Signal s6 to stop when finished all run d scripts  Important for CI builds          gosa   Test NOTE    This readme  and most likely a lot of the work in this repo were probably borrowed from the awesome work that  docent net fedora desktop ansible  did    oh my fedora24   This is my provisioner for the Fedora 24 Gnome3   Wayland generic desktop  Feel free to use it  modify  contribute etc  Always interested in your comments   write me    This repository has submodule dependency  All Ansible roles are placed in  roles  directory which is a git submodule    Usage      Clone this repo   Edit  group vars all yml   Install prerequisites on destination box   dnf  y install ansible sudo python dnf libselinux python   Create entry for your user in   etc sudoers d user   Enter proper  ansible ssh host  and  ansible ssh user  in  hosts  file   Make sure it works   ansible  m ping  your host    Rollout   ansible playbook playbook yml     Of course you may install just a specified part of the whole installation by using tags  List available tags with      ansible playbook playbook yml   list tags   And install specified part with      ansible playbook playbook yml   tags dropbox virtualbox   If you want to only update packages  like  dnf update  y  simply run      ansible playbook playbook yml   tags pkgs update   skip tags install   Contributing   committing changes     In order to modify those roles a bit to match your needs simply fork and work on your forked repo   If you feel like your change should be added to thise repo   create Pull Request  thanks btw       Managing multiple desktops   I use this repository in order to manage 5  laptops  There are two methods to achieve this multi   desktop configuration      First is to keep every workstation configuration in different branch  This way every workstation    has its own  playbook yml  and  hosts  file  Downside is the need for keeping branches in sync     which is a basic merge so it s not a rocket science     Second is to keep every workstation provisioner in different playbook  e g  in  plays  directory      In this scenario you keep all hosts configuration in one  hosts  file      Malcolm Jones  bossjones   https   github com geerlingguy ansible role solr issues 64   Known Problems   ansible   https   github com geerlingguy ansible role solr issues 64 https   github com ansible ansible issues 23358       TASK  ksylvan docker   fedora repo                  ok   hyena org            msg              age    215            changed   false           connection    close            content      DOCTYPE html  n n Index of  repo main fedora   n n Index of  repo main fedora   n      n 20                                           01 Dec 2016 19 36    n 21                                           01 Dec 2016 19 36    n 22                                           01 Dec 2016 19 36    n 23                                           10 Jan 2017 23 53    n 24                                           05 Apr 2017 23 12    n 25                                           05 Apr 2017 23 12    n  n            content length    687            content type    text html            date    Thu  06 Apr 2017 01 34 13 GMT            etag      8cb7fa660761d3b6a8a824f2bf396dc0              last modified    Thu  06 Apr 2017 00 39 19 GMT            msg    OK  687 bytes             redirected   false           server    AmazonS3            status   200           url    https   yum dockerproject org repo main fedora             via    1 1 9aaf336897fdd8a2dfd1b375c61d8b0b cloudfront net  CloudFront             x amz cf id    B9bF3GbU36hAYJhtIDXoQGXu9EiYcUUSXEWDDzyxMPzEakpPchZELw              x amz meta s3cmd attrs    uid 0 gname root uname root gid 0 mode 33261 mtime 1491434020 atime 1491434299 md5 8cb7fa660761d3b6a8a824f2bf396dc0 ctime 1491434020            x amz version id    hCdPIsuNZF5gW6GfubBq2gcvxNtxCAkw            x cache    Hit from cloudfront           TASK  ksylvan docker   ansible distribution version             ok   hyena org            msg    24      TASK  ksylvan docker   Look for the exact version             fatal   hyena org   FAILED       failed   true   msg    The conditional check     ansible distribution version     in   fedora repo content    failed  The error was  Invalid conditional detected  invalid syntax     line 1  n nThe error appears to have been in   Users malcolm dev bossjones oh my fedora24 roles ksylvan docker tasks main yml   line 27  column 3  but may nbe elsewhere in the file depending on the exact syntax problem  n nThe offending line appears to be  n n n  name  Look for the exact version n    here n           to retry  use    limit   Users malcolm dev bossjones oh my fedora24 playbook retry   PLAY RECAP                        hyena org                    ok 13   changed 0    unreachable 0    failed 1   Ansible failed to complete successfully  Any error output should be visible above  Please fix these errors and try again          https   jedi readthedocs io en latest docs usage html tab completion in the python shell            pythonrc py   try      from jedi utils import setup readline     setup readline   except ImportError        Fallback to the stdlib readline completer if it is installed        Taken from http   docs python org 2 library rlcompleter html     print  Jedi is not installed  falling back to readline       try          import readline         import rlcompleter         readline parse and bind  tab  complete       except ImportError          print  Readline is not installed either  No tab completion is enabled          dnf  y copr enable dperson neovim dnf  y install neovim dnf  y install python3 neovim python3 neovim gui       WARNING  IPv4 forwarding is disabled WARNING  bridge nf call iptables is disabled WARNING  bridge nf call ip6tables is disabled   net bridge bridge nf call ip6tables   1 net bridge bridge nf call iptables   1   net ipv6 conf default router solicitations   0 net ipv6 conf default accept ra rtr pref   0 net ipv6 conf default accept ra pinfo   0 net ipv6 conf default accept ra defrtr   0 net ipv6 conf default autoconf   0 net ipv6 conf default dad transmits   0 net ipv6 conf default max addresses   1    sbin sysctl  w kernel domainname  example com     sbin sysctl  w net bridge bridge nf call iptables  1            WARNING  IPv4 forwarding is disabled WARNING  bridge nf call iptables is disabled WARNING  bridge nf call ip6tables is disabled   net bridge bridge nf call ip6tables   1 net bridge bridge nf call iptables   1   net ipv6 conf default router solicitations   0 net ipv6 conf default accept ra rtr pref   0 net ipv6 conf default accept ra pinfo   0 net ipv6 conf default accept ra defrtr   0 net ipv6 conf default autoconf   0 net ipv6 conf default dad transmits   0 net ipv6 conf default max addresses   1    sbin sysctl  w kernel domainname  example com     sbin sysctl  w net bridge bridge nf call iptables  1        Fix  Re associate vagrant with vm   vmdk gets misplaced   https   github com hashicorp vagrant issues 1755   Example fix   for above     VBoxManage list vms   grep hyena   cut  d     f2   sed  s    g    sed  s    g      dev bossjones oh my fedora24  vagrant machines hyena org virtualbox id   Folder structure example    2 2 3     hyenatop in   dev user scarlett ansible  vagrant machines default virtualbox    featutre 1604   1      ls action provision action set name  creator uid      id               index uuid       synced folders reproduce pytest mock issue 84   reproduce pytest mock issue 84   Disclaimer   I put this repo together pretty quickly  in between the work week in an effort to repoduce the issues I saw while working w   pytest  and  pytest mock  for my personal project   scarlett os   I left a lot of the modules that I originally used in this repo  along w  some other things that might be a bit unecessary  The purpose of that was to create an environment that was as close as possible to what I m actually using     just minus some of the complexity    Requirements     Docker For Mac   Also tested on fedora 24 in virtualbox       Specifically i m using          docker   version Docker version 1 12 5  build 7392c3b     docker compose   version docker compose version 1 9 0  build 2585387    2 2 3    Malcolms MBP 3 in   dev           Setup   1  Start docker container via docker compose   make docker compose   2  Exec into the container using bash   make docker exec   3  Once in the container  run bootstrap command to install dependencies         from inside container   make bootstrap       4  Enable virtualenv and run tests   A  How to repoduce error         from inside container   workon repoduce pytest mock issue 84 make test       B  Run tests w  mocker stopall   to fix  leak    When you set this environment variable  mocker stopall   runs at the beginning and end of each test case          from inside container   workon repoduce pytest mock issue 84 ENABLE STOPALL 1 make test      docker java jmx newrelic demo   Just a demo application to visualize the type of statistics we can get from running jmx or newrelic in java docker scala jmx newrelic demo   Just a demo application to visualize the type of statistics we can get from running jmx or newrelic in scala Gnome Builder  from master branch  Image based on Fedora Rawhide   What do I need      local ip address   How do I get it    ifconfig en0   grep  inet    awk   print  2        or    npm install  g my local ip           socat   How do I get it    brew install socat    package manager  install socat           XQuartz   How do I get it    brew cask install xquartz             Helpful aliases   alias docker x11 socat  socat TCP LISTEN 6000 reuseaddr fork UNIX CLIENT    DISPLAY    alias docker x11 jess geary  docker run   rm   name geary  e DISPLAY  ifconfig en0   grep  inet      cut  d     f2  0 jess geary  alias my ip  ifconfig en0   grep  inet      cut  d     f2  alias docker eclipse  docker run  it  e DISPLAY  ifconfig en0   grep  inet      cut  d     f2  0 batmat docker eclipse  alias docker xquartz  open  a XQuartz    Prereq     Start Up socat   alias docker x11 socat  socat TCP LISTEN 6000 reuseaddr fork UNIX CLIENT    DISPLAY        I am not a GNOME developer  but found no easy way to try Builder on my machine  So I decided to create this image  Simple like this    To execute it      docker run   rm  it  e DISPLAY  DISPLAY  v  tmp  X11 unix  tmp  X11 unix leandrosansilva gnome builder     MY IP   ifconfig en0   grep  inet    awk   print  2        docker run   rm  e DISPLAY  MY IP 0      i  t     bossjones gnome builder docker image bash   MY IP   ifconfig en0   grep  inet    awk   print  2        docker run   rm  e DISPLAY 192 168 0 3 0      i  t      v  Users timlinux  home timlinux       kartoza qgis desktop qgis   docker run   rm  it  e DISPLAY 192 168 1 129 0   docker build  t kartoza qgis desktop git   github com kartoza docker qgis desktop   docker build  t bossjones gnome builder     PS  This image is  huge   about 2GB in size  s6 lab   Series of scripts to test out different s6 overlay execlineb scripts  Dev purposes only  Scarlett Devpi Docker   Inspired by      https   github com scrapinghub docker devpi   https   gist github com wassname 18895c4d62ed842fba1f   https   github com muccg docker devpi     This repository contains  Dockerfile  of  Devpi  for  Docker  s  trusted build  published to the public  Docker Registry     Dependencies     dockerfile ubuntu     Installation       Install  Docker         Download  trusted build  from public  Docker Registry    docker pull scrapinghub devpi        alternatively  you can build an image from Dockerfile   docker build  t  scrapinghub devpi  github com scrapinghub docker devpi     Usage   Run  devpi server   docker run  d   name devpi  p 3141 3141 scrapinghub devpi    Devpi creates a user named  root  by default  its password can be set with  DEVPI PASSWORD  environment variable  boss cheatsheets   cheat  allows you to create and view interactive cheatsheets on the command line  It was designed to help remind  nix system administrators of options for commands that they use frequently  but not frequently enough to remember    This repo contains a number of commands that bossjones likes to use when debugging things  feel free to contribute to the list by making a PR    Install custom cheatsheets      sh   clone this repo somewhere on your laptop   git clone https   github com bossjones boss cheatsheets   dev behance devops cheatsheets   Edit the cheat yaml config to reference the location of your cheatsheets     dev bossjones boss cheatsheets feature git search history   cat    config cheat conf yml     The editor to use with  cheat  e     Defaults to  EDITOR or  VISUAL    editor  vim   Should  cheat  always colorize output    colorize  true   Which  chroma  colorscheme should be applied to the output    Options are available here    https   github com alecthomas chroma tree master styles   style  monokai   Which  chroma   formatter  should be applied    One of   terminal    terminal256    terminal16m    formatter  terminal16m   Through which pager should output be piped   Unset this key for no pager     pager  less  FRX   The paths at which cheatsheets are available  Tags associated with a cheatpath   are automatically attached to all cheatsheets residing on that path      Whenever cheatsheets share the same title  like  tar    the most local   cheatsheets  those which come later in this file  take precedent over the   less local sheets  This allows you to create your own  overides  for    upstream  cheatsheets      But what if you want to view the  upstream  cheatsheets instead of your own    Cheatsheets may be filtered via  cheat  t    in combination with other   commands  So  if you want to view the  tar  cheatsheet that is tagged as    community  rather than your own  you can use  cheat tar  t community   cheatpaths      Paths that come earlier are considered to be the most  global   and will     thus be overridden by more local cheatsheets  That being the case  you     should probably list community cheatsheets first          Note that the paths and tags listed below are placeholders  You may freely     change them to suit your needs          Community cheatsheets must be installed separately  though you may have     downloaded them automatically when installing  cheat   If not  you may     download them here          https   github com cheat cheatsheets         Once downloaded  ensure that  path  below points to the location at which     you downloaded the community cheatsheets      name  behance                     a name for the cheatpath     path   Users malcolm dev behance devops cheatsheets   the path s location on the filesystem     tags    behance                   these tags will be applied to all sheets on the path     readonly  false                      if true   cheat  will not create new cheatsheets here         name  personal       path   Users malcolm documents cheat personal    this is a separate directory and repository than above       tags    personal         readonly  false                     new sheets may be written here     name  community     path   Users malcolm  config cheat cheatsheets community     tags    community       readonly  true         If you have personalized cheatsheets  list them last  They will take       precedence over the more global cheatsheets        name  personal       path   Users malcolm  config cheat cheatsheets personal       tags    personal         readonly  false     While it requires no configuration here  it s also worth noting that      cheat  will automatically append directories named   cheat  within the     current working directory to the  cheatpath   This can be very useful if     you d like to closely associate cheatsheets with  for example  a directory     containing source code          Such  directory scoped  cheatsheets will be treated as the most  local      cheatsheets  and will override less  local  cheatsheets  Likewise      directory scoped cheatsheets will always be editable   readonly  false            Update custom cheatsheets      sh   update cheatsheets   pushd    cheat git pull popd   View cheatsheets   cheat  l       Example   Forgot all of your go to commands for sysdig  No problem  just run    sh cheat sysdig   You will be presented with a cheatsheet resembling       sh   Basic Command List   Capture all the events from the live system and print them to screen   sysdig   Capture all the events from the live system and save them to disk   sysdig  qw dumpfile scap   Read events from a file and print them to screen   sysdig  r dumpfile scap   Print all the open system calls invoked by cat   sysdig proc name cat and evt type open   Print the name of the files opened by cat     sysdig  p  evt arg name  proc name cat and evt type open   List the available chisels     sysdig  cl   Run the spy ip chisel for the 192 168 1 157 IP address    sysdig  c spy ip 192 168 1 157   Output Format   By default  sysdig prints the information for each captured event on a single line  with the following format                  where  evt time is the event timestamp evt cpu is the CPU number where the event was captured proc name is the name of the process that generated the event thread tid id the TID that generated the event  which corresponds to the PID for single thread processes evt dir is the event direction    for enter events and   for exit events evt type is the name of the event  e g   open  or  read  evt args is the list of event arguments    The output format can be customized with the  p switch  using any of the fields listed by  sysdig  l     Filtering   sysdig filters are specified at the end of the command line  The simplest filter is a simple field value check    sysdig proc name cat   The list of available fields can be obtained with  sysdig  l   Checks can use one of these comparison operators                      and contains  e g    sysdig fd name contains  etc   Multiple checks can be combined through parentheses and the following boolean operators  and  or  not  e g    sysdig  not fd name contains  proc or fd name contains  dev     Chisels   Sysdig s chisels are little scripts that analyze the sysdig event stream to perform useful actions  To get the list of available chisels  type   sysdig  cl   For each chisel  you get the description and the list of arguments it expects  To run one of the chisels  you use the  c flag  e g     sysdig  c topfiles bytes   If a chisel needs arguments  you specify them after the chisel name    sysdig  c spy ip 192 168 1 157   Chiesls can be combined with filters    sysdig  c topfiles bytes  not fd name contains  dev    View the list of containers running on the machine and their resource usage   csysdig  vcontainers   View the list of processes with container context   csysdig  pc   See all the GET HTTP requests made by the machine   sysdig  s 2000  A  c echo fds fd port 80 and evt buffer contains GET   See all the SQL select queries made by the machine   sysdig  s 2000  A  c echo fds evt buffer contains SELECT   See queries made via apache to an external MySQL server happening in real time   sysdig  s 2000  A  c echo fds fd sip 192 168 30 5 and proc name apache2 and evt buffer contains SELECT   To get the list of available chisels  type   sysdig  cl   list process by top CPU inside container   name   sysdig  pc  c topprocs cpu container name mesos ad8a3341 0f7c 4a28 ac47 744c6767990e S18 4b5c695e f885 47f3 9a77 7b024ffa7aa   Show all the interactive commands executed inside the container   sysdig  pc  c spy users container name mesos ad8a3341 0f7c 4a28 ac47 744c6767990e S18 4b5c695e f885 47f3 9a77 7b024ffa7aa1       To see what cheatsheets are available  run  cheat  l     Note that  while  cheat  was designed primarily for  nix system administrators  it is agnostic as to what content it stores  If you would like to use  cheat  to store notes on your favorite cookie recipes  feel free    Installing Cheat CLI   It is recommended to install  cheat  with  pip     Global Install cheat   sh  sudo  pip install cheat   Virtualenv Install cheat   sh mkvirtualenv   python python2 cheat2 pip install cheat   Thanks   Borrowed some of the java cheats from https   github com chhsiao90 cheatsheets java reproduce travisci docker permissions issue   Simple environment to reproduce docker permissions issues  specifically when uid gid of user running container does not match user in container   ansible role bossjones cli tools   ansible role to install bossjones cli favorites  Only works on fedora 24 currently  Role Name   A brief description of the role goes here    Requirements   Any pre requisites that may not be covered by Ansible itself or the role should be mentioned here  For instance  if the role uses the EC2 module  it may be a good idea to mention in this section that the boto package is required    Role Variables   A description of the settable variables for this role should go here  including any variables that are in defaults main yml  vars main yml  and any variables that can should be set via parameters to the role  Any variables that are read from other roles and or the global scope  ie  hostvars  group vars  etc   should be mentioned here as well    Dependencies   A list of other roles hosted on Galaxy should go here  plus any details in regards to parameters that may need to be set for other roles  or variables that are used from other roles    Example Playbook   Including an example of how to use your role  for instance  with variables passed in as parameters  is always nice for users too      hosts  servers   roles           role  bossjones ipv6  x  42      License   BSD   Author Information   An optional section for the role authors to include contact information  or a website  HTML is not allowed   docker swarm vbox lab   lab to try out using docker swarm on virtualized environment using vagrant   Lots of influence from     https   github com deviantony docker elk     Stack Overview  All using docker swarm       Prometheus Server   Prometheus Alertmanager   Node Exporter   Portainer   Grafana   Nginx reverse proxy with SSL TLS  Let s encrypt certificate   ELK stack   Elasticsearch  Logstash  Kibana       Future   Fun things to try to get working       StackStorm  Event driven automation   Bro IDS   Redis Zmq RabbitMq   Some sort of queue solution     zq  Kafka based Job Queue for Python    etcd   Distributed reliable key value store for the most critical data of a distributed system   faas   Functions as a Service  OpenFaaS    opentracing   distributed tracing and context propagation   Zipkin   Jaegar   locustio   Loki   Simple  Distributed Tracing   Vizceral   WebGL visualization for displaying animated traffic graphs   Vizceral Example   Jenkins   For automation of tasks   Vault   hashicorp secret management   Augmented Traffic Control    Facebook Osquery   SQL powered operating system instrumentation  monitoring  and analytics    Hubble   Security compliance framework   doorman   osquery fleet manager   ChaosMonkey   resiliency tool that helps applications tolerate random instance failures   git server docker   Git Server in Docker   gitlab ce   GitLab Community Edition docker image based on the Omnibus package     Research      https   www public tem tsp eu  berger o docker install docker machine virtualbox html   https   unix stackexchange com questions 269912 send command to the shell via makefile   https   docs docker com get started part3    https   stackoverflow com questions 7897549 make ignores my python bash alias   https   botleg com stories monitoring docker swarm with cadvisor influxdb and grafana    https   github com bvis docker prometheus swarm blob master docker compose logging yml     Recover after swarm reboot     https   forums docker com t docker worker nodes shown as down after re start 22329 2   https   github com moby moby issues 23828   https   docs docker com engine swarm admin guide  monitor swarm health   https   docs docker com engine swarm admin guide  run manager only nodes       export discotoken  some token    How to fix  Error response from daemon  404 page not found    Source   DOCKER SWARM MODE SETUP WITH DOCKER MACHINE    http   perica zivkovic nl blog setup docker swarm with docker machine do    Example output from creating a docker swarm         2 2 3    Malcolms MBP 3 in   dev bossjones docker swarm vbox lab    master U 2        bin docker machine x86 64 env local export DOCKER TLS VERIFY  1  export DOCKER HOST  tcp   192 168 99 100 2376  export DOCKER CERT PATH   Users malcolm  docker machine machines local  export DOCKER MACHINE NAME  local    Run this command to configure your shell    eval     bin docker machine x86 64 env local     2 2 3    Malcolms MBP 3 in   dev bossjones docker swarm vbox lab    master U 2      eval     bin docker machine x86 64 env local     2 2 3    Malcolms MBP 3 in   dev bossjones docker swarm vbox lab    master U 2      make dm ls   bin docker machine x86 64 ls NAME                   ACTIVE   DRIVER         STATE     URL                         SWARM   DOCKER        ERRORS dev                             vmwarefusion   Stopped                                       Unknown local                           virtualbox     Running   tcp   192 168 99 100 2376           v17 06 2 ce scarlett 1604 packer            generic        Stopped                                       Unknown    2 2 3    Malcolms MBP 3 in   dev bossjones docker swarm vbox lab    master U 2      make bootstrap swarm   bin docker machine x86 64 create  d virtualbox swarm manager Running pre create checks    Creating machine     swarm manager  Copying  Users malcolm  docker machine cache boot2docker iso to  Users malcolm  docker machine machines swarm manager boot2docker iso     swarm manager  Creating VirtualBox VM     swarm manager  Creating SSH key     swarm manager  Starting the VM     swarm manager  Check network to re create if needed     swarm manager  Waiting for an IP    Waiting for machine to be running  this may take a few minutes    Detecting operating system of created instance    Waiting for SSH to be available    Detecting the provisioner    Provisioning with boot2docker    Copying certs to the local machine directory    Copying certs to the remote machine    Setting Docker configuration on the remote daemon    Checking connection to Docker    Docker is up and running  To see how to connect your Docker Client to the Docker Engine running on this virtual machine  run    bin docker machine x86 64 env swarm manager   bin docker machine x86 64 create  d virtualbox node 01 Running pre create checks    Creating machine     node 01  Copying  Users malcolm  docker machine cache boot2docker iso to  Users malcolm  docker machine machines node 01 boot2docker iso     node 01  Creating VirtualBox VM     node 01  Creating SSH key     node 01  Starting the VM     node 01  Check network to re create if needed     node 01  Waiting for an IP    Waiting for machine to be running  this may take a few minutes    Detecting operating system of created instance    Waiting for SSH to be available    Detecting the provisioner    Provisioning with boot2docker    Copying certs to the local machine directory    Copying certs to the remote machine    Setting Docker configuration on the remote daemon    Checking connection to Docker    Docker is up and running  To see how to connect your Docker Client to the Docker Engine running on this virtual machine  run    bin docker machine x86 64 env node 01   bin docker machine x86 64 create  d virtualbox node 02 Running pre create checks    Creating machine     node 02  Copying  Users malcolm  docker machine cache boot2docker iso to  Users malcolm  docker machine machines node 02 boot2docker iso     node 02  Creating VirtualBox VM     node 02  Creating SSH key     node 02  Starting the VM     node 02  Check network to re create if needed     node 02  Waiting for an IP    Waiting for machine to be running  this may take a few minutes    Detecting operating system of created instance    Waiting for SSH to be available    Detecting the provisioner    Provisioning with boot2docker    Copying certs to the local machine directory    Copying certs to the remote machine    Setting Docker configuration on the remote daemon    Checking connection to Docker    Docker is up and running  To see how to connect your Docker Client to the Docker Engine running on this virtual machine  run    bin docker machine x86 64 env node 02     bin docker machine x86 64 create  d virtualbox node 03    2 2 3    Malcolms MBP 3 in   dev bossjones docker swarm vbox lab    master U 2      MANAGER IP   docker machine ip swarm manager     2 2 3    Malcolms MBP 3 in   dev bossjones docker swarm vbox lab    master U 2      docker swarm   help   Usage   docker swarm COMMAND   Manage Swarm   Options          help   Print usage   Commands    init        Initialize a swarm   join        Join a swarm as a node and or manager   join token  Manage join tokens   leave       Leave the swarm   unlock      Unlock swarm   unlock key  Manage the unlock key   update      Update the swarm   Run  docker swarm COMMAND   help  for more information on a command     2 2 3    Malcolms MBP 3 in   dev bossjones docker swarm vbox lab    master U 2      docker swarm join token   help   Usage   docker swarm join token  OPTIONS   worker manager    Manage join tokens   Options          help     Print usage    q    quiet    Only display token         rotate   Rotate join token    2 2 3    Malcolms MBP 3 in   dev bossjones docker swarm vbox lab    master U 2      MANAGER IP     bin docker machine x86 64 ip swarm manager     2 2 3    Malcolms MBP 3 in   dev bossjones docker swarm vbox lab    master U 2      echo   MANAGER IP  192 168 99 104    2 2 3    Malcolms MBP 3 in   dev bossjones docker swarm vbox lab    master U 2        bin docker machine x86 64 ssh swarm manager docker swarm init   advertise addr   MANAGER IP  Swarm initialized  current node  9r8nhpx908lk2msh9odlge39v  is now a manager    To add a worker to this swarm  run the following command    docker swarm join   token SWMTKN 1 3qdkv66g9isfqftixhz34vxkawosw32vwjebgu0yprpxah5vms 0u8wxklm8ms8wcwxsrbu0rpla 192 168 99 104 2377    To add a manager to this swarm  run  docker swarm join token manager  and follow the instructions     2 2 3    Malcolms MBP 3 in   dev bossjones docker swarm vbox lab    master U 2      WORKER TOKEN     bin docker machine x86 64 ssh swarm manager docker swarm join token   quiet worker     2 2 3    Malcolms MBP 3 in   dev bossjones docker swarm vbox lab    master U 2      echo   WORKER TOKEN   SWMTKN 1 3qdkv66g9isfqftixhz34vxkawosw32vwjebgu0yprpxah5vms 0u8wxklm8ms8wcwxsrbu0rpla     2 2 3    Malcolms MBP 3 in   dev bossjones docker swarm vbox lab    master U 2        bin docker machine x86 64 ssh node 01 docker swarm join   token   WORKER TOKEN    MANAGER IP  2377 This node joined a swarm as a worker     2 2 3    Malcolms MBP 3 in   dev bossjones docker swarm vbox lab    master U 2        bin docker machine x86 64 ssh node 02 docker swarm join   token   WORKER TOKEN    MANAGER IP  2377 This node joined a swarm as a worker     2 2 3    Malcolms MBP 3 in   dev bossjones docker swarm vbox lab    master U 2      eval      bin docker machine x86 64 env swarm manager      2 2 3    Malcolms MBP 3 in   dev bossjones docker swarm vbox lab    master U 2      docker node ls ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS 9r8nhpx908lk2msh9odlge39v     swarm manager       Ready               Active              Leader lcar141vjpbmp8robql4sy6za     node 02             Ready               Active lgmpnbaga2yiowzzwecblreqo     node 01             Ready               Active    2 2 3    Malcolms MBP 3 in   dev bossjones docker swarm vbox lab    master U 2      echo   MANAGER IP  192 168 99 104    2 2 3    Malcolms MBP 3 in   dev bossjones docker swarm vbox lab    master U 2      docker service create        d     name portainer     publish 9000 9000   portainer portainer    H tcp     MANAGER IP  2376 k6ue1in8da665mraodqmnid7a      2 2 3    Malcolms MBP 3 in   dev bossjones docker swarm vbox lab    master U 2      docker ps CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES    2 2 3    Malcolms MBP 3 in   dev bossjones docker swarm vbox lab    master U 2      docker ps  a CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES    2 2 3    Malcolms MBP 3 in   dev bossjones docker swarm vbox lab    master U 2      docker node ls ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS 9r8nhpx908lk2msh9odlge39v     swarm manager       Ready               Active              Leader lcar141vjpbmp8robql4sy6za     node 02             Ready               Active lgmpnbaga2yiowzzwecblreqo     node 01             Ready               Active    2 2 3    Malcolms MBP 3 in   dev bossjones docker swarm vbox lab    master U 2      docker service   help   Usage   docker service COMMAND   Manage services   Options          help   Print usage   Commands    create      Create a new service   inspect     Display detailed information on one or more services   logs        Fetch the logs of a service or task   ls          List services   ps          List the tasks of one or more services   rm          Remove one or more services   scale       Scale one or multiple replicated services   update      Update a service   Run  docker service COMMAND   help  for more information on a command     2 2 3    Malcolms MBP 3 in   dev bossjones docker swarm vbox lab    master U 2      docker service ls ID                  NAME                MODE                REPLICAS            IMAGE                 PORTS k6ue1in8da66        portainer           replicated          1 1                 portainer portainer     9000  9000 tcp    2 2 3    Malcolms MBP 3 in   dev bossjones docker swarm vbox lab    master U 2      gs On branch master Your branch is up to date with  origin master     Changes not staged for commit     use  git add       to update what will be committed     use  git checkout          to discard changes in working directory        modified    Makefile     modified    README md    no changes added to commit  use  git add  and or  git commit  a      2 2 3    Malcolms MBP 3 in   dev bossjones docker swarm vbox lab    master U 2      git add      2 2 3    Malcolms MBP 3 in   dev bossjones docker swarm vbox lab    master S 2      git commit  m  Chg  portainer   master 4609578  Chg  portainer  2 files changed  44 insertions       2 2 3    Malcolms MBP 3 in   dev bossjones docker swarm vbox lab    master  1      gp Counting objects  4  done  Delta compression using up to 4 threads  Compressing objects  100   4 4   done  Writing objects  100   4 4   1 20 KiB   1 20 MiB s  done  Total 4  delta 3   reused 0  delta 0  remote  Resolving deltas  100   3 3   completed with 3 local objects  To github com bossjones docker swarm vbox lab git    6e63286  4609578  master    master    2 2 3    Malcolms MBP 3 in   dev bossjones docker swarm vbox lab    master            Dashboard for Grafana   Borrowed from https   github com botleg swarm monitoring     Source  https   github com botleg swarm monitoring blob master dashboard json   Prometheus   Borrowed from https   github com vegasbrianc prometheus   SSL configuration example     https   github com danguita prometheus monitoring stack     SMTP server for alertmanager     https   hub docker com r marvambass versatile postfix      networking external example     https   github com bvis docker prometheus swarm blob master docker compose logging yml     networking on swarm classic     Multi host networking with standalone swarms     Install Grafana Plugins   grafana cli admin       http   docs grafana org plugins installation    http   docs grafana org administration cli      Consul setup and config     https   medium com zendesk engineering making docker and consul get along 5fceda1d52b9   https   www consul io docs guides consul containers html   https   blog octo com en how does it work docker part 1 swarm general architecture      Networking problems     https   github com docker compose issues 2908   https   docs docker com v17 06 compose compose file  external 1     Example  In the example below  proxy is the gateway to the outside world  Instead of attempting to create a network called  projectname  outside  Compose will look for an existing network simply called outside and connect the proxy service s containers to it        version   2    services    proxy      build    proxy     networks          outside         default   app      build    app     networks          default   networks    outside      external  true           https   blog octo com en how does it work docker part 3 load balancing service discovery and security        LOOK AT ME  Service is not DNS resolvable from another one if containers run on different nodes https   github com docker swarmkit issues 1429       https   github com docker swarmkit issues 1716     https   github com nlandolfi mixer blob 75ecdd0ad2959b0088ea75810ed1755b83e74490 deploy kube conf import dashboard sh     IOT Solution     https   medium com  DazWilkin docker swarm and prometheus fd19462f1bf8     Node file Service Discovery     https   github com SphericalElephant ansible role prometheus node exporter blob master defaults main yml      collectors enabled conntrack diskstats entropy filefd filesystem loadavg mdadm meminfo netdev netstat sockstat stat textfile time uname vmstat   This is Ansible FYI    prometheus node exporter parameters        collectors enabled    prometheus node exporter collectors enable   join                web listen address    prometheus node exporter web listen address           log level info        collector diskstats ignored devices   ram loop fd  d          collector filesystem ignored mount points    sys proc dev run              collector netdev ignored devices     prometheus node exporter collector netdev ignored devices            collector textfile directory  var lib prometheus node exporter    Monitoring a Docker Swarm Cluster with Prometheus     https   chmod666 org 2017 08 monitoring a docker swarm cluster with prometheus     elasticsearch env vars example   ELASTICSEARCH URL http   elasticsearch 9200 XPACK GRAPH ENABLED false XPACK ML ENABLED false XPACK MONITORING ENABLED false XPACK REPORTING ENABLED false XPACK SECURITY ENABLED false   FIXME  Env vars   elasticsearch  kibana  logstash         For compatibility with container orchestration systems  these environment variables are written in all capitals  with underscores as word separators  The helper translates these names to valid Kibana setting names    example   services    kibana      image  docker elastic co kibana kibana 5 6 2     environment        SERVER NAME  kibana example org       ELASTICSEARCH URL  http   elasticsearch example org   docker defaults   Docker defaultsedit The following settings have different default values when using the Docker image    server host    0    elasticsearch url   http   elasticsearch 9200   elasticsearch username   elastic   elasticsearch password   changeme   xpack monitoring ui container elasticsearch enabled   true   one more example   source  https   www elastic co guide en elasticsearch reference current docker html   version   2  services    elasticsearch1      image  docker elastic co elasticsearch elasticsearch 5 6 2     container name  elasticsearch1     environment          cluster name docker cluster         bootstrap memory lock true          ES JAVA OPTS  Xms512m  Xmx512m      ulimits        memlock          soft   1         hard   1     mem limit  1g     volumes          esdata1  usr share elasticsearch data     ports          9200 9200     networks          esnet   elasticsearch2      image  docker elastic co elasticsearch elasticsearch 5 6 2     environment          cluster name docker cluster         bootstrap memory lock true          ES JAVA OPTS  Xms512m  Xmx512m           discovery zen ping unicast hosts elasticsearch1      ulimits        memlock          soft   1         hard   1     mem limit  1g     volumes          esdata2  usr share elasticsearch data     networks          esnet   volumes    esdata1      driver  local   esdata2      driver  local   networks    esnet        See  https   www elastic co guide en kibana current  configuring kibana on docker html swarm mode init script This stack deploys all the components needed to monitor a Docker Swarm cluster      prometheus     alertmanager     grafana     blackbox exporter     docker exporter     node exporter     cAdvisor     fregate  free mobile sms api   If you want to expose some components through Traefik reverse proxy you first have to have an overlay network with Traefik running in it  In my compose file this overlay network is called  traefik net    To deploy the stack run the following command        docker stack deploy   compose file prometheus yml   prometheus       This stack uses configs and secrets   compose  v3 3       Modify the pushover credentials in configs alertmanager yml   Modify the Traefik basic auths and url in promtheus yml   If using the free sms api put you username and password in    secrets fregate user   secrets fregate password       local mesos cluster   Quick setup to allow anyone to bring up a local mesos marathon environment using docker compose    config      local mesos cluster config json    local mesos cluster          docker machines              instances                  local mesos cluster                      docker                          tls verify    1                        host    tcp   192 168 99 101 2376                        cert path     Users user  docker machine machines local mesos cluster                        machine name    local mesos cluster                                                    pyspark lab   Scripts to try out pyspark with  Use with https   github com jupyter docker stacks tree master pyspark notebook   Spark practice     https   github com XD DENG Spark practice   https   github com mahmoudparsian pyspark tutorial   https   github com svenkreiss pysparkling   https   github com ksindi kafka pyspark demo   https   github com confluentinc cp docker images wiki Getting Started     https   docs confluent io current connect managing html       http   activisiongamescience github io 2016 06 15 Kafka Client Benchmarking        kafkacat   https   github com edenhill kafkacat         see info about your image   docker run   rm kafkacat   produce stuff  Ctrl C to exit    echo  msg 1    docker run  i   rm   net host kafkacat  b mybroker  t logs  P   consume stuff  Ctrl C to exit    docker run   rm  t   net host kafkacat  b mybroker  t logs  C   produce from file or command inside the container   echo 1   example log docker run   name test producer  d  v   pwd  example log  logs example log   entrypoint  bin bash   net host kafkacat      c  tail  f  logs example log   kafkacat  b mybroker  t logs  P  echo 2    example log   the last example runs in background so clean up   docker kill test producer    docker rm test producer       remote jmx with docker   https   ptmccarthy github io 2014 07 24 remote jmx with docker    An important Docker related note about the Tomcat configuration above is that the  Djava rmi server hostname must be set to the externally accessible IP address of the Tomcat server  You want to use the address of the Docker host  not the Docker assigned internal IP address    troubleshooting host networking on zookeeper   Should see something like this      2017 11 24 03 31 32 185  INFO Server environment java library path  usr java packages lib amd64  usr lib64  lib64  lib  usr lib  org apache zookeeper server ZooKeeperServer   2017 11 24 03 31 32 185  INFO Server environment java io tmpdir  tmp  org apache zookeeper server ZooKeeperServer   2017 11 24 03 31 32 185  INFO Server environment java compiler  NA   org apache zookeeper server ZooKeeperServer   2017 11 24 03 31 32 185  INFO Server environment os name Linux  org apache zookeeper server ZooKeeperServer   2017 11 24 03 31 32 185  INFO Server environment os arch amd64  org apache zookeeper server ZooKeeperServer   2017 11 24 03 31 32 185  INFO Server environment os version 4 4 17 boot2docker  org apache zookeeper server ZooKeeperServer   2017 11 24 03 31 32 185  INFO Server environment user name root  org apache zookeeper server ZooKeeperServer   2017 11 24 03 31 32 185  INFO Server environment user home  root  org apache zookeeper server ZooKeeperServer   2017 11 24 03 31 32 185  INFO Server environment user dir    org apache zookeeper server ZooKeeperServer   2017 11 24 03 31 32 195  INFO tickTime set to 2000  org apache zookeeper server ZooKeeperServer   2017 11 24 03 31 32 195  INFO minSessionTimeout set to  1  org apache zookeeper server ZooKeeperServer   2017 11 24 03 31 32 195  INFO maxSessionTimeout set to  1  org apache zookeeper server ZooKeeperServer   2017 11 24 03 31 32 206  INFO binding to port 0 0 0 0 0 0 0 0 32181  org apache zookeeper server NIOServerCnxnFactory    Monitoring  pyspark      https   github com JasonMWhite spark datadog relay   https   github com jvutukur Data Visualization   https   github com wangcunxin wespark blob 58805b88bb56c27d4116f5bf3e1efdd861798f1d bblink graphite kafka graphite streaming py   https   github com hopshadoop hops yarn ML blob 7e54f12bafeaae62fde40eb6a18ebcac4a6f5e9e yarn machine learning py   https   tlfvincent github io 2016 09 25 kafka spark pipeline part 1    https   github com divolte divolte spark     jmxtrans other examples   https   github com jmxtrans jmxtrans wiki MoreExamples   FIX for  Spark Twitter Streaming exception    org apache spark Logging  classnotfound   https   stackoverflow com questions 38893655 spark twitter streaming exception org apache spark logging classnotfound docker swarm marvel   ansible playbooks  scripts  and docker compose files based on marvel comics locations  Use with digital ocean  Based on https   github com bossjones docker swarm vbox lab   pip install   ignore installed   pre  https   github com pradyunsg pip archive hotfix 9 0 2 zip egg pip           pip install   upgrade setuptools  36 0 1 wheel  0 29 0   Regarding firewalls  remember this            icmp no PORT DEFINED     outbound rules  protocol icmp address 0 0 0 0 0 address    0 protocol tcp ports all address 0 0 0 0 0 address    0 protocol udp ports all address 0 0 0 0 0 address    0      boss golang labs   Repo so I can learn some golang  Starting off with version 1 9 2   Tutorials   Sentdex    Go Language Programming Practical Basics boss grafana graphite   FOR LEARNING PURPOSES  Docker image with StatsD  Graphite  Grafana 2 and a Kamon Dashboard  Based strongly o    https   github com kamon io docker grafana graphite   http   mintbeans com jvm monitoring docker    StatsD   Graphite   Grafana 4   Kamon Dashboards   This image contains a sensible default configuration of StatsD  Graphite and Grafana  and comes bundled with a example dashboard that gives you the basic metrics currently collected by Kamon for both Actors and Traces  There are two ways for using this image    Using the Docker Index   This image is published under  Kamon s repository on the Docker Hub  and all you need as a prerequisite is having  docker    docker compose   and  make  installed on your machine  The container exposes the following ports      80   the Grafana web interface    81   the Graphite web port   2003   the Graphite data port   8125   the StatsD port    8126   the StatsD administrative port      To start a container with this image you just need to run the following command    bash   make up   To stop the container  bash   make down   To run container s shell  bash   make shell   To view the container log  bash   make tail   If you already have services running on your host that are using any of these ports  you may wish to map the container ports to whatever you want by changing left side number in the    publish  parameters  You can omit ports you do not plan to use  Find more details about mapping ports in the Docker documentation on  Binding container ports to the host  and  Legacy container links     Building the image yourself   The Dockerfile and supporting configuration files are available in our  Github repository   This comes specially handy if you want to change any of the StatsD  Graphite or Grafana settings  or simply if you want to know how the image was built    Using the Dashboards   Once your container is running all you need to do is      open your browser pointing to http   localhost 80  or another port if you changed it    Docker with VirtualBox on macOS  use  docker machine ip  instead of  localhost   login with the default username  admin  and password  admin    open existing dashboard  or create a new one  and select  Local Graphite  datasource   play with the dashboard at your wish        Persisted Data   When running  make up   directories are created on your host and mounted into the Docker container  allowing graphite and grafana to persist data and settings between runs of the container    Now go explore    We hope that you have a lot of fun with this image and that it serves it s purpose of making your life easier  This should give you an idea of how the dashboard looks like when receiving data from one of our toy applications      go chatbot lab   Development only  I m trying to learnsome golang  First project  building a chatbot    Getting started   This project requires Go to be installed  On OS X with Homebrew you can just run  brew install go     Running it then should be as simple as    console   make     bin go chatbot lab   Testing   make test   Thanks   Cookiecutter    cookiecutter golang   Example Projects Using Same Cookiecutter   iothub   Problems     Editor not looking at vendor folder it is ln  s from other path VSCode     Basic Golang gotyas   You go code needs to exist in   GOPATH src   Eg           2 2 3     dev7 behance 1484 in   dev go workspace src github com bossjones     code go chatbot lab        mockgen           bin bash  e   source  https   github com rafrombrc gomock blob master update mocks sh   mockgen github com rafrombrc gomock gomock Matcher       gomock mock matcher mock matcher go mockgen github com rafrombrc gomock sample Index Embed Embedded   sample mock user mock user go gofmt  w gomock mock matcher mock matcher go sample mock user mock user go     echo   2  OK        A getting started guide for Go newcomers   https   github com alco gostart   Server code borrowed from coolspeed century project   It s just a simple chatbot i m building to help teach me how golang works  Actual server code borrowed from coolspeed century project  Will add more on top of that    Feature     High throughput   High concurrency    Automatic  High scalability  especially on many core computers   Think of 64 core computers  as much as 4 core ones       Detailed Information   You can find an even simpler chat server on    https   gist github com drewolson 3950226    In fact I started my scratch from that     Build and Run   1  First  you need to install  golang   of course    Download it from  https   golang org dl    or install go via your package management tool    For Ubuntu    sudo apt get install golang   For Mac OS X    brew install go   2  Now  just build    cd  into the repo directory    To build the server  execute    make build   3  Run   3 1 Run the chat server      bin go chatbot lab   3 2 Run the chat client    Client   You can use  telnet  as the client    telnet localhost 6666   type anything  vagrant cluster ubuntu1404   A ubuntu1404 cluster via Vagrant ansible role skeleton       We try to open source as much as possible of our infrastructure and services  This skeleton contains all basic information we use to develop and test  Ansible  roles  You can easily use it to start developing new roles or add testing to old ones    Usage     init a new role   init role sh  path to new role    update an existing role   update role sh  path to role    Testing   Make sure your user is in the  docker  group  To only test your current setup  do   molecule test    To test different versions of ansible  do   tox    If your role depends on other roles from  Ansible Galaxy   uncomment the dependency lines in  molecule yml  and add the dependencies in  tests requirements yml     License   GPL   Author Information   https   www systemli org linux dotfiles   linux dotfiles setup   currently tested with ubuntu  near future fedora  coreOS     Thanks     https   github com jessfraz dotfiles   https   github com jessfraz dockerfiles   https   github com blacktop dotfiles   https   dotfiles github io    https   github com bndabbs dotfiles   https   github com webpro dotfiles     LOOK AT THIS ONE     https   github com webpro dotfiles     Test in docker container       docker build   force rm   no cache  t dotfiles     docker run  it dotfiles latest bash       Install          1  cd    dotfiles  2  git pull 3  clear 4  git pull 5  clear 6  source    dotfiles install sh 7  source    nvm nvm sh 8  nvm ls 9  npm search bats 10  histor 11  history         wip    bashrc       source  malcolm  pyenv  pyenvrc source  malcolm  pyenv completions pyenv bash   tabtab source for serverless package   uninstall by removing these lines or running  tabtab uninstall serverless      f  Users malcolm  nvm versions node v10 3 0 lib node modules serverless node modules tabtab  completions serverless bash         Users malcolm  nvm versions node v10 3 0 lib node modules serverless node modules tabtab  completions serverless bash   tabtab source for sls package   uninstall by removing these lines or running  tabtab uninstall sls      f  Users malcolm  nvm versions node v10 3 0 lib node modules serverless node modules tabtab  completions sls bash         Users malcolm  nvm versions node v10 3 0 lib node modules serverless node modules tabtab  completions sls bash        profile         export PATH   HOME  cargo bin  PATH    export PATH   usr local bin  PATH  export PATH   usr local sbin  PATH    export PYENV ROOT   HOME  pyenv  export PATH   PYENV ROOT bin  PATH  export WORKON HOME    virtualenvs export PROJECT HOME   dev eval    pyenv init     pyenv virtualenvwrapper lazy     System wide  profile for sh 1      if    x  usr libexec path helper    then   eval   usr libexec path helper  s   fi     if      BASH no       no     then      r  etc bashrc         etc bashrc   fi         default    bashrc            f    fzf bash      source    fzf bash       added by travis gem      f  Users malcolm  travis travis sh      source  Users malcolm  travis travis sh   System wide  bashrc file for interactive bash 1  shells    if    z   PS1     then    return fi   PS1   h  W  u       Make bash check its window size after a process completes   shopt  s checkwinsize      r   etc bashrc  TERM PROGRAM           etc bashrc  TERM PROGRAM    export PATH   usr local bin  PATH  export PATH   usr local sbin  PATH    export PYENV ROOT   HOME  pyenv  export PATH   PYENV ROOT bin  PATH  export WORKON HOME    pyenv versions export PROJECT HOME   dev eval    pyenv init     pyenv virtualenvwrapper lazy         Environment Variables     SKIP DOTFILES PROVISION 1   SKIP DOTFILES PREREQ 1   skip prereq linux sh   SKIP DOTFILES FORCE INSTALL HOMEBREW 1   skip prereq osx sh   Role Name   A brief description of the role goes here    Requirements   Any pre requisites that may not be covered by Ansible itself or the role should be mentioned here  For instance  if the role uses the EC2 module  it may be a good idea to mention in this section that the boto package is required    Role Variables   A description of the settable variables for this role should go here  including any variables that are in defaults main yml  vars main yml  and any variables that can should be set via parameters to the role  Any variables that are read from other roles and or the global scope  ie  hostvars  group vars  etc   should be mentioned here as well    Dependencies   A list of other roles hosted on Galaxy should go here  plus any details in regards to parameters that may need to be set for other roles  or variables that are used from other roles    Example Playbook   Including an example of how to use your role  for instance  with variables passed in as parameters  is always nice for users too      hosts  servers   roles           role  bossjones ansible  x  42      License   Apache   Author Information   An optional section for the role authors to include contact information  or a website  HTML is not allowed   boss emulate networking vagrant   Boss emulate networking vagrant  Based on guide  http   www brianlinkletter com how to use virtualbox to emulate a network  Role Name   A brief description of the role goes here    Requirements   Any pre requisites that may not be covered by Ansible itself or the role should be mentioned here  For instance  if the role uses the EC2 module  it may be a good idea to mention in this section that the boto package is required    Role Variables   A description of the settable variables for this role should go here  including any variables that are in defaults main yml  vars main yml  and any variables that can should be set via parameters to the role  Any variables that are read from other roles and or the global scope  ie  hostvars  group vars  etc   should be mentioned here as well    Dependencies   A list of other roles hosted on Galaxy should go here  plus any details in regards to parameters that may need to be set for other roles  or variables that are used from other roles    Example Playbook   Including an example of how to use your role  for instance  with variables passed in as parameters  is always nice for users too      hosts  servers   roles           role  bossjones common  x  42      License   Apache   Author Information   An optional section for the role authors to include contact information  or a website  HTML is not allowed   Role Name   A brief description of the role goes here    Requirements   Any pre requisites that may not be covered by Ansible itself or the role should be mentioned here  For instance  if the role uses the EC2 module  it may be a good idea to mention in this section that the boto package is required    Role Variables   A description of the settable variables for this role should go here  including any variables that are in defaults main yml  vars main yml  and any variables that can should be set via parameters to the role  Any variables that are read from other roles and or the global scope  ie  hostvars  group vars  etc   should be mentioned here as well    Dependencies   A list of other roles hosted on Galaxy should go here  plus any details in regards to parameters that may need to be set for other roles  or variables that are used from other roles    Example Playbook   Including an example of how to use your role  for instance  with variables passed in as parameters  is always nice for users too      hosts  servers   roles           role  bossjones java  x  42      License   Apache   Author Information   An optional section for the role authors to include contact information  or a website  HTML is not allowed   Role Name   A brief description of the role goes here    Requirements   Any pre requisites that may not be covered by Ansible itself or the role should be mentioned here  For instance  if the role uses the EC2 module  it may be a good idea to mention in this section that the boto package is required    Role Variables   A description of the settable variables for this role should go here  including any variables that are in defaults main yml  vars main yml  and any variables that can should be set via parameters to the role  Any variables that are read from other roles and or the global scope  ie  hostvars  group vars  etc   should be mentioned here as well    Dependencies   A list of other roles hosted on Galaxy should go here  plus any details in regards to parameters that may need to be set for other roles  or variables that are used from other roles    Example Playbook   Including an example of how to use your role  for instance  with variables passed in as parameters  is always nice for users too      hosts  servers   roles           role  bossjones maven  x  42      License   Apache   Author Information   An optional section for the role authors to include contact information  or a website  HTML is not allowed   Role Name   A brief description of the role goes here    Requirements   Any pre requisites that may not be covered by Ansible itself or the role should be mentioned here  For instance  if the role uses the EC2 module  it may be a good idea to mention in this section that the boto package is required    Role Variables   A description of the settable variables for this role should go here  including any variables that are in defaults main yml  vars main yml  and any variables that can should be set via parameters to the role  Any variables that are read from other roles and or the global scope  ie  hostvars  group vars  etc   should be mentioned here as well    Dependencies   A list of other roles hosted on Galaxy should go here  plus any details in regards to parameters that may need to be set for other roles  or variables that are used from other roles    Example Playbook   Including an example of how to use your role  for instance  with variables passed in as parameters  is always nice for users too      hosts  servers   roles           role  bossjones swapfile  x  42      License   Apache   Author Information   An optional section for the role authors to include contact information  or a website  HTML is not allowed   Role Name   A brief description of the role goes here    Requirements   Any pre requisites that may not be covered by Ansible itself or the role should be mentioned here  For instance  if the role uses the EC2 module  it may be a good idea to mention in this section that the boto package is required    Role Variables   A description of the settable variables for this role should go here  including any variables that are in defaults main yml  vars main yml  and any variables that can should be set via parameters to the role  Any variables that are read from other roles and or the global scope  ie  hostvars  group vars  etc   should be mentioned here as well    Dependencies   A list of other roles hosted on Galaxy should go here  plus any details in regards to parameters that may need to be set for other roles  or variables that are used from other roles    Example Playbook   Including an example of how to use your role  for instance  with variables passed in as parameters  is always nice for users too      hosts  servers   roles           role  bossjones pysyslog  x  42      License   Apache   Author Information   An optional section for the role authors to include contact information  or a website  HTML is not allowed   Role Name   A brief description of the role goes here    Requirements   Any pre requisites that may not be covered by Ansible itself or the role should be mentioned here  For instance  if the role uses the EC2 module  it may be a good idea to mention in this section that the boto package is required    Role Variables   A description of the settable variables for this role should go here  including any variables that are in defaults main yml  vars main yml  and any variables that can should be set via parameters to the role  Any variables that are read from other roles and or the global scope  ie  hostvars  group vars  etc   should be mentioned here as well    Dependencies   A list of other roles hosted on Galaxy should go here  plus any details in regards to parameters that may need to be set for other roles  or variables that are used from other roles    Example Playbook   Including an example of how to use your role  for instance  with variables passed in as parameters  is always nice for users too      hosts  servers   roles           role  bossjones tuning  x  42      License   Apache   Author Information   An optional section for the role authors to include contact information  or a website  HTML is not allowed   boss ansible role update hosts   Inspired entirely by https   github com bertvv ansible role hosts   An Ansible role for managing the hosts file    etc hosts    Specifically  the responsibilities of this role are to      Add the default localhost entry    Add an entry for the host name bound to the host s default external IPv4 address  optional     Add entries for basic IPv6 addresses  e g  ip6 localnet  optional     Add entries for Ansible managed hosts  optional     Add entries specified in Yaml  optional  see below     Add entries specified in text files  optional       Requirements   No specific requirements   Role Variables   None of the variables below are required  When not set  the default setting is applied      Variable                                   Default                                Comments                                                                                                                                                                                                                                                                                                                       hosts add default ipv4                    true                                   If true  an entry for the host name is added  bound to the host s default IPv4 address                                 hosts add basic ipv6                      false                                  If true  basic IPv6 entries are added  e g  localhost6  ip6 localnet  etc                                              hosts add ansible managed hosts           false                                  If true  an entry for hosts managed by Ansible is added                                                                hosts add ansible managed hosts groups      all                                  Control which host entries are created when using  hosts add ansible managed hosts       hosts entries                                                                    A list of dicts with custom entries to be added to the hosts file  See below for an example                            hosts file snippets                                                              A list of files containing host file snippets to be added to the hosts file verbatim                                   hosts ip protocol                          ipv4                                  When adding Ansible managed hosts  this specifies the IP protocol   ipv4  or  ipv6                                     hosts network interface                      ansible default ipv4 interface      When adding Ansible managed hosts  this specifies the network interface for which the IP address should be added       hosts file backup                         no                                     If yes  backup of host file is created with timestamp                                                                                                                                                                                                                                                                             When setting  hosts add ansible managed hosts   an entry for the current host will also be added  Consequently   hosts add default ipv4  doesn t need to be set    Individual hosts file entries can be added with  hosts entries   a list of dicts with keys  name    ip  and  optional   aliases   Example    Yaml hosts entries      name  slashdot     ip  216 34 181 45     name  gns1     ip  8 8 8 8     aliases          googledns1         googlens1     name  gns2     ip  8 8 4 4     aliases          googledns2         googlens2   Dependencies   No dependencies    Example Playbook   See the  test playbook   Tests for this role are provided in the form of a Vagrant environment that is kept in a separate branch   tests   I use  git worktree 1   to include the test code into the working directory  Instructions for running the tests      Fetch the tests branch   git fetch origin tests   Create a Git worktree for the test code   git worktree add tests tests   remark  this requires at least Git v2 5 0   This will create a directory  tests      cd tests    vagrant up  will then create a VM and apply a test playbook   test yml        You may want to change the base box into one that you like  The current one   bertvv centos72  was generated using a Packer template from the  Boxcutter project  with a few modifications    Contributing   Issues  feature requests  ideas are appreciated and can be posted in the Issues section  Pull requests are also very welcome  Preferably  create a topic branch and when submitting  squash your commits into one  with a descriptive message     Example  etc hosts before refactor       root rsyslogd master 01    cat  etc hosts 127 0 0 1       localhost   The following lines are desirable for IPv6 capable hosts     1     ip6 localhost   ip6 loopback fe00  0 ip6 localnet ff00  0 ip6 mcastprefix ff02  1 ip6 allnodes ff02  2 ip6 allrouters ff02  3 ip6 allhosts 127 0 1 1       rsyslogd master 01   192 168 50 108 rsyslogd master 01 rsyslogd master 01 192 168 50 109 rsyslogd worker 01 rsyslogd worker 01 root rsyslogd master 01          ipv6 notes   SOURCE  https   unix stackexchange com questions 234412 whats the use for the special ipv6 addresses in etc hosts     1  This is the loopback address  whose IPv4 equivalent is 127 0 0 1  fe00  0  Can be compared to the Class E address space in IPv4  therefore it s in the reserved scope  reserved for future use  ff02  1  The group of all IPv6 nodes  including the routers  in the Link local scope  similar to a broadcast address of the subnet in IPv4  192 168 x 255   ff02  2  The group of all IPv6 routers in the Link local scope  also similar to a broadcast in IPv4  but only refering the routers   ff02  3  This exists no longer an is unassigned at the moment  Earlier it stood for the group of all hosts  excluding the routers  in the Link local scope  Role Name   A brief description of the role goes here    Requirements   Any pre requisites that may not be covered by Ansible itself or the role should be mentioned here  For instance  if the role uses the EC2 module  it may be a good idea to mention in this section that the boto package is required    Role Variables   A description of the settable variables for this role should go here  including any variables that are in defaults main yml  vars main yml  and any variables that can should be set via parameters to the role  Any variables that are read from other roles and or the global scope  ie  hostvars  group vars  etc   should be mentioned here as well    Dependencies   A list of other roles hosted on Galaxy should go here  plus any details in regards to parameters that may need to be set for other roles  or variables that are used from other roles    Example Playbook   Including an example of how to use your role  for instance  with variables passed in as parameters  is always nice for users too      hosts  servers   roles           role  boss ansible role bootstrap  x  42      License   Apache   Author Information   An optional section for the role authors to include contact information  or a website  HTML is not allowed     Influence   Strong strong influence on this repo from the debops bootstrap ansible role    THIS REPO IS ONLY FOR LEARNING PURPOSES      https   github com debops ansible bootstrap  Role Name   Dump all variables used by Ansible during playbook run to a file for inspection  This role is not active during normal playbook operation and should be used for development only    Based on DebOps role          Requirements   Any pre requisites that may not be covered by Ansible itself or the role should be mentioned here  For instance  if the role uses the EC2 module  it may be a good idea to mention in this section that the boto package is required    Role Variables   A description of the settable variables for this role should go here  including any variables that are in defaults main yml  vars main yml  and any variables that can should be set via parameters to the role  Any variables that are read from other roles and or the global scope  ie  hostvars  group vars  etc   should be mentioned here as well    Dependencies   A list of other roles hosted on Galaxy should go here  plus any details in regards to parameters that may need to be set for other roles  or variables that are used from other roles    Example Playbook   Including an example of how to use your role  for instance  with variables passed in as parameters  is always nice for users too      hosts  servers   roles           role  boss ansible role debug  x  42      License   Apache   Author Information   An optional section for the role authors to include contact information  or a website  HTML is not allowed   Role Name   A brief description of the role goes here    Requirements   Any pre requisites that may not be covered by Ansible itself or the role should be mentioned here  For instance  if the role uses the EC2 module  it may be a good idea to mention in this section that the boto package is required    Role Variables   A description of the settable variables for this role should go here  including any variables that are in defaults main yml  vars main yml  and any variables that can should be set via parameters to the role  Any variables that are read from other roles and or the global scope  ie  hostvars  group vars  etc   should be mentioned here as well    Dependencies   A list of other roles hosted on Galaxy should go here  plus any details in regards to parameters that may need to be set for other roles  or variables that are used from other roles    Example Playbook   Including an example of how to use your role  for instance  with variables passed in as parameters  is always nice for users too      hosts  servers   roles           role  boss ansible role kernel tuning  x  42      License   Apache   Author Information   An optional section for the role authors to include contact information  or a website  HTML is not allowed   Role Name   A brief description of the role goes here    Requirements   Any pre requisites that may not be covered by Ansible itself or the role should be mentioned here  For instance  if the role uses the EC2 module  it may be a good idea to mention in this section that the boto package is required    Role Variables   A description of the settable variables for this role should go here  including any variables that are in defaults main yml  vars main yml  and any variables that can should be set via parameters to the role  Any variables that are read from other roles and or the global scope  ie  hostvars  group vars  etc   should be mentioned here as well    Dependencies   A list of other roles hosted on Galaxy should go here  plus any details in regards to parameters that may need to be set for other roles  or variables that are used from other roles    Example Playbook   Including an example of how to use your role  for instance  with variables passed in as parameters  is always nice for users too      hosts  servers   roles           role  boss ansible role ulimit  x  42      License   Apache   Author Information   An optional section for the role authors to include contact information  or a website  HTML is not allowed     INSPIRATION   https   github com picotrading ansible ulimit   FOR LEARNING PURPOSES ONLY  Role Name   A brief description of the role goes here    Requirements   Any pre requisites that may not be covered by Ansible itself or the role should be mentioned here  For instance  if the role uses the EC2 module  it may be a good idea to mention in this section that the boto package is required    Role Variables   A description of the settable variables for this role should go here  including any variables that are in defaults main yml  vars main yml  and any variables that can should be set via parameters to the role  Any variables that are read from other roles and or the global scope  ie  hostvars  group vars  etc   should be mentioned here as well    Dependencies   A list of other roles hosted on Galaxy should go here  plus any details in regards to parameters that may need to be set for other roles  or variables that are used from other roles    Example Playbook   Including an example of how to use your role  for instance  with variables passed in as parameters  is always nice for users too      hosts  servers   roles           role  boss ansible role users  x  42      License   Apache   Author Information   An optional section for the role authors to include contact information  or a website  HTML is not allowed     Influence   https   github com singleplatform eng ansible users tree master tasks   THIS IS FOR LEARNING PURPOSES ONLY       Example        users      username  foo     name  Foo Barrington     groups    wheel   systemd journal       uid  1001     home   local home foo     profile          alias ll  ls  lah      ssh key           ssh rsa AAAAA     foo machine           ssh rsa AAAAB     foo2 machine  groups to create      name  developers     gid  10000 users deleted      username  bar     name  Bar User     uid  1002    Deleting users   The  users deleted  variable contains a list of users who should no longer be in the system  and these will be removed on the next ansible run  The format is the same as for users to add  but the only required field is  username   However  it is recommended that you also keep the  uid  field for reference so that numeric user ids are not accidentally reused    You can optionally choose to remove the user s home directory and mail spool with the  remove  parameter  and force removal of files with the  force  parameter    users deleted      username  bar     uid  1002     remove  yes     force  yes  Role Name   A brief description of the role goes here    Requirements   Any pre requisites that may not be covered by Ansible itself or the role should be mentioned here  For instance  if the role uses the EC2 module  it may be a good idea to mention in this section that the boto package is required    Role Variables   A description of the settable variables for this role should go here  including any variables that are in defaults main yml  vars main yml  and any variables that can should be set via parameters to the role  Any variables that are read from other roles and or the global scope  ie  hostvars  group vars  etc   should be mentioned here as well    Dependencies   A list of other roles hosted on Galaxy should go here  plus any details in regards to parameters that may need to be set for other roles  or variables that are used from other roles    Example Playbook   Including an example of how to use your role  for instance  with variables passed in as parameters  is always nice for users too      hosts  servers   roles           role  boss ansible core  x  42      License   Apache   Author Information   An optional section for the role authors to include contact information  or a website  HTML is not allowed     Influence   This is heavily influenced by the Debops project               Using this for learning purposes only             Role Name   A brief description of the role goes here    Requirements   Any pre requisites that may not be covered by Ansible itself or the role should be mentioned here  For instance  if the role uses the EC2 module  it may be a good idea to mention in this section that the boto package is required    Role Variables   A description of the settable variables for this role should go here  including any variables that are in defaults main yml  vars main yml  and any variables that can should be set via parameters to the role  Any variables that are read from other roles and or the global scope  ie  hostvars  group vars  etc   should be mentioned here as well    Dependencies   A list of other roles hosted on Galaxy should go here  plus any details in regards to parameters that may need to be set for other roles  or variables that are used from other roles    Example Playbook   Including an example of how to use your role  for instance  with variables passed in as parameters  is always nice for users too      hosts  servers   roles           role  boss ansible fact  x  42      License   Apache   Author Information   An optional section for the role authors to include contact information  or a website  HTML is not allowed   Role Name   A brief description of the role goes here    Requirements   Any pre requisites that may not be covered by Ansible itself or the role should be mentioned here  For instance  if the role uses the EC2 module  it may be a good idea to mention in this section that the boto package is required    Role Variables   A description of the settable variables for this role should go here  including any variables that are in defaults main yml  vars main yml  and any variables that can should be set via parameters to the role  Any variables that are read from other roles and or the global scope  ie  hostvars  group vars  etc   should be mentioned here as well    Dependencies   A list of other roles hosted on Galaxy should go here  plus any details in regards to parameters that may need to be set for other roles  or variables that are used from other roles    Example Playbook   Including an example of how to use your role  for instance  with variables passed in as parameters  is always nice for users too      hosts  servers   roles           role  boss ansible role environment  x  42      License   Apache   Author Information   An optional section for the role authors to include contact information  or a website  HTML is not allowed   Role Name   A brief description of the role goes here    Requirements   Any pre requisites that may not be covered by Ansible itself or the role should be mentioned here  For instance  if the role uses the EC2 module  it may be a good idea to mention in this section that the boto package is required    Role Variables   A description of the settable variables for this role should go here  including any variables that are in defaults main yml  vars main yml  and any variables that can should be set via parameters to the role  Any variables that are read from other roles and or the global scope  ie  hostvars  group vars  etc   should be mentioned here as well    Dependencies   A list of other roles hosted on Galaxy should go here  plus any details in regards to parameters that may need to be set for other roles  or variables that are used from other roles    Example Playbook   Including an example of how to use your role  for instance  with variables passed in as parameters  is always nice for users too      hosts  servers   roles           role  boss ansible role ubuntu devtop  x  42      License   Apache   Author Information   An optional section for the role authors to include contact information  or a website  HTML is not allowed   Role Name   A brief description of the role goes here    Requirements   Any pre requisites that may not be covered by Ansible itself or the role should be mentioned here  For instance  if the role uses the EC2 module  it may be a good idea to mention in this section that the boto package is required    Role Variables   A description of the settable variables for this role should go here  including any variables that are in defaults main yml  vars main yml  and any variables that can should be set via parameters to the role  Any variables that are read from other roles and or the global scope  ie  hostvars  group vars  etc   should be mentioned here as well    Dependencies   A list of other roles hosted on Galaxy should go here  plus any details in regards to parameters that may need to be set for other roles  or variables that are used from other roles    Example Playbook   Including an example of how to use your role  for instance  with variables passed in as parameters  is always nice for users too      hosts  servers   roles           role  boss ansible role nvm  x  42      License   Apache   Author Information   An optional section for the role authors to include contact information  or a website  HTML is not allowed     INSPIRATION    Completely inspired by https   github com pablocrivella ansible role nvm     ubuntu devtop vm   Just a devtop vm for me to try random shit in Role Name   A brief description of the role goes here    Requirements   Any pre requisites that may not be covered by Ansible itself or the role should be mentioned here  For instance  if the role uses the EC2 module  it may be a good idea to mention in this section that the boto package is required    Role Variables   A description of the settable variables for this role should go here  including any variables that are in defaults main yml  vars main yml  and any variables that can should be set via parameters to the role  Any variables that are read from other roles and or the global scope  ie  hostvars  group vars  etc   should be mentioned here as well    Dependencies   A list of other roles hosted on Galaxy should go here  plus any details in regards to parameters that may need to be set for other roles  or variables that are used from other roles    Example Playbook   Including an example of how to use your role  for instance  with variables passed in as parameters  is always nice for users too      hosts  servers   roles           role  boss ansible role bash it  x  42      License   Apache   Author Information   An optional section for the role authors to include contact information  or a website  HTML is not allowed   Role Name   A brief description of the role goes here    Requirements   Any pre requisites that may not be covered by Ansible itself or the role should be mentioned here  For instance  if the role uses the EC2 module  it may be a good idea to mention in this section that the boto package is required    Role Variables   A description of the settable variables for this role should go here  including any variables that are in defaults main yml  vars main yml  and any variables that can should be set via parameters to the role  Any variables that are read from other roles and or the global scope  ie  hostvars  group vars  etc   should be mentioned here as well    Dependencies   A list of other roles hosted on Galaxy should go here  plus any details in regards to parameters that may need to be set for other roles  or variables that are used from other roles    Example Playbook   Including an example of how to use your role  for instance  with variables passed in as parameters  is always nice for users too      hosts  servers   roles           role  boss ansible role apt  x  42      License   Apache   Author Information   An optional section for the role authors to include contact information  or a website  HTML is not allowed     FOR LEARNING PURPOSES ONLY  THIS IS HEAVILY INFLUENCED BY THE DEBOPS PROJECT    Role Name   A brief description of the role goes here    Requirements   Any pre requisites that may not be covered by Ansible itself or the role should be mentioned here  For instance  if the role uses the EC2 module  it may be a good idea to mention in this section that the boto package is required    Role Variables   A description of the settable variables for this role should go here  including any variables that are in defaults main yml  vars main yml  and any variables that can should be set via parameters to the role  Any variables that are read from other roles and or the global scope  ie  hostvars  group vars  etc   should be mentioned here as well    Dependencies   A list of other roles hosted on Galaxy should go here  plus any details in regards to parameters that may need to be set for other roles  or variables that are used from other roles    Example Playbook   Including an example of how to use your role  for instance  with variables passed in as parameters  is always nice for users too      hosts  servers   roles           role  boss ansible role pki  x  42      License   Apache   Author Information   An optional section for the role authors to include contact information  or a website  HTML is not allowed   Role Name   A brief description of the role goes here    Requirements   Any pre requisites that may not be covered by Ansible itself or the role should be mentioned here  For instance  if the role uses the EC2 module  it may be a good idea to mention in this section that the boto package is required    Role Variables   A description of the settable variables for this role should go here  including any variables that are in defaults main yml  vars main yml  and any variables that can should be set via parameters to the role  Any variables that are read from other roles and or the global scope  ie  hostvars  group vars  etc   should be mentioned here as well    Dependencies   A list of other roles hosted on Galaxy should go here  plus any details in regards to parameters that may need to be set for other roles  or variables that are used from other roles    Example Playbook   Including an example of how to use your role  for instance  with variables passed in as parameters  is always nice for users too      hosts  servers   roles           role  boss ansible role secret  x  42      License   Apache   Author Information   An optional section for the role authors to include contact information  or a website  HTML is not allowed
13,nik0spapp,authid    The attached MATLAB code implements Moshe Koppel s et al  improved method introduced in the paper indicated below  The code was developed during my master studies in Information Management at the University of the Aegean under  the scope of a semester project for the Machine Learning and Knowledge Discovery course    Koppel  Moshe  Jonathan Schler  and Shlomo Argamon   Authorship attribution in the wild   Language Resources and Evaluation 45 1  2011   83 94   URL   http   link springer com article 10 1007 2Fs10579 009 9111 2 LI true   The detect author m is the main file needed for execution and the rest of the files are dependencies for the former    Input parameters      Variable Description Example   known text Array with text vectors  t  1   t  2        t  n     snippets Array with unknown text vectors  s  1   s  2        s  n     authors Array with author names   Author 1    Author 2         Author n     s authors Array with author snippets  s  1   s  2        s  n     k1 Number of iterations 10   sigma Final decision threshold  0 100  80   distance function The desired distance function cosine     where t  i  and s  i  are n dimensional float vectors  e g   f  1   f  2        f  n       Output      Variable Description   snippet results Array with author per snippet   precision Precision scores   recall Recall scores   confusion matrix Table with confusion matrix   icrawler    In this repository you can find material related to the opinion mining and retrieval system  which was described in the following paper      incollection pappas13c   location    Samos  Greece    year    2013    isbn    978 3 642 37255 1    booktitle    Computational Linguistics and Intelligent Text Processing    volume    7817    title    Distinguishing the Popularity between Topics  A System for Up to Date Opinion Retrieval and Mining in the Web    author    Pappas  Nikolaos and Katsimpras  Georgios and Stamatatos  Efstathios    pages    197 209      Please check the available code of the individual components that are available     SD algorithm  web page segmentation and noise removal     https   github com nik0spapp webpage segmentation     Unsupervised sentiment classification using bootstrapping procedures for subjectivity and polarity classification     https   github com nik0spapp unsupervised sentiment     Contact    nik0spapp gmail com usent    The attached code is a Python implementation of a dictionary based sentiment classification procedure which combines two different bootstrapping procedures  namely for subjectivity and polarity detection  as in   3    4   respectively   The rule based polarity classifier is an extension of the one that was presented in   5    Moreover    TED comment annotations folder contains the files of the human study we conducted on TED comment sentiment classification  with 6 human annotators         E  Riloff and J  Wiebe  Learning extraction patterns for subjective expressions  In Proceedings of the 2003 conference on Empirical methods in natural language processing  2003      D  K  M Wiegand  Bootstrapping supervised machine learning polarity classifiers with rule based classification   In Proceedings of the ECAI Workshop on Computational Approaches to Subjectivity and Sentiment Analysis  2009      T  Wilson  J  Wiebe  and P  Hoffmann  Recognizing contextual polarity in phrase level sentiment  analysis  In Proceedings of the conference on Human Language Technology and Empirical Methods in  Natural Language Processing  2005       The code was used for an opinion mining and retrieval system presented at CICLing 2013   1     and for improving one class collaborative filtering   2            incollection pappas13c   location    Samos  Greece    year    2013    booktitle    Computational Linguistics and Intelligent Text Processing    volume    7817    doi    10 1007 978 3 642 37256 8 17    title    Distinguishing the Popularity between Topics  A System for Up to Date Opinion Retrieval and Mining in the Web    author    Pappas  Nikolaos and Katsimpras  Georgios and Stamatatos  Efstathios    pages    197 209       inproceedings pappas13a   author    Pappas  Nikolaos and Popescu Belis  Andrei    title    Sentiment Analysis of User Comments for One Class Collaborative Filtering Over  TED  Talks    booktitle    Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval    series    SIGIR  13    year    2013    isbn    978 1 4503 2034 4    location    Dublin  Ireland    pages    773  776    numpages    4            Installing dependencies   The available code for unsupervised sentiment classification requires Python programming  language and pip package manager to run  For detailed installing instructions please refer to  the following links    http   www python org getit    http   www pip installer org en latest    After installing them  you should be able to install the following packages     bash   pip install nltk     pip install stemmer    pip install numpy   pip install pickle   After you install nltk you will need some corpora to train the sequential POS tagger  pos py  and the nltk tokenizer   bash   python   python import nltk  nltk download    The issue of the above command will load a graphical interface that lets you manage several corpora related to nltk library  From the list select and download the following corpora    tokenizers punkt english    wordnet    brown    conll2000  and  treebank      Lastly  pyml library is needed for the SVM classifier that is used currently in our code    Download http   pyml sourceforge net  and then issue     bash     tar zxvf PyML 0 7 11 tar gz    cd PyML 0 7 11    python setup py build    python setup py install   Processing pipeline   The current pipline that is implemented in sentiment py is depicted in the following diagram  Initially  the input text is split into sentences and each sentence is fed to a high precision subjectivity classifier  If the sentence is classified as subjective then syntactic patterns are learned from this instance  In case  that the sentence is not detected as such then it is fed to the pattern based classifier  The pattern based classifier outputs the class of the sentence based on the learned patterns so far  If the instance is subjective then again more patterns are learned from it  otherwise it is fed to a high precision objectivity classifier  If the sentence is classified as objective  then it is ignored  otherwise it is fed to the polarity classifier  Finally  the polarity classifier estimates the numerical sentiment and normalized sentiment values and outputs the result  The instances with high confidence from the polarity classifier can be further used to train an SVM  classifier to improve further the classification performance  see paper for further details   At the current version this option is disabled  but you can easily enable it  Similarly  you can remove some of the components from the  pipeline according to your needs  e g  skip subjectivity classification           Examples   To estimate the total sentiment and total normalized sentiment  as described in the papers    you can simply execute the sentiment py file and give the desired block of text as an argument  Make sure that you escape symbols such as     and      Apart from the command line execution you  can integrate the library to your code and use directly the returned results  Below you can  find two simple examples for demonstrating purposes    bash   python sentiment py  I have to give much love and respect to Rony  Your work is Amazing              Loaded existing UBT tagger      Loaded existing pattern knowledge        Checking block of text   1  I have to give much love and respect to Rony   2  Your work is Amazing        Overall sentiment analysis    Parts   2  Sentiments     positive    positive    Scores    4  6 0   Results     positive     count   2   score   10 0   nscore   1 9            neutral     count   0   score   0   nscore   0            negative     count   0   score   0   nscore   0     subjective       100 00   objective        0 00    positive         100 00   neutral          0 00   negative         0 00     x  positive  10 00  1 90        bash   python sentiment py  I was blown away by some of the comments here posted by people who is either  uneducated  ignorant  self righteous or al of the above  I m irritated and saddened as I read these  finger pointing   i m right and you re wrong   type of posts             Loaded existing UBT tagger      Loaded existing pattern knowledge        Checking block of text   1  I was blown away by some of the comments here posted by people who is either uneducated  ignorant  self righteous or al of the above   2  I m irritated and saddened as I read these finger pointing  i m right and you re wrong  type of posts        Overall sentiment analysis    Parts   2  Sentiments     negative    negative    Scores     4   4 0   Results     positive     count   0   score   0   nscore   0            neutral     count   0   score   0   nscore   0            negative     count   2   score    8 0   nscore    0 3722943722943723     subjective       100 00   objective        0 00    positive         0 00   neutral          0 00   negative         100 00     x  negative   8 00   0 37      sdalg    The attached code is a Python version of the style density tree based algorithm  which was described in the following paper  The SD algorithm performs web page segmentation and noise removal and then returns the identified web page type  Article  Article with Comments and Multiple areas  along with the region annotations per type     inproceedings pappas12   author    Pappas  Nikolaos and Katsimpras  Georgios and Stamatatos  Efstathios    title    Extracting Informative Textual Parts from Web Pages Containing User generated Content    booktitle    Proceedings of the 12th International Conference on Knowledge Management and Knowledge Technologies    series    i KNOW  12    year    2012    isbn    978 1 4503 1242 4    location    Graz  Austria    pages    4 1  4 8    articleno    4    numpages    8    doi    10 1145 2362456 2362462          Installing dependencies   The available code for webpage segmentation requires Python programming  language and pip package manager to run  For detailed installing instructions please refer to  the following links    http   www python org getit    http   www pip installer org en latest    After installing them  you should be able to install the following packages     bash   pip install nltk     pip install urllib    pip install lxml   Examples   To run the SD algorithm simply execute the sd algorithm py file and give as parameter  the URL of your preference  Make sure that you use double quotes in case of weird parameters on the URL  check examples below  Lastly  the algorithm relies on two thresholds that have to be tuned on a subset of your target documents  see the related paper   otherwise the  segmentation may not be as expected           python sd algorithm py http   www bbc co uk news world africa 12328506     Create DOM tree        Calculating initial groups        Merging groups        Creating regions        Calculating distances from max region        Printing regions          Validating candidate comment group based on its content       INFO   Article detected  Article class    x   story body  Article title   Egypt protest   Carnival atmosphere  among demonstrators Article text   For many in Tahrir Square in central Cairo  the days are starting to take on a familiar pattern  After  nearly a week of demonstrations  many people now sleep here  There are a few tents and pieces of cardboard  that serve as beds on a small patch of grass in front of a government building  the Mugamma   We get just  four hours sleep or so and then we wake up to start the protest again   said Samah al Dweik  who has not been  to her home in Maadi  just outside the city  since Friday    We do not know how long we will have to continue   Only if Mubarak goes  will we go home         They declare   I m free  and  Game over  but also demand policy changes from Western countries that have  supported the Mubarak government   US  we hate your hypocrisy  read one banner  referring to the disparity  between American calls for human rights and democracy and its support of their president   Listen to the Egyptian  people   another demanded  Despite an official curfew  the numbers in the square swell in early evening and the  chants increase in volume  Protesters are only too aware of the government s hope that by delaying its response  to their demands it will drain their energy   But they say they are determined to prove otherwise      INFO   No comments found              python sd algorithm py http   www care2 com greenliving chocolate may reduce risk of heart failure html     Create DOM tree        Calculating initial groups        Merging groups        Creating regions        Calculating distances from max region        Printing regions          Validating candidate comment group based on its content       INFO   Article with comments detected   INFO   Article detected  Article class    x   article content  Article title   Chocolate May Reduce Risk of Heart Failure Article text   Forget what you ve heard about death by chocolate   A new Harvard study shows that chocolate may be good  for your heart  It s a great day for chocolate lovers everywhere  Murray Mittleman and his colleagues  at Harvard Medical School studied data on 31 823 middle aged and elderly Swedish women to assess the  relationship between chocolate and heart failure   The women who consumed an average of one to two servings   that s a fairly small amount  of high quality  cocoa rich chocolate per week had a 32 percent lower risk  of experiencing heart failure  Those women who ate 1 to 3 servings a month had a 26 percent lower risk of  heart failure  The scientists noted that the high concentration of phytonutrients called flavonoids in  dark chocolate are potent antioxidants that are likely responsible for the results   The flavonoids are  believed to lower blood pressure and reducing inflammation linked with heart failure  Keep in mind that  not just any chocolate will do   Forget the vast majority of candy bars on the market The study results  were achieved with high quality  cocoa rich chocolate   Read DARK chocolate   The darker the better    And  be sure the one you choose is low in sugar  has no trans or hydrogenated fats  and no artificial  colors  flavors  or other synthetic ingredients  Related Easy Greening  Chocolate 101Chocolate  Fact vs   FictionDark Chocolate Definitely Eases Emotional StressChocolate Tantric Pie Subscribe to my free  e newsletter World s Healthiest News for more cutting edge health news  tips  recipes  and more     Comment class   x  comment text contain floats Comments    I love chocolates and I m very pleased on how nutritious it is  I gained a lot of information about  chocolates on chocolarious com and now I m a certified chocoholic  lol Also  try checking out  milkdelight com  coffeefashion com  everything cake com  and zcocktails com       The darker the better   so says the author  However  keep in mind that something is better than nothing    like the bournvita  cocoa and other drinks  BTW  the article is a reminder to me to have today s  quota of my chocolate    I love chocolate  I can go to bed with chocolate in my mouth  Not good for my teeth or weight  but I  dont have a teeth or weight problem  I guess I m just lucky  I would have to eat at least 4oz s of  chocolate a day  Addicted to the great stuff  Thanks for the assurance I m doing the right thing by  eating so much of it       Thank you for sharing              python sd algorithm py  http   www lonelyplanet com thorntree forum jspa jsessionid 57DA8CB66960A9D820CAB16BB221094D app01 forumID 34 errorMsg The 20thread 20requested 20is 20not 20currently 20available      Create DOM tree        Calculating initial groups        Merging groups        Creating regions        Calculating distances from max region        Printing regions       INFO   Multiple similar regions detected   x  Texts     Hi All Am going for a six month trip to Central Asia  Nepal and China I need to sort our my  connectivity needs   I will have a mac and a smartphone with me  samsung Note 2  unlocked   I would  like to find the most convenient way to get on the internet and use the phone locally  no real need  for voice long distance  and i can always use skype for that  What options do i have considering  that    barring nepal   i will not be staying long in any country  2 3 weeks max  The main priority  is data really    I guess that when i have the ability to downoad data i can always work out my local  calls through skype   Is it better to buy a local sim card with data voice capability in every country   ie uzbekistan  tajikistan  kyrgyzstan  or should i go for an international sim card  which one    Or  maybe a combination of the two Ideas gratefully received   With thanks Str   more     Hello everybodyMy boyfriend  who is a carpenter and cabinet maker  is going to work for a few months  to help fix up an old 17th century farmhouse in a rural area of the H rault  France  There is no  internet connection for miles  so in order to communicate more effectively we were thinking of buying  a tablet that holds a SIM card to Skype or internet  When I was there I noticed Bouygues Telecom was  the main carrier and they have a good deal for a prepaid SIM card Formule 24 24  la recharge 20  est  valable 1 moisPour 20  recharg s  vous b n ficiez pendant 1 mois    d appels et de SMS illimit s 24 24    de 250 Mo d Internet 3G    de 4  de cr dit de r serve offertThat sorted  we would love some advice on  the tablet  Could you please recommend any that  has a Windows operating system  cause that s wha      Not quite sure this thread belong here  but oh well Originally from Australia and I am beginning to  travel the world by van from the 28th march onwards  starting in NZ and AUS Now I know whats required  for car registration in Australia  but I have absolutely no idea what I do with registration whilst  overseas planning to be overseas for 12  years  Do you just register in the country you are a citizen   Is there a world registration group Just what do I have to do to keep on the right side of the laws  around the globe Thanks for your timeand happy travels    Does anyone know how easy it is to draw a route on Google Earth and then export it into a SatNav app  on a mobile phone  to use off line I ve moved travelled routes from my Garmin GPS device into Google  Earth before  could probably do it the other way if necessary  I guess  but probably not enough storage  space for what I m thinking of   Thinking of getting a phone that has GPS in it  moving into the 21st  century   haven t actually ever owned a mobile phone yet      Flikr is not the only place to display photos nowadaze    Tumblr and Twitter will accept photos  uploaded directly from your Flikr account   with just one click of the mouse    Although both Tumblr  and Twitter have photo upload capabilities   Tumblr allows many more ways to display your photos on  the Net   And Twitter has a media sidebar feature for photos   which can appear as a slide show    right next to your Tweets   You don t have to use their twitpic feature if you have a Flikr account    http   vasenka tumblr com    I am looking for a good MS Windows based program that can keep track of places I have visited  I  would like to be able to view it hopefully by continent  country  states provinces regions etc  cities  towns etc  I currently do this on a MS Excel spreadsheet  but would like to be able to use it to view  maps indicating somehow the locations I have visited  as well as lists I would also like to add details  of my trips to each location  including possibly photos I live in the U S A   and am currently working  on visiting all 50 states  40 of them done  with 2 more to be added this year  That said  I have also  travelled internationally  To date I have visited 31 countires across 4 continents  and hope to do more  in the future Any thoughts  Anything out there that can do something like this    Hey everyone i recently took up photography as a hobby and a really enjoying it  i ll be leaving on my  RTW trip in about a month and will be going through parts of europe first then the middle east  asia  and south and then north america  yet i want to only carry essential gear for my photography yet still  want to take great shots  I m using a Sony A57 with a 18 35mm lens  still in the market for a larger  and better lens yet i want to ask those here who have done travel photography what essential gear should  i bring and what can i do without i ll certainly be taking as compact a tripod as i can find yet i ll  also be taking filters and lens hoods  this gear is already starting to feel bulky in my mind but can  i afford to do without these items in the least and is there any other ones i should consider  looking  forward to all replies    Sorry  not sure which branch to put this under I wanted to buy something online in a foreign currency   I have done this many times whilst abroad but just wanted to check with the company if it was okay  The  company told me that they only accept   UK pounds and euros  I wanted to pay in US   The question is   If I pay in US  for a euro transaction  does my bank send dollars for the equivalent amount and the  receiving bank does the exchange  Or  does my bank send euros  I am paying the exchange rate charges so  I assume my bank sends euros  If that is the case then the receiving company gets euros  so I am a bit  confused why they would say that  It is a European company  the bank is in Europe  I am European  the  delivery and invoice address is European  so there are no issues with customs tax Thanks    Greetings  travelers Heading to South Luangwa National Park in Zambia and then making way by bus to  Kruger in South Africa over a period of weeks Wondering if anyone reading this has had experience with putting a sim in a Boostmobile or Sprint HTC Design Evo 4g  Were you able to get it working on data    and if so what speeds  Did the wi fi hotspot function work  If not HTC  did anyone use a usb modem mi  fi wi fihotspot in South Africa  Botswana  or Zambia Here s and example of the device I am talking  about  Huwaei BROADBAND MIFI WIFI RouterOr did you purchase a local data modem stick and which company  did you use Thanks   My last few travel cameras have been from the Panasonic TZ series but the last 2 cameras that I ve  had have readily gotten what looks like dirt in the lens  Has anyone else experienced this problem  I  want to get a new camera to replace the one with dust in the lens  it is now no longer under warrenty  so can t get it fixed cheaply  and am wondering whether to go for something else this time and what   I like the compact high zoom format of the Panasonic TZ series but maybe this is also the reason for  the problems  Any recommendations   Thanks       wmil    The attached code is a Python implementation of the multiple instance learning algorithm for aspect based  sentiment analysis which was proposed in the paper listed below  Moreoever  the features  extracted from seven datasets are provided for research purposes      InProceedings pappas14    author       Pappas  Nikolaos  and  Popescu Belis  Andrei     title        Explaining the Stars  Weighted Multiple Instance Learning for Aspect Based Sentiment Analysis     booktitle    Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing  EMNLP      month        October     year         2014     address      Doha  Qatar     publisher    Association for Computational Linguistics     pages        455  466     url          http   www aclweb org anthology D14 1052      A more scalable version of the above algorithm based on stochastic gradient descent can be found here    wmil sgd      Installing dependencies   The available code requires Python programming language and pip package manager to run   For detailed instructions on how to install it along with a package manager please refer  to the following links  http   www python org getit  and http   www pip installer org en latest     Next  you should be able to install the following packages     bash   pip install numpy    pip install scikit learn   pip install scipy   Training and testing the model   The code extends BaseEstimator class from scikit learn package  so you should be able to use it as a common sklearn estimator  check more details on http   scikit learn org stable    For example     bash   python         import pickle from wmil import APWeights from sklearn metrics import mean absolute error data   pickle load open  features ted comments p    size   len data  X    k   int size 0 5  x train   data  X    k  y train   data  Y    k  x test   data  X   k   y test   data  Y   k   model   APWeights 20  e1 1 0  e2 1 0  e3 1 0  model fit x train  y train      Training       start iteration 0     MAE  0 103437  iteration 1     MAE  0 089629  iteration 2     MAE  0 087793  iteration 3     MAE  0 087565  iteration 4     MAE  0 087523  iteration 5     MAE  0 087515  iteration 6     MAE  0 087510  iteration 7     MAE  0 087511     end mean absolute error model predict x train  y train    training error 0 096217463769192518 mean absolute error model predict x test   y test    testing error 0 16325402985689552             Contact    npappas idiap ch
14,ematvey,NanoEngineer 1   CAD for design of molecular machines    What was done     Applied a few patches for OS X compatibility      Contacts   Original project   nanoengineer 1 com   Current source  https   github com kanzure nanoengineer pybacktest   Simple yet powerful backtesting framework in python pandas    Currently I don t plan to continue working on this project    About   It allows user to specify trading strategies using full power of pandas  at the same time hiding all boring things like manually calculating trades  equity  performance statistics and creating visualizations  Resulting strategy code is usable both in research and production setting    Strategies could be defined as simple this   python ms   pandas rolling mean ohlc C  50  ml   pandas rolling mean ohlc C  100  buy   cover    ms   ml     ms shift     ml shift    sell   short    ms   ml     ms shift     ml shift      And then tested like this   pybacktest Backtest locals      We use it in our research and production operations    Installation   pip install git https   github com ematvey pybacktest git  If you don t install it in virtualenv  you might need to prepend last line with sudo    Tutorial   Tutorials are provided as ipython notebooks in folder  examples   You run it from cloned repo or  watch via nbviewer     Status   Single security backtester is ready  Multi security testing could be implemented by running single sec backtests and then combining equity  Later we will add easier way  pystuff   Collection of useful Python codes RedisPyObj   Python redis proxy read write objects  dict  list  set    TODO    package  setup py   auto serialization golearn   Machine Learning with Go etsys   Toy trading simulator and infrastructure in Go ratelimiter   Rate limiting library written in Go  using redis as it s backend  pcache GoNets   Running Neural Networks in Go   GoNets allows you to specify and run feed forward neural networks  It does not support training  and I have no plans to implement that in immediate future  The goal of this package is to integrate nets trained elsewhere into golang projects    Specifying networks is as simple as   golang biases      float64 0 0  0 1   0 3  weights        float64       0 3  0 4   0 1         0 1  0 1  0 0         0 3  0 0  0 0     net  err    SoftmaxLayer InputLayer 3   biases  weights  predictions  err    net GetOutput   float64 3  2  1   gostat   Collection of statistical routines in golang   Forked from code google com p gostat on 28 04 2015 Python Data Science Lab in Docker Silk   Hierarchical Data Smoothing in Python   Loosly related to the following statistical procedures     https   en wikipedia org wiki Inverse probability weighting   https   en wikipedia org wiki Additive smoothing seq2seq with TensorFlow   Collection of unfinished tutorials  May be good for educational purposes    1    simple sequence to sequence model with dynamic unrolling     Deliberately slow moving  explicit tutorial  I tried to thoroughly explain everything that I found in any way confusing    Implements simple seq2seq model described in  Sutskever at al   2014  and tests it against toy memorization task        Picture from  Sutskever at al   2014   2    advanced dynamic seq2seq     Encoder is bidirectional now  Decoder is implemented using  tf nn raw rnn   It feeds previously generated tokens during training as inputs  instead of target sequence        Picture from  Deep Learning for Chatbots   3    Using  tf contrib seq2seq   TF  1 1      New dynamic seq2seq appeared in r1 0  Let s try it      UPDATE  that this tutorial doesn t work with tf version   1 1  API  I recommend checking out new  official tutorial  instead to learn high level seq2seq API  Deep Text Classifier   Implementation of document classification model described in  Hierarchical Attention Networks for Document Classification  Yang et al   2016      How to run       Create a virtual environment  activate it  and install requirements   python3  m venv env source env bin activate pip install  r requirements txt       Download the English model for spaCy        python  m spacy download en     Get  Yelp review dataset  and extract it in this directory   python3 yelp prepare py dataset review json python3 worker py   mode train   device  gpu 0   batch size 30     Results   I am getting 65  accuracy on a dev set  16  of data  after 3 epochs  Results reported in the paper are 71  on Yelp 15  No systemic hyperparameter optimization was performed  AI copywriter   Given a large corpus of correct English texts and a one way mechanism to corrupt a sentence in such a way that will be similar to mistakes that non native speaker makes  we can develop seq2seq model that acts like denoising autoencoder  It could be used by non proficient English writers to write better texts    Main hypothesis is that corruption does not have to be very clever  just sufficiently diverse  Currently I use a set of simple heuristics for corruption  but later hope to develop a generative model for that    Status   Trying to make it work  LSTMs seem to be less volatile then GRUs  Deeper nets  5 layers vs 3 layers  seem to converge to higher loss    Probably unnecessary warning  this is research project  not in any way suitable for production use  No API stability guarantee of any kind is given  If you want to derive from this work  copy paste it    How to run   Requires Python   3 5 and TensorFlow r1 0 0    Download Wikipedia dump e g  with    wget  c https   dumps wikimedia org enwiki latest enwiki latest pages articles xml bz2   Install deps    Run preprocessor    python3 wiki prepare py path to wikidump   It should take several hours  Corruption types that preprocessor is doing could be found in wiki data op py    Run training    python3 worker py   device  gpu 0   batch size 30   You may want to increase decrease batch size based on your GPU mem  I have GTX 970 with 4gb    Related work    atpaino  implemented similar model  on smaller corpus in Jan 2017  His work is based on tensorflow s  translate  example  which uses static RNN rollout with bucketizing  In this work I attempt to implement dynamic RNN rollout using tensorflow s lower level blocks    License   MIT pytorch rnn   Collection of RNN research models in PyTorch   Contents   Seq2Seq with attention   toy copy task general ai   Agent training  with all optional parameters    python reinforce commai py  t tasks micro1 json  o  output       id 0  l INFO   Task schedulers   Task scheduler  tasks micro1 json  contains only micro1 task with infinite rotation    Task scheduler     Round1 src tasks config challenge json  is a scheduler proposed by organizers  Carvana Semantic Segmentation   solution     Challenge link     This solution gets 0 9945 dice  which is within 0 25  from current top  but still places in the middle of the leaderboard  competition requires pixel perfect segmentation      This is mainly for fun solution  without any special tricks  no ensembling  no data augmentation   with very little hyperparameter tuning    Architecture  slightly customized UNet  so that it can process images in native 1918x1280 resolution    Framework  PyTorch   Running     Extract competition data in  input    Preprocess data with  python dataset py   20 min    Train to convergence with  python train py   12  hours    If necessary  generate submission with  python make submission py     License   MIT
15,keskarnitish,OBA   Author   Nitish Shirish Keskar   OBA  is a second order method for convex L1 regularized optimization with active set prediction   OBA belongs to the family of  Orthant Based  methods  such as OWL  and uses a selective corrective mechanism which brings about increased efficiency and robustness     Features   The OBA package     allows for solving general convex L1 regularized problems including Logistic Regression and LASSO    is written in pure MATLAB with minimal dependencies and emphasizes simplicity and cross platform compatibility     includes both Newton and quasi Newton options for the proposed algorithm      Usage Guide   The algorithm can be run using the syntax    x   OBA funObj lambda  options      Here     funObj  is an object with member functions for computing the function  gradient and Hessian vector products at the iterates  Logistic Regression and LASSO classes are provided with the package  The file  funTemplate m  can be used as a base for designing a custom function     lambda  is the positive scalar for inducing sparsity in the solution     options  is an optional argument for changing the default parameters used in OBA  For ease of use  the user can generate the default options struct using  options GenOptions    and change the parameters therein before passing it to OBA    The parameters and their default values are         options optol   termination tolerance            default  1e 6          options qn   Quasi Newton  0  Newton s Method   or 1  quasi Newton             default  0          options mem size   quasi Newton memory size            default  20          options maxiter   max number of iterations            default  1000          options printlev   print level  0  no printing  or 1            default  1          options CGtol   CG termination tolerance  for Newton s Method             default  1e 1          options maxCGiter   max number of CG iterations  Newton s Method             default  1000     For a detailed documentation of OBA and its associated functions  use  help OBA     Citation   If you use OBA for your research  please cite the paper   article OBA Keskar2016  author    N  Keskar and J  Nocedal and F   ztoprak and A  W chter   title    A second order method for convex  regularized optimization with active set prediction   journal    Optimization Methods and Software   volume    0   number    0   pages    1 17   year    0   doi    10 1080 10556788 2016 1138222   URL    http   dx doi org 10 1080 10556788 2016 1138222   eprint    http   dx doi org 10 1080 10556788 2016 1138222    Templates   Templates for  Smooth  Nonlinear Optimization minSQN   Stochastic Quasi Newton Optimization in MATLAB   Authors   Nitish Shirish Keskar  and  Albert S  Berahas   Please contact us if you have any questions  suggestions  requests or bug reports    Introduction   This is a package for solving an unconstrained minimization problem of the form  min f x     1 n  sum i f i x     minSQN allows for the user to solve large scale  sum of functions  optimization problems using one of 11 Stochastic Quasi Newton methods    The following table summarizes all the methods that minSQN contains  The methods are classified in terms of    Hyperparameters   Length of LBFGS memory  limited inf   if inf  BFGS method used    Powell damping   Hessian damping   Curvature pair update  y                                                                                                                                               Method    Hyperparameters   LBFGS Memory   Powell Damping   Hessian Damping       Curvature pair                Reference                                            finite inf          Y N              Y N                y  update                                                                                                                                                                                    SQN         alpha  L         finite             N                N          Hessian vector product       Byrd et  al   2014                                                                                                                                                    DSQN         alpha  L         finite             Y                N          Hessian vector product                                                                                                                                                                             oBFGS      alpha  delta         inf              N                Y           Gradient differencing    Schraudolph et  al   2007                                                                                                                                               oLBFGS      alpha  delta       finite             N                N           Gradient differencing   Schraudolph et  al   2007                                                                                                                 Mokhtari et  al   2014                                                                                                                                                 D oBFGS     alpha  delta         inf              Y                Y           Gradient differencing                                                                                                                                                                           D oLBFGS     alpha  delta       finite             Y                N           Gradient differencing                                                                                                                                                                              RES       alpha  delta         inf              N                Y           Gradient differencing     Mokhtari et  al   2014                                                                                                                                                  L RES      alpha  delta       finite             N                N           Gradient differencing                                                                                                                                                                            SDBFGS      alpha  delta         inf              Y                Y           Gradient differencing       Wang et  al   2014                                                                                                                                                  L SDBFGS     alpha  delta       finite             Y                Y           Gradient differencing                                                                                                                                                                             adaQN        alpha  L         finite             N                N          Hessian vector product      Keskar et  al   2015                                                                                          accumulated Fisher                                                                                                                                                                             Features   The minSQN package     is written in pure MATLAB with minimal dependencies and emphasizes simplicity  extendibility and cross platform compatibility     allows the user to run 11 different stochastic quasi Newton methods which are able to solve a vast array of problems  both convex and non convex      comes with an automatic hyperparameter tuning mechanism thus obviating the need for manually tuning the parameters for any of the included methods       Citation   If you use minSQN for your research  please cite the Github repository     misc minSQN2016     author    Nitish Shirish Keskar and Albert S  Berahas      title     minSQN    S tochastic  Q uasi  N ewton  O ptimization in  MATLAB       year    2016      url    https   github com keskarnitish minSQN       note     Online        Usage Guide   The algorithm can be run using the syntax    logger   minSQN problem options  hyperparameters     where     problem  is an object pertaining to a specific loss function and data set      options  is a struct containing the necessary parameters for use in the optimization algorithms      hyperparameters  is an array of hyperparameters necessary for the optimization algorithms such as the step size  damping constants and aggregation lengths  This is an optional argument  If it is not specified  minSQN uses its inbuilt automatic tuner  which we describe next  to find hyperparameters     Automatic Tuning    All method above have certain hyperparameters that need to be set or tuned  The second column in the table above indicates what hyperparameters are needed for each of the methods    In minSQN  we provide an automatic tuning mechanism that randomly samples hyperparameters  as in Bergstra et  al   2012   from a prespecified range of hyperparameter values  solves the problem several times  and returns the best optimization run and hyperparameter setting  The number of tuning steps is determined in the options  default is 10       Example  no tuning     To solve a problem using minSQN  the user must follow 4 steps      Construct the problem class  Logistic Regression and Least Squares are included with the minSQN code  Others can be coded easily using our template    Generate default options using  GenOptions    and over write them as needed   Set the hyperparameters necessary for the specific method  For instance  SGD requires the step size  RES requires the step size and the damping constant and SQN requires the step size and the aggregation length    Run minSQN         X   randn 5000 500   y   2  randn 5000 1    0 5    1  problem   lossFunctions LogReg X y     options   GenOptions    options method    SQN   sqn log untuned   minSQN problem options  5e 2 5          The output of  minSQN  would be        sqn log untuned                fhist   21x1 double  hyperparameters   0 050000000000000 5           w star   500x1 double        where fhist is the history of average loss function values over each epoch  hyperparameters returns the provided hyperparameters in the case when they are provided and returns their tuned values if the automatic tuning was chosen  see next example   w star  is the value of the iterate at the end of the optimization      Example  with tuner     The process for running the methods with automatic tuning is similar to above except no hyperparameters are passed as input  as in Step 3 above        X   randn 5000 500   y   2  randn 5000 1    0 5    1  problem   lossFunctions LogReg X y     options   GenOptions    options method    SQN   sqn log tuned   minSQN problem options         The output of  minSQN  in this example would be      sqn log tuned                fhist   21x1 double  hyperparameters   0 006962319523931 26           w star   500x1 double        fhist and w star are the function values and final iterate as explained in the previous example  In this case  hyperparameters  is the value of the best hyperparameters as chosen by the automatic tuning mechanism     Please refer to  demo m  for a short demonstration of using  minSQN  for solving a small Logistic Regression problem using three different methods and plotting the results  For a detailed documentation of minSQN and its associated functions  use MATLAB s  help   For instance  to obtain details about the different options and their significance  use  help GenOptions     References    SGD    Bottou  L   1998  Online learning and stochastic approximations  On line learning in neural networks  17 9   p 142    SQN    Byrd  R  H   Hansen  S  L   Nocedal  J     Singer  Y   2016   A stochastic quasi Newton method for large scale optimization  SIAM Journal on Optimization  26 2   1008 1031    oBFGS    Schraudolph  N N   Yu  J  and G nter  S   2007  A stochastic quasi Newton method for online convex optimization  In International Conference on Artificial Intelligence and Statistics  pp  436 443     oLBFGS    Schraudolph  N N   Yu  J  and G nter  S   2007  A stochastic quasi Newton method for online convex optimization  In International Conference on Artificial Intelligence and Statistics  pp  436 443     Mokhtari  A     Ribeiro  A   2015   Global convergence of online limited memory bfgs  Journal of Machine Learning Research  16  3151 3181    RES    Mokhtari  A  and Ribeiro  A   2014  Res  Regularized stochastic bfgs algorithm  Signal Processing  IEEE Transactions on  62 23   pp 6089 6104    SDBFGS    Wang  X   Ma  S   Goldfarb  D     Liu  W   2014   Stochastic Quasi Newton Methods for Nonconvex Stochastic Optimization  arXiv preprint arXiv 1607 01231    adaQN    Keskar  N  S     Berahas  A  S   2016   adaQN  An Adaptive Quasi Newton Algorithm for Training RNNs  European Conference Machine Learning and Knowledge Discovery in Databases   ECML PKDD 2016   Part I  Vol 9851  1 16   On Large Batch Training for Deep Learning  Generalization Gap and Sharp Minima   by Nitish Shirish Keskar  Dheevatsa Mudigere  Jorge Nocedal  Mikhail Smelyanskiy and Peter Tang   Paper link   arXiv preprint   Table of Contents     Introduction   Citation   Disclaimer and Known Issues   Usage     Update  April 24    Our code was written in Keras 1 X  there have been a lot of API changes in Keras 2 X which have broken our code  We re working on updating our code to support Keras 2 X but in the meantime  provide a preliminary PyTorch implementation  refer to the PyTorch folder for details   As always  we welcome any questions  suggestions  requests or bug reports     Introduction   This repository contains  Python  code needed to reproduce some of the figures in our  paper   The plots illustrate the relative  sharpness  of the minima obtained when trained using small batch  SB  and large batch  LB  methods  For ease of exposition  we use a Keras Theano setup but owing to the simplicity of the code  translating the code into other frameworks should be easy  Please contact us if you have any questions  suggestions  requests or bug reports    Citation   If you use this code or our results in your research  please cite     article Keskar2016      author    Nitish Shirish Keskar  Dheevatsa Mudigere  Jorge Nocedal  Mikhail Smelyanskiy and Ping Tak Peter Tang       title    On Large Batch Training for Deep Learning  Generalization Gap and Sharp Minima       journal    arXiv preprint arXiv 1609 04836       year    2016       Disclaimer and Known Issues     In the included code  we use Theano Keras to train the networks C1   C4 using a batch size of 256  for SB  and using 5000  for LB   Depending on your hardware  especially if using GPUs   you may run into memory issues when training using larger batch sizes  If this happens  you can either train using a different setup  such as CPUs with large host memory  or adapt our code to enable multi GPU training     The code for computing the  sharpness  of a minima  Metric 2 1  will be released soon  As is the case with the parametric plots  the code is quite straightforward  The code in Keras  pull request   3064  along with SciPy s  L BFGS B  optimizer can be used in conjunction to compute the values easily       Usage   To reproduce the parametric plots  you only need the two Python files   plot parametric plot py  and  network zoo py    The latter contains the model configurations for the C1 C4 networks  the former trains the model imported from  network zoo  using the SB and LB methods and plots the parametric plot connecting the two minimizers  The network is chosen using a command line argument   n   or    network   and the generated plot is saved in PDF form  For instance  to plot for the C1 network  one can simply run    KERAS BACKEND theano python plot parametric plot py  n C1    with the necessary Theano flags depending on the setup  The figure in the  Figures   folder should resemble    A Limited Memory Quasi Newton Algorithm for Bound Constrained Nonsmooth Optimization   by  Nitish Shirish Keskar  and  Andreas Waechter   Paper link   arXiv preprint   Table of Contents     Introduction   Citation   Disclaimer and Known Issues   Usage     Introduction   The proposed method  NQN  is a limited memory quasi Newton method for bound constrained nonsmooth optimization  It is an active set method in that it operates iteratively in a two phase approach of predicting the optimal active set and computing steps in the identified subspace  The code is written in pure Python and aims to mimic the calling syntax of  scipy optimize fmin l bfgs b   In order to replace an existing call to  fmin l bfgs b  with our code  simply import  NQN py  and replace  scipy optimize fmin l bfgs b  to  NQN fmin l bfgs b   In most cases  no changes to the calling arguments should be necessary  We include additional usage details at the end of this README    Please contact us if you have any questions  suggestions  requests or bug reports    Citation   If you use this code or our results in your research  please cite     article Keskar2016      author    Nitish Shirish Keskar and Andreas Waechter       title    A Limited Memory Quasi Newton Algorithm for Bound Constrained Nonsmooth Optimization       journal    arXiv 1612 07350       year    2016       Disclaimer and Known Issues     Our code is written in pure Python  Circumstantially  this can be slower than SciPy s L BFGS B implementation which is written in Fortran     If you set the value of the gradient sampling memory  M  to be greater than 1  you need the  CVXOPT  package for solving the QP  This can be easily installed via  pip   However  we do not recommend setting M to be greater than 1 unless you need extremely high precision solutions    Please write to us if you need the Python code for the test problems  other solvers wrappers or scripts for generating figures       Usage   In order to use our code  simply import the  NQN py  file and call  NQN fmin l bfgs b  with the calling options similar to  scipy optimize fmin l bfgs b   Concretely  the function signature is   fmin l bfgs b funObj  x0  gradObj  bounds None  m 20  M 1  pgtol 1e 5  iprint  1  maxfun 15000  maxiter 15000  callback None  factr 0     where    funObj      Function to minimize  x0          Initial starting point gradObj     Gradient of objective function  Unlike L BFGS B  for NQN  this  must  be provided  bounds      A tuple of bound constraints  Use   numpy inf  numpy inf  if a a variable in unconstrained m           L BFGS memory M           Gradient sampling memory pgtol       Scaled 2 norm tolerance for convergence iprint      Controls output  iprint 0 prints to STDOUT maxfun      Max  number of function evaluations maxiter     Max  number of iterations callback    Any function  called before every iteration factr       The iteration stops when  f k   f  k 1   max  f k   f  k 1   1     factr   1E 16   The only additional option we employ over SciPy s L BFGS B implementation is the size of the gradient sampling memory  M  which is initialized to 1  Further  the user can choose to employ a BFGS variant of our code  i e   retaining BFGS matrices instead of limited memory curvature pairs  by setting  m numpy inf     A sample usage of our code  on the popular Rosenbrock function  is      import scipy optimize import NQN import numpy   n   100 funObj   scipy optimize rosen gradObj   func   scipy optimize rosen der bounds      0 5 0 5  for i in range n   x0   numpy zeros n    scipy output   scipy optimize fmin l bfgs b func x0 fprime grad bounds bounds  NQN output   NQN fmin l bfgs b func x0 fprime grad bounds bounds        Similar to the SciPy L BFGS B implementation  we return the final iterate  corresponding function value and an  information dictionary   We use different termination flags from SciPy s L BFGS B implementation  They are  Termination Flag  0    Converged 1    Reached Maximum Iterations 2    Reached Maximum Function Evaluations 3    Converged to Nonstationary Point 4    Abnormal Termination in Line Search 5    Iterate Has NaN values 6    Numerical Issues 7    FACTR Convergence MyWebsite   The files for my website
16,jatrost,Development of this project has moved to the Apache git repo and has mostly been picked up by Josh Elser  http   people apache org  elserj accumulo pig    This code will remain here because it is still being used in production by some      build the JAR  Note  you will need to download the accumulo src  build it  and install it into your maven repo before this will work    mvn package    download the JARs needed by pig   mvn dependency copy dependencies  DoutputDirectory lib   DincludeArtifactIds zookeeper libthrift accumulo core cloudtrace    print the register statements we will need in pig   for JAR in lib   jar target accumulo pig 1 4 0 jar    do      echo register  pwd   JAR   done    Example output   register  home developer workspace accumulo pig lib accumulo core 1 4 0 jar register  home developer workspace accumulo pig lib cloudtrace 1 4 0 jar register  home developer workspace accumulo pig lib libthrift 0 6 1 jar register  home developer workspace accumulo pig lib zookeeper 3 3 1 jar register  home developer workspace accumulo pig target accumulo pig 1 4 0 jar    Run Pig  copy the register statements above and paste them into the pig terminal   Then you can LOAD from and STORE into accumulo      pig 2012 03 02 08 15 25 808  main  INFO  org apache pig Main   Logging error messages to   home developer workspace accumulo pig pig 1330694125807 log 2012 03 02 08 15 25 937  main  INFO  org apache pig backend hadoop executionengine HExecutionEngine   Connecting to hadoop file system at  hdfs   127 0 0 1  2012 03 02 08 15 26 032  main  INFO  org apache pig backend hadoop executionengine HExecutionEngine   Connecting to map reduce job tracker at  127 0 0 1 9001 grunt  register  home developer workspace accumulo pig lib accumulo core 1 4 0 jar grunt  register  home developer workspace accumulo pig lib cloudtrace 1 4 0 jar grunt  register  home developer workspace accumulo pig lib libthrift 0 6 1 jar grunt  register  home developer workspace accumulo pig lib zookeeper 3 3 1 jar grunt  register  home developer workspace accumulo pig target accumulo pig 1 4 0 jar grunt   grunt  DATA   LOAD  accumulo   webpage instance inst user root password secret zookeepers 127 0 0 1 2181 columns f cnt         using org apache accumulo pig AccumuloStorage   AS  row  cf  cq  cv  ts  val   grunt   grunt  DATA2   FOREACH DATA GENERATE row  cf  cq  cv  val  grunt   grunt  STORE DATA2 into  accumulo   webpage content instance inst user root password secret zookeepers 127 0 0 1 2181  using org apache accumulo pig AccumuloStorage    2012 03 02 08 18 44 090  main  INFO  org apache pig tools pigstats ScriptState   Pig features used in the script  UNKNOWN 2012 03 02 08 18 44 093  main  INFO  org apache pig newplan logical rules ColumnPruneVisitor   Columns pruned for DATA   4 2012 03 02 08 18 44 108  main  INFO  org apache pig backend hadoop executionengine mapReduceLayer MRCompiler   File concatenation threshold  100 optimistic  false 2012 03 02 08 18 44 110  main  INFO  org apache pig backend hadoop executionengine mapReduceLayer MultiQueryOptimizer   MR plan size before optimization  1 2012 03 02 08 18 44 110  main  INFO  org apache pig backend hadoop executionengine mapReduceLayer MultiQueryOptimizer   MR plan size after optimization  1 2012 03 02 08 18 44 117  main  INFO  org apache pig tools pigstats ScriptState   Pig script settings are added to the job 2012 03 02 08 18 44 118  main  INFO  org apache pig backend hadoop executionengine mapReduceLayer JobControlCompiler   mapred job reduce markreset buffer percent is not set  set to default 0 3 2012 03 02 08 18 44 120  main  INFO  org apache pig backend hadoop executionengine mapReduceLayer JobControlCompiler   creating jar file Job7611629033341757288 jar 2012 03 02 08 18 46 282  main  INFO  org apache pig backend hadoop executionengine mapReduceLayer JobControlCompiler   jar file Job7611629033341757288 jar created 2012 03 02 08 18 46 286  main  INFO  org apache pig backend hadoop executionengine mapReduceLayer JobControlCompiler   Setting up single store job 2012 03 02 08 18 46 375  main  INFO  org apache pig backend hadoop executionengine mapReduceLayer MapReduceLauncher   1 map reduce job s  waiting for submission  2012 03 02 08 18 46 876  main  INFO  org apache pig backend hadoop executionengine mapReduceLayer MapReduceLauncher   0  complete 2012 03 02 08 18 46 878  Thread 17  INFO  org apache pig backend hadoop executionengine util MapRedUtil   Total input paths  combined  to process   1 2012 03 02 08 18 47 887  main  INFO  org apache pig backend hadoop executionengine mapReduceLayer MapReduceLauncher   HadoopJobId  job 201203020643 0001 2012 03 02 08 18 47 887  main  INFO  org apache pig backend hadoop executionengine mapReduceLayer MapReduceLauncher   More information at  http   127 0 0 1 50030 jobdetails jsp jobid job 201203020643 0001 2012 03 02 08 18 54 434  main  INFO  org apache pig backend hadoop executionengine mapReduceLayer MapReduceLauncher   50  complete 2012 03 02 08 18 57 484  main  INFO  org apache pig backend hadoop executionengine mapReduceLayer MapReduceLauncher   100  complete 2012 03 02 08 18 57 485  main  INFO  org apache pig tools pigstats SimplePigStats   Script Statistics   HadoopVersion    PigVersion    UserId    StartedAt    FinishedAt    Features 0 20 2    0 9 2    developer    2012 03 02 08 18 44    2012 03 02 08 18 57    UNKNOWN  Success   Job Stats  time in seconds   JobId    Maps    Reduces    MaxMapTime    MinMapTIme    AvgMapTime    MaxReduceTime    MinReduceTime    AvgReduceTime    Alias    Feature    Outputs job 201203020643 0001    1    0    3    3    3    0    0    0    DATA DATA2    MAP ONLY    accumulo   webpage content instance inst user root password secret zookeepers 127 0 0 1 2181   Input s   Successfully read 288 records from   accumulo   webpage instance inst user root password secret zookeepers 127 0 0 1 2181 columns f cnt   Output s   Successfully stored 288 records in   accumulo   webpage content instance inst user root password secret zookeepers 127 0 0 1 2181   Counters  Total records written   288 Total bytes written   0 Spillable Memory Manager spill count   0 Total bags proactively spilled  0 Total records proactively spilled  0  Job DAG  job 201203020643 0001   2012 03 02 08 18 57 492  main  INFO  org apache pig backend hadoop executionengine mapReduceLayer MapReduceLauncher   Success  grunt   I recently needed a quick way to analyze millions of small binary files  from 100K 19MB each  and I wanted a scalable way to repeatedly do this sort of analysis   I chose Hadoop as the platform  and I built this little framework  really  a single MapReduce job  to do it   This is very much a  work in progress  and feedback and pull requests are welcome    The main MapReduce job in this framework accepts a Sequence file of   Text  BytesWritable   where the   Text  is a name and the  BytesWritable  is the contents of a file   The framework unpacks the bytes of  the  BytesWritable  to the local filesystem of the mapper it is running on  allowing the mapper to run arbitrary analysis tools that require local filesystem access   The framework then captures stdout and stderr from the analysis tool script and stores it  how it stores it is pluggable  see  io covert binary analysis OutputParser      Building    mvn package assembly assembly    Running    JAR target hadoop binary analysis 1 0 SNAPSHOT job jar    a local directory with files in it  directories are ignored for now  LOCAL FILES src main java io covert binary analysis  INPUT  dir in hdfs  OUTPUT  output dir in hdfs     convert a bunch of relatively small files into one sequence file  Text  BytesWritable  hadoop jar  JAR io covert binary analysis BuildSequenceFile  LOCAL FILES  INPUT    Use the config properties in example xml to basically run the wrapper sh script on each file using Hadoop   as the platform for computation hadoop jar  JAR io covert binary analysis BinaryAnalysisJob  files wrapper sh  conf example xml  INPUT  OUTPUT    From example xml       property       name binary analysis program  name       value   wrapper sh  value      property      property       name binary analysis program args  name       value   file   value      property      property       name binary analysis program args delim  name       value    value      property     This block of example instructs the framework to run  wrapper sh  using the args of    file    where    file   is replaced by the unpacked filename from the Sequence File   If multiple command line args are required  they can be specified by appending a delimiter and then each arg to the value of the  binary analysis program args  property   FileFormatToConverterJob   Useful for performing distributed file computation  mainly tailored for converting large binary files to a different format   Example  converting a weird compressed file format to a normal one that can use standard Hadoop tools    hadoop fs  ls files   awk   print  8      tmp all      OR hadoop fs  lsr   grep  v   d    awk   print  8      tmp all  mkdir file lists cd file lists split  l 10  tmp all cd    hadoop fs  put file lists file lists  JAR target hadoop binary analysis 1 0 SNAPSHOT job jar hadoop jar  JAR io covert util FileFormatToConverterJob  Dstream process command   opt decompress sh  file lists  hadoop dns mining   This is a small framework for performing large amounts of DNS lookups using Hadoop  This is a work in progress  pull requests are welcome    Here are the steps for getting it working    Download  compile and install the Maxmind JAR into maven   wget http   geolite maxmind com download geoip api java GeoIPJava 1 2 5 zip unzip GeoIPJava 1 2 5 zip cd GeoIPJava 1 2 5 source com maxmind geoip  javac   java cd           zip  r maxmind jar com  mvn install install file  Dfile maxmind jar  DgroupId com maxmind  DartifactId geo ip  Dversion 1 2 5  Dpackaging jar    Obtain the Maxmind IP Geo Database   wget http   geolite maxmind com download geoip database GeoLiteCity dat gz gzip  d GeoLiteCity dat gz    Obtain the Maxmind ASN Database   wget http   www maxmind com download geoip database asnum GeoIPASNum dat gz gzip  d GeoIPASNum dat gz    Create obtain large lists of domain names  e g  domains txt  and copy them into HDFS     you may want to split these domain files before placing in HDFS in order to use more mappers split  a 5  d  l 100000  domains txt domains  hadoop fs  put domains    data domains     Download and build this project   git clone https   jt6211 github com jt6211 hadoop dns mining git cd hadoop dns mining mvn package assembly assembly    Run the various MapReduce jobs     These are the records that will be requested REC TYPES A MX NS TXT  JAR target hadoop dns mining 1 0 SNAPSHOT job jar    performs A record  MX record  and NS record lookups on each domain provided using 50    resolving threads per Mapper using the nameserver of 8 8 8 8 and store the results in    HDFS in  data dns mining 01 raw   Note  choose the nameserver wisely  otherwise you may overload it   In testing I mainly     used a bind server deployed on each hadoop node so my nameserver was 127 0 0 1 time hadoop jar  JAR io covert dns collection CollectionJob        D dns collection num resolvers 50        D dns collection nameservers 8 8 8 8       IN         REC TYPES         data domains         data dns mining 01 raw    parse the raw responses into JSON  one record per RR in the DNS responses  time hadoop jar  JAR io covert dns parse ParseJob        data dns mining 01 raw        data dns mining 02 parsed    lookup any IP addresses in the results in the maxmind DBs and enrich the records time hadoop jar  JAR io covert dns geo GeoJob        files  usr local lib maxmind GeoLiteCity dat  usr local lib maxmind GeoIPASNum dat       GeoLiteCity dat       GeoIPASNum dat        data dns mining 02 parsed         data dns mining 03 enriched    run a filter job for the rec types requested as well as for rec types that commonly occur in    the results as part of normal queries   This will separate the various DNS records into their   own directories in HDFS for X REC  echo   REC TYPES SOA NS CNAME    sed  s    n g   sort  u    do     time hadoop jar  JAR io covert dns filtering FilterJob            type      REC              data dns mining 03 enriched            data dns mining 04 filtered type  REC   done    This is a JEXL expression that filters out target fields that are IP addresses    and returns the target field lowercased TARGET EXPR  if target       d 1 3    d 1 3    d 1 3    d 1 3      return target toLowerCase       extract the  target  field from the MX records time hadoop jar  JAR io covert dns extract ExtractorJob   TARGET EXPR         data dns mining 04 filtered type MX  data dns mining 05 extracted mailservers    extract the  target  field from the NS records time hadoop jar  JAR io covert dns extract ExtractorJob   TARGET EXPR         data dns mining 04 filtered type NS  data dns mining 05 extracted nameservers  HOST EXPR  if host       d 1 3    d 1 3    d 1 3    d 1 3      return host toLowerCase       extract the  host  field from the SOA records time hadoop jar  JAR io covert dns extract ExtractorJob   HOST EXPR         data dns mining 04 filtered type SOA  data dns mining 05 extracted nameservers SOA  This is a modified version of peframe  https   code google com p peframe   that produces JSON output   Usage    python peframe py FILE    There are no command line options  it just dumps json as one line   To install the project dependencies do this    pip install  r requirements txt Licensed under GNU GPLv3 Modern Honey Network   Multi snort and honeypot sensor management  uses a network of VMs  small footprint SNORT installations  stealthy dionaeas  and a centralized server for management    For questions regarding installation please review the  MHN Troubleshooting Guide    Search past questions on the  modern honey network Google Group    Or send emails to  modern honey network googlegroups com     HONEYPOT   Deployed sensors with intrusion detection software installed  Snort  Kippo  Conpot  and Dionaea     MANAGEMENT SERVER   Flask application that exposes an HTTP API that honeypots can use to    Download a deploy script   Connect and register   Download snort rules   Send intrusion detection logs   It also allows systems administrators to    View a list of new attacks   Manage snort rules  enable  disable  download   INSTALLING SERVER  tested Ubuntu 12 0 4 3 x86 64 and Centos 6 7      The MHN server is supported on Ubuntu 12  Ubuntu 14  and Centos 6 7      Ubuntu 16 is not supported at this time      Other flavors versions of Linux may work  but are generally not tested or supported      Note  if you run into trouble during the install  please checkout the  troubleshooting guide  on the wiki   If you only want to experiment with MHN on some virtual machines  please check out the  Getting up and Running with Vagrant  guide on the wiki    Install Git     on Debian or Ubuntu   sudo apt get install git  y    on Centos or RHEL   sudo yum install  y git    Install MHN     cd  opt    sudo git clone https   github com threatstream mhn git   cd mhn     Run the following script to complete the installation   While this script runs  you will be prompted for some configuration options   See below for how this looks      sudo   install sh    Configuration                                                                MHN Configuration                                                             Do you wish to run in Debug mode   y n n Superuser email  YOUR EMAIL YOURSITE COM Superuser password   Server base url   http   1 2 3 4     Honeymap url   http   1 2 3 4 3000    Mail server address   localhost     Mail server port  25    Use TLS for email   y n n Use SSL for email   y n n Mail server username        Mail server password        Mail default sender        Path for log file   mhn log       Running   If the installation scripts ran successfully  you should have a number of services running on your MHN server   See below for checking these    user precise64  opt mhn scripts  sudo  etc init d nginx status    nginx is running user precise64  opt mhn scripts  sudo  etc init d supervisor status  is running user precise64  opt mhn scripts  sudo supervisorctl status geoloc                           RUNNING    pid 31443  uptime 0 00 12 honeymap                         RUNNING    pid 30826  uptime 0 08 54 hpfeeds broker                   RUNNING    pid 10089  uptime 0 36 42 mhn celery beat                  RUNNING    pid 29909  uptime 0 18 41 mhn celery worker                RUNNING    pid 29910  uptime 0 18 41 mhn collector                    RUNNING    pid 7872   uptime 0 18 41 mhn uwsgi                        RUNNING    pid 29911  uptime 0 18 41 mnemosyne                        RUNNING    pid 28173  uptime 0 30 08    Manual Password Reset   If email based password resets are not working for you  here is another method      cd  MHN HOME   source env bin activate   cd server   python manual password reset py  Enter email address  YOUR USER YOUR SITE com Enter new password   Enter new password  again    user found  updating password    Deploying honeypots with MHN   MHN was designed to make scalable deployment of honeypots easier   Here are the steps for deploying a honeypot with MHN      Login to your MHN server web app    Click the  Deploy  link in the upper left hand corner    Select a type of honeypot from the drop down menu  e g   Ubuntu 12 04 Dionaea      Copy the deployment command    Login to a honeypot server and run this command as root    That s it      Integration with Splunk and ArcSight   hpfeeds logger can be used to integrate MHN with Splunk and ArcSight   Installation below    Splunk   cd  opt mhn scripts  sudo   install hpfeeds logger splunk sh    This will log the events as key value pairs to  var log mhn splunk log   This log should be monitored by the SplunkUniveralForwarder    Arcsight   cd  opt mhn scripts  sudo   install hpfeeds logger arcsight sh    This will log the events as CEF to  var log mhn arcsight log   Data   The MHN server reports anonymized attack data back to Anomali  Inc   formerly known as ThreatStream    If you are interested in this data please contact   modern honey network googlegroups com    This data reporting can be disabled by running the following command from the MHN server after completing the initial installation steps outlined above    opt mhn scripts disable collector sh   Support or Contact   MHN is an open source project brought to you by the passionate folks at Anomali  Inc  Please check out our troubleshooting guide on the wiki  We will also lend a hand  if needed  Find us at   modern honey network googlegroups com     Credit and Thanks   MHN leverages and extends upon several awesome projects by the Honeynet project  Please show them your support by way of donation    LICENSE   Modern Honeypot Network   Copyright  C  2014   Anomali  Inc    This program free software  you can redistribute it and or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation  either version 2 1 of the License  or  at your option  any later version    This program is distributed in the hope that it will be useful  but WITHOUT ANY WARRANTY  without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE   See the GNU Lesser General Public License for more details    You should have received a copy of the GNU Lesser General Public License along with this program  if not  write to the Free Software Foundation  Inc   51 Franklin Street  Fifth Floor  Boston  MA  02110 1301  USA files
17,shayanb,owght mdd   Model Driven Development Project   To Do List Manager mdd   Model Driven Development Project   To Do List Manager Instagram Realtime   Instagram PHP MySQL RealTime Stide ADS   System Call based Anomaly Detection System Python   STIDE technique  sequence time delay embedding    STIDE technique Readings    Forrest et al  employed a methodology motivated by immune systems   This characterizes the problem as distinguishing  self  from  non self   normal and abnormal behaviors respectively   An event horizon is built from a sliding window applied to the sequence of system calls made by an application during normal use  The sequences formed by the sliding window are stored in a table that establishes the normal behavior model  During the deployment  detection  phase  if the pattern from the sliding window is not in the normal behavior database it is considered a mismatch  Input to the Stide detector takes the form of system call traces of an application for which the detector is trained  Specifically  Stide builds a  normal database  by segmenting the training data  of system call traces  into fixed length sequences   To do so  a sliding window of N is employed over the training dataset and the resulting system call patterns are stored in the  normal database   During testing  the same sliding window size is employed on the data  Resulting patterns are compared against the  normal database  and if there is no match  a mismatch is recorded  Given a window size of N and system call trace length M  anomaly rate for the trace is calculated by dividing the number of mismatches by the number of sliding window patterns  i e  M   N   1    4    1  S  Forrest  S  Hofmeyr  A  SoMayaji  and T  Longstaff   A sense of self for Unix processes   in Security and Privacy  1996  Proceedings   1996 IEEE Symposium on  May  1996  pp  120 128    2  S  Forrest  S  A  Hofmeyr  and A  SoMayaji   Computer immunology   Commun  ACM  vol  40  no  10  pp  88 96  Oct  1997  Online   Available  http   doi acm org 10 1145 262793 262811    3  S  A  Hofmeyr  S  Forrest  and A  SoMayaji   Intrusion detection using sequences of system calls   Journal of Computer Security  vol  6  no  3  p  151  1998   Online   Available  http   search ebscohost com login aspx  direct true db tsh AN 1531432 site ehost  live   4  Kayac k  H  G     Zincir Heywood  A  N   2008   Mimicry Attacks Demystified  What Can Attackers Do To Evade Detection   A  N  Zincir Heywood  Ed    1 11  DESCRIPTION OF FILES                           Presentation and Paper          Presentation pdf          FInal Report pdf       Problem description and details regarding the dataset and methods       Sample Data          Anubis good          Malware      Sample files from the dataset used in this project    for further information you can check these links       http   anubis iseclab org       http   anubis iseclab org  action publications       Sample outputs       Sample output files from the python folder       Python files          compare STIDE py     STIDE technique implementation with Python          SysCallExtract py      Extract system call number sequences from the Anubis Dataset   you can change the window size and the shift size by changing the W and K values   LICENSE   The MIT License  MIT    Copyright  c  2013 Shayan Eskandari   Permission is hereby granted  free of charge  to any person obtaining a copy of this software and associated documentation files  the  Software    to deal in the Software without restriction  including without limitation the rights to use  copy  modify  merge  publish  distribute  sublicense  and or sell copies of the Software  and to permit persons to whom the Software is furnished to do so  subject to the following conditions    The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software    THE SOFTWARE IS PROVIDED  AS IS   WITHOUT WARRANTY OF ANY KIND  EXPRESS OR IMPLIED  INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT  IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM  DAMAGES OR OTHER LIABILITY  WHETHER IN AN ACTION OF CONTRACT  TORT OR OTHERWISE  ARISING FROM  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE  TAF   Trace Analysis Framework  Version 0 2        Parser and tools to extract system call sequences from strace trace of an application        exports CSV of system calls   Process ID  System Call Name Number  Arguments  Return Values        windows the system call sequence to W sized windows  default 6        Use STIDE technique to check two databases  Normal  Malicious  to flag the anomaly system call sequence windows in the final file       outputs flagged system call sequence windows and converts the anomalies to system call name sequences        Support for default switches with following the forks  strace  o trace txt  C  f file      for more information see the readme in  docs    TODO       Add support for adding fields in the trace  time  relative time             Automate the whole process  not sure if this is needed  cause personally I needed every output of each python file so that would be one run for all the outputs         STIDE technique Readings    Forrest et al  employed a methodology motivated by immune systems   This characterizes the problem as distinguishing  self  from  non self   normal and abnormal behaviors respectively   An event horizon is built from a sliding window applied to the sequence of system calls made by an application during normal use  The sequences formed by the sliding window are stored in a table that establishes the normal behavior model  During the deployment  detection  phase  if the pattern from the sliding window is not in the normal behavior database it is considered a mismatch  Input to the Stide detector takes the form of system call traces of an application for which the detector is trained  Specifically  Stide builds a  normal database  by segmenting the training data  of system call traces  into fixed length sequences   To do so  a sliding window of N is employed over the training dataset and the resulting system call patterns are stored in the  normal database   During testing  the same sliding window size is employed on the data  Resulting patterns are compared against the  normal database  and if there is no match  a mismatch is recorded  Given a window size of N and system call trace length M  anomaly rate for the trace is calculated by dividing the number of mismatches by the number of sliding window patterns  i e  M   N   1    4    1  S  Forrest  S  Hofmeyr  A  SoMayaji  and T  Longstaff   A sense of self for Unix processes   in Security and Privacy  1996  Proceedings   1996 IEEE Symposium on  May  1996  pp  120 128    2  S  Forrest  S  A  Hofmeyr  and A  SoMayaji   Computer immunology   Commun  ACM  vol  40  no  10  pp  88 96  Oct  1997  Online   Available  http   doi acm org 10 1145 262793 262811    3  S  A  Hofmeyr  S  Forrest  and A  SoMayaji   Intrusion detection using sequences of system calls   Journal of Computer Security  vol  6  no  3  p  151  1998   Online   Available  http   search ebscohost com login aspx  direct true db tsh AN 1531432 site ehost  live   4  Kayac k  H  G     Zincir Heywood  A  N   2008   Mimicry Attacks Demystified  What Can Attackers Do To Evade Detection   A  N  Zincir Heywood  Ed    1 11    The MIT License  MIT    Copyright  c   2013   Shayan Eskandari   Shayan   a t   theshayan com    Permission is hereby granted  free of charge  to any person obtaining a copy of this software and associated documentation files  the  Software    to deal in the Software without restriction  including without limitation the rights to use  copy  modify  merge  publish  distribute  sublicense  and or sell copies of the Software  and to permit persons to whom the Software is furnished to do so  subject to the following conditions    The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software    THE SOFTWARE IS PROVIDED  AS IS   WITHOUT WARRANTY OF ANY KIND  EXPRESS OR IMPLIED  INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT  IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM  DAMAGES OR OTHER LIABILITY  WHETHER IN AN ACTION OF CONTRACT  TORT OR OTHERWISE  ARISING FROM  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE  btc papers   Collaborate on the writings Open Wallet Format   Bitcoin Wallet Convertor  for now  1 0   OWF py reads the keys from Bitcoin QT wallet  e g wallet dat  and exports them to MultiBit wallet format  e g multibit key  but also applicable for Blockchain info imports   I used jackjack jj fork of Pywallet py  https   github com jackjack jj pywallet  to read wallet dat and it is included in this repository  so you need to install the pywallet dependencies prior to the use of this    Why should I use this    To Convert your wallet dat  or exported wallet from Bitcoin QT  to    MultiBit Wallet   Blockchain Importable format   Backup your private keys in a format that would not be as complicated as wallet dat and less possible to be corrupted   WARNING  the output is not encrypted so take good care of the file     Usage    Usage  OWF py  options    Options      version             show program s version number and exit    h    help            show this help message and exit     wallet WALLET       wallet dat or the exported wallet from Bitcoin QT to                         be converted     newwallet NWWALLET  New wallet name  multibit format   in the                         same directory as the   wallet   Example     OWF py   wallet   wallet dat     OWF py   wallet   wallet dat   newwallet   newwallet key   TODO     Less dependencies to pywallet py   Add the other way convertion  Using Import Private keys of Pywallet py    Add   passphrase to read encrypted wallet dat   Nicer code  for now it works fine but it could be implemented in less hacky code style   Should be tested on windows too  tested on Mac and Linux so far    Define a standard for wallet format as  OWF     Support   Please support the work by either writing code or donate to 1owfJHTsWrrCpgaaYjC1vbJevuQzYRTYn   It would be greatly appreciated   GoPro Black 3 Plus Firmware Analysis   Analysis and findings on GoPro Black 3 Plus Firmware SEC gov form retriever   Bulk download forms in  txt format from SEC gov mygithubpage PHP Bitcoin Point of Sale   Customized for  Cafe Aunja   Montreal   Some features    Easy to install  PHP and MySQL    Real time BTC CAD  USD  conversion   Uses Blockchain info and Blockexplorer API  No need for Bitcoind    Generates a new address for each transaction  Privacy Preserved    RSA Encrypted private keys   Report page for the merchant  with showing the sales price and the realtime price of bitcoin amount   Admin page to decrypt and export the private keys associated with the addresses that holds a balance in MultiBit key format     Open source projects used     Bitcoin SCI    Bitcoin Shopping Card Interface  0 5 4  beta            phpseclib           PHP Elliptic Curve library     Sweet Alert    A Beautiful replacement for javascript s  Alert     bitcoin prices    Display bitcoin prices in human friendly manner in fiat currency using bitcoinaverage com market data     Installation     Download this repo and upload it to your webserver   Create the MySQL database with the same schema as the one in DB SCHEMA sql and assign the DB user password   Edit sci config php and sci dbconnet php   Make sure to set admin report superadmin password and also Database credentials   Security String can be ANY 16 characters or more    Leave PUBLIC RSA Key alone at this point   We will come back to it    Save and upload config php and dbconnect php   visit URL sci admin php   Login with your  admin  password from config php   Click RSA KeyGen  Save the private key offline in a safe place  put the public key in config php    Optional  Add your logo to  sci img logo jpg  or change the refrence in  css main css  Done        Some clarifications on the admins    sci admin php  to be used on the first time to generate the RSA keys  and also in case you want to decrypt or check balance any specific address   sci report php  A simple report page that shows all the confirmed transactions in the database     sci superadmin php  Almost same as report php but has the option to rescan the whole database to check the balances or just checks the temporary table to see if there was any transactions that has not been added to the final table  also you can  extract all the bitcoin private keys assosiated with the addresses that has balance in them  with MultiBit key style  You can easily save the output in a  key file and import it in  MultiBit  or import it in blockchain info      Most of the core functionality is from Bitcoin SCI by Jacob Bruce  Some notes by bitfreak group      The Bitcoin Shopping Cart Interface package is a set of libraries and tools that     enable you to process bitcoin tansactions with only PHP  You can have your own Instant Payment Notification system without the need for a middleman  If you ve been wondering how to handle customer payment since MyBitcoin went down  look no further  because this is the safest solution    An elliptic curve library written in PHP is used to achieve server side generation of FRESH bitcoin addresses for each customer  The script monitors the status of a payment by making use of the data supplied by blockexplorer com  As such  there is no need to install a heavy duty service such as bitcoind on your server  The only limitation with this PHP package is that you can t make outgoing payments    The bitcoin private keys are now encrypted using RSA public key cryptography technology  This means that the bitcoins keys are encrypted with a public RSA key  but they can only be decrypted with a private RSA key  So even if a hacker gains access to your bitcoin keys  they wont be able to decrypt that data unless they have your private RSA key  You can manage your keys by visiting the sci admin php script    The SCI package comes with a simple example to give you an idea about how to generate new keys and initiate a new payment through the Bitcoin Payment Gateway  This is NOT full shopping cart software  you would typically use this script to offer Bitcoins as one method of payment  The sci config php file needs to be modified to work properly on your website  You may also need to customize the following files    sci process order php and  sci ipn control php   Note   PHP 5 3 or later  earlier versions of PHP should work but will not support alt coins  NOTE  if you do not have 5 3 installed and wish to use BitcoinSCI  open up lib bitcoin lib php and change line 38 and 42 from return static   to return self   PHPExtension BCMath must be installed  most webhosts have it enabled by default        Screenshots           This project was done to meet client s requirments  most of the funtionalities have the potencial to be a lot more complete or have another model for implementation  such as admin report view    Contributions are more than welcome    1ARH4G6BCKM8xoFucEtaKP3Vq5Ahr7dqcv   Todo s     Fast Confirmation   check blockchain info API with Z0ro confirmation  who is going to do a successful double spend for a coffee     One complete admin panel  preferebly with a seperate report page   Nicer User Interface   BIP32  for address generation     License   GNU General Public License v2  GPL 2    You may copy  distribute and modify the software as long as you track changes dates of in source files and keep all modifications under GPL  You can distribute your application using a GPL library commercially  but you must also disclose the source code  Plague  Social Platform Python API     All Plague users are connected to each other right from the start  Infection starts at the source of the information  and spreads to the nearest users like a virus  Infected users spread the information further by infecting the users closest to them  Plague allows you to incubate information epidemics of any size  The possibilities are endless     plague io     Available Functions    login user password    vote repost post id    vote skip post id    send text text    comment post id  text    photo post file text   Not yet working   post link media link  media link preview  text    post delete post id    signup name email password lat lon    get posts uid token    get infections nearby uid  token    to get UserId and Token run this      python   plague python api py  e  name mailinator com   p  PAs w0R1       to post a text plague      python   plague python api py  u UserId  T Token  t  Text goes here      to Spread  Vote up  a plague      python   plague python api py  u UserId  T Token  p Post Id     to comment on a plague      python   plague python api py  u UserId  T Token  p Post Id  c  Comment Text      for more options      python   plague python api py  h     Also you can change the longitude and latitude to any of your choosing   token    TOKEN  u id    USERID  lon    90 0000  lat    0 0000      Disclaimer   This is for personal and research use only  No one likes spammers    Keywords  Plague  Plague Netwrok  social  platform  Python  API  plague io  reverse engineering  RE SaruTobi31   Play SaruTobi 24 7     As the new update of  SaruTobi  is now more secure and prevents this type of attack  I m releasing the code       SaruTobi is literally Japanese for  Monkey Fly   and this is pretty much the premise of the game  The user     flings him across an 8 bit jungle collecting floating bitcoin along the way     Christian Moss     What is SariTubi31   Before version 1 31  you could simulate game play and get bitcoin rewards without playing the game  hence without generating revenue for the developer    How   On the first run sarutobi31 will generate required bitcoin addresses and on the second run forward it will simulate the game play    genwallet key  holds the private keys and can be imported in MultiBit or Blockchain info     pubkeys txt  holds the public keys and is for the use of the script   result       laravel session COOKIE 3 Donation Address  3MXxfNZoifLYdS8wJTpvfeDNPt9ZWuMAaN Distance  5946 Passphrase  XXX        thestatus    sent        amount    0 0001    laravel session COOKIE2 4 Donation Address  3MXxfNZoifLYdS8wJTpvfeDNPt9ZWuMAaN Distance  5418 Passphrase  XXX        thestatus    sent        amount    0 0001          Am I rich    I donated back half of what I earned from this script to Sarutobi  and will keep the rest to tip people in reddit like I usually do    However the concern I have regarding this type of games is to overpollute the blockchain    Other than having too many dust transactions  when I was trying to send a transaction of 0 1 BTC from these addresses  Gathered by 1000 tips of 0 0001 BTC   MultiBit was not able to broadcast the transaction and had to rebuild it s blockchain database many times to show the right balance    Why 31      Disclaimer   I m not responsible for anything  anywhere  anytime For Support Related Rapidleech Visit   www rapidleech com   Rapid Leech is a free server transfer script for use on various popular upload download sites such as megaupload com  Rapidshare com and more than 45 others  The famous Rapidleech script transfers files from Rapidshare  Megaupload  Depositfiles com  Easy share com  etc  via your fast servers connection speed and dumps the file on your server  You may then download these files from your server anytime later    Rapidleech script has being used by more than 5 million users worldwide and has being installed on more than 2000 servers  For webmasters  if you have not tried the script before  download and install now and you will see how convenient the script can be  You may also generate income by offering your Rapidleech sites to end users and earn income from advertising programs  Some webmasters are earning hundreds per day on the advertising program Google and yahoo Ads  from their Rapidleech sites  Script installation is extremely easy and does not require any database    For end users  you may search on our forum for readily available installed scripts on servers worldwide  You may use them but please support these sites by visiting their sponsors or donate in order to keep these sites available  Testnet blocksize fork data Raw   For some tests I use Blocktrail testnet webhooks   Here are all the blocks  in structured format  that my service rejected in last  2 weeks for various reasons  some are just timeout issues that took more time than the next block to reach my server and considered an older block  and the later ones are regarding the forks on testnet blockchain for blocksize proposals  BIP101  8MB vote          Some info about the data     Starting block         height  580259      arrival time  2015 10 27T12 40 55 0000    End Block        height  601430      arrival time  2015 11 11T07 13 27 0000    I may not have the time to make some sense out of this data  but I know someone will  Although there might be better ways to get this data  including data for the main branch   It might be challenging for some    Keep me posted if you did something cool with it    p s some interesting things to look at https   twitter com lopp status 664099176234532864   e g         height   601529      block time    2015 11 11T08 20 02 0000       arrival time   2015 11 11T06 33 13 0000   two1 library   OTB   21212121212121212121212121212    2121212121212121212121212121212   21212121212121212121212121212121   21212121                  212121   2121212       121212      121212   212121      21212121      212121   21212121212121212121      212121   212121212      12121      212121   212121         21212     1212121   212121     212121212121212121212   212121     212121212121   212121   212121                    212121   212121                    212121   21212121212121212121212121212121   2121212121212121212121212121212    21212121212121212121212121212  Two1 Python3 library   other files   1 0 0     2 0 1     2 3 1   2 3 6  April 21 2016    Disclaimer   Original Package metadata    Metadata Version  1 1 Name  two1 Version  1 0 0 Summary  Buy and sell anything on the internet with Bitcoin  Home page  https   github com 21dotco two1 Author  21 Author email  21 21 co License  MIT Description    two1    buy sell anything on the internet with Bitcoin    Package  two1 Version  2 0 1 1 Architecture  all Maintainer  21  21 21 co  Installed Size  840 Depends  python3  python3 arrow  python3 click  python3 pytest  python3 requests  python3 responses  python3 simplejson  python3 any     3 3 2 2    libpam systemd  python3 pyaes  python3 base58  python3 jsonrpcclient  python3 jsonrpcserver  python3 path py  python3 tabulate  python3 sha256  python3 mnemonic  python3 protobuf  python3 funcsigs Section  python Priority  optional Description  Buy and sell anything on the internet with Bitcoin     two1    buy sell anything on the internet with Bitcoin    Package  two1 Version  2 3 6 1 Architecture  all Maintainer  21  21 21 co  Installed Size  1040 Depends  python3  python3 arrow  python3 click  python3 pytest  python3 requests  python3 responses  python3 simplejson  python3 yaml  python3 any     3 3 2 2    libpam systemd  rng tools  minerd armhf    0 3 4 1   python3 base58 armhf    0 2 2 1   python3 click armhf    4 1 1   python3 funcsigs armhf    0 4 1   python3 jsonrpcclient armhf    2 0 1 1   python3 jsonrpcserver armhf    3 1 1 1   python3 mnemonic armhf    0 13 1   python3 path py armhf    8 1 2 1   python3 protobuf armhf    3 0 0a3 1   python3 pyaes armhf    1 3 0 1   python3 sha256 armhf    0 1 1   python3 tabulate armhf    0 7 5 1   zerotier one armhf    1 1 4 1  Section  python Priority  optional Description  Buy and sell anything on the internet with Bitcoin  Russian Roulette   Summary   Randomly sends a file from the server file system  one of which is wifi config file   etc wpa supplicant wpa supplicant conf  which contains SSID and the PASSWORD   Running the server     python3 russianroulette server py    Running the client  and pull the trigger    wget https   goo gl 2cEQ1e  O russianroulette client py   python3 russianroulette client py shoot    API    1  Get info   HTTP URI      Params  None   Result                                                                                         xx                                              x x                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          description    I can lose my wifi ssid and password        name    Russian Roulette        pricing               shoot                  minimum   1313                       version   101      Pricing    Free    2  Pull the Trigger   HTTP URI   shoot   Params  None   Result     Content of the chosen file     Pricing    1313    TODO      add more random stuff   reboot the machine if shot    reboot the client if shot    Blockchain 411   Blockchain Analytics   Lookup Bitcoin addresses based on their cluster and the clusters they ve had transactions with  and some other basic information   Running the server     python3 411 server py    Running the client   21 buy   maxprice 100 url http   10 151 47 208 21411 lookup address  BITCOIN ADDRESS     or for a nice command line interface     wget https   goo gl JMBFev  O 411 client py   python3 411 client py    API    1  Get info   HTTP URI      Params  None   Result                                                                                                                                                                                                                                                                                                       Name  Blockchain 411     Version  104 Description  If I know  I would tell on you    Endpoint   lookup address  address       Minimum Price  100   Commandline usage  python3   411 client py lookup  BITCOIN ADDRESS    Do you want to lookup a Bitcoin address   Y n         Pricing    Free    2  Lookup and address   HTTP URI   lookup address  address    Params  bitcoin address   Result          key    1AAXXXXXXXXXXXXXXXXXXXXXXXXNSfRs       balance            confirmed   0         sent   11 78123559         received   11 78123559           identity            cluster               name    NucleusMarket             type    darkweb             neighbours   75301            id    XJZNdo                  web               count   0            detail                        count            transactions               total   12            sent   6            received   6                   relationships            rel clusters in                             name    AgoraMarket                type    darkweb                id    x5BRmk                level   1                            tagged addresses                Pricing    100    TODO      add more APIs   Start Bootstrap     Creative   Creative  is a one page creative theme for  Bootstrap  created by  Start Bootstrap     Getting Started   To use this theme  choose one of the following options to get started    Download the latest release on Start Bootstrap   Fork this repository on GitHub   Bugs and Issues   Have a bug or an issue with this theme   Open a new issue  here on GitHub or leave a comment on the  template overview page at Start Bootstrap     Creator   Start Bootstrap was created by and is maintained by  David Miller   Managing Partner at  Iron Summit Media Strategies       https   twitter com davidmillerskt   https   github com davidtmiller     Start Bootstrap is based on the  Bootstrap  framework created by  Mark Otto  and  Jacob Thorton     Copyright and License   Copyright 2013 2015 Iron Summit Media Strategies  LLC  Code released under the  Apache 2 0  license  Solidity Logging Library   An expensive mid level library to do loggings in smart contracts     example      javascript import  logging sol     contract MyContract is logging     function MyContract         logging log  Initiated          function logTest string test  public      logging log test     Gas Used By calling this function to write  test    65877               view logs   Logging would log in two places for now  one the variable called lastLog and the other event log   javascript MyContract lastLog call    1467913219   Initiated          event log  00000000000000000000000000000000000000000000000000000000577e9299 0000000000000000000000000000000000000000000000000000000000000040 0000000000000000000000000000000000000000000000000000000000000009 496e697469617465640000000000000000000000000000000000000000000000   timestamp    577e9299    GMT  Thu  07 Jul 2016 17 34 17 GMT     http   www epochconverter com hex message    496e697469617465640000000000000000000000000000000000000000000000     Initiated        TODO     Log Stack   string   public logStack      mapping  uint    Log    public logStack      Optimize gas usage   Support more datatypes  address  bytes  uint    maybe using a toBytes converter to write logs and a printLog   function to print logStack      FirstBlood Crowdsale Page     app  mainApp     ethereum  tokenApp    Quick start     git clone  clone this repo    npm i  and  sudo npm install   lastly  npm start  and you are ready     keystamp crypto   need heroku project access Inside project directory run   locally   export NOTARIZE PRV MAIN NOTARIZER PRIVATE KEY export blockcypher api key blockcypher api key python manage py runserver 0 0 0 0 5000   Deply      git push heroku master heroku config set NOTARIZE PRV MAIN NOTARIZER PRIVATE KEY heroku config set blockcypher api key blockcypher api key         Test local server   python test client py local   Test heroku   python test client py   API   BASE URL   https   reghackto herokuapp com   Endpoints    Hashing         Hash file with SHA256    hashme HTTP POST   file url    URL  response    status    success    hash    dbfdad915a13827c1684b39ff9875b24efaebd239f815f54e2263fbb217ad5d2      Hash string with SHA256    hashme string HTTP POST   text     lorem test text   response    status    success    hash    dbfdad915a13827c1684b39ff9875b24efaebd239f815f54e2263fbb217ad5d2         Key generation         Generate master seed for OSC  Note that this end point might take longer to response  as it generates entropy for address generation     generate master seed HTTP GET POST response   status    success    xpub    xpub661MyMwAqRbcEkr1KVuZG4s7BXbGkoSjMGEGtPjFU976HPotfmmZMsssB9q2Gt9j6d4aNAVF2vgD3GB6fcufLxSWHz7TFkjgWmEsWMyE9PF    xprv    xprv9s21ZrQH143K2GmYDUNYtvvNdVknMLisz3Jg61Kduoa7QbUk8ETJp5ZPKrHPgNTgR2uCYgeXqVFgKCZpDsPjgXQM19A7j6vKaXncY58JLi2      generate firm key using master  OSC  seed and firm id  5 digit int     generate firm key HTTP POST   osc key    OSC XPRV    firm id   12345  response    status    success    xpub   xpub   xprv   xprv  path     path      generate advisor key using firm key and advisor id  5 digit int     generate advisor key HTTP POST   firm key    FIRM XPRV    advisor id   12345  response    status    success    xpub   xpub   xprv   xprv  path     path         Notarization         notarize and save the text to the Bitcoin blockchain  text is limited to 80 characters    notarizeme HTTP POST   text   FINAL HASH TO BE SAVED TO BC   response   status   success   txid   THE TRANSACTION ID OF THE TRANSACTION CONTAINING THE HASH         Validation         retreive hash stored in txid    get hash from bc HTTP POST   txid    THE TRANSACTION ID OF THE TRANSACTION CONTAINING THE HASH   response   status   success   hash    THE HASH OF THE SAVED DOCUMENT IN BLOCKCHAIN               verify if a file  file url  and a txid has the same hash   Note  Verified is the final flag that should be checked     validate file url HTTP POST   file url    http   site com image jpg    txid    915a13827c1684b39ff9875b24efaebd239f815f54e2263fbb217ad5d   response   status  success     verified  True   txid hash   THE HASH THAT WAS SAVED TO BC   file url hash   HASH OF THE FILE    file url    http   site com image jpg    txid    915a13827c1684b39ff9875b24efaebd239f815f54e2263fbb217ad5d       Did we just cryptographically proved THE HASH THAT WAS SAVED TO BC   HASH OF THE FILE    return verified        make it do this for you         gets the final puzzle pieces and puts them together    notarize this HTTP POST   file url   http   site com contract pdf    advisor signature   SIGNATURE OF DOCUMENTHASH USER RECEIPT WITH ADVISORS PRIVATE KEY    client authorization   TWILLIO CODE OR ANY OTHER AUTHORIZATION RECEIPT   note that you can send  file hash  instead of  file url  response    status    success    keystamp   HASH OF FINAL KEY THAT IS SAVED ON BC    txid   tx hash   final key    FILE HASH POST TWILLIO CODE OR ANY OTHER AUTHORIZATION RECEIPT  encode  utf 8         snapp python api   Snapp Python API Toshiba tv RE   Toshiba TV Reverse Engineering   Firmware   Series   L621U   Model   49L621U   File name  UPDATE CASTTV UHD 01 03 40 INIT CLEAR  zip   File size  136 MB   Last Modified  16 12 2016   Download and more info  http   tvna compal toshiba com us en download  series 529 model 544 A first look at browser based Cryptojacking   This is the repository for the paper and presentation material presented at IEEE SECURITY   PRIVACY ON THE BLOCKCHAIN   IEEE S B       Paper   A first look at browser based Cryptojacking     arxiv org   Presentation   IEEE S B Slides   CryptoReceipts shayan es   my personal web site     Forked From      Leonids  is a clean Jekyll theme perfect for powering your GitHub hosted blog  smart contract sanctuary         A home for ethereum smart contracts      This repo autosubmits contracts to  4byte directory   Feel free to contribute sources      Folder         Description                                        contracts      folder structure of dumped solidity contract sources     utils          utilities for dumping smart contracts from public sources     Contracts   The folder structure contains the solidity sources   contracts json  is more or less an index with some metadata of that day the contract was dumped    Utils   Scripts for dumping smart contracts from public sources  etherscan io  etherchain com    requires    pip install pyetherchain   Update   To use  List of Verified Contract addresses with an OpenSource license   you can download the csv file  add it to the util folder  and run  parse download contracts etherscan io py   with your etherscan API   This will add the new contracts to the appropriate folder   Contribute   Feel free to contribute smart contract sources  scripts for dumping sources or your analysis results with us    Want     deduplication script  link instead of duplicate    statistics   scripts to dump more sources   code hash  without comments  maybe compile and hash bytecode to dedup sources    libsubmarine                                                                                                                                                                                                                                                                                                                                                                                                  o o                     o o                               o o                     o o                               o o                     o o                              o o                     o o                     LGB                                                                                                          A work in progress implementation of better submarine sends for Ethereum    Authors   libsumarine s development was started by the  Submarines  group at the  2018 IC3 Ethereum bootcamp     Lorenz Breidenbach   Tyler Kell   Alex Manuskin   Casey Detrio   Derek Chin   Shayan Eskandari   Stephane Gosselin   Yael Doweck   Submarine Spec     A  User   B  Commit Address  No Private key    C  Libsubmarine   D  Dapp  Application  e g Auction    MPT  Verify Merkle Proof  This probably will be included in  C       Steps             TXcommit  1  A                       B                                                     TXunlock  3                                                    v                         C                              D     TXreveal  2                  pull  isFinalize                                                   v                         MPT   A                  D       finalize 4        Order of Transactions     Commit   A       B     Reveal   A       C     Unlock   B       C     Finalize   A       D       Generate  TXunlock   A  chooses a  e g  256 bit  witness  w  uniformly at random and computes  commit   Keccak256 addr A    addr C     value   d   w   gasPrice   gasLimit      commit  also used as  sessionId  in the other  A  then generates a transaction  TXunlock  committing to data  d     javascript to  C value   value nonce  0 data  commit gasPrice   gp gasLimit  gl r  Keccak256 commit   0  s  Keccak256 commit   1  v  27    This makes TXunlock replayable across chains             Note that  TXunlock  is replay able across chains because we use the pre EIP155  v   27   We don t really care because  B  needs to be funded explicitly anyways  greatly reducing the potential for replay attacks    A  then computes  ECRECOVER Txunlock   which outputs either  B  or    Invalid signature   If the output is     A  picks a new  w  and repeats this step  Otherwise   A  now knows  B     Done in  generate commitment   Commit   A  generates  TXcommit   sending   value    unlockgas  to  B   where   unlockgas    gp   gl   Let  commitBlock  be the block in which  TXcommit  was mined  Let  commitIndex  be the index at which  TXcommit  was included in the block    Reveal   A  sends  TXreveal  to  C   containing   value    d    w    commitBlock   and  commitIndex    C  Checks for the  sessionId  and save the variables in  Sessions sessionId    validates the reveal and performs application specific logic  Reveal transaction should include a deposit  revealDeposit  which will be refunded later if  A  is honest  This is to prevent DoS and cover the Cheat Fraud Proof gas fees    C  also records the blockHash of  commitBlock  to process future fraud proofs    Unlock   A   or any other party  broadcasts  TXunlock   Upon receiving  TXunlock    C  will  finalize  the status of the session for  A   assuming that  A  performed a valid reveal     C  can associate  TXunlock  with  A  thanks to  commit  data contained in it a k a  sessionId      Finalize   A  calls the Dapp  D  to check for the finalization state  if    Not finalized  returns  False    finalized  return  True    unlockAmount    d        console  07 25 01 17 47   master    Documents Github submarines    tree       README md    That d be me     contract         LibSubmarine sol    Registry Contract         MerklePatriciaVerifier sol    MP Verifier Contract         RLP sol    RLPReader         SafeMath sol     generate commitment    Generate AddressB and UnlockTx         Go             make transaction go   Go Implementation  NOT DONE         README md    Docs          init  py         generate submarine commit py   Python Implementation  VERIFIED for 0 1 0         requirements txt     generate merkle   Generate Merkle Proof          init  py         generateProof js         generator py         test generator py     requirements txt     test    Tests  duh         test MerklePatriciaVerifier py        test ReceiverContract py        test utils py   5 directories  18 files                 LibSubmarine sol   LibSubmarine Registry    Constructor   javascript constructor uint256  revealDeposit  uint256  challengePeriod      uint256 revealDeposit    Minimum deposit require for Reveal    This is to cover costs for challenge     Cheat Fraud Proof      uint256 challengePeriod    Number of blocks to wait for possible challenge  Sessions would not be finalized in the challenge period      Reveal   Reveal the commit transaction details  javascript reveal uint256  commitBlock  uint256  commitIndex  address  dappAddress  uint256  unlockAmount      bytes  data  bytes32  witness  uint256  gasPrice  uint256  gasLimit      uint256  commitBlock   The block number transaction    A      B   was confirmed in    uint256  commitIndex   The index of the transaction within the block  a k a  Position      address  dappAddress   The address of the DApp using the libsubmarine registry  The funds will be transferred to this address after finalization    uint256  unlockAmount   unlockAmount included in the unlock transaction  TXunlock    B      C      bytes  data   DApp specific Data included in the  TXunlock     bytes32  witness   Random bytes  witness  included in  TXunlock     uint256  gasPrice   Gas Price specified for  TXunlock     uint256  gasLimit   gasLimit specified  TXunlock   Unlock   Receives the  TXunlock    B      C   and changes the state of the session accordingly   javascript unlock bytes32  sessionId      bytes32  sessionId   sessionId which is the commit message   sessionId   Keccak256 addr A    addr C     value   d   w   gasPrice   gasLimit    Finalize   Finalizes the state of the session sessionId  and releases the funds   javascript finalize bytes32  sessionId      bytes32  sessionId   sessionId   isFinalizable   View function to show if the state is finalizable      isFinalizable bytes32  sessionId      Returns     javascript  true  unlockAmount  DAppData    OR    false  0            Challenge   Anyone can challenge a reveal to prove  A  cheated and the commit transaction has not happened they way it was revealed    If proven right   A  has cheated   unLockAmount will be transferred to the user reporting the fraud    javascript challenge bytes32  sessionId  bytes  proofBlob  bytes  unsignedCommitTx      bytes32  sessionId   sessionId    bytes  proofBlob     TODO    bytes  unsignedCommitTx      TODO     Tests   Install Solc   Installation guide     Install requirements   pip3 install  r requirements txt   run the tests   python3 test test ReceiverContract py     TODO     Explain fraud proofs   Explain how to deal with reordering attacks  e g  maliciously inserting  TXunlock  in front of  TXreveal    Statemachine with  locked  unlocked  x  unknown  revealed     Prove that you are correct  Proof of Correctness  and no need to wait for the challenge period to end    Better start end blocks for states  reveal challenge    Move test generator py to test folder   clean up the Tests       Disclaimer   This project is a Work in Progress    IC3 Ethereum Bootcamp  3 2018 hethfinder   Installation   pip install  r requirements txt Pyfaast   This is a python client for  Faa st  http   faa st    API    API Documentaion   api faa st   Please see the  example py  for more information on how to use    Installation   BASH python setup py install   Usage      PYTHON from pyfaast import Faast   faastObj   Faast       Faast TESTNET   TRUE   for testnet         Get Supported Currencies   PYTHON faastObj get supported currencies     Response       JSON          cmcID    ethereum     decimals   18    deposit   True    iconUrl    https   testapi faa st api v1 public static img coins icon ETH png     infoUrl    https   ethereum org     name    Ethereum     receive   True    symbol    ETH     walletUrl    https   faa st            Number of supported tokens  pairs       391   Sep 13 2018         Get pair price   PYTHON faastObj get pair price pair    BTC ETH     Response     JSON        pair    BTC ETH        price   0 03002374       deposit amount   0       minimum deposit   0 00017925           Trading   Create a Swap   PYTHON swapObj   faastObj create a swap swap pair    BTC ETH                         withdrawal address    0x08d62881d04f62a02ee80f45abf454f418c60e99                         refund address   None                        deposit amount   0                        user id                             affiliate margin   5                        affiliate payment address    0x08d62881d04f62a02ee80f45abf454f418c60e99                        Only swap pair and withdrawal address are mandatory   Response       JSON   affiliate margin   5    affiliate payment address    1fs4Vz12WGBgPe6LmE2TDnGeuAjFhws6k     created at    2018 09 13T08 27 19 865Z     deposit address    2N44At9pemAX3mXwXV86fdXsPRaoKYt5jqS     deposit currency    BTC     refund address    0x08d62881d04f62a02ee80f45abf454f418c60e99     status    awaiting deposit     swap id    df53cda2 ac44 4ed9 9439 1b7586bb3879     user id         withdrawal address    0x08d62881d04f62a02ee80f45abf454f418c60e99     withdrawal currency    ETH           TODO      implement    QUERY SWAPS BY WITHDRAWAL ADDRESS OR USER ID   FETCH REFRESH A SWAP         Testing   TESTS NOT IMPLEMENTED YET   install the needed test deps       pip install vcrpy nose coverage    Now we we can run the tests       nosetests pyfaast   with coverage   cover package pyfaast  Simple Auction   This is a template to run a simple auction on Ethereum Blockchain with web interface on github io Crop Insurance on Blockchain   A 2019  initc3  bootcamp project  built on Ethereum     Slides   https   docs google com presentation d 1esDRIJ6rhfc185wXPz6Q08MYKb9XoIUcYrCvQlrSdNI edit usp sharing   Live Demo   https   shayanb github io indemnity index html   Overview   The following documentation outlines an MVP for a peer to peer crop insurance ecosystem  powered by Ethereum  The objective of this MVP is to drive a holistically enhanced customer experience by enabling direct connectivity between demand side  Insurance Buyers  and supply side  Insurance Providers  participants and ensuring guaranteed  timely payouts of premiums and policies when applciable      Additionally  this MVP serves to demonstrate the coexistence of a robust  seamless secondary trading marketplace for digital insurance contracts  and the benefits of facilitating that marketplace on the same rails utilized for the primary creation process     Scope of MVP        Policy Creation  An Insurance Buyer submits an initial request for a new insurance policy that includes all desired terms  Insurance Providers view the requests and determine if they choose to accept the requests         Claim Submission  Insurance Buyers submit off chain claims to the chosen Oracle  requesting a determination that total crop failure has occurred prior to policy expiry  Oracles either reject the claim  or provide the Insurance Buyer with a signed message  indicating that total crop failure has occured         Policy Expiry         Trading         This MVP does not yet address  plot and land registries  bidding  matching  partial crop failures  premium   payout reinvestment options  market making  among other functionalities  See below for further discussion  Payments are currently made in ETH  however  a future state version may require the use of a more stable currency  or at least the ability to specify alternatives     Actors       Insurance Buyers   Farmers and landowners looking to purchase crop insurance to protect against the event of failed harvests     Note  farmers could independently insure their crops through this market  or pool assets to unlock lower premiums          Insurance Providers   Independent actuaries and smaller insurance companies who can provide coverage to Insurance Buyers in the event of crop failure and generate revenue from premiums in the absence of crop failure  Their decision to engage in the market is typically based on an assesment of risk factors  an analysis process that s typically unique to each provider         Oracles   trusted data sources that can verify the outcome of whether or not a crop has failed     Note  Oracles are not required to directly participate in the network within this MVP   Potential oracles     Satellite data   Trusted evaluation firms   Sensors             Insurance Traders   Insurance Providers can be Insurance Traders     Motivations for trading can be varied  most notably including  diversification of risk  definite profit opportunities             Workflow   1  Policy Creation   Actors  Insurance Buyer  Insurance Trader   Conditions   need to fill in   Process      A Farmer will create a proposal for insuring a plot of their land  then submit to the Insurance Provider marketplace  The proposal contains the following terms       Plot ID   one plot ID per proposal  Specified within plot registry  Total Size  km2    total area covered by all plots included within contract    Start Date   when the policy would start   End Date   when the policy would expire   Premium  ETH   km2    amount per km2 paid by the farmer for the insurance protection in the event that crop failure does not occur    Payout  ETH   km2    amount per km2 of protection that the farmer seeks and would be guaranteed by an insurance provider in the event of crop failure    Approved Oracle s    approved data provider s  who will be ultimately responsible for verifying any claims of failure up till and including expiry  corresponds to a whitelist of approved and trusted public keys      At submission  the farmer also transfers the total premium amount  ETH  to the proposal    Before the submitted proposal is confirmed  there is a check to ensure that the submitter  the Farmer  is the listed owner of that plot   The premium sits within the proposal and away from the farmer  but remains unlocked  farmer is able to withdraw   and the terms of the proposal can be modified accordingly  until the proposal is accepted by an Insurance Provider     An Insurance Provider will be able to view the proposal and choose to accept  At acceptance  the Insurance Provider also commits  sends  the total payout amount     to the proposal     Once accepted  and the funds from the Insurance Provider are deposited  the proposal is complete and is a fully formed contract or policy     2  Claim Submission   Actors  Insurance Buyer  Oracle   Conditions     Need to complete    Insurance contract is still valid   Current date must be equal to or less than date of expiry    Process    The Farmer will contact the Oracle  indicating they are raising an insurance claim  and request for a verification of the state of the Plot s   This communication will take place off chain     The Oracle will address the following question  has total crop failure occurred for the total plot within this contract  This determination  i e  analysis of data  will take place off chain         If NO  total crop failure has not occurred   the workflow is complete         If YES  total crop failure has occurred   The Oracle will then sign a transaction indicating Yes  total crop failure has occurred  with their trusted private key  This transaction will be submitted to the network by the farmer             There will be an automated verification process  confirming that the signature provided by the Oracle does  indeed  match one of the pre specified trusted public keys  a determination made originally within the Farmer s proposal   This will occur on chain             Total payout and total premium are released to the Farmer           The Insurance Policy is no longer valid        3  Expiry   Actors  Insurance Provider   Need to complete   Conditions     Current date is after expiry date   Policy ID exists    Requester is a listed insurance provider with stake in the policy  their stake still exists   No claims have been made on this policy   Message sender has not previously withdrawn funds from this contract   After expiry date  if no claims have been made and both payout and premium exist within the contract  the Insurance Provider will be able to withdraw funds from the contract  The Insurance Provider will submit a transaction requesting this withdrawal  and if all conditions are met  both premium and payout is sent to the Insurance Provider  in accordance with their ownership stake in thep policy      4  Trading   Actors  Insurance Provider  Insurance Trader   Need to edit  Conditions   Insurance contract is still valid Current date must be equal to or less than date of expiry   Workflow  The Insurance Provider  Existing Holder  seeks to exit their total position or percentage of their position held within an Insurance Policy     The Insurance Provider  Existing Holder  will create a request to sell  specifying     Change in Payout  ETH    This is effectively the price of the trade  Trade settlement completes once the Insurance Trader deposits this amount into the Insurance Policy  and the same amount is withdrawn and transferred to the Insurance Provider  decreasing their liability position within the policy in the event of crop failure  This number must be equal to or less than the total payout amount of the policy      Change in Premium  ETH    This amount corresponds to the change in ownership stake that the Insurance Provider has in the total premium  After trade settlement  this amount would be owed to the Insurance Trader instead of the Insurance Provider in the event crop failure       The Insurance Provider would then submit the request to the marketplace     The Insurance Trader  another Insurance provider  who seeks to enter into or expand position within the same Insurance Policy would accept the terms of the trade  In doing so  the Insurance Trader must send the total amount corresponding to the price of the transfer  ETH  to the Insurance Contract  That same amount is then released from the contract and sent to the Insurance Provider  The ownership positions are updated accordingly  The Insurance Trader is now a listed Insurance Provider within the policy     Future State     Plot Registries   Bidding   Insurance Provider proposals   Matching engine   Partial crop failures   Exchange and market making tools   Premium and payout reinvestment options   More flexibility with Oracles  i e  approval rules   k of N Oracle approvals  etc        Security Consideration     Oracles    Double Indemnity   Payout safety     API Design   Data Structures   struct plot       uint plotId      address owner      struct insuranceRequest       uint plotId      uint startDate      uint endDate      uint premium      unit coverRequired      struct policy       uint policyId      mapping address    uint  collateralLiabilities      mapping address    uint  premiumDividends      mapping address    boolean  premiumDividendPayouts      uint startDate      uint endDate      struct tranche       address seller      uint collateralLiabilityChange      boolean sold      public plot   plots  public insuranceRequest   insuranceRequests  public policy   policies  public tranche   tranches    Functions     submitInsuranceRequest     plotId  startDate  endDate  premium  coverRequired     require plots plotId     0    require msg sender    plots plotId  owner    require msg value    premium    require premium   coverRequired    require startDate   now    require endDate   startDate    require endDate   startDate   365 days          cancelInsuranceRequest       insuranceRequestId     require insuranceRequests insuranceRequestId     0    require policies insuranceRequestId     0    require msg sender    plots plotId  owner          submitClaim       policyId  claimAmount     require policies policyId     0    require msg sender    policies policyId  insuredParty    require policies policyId  endDate   now    require verifiers ecrecover hash  v  r  s      0    require policies policyId  claimAmount    policies policyId  totalCollateral          provideCover       insuranceRequestId     require insuranceRequests insuranceRequestId     0    require msg value    insuranceRequests insuranceRequestId  coverRequired    require msg sender    insuranceRequests insuranceRequestId  insuredParty          sellPolicyPremiumDividend       collateralLiabilityChange  premiumDividendChange  policyId     require policies policyId     0    require policies policyId  collateralLiabilities msg sender     0    require policies policyId  premiumDividends msg sender     0    require policies policyId  collateralLiabilities msg sender    collateralLiabilityChange    require policies policyId  premiumDividends msg sender    premiumDividendChange          cancelPolicyPremiumDividendSale       trancheId     require msg sender    tranches trancheId  seller    require tranches trancheId  sold    true          buyPolicyPremiumDividend       tracheId     require tranches trancheId     0    require tranches trancheId  sold    true    require msg value    tranches tranchId  collateralLiabilityChange    require msg sender    tranches trancheId  seller          requestPolicyPremiumPayout       policyId     require policies policyId     0    require policies policyId  endDate   now    require policies policyId  collateralLiabilities msg sender     0    require policies policyId  premiumDividends msg sender     0    require policies policyId  premiumDividendPayouts msg sender     true          Diagrams            This is the web site at  https   chia network    Pull requests accepted    If you have ideas for the FAQ  check out  https   github com Chia Network website tree master webroot faq   You can see this web site locally with python 2 as follows    bash       cd webroot       python  m SimpleHTTPServer   or with python 3    bash       cd webroot       python3  m http server   Then browse to http   127 0 0 1 8000    Translations   This site uses  l20n js  v5 0 to translate static html content  Please take a moment to familiarize yourself with its rules and syntax before adding content or translations    Guidelines for new content in existing files     All text section elements must include the  data l10n id  text id   attribute      html      span data l10n id  new text section  This is a new section of text  span        Before adding new translation entries  make sure one does not already exist by checking the  app en US ftl       Add new text entries and english translations to  app en US ftl       Add the same entries to the remaining  translation files       Guidelines for new html files     All html files must include the following in the  head  section      html      meta name  defaultLanguage  content  en US        meta name  availableLanguages  content  en US  fr  ja  nl  de  es  sr  pt BR  tr        link rel  localization  href    locales app  locale  ftl        script defer src    js l20n min js    script    Guidelines for adding languages   For html     Add the two character locale code to the following meta element in all existing html files     html      meta name  availableLanguages  content  en US  fr  ja  nl  de  es  sr  pt BR  tr       Add the translated name of the language to the dictionary at the top of  locales js     javascript var names         en US    English       fr    Fran ais       ja              nl    Nederlands       de    Deutsch       es    Espa ol       sr    Srpski       pt BR    Portugu s       tr    T rk         Copy  locales app en US ftl  and rename it with the locale code being added    Translate away      For the FAQ     Translations of the FAQ are kept in markdown  and don t use l20n   Copy  faq md  to the  translated markdown files  folder  changing its name to  faq  2 letter locale code  md   Translate away    DlAGRAMS   A collection of icons to be used in Draw io  http   app diagrams net  containing      Popular Cryptocurrencies Icons   Wallets   Applications   Decentralized Finance  DeFi      Usage       Use in the Browser   draw io       Import in your Browser Instance  Go to  file       Open Library From       URL   and paste this address  https   dlagrams net DeFi xml       On native apps  Clone this repo  and then import  Library name xml   e g   defi xml   by going to  file       Open Library       Still need help   Work with custom shape libraries   Screenshot                                                                                                                                                                                                                                                                             Instant retro UI for calling any contract function you want   See it online    eth95 dev  here s an example with DAI    https   eth95 dev  network 1 address 0x6b175474e89094c44da98b954eedeac495271d0f   Direct Link   Features       Call any contract function as long as you have the ABI     Connect via localhost 8545  MetaMask  or a custom node URL     Watches your artifacts folder and automatically updates the UI     Encode your calls for a proxy to call on your behalf      Set a custom signer or a custom contract address     Built in log for easy visibility     Install   Works on any dapp project  Truffle  Buidler  etc   as long as you point it to the JSON artifacts  And even if you don t have a project  you can run it by itself and manually add ABIs and artifacts        Install   shell npm install  g eth95       Run with path to your artifacts folder    shell eth95   build contracts       How it works   When  eth95  is run  an Express server is fired up and a frontend  packaged by  Parcel   is served at  localhost 3000   you can define the port with a flag   p 1234    The server will watch the directory you passed in for any changes to your artifacts and send those changes down to the frontend via Websockets    Note that you can also add any contracts you want if you have the ABI or Artifact  there is an Add Contract button     Contact   If you have any questions or comments  please file an issue
18,SimonVilleneuve,simonvilleneuve github io
19,nicholas,pip install tensorflowjs  1 7 4   tensorflowjs converter       input format tf saved model       output format tfjs graph model       weight shard size bytes 10485760       skip op check     assets exported container     assets converted tfjs
20,devottam,Redditext   Extract text from URLs for reading purposes  Right now  it only gets the text from  Reddit   Future plan is to extend it to be more friendlier to any URLs given    Plans         Extend it such that it can take any URL       If the article is good and want to send to kindle  have the interface for that
21,skytreader,This is just a test GitHub project for me  Trying to learn both CodeIgniter and git    I intend to use this personally and  as such  my branch of development will not focus on things which an actual library might need such as keeping tab on library members and borrowed books  For the most part  it is just an interface to a book database I designed  PyGame Objects     Want a sample on how to work with this framework  Check out the docs in the  wiki   particularly the  Walkthrough     The most awesome framework for PyGame you will ever encounter       But  it s not there yet  Right now  it s just a simple abstraction for some commmon code patterns I find while using PyGame    Built on   see   travis yml  as well as  requirements txt   Needs Python 3 7  as this relies on language features only available from there    Development   Aside from   travis yml   the Dockerfile is provided for development  The Dockerfile takes an argument  userid  which should be a user id outside the container that has access to   tmp  X11 unix   If you are in a graphical desktop environment  it would suffice to pass the   UID  environment variable like so    docker build  t pygame objects   build arg userid  UID      You can also pull the image via   docker run skytreader pygame objects latest    At this point  you only have the image  To develop and run games with it  you should use the provided  duckrunner  script  It takes the package path to the script that invokes your game loop  For example  to run the included snake game  call  duckrunner demo snake game     Current Status   Right now  I m working on adding extra game making functionalities  stuff that will maybe come useful if you do an RPG  platformer  arcade  etc   type of game  I m also working on adding new native drawing functionalities    File Organization   components  houses the main framework   sample sprites  contains the sprites I used for the test and demo files   tests  contains  well  tests    Since I tried to follow PEP 328  it may not be that straightforward to run the tests  For convenience  navigate to the  runscripts  directory  pick the test  as directories  you would like to run  and execute the runscripts from there    The tests were written after every feature I finished  The tests are a mix of demos and unit tests  Every now and then  I also write some mini games to compile demonstrations of some feature I m working on    Git Organization   Documentation is at the wiki  My todo list I made as issues  I am currently working to make the documentation comprehensive  no more peeking at the framework code    However  something I was not able to forsee is that  the documentation for Milestone 1 and for the code at the repo head has been mixed at the wiki  Fortunately  they are not yet that different from each other and when and where they differ  it is easy to note  In the near future  I plan to include markdown files of the docs along with the code    License   All code in master  except when noted  are licensed under MIT    All sprites under  sample sprites tiles  are licensed under a  Creative Commons Attribution ShareAlike 4 0  license  About   A little app for automatically generating exchange gift pairings  The gift giving graph produced will be acyclic  i e   no two persons ever will be assigned to each other   Not yet very abstracted  it expects that an  admin  will enter the names of those invited into the database    Future   I intend this to be a Facebook app  create an event  run the app  get your pairings   But while I am not yet very proficient with the Graph API  we have this   D The Large Hadron Colliders   presents   xkcd hats dragons   My  unfinished  work at the first ever Python Philippines  Hackathon for Creative Pythonistas    Dodge the dragons  Get to the Large Hadron Colliders  Summon the God Particle and survive     No event handling yet though  Awww
22,squiddy,Gerrard   Styleguide generator   Generates a styleguide from your stylesheets  Documentation format is based on  Knyle Style Sheets   but adds a section to specify a HTML template to use when generating the guide  louie   gamepad browser control   Think Steam  Big Picture  mode control in your browser    Why  I liked the idea behind the controls  Doesn t matter if it ends up being useful or not  Take this as a proof of concept  it won t be able to hold up against Steams optimized browser    Features  WIP      Chrome Chromium   Support   Zoom   Moving around page   Highlighting and clicking links   basic Gamepad API support     TODO   Client     implement daisywheel   vimium link navigation as alternative to clicking links   kinetic scrolling   request fullscreen   keep zoom level consistent across pages     Browser Extension     enable disable globally   Vice City MP3 Control   Mod for GTA Vice City to take control of the mp3 radio station   This project is unmaintained   Really crappy video without sound   git rs     A git reader implemented in Rust  The primary goal here is to learn Rust  so this won t even be near feature complete    Development   Running tests   cargo test   Run clippy checks   rustup run nightly cargo clippy   License   git rs is released under the  MIT License   vscode xcolor       Pick any color on the screen and have the hexadecimal representation insert into the currently active editor  This is a thin wrapper around the  xcolor  program    Features     Usage   Once you have an editor open  you can trigger the  Pick screen color  command  You should see a small preview window attached to your cursor  once you selected a color  the hexcode will be inserted at the current position of the cursor    Requirements   This extension requires you to have  xcolor  installed globally  See https   github com Soft xcolor for installation instructions    Note that  xcolor  only supports linux systems running X11  gitlab ci status  a live overview of CI pipelines     Note  This is a pet project of mine  so I might try different things from time to time      Motivation   When working together with multiple colleagues  sharing a single gitlab runner  tool that runs your pipeline jobs  quickly led to some problems      my build is stuck  what is the runner doing    is my build done      Especially stuck builds were annoying because we have to navigate through all CI enabled projects on gitlab and check the pipeline status    Some problems go away after you add concurrency   more runners  nevertheless having a overview what s happening right now is useful sometimes    How does it work    For each project you want to see in the overview  a new webhook has to be set up that points to the instance of gitlab ci status  Updates are send for pipeline and job events  A server persists the state on disk and the client periodically fetches the state and renders it using React    Installation   TODO   Ideas     utilize SSE  server side events  to only send updates to clients  not the   whole state   advent of code 2018   My solutions to puzzles in this year s Advent Of Code  https   adventofcode com 2018   This year I m probably going with at least Go  Nim and Rust  Let s see how that works out     runmap   Create a map of runs from GPX files  Used on my homepage      Usage   go build  o runmap   runmap   imageSize 1000 path to directory with gpx files    Output will be written as  output png  in the current working directory  polar flow exporter   Downloads GPX data of  optionally filtered  trainings from Polar Flow  grafana image panel   Shows and refreshes an image  That s it    We have a slackbot running at work that accepts image URLs and we show the current image on one of our grafana dashboards    Right now it s about 5 lines of plain JavaScript to embed the image and refresh it from time to time  This repository is an attempt to complicate things    or rather learn a thing or two about grafana plugins        Developed against Grafana 6 0 x   Screenshots       Setup   Install dependencies      yarn   Make a production build      yarn build   Continously build the plugin      yarn watch   Development   I m using docker to run grafana and point it s plugin directory to this repository      docker run  p 3000 3000  v   pwd   plugins   e GF PATHS PLUGINS   plugins   grafana grafana 6 0 2   Open  localhost 3000  in your browser and add the image panel  Homer    Homer is a  statsd  compatible stats aggregator written in Rust  I m basing this implementation on github s statsd implementation  brubeck     Why   To write something in Rust  I ve been reading statsd related code lately  especially a C implementation  and I d like to see whether I can make it shorter and easier to understand for my usecase    What s working      x  configuration loading       statsd    x  receive packets via UDP    x  basic packet parsing  no support for sampling rates        performance  recvmmsg  multi thread            aggregation    x  counter       gauge       timer       sets  maybe later            carbon    x  plain text protocol    x  periodic flushing       pickle protocol  maybe later            logging       proper error handling     Setup   Get the nightly rust compiler in version 1 36 0 and run  cargo run   Configuration is possible by editing the  config toml  in the working directory  wcolor     WORK IN PROGRESS    Simple color picker for Wayland  Pick any color on the screen using the mouse  This is basically the Wayland aquivalent to  xcolor     Usage   Run  wcolor  and select a color  Its hexadecimal RGB representation will be printed to standard output    Screenshot     Setup     zig build   zig build run     References     https   github com ifreund zig wayland   https   github com Soft xcolor  the X11 version    https   github com emersion grim   https   github com emersion slurp  consulting the source of grim and slurp helped a lot in understanding Wayland    Annotations for grafana from gitlab deployments       Provides a grafana datasource to get annotations for each deployment in gitlab  Useful to see whether code you deployed had any impact      Usage   shell   go build   export GITLAB URL https   gitlab example com   export GITLAB TOKEN apitoken    export HTTP ADDRESS  8080     gitlab deployment annotations   Log into grafana   http   localhost 3000   and add a SimpleJSON datasource and point it to your datasource  e g   http   localhost 8080   Add annotations to a dashboard using your datasource and configure the query  which accepts JSON   which allows you to select the project you want to display  for example    json   project id   16   environment    Live     Development     Build a grafana container image with the simple json datasource plugin preinstalled using this Dockerfile      Dockerfile FROM grafana grafana RUN grafana cli   pluginsDir   GF PATHS PLUGINS  plugins install grafana simple json datasource     docker build  t gf       Run a local instance of grafana       docker run   net host gf     Run and configure the datasource  see usage    Personal CLI for GitLab     Provides some shortcuts and workflows when working with GitLab at work  Totally not inspired by https   github com cli cli  libcst easy matcher   An experiment to create matchers from source code when working with  LibCST   A matcher enables you to check whether a certain node in the tree generated by LibCST matches a certain pattern    Status   python a   3 a           3 b   bar x 3  b   bar x     b      x 3    See  tests test libcst easy matcher py  for all examples    Motivation   If I want to check that a statement matches    python client  cookies     get cookie request    I d write something like this    python m Assign      targets           m AssignTarget              target m Subscript                  value m Attribute                      value m Name  client                        attr m Name  cookies                                                         value m Call          func m Name              value  get cookie                     args               m Arg                  value m Name                      value  request                                                      I m experimenting here to instead do it like this     python create matcher  client  cookies     get cookie request      Placeholders   Just matching against concrete values only get s you so far  so I m evaluating the use of placeholders like this    python      get cookie request  client       get cookie request  client  cookies        request  client  cookies     get cookie       All of which should match the example code       might not be the best choice in the long run  but I needed a valid identifier to be able to still parse the code with LibCST    License   MIT swaylock configurator   Visualize changes to  swaylock s  configuration      Does not support all configuration options  mostly text related settings are missing     Live Demo   Screenshot   pytest overwatch     pytest overwatch  is a Jest inspired interactive test runner plugin for pytest  It reruns tests whenever files change and allows you to select a subset of tests to run    Work in progress    Features      x  re run tests on file change    x  select subset of tests based on filename       select subset of tests based on test name    x  support dropping into debugger on test failure     In action     Motivation   pytest  is my go to test runner for python projects and I use it heavily at work  I usually use the    looponfailure  feature of the  pytest xdist  plugin on the side  however having worked quite some time with Jest in the javascript world  I was missing two things      ability to rerun all the selected tests constantly   instead of just the   failed ones   to discover potential new failures   running a subset of all tests easily     Related projects     https   github com pytest dev pytest xdist   https   github com joeyespo pytest watch   https   github com facebook jest   oog   Simple android app showing the front camera in full screen  I m using this in combination with  scrcpy  as a webcam    Don t read this is you ever wrote an android app  I m probably violating  all  best practices here  but I kept it simple to understand it  Similar example apps introduced things like fragments and navigation right away  and I don t understand those yet    This is also really hardcoded to work with my smartphone  gitlab to sqlite     Save data from GitLab to a SQLite database    Attribution   The overall structure and CLI is taken from https   github com dogsheep github to sqlite       How to install   Authentication   Using custom gitlab instance   Fetching projects   Fetching pipelines     How to install     pip install gitlab to sqlite    Authentication   Create a GitLab personal access token  https   gitlab com   profile personal access tokens   Run this command and paste in your new token      gitlab to sqlite auth    This will create a file called auth json in your current directory containing the required value  To save the file at a different path or filename  use the   auth myauth json option    As an alternative to using an auth json file you can add your access token to an environment variable called GITLAB TOKEN    Using custom gitlab instance   When running  auth  you may specify an optional    host  parameter pointing to a custom instance      gitlab to sqlite auth   host gitlab internal    Fetching projects   The  projects  command retrieves a single project      gitlab to sqlite projects gitlab db group project name    Fetching pipelines   The  pipelines  command retrieves updated or created pipelines with their corresponding jobs      gitlab to sqlite pipelines gitlab db group project name    This command can be run regularly  Based on the most recent created or updated pipeline it only fetches changes that happened afterwards  swc plugin glob import
23,tboquet,configs   dotfiles configurations okeydockey   Chains of Dockerfiles for ML and stats research    Requirements and installation     nvidia docker   one or more NVIDIA gpu s  with cuda compute capabilities   3 0   CUDA driver installed on the Host OS     For more information about the nvidia docker tool  please take a look at the  requirement  and the  installation  steps in the  nvidia docker wiki     Specific example    If you want to use your GPU 0 and GPU 1  as listed by nvidia smi   be able to serve an ipython notebook via the port 8888 and mount a volume where some notebooks are located you could use   NV GPU  0 1  nvidia docker run  it  p 8888 8888  v   notebooks  notebooks tboquet nameoftherepo presentations   My presentation repo keras contrib   Keras community contributions     This library is the official extension repository for the python deep learning library  Keras   It contains additional layers  activations  loss functions  optimizers  etc  which are not yet available within Keras itself  All of these additional modules can be used in conjunction with core Keras models and modules    As the community contributions in Keras Contrib are tested  used  validated  and their utility proven  they may be integrated into the Keras core repository  In the interest of keeping Keras succinct  clean  and powerfully simple  only the most useful contributions make it into Keras  This contribution repository is both the proving ground for new functionality  and the archive for functionality that  while useful  may not fit well into the Keras paradigm      Installation   For instructions on how to install Keras  see https   keras io  installation   shell git clone https   www github com farizrahman4u keras contrib git cd keras contrib python setup py install   Alternatively  using pip    shell sudo pip install git https   www github com farizrahman4u keras contrib git   For contributor guidelines see  CONTRIBUTING md     Example Usage   Modules from the Keras Contrib library are used in the same way as modules within Keras itself       python from keras models import Sequential from keras layers import Dense import numpy as np   I wish Keras had the Parametric Exponential Linear activation     Oh  wait      from keras contrib layers advanced activations import PELU   Create the Keras model  including the PELU advanced activation   model   Sequential   model add Dense 100  input shape  10     model add PELU      Compile and fit on random data   model compile loss  mse   optimizer  adam   model fit x np random random  100  10    y np random random  100  100    nb epoch 5  verbose 0    Save our model   model save  example h5    python from keras models import load model from keras contrib layers advanced activations import PELU   Load our model   model   load model  example h5         A Common  Gotcha    As Keras Contrib is external to the Keras core  loading a model requires a bit more work  While a pure Keras model is loadable with nothing more than an import of  keras models load model   a model which contains a contributed module requires an additional import of  keras contrib        python   Required  as usual   from keras models import load model   Recommended method  requires knowledge of the underlying architecture of the model   from keras contrib layers advanced activations import PELU   Not recommended  however this will correctly find the necessary contrib modules   from keras contrib import     Load our model   model   load model  example h5       deep learning models   A repository holding weights of trained models and entire models
24,cameronbriar,Quotes   A place to store my quotes    Information   It s currently done in PHP with MySQL    My goal is to redo this project in other fun ways    Demo   http   cloudedbox com quote  FALT   Fresno Audiovisual Lexicon Tool   FALT is a tool to help investigate the similarities between audio alone communication and visual alone communication       To install  Debian       Install required Python modules  pip install django  1 4 2 pip install python Levenshtein    Get the FALT project git clone https   github com cameronbriar FALT git cd FALT    Fix  UPDATE ROOT DIRECTORY  variable in  FALT settings py  based on project directory location  e g    home user FALT      Run the server python manage py runserver 0 0 0 0 8000    Visit http   localhost 8000 in a browser   Click  About  to learn more  the way it s put together House Issue Tracker   My new house s todo txt   CLI Interface   Create   To create an issue from the command line  simply    curl   user  cameronbriar password    request POST   data    title     Shampoo the carpets    body     Rent a RugDoctor and clean the filthy  blue carpets    labels      interior    contingent      https   api github com repos cameronbriar house issues    Request   To retrieve an issue from the command line  simply    curl   user  cameronbriar password    request GET https   api github com repos cameronbriar house issues  Python and Curses   import curses  Details   iPython Notebook       ANSIBLE                       oo                                                      Javascript for Beginners   Geekwise  2016    Outline       week 1      Intro to JavaScript  the tools  and the DOM         day 1         day 2     week 2      Syntax  Variables  Conditions  Functions  and For Loops         day 3         day 4     week 3      Functions  For Loops  Events  and  The Grid          day 5         day 6     week 4      TBD         day 7         day 8     week 5      TBD         day 10         day 9     week 6      TBD         day 11         day 12    Instructors    thomas  cameron    2016  Hello     ReviewNinja Slack Bot   A simple Slack Bot for  ReviewNinja   Intended to be deployed along side ReviewNinja to faciliate notification via  Slack     Credits      SAP  https   raw githubusercontent com reviewninja review ninja master sap logo png      heart  from the github team   sap Available Soon   My Online R sum  Adventures w Angular 2     Angular 2  ng2    AWS   Docker   Continuous Delivery   etc
25,andyone,dotfiles   A set of zsh  git  tig  and tmux configuration files    Installation   bash   curl  fsSL https   andy one dotfiles install sh    Extra commands     Name   Description                             tx    Start or attach to tmux session      sshk     ssh  command without checking and saving host key      scpk     scp  command without checking and saving host key      g     grep  shortcut      e    Editor shortcut      d    Docker shortcut      dr    Docker  run  shortcut      de    Docker  exec  shortcut      hf     grep  over zsh history      txc    Rename current tmux window to short path to current directory      goc    Create HTML coverage report for Go sources      gcl    Clone repository with Go sources      bkp    Create backup for file or directory      git release  version     Add signed version tag for the latest commit to the master branch      git tag delete  tag     Delete tag everywhere      git tag update  tag     Update tag to the latest commit      git pr  pr     Fetch PR with given ID from GitHub      git undo    Undo previous commit     Git aliases     Alias   Original                               st      status         ci      commit         br      branch         co      checkout       df      diff           dfi     icdiff         lg      log           Tmux cheatsheet     Shortcut   Action                            CTRL B    Prefix key      CTRL B     T    Show current time      CTRL B          Create new window      CTRL B     R    Rearrage windows      CTRL B     W    List windows      CTRL B     S    Show current tmux sessions      CTRL B           Split window vertically      CTRL B          Split window horizontaly      CTRL B          Set window name      CTRL B     N    Next window      CTRL B     P    Previous window      CTRL B     A    Toggle panes syncing      CTRL        Move current window to the left   reorder windows        CTRL        Move current window to the right   reorder windows        CTRL B     Q    Show pane numbers      CTRL B     X    Kill pane      ALT        Select pane on the left      ALT        Select pane on the right      ALT        Select upper pane      ALT        Select bottom pane      CTRL B     Space    Set panes layout      CTRL B     PgUp    Scroll up      CTRL B     PgDn    Scroll down      F1    Select window  1      F2    Select window  2      F3    Select window  3      F4    Select window  4      F5    Select window  5      F6    Select window  6      F7    Select window  7      F8    Select window  8      F9    Select window  9      F10    Select window  10      F11    Select window  11      F12    Kill current window     For function keys support in XShell 5  you should use custom  mappings file                       Installation     Usage example     Build Status     License     redy  is a tiny Redis client based on  radix v2  code base    Installation   Make sure you have a working Go 1 17  workspace   instructions    then    go get github com essentialkaos redy v4   For update to latest stable release  do    go get  u github com essentialkaos redy v4   Usage example      go package main   import      fmt     time     github com essentialkaos redy v4      func main       rc    redy Client      Network       tcp       Addr          127 0 0 1 6379       DialTimeout  15   time Second        err    rc Connect     if err    nil       fmt Printf  Connection error   v n   err      return       r    rc Cmd  SET    ABC   1    if r Err    nil       fmt Printf  Command error   v n   r Err      return       r   rc Cmd  GET    ABC     if r Err    nil       fmt Printf  Command error   v n   r Err      return       val  err    r Int     if err    nil       fmt Printf  Parsing error   v n   err      return       fmt Printf  ABC     d n   val          Build Status     Branch       Status                              master           develop          License   MIT     Hi  My name is Anton  and I do some technical stuff  I develop tools and services with Go  C  Python  and Bash  Also  I have good experience in writing RPM specs and building packages       Some of my projects                                      More projects made by me can be found on the   essentialkaos  organization page      Twitter     Instagram     Dribble     Flickr   sublime settings   This repository contains configuration files for  Sublime Text 4       win preferences json    Main Sublime Text configuration for Windows   win LSP sublime settings     LSP  configuration for Go on Windows   mac preferences json    Main Sublime Text configuration for macOS   mac LSP sublime settings     LSP  configuration for Go on macOS   action info   This is a very simple project for printing info about the GitHub Actions runners environment
26,jakerockland,Flock   Personalized suggestions of who to follow on Twitter  a Hack Arizona 2015 project  all about the benjamins   Stock predictions  made better    Project Abandoned    jakerockland com   Personal website for me  Jake Rockland  azduievals   Website for AZDUIEVALS  a Southern Arizona company providing a new mobile service for DUI MVD revocation evaluations  scrapbook     A Flask web application for scrapbooking the 21st century    Screenshot   tictactoe   tic tac toe machine learning experiment find s   Implementation of Find S Algorithm as Machine Learning Exercise data structure essentials   Implementation of the basics of algorithms and data structures including searching  sorting  lists  stacks  queues  hash tables  trees  and graphs  regex fun   Practice exercises using regular expressions  sandbox   Programming sandbox for miscellaneous programming exercises  learn go   Practice sandbox for learning Go language  aNonreport   A distributed blockchain supported human rights violation reporting system    Developed initially as a HackUPC 2016 Project    Inspiration   Technologists and human rights advocates are often kept isolated from each other  speaking languages not easily understood by those inhabiting the other camp  Merging innovative technologies to facilitate advocacy for human rights seems imperative  yet is seldom practiced    aNonreport is the result of conversations between several programmers  Jack Rockland   Jack Kleeman  and a human security researcher with experience in violent conflicts  Galen Englund  Through further consultations with experts from the International Crisis Group  Organization for Security and Cooperation in Europe  OSCE   and the UNHCR  they realized a gap in current human rights reporting technologies that could be improved through distributed blockchain based programming  Joined at hackUPC by Suraj Shetty  aNonreport is a hackathon project inspired by real world needs    Disseminating information about human rights violations is easier than ever  but tracking and acting upon the vast quantities of data available has become increasingly difficult  Simultaneously  people who do report violations risk persecution  Existing anonymous solutions  such as SecureDrop  are centralized  relying on servers that are both expensive to implement and vulnerable to attack  Further  these reporting systems leave information ultimately in the hands of a single organization  New programming platforms allow for decentralized reporting techniques that could empower both witnesses and reporting organizations that have traditionally depended on in house  expensive solutions to receive information from the field    What it does   aNonreport is an online platform that enables users to file reports anonymously  including video   images  that can be geo tagged and immediately sent to one or more monitoring organizations  It uses the latest in decentralized  anonymous communication technologies based on the same secure blockchain technology that bitcoin uses  By eliminating centralized servers and relying on multiple layers of encryption  users are assured of their anonymity  while receiving organizations are able to use a real time updating system that is immune to traditional hacking exploits    aNonreport will have numerous applications  from election day monitoring in domestic contexts to reporting attacks on civilians during  and even providing a whistleblowing platform for existing organizations  A scalable solution  the platform will eventually be able to enable one user s report to be sent to multiple organizations simultaneously  while ensuring a complete digital chain of custody that can be verified for investigative purposes    Possible applications include use by      Existing organizations that already have individuals in the field monitoring human rights violations    Instantly reporting attacks during wartime  such as barrel bombs or chemical weapons in Syria by trained human rights monitors      Election monitoring reporting by field observers     Post facto interviewing of witnesses during human rights investigations       Individuals who want to anonymously report violations that they have experienced or documented such as      Refugees or migrants en route submitting reports on their own experiences   Citizens reporting on police violence or abuse   Witnesses of corruption by citizens  especially when paired with apps like CameraV     How is aNonreport different than existing solutions like SecureDrop or Ushahidi    aNonreport is different from any current reporting solution because it takes advantage of a completely decentralized  yet verifiable distribution network through IPFS and Ethereum  In plain English       There is no central server to attack  meaning that all stored data cannot be located or hacked  There is no point of failure once uploaded    All data is  fingerprinted  via a randomized security key  which allows organizations to track who has accessed the data   verify that it has not been corrupted  The user who uploaded the information remains entirely anonymous      Both users and organizations have to properly implemented security protocol to protect their ends of the system  but aNonreport is designed to assure completely transparent  open source  end to end security     How we built it   aNonreport is designed from the ground up to use Ethereum and MaidSafe  providing a next generation decentralized platform with applications that can circumvent almost all censorship or interference from third parties  Self enforcing contracts implemented through Embark form the basis of the platform and are programmed in Ethereum  Encryption of files is handled through PGP AES 128 and RSA 1024  All uploaded files are currently hashed in MD5 and  before public release  will be hashed in SHA256 to provide verification capabilities  The file uploads will eventually be handled through MailSafe  a a secure data storage platform that puts small portions of encrypted files on different computers  This ensures a file network without any centralized server and increased anonymity   korkboard au revoir Swisp             A simple Scheme  Lisp dialect  interpreter written in Swift    Inspired by   How to Write a  Lisp  Interpreter  in Python    by  Peter Norvig     Usage   Swisp can be built and run locally using the Swift compiler    git clone https   github com jakerockland Swisp git cd Swisp swift build  build debug Swisp   To run Swisp with an input file containing your Lisp code     build debug Swisp  i filename   To run Swisp in interactive REPL mode     build debug Swisp   Example of REPL in action    Swisp   define r 10  Swisp     pi    r r   314 159265358979 Swisp   if       11 11  120     7 6  oops  42 Swisp    To run the tests    swift test   Contributions   I welcome contributions  however  please add relevant unit tests for any new features or procedures    License   Copyright  c  2016 Jake Rockland  http   jakerockland com    Permission is hereby granted  free of charge  to any person obtaining a copy of this software and associated documentation files  the  Software    to deal in the Software without restriction  including without limitation the rights to use  copy  modify  merge  publish  distribute  sublicense  and or sell copies of the Software  and to permit persons to whom the Software is furnished to do so  subject to the following conditions    The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software    THE SOFTWARE IS PROVIDED  AS IS   WITHOUT WARRANTY OF ANY KIND  EXPRESS OR IMPLIED  INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT  IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM  DAMAGES OR OTHER LIABILITY  WHETHER IN AN ACTION OF CONTRACT  TORT OR OTHERWISE  ARISING FROM  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE  Go vs  Swift   The Languages of The Modern Tech Giants     About   My final research project for CSCV 372  Comparative Programming Langugages   Abstract   This project stands as a comparative exploration of Go and Swift  the recent flagship languages developed by Google and Apple  respectively  Specifically  I would like to explore first what pushed these modern tech behemoths to develop two new programming languages and second how these two languages are similar and how they differ  given that they were released at relatively similar times    License   Copyright 2016 Jacob Rockland   Licensed under the Apache License  Version 2 0  the  License    you may not use this file except in compliance with the License  You may obtain a copy of the License at   http   www apache org licenses LICENSE 2 0   Unless required by applicable law or agreed to in writing  software distributed under the License is distributed on an  AS IS  BASIS  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND  either express or implied  See the License for the specific language governing permissions and limitations under the License  Pooh     A simple program for adding one of Github s  useful  gitignore templates  to your project    Inspired by a desire to build something simple yet functional with Ruby    Installation   Pooh can be downloaded and run as a standalone file  or can also be installed with  Homebrew     bash   brew tap jakerockland pooh   brew install pooh   Usage   To use Pooh just pass the command your desired language or framework    For example  to use Github s template for Swift  use  pooh swift   Pooh is case insensitive and support all languages found  here     For information on additional command line options  use  pooh   help     Contributions   I welcome contributions    License   Copyright  c  2018 Jake Rockland  http   jakerockland com    Permission is hereby granted  free of charge  to any person obtaining a copy of this software and associated documentation files  the  Software    to deal in the Software without restriction  including without limitation the rights to use  copy  modify  merge  publish  distribute  sublicense  and or sell copies of the Software  and to permit persons to whom the Software is furnished to do so  subject to the following conditions    The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software    THE SOFTWARE IS PROVIDED  AS IS   WITHOUT WARRANTY OF ANY KIND  EXPRESS OR IMPLIED  INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT  IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM  DAMAGES OR OTHER LIABILITY  WHETHER IN AN ACTION OF CONTRACT  TORT OR OTHERWISE  ARISING FROM  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE  homebrew pooh   Homebrew formula for tapping  Pooh   SafeSexStats   Planned Parenthood is a great resource for learning about different types of contraceptives and how to   have   sex   safely      However  I m a fan of the saying  two is one and one is none  and wanted an easy way to understand the combined effectiveness of multiple contraceptives in preventing unwanted pregnancy and sexually transmitted diseases  so I built  SafeSexStats     This project was bootstrapped with  Create React App     Available Scripts   In the project directory  you can run    yarn start   Runs the app in the development mode   Open  http   localhost 3000  to view it in the browser    The page will reload if you make edits   You will also see any lint errors in the console    yarn predeploy   Prepares a build to be shipped    yarn deploy   Ships the updated build
27,apaszke,console timestamp   It s a simple date formatter for Node js  Perfect for logging    There s one function and one String property included    Timestamp function   timestamp  format  time      Both arguments are optional  You can call it with a string which becomes the format  defaults to  hh mm ss    while the second argument can be a number or a Date object to print  becomes current time by default   It replaces specific parts of format string      YYYY  with  4 digit Year   YY  with 2 digit Year   MM  with Month   DD  with Day   hh  with Hours   mm  with Minutes   ss  with Seconds   iii  with Miliseconds     String getter   Now  you can use a new String getter   hh mm ss  timestamp  which will replace placeholders in the string  You can t specify a date then  The values are automatically set to current time  It s just a shortcut for the regular function    Examples      javascript var timestamp   require  console timestamp      var now   new Date    var number   478921    console log  hh mm ss  timestamp     13 58 29 console log  DD MM YY hh mm  timestamp     28 03 14 14 43 console log timestamp       15 43 20 console log timestamp  DD MM YYYY hh mm ss iii       04 07 2014 14 32 45 891 console log timestamp   SERVER TIME hh mm         SERVER TIME 14 23  console log timestamp null  number      01 07 58 console log timestamp  MM DD hh mm   now      11 27 12 43       Licensed under MIT license  Copyright  c  2014 Adam Paszke interactive   A simple togglable interactive mode for node js    Start interactive mode from anywhere in your code  Interact with your global objects and evaluate new pieces  Use arrow keys to navigate through commands you ran  even in previous sessions    Install   npm install interactive   Please note that it switches stdin to rawMode  so if you have any listeners on it  it s best to close them first    Exports   There are two functions available      start  name      Starts interactive mode   If name is specified it s printed at the beginning of interactive session       stop      Stops interactive mode         Special options   Have an complicated object you need to set up often  Bored of repeating code    Try   l  filename   to run a code snippet from a file  Filename can be either relative to your project root or absolute    To quit type   q   Example usage    test js  javascript console log  this is output from test js    a   7   yourscript js    javascript var interactive   require  interactive    interactive start  Point 1        Interactive mode started at  Point 1     a   5   5      l test js   this is output from test js   undefined     a   7      q   Interactive mode off       Licensed under MIT license  Copyright  c  2014 Adam Paszke qlt   Quick and lightweight logging tool for node js  Free yourelf from writing console log all the time    Create output on the fly and print it with just a few keystrokes    javascript var how is it    awesome     Wow  this is     how is it  logn    This produces the same  Wow  this is  log  how is it logn    Wow  this is awesome    It s a more convinient  cleaner and even faster  25  compared to console log  way to display output from your app    Great  How can i use it    All datatypes  not only strings   are provided with two new properties      log   prints value to stdout    logn   prints value and newline to stdout      Changing   log  property of object prototype collides with console log  so objects use   print  instead of   log     logn  is the same for all    Install   npm install qlt   Example   There s no need to assign this module to a variable  Just require it and you re good to go       javascript require  qlt    var a   5  var b    a   var c    1 2   var d   true  var e         a 5      b 6    var f   new Date      a log  b logn  c log  d logn  e logn  e print    e is an object  f logn    How convinient  This is a      a      and this is c      c  log        Outputs    5a  1 2 true   a  5  b  6     a  5  b  6  Mon Mar 31 2014 21 11 31 GMT 0200  CEST  How convinient  This is a  5  and this is c  1 2   Comming soon     stderr support   tcp ping   TCP ping utility for node js  You can test if chosen address accepts connections at desired port and find out your latency  Great for service availability testing    Why not  ping  wrapper      It s much faster than  ping  tool  as soon as connection gets accepted  it s dropped and a new measure is conducted immediately   so there s no unnecessary waiting between requests    It allows you to test a specific service  not the whole connection   Some servers drop ICMP echo without any response  even when online  TCP can work in such cases      Install   npm install tcp ping   Functions   ping options  callback    options  is an object  which may contain several properties      address  address to ping  defaults to  localhost     port  defaults to  80     timeout  in ms  defaults to 5s    attempts  how many times to measure time  defaults to 10      callback  should be a function with arguments in node convention    function err  data      Returned data is an object which looks like this   javascript     address   46 28 246 123     port  80    attempts  10    avg  19 7848844    max  35 306233    min  16 526067    results             seq  0  time  35 306233          seq  1  time  16 585919                  seq  9  time  17 625968            probe address  port  callback    callback  is a node style callback  function err  data    where data is true if the server is available and false otherwise    Usage      javascript var tcpp   require  tcp ping      tcpp probe  46 28 246 123   80  function err  available        console log available         tcpp ping   address   46 28 246 123     function err  data        console log data           terminal control   Terminal escape codes made easy  Get in full control    If your application runs in different environments  don t worry  The init function checks if the process runs in a tty  If it s not  no escape codes will be output even if you call the methods  You can override this behavior by calling  init true      Installation   Run the following commands to download and install the package    sh   npm install terminal control   Usage       var term   require  terminal control   init   autoClean      term setBold true   term moveCursorTo 5 5   console log  This text is displayed with an offset           Docs   Init     init  override                       initialize and check if the programm runs in a tty  if override    true    autoClean                            automatically reset text style to default on process exit     Text style     resetTextStyle                       restore default text style   setBold  set                         turn bold on or off  calling without argument toggles    setLowIntensity  set                 turn low intensity on or off  calling without argument toggles    setUnderline  set                    turn underline on or off  calling without argument toggles    setBlinking  set                     turn blinking on or off  calling without argument toggles    setInvisible  set                    turn invisible text on or off  calling without argument toggles      Window size     setWindowSize top  bottom            set top and bottom line of a window     Cursor movement     moveCursorUp  lines                  move cursor up by a number of lines  default 1    moveCursorDown  lines                move cursor down by a number of lines  default 1    moveCursorRight  columns             move cursor right by number of columns  default 1    moveCursorLeft  columns              move cursor left by number of columns  default 1    moveCursorToUpperLeft                move cursor to the upper left corner   moveCursorTo x y                     move cursor to desired position   scrollUp  lines                      scroll up by a number of lines  default 1    scrollDown  lines                    scroll down by a number of lines  default 1    nextLine                             go to new line   saveCursor                           save cursor position and text attributes   restoreCursor                        restore saved cursor position and text attributes     Clear line     clearLineCursorRight                 clear text in a line on the right from the cursor   clearLineCursorLeft                  clear text in a line on the left from the cursor   clearLine                            clear current line     Clear screen     clearScreenCursorDown                clear text upon cursor   clearScreenCursorUp                  clear text below cursor   clearScreen                          clear whole screen  doesn t reset the position      Others     ringBell                             ring the terminal bell     License   See the  LICENSE  file  Youtube Scrobbler   YTS is the most intuitive of all the scrobblers available for the chrome browser  The SmartTag feature automatically generates suggestions so you hardly ever have to type anything on your own  Correcting track tags has never been so quick and easy  supermongo   The last MongoDB GUI   CLI you ll ever need  Node js based  HabitUI ariel   ariel  is a restricted Boltzmann machine based neural network which can be taught how to play music    Input Output    ariel uses parsed MIDI tracks as input and returns a fully legal MIDI track that can be loaded to any MIDI player    Neural network    The implementation of the neural network is strongly based on the  dnn   library for node js  with a handful of additional hacks and tricks  Kaggle Grasp and Lift Detection   Code in this repository can be used to train and sample both LSTM and CNN  quite experimental  models on Kaggle Grasp and Lift EEG Detection competition data    I have no idea what it s leaderboard score is because I forgot about the entry deadline      Notes   These models probably aren t performing really well  I had little knowledge in signal processing and EEG domain and I ve not spent enough time on this competition to get satisfying results  Anyway  it was a great opportunity to learn how LSTMs work and how to use python for data processing    Setup   After cloning the repo you should run  setup sh   which will prepare the directory structure and preprocess the data  It should be ready to work afterwards    Main scripts   There are two other scripts attached     do subject sh num    trains an LSTM model for subject number  num  and evaluates it on validation set     do submission sh    trains an LSTM model for each subject separately  and generates both validation and submission files   Pipeline   First python scripts are used for preprocessing      calc mean std py  applies low pass filter  calculates mean and std of the data and saves it in  data mean std pickle  for future use    modify data py  applies preprocessing  selects some files for validation and saves them in  data preprocessed   validation files have   val  appended to their name     train lstm lua   train conv lua  reads preprocessed data  puts it into Torch tensors and caches them in  data torch   Then it trains the corresponding model saving checkpoints in  cv   folder    sample lua  recreates model from specified checkpoint and inputs validation or test data through it  Sampled files are saved in  tmp sampled files     calc roc py  reads files from  tmp sampled files   finds corresponding event files and compares them to generate ROC curves and calculate AUC      How to use included scripts    All python and lua scripts can be called with    help  flag and list supported arguments  termtile   termtile is a set of scripts  which set you free from your mouse and touchpad  Don t distract yourself and manage all your terminal windows with a handful of commands    Now with multi screen support    All scripts are written in AppleScript  so they are unfortunately limited to OS X at the moment       Tip  enable Command key option in Preferences   Profiles   Window  so you ll see the keyboard shortcut which jumps to a particular window    Installation   Just paste the following lines into Your terminal    bash git clone https   github com apaszke termtile cd termtile    install sh   You can delete the cloned repository afterwards    Aliases   install sh  can configure all the aliases for you    Don t worry about the conflicts   There are three ways of dealing with them already implemented      If you re already using  ll  then the defaults can be changed    ll      fl   rr      fr   You can specify a prefix for all commands    If you already have a conflicting command and you deny to overwrite it  the script will ask you for another name  so you can choose it for each alias separately  prefix will still be applied       Default aliases     ll    fill left half of the screen    rr    fill right half    up    fill upper half    down    fill lower half    ul    fill upper left quarter    ur    fill upper right quarter    dl    fill lower left quarter    dr    fill lower right quarter    big    make the window bigger    cen    center the window    max    maximize the window    sn    move to the next display    fs    toggles terminal app fullscreen   You can always customize them on your own  They always look like this       bash alias ll  osascript    termtile tile scpt left                                      osascript    path to script     args         They are also very convenient in conjunction with other commands e g    bash alias vim  big    cen    vim    Included scripts   Currently there are three scripts    tile applescript   Distributes windows across 2 x 2 grid  affects only the last active window   Accepted arguments      center applescript   Centers the window    resize applescript   Makes the window comfortably sized for tasks requiring more space  eg  vim   Default size is 1000x600  but it can be changed with the arguments  first is width  second is height     maximize applescript   Maximizes the window    changeScreen applescript   Moves the window to another screen  Currently only supports moving to the next display on a list    fullscreen applescript   Toggles fullscreen for the terminal app    Troubleshooting   If you re using any scripts  which can affect your dock settings  then it might mess up the values termtile reads  If it starts working in a wierd way after you change the dock position or size please try to reboot your machine    Contributing     If you have any suggestions feel free to file an issue    Pull requests are very welcome  but consider creating an issue first  so we can decide together if it s worth spending time on it  blush      License   Licensed under MIT license  Copyright  c  2015 Adam Paszke tundra   Identification of people on videos based on their walking style using convolutional and recurrent neural networks    To do     Add a section about using your own neural network with  tundra       Installation   We use a lot of technologies  so you ve got to do a lot of stuff    You can do it step by step with this little tutorial or just scroll down to see a longer code snippet that you can just copypaste into your terminal    Get required software first      Ruby and  Ruby on Rails  Frontend Server    You will have to run  bundle install  in  front server  directory       Node js  Backend Prediction Server    You will have to run  npm install  in  prediction server  directory       Torch7  Neural Networks    Torch7 installation guide       ffmpeg  Data Preprocessing    You download it here         Copypaste script for a quick start   bash git clone https   github com apaszke tundra cd tundra front server bundle install cd    prediction server npm install   How to launch    Run Frontend Server    bash   run frontend server sh   Run Prediction Server   bash   run prediction server sh   How to use   Now you can open your browser at  localhost 1337  and test the neural network    Trivia   When it started   We ve done a prototype of tundra during AGHacks 2015    Inspiration   People can easily recognize the walking style of their friends  Let s see if we can teach a computer to identify walking people on videos    What it does   It identifies people on videos based on their walking style    How we built it   We build a neural network in torch and trained in on Amazon Web Service because we needed a lot of computing power  Additionally  we ve made a website client in Ruby on Rails and a back end server in Node js which runs a trained neural network  When this neural network receives a video it returns the probabilities for every person it knows that the person is walking in the uploaded video    Was it easy to create a neural network good enough for pitching at AGHacks 2015      What s next for tundra force dev team   We re going to        Why  tundra force    Well  the  tundra  word just sounds awesome  doesn t it    THNN   Compiling   From the main folder  run  cmake   make   to check that it s working  run from the main folder  th tests SpatialConvolutionMM lua quantize   This package allows to train regular torch neural networks with quantized weights    How to use it    lua local q   require  quantize  model quantize q fixed 1  4     To enable call   quantize    on a model and pass a quantizer function  For now there s one provided with the package    q fixed integer  reminder   rounds all weights to a range representable using  integer  bits for integer part and  reminder  bits for remainder part  it also adds one implicit bit for the sign   nvtx   This package exposes a Lua API to NVIDIA Toolkit Extensions  It s a very convenient way to annotate code regions  that makes profiling with NVIDIA Visual Profiler easier    Installation   Please make sure that you have CUDA Toolkit installed and run the following commands    bash git clone https   github com apaszke torch nvtx cd torch nvtx  luarocks make rocks nvtx 0 1 1 rockspec   How to use it    There are two kinds of events   marks and ranges    Marks allow you to highlight events that happen during the execution    Ranges can show how long did some part of your program has been running  and they are organised in a stack fashion   you can mark sub parts with nested ranges    API     nvtx pushRange name   color      Put a new range on the stack   nvtx popRange      Pop last range   nvtx mark name   color      Mark an event     color  argument is optional and allows you to customise how individual ranges marks will be displayed in the Visual Profiler    Please note that you should pop all ranges before your program quits  or they won t be displayed in the profiler  Copy  defaults cfg  to  config cfg  and set the path to your binaries first  pytorch  alpha 4      What is PyTorch    Reasons to consider PyTorch   Installation   Binaries   From source   Getting Started   Communication   Timeline   pytorch vs torch  important changes       Python     Linux CPU        Linux GPU                                                         2 7 8            2 7                 3 3              3 4              3 5                 Nightly          The project is still under active development and is likely to drastically change in short periods of time  We will be announcing API changes and important developments via a newsletter  github issues and post a link to the issues on slack  Please remember that at this stage  this is an invite only closed alpha  and please don t distribute code further  This is done so that we can control development tightly and rapidly during the initial phases with feedback from you    What is PyTorch    PyTorch is a library that consists of the following components                                                                         torch                      a Tensor library like NumPy  with strong GPU support     torch autograd             a tape based automatic differentiation library that supports all differentiable Tensor operations in torch     torch nn                   a neural networks library deeply integrated with autograd designed for maximum flexibility     torch optim                an optimization package to be used with torch nn with standard optimization methods such as SGD  RMSProp  LBFGS  Adam etc      torch multiprocessing      python multiprocessing  but with magical memory sharing of torch Tensors across processes  Useful for data loading and hogwild training      torch utils                DataLoader  Trainer and other utility functions for convenience     torch legacy  nn  optim    legacy code that has been ported over from torch for backward compatibility reasons     Usually one uses PyTorch either as      A replacement for numpy to use the power of GPUs    a deep learning research platform that provides maximum flexibility and speed     Reasons to consider PyTorch   Python first   PyTorch is not a Python binding into a monolothic C   framework   It is built to be deeply integrated into Python   You can use it naturally like you would use numpy   scipy   scikit learn etc   You can write your new neural network layers in Python itself  using your favorite libraries    Imperativeness first  What you see is what you get    PyTorch is designed to be intuitive and easy to use   When you are debugging your program  or receive error messages   stack traces  you are always guaranteed to get error messages that are easy to understand and a stack trace that points to exactly where your code was defined  Never spend hours debugging your code because of bad stack traces or asynchronous and opaque execution engines    Performance and Memory usage   PyTorch is as fast as the fastest deep learning framework out there  We integrate acceleration frameworks such as Intel MKL and NVIDIA CuDNN for maximum speed    The memory usage in PyTorch is extremely efficient  and we ve written custom memory allocators for the GPU to make sure that your deep learning models are maximally memory efficient  This enables you to train bigger deep learning models than before    Multi GPU ready   PyTorch is fully powered to efficiently use Multiple GPUs for accelerated deep learning   We integrate efficient multi gpu collectives such as NVIDIA NCCL to make sure that you get the maximal Multi GPU performance    Simple Extension API to interface with C   Writing new neural network modules  or interfacing with PyTorch s Tensor API is a breeze  thanks to an easy to use extension API that is efficient and easy to use    Installation   Binaries     Anaconda  bash conda install pytorch  c https   conda anaconda org t 6N MsQ4WZ7jo soumith     From source   Install optional dependencies   bash export CMAKE PREFIX PATH  anaconda root directory  conda install numpy mkl conda install  c soumith magma cuda75  or magma cuda80   Install PyTorch   bash pip install  r requirements txt pip install     Getting Started   Three pointers to get you started     Tutorials  notebooks to get you started with understanding and using PyTorch     Examples  easy to understand pytorch code across all domains    The API Reference   http   pytorch org api    Communication     github issues  bug reports  feature requests  install issues  RFCs  thoughts  etc    slack  general chat  online discussions  collaboration etc  https   pytorch slack com    If you need a slack invite  ping me at soumith pytorch org   newsletter  no noise  one way email newsletter with important announcements about pytorch  You can sign up here  http   eepurl com cbG0rv     Timeline   We will run the alpha releases weekly for 6 weeks  After that  we will reevaluate progress  and if we are ready  we will hit beta 0  If not  we will do another two weeks of alpha        alpha 0  Working versions of torch  cutorch  nn  cunn  optim fully unit tested with seamless numpy conversions       alpha 1  Serialization to from disk with sharing intact  initial release of the new neuralnets package based on a Chainer like design       alpha 2  sharing tensors across processes for hogwild training or data loading processes  a rewritten optim package for this new nn        alpha 3  binary installs  contbuilds  etc        alpha 4  multi GPU support  cudnn integration  imagenet   resnet example     alpha 5  a ton of examples across vision  nlp  speech  RL    this phase might make us rethink parts of the APIs  and hence want to do this in alpha than beta   alpha 6  Putting a simple and efficient story around multi machine training  Probably simplistic like torch distlearn  Building the website  release scripts  more documentation  etc    beta 0  First public release     The beta phases will be leaning more towards working with all of you  convering your use cases  active development on non core aspects    pytorch vs torch  important changes   We ve decided that it s time to rewrite update parts of the old torch API  even if it means losing some of backward compatibility    This tutorial  takes you through the biggest changes  and walks you through PyTorch   For brevity    Tensors      clear separation of in place and out of place operations   zero indexing   no camel casing for Tensor functions   an efficient Numpy bridge  with zero memory copy    CUDA tensors have clear and intuitive semantics     New neural network module  Combines nn  nngraph  autograd       Design inspired from Chainer   Modules no longer hold state  State is held in the graph   Access state via hooks   Execution engine   imperative execution engine  default    lazy execution engine   allows graph optimizations and automatic in place   fusing operations       Model structure is defined by its code   You can use loops and arbitrarily complicated conditional statements             To reiterate  we recommend that you go through  This tutorial   Serialization   Pickling tensors is supported  but requires making a temporary copy of all data in memory and breaks sharing    For this reason we re providing  torch load  and  torch save   that are free of these problems    They have the same interfaces as  pickle load   file object  and  pickle dump   serialized object  file object  respectively    For now the only requirement is that the file should have a  fileno  method  which returns a file descriptor number  this is already implemented by objects returned by  open      Objects are serialized in a tar archive consisting of four files     sys info    protocol version  byte order  long size  etc     pickle    pickled object    tensors    tensor metadata    storages    serialized data   Multiprocessing with Tensor sharing   We made PyTorch to seamlessly integrate with python multiprocessing  What we ve added specially in torch multiprocessing is the seamless ability to efficiently share and send tensors over from one process to another    technical details of implementation   This is very useful for example in    Writing parallelized data loaders   Training models  hogwild   where several models are trained in parallel  sharing the same set of parameters    Here are a couple of examples for torch multiprocessing      python   loaders py   Functions from this file run in the workers   def fill queue     while True        tensor   queue get         tensor fill  10        queue put tensor    def fill pool tensor     tensor fill  10           python   Example 1  Using multiple persistent processes and a Queue   process py   import torch import torch multiprocessing as multiprocessing from loaders import fill   torch multiprocessing Queue automatically moves Tensor data to shared memory   So the main process and worker share the data   queue   multiprocessing Queue   buffers    torch Tensor 2  2  for i in range 4   for b in buffers    queue put b    processes    multiprocessing Process target fill  args  queue    start   for i in range 10            python   Example 2  Using a process pool   pool py   import torch from torch multiprocessing import Pool from loaders import fill pool   tensors    torch Tensor 2  2  for i in range 100   pool   Pool 10  pool map fill pool  tensors        Some notes on new nn implementation   As shown above  structure of the networks is fully defined by control flow embedded in the code  There are no rigid containers known from Lua  You can put an  if  in the middle of your model and freely branch depending on any condition you can come up with  All operations are registered in the computational graph history    There are two main objects that make this possible   variables and functions  They will be denoted as squares and circles respectively      Variables are the objects that hold a reference to a tensor  and optionally to gradient w r t  that tensor   and to the function in the computational graph that created it  Variables created explicitly by the user   Variable tensor    have a Leaf function node associated with them      Functions are simple classes that define a function from a tuple of inputs to a tuple of outputs  and a formula for computing gradient w r t  it s inputs  Function objects are instantiated to hold references to other functions  and these references allow to reconstruct the history of a computation  An example graph for a linear layer   Wx   b   is shown below      Please note that function objects never hold references to Variable objects  except for when they re necessary in the backward pass  This allows to free all the unnecessary intermediate values  A good example for this is addition when computing e g    y   Wx   My        Matrix multiplication operation keeps references to it s inputs because it will need them  but addition doesn t need  Wx  and  My  after it computes the result  so as soon as they go out of scope they are freed  To access intermediate values in the forward pass you can either copy them when you still have a reference  or you can use a system of hooks that can be attached to any function  Hooks also allow to access and inspect gradients inside the graph    Another nice thing about this is that a single layer doesn t hold any state other than it s parameters  all intermediate values are alive as long as the graph references them   so it can be used multiple times before calling backward  This is especially convenient when training RNNs  You can use the same network for all timesteps and the gradients will sum up automatically    To compute backward pass you can call   backward    on a variable if it s a scalar  a 1 element Variable   or you can provide a gradient tensor of matching shape if it s not  This creates an execution engine object that manages the whole backward pass  It s been introduced  so that the code for analyzing the graph and scheduling node processing order is decoupled from other parts  and can be easily replaced  Right now it s simply processing the nodes in topological order  without any prioritization  but in the future we can implement algorithms and heuristics for scheduling independent nodes on different GPU streams  deciding which branches to compute first  etc  Testy do zadania z Prologu   Program zak ada  e implementacja obu funkcji z zadania znajduje si  w pliku  auto pl   mo na to zmieni  w  test words pl    Testy generuj  wszystkie s owa o wyprowadzeniach do zadanej g  boko ci  oraz dla ka dego takiego s owa dokonuj  losowych przekszta ce   i sprawdzaj  czy automat nie przyjmuje takiego s owa  je eli nie nale y ju  do j zyka     Zale no ci   Python odpala interpreter  swipl  zamiast  sicstus a   swipl  jest darmowy  wi c mo na go u siebie  atwo zainstalowa      bash pip install nltk   Uruchamianie   bash python test words py   D ugo   test w mo na edytowa  zmieniaj c parametr  depth 8  w  test words py   oczywi cie liczba s  w ro nie wtedy wyk adniczo   onnx fb builds     Travis   Jenkins                                    Why Universe Repo      It integrates multiple repos  including onnx  pytorch  caffe2  to provide a convenient environment for developers    OnnxBot automatically updates the submodules in  repos  folder  runs the tests  and automatically detects the breaking changes from different repos      Installation   shell   install sh      or   install develop sh to install the most of the repos in develop mode    test sh   Dockerfile  may be helpful for setting up the whole repo    Repo Structure     repos   submodules   test   test scripts   jenkins   jenkins CI configuration   travis   travis CI configuration     Test   Test Script   This repo contains several test scripts in  test  folder     test caffe2 py   PyTorch     ONNX     Caffe2 end to end tests  in which the decimal precision is checked     test models py   PyTorch     ONNX model tests  check the output ONNX graph with expected files     test operators py   PyTorch     ONNX operator tests  check the output ONNX graph with expected files     test verify py   PyTorch     ONNX verification tests  check the exporting raises exceptions in certain conditions     test pytorch helper py   PyTorch     ONNX helper tests  check the pytorch related helper functions    To update the  expected  files  please add flag    accept  to the end of the command    Convert Test from PyTorch   Using  convert pytorch test py   we can easily turn some existing PyTorch testing cases to ONNX backend test cases  Just run   shell python test convert pytorch test py   Generate Test from PyTorch Operator   Script  test operators py  is also able to generate ONNX backend test cases based on operator tests  Just run   shell python test test operators py   onnx test   Check the Generated Tests   All the generated ONNX backend test data is stored in  repos onnx onnx backend test data model     But the generated test cases may not pass end to end tests  so we have another script  filter generated test py  to check whether the cases are passed or not  and the failed data will be moved to  test failed generated     filter generated test py  supports the following flags      v   verbose      delete   delete failed test cases  by default  they are moved to  test failed generated        no expect   do not generate expect files for each test  by default  expected files are generated in  test expect  folder for debugging purpose    More Docs     ONNX IR spec   ONNX operator spec   ONNX tutorials   PyTorch docs   Caffe2 docs     Tensor Comprehensions  TC  is a fully functional C   library to  automatically  synthesize high performance machine learning kernels using  Halide    ISL  and NVRTC or LLVM  TC additionally provides basic integration with Caffe2 and PyTorch  We provide more details in our paper on  arXiv     This library is designed to be highly portable  machine learning framework agnostic and only requires a simple tensor library with memory allocation  offloading and synchronization capabilities    For now  we have integrated TC with the  Caffe2  and  PyTorch     A simple example   The following illustrates a short but powerful feature of the library  the capacity to JIT compile high performance machine learning kernels on demand  for specific sizes    python import tensor comprehensions as tc import torch lang       def tensordot float N  C1  C2  H  W  I0  float N  C2  C3  H  W  I1      O        O n  c1  c3  h  w      I0 n  c1  c2  h  w    I1 n  c2  c3  h  w        N  C1  C2  C3  H  W   32  512  8  2  28  28 tensordot   tc define lang  name  tensordot   I0  I1   torch randn N  C1  C2  H  W  cuda    torch randn N  C2  C3  H  W  cuda   best options   tensordot autotune I0  I1  cache True  out   tensordot I0  I1  options best options    After a few generations of  autotuning  on a 2 GPU P100 system  we see results resembling      We have not yet characterized the precise fraction of peak performance we obtain but it is not uncommon to obtain 80   of peak shared memory bandwidth after autotuning  Solid register level optimizations are still in the work but TC in its current form already addresses the productivity gap between the needs of research and the needs of production  Which is why we are excited to share it with the entire community and bring this collaborative effort in the open    Documentation   General   You can find detailed information about Tensor Comprehensions  here     C   API   We also provide documentation for our C   API which can can be found  here   Installation   Binaries   We provide conda package for making it easy to install and use TC binary  Please refer to our documentation  here  for instructions    From Source   You can find documentation  here  which contains instructions for building TC via docker  conda packages or in non conda environment    Communication     Email   tensorcomp fb com   GitHub issues   bug reports  feature requests  install issues  RFCs  thoughts  etc    Slack   For discussion around framework integration  build support  collaboration  etc  join our slack channel https   tensorcomprehensions herokuapp com       Code of Conduct   See the  CODE OF CONDUCT md  file for more details    License   Tensor Comprehensions is distributed under a permissive Apache v2 0 license  see the  LICENSE  file for more details    Contributing   See the  CONTRIBUTING md  file for more details        JAX  Autograd and XLA    Quickstart     Transformations     Install guide     Neural net libraries     Change logs     Reference docs     Code search   News    JAX tops largest scale MLPerf Training 0 7 benchmarks    What is JAX    JAX is  Autograd  and  XLA   brought together for high performance machine learning research    With its updated version of  Autograd   JAX can automatically differentiate native Python and NumPy functions  It can differentiate through loops  branches  recursion  and closures  and it can take derivatives of derivatives of derivatives  It supports reverse mode differentiation  a k a  backpropagation  via  grad  as well as forward mode differentiation  and the two can be composed arbitrarily to any order    What s new is that JAX uses  XLA  to compile and run your NumPy programs on GPUs and TPUs  Compilation happens under the hood by default  with library calls getting just in time compiled and executed  But JAX also lets you just in time compile your own Python functions into XLA optimized kernels using a one function API   jit   Compilation and automatic differentiation can be composed arbitrarily  so you can express sophisticated algorithms and get maximal performance without leaving Python  You can even program multiple GPUs or TPU cores at once using  pmap   and differentiate through the whole thing    Dig a little deeper  and you ll see that JAX is really an extensible system for  composable function transformations   Both  grad  and  jit  are instances of such transformations  Others are  vmap  for automatic vectorization and  pmap  for single program multiple data  SPMD  parallel programming of multiple accelerators  with more to come    This is a research project  not an official Google product  Expect bugs and  sharp edges   Please help by trying it out   reporting bugs   and letting us know what you think       python import jax numpy as jnp from jax import grad  jit  vmap   def predict params  inputs     for W  b in params      outputs   jnp dot inputs  W    b     inputs   jnp tanh outputs    return outputs   def logprob fun params  inputs  targets     preds   predict params  inputs    return jnp sum  preds   targets   2    grad fun   jit grad logprob fun      compiled gradient evaluation function perex grads   jit vmap grad fun  in axes  None  0  0       fast per example grads       Contents     Quickstart  Colab in the Cloud   Transformations   Current gotchas   Installation   Neural net libraries   Citing JAX   Reference documentation     Quickstart  Colab in the Cloud   Jump right in using a notebook in your browser  connected to a Google Cloud GPU  Here are some starter notebooks     The basics  NumPy on accelerators   grad  for differentiation   jit  for compilation  and  vmap  for vectorization     Training a Simple Neural Network  with TensorFlow Dataset Data Loading   JAX now runs on Cloud TPUs   To try out the preview  see the  Cloud TPU Colabs     For a deeper dive into JAX     The Autodiff Cookbook  Part 1  easy and powerful automatic differentiation in JAX     Common gotchas and sharp edges    See the  full list of notebooks     You can also take a look at  the mini libraries in  jax experimental   like  stax  for building neural networks  and  optimizers  for first order stochastic optimization   or the  examples     Transformations   At its core  JAX is an extensible system for transforming numerical functions  Here are four of primary interest   grad    jit    vmap   and  pmap     Automatic differentiation with  grad   JAX has roughly the same API as  Autograd   The most popular function is  grad  for reverse mode gradients       python from jax import grad import jax numpy as jnp   def tanh x      Define a function   y   jnp exp  2 0   x    return  1 0   y     1 0   y    grad tanh   grad tanh     Obtain its gradient function print grad tanh 1 0       Evaluate it at x   1 0   prints 0 4199743         You can differentiate to any order with  grad        python print grad grad grad tanh    1 0     prints 0 62162673         For more advanced autodiff  you can use  jax vjp  for reverse mode vector Jacobian products and  jax jvp  for forward mode Jacobian vector products  The two can be composed arbitrarily with one another  and with other JAX transformations  Here s one way to compose those to make a function that efficiently computes  full Hessian matrices        python from jax import jit  jacfwd  jacrev   def hessian fun     return jit jacfwd jacrev fun          As with  Autograd   you re free to use differentiation with Python control structures       python def abs val x     if x   0      return x   else      return  x   abs val grad   grad abs val  print abs val grad 1 0       prints 1 0 print abs val grad  1 0      prints  1 0  abs val is re evaluated        See the  reference docs on automatic differentiation  and the  JAX Autodiff Cookbook  for more    Compilation with  jit   You can use XLA to compile your functions end to end with  jit   used either as an   jit  decorator or as a higher order function       python import jax numpy as jnp from jax import jit   def slow f x       Element wise ops see a large benefit from fusion   return x   x   x   2 0   x   jnp ones  5000  5000   fast f   jit slow f   timeit  n10  r3 fast f x       4 5 ms   loop on Titan X  timeit  n10  r3 slow f x       14 5 ms   loop  also on GPU via JAX        You can mix  jit  and  grad  and any other JAX transformation however you like    Using  jit  puts constraints on the kind of Python control flow the function can use  see the  Gotchas Notebook  for more    Auto vectorization with  vmap   vmap  is the vectorizing map  It has the familiar semantics of mapping a function along array axes  but instead of keeping the loop on the outside  it pushes the loop down into a function s primitive operations for better performance    Using  vmap  can save you from having to carry around batch dimensions in your code  For example  consider this simple  unbatched  neural network prediction function    python def predict params  input vec     assert input vec ndim    1   for W  b in params      output vec   jnp dot W  input vec    b     input vec  on the right hand side      input vec   jnp tanh output vec    return output vec   We often instead write  jnp dot inputs  W   to allow for a batch dimension on the left side of  inputs   but we ve written this particular prediction function to apply only to single input vectors  If we wanted to apply this function to a batch of inputs at once  semantically we could just write   python from functools import partial predictions   jnp stack list map partial predict  params   input batch      But pushing one example through the network at a time would be slow  It s better to vectorize the computation  so that at every layer we re doing matrix matrix multiplies rather than matrix vector multiplies    The  vmap  function does that transformation for us  That is  if we write      python from jax import vmap predictions   vmap partial predict  params   input batch    or  alternatively   predictions   vmap predict  in axes  None  0   params  input batch        then the  vmap  function will push the outer loop inside the function  and our machine will end up executing matrix matrix multiplications exactly as if we d done the batching by hand    It s easy enough to manually batch a simple neural network without  vmap   but in other cases manual vectorization can be impractical or impossible  Take the problem of efficiently computing per example gradients  that is  for a fixed set of parameters  we want to compute the gradient of our loss function evaluated separately at each example in a batch  With  vmap   it s easy    python per example gradients   vmap partial grad loss   params   inputs  targets    Of course   vmap  can be arbitrarily composed with  jit    grad   and any other JAX transformation  We use  vmap  with both forward  and reverse mode automatic differentiation for fast Jacobian and Hessian matrix calculations in  jax jacfwd    jax jacrev   and  jax hessian     SPMD programming with  pmap   For parallel programming of multiple accelerators  like multiple GPUs  use  pmap   With  pmap  you write single program multiple data  SPMD  programs  including fast parallel collective communication operations  Applying  pmap  will mean that the function you write is compiled by XLA  similarly to  jit    then replicated and executed in parallel across devices    Here s an example on an 8 GPU machine       python from jax import random  pmap import jax numpy as jnp   Create 8 random 5000 x 6000 matrices  one per GPU   keys   random split random PRNGKey 0   8  mats   pmap lambda key  random normal key   5000  6000    keys    Run a local matmul on each device in parallel  no data transfer    result   pmap lambda x  jnp dot x  x T   mats     result shape is  8  5000  5000    Compute the mean on each device in parallel and print the result   print pmap jnp mean  result     prints  1 1566595 1 1805978     1 2321935 1 2015157          In addition to expressing pure maps  you can use fast  collective communication operations  between devices       python from functools import partial from jax import lax    partial pmap  axis name  i   def normalize x     return x   lax psum x   i     print normalize jnp arange 4       prints  0          0 16666667 0 33333334 0 5                 You can even  nest  pmap  functions  for more sophisticated communication patterns    It all composes  so you re free to differentiate through parallel computations       python from jax import grad    pmap def f x     y   jnp sin x     pmap   def g z       return jnp cos z    jnp tan y sum      jnp tanh x  sum     return grad lambda w  jnp sum g w    x    print f x        0            0 7170853        3 1085174    0 4824318       10 366636    13 135289         0 22163185   0 52112055     print grad lambda x  jnp sum f x    x         3 2369726    1 6356447        4 7572474   11 606951        98 524414    42 76499          1 6007166    1 2568436           When reverse mode differentiating a  pmap  function  e g  with  grad    the backward pass of the computation is parallelized just like the forward pass    See the  SPMD Cookbook  and the  SPMD MNIST classifier from scratch example  for more    Current gotchas   For a more thorough survey of current gotchas  with examples and explanations  we highly recommend reading the  Gotchas Notebook   Some standouts      JAX transformations only work on  pure functions   which don t have side effects and respect  referential transparency   i e  object identity testing with  is  isn t preserved   If you use a JAX transformation on an impure Python function  you might see an error like  Exception  Can t lift Traced      or  Exception  Different traces at same level     In place mutating updates of    arrays   like  x i     y   aren t supported  but  there are functional alternatives   Under a  jit   those functional alternatives will reuse buffers in place automatically    Random numbers are    different   but for  good reasons     If you re looking for  convolution    operators      they re in the  jax lax  package    JAX enforces single precision  32 bit  e g   float32   values by default  and     to enable    double precision      64 bit  e g   float64   one needs to set the  jax enable x64  variable at    startup  or set the environment variable  JAX ENABLE X64 True      Some of NumPy s dtype promotion semantics involving a mix of Python scalars    and NumPy types aren t preserved  namely  np add 1  np array  2      np float32   dtype  is  float64  rather than  float32     Some transformations  like  jit    constrain how you can use Python control    flow      You ll always get loud errors if something goes wrong  You might have to use     jit  s  static argnums     parameter       structured control flow    primitives     like     lax scan      or just use  jit  on smaller subfunctions      Installation   JAX is written in pure Python  but it depends on XLA  which needs to be installed as the  jaxlib  package  Use the following instructions to install a binary package with  pip   or to build JAX from source    We support installing or building  jaxlib  on Linux  Ubuntu 16 04 or later  and macOS  10 12 or later  platforms  Windows users can use JAX on CPU via the  Windows Subsystem for Linux   We re not currently working on native Windows support  but contributions are welcome  see   438      pip installation   To install a CPU only version  which might be useful for doing local development on a laptop  you can run   bash pip install   upgrade pip pip install   upgrade jax jaxlib    CPU only version   On Linux  it is often necessary to first update  pip  to a version that supports  manylinux2010  wheels    If you want to install JAX with both CPU and GPU support  using existing CUDA and CUDNN7 installations on your machine  for example  preinstalled on your cloud VM   you can run   bash pip install   upgrade pip pip install   upgrade jax jaxlib  0 1 56 cuda110  f https   storage googleapis com jax releases jax releases html   The jaxlib version must correspond to the version of the existing CUDA installation you want to use  with  cuda110  for CUDA 11 0   cuda102  for CUDA 10 2   cuda101  for CUDA 10 1  and  cuda100  for CUDA 10 0  You can find your CUDA version with  install path    bash nvcc   version   Note that some GPU functionality expects the CUDA installation to be at   usr local cuda X X   where X X should be replaced with the CUDA version number  e g   cuda 10 2    If CUDA is installed elsewhere on your system  you can either create a symlink    bash sudo ln  s  path to cuda  usr local cuda X X   Or set the following environment variable before importing JAX    bash XLA FLAGS   xla gpu cuda data dir  path to cuda   Please let us know on  the issue tracker  if you run into any errors or problems with the prebuilt wheels    Building JAX from source   See  Building JAX from source     Neural network libraries   Multiple Google research groups develop and share libraries for training neural networks in JAX  If you want a fully featured library for neural network training with examples and how to guides  try  Flax   Another option is  Trax   a combinator based framework focused on ease of use and end to end single command examples  especially for sequence models and reinforcement learning  Finally   Objax  is a minimalist object oriented framework with a PyTorch like interface    DeepMind has open sourced an ecosystem of libraries around JAX including  Haiku  for neural network modules   Optax  for gradient processing and optimization   RLax  for RL algorithms  and  chex  for reliable code and testing    Citing JAX   To cite this repository     software jax2018github    author    James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and Skye Wanderman Milne     title     JAX   composable transformations of  P ython  N um P y programs     url    http   github com google jax     version    0 2 5     year    2018       In the above bibtex entry  names are in alphabetical order  the version number is intended to be that from  jax version py   and the year corresponds to the project s open source release    A nascent version of JAX  supporting only automatic differentiation and compilation to XLA  was described in a  paper that appeared at SysML 2018   We re currently working on covering JAX s ideas and capabilities in a more comprehensive and up to date paper    Reference documentation   For details about the JAX API  see the  reference documentation     For getting started as a JAX developer  see the  developer documentation   Experimental Libtorch 1 0 FFI   Experimental work on next gen ffi bindings into the c   libtorch library in preparation for 0 0 2 which targets the 1 0 backend    General approach      Use generated  Declarations yaml  spec instead of header parsing for code generation    Try inline cpp functionality to bind the C   API instead of the C API  Benchmark potential template haskell overhead vs  code generating C wrappers for C   functions      libtorch dependency retrieval and testing   libtorch test   and  deps   have scripts that retrieve libtorch and mkl dnn library dependencies and builds tests them using a  minimal example     To build    cd libtorch test make   The makefile pulls in libtorch   mkl dnn  not included in libtorch  prebuilt shared library files    If the dependencies are retrieved and built successfully  you should see at the end of the build process the expected output of a random 2x2 matrix  exact values may differ           50   Building CXX object CMakeFiles libtorch test dir cpptest cpp o  100   Linking CXX executable libtorch test  100   Built target libtorch test export LD LIBRARY PATH  Users huanga13 projects personal ffi experimental libtorch test deps mklml mac 2019 0 1 20181227 lib  Users huanga13 projects personal ffi experimental libtorch test deps libtorch lib source   set dyld path sh   cd build     libtorch test  0 5790  0 5507  0 6433  0 9908  0 6380  0 3997   Variable CPUFloatType  2 3      yaml    binding codegen  WIP    To run    stack build codegen stack exec codegen exe   To get CLI options    stack exec codegen exe      help   ffi testing  WIP    To run without repl          Download libtorch binary   pushd deps   get deps sh popd   Set environment variable LD LIBRARY PATH    source setenv   Build and test with stack   stack test   Build and test with cabal   setup cabal project freeze using stackage  extra include dirs and extra lib dirs for cabal     setup cabal sh cabal new test all       To run  currently works in the repl     stack ghci   ghc options   fobject code   ffi     Prelude Main  main Hello torch   0 2401  0 0901  0 9807  0 9168  0 3757  0 4029   Variable CPUFloatType  2 3     1  1  1  1   Variable CPUFloatType  2 2     0 1232  1 5721  0 5392  0 2395   Variable CPUFloatType  2 2     1 1232  2 5721  1 5392  1 2395   Variable CPUFloatType  2 2    Prelude Main    Contributions   Contributions PRs are welcome   Dex    Dex  named for  index   is a research language for array processing in the Haskell ML family  The goal of the project is to explore      Type systems for array programming   Mathematical program transformations like differentiation and integration   User directed compilation to parallel hardware   Interactive and incremental numerical programming and visualization     To learn more  check out our  workshop paper  or these example programs      Tutorial   Dex prelude   Mandelbrot set   Estimating pi   Sierpinsky triangle   Basis function regression   Brownian bridge   MNIST nearest neighbor classifier     Please note that Dex is an experimental research project at an early stage of development  We welcome contributions  There s plenty of work to do    Setup     Install  stack   Install LLVM 9  e g   apt get install llvm 9 dev  on Ubuntu Debian      On macOS  the best approach seems to be to build LLVM from source       as described here       Building     Build Dex   make   Run tests   make tests   Set up alias  e g  in  bashrc   alias dex  stack exec dex         Running     Traditional REPL   dex repl   Execute script   dex script examples tutorial dx   Notebook interface   dex web examples tutorial dx     License   BSD 3   This is an early stage research project  not an official Google product  Word level language modeling RNN   This repository is a slight modification of the  word level language modeling RNN PyTorch example     This example trains a multi layer RNN  Elman  GRU  or LSTM  on a language modeling task  By default  the training script uses the Wikitext 2 dataset  provided  The trained model can then be used by the generate script to generate new text    bash python main py   epochs 6             Train a LSTM on Wikitext 2 with CUDA python main py   epochs 6   model Transformer   lr 5                                              Train a Transformer model on Wikitext 2 with CUDA python generate py                           Generate samples from the trained LSTM model  python generate py   model Transformer                                              Generate samples from the trained Transformer model    The model uses the  nn RNN  module  and its sister modules  nn GRU  and  nn LSTM   which will automatically use the cuDNN backend if run on CUDA with cuDNN installed    During training  if a keyboard interrupt  Ctrl C  is received  training is stopped and the current model is evaluated against the test dataset    The  main py  script accepts the following arguments    bash optional arguments     h    help                       show this help message and exit     data DATA                      location of the data corpus     model MODEL                    type of recurrent net  RNN TANH  RNN RELU  LSTM  GRU      emsize EMSIZE                  size of word embeddings     nhid NHID                      number of hidden units per layer     nlayers NLAYERS                number of layers     lr LR                          initial learning rate     clip CLIP                      gradient clipping     epochs EPOCHS                  upper epoch limit     batch size N                   batch size     bptt BPTT                      sequence length     dropout DROPOUT                dropout applied to layers  0   no dropout      decay DECAY                    learning rate decay per epoch     seed SEED                      random seed     log interval N                 report interval     save SAVE                      path to save the final model     transformer head N             the number of heads in the encoder decoder of the transformer model     transformer encoder layers N   the number of layers in the encoder of the transformer model     transformer decoder layers N   the number of layers in the decoder of the transformer model     transformer d ff N             the number of nodes on the hidden layer in feed forward nn   With these arguments  a variety of models can be tested  As an example  the following arguments produce slower but better models    bash python main py   emsize 650   nhid 650   dropout 0 5   epochs 40 python main py   emsize 1500   nhid 1500   dropout 0 65   epochs 40 IREE LLVM Sandbox   DISCLAIMER  This is not an officially supported Google project  It is a sandbox for quick iteration and experimentation on projects related to the IREE project  MLIR  and LLVM    This repository contains experimental work by the IREE team closely related to LLVM and MLIR  usually with the aim of upstreaming in some form  The main project is at https   github com google iree    As an experimental project  build greenness  documentation  and polish are likely to be minimal  as it instead prioritizes easy experimentation    License   Licensed under the Apache license with LLVM Exceptions  See  LICENSE  for more information    Build instructions   This project builds as part of the LLVM External Projects facility  see documentation for the  LLVM EXTERNAL PROJECTS  config setting   There are many ways to set this up  We recommend using our  configure py  script below    It is left to the reader to adapt paths if deviating  We assume below that projects are checked out to   HOME src     Check out projects   TODO  Simplify instructions    In your   HOME src  directory  check out each project    Required      git clone https   github com google iree llvm sandbox     We use the following environment variables defaults in these instructions      IREE LLVM SANDBOX SOURCE DIR    HOME src iree llvm sandbox   IREE LLVM SANDBOX BUILD DIR     IREE LLVM SANDBOX SOURCE DIR  build     Python prerequisites  if using Python    Follow the instructions for  MLIR Python Bindings     which python python  m venv    venv mlirdev source    venv mlirdev bin activate python  m pip install   upgrade pip python  m pip install  r requirements txt   Configure and build    The sandbox can be optionally built with or without IREE integration  for accessing IREE specific IR and evaluating on IREE compatible targets     Building with IREE    Checkout the  IREE  GitHub repo next to this directory and initialize submodules     cd       git clone https   github com google iree   recurse submodules third party llvm project    And configure build the project    python configure py   iree path    iree   Note that the  third party llvm project  bundled with IREE will be used    Building without IREE    You must checkout  llvm project  at a compatible commit     cd       git clone https   github com llvm llvm project git    And configure build the project  By default the  configure py  script will look in    IREE LLVM SANDBOX SOURCE DIR     llvm project   this can also be overridden with    llvm path      python configure py   Using the Python API       source  env    export PYTHONPATH   Sanity check  should not error     python  c  import mlir iree sandbox    Run a matmul    export MLIR RUNNER UTILS LIB   IREE LLVM SANDBOX BUILD DIR  lib libmlir runner utils so    cd   IREE LLVM SANDBOX SOURCE DIR     python  m python matmul test       Using mlir proto opt      IREE LLVM SANDBOX BUILD DIR   bin mlir proto opt        iree llvm sandbox test test constant mlir      linalg comprehensive module bufferize   TODOs      hook up a lit test target    re add npcomp instructions once it is upgraded to use the same build setup      Python driven parameter search   Python tests come with a tool to perform as simple randomized search  The search is going to randomly instantiate a given op to some cocnrete dimensions and type variables and try to compile it using mlir    The results are persisted in the  output   folder by default in a structure that includes a name of the expert compiler  the name of the op and the success failure timeout status code  The results contain the full program output  including potential compilation errors  and an accompanying   sh  file that can be used to re run the same configuration again    Collecting random measurements   To run the search with default settings    export PATH    IREE LLVM SANDBOX BUILD DIR  bin  PATH  cd   IREE LLVM SANDBOX SOURCE DIR  alias search cli  python  m python local search search cli  search cli   To run with a different linalg op  use    op  flag    search cli   op matvec   To specify the name of the expert compilers  use    expert   see  experts py  for all available expert definitions     search cli   experts ExpertCompiler1   To specify the possible types  use    types  flag    search cli   types f32 f64   Alternatively  one can also force some variables to concrete values  while others will ramain random using    assign     search cli   assign M 16 N 32 K 64   To specify range of possible values for dimensions  use    range  flag  where numbers correspond to arguments of the corresponding  range  function in Python     search cli   range 128 256 8   The search can be run using multiple processes at once  via    par  flag    search cli   par 72   Each process collects the fixed number of random samples  customized via    samples  flag    search cli   samples 100   Showing ranked results   One can see a ranked list  based on llvm mca performance estimates    alias rank cli  python  m python local search rank cli  rank cli   You can customize the    op   the number of the output results     limit   and the metric used for ranking     by   through additional command line flags    The metrics are coming from either  runtime  or  mca  input files that can be specified using    input  flag  By default results are ranked by the measured runtime  mlir hs   Haskell bindings for MLIR     This is an early stage project  All details are subject to arbitrary changes      Note that the  main  branch tracks the current HEAD of  LLVM  and so it is likely to be incompatible with any past releases  We are planning to provide release specifi branches in the future  but only once the API stabilizes  For now your best bet is to develop against MLIR built from source  See the  Building MLIR from source  section for guidance    Building   The only prerequisite for building mlir hs is that you have MLIR installed somewhere  and the  llvm config  binary from that installation is available in your  PATH   a good way to verify this is to run  which llvm config      If that s looking reasonable  we recommend using  Stack  for development  To build the project simply run  stack build   while the test suite can be executed using  stack test     Building MLIR from source   The instructions below assume that you have  cmake  and  ninja  installed  You should be able to get them from your favorite package manager        Clone the latest LLVM code  or use  git pull  if you cloned it before        bash      git clone https   github com llvm llvm project      cd llvm project       Create a temporary build directory       bash      mkdir build    cd build       Configure the build using CMake  Remember to replace   PREFIX  with the directory      where you want MLIR to be installed  See  LLVM documentation       for extended explanation and other potentially interesting build flags        bash      cmake    llvm                                   G Ninja                                Use the Ninja build system         DLLVM ENABLE PROJECTS mlir             Build MLIR         DCMAKE INSTALL PREFIX  PREFIX          Install prefix         DLLVM BUILD LLVM DYLIB ON              Build the dynamic library         DLLVM BUILD EXAMPLES OFF               Save some time       For development purposes we additionally recommend using        DCMAKE BUILD TYPE RelWithDebInfo  DLLVM ENABLE ASSERTIONS ON       to retain debug information and enable internal LLVM assertions        Build and install MLIR  Note that it uses the installation prefix specified      in the previous step        bash      ninja install       Contributing   Contributions of all kinds are welcome  If you re planning to implement a larger feature  consider posting an issue so that we can discuss it before you put in the work    License   See the LICENSE file    mlir hs is an early stage project  not an official Google product
28,Jackneill,Go Micro          Go Micro is a framework for distributed systems development    Overview   Go Micro provides the core requirements for distributed systems development including RPC and Event driven communication   The  Micro  philosophy is sane defaults with a pluggable architecture  We provide defaults to get you started quickly  but everything can be easily swapped out     Features   Go Micro abstracts away the details of distributed systems  Here are the main features        Authentication    Auth is built in as a first class citizen  Authentication and authorization enable secure  zero trust networking by providing every service an identity and certificates  This additionally includes rule  based access control        Dynamic Config    Load and hot reload dynamic config from anywhere  The config interface provides a way to load application  level config from any source such as env vars  file  etcd  You can merge the sources and even define fallbacks        Data Storage    A simple data store interface to read  write and delete records  It includes support for memory  file and  CockroachDB by default  State and persistence becomes a core requirement beyond prototyping and Micro looks to build that into the framework        Service Discovery    Automatic service registration and name resolution  Service discovery is at the core of micro service  development  When service A needs to speak to service B it needs the location of that service  The default discovery mechanism is  multicast DNS  mdns   a zeroconf system        Load Balancing    Client side load balancing built on service discovery  Once we have the addresses of any number of instances  of a service we now need a way to decide which node to route to  We use random hashed load balancing to provide even distribution  across the services and retry a different node if there s a problem         Message Encoding    Dynamic message encoding based on content type  The client and server will use codecs along with content type  to seamlessly encode and decode Go types for you  Any variety of messages could be encoded and sent from different clients  The client  and server handle this by default  This includes protobuf and json by default        gRPC Transport    gRPC based request response with support for bidirectional streaming  We provide an abstraction for synchronous communication  A request made to a service will be automatically resolved  load balanced  dialled and streamed        Async Messaging    PubSub is built in as a first class citizen for asynchronous communication and event driven architectures   Event notifications are a core pattern in micro service development  The default messaging system is a HTTP event message broker        Synchronization    Distributed systems are often built in an eventually consistent manner  Support for distributed locking and  leadership are built in as a Sync interface  When using an eventually consistent database or scheduling use the Sync interface        Pluggable Interfaces    Go Micro makes use of Go interfaces for each distributed system abstraction  Because of this these interfaces  are pluggable and allows Go Micro to be runtime agnostic  You can plugin any underlying technology  Find plugins in   github com micro go plugins         Getting Started   To make use of Go Micro      golang import  github com micro go micro v2       create a new service service    micro NewService      micro Name  helloworld           initialise flags service Init        start the service service Run         See the  docs  for detailed information on the architecture  installation and use of go micro    License   Go Micro is Apache 2 0 licensed  fmt   Different formatting config files config   Configs
29,WesleyyC,JavaScriptChat    This is a chat room program with two JavaScript programs  server program and client program  run by node js    Screenshoot     About Node js   As an asynchronous event driven framework  Node js is designed to build scalable network applications and you can download it from  their website  or use  Homebrew     Usage   Use  cd  directing to the folder where you put the two JavaScript File    Install the net module and readline module       sudo npm install net     sudo npm install readline     Run server by       node   harmony chat server js     Run client by       node   harmony chat client js  username      Notice   To connect to the server  change the line of  chat client js    javascript client   net connect  host  localhost   port  5000    to   javascript client   net connect  host  Server Computer IP Address   port  5000     Contact   Author  Wesley Wei Qian  wesley chin0919 gmail com ImageEditor   An image editor in terminal interface written in Java with flipping and cropping image  invert and reverse color functionality    Screenshoot     Usage   Please ensure you have installed Java with version 7 or above    Run Image Editor   shell sh run sh   Working In Progress      Images blending overlap    Simple filter on the image    Honey   A website for couple sharing their shopping list    P S   Database will be integrated with the server with REST  Lazy Script   Some python scripts I wrote when I feel lazy    In the end of the day  Computer Scienctist don t do things themselves      getFileWeb py   Use wget download all the file  with a specific type  from the base url that is given      Jo s Flash Drive  labelJPG py   Label all the pictures  jpg  in the folder with number starting from 0      Jo s Flash Drive  updateREADME py   A script that change the content in a secret file for Jo  It will automatically update the date  locations  numbers given the status of the folder  SpeechRecognitionScript   Some of my python scripts used in COSI 115AJ Speech Recognition and Dialog Design    You are welcome to use and change them      changeRate py   Use sox change the rate of all the  wav file in the current directory      checkRate py   Use sox check the rate of all the  wav file in the current directory      cleanID py   Format the ID in the hypotheses to cater the format rule of SCLite      createDic py   Create a list vocabulary use in the grammar      getFileName py   Get a list of the name of the  wav file in the current directory      hypCreater py   Grep the hypotheses from all the  emma file sended back from the AT T Meshup      loopFile py   Send all the  wav file in the current directory to the AT T Meshup and get the hypotheses in emma format    P S   This code is written when I first encounter my love Python  so some of them should be cleaner and in better format  Tic Tac Toe   We are currently working on a raspberry pi intergration to bring this virtual board game to an actual board game    A perfect Tic Tac Toe player using the strategy in Newell and Simon s 1972 tic tac toe program  http   en wikipedia org wiki Tic tac toe Strategy     Feel free to play around with the code and let me know if there are any bugs    Software Setup   Python Version   Please make sure you are running Python 2 instead of Python 3    Run the standalone game without a Raspberry Pi   python  project  original code tic tac toe py   Run the game on Raspberry Pi     Open up terminal in Raspbian   Start Python 2 shell with IDLE as Root so that you can access GPIO on the board     sudo idle         Open up the project in an editer  File    Open      pi interface py   Inside the editer  hit F5 to run      Hardware Setup   We are currently setting up and connecting the game board using a breadboard and jumper wires  After we are done debugging  we will see if we can get a PCB and solder the components in    Circuit Components    Component Count             Bi color LEDs 9   push buttons 3   Resisotr  330 Ohm 1   Jumper wires At least 19    Circuit Schematics   Coming soon      Most of the code is finished in less than three hours  so there are a lot of if else statment and not perfectly structured  So pardon me for the mess and feel free to fork modify the code  Google Code U   The repository for sharing my answers on Google Code U Program s Problem Set     Problem Set 1 Update 03 24 15    Problem Set 2 Update 04 09 15    Problem Set 3 Update 04 22 15    Problem Set 4 Update 05 14 15 Kaggle  Restaurant Revenue Prediction   Competition Detail  https   www kaggle com c restaurant revenue prediction   Write Up  https   docs google com document d 1KU uzsLLz53S7SwKKnEjk6olbyGaEYRHjF zv 5HuMk edit usp sharing   Author  Ari Ben Elazar  Will Burstein  Wesley Wei Qian    Final Rank   Rank  38th 2256   2     Approach Records   The models we build is seperated by folders  more detail will come soon    Data Description   File Description     train csv  the training set  Use this dataset for training your model     test csv  the test set  To deter manual  guess  predictions  Kaggle has supplemented the test set with additional  ignored  data  These are not counted in the scoring    sampleSubmission csv  a sample submission file in the correct format     Field Description     Id   Restaurant id     Open Date   opening date for a restaurant   City   City that the restaurant is in  Note that there are unicode in the names     City Group  Type of the city  Big cities class  1  in our processed data   or Other class  0  in our processed data      Type  Type of the restaurant  FC  Food Court class  2  in our processed data   IL  Inline  class  1  in our processed data   DT  Drive Thru class  3  in our processed data   MB  Mobile class  4  in our processed data    P1  P2   P37  There are three categories of these obfuscated data  Demographic data are gathered from third party providers with GIS systems  These include population in any given area  age and gender distribution  development scales  Real estate data mainly relate to the m2 of the location  front facade of the location  car park availability  Commercial data mainly include the existence of points of interest including schools  banks  other QSR operators    Revenue  The revenue column indicates a  transformed  revenue of the restaurant in a given year and is the target of predictive analysis  Please note that the values are transformed so they don t mean real dollar values     Scheme Evaluator   This is a modified version of evaluator for Scheme based on the book STRUCTURE AND INTERPRETATION OF COMPUTER PROGRAMS  SICP    Scheme   Scheme is a minimalist  multi paradigm dialect of Lisp  It s  simple and elegant     Install     There is a version provided by  GNU   Or you can install  DrRacket     The Evaluator   This is a modifeid version of the Scheme interpreter written in Scheme which support lazy evaluation    How to Run   To run the evaluator   shell scheme  load mceval scm   The Lazy Evaluation   In Scheme  if we define the following  try   Scheme  define  try a b     cond     a 0  1     else b     If we run   try 0    1 0     the system will throw me an error because it will evaluate     1 0    even though it is not used in the  try  function    However  in my modified version  user can choose to delay some of the argument which can improve computation efficiency and provide more flexibility in coding  For example  if we define  try  in this way in the new evaluator   Scheme  define  try a  delayed b      cond     a 0  1     else b     and now if we run   try 0    1 0    again  the system will simply return  1  instead of throwing an error    A Different Version of Stream   In Scheme  there is a built in  stream  which is similar to  cons  but with a delayed second argument  Now with the new  dealyed  tag  we can build a  new cons  which inherits the built in  cons  but add a  delayed  tag for the second argument    Usage    Scheme  define  integers from n     new cons n  integers from    n 1      which will give us a function that can return a infinite list of integers starting from n    This is a great example of how delayed argument can provide better computation efficiency    Other functionality   Besides the  delayed  tag  there are other improvement from the original meta circular evaluator introduced in SICP     let  which is the same as the built in  let     let   which is the same as the built in  let      make unbound  which will unbound the variable in the current scope with its associated value Spatial Pattern Recognition   Wesley Wei Qian on 03 09 17   Update   I have made some improvements in the underlying graph matching alogrithm and attribute update procedure  There are some siginificant code changes from V1 0  but I haven t gotten the time to update the doc yet  The improvement is huge and we are able to separte graph with embedded pattern from random graph with sharp difference      Contents     Introduction     Code Explanation     Basic Component   Usage   Advanced Usage   Some Implementation Detail       Result       Introduction        This package implments a probabilistic parametric model which can be trained to operate soatial pattern recognition task on ARGs  which comes from  the idea of P  Hong   T S  Huang          The package is implmented in MATLAB using OOP and it is very easy to change the converging function  matching compatabiltiy function for different tasks  e g   image video retrieval  understand chemical comounds structure  discover gene regulation pattern  etc     Code Explanation        In this section  I will explain the basic structure of the code base and how to use them    Basic Component       Class Components       sprMDL m      which is the most important class representing the trained model       The constructor will take a cell array of sample ARGs and the number of components and start the training     The model is trained using an EM algorithm  whose converging condition and maximum iteration can be changed    It has a number of model ARGs representing different component in the model and different weight associated with them    Once the model is built  you can ask for the pattern that is being summarized in the model or if a new ARG has a similar pattern of the given sample ARGs  and if so  what is the pattern ARG           ARG m      which represents the basic  sample  ARG with a cell array of Nodes and a symmetric cell matrix of Edges       It is created with a full  NxN  symmetric cell array representation of the graph  where each cell consists of the vector attributes associated with the edge  and a  N  length cell array of nodes attributes          node m      which represents the node of a basic ARG       It is created in the basic ARG construction  with an ID  construction order  and an attributes field          edge m      which represents the edge of a basic ARG       It is created in the basic ARG construction  with the edge attributes  two IDs of the edge endpoint and a cell array of Nodes  this constuction can be simplify           mdl ARG m      which is very similar to the ARG m but represents an individual componennt in the probabilsitic parametric model and has more functionality      It is created with a basic ARG as each component in the model is first initalized as a sample ARG    Different than the basic ARG  model ARG can modify its structure  update nodes edges accroding to the main model class          mdl node m      which inherits from the  node m  class and represent the node in the model ARG      It is created with a basic node during the construction of model ARG    It has the ability to update its attributes vector and calcualte the inverse of its attributes          mdl edge m      which inherits from the  edge m  class and represent the edge in the model ARG      It is created with a basic edge during the construction of model ARG    It has the ability to update its attributes vector and calcualte the inverse of its attributes              Function Components       graph matching m      is another important component which return a matching matrix denoted the matching score between nodes of sample ARG and model ARG      Graph Matching Subgraph Isomorphism problem is a long known NP problem  so to solve the probelm we use  God and Rangarajan s graduated assignment approach  to get a good approxiamation efficently    An individual implementation of such algorithm can be found in the     GraphMatching  folder  where the function will take two basic ARG instead of a basic ARG and a model ARG as required by the model training here          converege m      is a function associate with  graph matching m  as it determines if the training result converges      User can chagen this function with iteration variable  I   and converging variable  beta   in  graph matching m  to decide the accuracy of the matching score returned from  graph matching m           heuristic m      is another function associate with  graph matching m  as it is used to decorated the result from tha matching function      For our purpose  we simply return what the algorithm returns  but for Graph Matching Subgraph Isomorphism problem  such function can be used to pick the best matching between two ARGs  as written in     GraphMatching heursitic m           edge compatability m  and  node compatability m       is the function calculate the matching score between nodes and edges according to their attributes and covariance matrix      In our implementation  we assume both node and edge follow a Gaussian distribution  but this can be changed accroding to the application scenario          mdl converge m       is a fucntion associated with the main model class  sprMDL m  and it determines if the model is converged      By changing this function and iteration variable  iteration EM  and converging variable  e mdl converge  in  graph matching m   user can define the training accuracy of such model              Test Demo Components       test m      is a simple setup to demo the model  where as you can notice  the nodes attributes can be vecotr        ModelTest m      is a random test script to test the accurracy and efficiency of the model      The test first build up a base pattern and buil some training samples with such pattern  The model is them trained with these samples and test against some test samples with such patterns and some samples genreated randomly     User can show the pattern of the model and its componenets by making variable  view pattern  to  1  instead of  0     User can also change other variables  e g  pattern size  number of components  pattern connected rate  in the script for different testing purposes             GraphMatching test m      is a simple setup to demo the graph matching algorithm           GraphMatching RandomGraphTest m       is a random test script to test the accuracy and efficiency of the graph matching function      The test first build a random graph  then create another graph by permutating the first graph  nodes order and attributes   The matching correcteness is then being calculated    User can also change other variables  e g  number of rounds  the size of the graph  noise rate  in the script for different testing purposes              Usage        Despite all the complex components  the usage of such model is rather simple  Assume you have      a set of training sample ARGs listed in a cell array  sample ARGs     an ideal number of component for the model  number of component     a test ARG that you want to know if has the similar pattern as the set of samples  test ARG            Then to use the model  you can do the following          We first train a model by sending it the samples and the number of component    and a model will be returned    mdl   sprMDL sample ARGs  number of component        Then we can ask for the pattern that is summarized in the model    and a summarized pattern will be returned as an ARG    in the mean time  a biograph will show up to visualze the pattern    summarized pattern   mdl summarizedPattern         Besides showing the whole model  you can also ask to show a specific component  the ith     and the function will return a structure consisting the information of this model    including nodes attributes  nodes frequency  edges matrix    as well as a visualization component bg that can be viewed as biograph    component struct   mdl mdl ARGs i  showARG    view component struct bg        We can also check if the test ARG has a similar pattern    and a boolean value we tell us if the test ARG has a simialr pattern    tf   mdl checkSamePattern test ARG        If you are greeedy  you can also ask for which part of the test ARG are similar to the sample ARG    If there is indeed a similar pattern  the pattern will be returned as an ARG    Otherwise  NaN will be return   the similar pattern   mdl getSamePattern test ARG            Advanced Usage        Since the implementation is written in OOP  it can be modified easily  However  reading through the  implementation detail  in the following secation is highly recommended         As I mentioned in the basic component section  user can modify the edge node compatability functiona and the converging function for both the graph matching algorithm and EM algorithm during the model training  However  there are other properties that the user can tune to fit their application best         For the spatial pattern recognition model  user can change the following constants in  sprMDL m             properties  Constant            Maximum EM algorithm running rounds             iteration EM   30          Converging epsilon  the converging thredshold      e mdl converge   1e 4         Structure Modification Setup       We don t want to delete nodes in early stage       so choose such number carefully       The thredshold is set up as e delete base   e delete iter iter         e delete base   1          e delete iter   0 5  end             Moreover  in the current implementation  the model use the lowest matching score from sample to set the thredshold for judging if a new ARG has the same pattern  Such thredshold setting system can also be changed according to user s application         For the graph matching probelm  user can change the following variables  graph matching m               beta is the graduaded assignemnt update         beta 0   0 5          beta f   10          beta r   1 075      I control the iteration number for each round     I 0   4      I 1   30     e is the converging epsilon thredshold     e B   0 5      e C 0 05     node attriubute compatability weight in the score     alpha   0 1                 Last but not least   even though some implementation is discussed in the following section  understand the underlying idea of  the spatial pattern recognition model  and  the graph matching  is very important  so pelase take a look at these two paper    Some Implementation Detail     Null Edge Node Representation     During the development of this project  we found that matrix operation on  NaN  value is much slower than  0  value  so we decide to use  0  value to represent a null value  or a non existen edge or node  In the compatability functoin  if the function detect a zero  it will return a zero score so that a non existen edge node would not have any impact on the result    However  if you do have a zero attributes for your existen edge node  a good get around can be adding a small constatn to all your edge node attributes and then set tha  NaN  value to  0         Edge Compatability Calculation during Graph Matching   As you might notice  we implemented two methods to calculating the edge compatability depends on the connected rate of the graph    The first way is we go nodes by nodes in graph to pariing four nodes  two nodes for each graph   If we have a graph with  A  nodes and another graph with  I  nodes  this operation can be as expensive as  O A 2 I 2    Thanks to the effecient matrix operation in MATLAB  we can convert this potential four  for  loop operation into simple matrix operation     However  a lot of time will be wasted if the graph has a very low connected rate and has very little edges  So we discvoer the second way  where we only calculate the matching score for existen edges using a sparse matrix    Now you might think the second method can definitely saves time  but it is not true  Since we need to use some nested for loop in the second method  if the connected rate is high enough  the first method can actually beat the second method  That s why we only use the second method when the connected rate is less than  0 4  in the code        Node Compatability Weight   In the code for the graph matching function  there is a variable  alpha  that can weight the node compatabiltiy score and you can see it as a control to how much the node compatability impact the algorithm against the edge compatabilyty    For example  if you have a lot of nodes that has the same attribute while there are only a few number of edges  The edge compatability will be much more important during your graph matching task  In this case  you would like to lower the  alpha  so that the algorithm can focus on the edges    However  if your graph has a lot of similar edges while the variance for node attributes are very high  you would like the algorithm to focus more on the node as against the edges  In this case  you would like to make  alpha  larger        Model Modification During the Training   As I mentioned above  when we initialize our pattern recognition model  we initialzied our components by randomly picking samples  Therefore if we didn t do any strutrue modification on our components  our components can have much more nodes than it needed to represent the summarized pattern  which means more background noise is introduced    To deal with this case  we add a structure modification step so that we can cut the unnecessary node  or less weighted node in each EM iteration  To do so  we set up an thredshold score and change this thredshold score at each iteration so that the cutting rule is more ane more strict after each iteration    However  we don t want to cut nodes at the beginning of each training since our initialization can be wrong and the node we cut can be an important representation of the model        Component Weight and Component Nodes Number   During the development  I found out that the model tends to give a higher weight for the component with more nodes  even though the pattern they need to summarized is far less than it  It makes sens because a componennt with more nodes simply has a better chance to match somthing from our sample patterns    However  in this case  the model might say yes to some random pattern instead of the one they summarized  Therefore  we introduce a normalization in the calculation of sample component matching score  where we divdie the score by the  log  of the component s size     Therefore  we can make the components more fair to the components having less weight and taking a  log  can make such change smoother        Implementation Efficiency   As you can imagine  EM algorithm training and graph matching problem can both be very expensive task in terms of time efficeincy  In the graph matching function  we use a lot of matrix operation so that MATLAB can optimize the runtime for us  However  during the development of the spatial pattren recognition model  we found it hard to make matrix representation of our information so there are some huge nested  for  loop in the code  which can slow the running time    So this is definitely an improvement we can continue to work on         Result        Even though more advanced test might be needed  here are some of the result we currently have         For the spatial pattern recognition model  it takes about  5 hours  to train a model with a 10 node pattern  fifty 20 node sample  five components  4  connected rate on an average commercial machine  Then the model is test on fifty testing samples with the patterns and fifty random samples may or maynot have the patterns  The recognition rate is  92   compared to a  56   recognition rate for the random samples         For the graph matching function  it takes about  30 minutes  to run a match on two ARGs with 100 nodes and a 5  connected rate permutating in 10  noise  We run 10 rounds of such match and the average correct rate is about  99    ContactTransfer   As part of the Google CodeU program  we create Contact Transfer       ContactTransfer is an easy way to  transfer contact information between to phones with a touch     We currently support phone number  email  and Facebook  With both QR and NFC capabilities  no matter if you are in the dark or in a remote area  you can count on CT to get you the information you need to keep in touch with new friends  i e  THE guy or THE girl you just met     Presentation  http   prezi com sgfdjd57h3aw  utm campaign share utm medium copy rc ex0share   Design Doc  https   docs google com document d 1kVSDI9YRPA6fDIUUMJJCfEAUMivI9Fpr0ir7c7eZ9Mo edit hl en forcehl 1 heading h x958gwj7e0s1   Trello Board  https   trello com b 0tDuUtOF contact transfer Protein Domain Learning   Pattern learning for protein domain SEEDHack   SEED Hackathon Project  Android Application for uploading water quality data in a SNS context   Application Demo   https   youtu be EN9ktS1mCvU   AndroidManifest s Google Map API KEY is masked  Please provide your own API key    Google Map API is funky on Android Studio Emulator  Test on an actual device is recommended  Novels Punctuation   Inspired by the medium post   Punctuation in novels    I start to wonder what it feels like to read something just with the punctual  No words  no number  Just punctuations    So I write this python script in a cafe to help convert a PDF format reading into txt format only with the punctuation     Setup   First we need to setup the pdf reading component by     cd NoWords   python pdfminer setup py   Usage   To convert the file  do        python NoWords py      for example     python NoWords py sample AAW pdf       and the output txt file should be in the same path of the input file    Output as PDF    I have tried to convert the txt to PDF directly in python  but it is not as nice as using Mac TextEditor to export PDF file   If you want to use the python pdf converter  you can set the following flag in  NoWords py  to True   pdf   False   Let s look at  Pride and Prejudice      Amino Acid Embedding   Word2Vec is thriving in computation linguistic as it helps us getting spatial semantic from word and better similarity function between word and word    Can we use the same training methodlogy else where    In Bioinformatic  a very important task is protein sequence alighnment and such alighnment depends a whole lot on how we calculate the similarity of two amino acids are  So in the 90s  Henikoff and Henikoff developed a database of  blocks  based on sequences with shared motifs   2 000 blocks of aligned sequence segments from  500 groups of related proteins   Based on this data  people developed a matrix called BLOSUM to represent the similairty between amino acid    However  time flies and scientist are able to generate a lot of protein amino acid sequence with the new technology  but no one has touched the BLOSUM matrix yet  So we propose to use the same methodlogy in word2vec training to train a vector representation of each amino acid  which consequently can give us a similiarty score between them    t SNE Result     Amino Acid Categorization   Art in Code    Making figure is very often in our day to day scientific research  With such great amount of figures  I found some of them very artsy and they bring me extra pleasure when I am tired staring at the screen  Therefore  I start this repo to curate these awesome figure I made and I hope you will enjoy it too    Mondrian s     This graph is made when I tried to show the matching score between two attirubted related graphs  The black and white boxes remind me  Piet Mondrian s work     Horse Tail     This graph is made when I tried to compare some normal distributions result  It s a great color and it looks like a horse tail  The pixel points present a great texture    Play Station  in candy flavor      This graph is made when I tried to use t sne to visualize my amino acid vector representation with a zoom in effect  I use different shape and color to represent different amino acids and they turn out to be the key in our Play Station controller  with a more colorful vibe    Without Words     This graph is made part by purpose as I want to see what a Shakespear script would look like if I pull out all the words  While there is no actual substance  the comma  question mark and exclamation mark make up a little emotion in the piece  I find that amazing  Read  Love  Pray     This is a list of papers  bookmark tabs  that I particularly enjoy and would love to share with you  If you have papers that you want to share  just put it down as an issue and I will update the list    Table of Contents         Machine Learning           Neuroscience         Distributed System         Machine Learning       A Few Useful Things to Know about Machine Learning       This article summarizes twelve key lessons that machine learning researchers and practitioners have learned  While these lessons are simple to understand  I learn something new everytime I read it        Deep learning       This is a review paper by Yan LeCun et al  and it gives a good general picture about deep learning  In my opinion  what a wikipedia page should look like after the summary section        Dropout       Regularization is a very important component in machine learning  This paper in particular  talks about the regularization challenge in deep learning  While deep learning in some way mimic human s brain strucutre with layers of process  it does not provide the flexibility in structure as human brain do  This paper describes a simple way to create a robust deep learning model that allow some of the  neurons  turn down during the reading  What is particularly beautiful about this paper is that it starts with a biology motivation and end with a great mathmatical solution for machine learning        Go Game and Deep Learning       This is a very high profile paper  While the concepts in the paper are not new  the way Deepmind teams put them together is an engineering gem  Plus  it is a interesting challenge as Go is not an easy game          Distilling the Knowledge in a Neural Network       Simple tricks can help improve performance significantly but they usually come from insightful observation        Generative Adversarial Nets       So Jerry is trying to generate images to trick Tom    This is a theoretical sound paper shedding lights towards a new path to deep learning  Let s generate and make inference while rationalizing the network        Steerable CNNs       Rationalize and understand CNN via group theory  It s math heavy  but isn t ML all about math          Tree LSTM       A paper about generativing sentence embedding via a LSTM with tree structure  I like this paper because it gives a great intro to the related work and generalizes to the classical sequential LSTM  Beautifullly formulated          Neuroscience       A Quantitativer Description of Membrane Current       This paper far before we knew anything about ion channel in neuro signaling  but via rigorous experiment setup and amazing imagination  Hodgkin and Huxley formula the ion channel behavior which still makes sense today   It does take a while to get used to their notation since they used different sign for polarization and hyperpolarization           Transient Dynamics versus Fixed Points in Odor Representations by Locust Antennal Lobe Projection Neurons       How PCA  a classic feature reduction techniques in ML  could help us visualize neuro coding and reveal the  state  coding regime        Cortical Neural Ensemble Responses Emerges Suddenly       By the same token as the above paper  how hidden markov model could be used to explain neuro coding  specifically temporal coding        Distributed System       Paxos made simple       This is not a paper but a very simple and clear explanation for Paxos  which is considered to be difficult to understand  I think Paxos is a super neat idea to replicate nodes and imagine a bunch machine vote about what they want to do together is hilarious  While not very efficient communication wise  forcing partition overlap using majority is so simple in math and so powerful in system        Dynamo       Dynamo is an interesting paper for me because it shows what engineering really is  Amazon has a unique need to make sure all the put operation to went through with a 99 9  SLA and they specifically design a system for it  This is also a very well written paper with good description of related work and their motivation  which shows a great deal of engineering mind set  The ring hash   vitrual node idea is also very cute and actually inspire some  other interesting distributes system   Uber for realtime dispatching   which again  is an unique problem with a specifically engineered solution        ZooKeeper       Love open source work and good modulization  While there are other synchronized core out there  I find reading this paper entertaining because it shows how we can use a simple primitive to implement other complex system        ZooNet Composition of Synchronized Core       This is a relatively new paper continue the topic of ZooKeeper  While synchornized core is simple to use  it is hard to put them together and have multiple applications with their own ZooKeeper instance synchornizing with each other  However  this paper used a incredibly simple client layer code to fix the problem  150 lines of code  and that really captures the beauty of solving system problem  Moreover  they find a  bug  in the ZooKeeper after 3 x version iteration and improve the performance  again  with some very simple code idea        MapReduce       I am a big fan of functional programing so reading this paper is like an orgasm with the  map  and  reduce  function  It is neat that Jeff Dean and his wingman  Sanjay Ghemawat  decided to think of distributed computing problem as map and reduce and endup modulized the idea with cute engineering refinement discussion        Spanner       Another paper from Google Research  This is a super complicated system including Paxos  time clock etc  However  it really push the boundary about what s the best we can do with the most advanced technology and knowledge in system s world  Its a good engineering lesson to see how all the piecies are putting together and make sense        Farm       Well  yet another paper yells  We want it all    This paper is super cool with its  one sided  disk read  i e  control other computers  disk read write to memory without interupting their cpu  and memory cahce database  which is possible with a special battery that can move memory into disk during sudden power failure   We want it all  Sahnnon s Ultimate Machine on his 1100100     Inspired by the late artificial intelligence pioneer Marvin Minsky  he designed what was dubbed the Ultimate Machine  flick the switch to  On  and a box opens up  out comes a mechanical hand  which flicks the switch back to  Off  and retreats inside the box  Shannon s home  in Winchester  Massachusetts  Entropy House  he called it   was full of his gizmos  and his garage contained at least thirty idiosyncratic unicycles one without pedals  one with a square tire  and a particularly confounding unicycle built for two         New York Times    I found such idea fascinating and it has an incredible sense of humor  Therefore  I create this webpage and it will open a pop up window as the Ultimate Machine and the window will close itself after three seconds              The beauty of this machine it can close itself  so I would like to really create a page that can actually close itself  However  for obvious secruity reason  modern browser does not allow such thing unless with so unsustainable hack so I end up using this page as a parent page to launch the machine as a child page and close it with this parent page      NOTE    The machine will automatically open when you open up this page    If the machine does not load  please enable the pop up window for this page on you browser    Graph Matching   This is a graph matching algorithm implmentation of a graduated assignment algorithm for graph matching using OOP scheme in MATLAB    To generalize and recognize spatial pattern  a probabilistic parametric model is built  To register a sample ARG or check a test ARG  a graph matching probelm is presetend  However  graph matching is a long known NP problem  so a lot of algorithms are derived to get an approximated solution in a reasonable time  In this project  we mean to use one of this algorithm to help us handle graph matching problem efficiently    According to Gold and Rangarajan  a graduated assignment algorithm is developed to solve the problem efficiently and the code here is an attemp to implment and improve their algorithm mentioned in their paper  https   www cise ufl edu  anand pdf pamigm3 pdf   The improved algorithm allows null matching to propagate which in terms improve the precision of matching result while maintain the recall and the additional noise prevent matching to local optimal    Test Result       No Null Propagation  old algo   red     Recall  0 75 Precision  0 72 F  0 73 Size  40 Connected Rate  0 2 Noise Rate  0 1           Null Propagation   Stochastic  new algo   blue     Recall  0 98 Precision  0 92 F  0 95 Size  40 Connected Rate  0 2 Noise Rate  0 1           Comparison   round  100 p recall   10  6 p precision   10  6 p F   10  6       DT In The House   Make Donal Trump Speaks Again    a stack LSTM RNN that speaks like Donal Trump by generating character one at a time    Run the Program   RNN traning is generally computationally expensive  so you should run the program with GPUs if possible       python   run the following program to   1  generate a index   2  generate a model   3  generate a sample speech   generate a model   python train model py   choose the best model from model tmp folder and name it model DT hdf5   generate sample speech from the model   python generate speech py         Sample Output     Remark     The input for the model is character not word  so that s why you can see some typos and jeburish in the text  However  in most of the cases  the model is actaully able to learn English  which is quiet amazing    The model is capturing some phrases like  hillary clinton  and  thank you  and god bless      I ran into the problem where the RNN generate a repeated pattern if I stick with the softmax result  Therefore  I adjust the softmax result with a diversity factor and run a multinomial instead      Special Note      The project is inspired by Andrej Karpathy s note on  how effective RNN could be     RyanMarcus  s  Edgar Allan Poetry  has inspired me to keep track of char index and add a diversity factor with the multinomial    Senior Thesis   One of the most amazing capabilities of human beings is to extract common spatial patterns from observations and use these patterns to make inferences   recognizing a car by summarizing the important components of cars  like rims  windows  trunks etc   and their spatial relationship while ignoring the specific design of different cars    In our work  we turn a set of spatial representations into a set of attributed relational graphs  ARGs   which consist of nodes and edges with a vector representing their individual features  We then train a probabilistic parametric model that extract the common sub graph of the ARGs set    Our key contributions are 1  introducing a stochastic process to the graph matching step which utilizes a graduated assignment algorithm  2  adding a null node network in each ARG to avoid matches among background nodes  i e  nodes not in the pattern   We also apply the model to crystallography protein structure data to learn the common structure among proteins that share a certain function  To utilize the general algorithm to our protein data and model the structure data as ARG  we 3  introduce an additional term in our objective function rep  resenting the protein backbone  and 4  use a local substitution vector to model the similarity between two different amino acids based on their specific local environment    Graph is a powerful representation and can be used to model a lot of things like neuron morphologies and social networks  Utilizing such algorithm can help machines to understand many of the patterns in real life  and human to mine pattern in large scale    Link to the thesis  fMRI Classification   Multi Class Classification on fMRI data for different body activiies   The final model is implemented in Tensorflow with layers of 3D convolution  fully connected  and dropout etc  Nick Smith ing   An visual effect project inspired by artist  Nick Smith     Here are some of his work          Here are my visual effect with average blurring          Here are my visual effect with Gaurssian blurring          Todo       x  Customized Box Size    x  Customized Border Size    x  Gaussian Blurring Effect       Face Blurring Only with Face Recognition   Symbolic RXN   Integrating Deep Neural Networks and Symbolic Inference for Organic Reactivity Prediction   ChemRxiv    Code for the inference pipeline is here  and the complete training pipeline will be updated once the paper is accepted    System Requirements     conda 4 7 10   from  here  with Python 3 6 9    tensorflow gpu 1 12 0   from conda  and GPU is recommended    rdkit 2019 09 1   from conda    gurobi 8 1 1   from conda  and free academic licence can be found  here     h5py 2 9 0  from conda    other softwares required by the above packages     Installation Guide   Follwoings are the intructions for installing our software and the dependencies specified above       bash   the total installation time should be less than 15 minutes   download the source code   git clone https   github com WesleyyC symbolic rxn git cd symbolic rxn   create a py3 conda env   conda create  n symbolic rxn python 3 conda activate symbolic rxn     install the dependencies   tensorflow gpu   conda install tensorflow gpu 1 12 0   RDKit   conda install  c conda forge rdkit 2019 09 1   gurobi   conda config   add channels http   conda anaconda org gurobi conda install gurobi 8 1 1       Data Preprocessing   To better balance the GPU CPU workload during training inference  we precompute the graph features and store them in HDF5 format for fast random query  Followings are the instructions for digesting the raw text input into our graph feature representation       bash   prepare the data   unzip data zip   digesting a small demo dataset w  2000 reactions   30 secs    python  m reactivity prediction data digestion   input data demo txt   output data hdf5 demo    digesting the full test dataset   10 mins    python  m reactivity prediction data digestion   input data test txt   output data hdf5 test       Downloading the Pre Trained Models   We provided the pre trained models for reproducing our Top K performance on the USPTO dataset       bash   download the ensembles models   wget https   www dropbox com s x8ruovxxsc5q7q6 ckpt zip   unzip the models   unzip ckpt zip       Running a Sample Pre trained Model on the Small Demo Dataset   Followings are the instructions for running a single pre trained model on the provided demo dataset       bash   reactivity prediction w  existing ckpt   1 mins with GPU    python  m reactivity prediction run model       mode infer   ckpt ckpt mdl 0       eval input data hdf5 demo   eval output demo output   evaluate reactivity prediction   30 secs    python  m reactivity prediction eval   input demo output delta predictions pkl   and the program will report the bond changes coverage                                                                                  Reaction Bond Prediction                                                                                 as well as the delta prediction accuracy F 1 socre                                                                                      Delta Evaluation                                                                                     run symbolic inference and evaluation   30 mins with multi cores    python  m octet sampling run sampler   input demo output delta predictions pkl    and the program will report in the last row with the Top K prediction accuracy                                          2000 of Reactions Evaluated                                        Gurobi  Top1      Top2      Top3      Top5      Top20      Average Time              Reproducing the Full USPTO Test Dataset with the Ensemble Models    Followings are the instruction for reproducing our reprorted Top k prediction accuracy on the USPTO test dataset using the ensemble models       bash   running the following script to genereate ensembled predictions   80 mins or  10 mins model    bash reactivity prediction run ensemble inference sh   evaluate reactivity prediction   10 mins    python  m reactivity prediction eval   input ensemble output delta predictions pkl   run symbolic inference and evaluation   1 day    python  m octet sampling run sampler   input ensemble output delta predictions pkl       Contact     Wesley Wei Qian  weiqian3 illinois edu    toy    A  guide  to our little toy  island in animal crossing the new horizon
30,neocxi,Greyjoy Octopress Theme   A simple white and grey colored theme for Octopress  Named after the messed up Theon Greyjoy    Demo   See http   reza re   Install     cd  themes   git clone git   github com rezajatnika greyjoy git   rake install greyjoy    rake generate   Notes   Issues   The sass files still messed up  hope I will clean it later  stat133  PixelSNAIL   This is a Python3    Tensorflow  implementation  of PixelSNAIL    This code base is based on OpenAI s  PixelCNN    code    Setup   To run this code you need the following      a machine with multiple GPUs   Python3   Numpy  TensorFlow     Training the model   Use the  train py  script to train the model    Pretrained model checkpoint   You can download our pretrained  TensorFlow   CIFAR10 model  and  ImageNet model   CIFAR10   python train py            data set cifar            model h12 noup smallkey            nr logistic mix 10            nr filters 256            batch size 8            init batch size 8            dropout p 0 5            polyak decay 0 9995            save interval 10   ImageNet   python train py            data set imagenet            model h12 noup smallkey            nr logistic mix 32            nr filters 256            batch size 8            init batch size 8            learning rate 0 001            dropout p 0 0            polyak decay 0 9997            save interval 1
31,pfista,comparch   Each folder at root is a separate programming assignment for Computer Architecture CS 429 pfista s  vim files   Installation   Clone the repo to your home directory   cd   git clone https   github com pfista  vim git    Create a symbolic link to the  vimrc stored in  vim  You may need to rename your old  vimrc to something different   ln  s  vim  vimrc  vimrc    To install all the bundles just open up vim and do a   BundleInstall   Other   Go ahead and fork this if you would like to make your own  vimrc customizations or add and remove different bundles and colorschemes etc  About   GaragePi is a simple application that uses a Raspberry Pi along with   socket io  and  node js  to toggle a  garage door opener    GaragePi provides a web interface to toggle a gpio pin to send voltage to a garage door opener  effectively simulating a button press      pfista io   pfista io is a theme for octopress  forked from  macjasp    Install               cd octopress       git clone git   github com pfista pfista io git  themes pfista io       rake install  pfista io         rake generate    Font   The font is currently using Proxima Nova from Typekit  if you don t set this up the fall back is Helvetica Neue    If you spot any errors then let me know  alternatively take it on and make it better    License    The MIT License    Copyright   2013 carmo org uk   Permission is hereby granted  free of charge  to any person obtaining a copy of this software and associated documentation files  the  Software    to deal in the Software without restriction  including without limitation the rights to use  copy  modify  merge  publish  distribute  sublicense  and or sell copies of the Software  and to permit persons to whom the Software is furnished to do so  subject to the following conditions    The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software    THE SOFTWARE IS PROVIDED  AS IS   WITHOUT WARRANTY OF ANY KIND  EXPRESS OR IMPLIED  INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT  IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM  DAMAGES OR OTHER LIABILITY  WHETHER IN AN ACTION OF CONTRACT  TORT OR OTHERWISE  ARISING FROM  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE  hwgen   Generate a homework template file for Norman s CS439 course    I made a simple script I m going to use this semester to create homework files for submission   It creates a template file according to the file format and file naming conventions listed  here     Installation   Setup   Download the generate homework sh by itself     curl  O https   raw github com pfista hwgen master generate homework sh   or  clone the repository like so      git clone https   github com pfista hwgen git   then just make the script executable   chmod  x generate homework sh   Usage   To use it first open  generate homework sh  and edit the variables at the top with your personal info    Then just call the script with the homework number as an argument   For example  to generate your homework  1 file      generate homework sh 1   Use at your own risk   Pull requests welcome  This repo contains programming projects for CS 331 Algorithms and Complexity with Tandy Warnow HiddenFileToggle   A status bar app to toggle hidden files in Finder    Trust  aka Wufi       An experimental p2p  reputation  crypto currency   The general goals      users can rate the actions and content of other users  on or offline  by sharing or nixing Wufi with a user   users can then see any other user s rep from their own perspective in the network   users can spend and earn Wufi through connections made by sharing Wufi     Stated  inherent features      collusion resistant  you cannot receive Wufi for stuff from users you have not directly or indirectly boosted nixed  so making false accounts will not increase your Wufi Score   sybil attack resistant  boosting any user s Wufi Power only gives that user a higher proportion of your total boosted amount  unlike Facebook  likes  or Twitter  retweets  and other spammable voting systems   this means giving a user trillions of Wufi only devalues the social capital of everyone else you have previously boosted   self promotion and slander resistant  creating fake identities only increases decreases your real account s Wufi score from the spam account s perspective  if these spam accounts are never boosted nixed by real users  then the spam account s perspective provides no influence to any other real user         while your reputation within a context may be negative  the worst total  Wufi Score you can have is 0     Future  desired  plans features        Incorporation with some p2p or federated social networking platform  probably twister  or pump io or Trsst  for public sharing of gifts  nixes  payments and contextual hashtags       Some terms       yet to be finalized      sharing    raises a user s rep  increasing their wallet and your pocket balances   nixing   lowers a user s rep  decreasing their wallet and your pocket balances   wallet   the amount of Wufi a user can spend without drawing on their pocket balance   pocket   the net total amount of Wufi a user has shared and nixed     new temporarily placed musings      sharing raises your rep    more whuffie to spend in more of the network     nixing lowers your rep    less whuffie to spend in less of the network   unlike cash  where there is a single issuer and so the game is to acquire as much as possible  everyone can create whuffie  so the game is about getting as much as possible from as much of the network as possible       access vs ownership     tipping a user gives them more whuffie to spend in more places more of the network   Curfew   Time based location sharing for Android   More info on the  project page Welcome to GitHub Pages   You can use the  editor on GitHub  to maintain and preview the content for your website in Markdown files    Whenever you commit to this repository  GitHub Pages will run  Jekyll  to rebuild the pages in your site  from the content in your Markdown files    Markdown   Markdown is a lightweight and easy to use syntax for styling your writing  It includes conventions for      markdown Syntax highlighted code block   Header 1   Header 2   Header 3     Bulleted     List       Numbered     List     Bold  and  Italic  and  Code  text   Link  and         For more details see  Basic writing and formatting syntax     Jekyll Themes   Your Pages site will use the layout and styles from the Jekyll theme you have selected in your  repository settings   The name of this theme is saved in the Jekyll   config yml  configuration file    Support or Contact   Having trouble with Pages  Check out our  documentation  or  contact support  and we ll help you sort it out
32,magsol,PHP Twitterbot   DEPRECATION NOTICE   As of the release of  PyBot   this Twitter bot implementation has been deprecated  No further updates will be made to this repository  Apologies for any inconvenience  PHP just wasn t made for this sort of thing    This is designed to be a relatively lightweight  flexible framework for deploying your own customized and automated twittering bot that more or less does whatever you program it to do      Author  Shannon Quinn   Project Wiki  http   www magsol me wiki index php5 title SpamBot   Git repository  https   github com magsol Twitterbot   Version  2 0  alpha      Overview   The general idea here is to provide a framework by which you can implement as simple or complex a twitterbot as you like without having to worry about anything but the specific behavior you want to implement  If you want a bot that does nothing but sample the public timeline  you can practically use this framework out of the box  If you want a box that reads posts  makes posts  adds friends  changes its profile  and develops a cure for cancer you can absolutely do that too  though it may take a bit of work   The point is you ll only have to focus on implementing that specific behavior  everything else  in theory  P   has been taken care of    The core of this framework is the concept of an  Action   ideally  it encapsulates a single concrete activity the twitterbot performs  posting  or direct messaging  or changing background images  etc   In order to create a new action  you begin by subclassing the Action class    Say we want our bot to post the current time and a  Hello   message every hour  We begin by defining this class as follows    require once BOTROOT    action php    class ClockAction extends Action      We are then required to implement at least two other methods  a constructor and a  run    method    public function   construct  name   active   params       this  name    name     this  isActive    active    foreach   params as  k     v         this   k    v        parent    construct  name   active  array        recommended     public function run          do stuff here that will post hourly update      hint  make use of util TwitterAPI php       if the action completes without any errors       return parent  SUCCESS  otherwise  return parent  FAILURE   return parent  SUCCESS       With this basic framework  you can extend it to do just about anything you d like  Set   this  frequency  in the    construct    method you wrote to provide a different frequency of your Action firing  or  for even greater control  override the  setNextAttempt    method to completely redefine the frequency with which your Action fires  You can override the  post run    method to perform any custom post Action clean up or logging    Put your  ClockAction php  file into the actions  folder  along with any additional dependencies it may require   and point this Twitterbot to it by setting up  config php  to point to it  details below in the SETUP section  step 1   Once this is complete  fire up the bot s daemon  sit back  and let the Twitter trolling begin      Notes   THIS IS STILL VERY MUCH EXPERIMENTAL AND NOT PARTICULARLY ROBUST   I implemented the process control without much previous experience in it  most of my experience is in multithreading with C   not very applicable to this   so it is still very rough around the edges  particularly with the database connection management    If you encounter any bugs  please don t hesitate to report them  either to the github page  or you can email me  magsol at gmail    Requirements   In order to run this bot  you need      PHP 5 x  with pcntl    PEAR and its basic libraries   MySQL 5 x  though will probably work with 4 x    A brave soul     Installation     Fill out the necessary fields in config php  there are several     BOT ACCOUNT   The display name for your bot    BOT PASSWORD   The password to log into your bot  will be removed once Twitter s     Streaming API is integrated into OAuth   for now  a necessary evil     CONSUMER KEY   OAuth Consumer Key  obtained by creating an app for this bot    CONSUMER SECRET   Same as above    OAUTH TOKEN   Same as above    OAUTH TOKEN SECRET   Same as above    DB NAME   Name of the database this application s data can be stored in    DB HOST   Host on which the database resides  usually  localhost      DB USER   Username of the account that has admin access to the DB NAME database    DB PASS   Password for the above user      Within the  actions array you have optional definitions  required if you want the bot to do anything interesting  to customize how your bot behaves  If you leave everything blank  it will simply aggregate posts from Twitter s public timeline  you ll see them grow quite fast within your database     If you choose to add some fields  you ll need to implement your own Action subclass that performs whatever action of interest you want  Obeying the usual object oriented programming guidelines generally makes for better behaved bots  but in general you can have your action do whatever you want  Just be sure to fill in the required fields for any defined Action      name  can be anything  used mainly for debugging      class  case sensitive name of the class you created      file  case sensitive name of the PHP file in which your class resides      active  boolean indicating whether or not this action should be fired      args  optional array of arguments  specific to your action      Run the bot s install script      php install php   This will use the database values defined in your  config php  to set up your database s schema  For optimal behavior  please only fire this script once  It shouldn t cause any unintended behavior if you execute it multiple times  it will simply quit if it detects the tables exist already   but why would you need to run the install script multiple times anyway      Test the bot s settings      php run php   tests only   This will have the bot run a battery of tests against the settings you ve indicated  If there are any failures  it will halt immediately and let you know  wrong database username password  a missing required field for a custom Action  etc       Run the bot      php run php   You can include a   h  flag to display all the available options  By default  run with no arguments   the script will execute the battery of tests like in step 3 but will  pending all tests passing  start up the bot itself  The bot will behave as a daemon  detaching itself from the terminal and spawning a child process for each custom Action defined in  config php   In order to kill the daemon and its child processes  run the command    php run php   stop   Acknowledgements   There are several people who made this project possible        Rob Hall    the original inspiration  with his hilarious  postmaster9001 bot   whose awful Perl hack job implementation inspired me to make something more flexible and robust        Abraham of twitteroauth    His library makes the implementation of OAuth authentication in this project possible  That s one huge black box that is reduced to one or two lines of code on my part to worry about  Awesome  awesome work        Fennb of Phirehose    Again  a huge  rich  robust API that makes my life really freaking easy  This makes data aggregation possible in this project  enabling me to spend my time doing interesting things with the data rather than finding ways of crawling Twitter  Keep up the good work        Sebastian Bergmann of PHPUnit    Author of a phenomenal unit testing framework for PHP  it s still on my to do list to incporporate it into this bot s testing  It will definitely happen  and this guy has made it a lot easier        George Schlossnagle of  Advanced PHP Programming   from which I learned just about every trick in this project for process management  particularly in setting up the structure for the Twitterbot and Action classes  It was a phenomenal resource and made the daemon aspect of this project feasible      BertramBot   A StarCraft II Bot   Overview   This was inspired by  sentdex s recent YouTube series on creating StarCraft II bots  using Python  Given the history of this repository  it s something I ve been wanting to do for awhile    Prerequisites     Python  3 6   not  3 7    python sc2   though take note of  sentdex s fork  that includes an  on end  method for capturing the final result    Mahout Data Generation   For my ongoing work to maintain and upgrade the spectral clustering algorithms in Apache Mahout  I use these scripts to generate data in the necessary affinity matrix format  Eventually the algorithms will be able to accept data in traditional formats  e g  each line is a data point  and compute the affinities automatically  at which time this repository will cease being useful  But until then  those scripts reside here    Overview   These scripts were mainly created for the purpose of troubleshooting and checking the spectral clustering library in Apache Mahout  but can be used for any purpose requiring some random data  Currently  here s how things work        generate data py  is the one stop shop for creating gaussian or radial clusters of data  There are tons of parameters to choose from in tweaking your dataset  though they all have reasonable defaults   so run the script with the   h  flag first to get a feel for what it can do  It will also output the raw data and the data affinities to text files in standard CSV formats  and in the format Mahout expects  as of version 0 7   to make troubleshooting the datasets much simpler        view data py  is a purely troubleshooting script that will plot the raw data you ve generated in the previous step  in addition to plotting the affinities also created  You can use this script to assure yourself that the affinities in the CSV format and in the Mahout formats are identical  and that you know what the data looks like        solve py  implements in a traditional fashion what Mahout implements in MapReduce  spectral k means clustering of the affinity data  It will then plot the clustering results so you know what to expect from Mahout s output        To run these scripts  you need    Python 2 7    NumPy 1 5   SciPy 0 10   core computations    matplotlib 1 1   viewing the data    scikit learn 0 11   computing affinities  performing clustering  10 605 Machine Learning with Big Data   This repository is mainly a static collection of assignments I completed for the Spring 2013 rendition of  10 605 Machine Learning with Big Data  at Carnegie Mellon University     Descriptions     hw4a   Naive Bayes on Hadoop    hw4b   Phrase finding on Hadoop    hw5   SGD  incomplete     hw7   K Means clustering on Hadoop    Itty Bitty Webserver   Lightweight webserver  web proxy  and RPC image compression    Overview   This was the third and final iteration of my CS4210 Operating Systems project in the spring of 2008  It consists of a multithreaded webserver  proxy  and RPC image compression node    WARNING  It s extremely finicky  Use with caution    Requirements   Any  nix system should work just fine  You ll need all the POSIX standards  in addition to the Xmlrpc c library    http   xmlrpc c sourceforge net   Installation   Pretty straightforward    make  client   server   proxy     To build the RPC server    cd rpc make    Running   Here I ll demonstrate each module of the application and its required arguments  followed by example usages    Server      server  port   threads   docroot    o    server 3333 10   server 4444 5 someDir  o    Client      client   p  proxy   port  http     server   port   threads   doclist    client localhost 3333 10 documentlist txt   client  p localhost 4444 http   www cc gatech edu 80 5 docs txt    Proxy      proxy  port   threads    c  RPC server   port     o    proxy 3333 10  o   proxy 4444 5  c localhost 8080    RPC Server      rpcserver  port    rpcserver 8080    Notes   Shared memory works fully in this release  its implementation moved much faster when it was revealed to me that pointers in shared memory need to be stored as relative offsets  rather than absolute addresses   As in the previous release  no socket communication occurs between client and server if optimizations on both ends are enabled    The proper functioning of this release depends on the Xmlrpc c RPC package   This marshals and unmarshals data within an XML schema and transmits the data as HTTP packets   The implementation also took care of concurrency issues   It is an extremely mature application   The only problem is it has to exist  along with the curl library  on whatever machine acts as an RPC server   Luckily  every CoC machine has curl  and it is a very simple task of installing the Xmlrpc c package and having its shared libraries compiled within the home directory    JPG request and transfer still fails sometimes   I was unable to ascertain  exactly why   The proxy occasionally hangs or outright crashes on particular requests to arbitrary web servers   This may be due to broken pipes    Also  images in excess of 380K are not transferred correctly   The reason for this is unknown at this time  but it probably has something to do with the Xmlrpc c implementation  Stratagem   RTS for the ages    Overview   This was the final project for my CS 2335 Software Practicum course at Georgia Tech in the fall of 2004  The assignment was to create a networked  multiplayer  real time strategy game in Java within six weeks     There were 100 points possible for this assignment  which  in turn  was worth 20  of our final grade   in addition to the possibility of 50 extra credit points for various nonessential features  map editors  instant messaging  OpenGL   3D graphics engines  and so on  My team  with only three people  including myself   scored 148 5 out of 100 for fulfilling the core functionality and implementing several extras    Unfortunately  other than needing Ant to build the project  I don t recall how to run it  so best of luck  Appointment Management   Georgia Tech 2008 Computer Science capstone project    Overview   This was the Computer Science capstone project  CS 4911  at Georgia Tech during the summer of 2008  Students were split into teams of 4 6 and given a broad range of projects to choose from  and tasked with developing a project plan around the iterative design process and three project sprints  the first of 3 weeks  the second of 4  and the last of 3  Our project focused on developing a web based appointment management and event synchronization application that could be used easily by users of varying demographics  that could be administrated simply  and which was robust enough to have its functionality extended in the future    Details   We worked in a team of four people      Amanda Glosson  Project Manager   Shannon Quinn  Implementation Lead   Chris Gray  Design and Documentation   Robert Bush  Testing     Our supervisor was Dr David Smith of the College of Computing at Georgia Tech  He tasked us with developing a PHP MySQL based application which would allow users to register and set up appointments  At the time  he required three different types of users  administrators  which  at the time  consisted solely of him   counselors  and regular users  The application on a whole would keep a calendar of days and times for which users and counselors could sign up  The administrator would oversee all activity and approve appointments that were requested  The counselors would log on and use the calendar to indicate their times and days of availability  Regular users would log on  look at the calendar   which would have listed the available dates and times of open appointments   and request appointments    Certain restrictions were built into the system in such a way that they could be lifted or expanded upon later  For instance  regular users were only allowed to register for dates and times for which at least two counselors   one of whom had to be of the same gender as the regular user   had indicated they were available  Furthermore  the administrator needed to be notified of these matchings  so he she could approve or deny the requested appointment  and perhaps swap out counselors at the slot  Permissions for the users needed to be flexible as well  so they could be extended at a later time if and when the system grew    Another twist to the project was the intense need for ease of use and the widest range of browser compatibility possible  This all but ruled out AJAX and Javascript  as well as Flash and any other client side technologies that could be used to streamline the interface  Strict adherence to HTML and compatible CSS was required so the widest range of browser types and versions could be supported  and the user interface needed to be easy to use and intuitive    We received excelling scores on our project  Each of us received outstanding final grades in the course  mine was an A   The project is still in production use today and has been further extended to meet the changing needs of the organization for which it was deployed  Social Network Portal   This was the final project for fall 2009 s rendition of 15 637 at Carnegie Mellon  The project was to implement a portal from which users could unify and access their social networking accounts    Description   This was a project on which I worked with Arpit Tandon  He devised the original idea to design a portal from which a user could administrate all their social networking accounts  negating the need to log into each and every one separately to administrate each one  similar to power com   Obviously  implementing the full functionality of each social networking site was beyond the scope of what we could accomplish in the specified time frame  so we focused instead on  read ing each site we integrated    We began with the mission of integrating OpenSocial sites  but this soon evolved into integrating sites which supported OAuth  Using a single OAuth library  we could implement 3 legged authentication with each site  enabling authentication with each site which simultaneously preventing the need to locally store users  login credentials for each site    As of this release  the supported social networks are Twitter  MySpace  and LinkedIn  Twitter is the only network which allows posting  all others are read only    Here s a live demo  http   www magsol me final637  Garmin   Let s face it  Garmin Connect doesn t have the most exquisite statistics to slice up your running data  This is an attempt to bridge that gap between the raw data and seeing the long  and possibly short  term trends    Ultimately I d like to make this into a web service  but it depends  entirely  on Garmin Connect s website  if they change how forms are submitted  the data scraping portion of this service will break until I can refactor my scripts to emulate it again    Quickstart   To download all your Garmin workouts in TCX format  basically XML   perform the following steps      This should work in either Python 2 or Python 3        Download the  download py  file  either by checking out the repository  downloading the ZIP archive  or by literally copy pasting the text into a file    Make sure you have the  mechanize  Python package installed  instructions to do so are below  though this will hopefully be replaced very soon     Run the command      python download py  u your garmin username   When prompted for your password  type it in  it won t be saved   and within a few seconds you should see your activities being downloaded    Alternatively  you can also set up a CSV file with only 1 line that looks like this    your garmin username your garmin password   Let s say you saved that file with the name  garmin login csv   Then run the following command    python download py  c garmin login csv   Again  you should see activities downloading in a few seconds    If you run into any problems  please create a ticket    Packages   If any of the following packages require dependencies  they will be listed  To install these dependencies  you can use either your favorite package manager  or install  pip  and run    pip install package        download py   A script for downloading all Garmin Connect data as TCX files for offline parsing   Dependencies  mechanize       monthly py   A script for updating one s Twitter account with monthly statistics  Currently  the statistics and format are identical to those seen on  DailyMile  for their weekly statistics  I just thought it d be neat to have monthly updates  too   Dependencies  tweepy  mechanize     Particle Swarm Optimization   This is the particle swarm optimization  PSO  code I wrote for the Lab 5 assignment as a TA for the Spring 2012 rendition of Cell   Systems Modeling at Carnegie Mellon University      lab5 py   This is the file the students had to complete  It contains definitions for but missing implementations of the update methods required for PSO    answer py   Identical to  lab5 py  but with the solutions filled in    util py   Some utility methods to help visualize the algorithm    mysterious npy   The  secret image  that is slowly revealed as the PSO algorithm iterates over its surface    Lab 5 pdf   The instructions for the lab    Pygencuts   This became its own package  and  graduated  from a gist to its own full repository    Pygencuts is a Python implementation of the Eigencuts spectral clustering algorithm  originally  developed in Matlab by Dr  Chakra Chennubhotla     It is released under the  Apache 2 0 License     Copyright 2013 University of Pittsburgh  Licensed under the Apache License  Version 2 0  the  License    you may not use this file except in compliance with the License  You may obtain a copy of the License at      http   www apache org licenses LICENSE 2 0  Unless required by applicable law or agreed to in writing  software distributed under the License is distributed on an  AS IS  BASIS  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND  either express or implied  See the License for the specific language governing permissions and limitations under the License   Garmin for WordPress   A github mirror of the  WordPress plugin     Description   This plugin creates a widget for WordPress blogs that can show a number of recent workouts from Garmin Connect  It allows for customization of whether or not to display properties of the workout  including      Filter by type of workout   Show only the most recent  x  workouts   Calories burned   Heart rate   Distance   Average pace   Description      and others     Installation   If you want to automatically upload the plugin to your WordPress installation  and have the ability to do so   follow these steps      Go to the Administrator dashboard  Select  Plugins    Add New  from the menu on the left    From the links at the top  click  Upload     Navigate to the zip file containing the plugin and select it      If you prefer to manually upload the plugin  or don t have the ability to automatically upload it  follow these steps      Navigate to your WordPress plugins directory  This is located in    wordpress rootdir wp content plugins      Unzip the Garmin for WordPress plugin  and place the folder in the  plugins  directory      Once the plugin is in place  you can activate it via the Plugins menu on the Administrator interface  Once activated  go to the Settings panel  A new submenu titled  Garmin Connect  should appear  Click on it  and configure the plugin with your Garmin Connect username and password    Finally  to display the plugin on your WordPress site  navigate to the Widgets menu   Garmin Connect  should appear as an available plugin  Simply drag it to whatever portion of your site you wish to display it on  configure it from the available options  and you re set to go    Other   This plugin was originally created by WordPress user  Coded Robot LLC   However  the plugin has not been updated since Dec 2011  For that reason  I forked it here so I could provide slightly more up to date maintenance  Credit for the creation of the widget  however  does not rest with me    Furthermore  I have relicensed this plugin  see below   Its original license was GPL  but this release carries the Apache 2 0 license    License   Copyright 2013 Shannon Quinn  Licensed under the Apache License  Version 2 0  the  License    you may not use this file except in compliance with the License  You may obtain a copy of the License at      http   www apache org licenses LICENSE 2 0  Unless required by applicable law or agreed to in writing  software distributed under the License is distributed on an  AS IS  BASIS  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND  either express or implied  See the License for the specific language governing permissions and limitations under the License   PyBot  v0 3 0   This is a port of  my previous attempt at a Twitterbot   the primary difference being this is in Python instead of PHP  Arguably an improvement all by itself      PyBot is designed to be a modular and lightweight framework in which users can create and deploy autonomous bots that do all sorts of things on Twitter  PyBot helps you create the bots  but it s still largely up to you to implement how they work    Installation   Download the source  Make sure the dependencies are satisfied  Yay    Dependencies     Python 3 5    tweepy 3 3    Any other dependencies are bot dependent     Documentation   v0 3 0 is a significant overhaul of the source from v0 1 0   Please read the following documentation carefully  especially if you are familiar with PyBot s previous architecture    Creating new bots   You can use the provided script in the  sbin  folder to create new bots    sbin create pybot py    This will give you a list of arguments you can provide  The only required argument is the bot s name  this can be anything you want  and it bears no intrinsic connection to the Twitter user you connect the bot to  The name you give this script is purely to distinguish between your PyBots    Optionally  you can provide your OAuth credentials if you have them already   api key    api secret    access key   and  access secret    Otherwise  the script will take you through the process of registering an app on Twitter  generating the necessary credentials  and integrating them with your bot    Implementing an action   The core functionality of PyBot revolves around the concept of an  action   Activities such as posting a tweet  reading a reply   favorite ing  a tweet  or searching for keywords all constitute different types of actions    There are two phases to an action  a  delay interval  and a  callback   During the delay interval  or waiting time between handling time of actions  your bot essentially sleeps  Depending on the action  it may still be doing something behind the scenes  e g  reading from Twitter s Streaming API   but for all practical purposes it is sleeping    Once the delay interval has elapsed  the callback phase kicks in  This is where the action is explicitly handled    As an example  let s say you want your bot to post the time every hour  Our  HourlyBot  will have a 1 hour tweet interval  set using this configuration option    def bot init self        self config  tweet interval     60   60                  Other configuration options here               the intervals are in seconds  so to get 60 minutes we need 3 600 seconds  or 60   60    By itself  this means  HourlyBot  will activate a  tweet  callback every 60 minutes  With the interval in place  now we have to implement the actual callback  This is done with the  on tweet    method    def on tweet self       from datetime import datetime     self update status  It is  s     datetime strftime datetime now      I  M p       And that s it  The PyBot internals take care of logging  saving state  putting the bot to sleep between callbacks  and waking it up at the correct intervals  See the  examples   folder for more examples    Starting a bot   Run the command   python your bot py    This will start the specified bot  The above script generates a bot that has a single action defined  you can specify more if you want  However  if you remove all actions  this will be detected and the bot will automatically terminate  Otherwise  it will simply run forever    Stopping a bot   A simple CTRL C should do the trick  This will send a SIGTERM signal to your bot  which has a handler in place to catch the termination signal and gracefully shut down    Acknowledgements   The original inspiration for this bot came from  Rob Hall s postmaster9001  in the late 2000s  and gave birth to the  now deprecated  PHP version linked above    Architectural aspects of PyBot were inspired in part from  muffinista s chatterbot  and  thricedotted s twitterbot   In particular  the blacklist and DSL aspects come from muffinista  while the object oriented design and functional callbacks are taken from thricedotted    If you are familiar with thricedotted s Python twitterbot  you will find many similarities in PyBot  I chose not to make PyBot a direct fork of twitterbot  as it is not backwards compatible at all  Still  it retains enough architectural similarity to warrant mention  Pairwise Affinities on Hadoop   This is a proof of concept for generating a pairwise affinity matrix on a map reduce environment  such as Hadoop   This affinity matrix could then be used for something like  spectral clustering on Mahout     The initial job of counting the raw data and numbering it accordingly was taken almost in its entirety from this  June 2012 post on waredingen nl     The initial job can likely also be factored out in a framework like Mahout  Replacing it would need to be something like the text to SequenceFile conversion job  Effectively the purpose of the first job is to     Determine how many data points there are  and   Index each data point  making it aware of where it is in the entire set      By whatever other means these objectives can be accomplished  then the initial job can be scrapped entirely    How to use       Provide a text file  or directory of text files  where each line is an n dimensional point  each separated by a comma  For example     1 67454329673  1 65982727489  1 62661761615  1 68596505587  1 42286584711  1 66193468748  1 58282334852  1 82754404428  1 50182036382  1 62347000619  1 52781414121  1 67665119321            Run  RowNumberJob  on this data        Look in  PairwiseJob BlockReducer evaluate    and modify this to whatever you want your pairwise operation to be  currently an RBF kernel         Run  PairwiseJob  on the output of  2  providing a blocking factor   h   and the number of data points   v          That s it  you re done        Dependencies     Hadoop 2 2 0   Java 7     License   Copyright 2014 Shannon Quinn  Licensed under the Apache License  Version 2 0  the  License    you may not use this file except in compliance with the License  You may obtain a copy of the License at      http   www apache org licenses LICENSE 2 0  Unless required by applicable law or agreed to in writing  software distributed under the License is distributed on an  AS IS  BASIS  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND  either express or implied  See the License for the specific language governing permissions and limitations under the License   Summer 2014 TecBIO REU   Fun with cilia  autoregressive modeling  and spectral graph theory  Because math is  fun    Basics of Git   Here are a few commands that should get you up and running with github    git clone git github com magsol tecbio git   This command  clones  the repository into your local machine  automatically creating a folder called  capstone  in your current directory    git status   Issue this at anytime to see the status of your local repository  It will list files that have been changed from the last commit  as well as files that are in your repository but which have not yet been added to version control    git add some new file txt   When you create a new file in your repository  initially it is not under version control and will not appear in any commits you make  You have to explicitly place it under git control with this command    git commit file you want to commit txt  m  Commit message     Once you ve done some editing to the codebase  you ll want to commit these changes to the repository  This is where the distinction between the local repository on your machine and the remote repository  e g  on github  becomes important  this commits the changes to your  local  repository  with the accompanying message in quotes  usually you want this to be a description of what changes you re committing  new features  bug fixes  refactoring  etc     git push   This is how you sync your local repository with the remote one  e g  github   When you ve made one or more commits to your local repository  you can push these commits out to the remote repository with this command  All the commits will show up on the github web page  and your collaborators  i e   me  will be able to see your changes    git pull   Get in the habit of issuing this command almost as often as  git status   It will pull down any changes pushed by others to the remote repository into your local repository    Branching and merging   This is some advanced stuff that you won t need just yet but which is still incredibly useful  We ll go over this in detail later  but it allows multiple people to implement huge changes to the codebase without affecting the efforts of everyone else  In effect  branching allows you to  fork  off the codebase and implement any edits you want while others continue to commit to the  main  branch  You can even switch between branches to work on different versions of the same codebase simultaneously  Eventually  you ll want to merge the branches back together  which is a non trivial task  but git makes it tractable  Here s a useful link  http   git scm com book en Git Branching Basic Branching and Merging   Useful commands   git branch  d  branchName     Deletes a local branch    git push origin   delete  branchName     Deletes a branch on github  Twitletic   Provides a web front end for registering a Twitter account to post monthly workout statistics    How to use   This is built as a web front end for users to add an app to their Twitter account which will post monthly athletic activity summaries  similar to what is seen on  DailyMile  on a weekly basis  Users will simply go through the OAuth procedure to authorize the app to post to their Twitter accounts  and will provide any credentials necessary to access the sites containing workout information    Supported Sites     Garmin Connect  username   password      Supported Activities     Running     Roadmap     Add  Strava    MapMyRun  to supported sites   Add  Cycling  to supported activities   Customizable tweet format   SSL TLS encryption for protecting usernames and passwords in transit     Dependencies     PHP   MySQL   twitteroauth   Smarty   Python   mechanize   Pyspark Affinities   Proof of concept for computing pairwise affinities  a la spectral clustering  in a Pyspark environment  Spark LSH   Locality sensitive hashing for  Apache Spark   Largely a PySpark port of the  spark hash project     Prerequisites     Spark 1 2    Python 2 7    SciPy 0 15    NumPy 1 9      Implementation Details   This project follows the main workflow of the spark hash Scala LSH implementation  Its core  lsh py  module accepts an RDD backed list of either dense NumPy arrays or PySpark SparseVectors  and generates a model that is simply a wrapper around all the intermediate RDDs generated  More details on each of these steps will follow    It is important to note that while this pipeline will accept either dense or sparse vectors  the original hash function from  spark hash  will almost certainly fail with dense vectors  resulting in all vectors being hashed into all bands  Work is currently underway to implement alternative hash functions that more evenly split dense vectors  For the sparse case  the results duplicate those of  spark hash     Usage   Usage follows that of the spark hash project  Parameters remain the same    Parameters   Command line parameters        bins   m    Number of bins in which to hash the data  A smaller number of bins will increase the number of collisions  producing larger clusters      numrows   n    Number of times to hash the individual elements  A larger number will diversify the signatures  increasing the likelihood that similar elements will be hashed together  Put another way  this is the number of bits in the signatures      bands   b    Number of bands in which to split the signatures  Each band will have  n   b  elements  A smaller number of bands will increase the confidence in element similiarity      minbucketsize   c    Minimum allowable bucket size  Any buckets with fewer than this many elements will be dropped entirely      Other parameters      p   For use in the  minhash  hashing function  It is an integer that is larger than the number of dimensions of the original data  and is used to generate the random numbers that seed the minhash function      Tuning   As described in the MMDS book  LSH can be tuned by adjusting the number of rows and bands such that    threshold    1 0   bands      1 0    rows   bands      Naturally  the number of rows  bands  and the resulting size of the band  rows bands  dictates the quality of results yielded by LSH  Higher thresholds produces clusters with higher similarity  Lower thresholds typically produce more clusters but sacrifices similarity     Regardless of parameters  it may be good to independently verify each cluster  One such verification method is to calculate the jaccard similarity of the cluster  it is a set of sets    SciPy jaccard similarity  is used  although future development will allow for user supplied distance functions    Future   Work on this project is ongoing and includes      Additional hashing functions  in particular to address the propensity for dense vectors to generate signatures of all 0s using  minhash     Density sensitive hashing   as outlined in the  Lin  et al  IEEE 2012 paper     Dimension spans   as outlined in the  Hefeeda  et al  HPDC 2012 paper         User supplied hash functions  will be allowed    User supplied distance metrics  will be allowed    Connection to distributed affinity matrix computation  as the initial groundwork for spectral clustering on Spark    BSEC 2015 Conference Materials   This repository contains the pptx slides and IPython Notebook file used to demonstrate basic large scale image analysis using thunder python    If you want to simply view the IPython notebook  just check out this repository  make sure you have Jupyter installed  and run the following command    jupyter notebook path to ipynb file ipynb   If you want to run and  edit  the IPython notebook in a fully interactive way  you ll need to install a few prerequisites  as well as perform some setup    Install Prerequisites     python 2 7    jupyter 4 0    Spark 1 4    built against  Hadoop 1 X   thunder is not yet compatible with Hadoop 2 X    thunder python 0 5    awscli     Download the data   We used the FERET dataset   pulling out all the  tif files into a single directory  we did not make use of any ground truth information    Once the data is downloaded  the next step is to put the data on S3    Upload to Amazon S3   We use the AWS CLI tool installed earlier to streamline the process of interacting with S3  These operations can easily be performed using the AWS browser based dashboard  if that is your preference      Create an S3 bucket      aws s3 mb s3   mybucket   region us east 1   Important to note  there are some issues that can arise if the region specified in S3 differs from the region in which your EC2 instances  next part  are spun up  For this reason  it is always good to specify the region rather than let AWS try to infer it      Copy the local image data to the S3 bucket      aws s3 cp path to image folder s3   mybucket path   recursive   If you want to cut down on the verbosity of the  cp  command  you can add the    only show errors  flag  and it will complete silently unless an error is encountered    Allocate EC2 instances   You can follow the instructions in  thunder project s documentation   in particular  make sure you ve set up your AWS access key  The critical portions of the setup process are reproduced here      Launch a cluster  For BSEC2015  we used a 15 instance cluster      thunder ec2  k mykey  i   mykey pem  s 15   copy aws credentials launch bsec2015   The option    copy aws credentials  automatically copies your AWS keys to the cluster as it is being set up  This allows your cluster automatic access to S3  If you don t use this option  you ll need to copy the keys manually once the cluster is running  otherwise your cluster won t be able to access the images you just uploaded to S3  This step will take awhile      Follow the instructions in the thunder project documentation for  using the IPython notebook   specifically   Method 1  Connect directly over SSL   This involves configuring your EC2 cluster master to accept incoming HTTPS connections      Run the notebook   Once your IPython server is running  you can open up the IPython notebook file stored in this repository  configure it for your setup  and you re good to go    One last note   When we ran this notebook using 15 AWS EC2 instances  m3 2xlarge  on the full FERET dataset   90 000 images   it took  roughly 50 minutes  to run to completion  Source for Stochastic Stenography   This repository contains the source for https   magsol github io    Building the Blog   Clone the repository   make sure submodules are included     git clone https   github com magsol magsol github io source git   git submodule update   init   recursive   Install the required packages      conda create  n pelican python 3 7 jupyter notebook   source activate pelican   conda install  c conda forge pelican Markdown ghp import   Install the optional packages for the activated plugins          For  filetime from git  https   github com getpelican pelican plugins tree master filetime from git     conda install  c conda forge gitpython    For  summary  https   github com getpelican pelican plugins tree master summary     conda install  c conda forge beautifulsoup4  For the extension embedding tweets  https   github com lqez pelican embed tweet    wget https   github com lqez pelican embed tweet raw master embed tweet py   mv embed tweet py    plugins    Edit the pelicanconf py file accordingly          Build the html and serve locally      make html   make serve   open http   localhost 8000   Deploy to github pages     make github   Attribution   This is based off the templates of both  Jake VanderPlas  and the theme s author   Daniel Rodriguez   Thanks to both for all their work on Pelican blogs  LDA of Twitter friends   This is based in principle on  Alex Perrier s blog post on topic modeling of Twitter followers   but coded essentially from scratch  codejam   Various Google CodeJam ings  Spring 2018  CSCI 8360   Spring 2018 rendition of CSCI 8360 Data Science Practicum  PySC   PySpark Spectral Clustering    Overview and Goal   The goal of this project is to ascertain a performance benchmark for spectral clustering in Spark  particularly as compared to the native Scala based implementation  here     It is also meant as a baseline for comparison against the  dask ml spectral clustering  pipeline    Ultimately  this would be a jumping off point for experimenting with different methods of approximating the affinity matrix  implicit computation of the leading eigenvectors  adaptive neighborhood sizes  and other strategies for improving runtime    This is super bleeding edge  pre alpha work that is not meant for production    Dependencies     Python 3 5   Spark 2 2     Other dependencies as development continues    Related Projects   pyspark lsh   This was an early attempt to build a locality sensitive hashing library from scratch using PySpark  This approach could still be useful in building a collection of high probability neighbors for the affinity graph    PySpark Affinities   This was a an initial foray into computing affinities of images in an efficient  NumPy friendly way  However it tended to rely on full cartesian products  infeasible in a distributed environment    How to Contribute   Send in a PR  Would love the help    License   MIT  Athens Road Runners Grand Prix Ranking   Development repository for scripts related to ranking members in their races  Jupyter   Conda   Doing some Jupyter   Anaconda data science in Docker  CyVerse FOSS Workshop   RStudio and other stuff from the  June 2   7 workshop in Tucson  AZ   Drake demo   Because sometimes  shit just happens   DataSciBun Twitter Bot   Quickstart   Runs forever with the command      nohup python datascibun py   datascibun out     Note  the  nohup  prefix and    datascibun out    suffixes are optional and pertain to additional logging  as well as backgrounding and detaching the process from the terminal so it can run forever  You can just as easily run  python datascibun py  by itself    Dependencies   In a nutshell     Python 3 7   since f strings are involved     NumPy  for random number generators      markovify      pybot   See  environment yml  for a full list of dependencies    Credentials   You ll need to go to  dev twitter com  and create an app  From there  you ll need to generate OAuth credentials and populate the following fields in  datascibun py     self config  api key      your api key here  self config  api secret      your api secret here  self config  access key      your access key here  self config  access secret      your access secret here    Acknowledgements   In addition to those  on the pybot repo   also inspired by  Amelia Gapin  s  bot     Finally  credit to  Cathryn  for the tweeting ideas  bio  and profile pic  StarCraft II  Ladder Bots   This is an attempt to set up a ladder system where bots can square off against each other  much like what is done on  sc2ai net     The point is ultimately to make an easily deployable framework  e g   on Kubernetes or an arbitrary virtual machine  where this can be used in a classroom setting  Students studying reinforcement learning can build their bots  upload them to the ladder server  and have the server play their bots against each other  CyVerse Advanced Container Camp 2021   2021 rendition of the CyVerse Advanced Container Camp  k3s at home   Eating some pi e  and clustering some nodes    Current setup     5x Raspberry Pi    k3s    Synology NAS DS920  Wildlife Cameras          Scripts related to the outdoor wildlife cameras    Overview   There are three cameras       Pi Camera Module 2 NoIR      Luxonis  OAK 1      Luxonis  OAK D Lite   The Luxonis cameras are meant for daytime use  while the NoIR camera will operate at night    This repo will also host any processing scripts  though the Luxonis cameras have many built in processing models that we ll leverage    Dependencies   These are running off a  Raspberry Pi 3B    so we re already a few years old  but that s ok      running  Mambaforge   mamba 0 27 0   conda 22 9 0   python 3 9 13   DepthAI  install instructions  for Raspberry Pi OS  in practice  this ends up using system tools  and so all it needs is basic Python   pip    imagezmq   This hasn t been updated in a bit    am hoping to submit some PRs as this progresses    but it still works  even with the current versions of all the dependencies listed   pip install imagezmq   used conda forge versions of  opencv    pip   and  pyzmq   PiCamera2   apt get install build essential python3 libcamera python3 kms     mamba install piexif pillow   pip install pidng simplejpeg v4l2 python3 python prctl   pip install   no deps picamera2   the final step here involved setting symlinks for  pykms  and  libcamera  inside the    MAMBA ROOT      This initializes a default environment that works well with both PiCamera2 and DepthAI devices    Troubleshooting   New versions of Picamera2   A common problem I ve been running into is that  when a new version of Picamera2 is released  the previous versions break  even to the point of not being able to ask the version that s installed to compare against the newest version     However   a recent change is that the group is now releasing the latest Picamera2 as apt packages   They re still available on PyPI  and I m continuing to use them there  too  but evidently this seems to be the long term way forward    For the time being  though  continuing to use  pip install  U   no deps picamera2  seems to work  Hi there               I am Shannon Quinn  currently     an Associate Professor at University of Georgia   on sabbatical     a Solutions Architect at  Quansight     interested in anything i      open source  ii    PyData  and iii    donuts     What I do here        I m currently learning        Ask me about        Fun fact        How to reach me      Shoot me a message via twitter or e mail  or even create an  issue
33,sesteel,angularjs treewidget   An example of how to build a tree widget using AngularJS    node webkit kit     Node webkit kit is a project template node webkit applications   It is configured for continuous testing  building and delivery   You may fork  this project to start other  new node webkit projects   If you use this  project and make generally useful improvements  please submit pull requests  for your modifications so everyone may benefit      To build you must first install Node js on the dev build system   Then  within the project directory  either for this project or your forked and renamed  project  run each of the following commands      npm install  g grunt cli                save dev   npm install grunt                       save dev   npm install grunt contrib clean         save dev   npm install grunt contrib less          save dev   npm install grunt contrib uglify        save dev   npm install grunt contrib copy          save dev   npm install grunt node webkit builder   save dev   npm install rimraf                      save dev    npm install grunt plato                 save dev     Using the Template for Development   TBD   Current work has moved to    https   github com vizstra ui   where opengl has replaced Cairo for drawing calls   The tradeoffs  complexity  to get efficient text rendering using Cairo are not something I want to make at the moment   At least not without trying a different approach using OpenGL first    go view   Go View is an experimental project to build a rudimentary GUI library for Go upon a Cairo backend   It is being built out in the open for anybody to use  but it is a personal project and should be viewed as such   I am exploring tangential issues surrounding UI and UX design   Below  I have placed a few screenshots to give a sense of the asthetic    Screenshot from the Button Example      Screenshot from the Checkbox Example      Screenshot from the Progress Bar Example      Screenshot from the Text Box Example      Screenshot from the Editor Example    documint rayray   A Goroutine Experiment   This project demonstrates the use of WaitGroups and goroutines to divide and conquer the rendering of a single ray traced frame   By changing the the number of sections in use by the algorithm  we see various performance characteristics as listed here        Sections   CPU Load    Threads   Time  ms                                                           1            100        9            524           2            198        9            275           5            337        9            145           10           571        9            145           20           617        9            129           50           733        9            127           In the above  each frame is rendered 200 times and the average frame render time is recorded
34,razh,capstone   Git reference   There are GitHub apps for Mac and Windows available      GitHub for Mac   GitHub for Windows     We will should be using Vincent Driessen s  Git branching model   this is a good read for any programmers who want to work in a professional colloborative environment   It s intuitive for the most part  but I ll try to summarize    develop     master                    O          O                    O                               O                               O          O                     The arrows are merges  the Os are commits  Also  his diagrams are much better     There are two named branches      The  master  branch  which contains stable  release worthy code  think major versions of operating systems     The  develop  branch  which is what we branch off whenever we want to create new features  think nightly builds  It should contain      In addition  there are three categories of supporting branches we can create      Feature branches   Release branches   Hotfix branches     Since we re not working in a production environment where we have to ship code  we re only going to be using feature branches    When working with multiple people  there are two models of collaborative development      Shared repository model   Fork and pull     Because of how small the team is  following the shared repository would allow us to develop faster    GitHub for Mac instructions   The user interface for GitHub Windows is probably similar enough to the Mac version such that these instructions will be applicable for both        Download and install the GitHub app        Go to the  develop  branch  of the repo        Click  Clone in Mac Windows         This should open up the GitHub app  Sign in with your credentials if you haven t already done so        Choose where to save the folder        Click on repo when the download is finished        Click on the  Branches  tab        The  develop  branch should be there  Click the     to create a new feature branch with your desired name  e g   area weapon  or  sound manager    This new branch should now be the current branch        Make a change  Code  Program  Hack        Click on the  Changes  tab        Type in a commit summary   Provide useful information on the changes you ve made         Click the     button with arrows around it and then  Commit and Sync   You can just click Commit if you don t want to upload your changes just yet  though you ll still have to sync later        Merging back into the  develop  branch   When you re finished programming the feature and you ve committed all of your changes  you re now ready to merge back into the  develop  branch        Go to your branch page located at  https   github com  USERNAME  capstone tree  FEATURENAME          Click on  Pull Request         Make sure the  base branch  is set to  develop         Enter a short summary of the feature        Click  Send pull request         Assuming there are no major conflicts  your feature should now be in the  develop  branch    Merging new changes into your personal branch   If there s a branch with some changes that you want in your personal branch  you ll want to merge the changed branch into your personal branch    Command line instructions   Getting the repository from GitHub  Use this command in your desired working directory    git clone https   github com razh capstone git    To actually start working on something  we need to create our own personal branch    Branching off from the develop branch to work on something called  featureName     git checkout  b featureName develop    Committing our changes    git commit  m  Commit comment      Sending all of our commits to the GitHub server    git push origin featureName    Getting the latest updates    git fetch origin featureName git merge featureName    Merging from the featureName branch back into develop    git checkout develop git merge   no ff featureName git branch  d featureName git push origin develop    Code style     Make sure you ve set your code editor to use Unix line endings    Make sure your code is formatted in a readable manner    Comment your code   Okay  I need to do that     capstone java spellquest moai   Unfinished anagram game prototype written in Moai  The source code is poorly documented and was written while learning Lua  If you ve stumbled across this  it would be best to ignore it and move on with your life     about   Personal landing page  rosewood js   Guitar fretboard app built with backbone js  kanban bit   Kanban board with an 8 bit aesthetic  spindle   Interactive fiction editor   la Twine  qualify js   Test your JavaScript knowledge  angularjs intro   An attempt at a guide for AngularJS  explanations   Explanations of various data structures  algorithms  and other computer science miscellanea   sound grid   Tone matrix monome esque sequencer  jquery rain js   jQuery plugin for making it rain  rescue the dog   Follow the story beats from Blake Snyder s Save the Cat  vmf sketch js   Rapid prototyping tool for  vmf  Valve Map Format  files  razh github io backbone flashback   A simple undo redo manager for Backbone Models and Collections    Usage   Boilerplate code for the following code examples     javascript var manager   new Backbone Flashback      var Model   Backbone Model extend     defaults  function         return         id    uniqueId          foo                      var Collection   Backbone Collection extend     model  Model           Models      javascript var model   new Model   foo   a        manager begin model   model set  foo    b    manager end      model get  foo        b    manager undo    model get  foo        a    manager redo    model get  foo        b        Collections      javascript var collection   new Collection       foo   a         foo   b             Change the first model  manager begin collection   collection at 0  set  foo    c    manager end      manager begin collection   collection remove collection at 0    manager end      collection pluck  foo         b     manager undo    collection pluck  foo         c    b     manager undo    collection pluck  foo         a    b     manager redo    collection pluck  foo         c    b         Arrays of Models      javascript var models       new Model   foo   a        new Model   foo   b          manager begin models   models 0  set  foo    c    models 1  set  foo    d    manager end           c    d    given  models map function model    return model get  foo         manager undo         a    b     manager redo         c    d         Requirements   Backbone Flashback depends on  Underscore  and  Backbone   For Flashback to work  models must have unique ids assigned to them    If you re using  RequireJS   you ll need to shim Flashback and its dependencies     javascript requirejs config     shim         underscore           exports                  backbone           deps    jquery    underscore          exports   Backbone              flashback           deps    underscore    backbone          exports   Backbone Flashback               paths         jquery    path to jquery        underscore    path to underscore        backbone    path to backbone        flashback    path to flashback            define   flashback    function Flashback      var manager   new Flashback              Methods   manager begin target    Begin tracking changes to the  target     The  target  can be a Backbone Model or Collection  or an array of Models or Collections    manager end     Stop tracking the  target  and call  save    on its current state if any changes were made to  target  since  begin    was called    manager save target    Save a snapshot of the current state of the  target  and deletes all states stored in the redo stack    manager undo     Undo the last saved history state    manager redo     Restore the last undone history state    manager canUndo     Return the number of states currently on the undo stack    manager canRedo     Return the number of states currently on the redo stack    Development   Install  NodeJS   If you have  Homebrew     brew install node    Install  Grunt     npm install  g grunt cli    Install project dependencies    npm install    Install  Karma  and  PhantomJS     npm install  g karma npm install  g phantomjs    Or  if you have Homebrew    brew install phantomjs    Building   grunt    This will run  JSHint   execute all tests  and minify with  UglifyJS2     Running tests   Flashback uses  Karma   v0 10  with the  Jasmine  testing framework and  PhantomJS  for headless testing    To run tests with the  autoWatch  option on  where Karma will watch for file changes and run tests automatically    karma start    You can also run tests via Grunt tasks    Equivalent to  karma start     grunt karma unit    Run tests once  for continuous integration    grunt karma continuous    Run tests in Chrome    grunt karma browser    License   MIT  experiments   Tests  experiments  prototypes  etc  experiments app   Single page application tests and experiments  experiments three js   Experiments with three js  pb div   Version           Link                       Normal            http   razh github io pb div Intro animation   http   razh github io pb div intro html Canvas            http   razh github io pb div canvas html Circles           http   razh github io pb div circle html   Developed with Chrome Canary and Mavericks    Tested with latest versions of Chrome Stable Firefox Firefox Nightly Safari Mobile Safari Chrome for Android  system global game jam 2014 experiments three js   Experiments with three js coordinates axial tribune Neptune Blue   js13k 2014   Play the original  js13k submission     Space ship    Space ocean    Space rocks    Space fireballs    Space health power ups    Endless running    Controls     Left      A   or   Left Arrow     Right      D   or   Right Arrow     Start      Space       Expand contract the game window with the       button on the right    Releases   1 0 0   Initial release    Development   To start a development server      gulp watch    To run test projects      gulp   gulpfile test gulpfile js    To build for production      gulp build  p    Notes   This is built with a stripped down and heavily modified version of the  three js  CanvasRenderer    Thanks to     end3r  for js13k    ooflorent  for  js13k boilerplate     osc dev demoinfogo viewer third person camera tests react tween layout experiment flying machines static prototype boilerplate js13k 2015 react intro fall 2015 generative tense optical flow worker noise machine poise weirwood neon js13k 2016 x 10k apart 2016 scattered showers js13k 2017 game off 2017 js13k 2018     game off 2018 js13k 2019 js13k 2020   4    0    4    js13k 2021   https   razh github io js13k 2021 ln sketches js13k 2022   https   razh github io js13k 2022
35,jaychoo,Gaffer   Gaffer  using Flask  is an attempt to make writing scalable API interface easier   Since the core of the project uses Flask  the project was named Gaffer  which is another name for a glass blower  python tcpdump app        Credits      Managed by  Ghar     Much code and ideas ripped off from  Mathias Bynens     Other stuff  prompt mainly  from  Armin Ronacher     Flask Boilerplate   Boilerplate for Flask web application    Inspired by       https   github com mattupstate overholt   https   github com mitsuhiko flask wiki Large app how to   https   github com mjhea0 flask boilerplate   http   blog miguelgrinberg com post the flask mega tutorial part i hello world   https   github com jquacinella flask baseapp   http   www initializr com   http   html5boilerplate com   http   getbootstrap com     Management Commands  inspired by Django manage py    Create new app  will generate initial files for new app in  app      python manage py add module NAME Creating module directory and files      init   py models py views py constants py utils py Creating directory in templates folder     Flask BaseApp   BaseApp is an example Flask application with some basics already setup  This was inspired by a few places      https   github com mattupstate overholt   https   github com mitsuhiko flask wiki Large app how to   https   github com mjhea0 flask boilerplate   http   blog miguelgrinberg com post the flask mega tutorial part i hello world     Management Commands   Management commands can be listed with the following command      python manage py    These can sometimes be useful to manipulate data while debugging in the browser    Tests   To run the tests use the following command      nosetests  pypes   Python pipe utility   rdfgo   N3 utility for Go settings TDX dotfiles
36,wgmueller1,nullspace   wgmueller1 github io html5 drag   Testing HTML5 drag and drop d3 node FA   testing d3 force directed graph with FA icons peak detection   Requires wmtsa package  sparkwc svg2pdf   service for exporting rendered svg to pdf sparkLDA   spark LDA example Realtime Word Cloud   When I think of predictive analytics  I think of word clouds   When I think of realtime analytics  I think of streaming word clouds   This projects brings realtime analytics to the forefront of Data Science by using the streaming twitter API to generate real time word clouds      Usage   node wordcloud js Repository for work on social sandbox unicorn   Unicorn is a python based web framework used for exploratory analysis of text corpora   Unicorn leverages many existing open source software projects to ingest documents  extract information  provide full text search  and visually display relevant content    Quick Start Installation   You can get up and running with unicorn very quickly using Vagrant and the  VagrantFile  that comes with the repository by running  vagrant up  This provisions an Ubuntu trusty virtualbox image  You should move the  Vagrantfile  into the same directory in which you ve cloned the unicorn repository    Next  remove   template  from runconfig py and app config py  Within app config py make sure you set the  admin username  and  admin password   You will use these for your first login  Also within app config py change the database connection string   db conn str    If you want to use our defaults embedded in  db setup sh  and  install sh  you should set the  db conn str  to    db conn str    postgresql   unicorn unicorn 127 0 0 1 5432     Now  in the main directory  execute     bash install sh   Since the  Vagrantfile  is forwarding your port 5000 to your host  you should be able to navigate to http   localhost 5000 unicorn to access the application    Removing the default index   The quickstart comes pre loaded with 1 000 historical documents from the National Archives  If you want to clear this out  run     curl  XDELETE  http   localhost 9200 dossiers     curl  XPUT  http   localhost 9200 dossiers        sudo elasticdump       bulk true       input dossiers mapping json       output http   localhost 9200 dossiers       type mapping       debug true   Configuring PostgreSQL   unicorn uses Postgres for user authentification  If you would like to alter the defaults on  db conn str   prior to running the quickstart you can replace the values  unicorn  within the  db setup sh  file to whatever you would like  You should only change    sudo  u postgres psql  c  CREATE USER unicorn WITH SUPERUSER CREATEROLE CREATEDB PASSWORD  unicorn       change both instances of  unicorn  to create a different default user   Note you then need to update the  db conn str     PDF Viewer   unicorn requires a browser plug in to render PDFs called PDF Viewer  It is available  here for Google Chrome  and  here for Firefox   unicorn should work natively with Safari    License   Unicorn is under ongoing development and is freely available for download under The MIT License  MIT  open source licensing  Unlike GNU General Public License  GPL   MIT freely permits distribution of derivative work under proprietary license  without requiring the release of source code    Acknowledgements   This project was funded by DARPA under part of the XDATA program  Cython Tutorial   this is going to be a non shit tutorial on how to use eigen C   library with python to speed up nonparametric bayesian methods question classification   Experiments on question classification using recurrent neural networks   This uses dataset located here  http   cogcomp cs illinois edu Data QA QC  LineUp js  Visual Analysis of Multi Attribute Rankings   LineUp is an interactive technique designed to create  visualize and explore rankings of items based on a set of heterogeneous attributes  This is a D3 based re implementation with limited functionality relative to the original stand alone LineUp  which you can check out at http   lineup caleydo org oddscrawler   crawler for boxing odds   Selenium based Scrapy crawler for obtaining historical and current odds for boxing matches    ranknn   experiments using neural networks for performance ranking
37,yassersouri,Corpus Format   CoNLL  You can find more info  here     Projects   Corpus Builder   References   This project uses SCICT PersianTools which is built by  SCICT  and is opensource under GPL  You can find the source code  here     What it does   We use the  Program cs  file to  RefineAllFiles  that are inside the  directory specified   Then we will  separate all sentences and words  and create new files in new directories  You can overwrite your current fies if you call it like  SeparateAllSentencesAndWords dir dir       Example Output   Check  here     Tagger   References   This project uses lots of SCICT NLP DLLs which is built by  SCICT  and is opensource under GPL  You can find the source code  here   It also uses YAXLib   Tagger cs  and  Token cs  is written by  Mohammad Hedayati     What it does   Output of the  Corpus Builder poject  is a set of words in each line where each sentence ends with an empty line at the end  Each word then is tagged with the tagger and its Lemma POStag person and number is extracted  these information is then placed at the same line of the word with tab separated    Example Output   Check  here   Salam   Text Classification with python   This is an experiment  We want to classify text with python    Dataset   For dataset I used the famous  Twenty Newsgrousps  dataset  You can find the dataset freely  here      I ve included a subset of the dataset in the repo  located at  dataset   directory  This subset includes 6 of the 20 newsgroups   space    electronics    crypt    hockey    motorcycles  and  forsale     When you run  main py  it asks you for the root of the dataset  You can supply your own dataset assuming it has a similar directory structure    UTF 8 incompatibility   Some of the supplied text files had incompatibility with utf 8    Even textedit app can t open those files  And they created problem in the code  So I ll delete them as part of the preprocessing    Requirements       python 2 7       python modules        scikit learn  v 0 11      scipy  v 0 10 1    colorama   termcolor   matplotlib  for use in  plot py       The code   The code is pretty straight forward and well documented    Running the code   python main py    Experiments   For experiments I used the subset of the dataset  as described above   I assume that we like  hockey    crypt  and  electronics  newsgroups  and we dislike the others    For each experiment we use a  feature vector   a  classifier  and a train test splitting strategy    Experiment 1  BOW   NB   20  test   In this experiment we use a Bag Of Words   BOW   representation of each document  And also a Naive Bayes   NB   classifier    We split the data  so that  20   of them remain for testing    Results                      precision    recall  f1 score   support   dislikes       0 95      0 99      0 97       575       likes       0 99      0 95      0 97       621   avg   total       0 97      0 97      0 97      1196       Experiment 2  TF   NB   20  test   In this experiment we use a Term Frequency   TF   representation of each document  And also a Naive Bayes   NB   classifier    We split the data  so that  20   of them remain for testing    Results                      precision    recall  f1 score   support   dislikes       0 97      0 92      0 94       633       likes       0 91      0 97      0 94       563   avg   total       0 94      0 94      0 94      1196       Experiment 3  TFIDF   NB   20  test   In this experiment we use a  TFIDF  representation of each document  And also a Naive Bayes   NB   classifier    We split the data  so that  20   of them remain for testing    Results                      precision    recall  f1 score   support   dislikes       0 96      0 95      0 95       584       likes       0 95      0 96      0 96       612   avg   total       0 95      0 95      0 95      1196       Experiment 4  TFIDF   SVM   20  test   In this experiment we use a  TFIDF  representation of each document  And also a linear Support Vector Machine   SVM   classifier    We split the data  so that  20   of them remain for testing    Results                      precision    recall  f1 score   support   dislikes       0 96      0 97      0 97       587       likes       0 97      0 96      0 97       609   avg   total       0 97      0 97      0 97      1196       Experiment 5  TFIDF   SVM   KFOLD   In this experiment we use a  TFIDF  representation of each document  And also a linear Support Vector Machine   SVM   classifier    We split the data using Stratified  K Fold  algorithm with  k   5     Results     Mean accuracy  0 977      0 002 std    Experiment 5  BOW   NB   KFOLD   In this experiment we use a  TFIDF  representation of each document  And also a linear Support Vector Machine   SVM   classifier    We split the data using Stratified  K Fold  algorithm with  k   5     Results     Mean accuracy  0 968      0 002 std    Experiment 6  TFIDF   SVM   90  test   In this experiment we use a  TFIDF  representation of each document  And also a linear Support Vector Machine   SVM   classifier    We split the data  so that  90   of them remain for testing  Only 10  of the dataset is used for training    Results                      precision    recall  f1 score   support   dislikes       0 90      0 95      0 93      2689       likes       0 95      0 90      0 92      2693   avg   total       0 92      0 92      0 92      5382       Experiment 7  TFIDF   SVM   KFOLD   20 classes   In this experiment we use a  TFIDF  representation of each document  And also a linear Support Vector Machine   SVM   classifier    We split the data using Stratified  K Fold  algorithm with  k   5     We also use the whole  Twenty Newsgroups  dataset  which has  20  classes    Results     Mean accuracy  0 892      0 001 std    Experiment 7  BOW   NB   KFOLD   20 classes   In this experiment we use a Bag Of Words   BOW   representation of each document  And also a Naive Bayes   NB   classifier    We split the data using Stratified  K Fold  algorithm with  k   5     We also use the whole  Twenty Newsgroups  dataset  which has  20  classes    Results     Mean accuracy  0 839      0 003 std    Experiment 8  TFIDF   5 NN   Distance Weights   20  test   In this experiment we use a  TFIDF  representation of each document  And also a K Nearest Neighbors   KNN   classifier with  k   5  and  distance weights     We split the data using Stratified  K Fold  algorithm with  k   5     Results                      precision    recall  f1 score   support   dislikes       0 93      0 88      0 90       608       likes       0 88      0 93      0 90       588   avg   total       0 90      0 90      0 90      1196       Experiment 9  TFIDF   5 NN   Uniform Weights   20  test   In this experiment we use a  TFIDF  representation of each document  And also a K Nearest Neighbors   KNN   classifier with  k   5  and  uniform weights     We split the data using Stratified  K Fold  algorithm with  k   5     Results                      precision    recall  f1 score   support   dislikes       0 95      0 90      0 92       581       likes       0 91      0 95      0 93       615   avg   total       0 93      0 93      0 93      1196       Experiment 10  TFIDF   5 NN   Distance Weights   KFOLD   In this experiment we use a  TFIDF  representation of each document  And also a K Nearest Neighbors   KNN   classifier with  k   5  and  distance weights     We split the data using Stratified  K Fold  algorithm with  k   5     Results     Mean accuracy  0 908      0 003 std    Experiment 11  TFIDF   5 NN   Distance Weights   KFOLD   20 classes   In this experiment we use a  TFIDF  representation of each document  And also a K Nearest Neighbors   KNN   classifier with  k   5  and  distance weights     We split the data using Stratified  K Fold  algorithm with  k   5     We also use the whole  Twenty Newsgroups  dataset  which has  20  classes    Results     Mean accuracy  0 745      0 002 std    So What    This experiments show that text classification can be effectively done by simple tools like TFIDF and SVM    Any Conclusion    We have found that TFIDF with SVM have the best performance    TFIDF with SVM perform well both for 2 class problem and 20 class problem    I would say if you want suggestion from me  use  TFIDF with SVM   academic notes   latex format of my academic notes  Camera Calibration for Soccer Videos   A customized implementation of G Thomas 2007  paper on calibrating a camera for sports videos like Rugby and Football    Description   In a first attempt I want to implement the  Initialization  part of the paper  We already have the camera positions so there is no problem starting from here    Initialization   In this paper  rather than attempting to establish the  correspondence  between world lines and peaks in Hough space  author uses the Hough transform as a means to  quickly  establish a measure of how well the image  matches  the set of lines that would be expected to be visible from a given pose  A  match value  for a set of lines can be obtained by adding together the set of bins in Hough space that correspond to the lines we are looking for  So testing for a set of  N  lines is  O N   computationally  which is fast because  N  is usually less than 20    They use this  matching  in an  exhaustive  search process to establish the match value for each pose that we consider  The  speed  of this method must be evaluated    For each  pre determined camera position   the algorithm searches the full range of plausible values of pan  tilt  and field of view  calculating the match value    In this paper  as the only one I ve seen  the  curved lines  are represented as a series of line segments  This needs to be tested  Author used one segment for every 20 degree  So the central circle in the soccer field is represented by 18 line segments  This eliminated the need to handle curves and lines separately  and thus simplifies the implementation    Line detection filter   Author designed the line detection filter assuming that we know the orientation  vertical vs horizontal  and width of the line that we want to filter out  But in reality we do  not  know the orientation and width of the line  thus we apply this filter with several width size and orientation combinations and add their output together    The filter is designed so that it will ignore regions which are significantly wider that a pitch line  The filter is applied either horizontally or vertically  The width thus needs to be adjusted to reflect the its  width in the horizontal or vertical direction  as appropriate  The  local maximum  of the filter output is taken to identify  a pixel at the center of the line     The filter is applied to the  blue  component of the image    The adjacent pixels must also have a color in the range expected for grass  For this the author suggests using a hue based chroma keyer  The keyer needs to merely indicate areas unlikely to be grass  so that immediately adjacent areas are not considered as possible lines    The variant of Hough transform   Spatialised Hough transform   A variant of Hough transform is used that maintains some of the spatial information  Because the peak in the Hough transform from a short line segment  or a curve line segment  may be no higher than a peak caused by samples from several other line segments and from the limbs of a player  that coincidentally happen to be co linear  Thus short line segments may be incorrectly inferred  The catch is that information that caused the genuine peak came from samples in a specific localized area  whilst the other came from a spatially diverse area    How to retain spatial information in Hough transform   Simply divide the image into S 1D segments  This maintains a common set of bins for the whole image  with each bin being sub divided into S sections  For simplicity we divide the line by reference to either the horizontal portion of the image in which it lies  for lines that are closer to horizontal than vertical       or the vertical portion  for lines that are closer to vertical     Thus to determine the sub bin that a given pixel contributes to  it is only necessary to check its  x  or  y  coordinate  depending on the angle that the bin in the transform corresponds to    The resulting Hough space has 3 dimensions  distance and angle  as in a conventional Hough transform  and  distance from picture edge   measured from either bottom or left edge of the picture frame depending on the slope of the line  This third axis  distance from picture edge  has length S    The catch is that both vertical and horizontal are divided into S sub regions  so upper and left sections have a  common  sub Hough  Also not all lines use all sub Houghs  But since memory is not an issue for now  and this method is really quick and simple  we will stick with this  If we add together all the sub Houghs we get a conventional Hough transform    How to use the line detection filter   The line detection filter assumes that we know the  orientation  and  width  of the lines  which we do not  With a range of assumed line widths  and using both the vertical and horizontal direction  we apply the filter to the image several times  We add the outputs together  then for each pixel which is more than a  threshold   we add a value to the appropriate sub bins  proportional  to the summed output of the line detection filter    Implementation   Step 1   Use the line detection output with several parameters and calculate the spatialised hough transform    Step 2   Search the possible values of pan  tilt and zoom for a  specific camera position   For each plausible value of pan  tilt and zoom  project the lines of the pitch model into the image  The  step size  for search is set to be equivalent to a fixed number of pixels in the image        For each  candidate pose  that  at least 3 lines are visible   we compute the list of sub bins that correspond to visible lines  add their contents together to get the match score of this pose  We  ignore poses  close to other poses with a higher match value  as they are not useful local maxima    How to speed up the search   As the set of sub bins to be used for each camera pose only depends on camera position and pitch model  this list can be pre computed for a specific camera position and thus significantly increase the search speed    The set of 5 to 10 best poses are used in step 3  Note that poses close to each other are ignored    Step 3   The third step is tracking which we are not concerned with right now    Tests     The speed of initialization    Evaluate the assumption that a curved line can be represented by line segments    Evaluate the effect of including curved lines as line segments versus completely eliminating them    Evaluate the effect of the spatial Hough transform versus regular Hough transform    The line detection filter used is complicated and time consuming  What is the effect of changing this filter to a simpler filter      Blue component   Does blue component of the image distinguish well between the green grass and the white line    Yes it does  Certainly you don t want the green component  Below you see the blue component     Effect of chroma keyer on line detection output   Below you can see the output of using the chroma keyer on the output of the line detection filter    Without Chroma Keyer       With Chroma Keyer       As you can see  when not using the chroma keyer there are lots of noise in the output of the line detection filter  spatially in area of crowd in the stadium and around the soccore pitch    Questions   Q   How do we know if a line in the image is closer to horizontal or vertical      A   When calculating the Hough transform  each pixel in the frame space  corresponds to a set of bins in the Hough space  According to the angle that the bin corresponds to we can decide whether the line is more horizontal or more vertical  then by checking its  x  or  y  coordinates we decide which sub Hough space we should use    Q   I have no idea why applying the line detection filter several times and then adding their outputs together makes sense  Why first design a powerful line detection filter with orientation and line width parameters  and then add together output of the filter for different values of its parameters blindly    Q   How is the step size determined in the search step  It is set to be equivalent to a fixed number of pixels in the image  but the fixed number of pixels seems to change with equal increase in zoom value  In the paper the author says for zoom consider the motion caused at the edge of the image    Tasks       Implement line detection filter        Implement the spatialised Hough transform described in the paper      Implement the search algorithm for initialization      Code   The code is written in Python 2 7    Dependencies      OpenCV 2 with python bindings    numpy    1 6     High level details      I ve used  CvMat  for images    I ve used numpy arrays   No numpy matrix is used    I ve tried my best to prevent using  for  loops in code      To Do     cleanup code for hue based chroma keyer      write comment for  line filter       test the overflow error on the chroma keyer      Notice   This work is done by  Yasser Souri  in collaboration with Mehran Fotouhi in  Image Processing Laboratory  IPL     Computer Engineering Department  in  Sharif University of Technology   under supervision of  Prof  Shohreh Kasaei     Some parts of the  Description  section is copied from the G Thomas 2007  paper    Images in  images  directory are copy righted  Academic or non academic use requires permission from Image Processing Laboratory in Sharif University  If you cite our work  it is basically OK for academic use  but asking for permission is needed    Reference   G Thomas 2007   Graham A  Thomas  Real time camera tracking using sports pitch markings  J  Real Time Image Processing  2007    Contact   For inquiries regarding the implementation and licensing please contact Yasser Souri    Image Processing Laboratory  IPL  Room 822  Computer Engineering Department Sharif University of Technology yassersouri gmail com ysouri ce sharif edu    Or you could simply use the  github issues   weka java   doing machine learning with WEKA  in JAVA  Azadi Stadium s Football Pitch Model   Azadi Stadium s Football Pitch Model   The points   This is a semi documentation  specifying the naming convention of the points and the world coordinate system and origin    Parametric Camera Projection Matrix   Parametric Camera Projection Matrix   Pinhole Camera  pykov   python implementation of inference and learning in HMM   Here I try to implement inference  evaluation and decoding  and learning  algorithms in python for Hidden Markov Models  These algorithms are fairly easy dynamic programming ones    Dependencies     python 2 7     numpy    1 8 0     Features   log scale   All the computations are done in log scale for more stability and robustness  This way really small probability values can be computed effectively    static observation matrix   The matrix  B   the probability of each observation in each state is static and cannot change  For dynamic matrix  B  one can make small edits in the evaluation algorithm  But I suggest going for a  sum product  algorithm in more complicated cases    multinoulli observation model for learning with EM    In learning with EM  Baum Welch  algorithm  The observation model that is currently implemented is the multinoulli  that is in each state the observations are discrete each is generated with a static parameter     TODO       Implement evaluation using Forward algorithm        Implement decoding using Viterbi algotithm        Implement the algorithms using the logarithm scale to make it possible to calculate very large or very small numbers        Implement learning      Test the implementations    Covert to a python package    Create some documentation    Put the package on pypi    Implement some actually useful demos like POS tagging   part 19 6 2 1 of Murphy s Book      Reference   You can read more about HMM algorithms in many references including  Wikipedia    paper by Rabiner  or chapter 17 of  Murphy s Book   pitch annotator   Annotate sports pitch lines in images mahv         Criminisi s Image Inpainting Algorithm   Requirements     c     opencv 2 4 9   cmake    2 8     Running the code   cd build cmake    make   main app   The GUI for selecting target region   What is the target region    Target region is the part of the image that one needs to remove  The target region will get filled such that the human eye will not notice the removal    Behavior   The user must be able to select any region of the image with clicks of mouse  The region that the user selects will be a polygon resulted from clicks of the mouse    At first the user is in  no selection  mode    The user starts with a click  When the first click is done  the user is in  selection  mode  In this mode more and more points are added to the set of points that make the polygon  or by using  u  keystroke  as undo  the last point gets removed from the polygon  Pressing  u  can result in going back to  no selection  mode if the set of points gets empty    Exiting the  selection  mode can happen in two ways  1  When the user uses  right click   which results in finishing the polygon and going to  done  mode  Or When the user clicks  esc  keystroke  as cancel  that results to going back to  no selection  mode    Simple Augmented Reality   With only OpenCV and Python   input   An image with a chessboard  only one would be enough  but in this example we have 3      annotation   Now we need some chessboard corners for camera calibration  the user interface for clicking the corners is quit neat      object   Here we need an object in PLY format  The format the meshlab can view    Output   The dinasour can now appear anywhere on the board      It can also walk on the walls       Omgh         Omgh means  depth  in Persian  It must be related to deep learning then  Writings related to my seminar   These are the writings related to my seminar report and presentations    All rights are reserved for  Sharif University of Technology    Department of Computer Engineering   Seminar Report   Seminar Presentation Fast Bird Part Localization for Fine Grained Categorization   Accepted to   FGVC 2015   Reviews   Final Paper   Poster    coming soon   Copyright   All rights are reserved for  Yaser Souri   You are not allowed to use any of the materials in this repository in any form  except reading them  without direct permission from Yaser Souri  fast bird part localization   Code for Fast Bird Part Localization part of the following paper    Fast Bird Part Localization for Fine Grained Categorization    Yaser Souri  Shohreh Kasaei    The Third Workshop on Fine Grained Visual Categorization  FGVC3  in conjunction with CVPR 2015   The code for classification part is very simple and not included in this repository    Requirements     Python 2 7  This might not work with Python 3    A recent installation of  caffe  with its python wrapper   I have installed  this version  of caffe     OpenCV 2 4 with its Python wrapper    All other python requirements are mentioned in  requirements txt     Getting Started   For testing or training you need the pretrained CaffeNet network  You can download it from  this url   After downloading it  make sure you change the  src settings py  file and change the  CAFFE NET PRETRAINED  variable accordingly    Training a new head detector   This can be done using the CUB dataset  First download the CUB 200 2011 dataset from  here  and extract it  Then change  src settings py  file and set the  CUB FOLDER PATH  varialble accordingly    Then run the following command   shell python create rf model py   This will create a head detector for you in the models directory  To run this script you will night large amount of RAM   30GB     Changing  part name  variable in  create rf model py  file to  body  instead of  head  will create a detector for body    Testing   For localization you can run something like this       python import sys sys path append   path to projectroot    from fast bird part localization import settings sys path append settings CAFFE PYTHON PATH  import caffe from fast bird part localization import detector   fbp   detector   path to project models head model mdl     img   caffe io load image   path to bird jpg   head  head prob   fbp detect img  fbp draw img  head  head prob      This is the result you get      You can also take a look at the  notebook example   Work in Progress   I m in progress of cleaning up my original code and putting it here  So please wait a little while    TODO       X  Extracting code from notebook about postprocessing      X  Extracting code from notebook about visualization      X  Adding description on how to use the pretrined model      X  Adding description on how to train a new model from CUB         Adding description on how to train a new model from own data         Adding code for PCP evaluation         Adding bounding box regression for better PCP  Sharif s Master Thesis Template   LaTeX template for Sharif s Computer Engineering Master Thesis   I am  not  the creator of this template  And I don t know who is  If you are the original creator of this template let me know to give you credit here    Ali Zarezade  gave me this template    I have used this template for my master thesis in AI group  and I think it shall work for other groups and departments with appropriate adjustments as well    Ghiaseddin                This repo contains the code for the paper  Deep Relative Attributes  by Yaser Souri  Erfan Noury  Ehsan Adeli Mosabbeb    The paper   Deep Relative Attributes  by Yaser Souri    yassersouri    Erfan Noury    erfannoury    Ehsan Adeli Mosabbeb    eadeli    ACCV 2016    The paper on arXiv   arxiv 1512 04103   The name   The name is  Ghiaseddin  which is written as              in Persian Arabic  It is pronouned as  Ghiy th ad D n   Ghias or        is the Persia Arabic word that refers to the act of comparing two things  which is what we actually do in relative attributes   Furthermore Ghiaseddin has a relation to  Ghiy th al D n Jamsh d al K sh                              where  Ghiaseddin  is pronounced similar to the first name of  Jamsh d al K sh   but written with different letters in Persian Arabic               vs                  Dependencies   The code is written in Python 2 7 and uses the  Lasagne  deep learning framework which is based on the amazing  Theano   These two are the main dependencies of the project  Besides these you will be needing CUDA 7 and cuDNN 4  It might work without CUDA or with lower versions but I have not tested it    To visualize the training procedure I have used  pastalog   If you want to see the loss decrease in realtime you will have to install it  optional     For a complete list of dependencies and their versions see  requirements txt     Downloading files   If you want to perform training yourself  you need to download some files  initial weights files and dataset images     Downloading datasets   Zappos50K   bash python  path to project ghiaseddin scripts download dataset zappos py   LFW10   bash python  path to project ghiaseddin scripts download dataset lfw10 py   OSR and PubFig   bash python  path to project ghiaseddin scripts download dataset osr pubfig py   Downloading initial weights  models pretrained on ILSVRC    GoogLeNet   bash python  path to project ghiaseddin scripts download weights googlenet py   VGG16   bash python  path to project ghiaseddin scripts download weights vgg16 py   Running our experiments  reproducing our results    We have used Titan Black  Titan X  and Titan 980 Ti GPUs to produce our results    The random seed can be set at  ghiaseddin settings py   We have used 0  1 and 2 as our random seeds for Zappos50k2  LFW10  OSR and PubFig experiments   Zappos50k1 already has 10 different splits of training data so we have only run the full experiment once with 0 as random seed    To reproduce our results you can run the following scripts which will output the accuracies    bash   run zappos1 sh   for Zappos50k1 experiment   run zappos2 sh   for Zappos50k2 experiment   run lfw sh   for LFW10 experiment   run osr sh   for OSR experiment   run pubfig sh   for PubFig experiment   Our results   We report mean and std of ranking prediction accuracy over 3 different runs for OSR  PubFig  LFW10 and Zappos50k2  fine grained  and over the 10 splits  provided with the dataset  for Zappos50k1    Currently  7th Sep 2016  our results on OSR  PubFig  Zappos50k1 and Zappos50k2 are state of the art to our knowledge    OSR     Method          Natural           Open              Perspective       Large Size        Diagonal Plane          Depth Close       Mean                                                                                                                                                                        Ours  VGG16     99 40   0 10     97 44   0 16     96 88   0 13     96 79   0 32     98 43   0 23     97 65   0 16      97 77   0 10        PubFig     Method          Male              White             Young             Smiling           Chubby            Visible Forehead      Bushy Eyebrows           Narrow Eyes               Pointy Nose              Big Lips               Round Face              Mean                                                                                                                                                                                                                                                                  Ours  VGG16     95 50   0 36     94 60   0 55     94 33   0 36     95 36   0 56     92 32   0 36     97 28   0 49     94 53   0 64     93 19   0 51     94 24   0 24     93 62   0 20     94 76   0 24      94 52   0 08        LFW10     Method          Bald Head         Dark Hair         Eyes Open         Good Looking      Masculine Looking    Mouth Open        Smile             Visible Teeth     Visible Forehead    Young             Mean                                                                                                                                                                                                                                                     Ours  VGG16     81 14   3 39     88 92   0 75     74 44   5 97     70 28   0 54     98 08   0 33        85 46   0 70     82 49   1 41     82 77   2 15     81 90   2 00       76 33   0 43      82 18   1 08        Zappos50k1     Method          Open              Pointy            Sporty            Comfort           Mean                                                                                                                                    Ours  VGG16     95 37   0 82     94 43   0 75     97 30   0 81     95 57   0 97      95 67   0 49        Zappos50k2  fine grained      Method          Open              Pointy            Sporty            Comfort           Mean                                                                                                                                    Ours  VGG16     73 45   1 23     68 20   0 18     73 07   0 75     70 31   1 50      71 26   0 50        Doing your own experiments   Training a new model   First start the pastalog server  Optional     bash  path to project ghiaseddin scripts start pastalog sh   Then you can use ghiaseddin to train       python import sys sys path append   path to ghiaseddin    import ghiassedin   zappos   ghiaseddin Zappos50K1 ghiaseddin settings zappos root  attribute index 0  split index 0  googlenet   ghiaseddin GoogeLeNet ghiaseddin settings googlenet ilsvrc weights  model   ghiaseddin Ghiaseddin extractor googlenet  dataset zappos    possibility to add other options   train the model for 10 epochs   losses      for i in range 10       loss   model train one epoch       losses append loss    or like this   losses   model train n epoch 10    here losses is a list of size 10   save the trained model   model save   path to model pkl         Calculating accuracy of a model      python   calculates the relative attribute prediction accuracy   print model eval accuracy         Visualizing saliency      python   randomly generates saliency maps for 10 samples of the testing set   fig   model generate saliency size 10    or you can specify which pairs   fig   model generate saliency  10  20  30  40     and you can easily save the figure   fig savefig   path to file saliency png         Here are some example saliencies  Not all saliencies are easily interpretable as these     OSR   Natural       Zappos50k1   Open       Zappos50k1   Pointy       LFW10   Bald Head       LFW10   Good Looking       Reference   If you use this code in your research please consider citing our paper     inproceedings souri2016dra    title  Deep Relative Attributes     author  Souri  Yaser and Noury  Erfan and Adeli  Ehsan     booktitle  ACCV     year  2016      Feedback   We are not experts in Theano and or Lasagne or in Deep Learning  So please provide us with your feedback  If you find any issues inside the paper please contact Yasser Souri  yassersouri gmail com   If you have issues or feedback related to the code  please use the  Github issues  section and file a new issue  paper notes   yet another paper notes repository   Papers   Future Prediction       An Uncertain Future  Forecasting from Static Images using Variational Autoencoders  Walker et al  ECCV 2016   Review       Visual Dynamics  Probabilistic Future Frame Synthesis via Cross Convolutional Networks  Xue et al  NIPS 2016   Review       Optical Flow     Dense Optical Flow Prediction from a Static Image  Walker et  al   ICCV 2015   Review   Yadbegir             Learn about things pytorch deep sets   PyTorch implementation of parts of  Deep Sets   NIPS 2017    Requirements     Python 3 6   PyTorch 0 3   torchvision   matplotlib   click   tqdm   Task Driven Object Detection   Author s implementation of the paper   What Object Should I Use    Task Driven Object Detection      If you use our code  please cite our paper    latex  inproceedings cocotasks2019      Author       Sawatzky  Johann and Souri  Yaser and Grund  Christian and Gall  Juergen       Title         What Object Should I Use    Task Driven Object Detection        Booktitle     CVPR        Year         2019      Table of Contents     Requirements   Installing COCO API       Getting The Data   COCO Dataset   COCO Tasks Dataset   Final Directory Structure       Reproducing Results   General Information   Settings   Seeds   Running on Detections vs  on Ground Truth Bounding Boxes   Changing the Detector       Baselines   1  Classifier Baseline   2  Ranker Baseline       Ablation Experiments and Proposed Method   1  Ablation  Joint Classifier   2  Ablation  Joint Classifier   Class   3  Ablation  Joint GGNN   Class    Weighted Aggregation    4  Proposed Method    Fusion            Bugs  Feedback and Questions     Requirements   The two main dependencies are      Python 3 6   PyTorch 1 0 x     Other dependencies are listed in the  requirements txt  file    Installing COCO API   There is only 1 special dependency and that is COCO s Python API  The code for the API is already included in the repository  You just need to do the following to compile it    bash cd src external coco PythonAPI make   Getting The Data   COCO Dataset   We need the train and validation COCO 2014 dataset  These can be downloaded from  here   Or you can run the following commands to download them       bash mkdir   mscoco    I assume your main directory will be at your home directory    what I usually do is  I symlink   mscoco to a place that has enough space    wget http   images cocodataset org zips train2014 zip wget http   images cocodataset org zips val2014 zip unzip train2014 zip  d   mscoco  unzip val2014 zip  d   mscoco        COCO Tasks Dataset   Other than the COCO dataset images  you need the COCO Tasks dataset annotation  These can be downloaded from  here   Or you can run the following commands to download them  For the following command to run  you need  Git LFS  installed    bash cd   mscoco    The same root directory as above  git clone  b cvpr2019   depth 1 git github com coco tasks dataset git coco tasks   Final Directory Structure   At the end inside    mscoco  you should have a directory structure like this      mscoco      train2014          COCO train2014 000000000009 jpg         COCO train2014 000000000025 jpg                 val2014          COCO val2014 000000000042 jpg         COCO val2014 000000000073 jpg                 coco tasks          annotations              task 1 test json             task 1 train json                         image lists              imgIds task 1 test txt             imgIds task 1 train txt                         detection faster json               Reproducing Results   General Information   Settings   We have a special settings file   src coco tasks settings py     The main thing that is specified in the settings file is the location of stuff    Here is a description of the things that you might want to change    python COCO ROOT    location of the root folder for data TB ROOT    location that Tensorboard will write its data SAVING DIRECTORY    location where we will save trained models and results   Seeds   We tried for our results to be reproducible  We set the random seeds    python random seed random seed  np random seed random seed  torch manual seed random seed  torch cuda manual seed all random seed    But this is not enough  There is still sources  e g  cuDNN  that introduce stochasticity  To combat this  we run our training and testing 3 times with 3 different random seeds  0  1 and 2    On all of the following training and testing scripts  one can easily set the seed using the    random seed 0  command line option  0 is the default seed    Running on Detections vs  on Ground Truth Bounding Boxes   As mentioned in our paper  we train on ground truth bounding boxes and test in two settings  on detected bounding boxes  by default  and on ground truth bounding boxes  perfect detector setting     By specifying    test on gt True  you can perform the testing on the ground truth bounding box setting  This command line option works on all of the following python scripts  If you have already once run the code without this option  it means that you have already trained your model and tested it on detected bounding boxes  You can now only perform testing on ground truth bounding boxes by    test on gt True   only test True   This will use the already trained models and only performs testing    Changing the Detector   We have trained Faster RCNN and YOLOv2 of a subset of the COCO images that do not intersect with our test set  We them performed object detection on all images in our test set  The result is provided as part of our dataset release  If you prepared your data as I described above  they should be located at    mscoco coco tasks detections faster json  and    mscoco coco tasks detections yolo json     By default our code is set to use the Faster RCNN detector  as specified in the  settings py  file    python COCO TASKS TEST DETECTIONS   os path join COCO TASKS ROOT   detections faster json     You can change that line in the  settings py  file to use YOLO    python COCO TASKS TEST DETECTIONS   os path join COCO TASKS ROOT   detections yolo json     Baselines   1  Classifier Baseline   This baseline corresponds to 4th row of Table 2 in our paper  Run the following   bash python src single task classifier baseline py   task number 1   random seed 0   This is a single task baseline  which means the model is trained for each task separately    A good bash script to do all the training and testing would be       bash     usr bin env bash   task numbers  1 2 3 4 5 6 7 8 9 10 11 12 13 14  seeds  0 1 2  for i in    task numbers       do     for s in    seeds           do         python src single task classifier baseline py   task number   i    random seed   s          python src single task classifier baseline py   task number   i    random seed   s    test on gt True   only test True     done done       2  Ranker Baseline   This baseline corresponds to 3rd row of Table 2 in our paper    This baseline is different from all other baseline and methods in one major way  for training the ranker we have to use pairwise comparisons    To generate these training pairs you need to first run the following script    bash python src create pair files for ranker py   Then you can train the actual baseline    bash python src single task ranker baseline py   task number 1   seed 0   Ablation Experiments and Proposed Method   1  Ablation  Joint Classifier   This ablation experiment corresponds to 2nd row of Table 3 in our paper    a  joint classifier     bash python src ablation joint classifier py   random seed 0   There is no need to run this script for tasks one by one  it will train for all tasks at the same time and will test for each task one by one    2  Ablation  Joint Classifier   Class   This ablation experiment corresponds to 3rd row of Table 3 in our paper    b  joint classifier   class     bash python src ablation joint classifier withclass py   random seed 0   3  Ablation  Joint GGNN   Class    Weighted Aggregation    This ablation experiment corresponds to 4th and 5th rows of Table 3 in our paper    c  joint GGNN   class  and   d  joint GGNN   class   w  aggreg    Notice that the Weighted Aggregation only has an effect at test time  So if you train a model  you can test it with and without weighted aggregation    Run the code below to train a model for this ablation experiment and test it without weighted aggregation    bash python src ablation ggnn py   random seed 0   weighted aggregation False   Now you can test it with weighted aggregation like this    bash python src ablation ggnn py   random seed 0   weighted aggregation True   only test True   4  Proposed Method    Fusion    This corresponds to our proposed method which are reported in 6th and 7th rows of Table 3 in our paper    e  proposed  and   f  proposed   fusion     Notice that fusion only has an effect at test time    Training a model and testing it without fusion    bash python src ggnn full py   random seed 0   fusion none   Testing the trained model with fusion    bash python src ggnn full py   random seed 0   fusion avg   Bugs  Feedback and Questions   Feel free and open issues on this repository or contact me directly  souri iai uni bonn de     Fandak         Coming soon      Will help you train your models for research    Installation    exclamation  Requires Python 3 7  snake     pip install fandak   Release Notes   Release Notes   Examples   See  examples  directory    Visualizing the effects of hyper parameters   Sample usage    bash python  m fandak hyper  path to root metric1  metric2     exp name baseline       params list path to params list txt    If no exp name is provided  then all are going to be considered  exp name can be something usable from glob    We also have to specify the params list  What to have in the figure  Otherwise  we look at the experiments and find the params that are not the same among them  If a param is missing from a config file  then it is replaces with      MuCon     Official implementation of  Fast Weakly Supervised Action Segmentation Using Mutual Consitency      article mucon2021      Author       Souri  Yaser and Fayyaz  Mohsen and Minciullo  Luca and Francesca  Gianpiero and Gall  Juergen       Title         Fast Weakly Supervised Action Segmentation Using Mutual Consistency        Journal     PAMI        Year         2021      General Information   The code base is built using the  fandak  framework    Dataset   Breakfast   Step 1   Downloading Breakfast dataset with I3D features   https   zenodo org record 5179904  These features are obtained from the  MS TCN  project and only restructured      Everything needed is included in  breakfast i3d zip  file    Step 2   Create a directory at    work MuCon   If you don t have enough space at your home directory       then use a symlink    Then do the following       bash cd   work MuCon mkdir datasets cd datasets   If you have already downloaded the zip move it here   mv  path to downloaded breakfast i3d zip      Otherwise download the zip here   wget  https   zenodo org record 5179904 files breakfast i3d zip download 1    output document  breakfast i3d zip  unzip breakfast i3d zip rm breakfast i3d zip   optionally  if you don t need it        MuCon   Step 1  Create the root directory   bash cd   work MuCon mkdir root   Intermediate results  models  tensorboard logs  etc  will be saved in this directory  If you don t have enough space at     create a symlink    Step 2  Copy the repo   Docker and Nvidia Docker is the most important requirements  All other requirements will be installed automatically inside docker    bash cd   repo    or any other location you wish  git clone https   github com yassersouri MuCon git cd MuCon   Step 3  Build the docker image   bash cd docker pytorch1 1   build sh   Step 4  Run the experiment   Using the below code you can run the default training and testing on split 1 of the breakfast dataset   Note   You have to set the  CUDA VISIBLE DEVICES  environment variable   bash cd   repo MuCon export CUDA VISIBLE DEVICES 0   run docker pytorch1 1 sh python src train test mucon py   cfg src configs docker inside yaml   set dataset split 1  There are various other configs available as well     For data   src core config py     For MuCon   src configs mucon default py  You can change the configs by passing inline arguments  passing address to yaml files or changing the defaults    The training progress is shown as below    2021 08 27 16 07 28 884565  Training for run number  1  2021 08 27 16 07 28 884773  Training epoch 1     100                                                                    1460 1460  01 39 00 00  14 67it s   2021 08 27 16 09 08 437369  Training epoch 2      36                                                                     521 1460  00 36 00 57  16 23it s    This should take less than 12 hours to train and test on modern hardware and GPU    At the end of the program it will output the results of training  which looks like something like this    MuConEvaluatorResult y mof 0 4374427438716199  y mof nbg 0 4255722067659505  y iod 0 724194675038116  y iou 0 39804877235464414  s mof 0 441788286580743  s mof nbg 0 4227709493882079  s iod 0 4809540489892753  s iou 0 3644463243702167  s iod nbg 0 4265855312320258  s iou nbg 0 31716805755780547  s mat score 0 7588650138682775  s len diff 1 3214285714285714  vit mof 0 47972540750568615  vit mof nbg 0 46599277794190475  vit iod 0 5163879039088691  vit iou 0 39922937940944814  vit iod nbg 0 46699351233834946  vit iou nbg 0 35887711811445244    Here you can see various metric  The main metric of interest is  vit mof   FIFA   Official Implementation for  FIFA  Fast Inference Approximation for Action Segmentation    Data   Step 1 is to download the data  You can use the  download data py  script to download the  data for the Breakfast dataset       shell python download data py   then select the I3D breakfast and select Okay    after that downloading starts          Coming Soon   The code to reproduce the results will be released soon  Code for  Robust Action Segmentation from Timestamp Supervision    BMVC 2022   Official implementation of  Robust Action Segmentation from Timestamp Supervision      inproceedings robust seg2022      title            Robust Action Segmentation from Timestamp Supervision        author          Souri  Yaser and Abu Farha  Yazan and Bahrami  Emad and Francesca  Gianpiero and Gall  Juergen       year           2022      booktitle        BMVC       Major parts of the code is adapted from  1     Requirements     Python    3 7    CUDA GPU     Other python requirements are specified in the  requirements txt  file    Data   Download the data from  https   zenodo org record 3625992  Xiv9jGhKhPY  and extract it into  data   at the root of the repository  This is the data provided by  1     Running the Experiments   Below is an example of how to run the experiment  One needs to adjust the arguments to the script for different dataset  splits  and the amount of timestamp annotations    shell python src main py       dataset 50salads       split 1       timestamp percentage 90   Parameter     The parameter   from the paper can be specified in the code by setting the  pgt config loss mul empty  argument  For example    shell python src main py       dataset 50salads       split 1       timestamp percentage 90       pgt config loss mul empty 0 5   Running the  1  baseline   The type of the pseudo ground truth should be set by  pgt type baseline     shell python src main py       dataset 50salads       split 1       timestamp percentage 90       pgt type baseline   Running the Oracle experiment   The type of the pseudo ground truth should be set by  pgt type oracle     shell python src main py       dataset 50salads       split 1       timestamp percentage 90       pgt type oracle   References    1  Temporal Action Segmentation from Timestamp Supervision   Zhe Li  Yazan Abu Farha  Juergen Gall   IEEE CVF Conference on Computer Vision and Pattern Recognition  CVPR   2021    https   github com ZheLi2020 TimestampActionSeg misc
38,transcranial,DiseaseGraph   Link   DiseaseGraph is a visual representation of the relationships shared by diseases and the strengths of these relationships  Nodes represent diseases and link widths represent the strengths of association between them    Each graph has a central node from which all other nodes and links are created  The node sizes and the link widths are all normalized to each graph  so that the actual numerical values may differ from one graph to another    Within the database are 4 672 diseases or disorders  These are categorized into 27 broad categories  and a disease may fall into one or more of these categories    Over 11 million PubMed abstracts were analyzed  The size of the nodes represent how often a disease is mentioned in the medical literature    The method used here captures high level abstract relationships  with no delineation of the type of relationship  The association link between two diseases may be based on similar presentations  similar pathologic process  similar affected anatomic locations  or any number of relationship types  Two linked diseases may be even causal  but there is no inference of causality or temporality in the graph itself    Copyright 2013 4  Leon Chen  Trove   Radiology resident dashboard   Screenshots                             Notes   Installation instructions      1   unzip   into desired folder     2   Modify   of trove dev server config environment index js    port  process env PORT      to desired serving port      3   Start up a mongod  MongoDB  instance     4   To deploy  type in the command  npm start   Recommended Usage      Deploy TROVE in two instances          Instance 1 will be the main listener for HL7 JSON messages          Instance 2 will be the main web server          See step 2   for assigning different ports to the two instances   Database Information  MongoDB       When  npm start  is called from 4   of Installation instructions  it will create a database instance   trove dev   In trove dev  there are two main collections to be associated with            Studies  db studies            Users  db users    Users must be of the form          alt name     Zoe Miller        badges             full name     Zoe Anne Miller        minnies    1766       userId     118        username     zam7001     The full name is used with assistant radiologist and radiologist to associate the order with a  userId   The username is used with passportjs to do authentication    IMPORTANT  users must be created manually or from script previously before any data can be stored from the HL7 JSON listener route    Authentication   User Creation      Please see          http   danialk github io blog 2013 02 23 authentication using passportjs    https   github com jaredhanson passport local blob master examples express3 mongoose app js     for information associated with authentication  The current distribution will require a developer to modify the code for authentication to work properly    Server HL7 JSON processing Information      The main route for processing HL7 information is located in trove dev server api study study controller js          processHL7JSON         this method expects JSON messages of the form                                 accession                        Assistant Radiologist names should be of form  Doe  John                     If this field is not filled  the server will try to parse a name from the  report   The name is found by matching the string pattern   var regex      Prepared sBy                             assistant radiologist                        Radiologist names should be of form  Doe  John                     If this field is not filled  the server will try to parse a name from the  report   The name is found by matching the string pattern  var regex      Study sinterpreted sand sreport sapproved sby                            radiologist                      report                      service code                      service description                      result time                        Result status general is a single character of form   P    F                          First  P  status report is the report filed by Assistant Radiologist Resident  Any  P  status message recieved afterwards will be treated as a possibly edited report by the attending radiologist                     F  status means the report has been finalized                   result status                      scheduled time     of form i e   201501010808                        d d d d  year   d d  month   d d  date   d d  hour   d d  minutes    d d  seconds  In that order                   transcribed time     of form i e   201501010808                   completed time    of form i e   201501010808                lorecat   Webapp built using Node and Angular which creates interactive quizzes from wikipedia pages  powered by NLP   Method for Real Time Cortical Oscillation Detection and Phase Locked Stimulation   MATLAB source files for    Link  Chen  L  Leon  et al   Real time brain oscillation detection and phase locked stimulation using autoregressive spectral estimation and time series forward prediction   Biomedical Engineering  IEEE Transactions on 60 3  2013   753 762    Link  Chen  L  Leon  et al   A method for real time cortical oscillation detection and phase locked stimulation   Engineering in Medicine and Biology Society  EMBC  2011 Annual International Conference of the IEEE  IEEE  2011    main functions   findOptParams   offline genetic algorithm for optimizing algorithm parameters   findBestChannel   finds best single channel based on  power in frequency band  temporal coherence of bandpass filtered signal  and by both metrics combined   stimSim   simulated stimulation over an entire dataset  output is summary of stimulation times  frequency  phase  as well as summary of phase locking metrics    stimRT   real time stimulation  output is  t delay     auxillary support functions   freqBandOpt   instPhaseFreq   linearPredAR   zeroPhaseFilter   temporalCoherence Filopod   Full text search and snippet extraction across the full combined corpus of high quality biomedical journal articles  using a pre trained weighted ontology network graph  Note  this package has been merged into  LevelGraph N3   Tricorder   A terminal based system monitoring application written in Go  built on top of  gizak termui  and  shirou gopsutil      Note  CPU monitoring currently does not work on Mac OS X    Work in progress    License   MIT jupyter themer       Apply custom CSS styling to your jupyter notebooks  Contributions are welcome    Mix and match themes by      layout  example   wide           typography  example   serif           color  example   night         You can always revert back to the default      Installation   sh pip install jupyter themer   or   sh python setup py install   Usage   NOTE  Old ipython configuration files  for example at the default     ipython   may need to be removed in order for jupyter to use the correct files    sh usage  jupyter themer   c COLOR    color COLOR                          l LAYOUT    layout LAYOUT                          t TYPOGRAPHY    typography TYPOGRAPHY                          f CODE FONT    font CODE FONT                          b BACKGROUND    background BACKGROUND                          s OPTION    show OPTION                          p PATH    css path PATH    If no arguments are supplied  the program will revert the jupyter notebook style back to default    Any combination of the style types can be specified  and the program will mix together the associated color layout typographic code font background css files accordingly  writing it to the  custom css  file used by the notebook  Use   p  or    css path  to specify a custom path for the css file  for example if you need per user configuration    To get a list of available options use the  show  argument  e g   jupyter themer   show color   For all running notebooks  a quick browser refresh will be needed to apply the stylesheet    Available themes    c    color     3024 day   3024 night   abcdef   ambiance   base16 dark   base16 light   blackboard   cobalt   colorforth   dracula   eclipse   elegant   erlang dark   icecoder   lesser dark   liquibyte   material   mbo   mdn like   midnight   monokai   neat   neo   night   paraiso dark   paraiso light   pastel on dark   rubyblue   seti   solarized light   solarized dark   the matrix   tomorrow night bright   tomorrow night eighties   ttcn   twilight   vibrant ink   xq dark   xq light   yeti   zenburn      l    layout     wide   hovertable      t    typography     serif      f    font     Ubuntu Mono      b    background     dark     License   MIT License transcranial github io hypercube   A simple hypercube tesseract animation  Created in Go using the awesome  ln  library  An older implementation in three js also exists in the  js  folder          License   MIT How To Achieve Atom Editor Transparency   In atom  there is no easy config  yet  to set window or background transparency as you would in iTerm or TextMate  Here s a straightforward guide on how to achieve transparent window awesomeness    This has been tested on both macOS and Ubuntu 14 04 desktop  for Atom versions 1 0 up through 1 11          Atom must be built from source with 2 additional lines of code  This makes Atom run as a frameless window which allows transparency to be enabled within Electron  After  cloning or forking Atom   add the following to  options     coffeescript frame  false transparent  true   in  src browser atom window coffee   pre v1 9  or  src main process atom window coffee  in versions 1 9     changing this    coffeescript options     show  false   title   Atom    backgroundColor    fff          to this    coffeescript options     frame  false   transparent  true   show  false   title   Atom     backgroundColor    fff          Note  backgroundColor  is commented out    Then run    sh   script clean      script build   Refer to the official  build guides  for additional instructions if necessary  You may want to build a debian package  for example    This can take awhile  but once complete  fire up Atom    On linux  add an additional    enable transparent visuals  flag while starting atom    In Atom v1 7   atom must be started with an additional    disable gpu  flag   Otherwise  there will be a lot of UI flickering    Open up your editor LESS stylesheet     shift p  or  ctrl shift p   then  Application  Open Your Stylesheet    and add the following CSS  This is a basic guide   you can experiment with your own settings to get the effect you want  For example  to avoid text on text collisions in the autocomplete popups  I set  atom overlay      to near complete opacity       css html  html       background  rgba 0  0  0  0   important      atom pane  atom panel  atom notification     background  rgba 0  0  0  0 5   important      atom overlay         background  rgba 0  0  0  0 9   important      atom text editor  shadow      cursor line       background color  rgba 0  0  0  0 2   important         selection  region       background color  rgba 0  0  0  0 2   important         gutter       background color  rgba 0  0  0  0   important              In the CSS above  this works pre v1 9    css html       background  rgba 0  0  0  0   important      but for v1 9   this must be    css html  html       background  rgba 0  0  0  0   important      That s it  pretty simple  stochastic depth   Implementation of Stochastic Depth Networks in Keras    http   arxiv org abs 1603 09382    Huang G  Sun Y  Liu Z  Sedra D  Weinberger K  Deep networks with stochastic depth  arXiv preprint arXiv 1603 09382  2016 Mar 30  wide resnet   Implementation of Wide Residual Networks in Keras    http   arxiv org abs 1605 07146    Zagoruyko S  Komodakis N  Wide Residual Networks  arXiv preprint arXiv 1605 07146  2016 May 23  inception v4   Implementation of Google s Inception v4 architecture in Keras    https   arxiv org abs 1602 07261    Szegedy C  Ioffe S  Vanhoucke V  Inception v4  inception resnet and the impact of residual connections on learning  arXiv preprint arXiv 1602 07261  2016 Feb 23  inception resnet v2   Implementation of Google s Inception   ResNet v2 architecture in Keras    https   arxiv org abs 1602 07261    Szegedy C  Ioffe S  Vanhoucke V  Inception v4  inception resnet and the impact of residual connections on learning  arXiv preprint arXiv 1602 07261  2016 Feb 23  StatusBoard   Simple HTTP status checker written in Go  complete with a dashboard for all your configured endpoints    The front end page will automatically subscribe to update events  which are Server Sent Events  All concurrently connected clients will receive the same update events  Slack error notifications can also be configured          Installing   go get github com transcranial statusboard   Config   Modify  static config json   Add as many HTTP HTTPS endpoints as you need  Each endpoint can be configured with its own status check interval  in seconds   and timeout limit  in milliseconds   The only requirement is that  id  be unique for each endpoint    Slack notifications   Add your Slack webhook URL and message settings to the config for error notifications  To skip Slack notifications  these can be left as empty strings    Start server   go run server go   http   localhost 8080   The page will automatically subscribe to update events  Currently it s configured to display events from the most recent hour    Nginx   If running behind an Nginx proxy  the following is required for the SSEs to work    nginx proxy set header Connection     proxy http version 1 1  chunked transfer encoding off    Notes   There is just enough functionality to be useful  but the advantage is that it s extremely easy to setup  If more advanced features are required  such as TCP endpoints  advanced SSL requirements  data persistence  etc   there are some other great libraries  such as Sourcegraph s  Checkup     License   MIT Kaggle 2nd Annual Data Science Bowl Kaggle ultrasound nerve segmentation     This project is no longer active  Please check out  TensorFlow js     The  Keras js demos  still work but is no longer updated                  Run Keras models in the browser  with GPU support using WebGL         Interactive Demos         Documentation           Run  Keras  models in the browser  with GPU support provided by WebGL 2  Models can be run in Node js as well  but only in CPU mode  Because Keras abstracts away a number of frameworks as backends  the models can be trained in any backend  including TensorFlow  CNTK  etc    Library version compatibility  Keras 2 1 2   Interactive Demos               Check out the  demos   directory for real examples running Keras js in VueJS      Basic Convnet for MNIST   Convolutional Variational Autoencoder  trained on MNIST   Auxiliary Classifier Generative Adversarial Networks  AC GAN  on MNIST   50 layer Residual Network  trained on ImageNet   Inception v3  trained on ImageNet   DenseNet 121  trained on ImageNet   SqueezeNet v1 1  trained on ImageNet   Bidirectional LSTM for IMDB sentiment classification     Documentation   MIT License Keras js  demos data   For development or to run Keras js demos locally  copy contents of this repo to  demos data   folder in the Keras js repo  tf keras cuda docker   Docker image for Tensorflow and Keras with CUDA support   Requires  nvidia docker     CUDA  8 0   cuDNN  5   Python  3 5   Tensorflow  0 12 0rc0   Keras  1 1 2     Additional libs    ipython   numpy   scipy   pandas   pillow   scikit image   scikit learn   pydicom   h5py dockerfiles   Collection of various dockerfiles Keras js documentation   Documentation source for  Keras js     Development      mkdocs serve     Deploy      mkdocs gh deploy   MIT License Apollo Client Issue Reproduction   Welcome  If you are here then you were likely referred to this repo when reporting an error to  apollographql apollo client   The core team is invested in making the best client for GraphQL possible  so when you hit an error it is important to the team that the error is resolved as soon as possible    Unfortunately  describing an error in GitHub is often not enough to truly understand the reported issue  By creating a small reproduction test case using this template repo the Apollo Client team will be able to identify and fix your error much faster then they could without    This repo was created with  create react app  for a great developer experience  If you are not using React then a small reproduction case with your framework of choice would go a long way    To make changes in the GraphQL schema make sure to look at the    src index jsx  file where we define a GraphQL schema using  GraphQL js  which will run in the browser    Reproduction Creation Steps     Fork this repository to your GitHub account    By default  cloning this repostiory gives you an Apollo Client 3 0 based application  If you would like to start with a legacy Apollo Client 2 6 application  clone or checkout the  ac2  branch     sh   git clone   branch ac2 git github com apollographql react apollo error template git     Or  after cloning the repository    git checkout  t origin ac2   After cloning  install all dependencies with  npm install     Start the development server with  npm start     Make the changes that will reproduce this error locally    When ready  push your changes back to GitHub and let the  apollo client  team know where they can be found
39,fredmonroe,cts cnn A Basic Stable IDE config for Neovim     Why does this repo exist      This config attempts to provide a rock solid fully featured starting point for someone new to Neovim  or just tired of maintaining the basic IDE components of their config      What makes it  rock solid       All of the included plugins are pinned to a version that ensures they are compatible and will not update potentially introducing errors into your config  For every Neovim release I will update this repo along with the community to keep it up to date with the newest versions    As I mentioned  this config is meant as a starting point for people new to Neovim who want a familiar IDE experience  The config has a very simple structure that makes it easy to add new plugins    Install Neovim 0 8   You can install Neovim with your package manager e g  brew  apt  pacman etc   but remember that when you update your packages Neovim may be upgraded to a newer version    If you would like to make sure Neovim only updates when you want it to than I recommend installing from source    NOTE  Verify the required  build prerequisites  for your system    sh git clone https   github com neovim neovim git cd neovim git checkout release 0 8 make CMAKE BUILD TYPE Release sudo make install   Install the config   Make sure to remove or move your current  nvim  directory   sh git clone https   github com LunarVim nvim basic ide git    config nvim   Run  nvim  and wait for the plugins to be installed   NOTE   You will notice treesitter pulling in a bunch of parsers the next time you open Neovim    NOTE  Checkout this file for some predefined keymaps   keymaps   Get healthy   Open  nvim  and enter the following     checkhealth   You ll probably notice you don t have support for copy paste also that python and node haven t been setup   So let s fix that   First we ll fix copy paste       On mac  pbcopy  should be builtin       On Ubuntu       sh   sudo apt install xsel   for X11   sudo apt install wl clipboard   for wayland   Next we need to install python support  node is optional      Neovim python support     sh   pip install pynvim     Neovim node support     sh   npm i  g neovim   We will also need  ripgrep  for Telescope to work      Ripgrep     sh   sudo apt install ripgrep     NOTE  make sure you have  node  installed  I recommend a node manager like  fnm     Fonts   I recommend using the following repo to get a  Nerd Font   Font that supports icons    getnf   Configuration   LSP   To add a new LSP   First Enter     LspInstallInfo   and press  i  on the Language Server you wish to install   Next you will need to add the server to this list   servers   Formatters and linters   Make sure the formatter or linter is installed and add it to this setup function   null ls   NOTE  Some are already setup as examples  remove them if you want   Plugins   You can install new plugins here   plugins     Plugins     packer   plenary   nvim autopairs   Comment nvim   nvim ts context commentstring   nvim web devicons   nvim tree lua   bufferline nvim   vim bbye   lualine nvim   toggleterm nvim   project nvim   impatient nvim   indent blankline nvim   alpha nvim   tokyonight nvim   darkplus nvim   nvim cmp   cmp buffer   cmp path   cmp luasnip   cmp nvim lsp   cmp nvim lua   LuaSnip   friendly snippets   mason nvim   nvim lspconfig   mason lspconfig nvim   null ls nvim   vim illuminate   telescope nvim   nvim treesitter   gitsigns nvim   nvim dap   nvim dap ui   DAPInstall nvim         The computing scientist s main challenge is not to get confused by the complexities of his own making        Edsger W  Dijkstra
40,Yotooon,image tiles   A simple script for breaking images into tiles    requirements     Python 2 7 x   PIL  pip install pillow     usage       usage  tile py   image   Chop up an image into tiles    optional arguments     i      image                           input image    r INIT RESIZE    init resize INIT RESIZE                         resize image before processing  example  500x500 or 500x to resize width to 500px                          keeping the aspect ratio     t TILE SIZE    tile size TILE SIZE                         tile size  default  2x2     m OVERLAP MARGIN    overlap margin OVERLAP MARGIN                         tile margin  overlap size  default  0     o      output                           output directory     q3df front   Q3Defrag frontend based on InfernoJS and SASS    Running     npm install   npm start dev  to generate development builds or  npm start build  to generate production builds      Nginx config       server       server name  q3df nuc    set  root path  home nuc www q3df front public  root  root path  charset utf 8  index   index html index htm   error log   home nuc www q3df front logs nginx error log   client max body size 8m     serve static files directly location         jpg jpeg gif png webp ico html xml txt woff woff2         root               root path      access log        off      expires           max        Only allow these request methods       Do not accept DELETE  SEARCH and other methods    if   request method      GET HEAD POST           return 444               Notes   You need to have a web server already installed and pointed to the  public  folder Defrag Twitch Console Extension   I advise to use the  Twitch Developer Rig  to get this thing up for testing    This project is based on  React boilerplate   For more detailed instructions see  the instructions file
41,solaremperor,PeerSecure   An Asymmetric key Crypto System library for Arduino that works with any encryption algorithm that uses a 128 bit key eg  AES  bonn randopt   Bayesian Optimization for  Randopt  with     Gaussian Processes    Tree of Parzen Estimators  and   Neural Networks      Here s a simple demo that exposes the universal API  just swap  Bonn  for  Bogp  or  Botpe        python     usr bin env python   import randopt as ro from bonn import Bonn   def loss x  y  z       return x 2   y 2   z  2   if  name       main        e   ro Experiment  bo simple              x   ro Choice  0 0  1  2  3  4  5  6  7             y   ro Gaussian 0 0  3 0            z   ro Uniform 0 0  1 0                bo   Bonn e   e sample all params   res   loss e x  e y  e z  e add result res   for i in xrange 200       bo fit       bo sample e      res   loss e x  e y  e z      print res     e add result res             trial   i                   CoSMo 2017   My repo for the summer school in Computational Sensorimotor Neuroscience  CoSMo  2017    More info about CoSMo can be found at http   www compneurosci com CoSMo    The CoSMo 2017 wiki is here  http   klab smpp northwestern edu wiki index php5 CoSMo 2017   Changing the year number in the end can take you to other year s CoSMo wikis    Day 1  Jul 31  2017      Introduction Organization   Gunnar Blohm   Philosophy of Modelling   Konrad Kording   Sensorimotor Overview   Gunnar Blohm    Plotting Neural Data  afternoon    konradP2     Day 2  Aug 1  2017      Bayesian modelling and Cue combination   Konrad Kording   Decision Making   Paul Schrater   How to Model    Gunnar Blohm     Day 3  Aug 2  2017      Kalman Filters   Paul Schrater   Linear Systems Theory   Gunnar Blohm   Bayesian Nets   Paul Schrater   Konrad Kording   Paper Writing 101   Konrad Kording   more info at http   klab smpp northwestern edu wiki index php5 Paper Writing 101     Day 4  Aug 3  2017      Optimal Feedback Control   Gunnar Blohm   LQG  Kalman LQG   Konrad Kording   Gunnar Blohm     Day 5  Aug 4  2017      could a neuroscientist understand a microprocessor   Konrad Kording   Model Fitting   Luigi Acerbi   Model Evaluation   Gunnar Blohm     Day 6  Aug 5  2017      The Bayesian Brain   Jan Drugowitsch     Sunday  Aug 6  2017      Holiday    Watch some Game of Thrones  Season 7   Episode 4  because it is 2017     Day 7  Aug 7  2017      Decision making and motor Control   Alaa Ahmed   Motor Control   Reza Shadmehr     Day 8  Aug 8  2017      Separate systems for moving and holding still   Reza Shadmehr     Day 9  Aug 9  2017      Understanding Motor Disorders  A Task Dynamic Approach   Dagmar Sternad     Day 10  Aug 10  2017      More about yesterday s tasks   just with different paradigms   Dagmar Sternad     Day 11  Aug 11  2017      Day given to us to complete our project     Day 12  Aug 12  2017      Project presentations begin 12 30 PM and will go on till 5 PM    FrontPanelWrapper   A simple I O broker  board manager and wrapper for the OpalKelly FrontPanel  FPGA USB PCI communication driver  written in and for C      Features     Handles multiple OpalKelly FPGA boards using a simple syntax    Automatically selects the appropriate FrontPanel APIs based on the bus type  USB 2 0 USB 3 0 PCI     Burn a bitfile to the FPGA as soon as we open a connection to it    Sensible error messages         Semi automatic PLL handling for quick and easy deployment      Usage     This library easy to use so long as FrontPanel is already installed on your system  Simply include the include the source code   h   cpp files  and use it in your code    Linux users may also consider using the compiled shared   so  libraries along with the header file    Coming soon     Windows users may  alternatively use the compiled dynamic link library  DLL  along with the header file    Coming soon       OpalKelly FrontPanel Installation   Instructions to install FrontPanel on your computer are available along with the SDK in the  README txt  file     Disclaimers   NOTE 1    FrontPanel  is a registered tradmark of  OpalKelly  which also owns all the code and binaries with the included FrontPanel SDK and APIs  The latest versions can be obtained  here     NOTE 2    FrontPanelWrapper  is distributed under the  MIT license   ingeniaWrapper   A wrapper for ingenialink tailor made to enable multi controller instantiation and monitoring ArduDAQmx   ArduDAQmx  is a portable and simple Arduino esque C wrapper for for the National Instruments NI DAQmx ANSI C API    NOTE  This library is a work in progress and not ready for use yet    Introduction   National Instruments provides a powerful and fully functional C developer API for their DAQ hardware called DAQmx  Unfortunately  there isn t a lot of easily accessible documentation or examples to get people up and running with it  Furthermore  the library is not beginner friendly with a large amount of configuration necessary before users record thir first data sample    ArduDAQmx tries to bridge this gap by providing a simple interface wrapper that makes using NI DAQmx easier by  mostly  emulating the function calls a programmer would make when using an Arduino while also exposing additional features such as timing and triggering support    Usage     This wrapper should work on all platforms supported by NI DAQmx  As of October 2018  NI DAQmx is supported by Windows and Linux operating systems    Before using this library  please install the NI DAQmx software available on the  National Instruments website    You need to get a copy of the NI DAQmx library files from the location where   they are stored as described on the  NI website     Maybe move these files to a particular directory    You may now use the ArduDAQmx wrapper in tandem with the NI DAQmx library      ArduDAQmx Documentation   Documentation for the ArduDAQmx wrapper is available  here     Legal   ArduDAQmx is distributed under the MIT license    DAQmx  and NI DAQmx are trademarks of the National Instruments Corporation    DAQmx software including libraries are governed by the copyrights and patents awarded to the National Instruments Corporation    Please check the  LICENCE  file for more legal information on ArduDAQmx    Location of additional legal information on National Instruments software    Notices are located in the   National Instruments   Legal Information and  National Instruments   directories    EULAs are located in the   National Instruments  Shared MDF Legal license  directory    Review   National Instruments   Legal Information txt  for information on including legal information in installers built with NI products  QtScope   A simple and configurable oscilloscope written in C   using Qt 5  EasyDAQmx   EasyDAQmx  is a simple wrapper for the National Instruments NI DAQmx ANSI C API    NOTE  This library is a work in progress and not ready for use yet    Introduction   National Instruments provides a powerful and fully functional C developer API for their DAQ hardware called DAQmx  Unfortunately  there isn t a lot of easily accessible documentation or examples to get people up and running with it  Furthermore  the library is not beginner friendly with a large amount of configuration necessary before users record thir first data sample    EasyDAQmx  tries to bridge this gap by providing a simple interface wrapper that makes using NI DAQmx easier by  mostly  emulating the function calls a programmer would make when using an Arduino while also exposing additional features such as timing and triggering support    Usage     This wrapper should work on all platforms supported by NI DAQmx  As of October 2018  NI DAQmx is supported by Windows and Linux operating systems    Before using this library  please install the NI DAQmx software available on the  National Instruments website    You need to get a copy of the NI DAQmx library files from the location where   they are stored as described on the  NI website     Maybe move these files to a particular directory    You may now use the ArduDAQmx wrapper in tandem with the NI DAQmx library      EasyDAQmx Documentation   Documentation for the wrapper will be available soon    Legal   EasyDAQmx  is distributed under the MIT license    DAQmx  and NI DAQmx are trademarks of the National Instruments Corporation    DAQmx software including libraries are governed by the copyrights and patents awarded to the National Instruments Corporation    Please check the  LICENCE  file for more legal information on ArduDAQmx    Location of additional legal information on National Instruments software    Notices are located in the   National Instruments   Legal Information  and   National Instruments   directories    EULAs are located in the   National Instruments  Shared MDF Legal license  directory    Review   National Instruments   Legal Information txt  for information on including legal information in installers built with NI products  EE 599 SurajChakravarthiRaja 1389288844   EE599  Accelerated Computing using FPGAs  Assignments   cLinkedList   A simple linked list manager for C C     This is a simple linked list manager for C that I ve been using for a long while now  It may not be as optimized as Boost s libraries for linked lists  but it is very simple to use and has been tested rigorously    This library allows you to dynamically create and manage doubly linked lists of elements that already exist in memory using pointers  This means that the actual data is untouched by this library which essentially manages a linked list of pointers to data    Supported platforms and binaries   In addition to the source  you can download  cLinkedList  as a precompiled library  It s currently  only available as a static library for Windows  WIN32  x86    but I will soon make a DLL available too    At some point  I will also build shared and static libraries for 64 bit PC Linux    Usage   Download the repo and place it in your project directory  Be sure to add the  include  and  bin  directories of the project to your compile and linker paths respectively  You do not need the  source  and  cLinkedList  directories which can be safely remove    Supported functions   1  Linked list creation functions     Create and  initialize  a linked list in dynamic memory    Insert a data item AFTER  an existing linked list element    Insert a data item BEFORE  an existing linked list element    Append a data item  to the end of a linked list    Prepend a data item  to the start of the linked list      2  Linked list reading functions     Get the  first list element data item  of a linked list    Get the  last list element data item  of a linked list    Find the  next list element data item  to a specific element of a linked list    Find the  previous list element data item  to a specific element of a linked list    Find the list element  associated with a particular data item  if linked     Find the data item  in a list if it has been linked      3  Linked list status functions     Get the length  of the linked list    Is the linked list  empty      4  Linked list unlinking functions     Unlink a particular list element  in a linked list    Unlink a list element pointing to specific data item  in a linked list    Unlink all list elements  from a linked list      Documentation   TBD   Doxygen auto generated documents to be added soon    Licensing   This library and its source code is distributed under the  Mozilla Public License v2 0   telecon   A cross platform near real time telemetry and control GUI for C   written using the  wxWidgets  library    Necessary prerequisite  wxWidgets   Linux   On most Linux distributions  wxWidgets packages are available in system repositories under the name  wxGTK   Note that to develop applications using wxWidgets you may need to install the  development  packages and not just the libraries needed for running the applications using wxWidgets    For example  to install wxWidgets 3 0 on Ubuntu 20 04 LTS  you need to run the command  apt get libwxgtk3 0 gtk3 dev  on a terminal    Windows     Download  the latest stable binary zip files for wxWidgets  v3 1 6 used here     You must download the  header files     Then choose either 32 bit or 64 bit app environments to download the corresponding  development files    release DLLs   and  release DLL PDB files         Unzip the files into  C  wx 3 1 6    The  header files  zip is unziped as the  include  subdirectory    The zipped  development files  contain the  build  and  lib  subdirectories along with the  wxWdigets prop  Visual Studio proerty sheet file    The  release DLLs  and  release DLL PDB files  also unzip into the  lib  subdirectory adding additional files to it        To avoid having to hard code the install directory of wxWidgets into your applications  define  wxwin  as an environment variable containing the path   C  wx 3 1 6    to the files we just downloaded and unziped    If using DLLs  append the location of wxWidget s DLL files to the  Path  or  PATH  environment variable  For example  for wxWidgets 3 1 6  the default location of the DLL files is  C  wx 3 1 6 lib vc14x dll    This allows the DLLs to be found by your application during runtime      Included prerequisite  wxMathPlot library   This project ships with a  modified  copy of the wxMathPlot library  It is an open source scientific plotting library for wxWidgets  You can find it in  lib wxMathPlot   and is distributed under the wxWindows license  From June 2007 the project is maintained by  Davide Rondini   who carries on the original work by  David Schalig   The authors can be contacted via the  wxMathPlot s homepage     Setting up the development environment   Windows  Visual Studio 2019 or above      Create a new empty project    Go to  View Property Manager Add Existing Property Sheet  and add  wxWidgets prop   from the downloaded development files in wxWidget s root directory  to it    Change the  Project Properties Linker System Subsystem   property to  Windows     Set  Project Properties Configuration Properties Advanced Character Set  to  Use Unicode Character Set     To  Project Properties C C   General Additional Include Directories    append    SolutionDir    include  and    SolutionDir    lib wxMathPlot include     To  Project Properties Linker General Additional Linker Directories    append    SolutionDir    lib wxMathPlot bin       Coding tips     Be sure to include the headers   wx wxp h   or precompiled headers   wx wxprec h   as necessary to the main source file    Also remember that every app should define a new class derived from the  wxApp  with an overridden  OnInit    function to initialize the program    T5control   Control API for the T5 system at the USC Valero Lab tsLogger
42,farrajota,Caltech s functions for data extraction   Extracts all images and annotations of the  Caltech INRIA Tud Brussels ETH Daimler  Pedestrian dataset from data files with the   vbb  format into   jpg  and   txt  files  respectively    Requirements     Matlab  R2012a or above      Usage   To use this code simply call  extract file  data folder path    dataset name    store folder name    to extract the desired dataset    Parameters default values     data folder path               dataset name usa      store folder name  extracted data   Current available datasets for extraction    Caltech Pedestrian   INRIA Pedestrian   ETH Pedestrian   TUD Brussels Pedestrian   Daimler Pedestrian   Disclamer   This code has been adapted from pdollar s  toolbox  and  evaluation code   Label ignore filter for torch nn   Ignore bypass some inputs according to a specific set of ignore labels   This package allows for generally available  out of the box criterions in  torch nn  to be used in situations where a specific label of a certain class output  is needed to be overlooked ignored when backproping through a network model    Install   To install the package simply do the following   git clone https   github com farrajota criterion filter cd criterion filter    luarocks make   Usage   This package consists of two submodules       criterion filter Single Criterion  Ignore Label   wraps an input criterion and filters all indexes which have the same label as in  Ignore Label      criterion filter Parallel    is a direct refactoring of the  nn ParallelCriterion    container  where   add Criterion  Weight  Ignore label   allows you to specify a set of ignore labels for each criterion that you add      The following code snippets shows how to use the package for cases using single and multiple criterias    Single criterion   Example 1    lua      Example using ClassNLLCriterion    Computes the loss for an input tensor with all allowed targets  and proceed to compute the loss with a blacklisted label      require  criterion filter       1  define model model   nn Sequential   model add nn Linear 10 4        2  define criterion nll   nn ClassNLLCriterion   criterion   criterion filter Single nll  4     set to ignore class 4      3  define input data  input   torch Tensor 5 10  uniform   target   torch Tensor 5  random 1 3       4  compute loss  forward pass only     4 1   no labels to be ignored at this point  output   model forward input  print  Output Tensor    print output  err1   criterion forward output target  print  Error with no ignored labels       err1       4 2  set one target label to 0  to be ignored  target 5    4 err2   criterion forward output target  print  Error after setting one target label to 0      err2  print  Error equal       tostring err1    err2    Example 2 lua      Example using  MSECriterion   Computes the loss and gradient for an input tensor   targets  and proceeds to compute the loss and gradient with blacklisted labels     require  criterion filter       1  define model model   nn Sequential   model add nn Linear 10 4        2  define criterion mse   nn MSECriterion   criterion   criterion filter Single mse   torch Tensor 4  fill 0     1 2 3 4        set to ignore class two labels      3  define input data  input   torch Tensor 5 10  uniform   target   torch Tensor 5 4  uniform        4  compute loss gradient  forward backward pass     4 1   no labels to be ignored at this point  output   model forward input  err1   criterion forward output target  gradOutput1   criterion backward output target  clone   print err1  print gradOutput1       4 2  set one target to 0 s  to be ignored  target 1  fill 0  err2   criterion forward output target  gradOutput2   criterion backward output target  clone   print err2  print gradOutput2      4 3  set another target to 0 s and one to  1 2 3 4   target 3  fill 0  target 4  copy torch Tensor 1 2 3 4   err3   criterion forward output target  gradOutput3   criterion backward output target  clone   print err3  print gradOutput3        Multiple criterions   Example 1    lua      Example using multiple criterions   ClassNLLCriterion   ClassNLLCriterion    Here  the loss is computed using two equal criterias with equal weigths   Check how the loss value differs when it is computed with ignored filtered blacklisted labels     require  criterion filter       1  define model model   nn Sequential   prl   nn ConcatTable   prl add nn Linear 10 10   prl add nn Linear 10 10   model add prl       2  define criterions criterion   criterion filter Parallel   criterion add nn ClassNLLCriterion    1  6     set different ignore labels criterion add nn ClassNLLCriterion    1  7     set different ignore labels      3  define input data input   torch Tensor 5 10  uniform   target1   torch Tensor 5  random 1 10  target1 1  1 target2   torch Tensor 5  random 1 10  target2 1  1      4  compute loss    4 1  without ignore labels output   model forward input  print output 1   print output 2   print target1  print target2  err1   criterion forward output  target1  target2   print err1       4 2  now with ignored blacklisted labels target1 1    6 target2 1    7  err2   criterion forward output  target1  target2   print err2       4 3  flip labels  target1 1    7 target2 1    6  err3   criterion forward output  target1  target2   print err3        Example 2     lua      Example using multiple criterions   ClassNLLCriterion   MSECriterion    Here  the loss is computed using two different criterias and weigths   Check how the loss value differs when it is computed with ignored filtered blacklisted labels     require  criterion filter       1  define model model   nn Sequential   prl   nn ConcatTable   prl add nn Linear 10 5   prl add nn Linear 10 4   model add prl       2  define criterions criterion   criterion filter Parallel   criterion add nn ClassNLLCriterion    1      no ignore label defined criterion add nn MSECriterion    0 5  torch Tensor  1 1 1 1       set an ignore label      3  define input data input   torch Tensor 5 10  target1   torch Tensor 5  random 1 5  target2   torch Tensor 5 4  uniform        4  compute loss    4 1  without ignore labels output   model forward input  err1   criterion forward output   target1  target2   print  Error without ignore labels       err1       4 2  now with ignored blacklisted labels target2 2  fill 1  target2 5  fill 1  err2   criterion forward output   target1  target2   print  Error with ignore labels       err2      Train an object classifier on  ImageNet  using multiple gpus in Torch7   This repo shows how to train a object classifier over ImageNet Cifar10 Cifar100 MNIST using a multi threaded  multi gpu approach    Features     Several types of networks like AlexNet  Overfeat  VGG  Googlenet  etc  are available for training    Multi GPU support    Data loading processing using multiple threads    Easily apply data augmentation    Integration with the  dbcollection  package      Requirements     NVIDIA GPU with compute capability 3 5   2GB  ram    torch7   torchnet   dbcollection     Running the code   The main script comes with several options which can be listed by running the script with the flag   help  bash th main lua   help   To train a network using the default settings  simply do   bash th main lua     Note  You must have the ImageNet ILSVRC2012 dataset  or any other dataset  setup before running this script  For more information about how to setup your datasets using  dbcollection  see  here       By default  the script trains theAlexNet model on 1 GPU with the CUDNN backend and loads data from disk using 4 CPU threads    To run an alexnet model using two or more GPUs  set  nGPU  to the number of GPUs you want to use  in this example only two are used    bash th main lua  nGPU 2  netType alexnet   In case you want to specify which gpus do use  do the following   bash CUDA VISIBLE DEVICES 0 1 th main lua  nGPU 2  netType alexnet     Note  this will select the first two GPUs detected in your system      To use more threads for data loading processing  use the  nThreads  flag to specify the number of threads you want to use    bash th main lua  nThreads 2   For a complete list of available options  please see the  opts lua  file or run  th main lua   help  in the command line    Data loading benchmark comparison   For most datasets  loading the necessary metadata  filenames  labels  etc   from disk should carry a very small  almost  insignificant overhead compared to loading metadata from memory    To showcase this  some scripts under  benchmark   for the ImageNet ILSVRC2012 and Cifar10 datasets are available for benchmarking this  Here it is used the average time for 1000 data fetches with  batchsize 128  and  nThreads 4     The  train  scores use more data augmentation preprocessing compared to the  test  scores which uses less data augmentation techniques    Dataset   train   test                 Cifar10   disk     0 01509s   0 00953s Cifar10   ram      0 00772s      0 00557s  ILSVRC2012   disk     0 34635    0 35729  ILSVRC2012   ram      0 34553    0 36107     Note  This tests were done using a 6 core Intel R  Core TM  i7 4790K CPU   4 00GHz  32GB ram  2TB SSHD and Ubuntu 14 04  Note that the overhead is very small when using datasets with bigger images like the Imagenet  meaning that the overhead can be hidden by using enough cores or a faster disk      Code Description       main lua    250 lines    Script using torchnet s api for training and testing a network over ImageNet        utils lua    125 lines    Multi gpu functions for loading storing setting a model        transforms lua    500 lines    Data augmentation functions  mostly derived from  here  and  here         configs lua    200 lines    Setup configurations  options  model  logger  etc         statistics lua    100 lines    Computes the dataset s mean std statistics for 10000 samples and stores it to    cache  dir        model lua    40 lines    Creates Loads a model from training testing        data lua    110 lines    Contains the methods to featch load data of the available datasets        License   MIT license  see the LICENSE file    Disclamer   This code has been inpired on torchnet s  mnist training example   soumith s  multi gpu ImageNet training code  and  karandwivedi42  multigpu torchnet   Caltech Pedestrian Dataset Extractor   Extract images and annotation files from the Caltech Pedestrian Dataset using python    This script extracts and converts data from   seq  and   vbb  files  to   jpg  images and   json  files  Individual images   annotation files are extracted and stored per sequence and per video for easier access    Requirements     python  2 7 or python  3 3   numpy   scipy   json     Usage   In order to start using this script you must first untar all files before running the script    Then  to extract files just run the following comand in a terminal    bash python converter py  data path  path to data    This will create a dir named  extracted data   and store all extracted data in there  If you wish to specify a different directory to store your data you can set the   save path  input argument to a different path    bash python converter py  data path  path to data   save path  path to extract      Note  A valid   data path  is required and must be provided in order for the script to work      License   Licensed under the  MIT  license    Acknowledgements   This code is based on  hizhangp  caltech converter  code  Articulated Human pose evaluation benchmark   Benchmark your method against several other methods on the popular  FLIC  and  LSP  datasets    How to use   To benchmark the algorithms simply run the scripts files  benchmark flic m  and  benchmark lsp m  to evaluate the algorithms on the FLIC and LSP datasets  respectively    Adding a new algorithm to the list   Adding predictions of a new algorithm is fairly simple      Create a folder with the name of the algorithm in  algorithms      Create a file called  algorithm txt  and assign a label alias name for the algorithm to be used to identify the algorithms name in the plot s legend    Add the predictions files with the keypoints sticks coordinates with the names  pred keypoints lsp oc mat    pred keypoints lsp pc mat    pred sticks lsp oc mat  and  pred sticks lsp pc mat         Note  The scripts will skip the missing files when benchmarking a method      Options   Several options are available for configuration  These  however  require the user to change the file manually    Plot options      list   specify which algorithms to plot  If empty  plots all algorithms    bSave   save plot images to  plots   folder  if set to  true      printLegend   prints a legend in every plot  if set to  true      pcp threshold   PCP evaluation threshold    pck threshold   PCK evaluation threshold      Available datasets   For now  the available datasets for PCK and PCP evaluation are the  FLIC  and  LSP   Other datasets may be introduced if it is justifiable for inclusion     inproceedings modec13      title  MODEC  Multimodal Decomposable Models for Human Pose Estimation       author  Sapp  Benjamin and Taskar  Ben       booktitle  In Proc  CVPR       year  2013        inproceedings Johnson11      title    Learning Effective Human Pose Estimation from Inaccurate Annotation       author    Johnson  Sam and Everingham  Mark       year    2011       booktitle    IEEE Proc  CVPR      Benchmark results   All available methods for the LSP benchmark were downloaded from  MPII s website      FLIC methods were gather from some authors s predictions available online    Results of the algorithms are shown bellow    Frames Labeled In Cinema  FLIC    PCK 0 2    Observer Centric     Method   Elbow   Wrist                          Sapp et al   CVPR 13    72 5   54 5     Yang et al   CVPR 16    91 6   88 8     Chen et al   NIPS 14    89 8   86 8     Wei et al   CVPR 16    92 5   90 0     Newell et al   arXiv 16    98 0   95 5                                                               Leeds Sport Pose  LSP    PCP 0 5    Person Centric     Method   Torso   Upper leg   Lower leg   Upper arm   Forearm   Head   PCP                                                       Wang et al   CVPR 13    87 5   56 0   55 8   43 1   32 1   79 1   54 1     Pishchulin et al   ICCV  13    88 7   63 6   58 4   46 0   35 2   85 1   58 0     Tompson et al   NIPS 14    90 3   70 4   61 1   63 0   51 2   83 7   66 6     Fan et al   CVPR 15    95 4   77 7   69 8   62 8   49 1   86 6   70 1     Chen et al   NIPS 14    96 0   77 2   72 2   69 7   58 1   85 6   73 6     Yang et al   CVPR 16    95 6   78 5   71 8   72 2   61 8   83 9   74 8     Rafi et al   BMVC 16    97 6   87 3   80 2   76 8   66 2   93 3   81 2     Belagiannis et al   arXiv 16    96 0   86 7   82 2   79 4   69 4   89 4   82 1     Lifshitz et al   ECCV 16    97 3   88 8   84 4   80 6   71 4   94 8   84 3     Pishchulin et al   CVPR 16    97 0   88 8   82 0   82 4   71 8   95 8   84 3     Yu et al   ECCV 16    98 0   93 1   88 1   82 9   72 6   83 0   85 4     Insafutdinov et al   ECCV 16    97 0   90 6   86 9   86 1   79 5   95 4   87 8     Wei et al   CVPR 16    98 0   92 2   89 1   85 8   77 9   95 0   88 3     Bulat et al   ECCV 16    97 7   92 4   89 3   86 7   79 7   95 2   88 9                                                                       PCK 0 2    Person Centric     Method   Head   Shoulder   Elbow   Wrist   Hip   Knee    Ankle   Total                                                              Wang et al   CVPR 13    84 7   57 1    43 7    36 7    56 7    52 4   50 8   54 6     Pishchulin et al   ICCV  13    87 2   56 7    46 7    38 0    61 0    57 5   52 7   57 1     Tompson et al   NIPS 14    90 6   79 2    67 9    63 4    69 5    71 0   64 2   72 3     Fan et al   CVPR 15    92 4   75 2    65 3    64 0    75 7    68 3   70 4   73 0     Chen et al   NIPS 14    91 8   78 2    71 8    65 5    73 3    70 2   63 4   73 4     Yang et al   CVPR 16    90 6   78 1    73 8    68 8    74 8    69 9   58 9   73 6     Rafi et al   BMVC 16    95 8   86 2    79 3    75 0    86 6    83 8   79 8   83 8     Yu et al   ECCV 16    87 2   88 2    82 4    76 3    91 4    85 8   78 7   84 3     Belagiannis et al   arXiv 16    95 2   89 0    81 5    77 0    83 7    87 0   82 8   85 2     Lifshitz et al   ECCV 16    96 8   89 0    82 7    79 1    90 9    86 0   82 5   86 7     Pishchulin et al   CVPR 16    97 0   91 0    83 8    78 1    91 0    86 7   82 0   87 1     Insafutdinov et al   ECCV 16    97 4   92 7    87 5    84 4    91 5    89 9   87 2   90 1     Wei et al   CVPR 16    97 8   92 5    87 0    83 9    91 5    90 8   89 9   90 5     Bulat et al   ECCV 16    97 2   92 1    88 1    85 2    92 2    91 4   88 7   90 7                                                                            Acknowledgements   This code is a modified version of the original code made available by  MPII     License   The available code is released under the MIT license  Fast RCNN package for torch7   Fast RCNN  implementation for Torch7 as a package with methods for training and testing an object detector network    Features     Simple API for training  testing  detecting and visualizing objects in images    Multi threaded data loading preprocessing    Multi GPU support    Common data augmentation techniques  color jitter  scaling  etc      Pascal VOC   MS COCO mAP evaluation schemes    Proposals data augmentation during train     Installation   Requirements     NVIDIA GPU with compute capability 3 5   2GB  ram    Torch7   tds   matio   cudnn   inn   torchnet     Package installation   To install this package you need to have  Torch7  installed on your machine and some other packages  To install this packages  simply do    bash luarocks install tds luarocks install cudnn luarocks install inn luarocks install matio luarocks install torchnet   Finally  to install this package do the following    bash git clone https   github com farrajota fast rcnn torch cd fast rcnn torch    luarocks make rocks     Usage   To call this package simply do    lua local fastrcnn   require  fastrcnn     This loads a table with the necessary methods for creating  training and testing a Fast R CNN network  Also  it contains a method for detecting objects in images and for visualizing the detections with a window frame  requires  qt  to work     train   lua fastrcnn train dataLoadTable  rois  model  modelParameters  opts    Trains a model on a given dataset with some proposals    Parameters     dataLoadTable   table with methods for loading data    type table     rois   Region of Interest bounding box proposals    type table     model   a Fast R CNN style network    type table     modelParameters   model parameters  color space  meanstd  pixel scale and stride     type table     opts   training options    type table       test   lua fastrcnn test dataLoadTable  rois  model  modelParameters  opt    Test a model on a dataset  mAP score     Parameters     dataLoadTable   Table with methods for loading data    type table     rois   Region of Interest bounding box proposals    type table     model   A Fast R CNN network    type table     modelParameters   The model s parameters  color space  meanstd  pixel scale and stride     type table     opts   Testing options    type table       detector   lua imdetector   fastrcnn Detector model  modelParameters  opt    Object detector class  This provides a simple interface to image inference    Parameters     model   A Fast R CNN network    type table     modelParameters   The model s parameters  color space  meanstd  pixel scale and stride     type table     opts   Testing options    type table       Object detector class    lua scores  bboxes   imdetector detect im  proposals    Receives an image and region proposals as input and outputs scores and bounding boxes    Parameters     im   Image tensor    type torch Tensor     proposals   Region of Interest bounding box proposals   type torch Tensor       utils   This package contains several utility methods for creating models  loading roi proposals from file or visualizing object detection with a window frame        model  methods for storing  loading and setup a Fast R CNN model        nms  non maximum suppression methods        load  load roi proposals from file  ony Matlab files atm         visualize detections  visualize detections with a window frame          Note  visualize detections requires  qt  to work  For that  you need to use the  qlua  interpreter      Demos   This  repo  contains code examples on how to train test an object detector using this module for the Pascal VOC 2007  2012 and MS COCO datasets    Another  repo  contains code examples on how to train test an object detector for pedestrian detection on the Caltech Pedestrian dataset    License   MIT license  see the LICENSE file    Acknowledgements   This package was heavily inspired by the following repositories   Fast RCNN    Fast RCNN for Torch7  and  facebook multipathnet   Fast R CNN example code   This example code showcases the use of the Fast R CNN package for training and testing a network for object detection    Installation   Requirements     NVIDIA GPU with compute capability 3 5   2GB  ram    Torch7   Fast RCNN package   dbcollection     Packages dependencies installation   To use this example code  some packages are required for it to work   fastrcnn  and  dbcollection     fastrcnn   To install the Fast R CNN package do the following      install all the necessary dependencies      bash luarocks install tds luarocks install cudnn luarocks install inn luarocks install matio luarocks install torchnet     download and install the package      bash git clone https   github com farrajota fast rcnn torch cd fast rcnn torch    luarocks make rocks       For more information about the  fastrcnn  package see  here       dbcollection   To install the dbcollection package do the following        install the Python module    pip install dbcollection  0 1 7   or   conda install  c farrajota dbcollection  0 1 7       install the Lua Torch7 dbcollection wrapper      download the Lua Torch7 git repo to disk      git clone https   github com dbcollection dbcollection torch7     install the package   cd dbcollection torch7    luarocks make           For more information about the dbcollection package see  here       Usage   To start using the code  clone this repo to your home directory    git clone https   github com farrajota fastrcnn example torch   If you clone the repo into a different directory  please make sure you modify  projectdir lua  and point to the new path before using the code    Data setup   The necessary data is available for download by calling the following command in the terminal    bash th download download all lua   This will download the following data      pre trained models on Imagenet   pre processed roi proposals on the caltech pedestrian dataset   annotations in the COCO format for evaluating the network accuracy using the  coco  evaluation protocol        Note  this data can also be downloaded manually by following the next steps      Pre trained models   Several pre trained models are available to be used in this example code  To download these networks  simply run the following command in the terminal  assuming you are in the root dir of the repo     bash th download download pretrained models lua   This will download the following network types pretrained on the ImageNet ILSVRC2012 dataset  alexnet  zeilernet  googlenet  vgg and resnet   RoI Proposals   To download the roi proposals  simply run the following command in the terminal  again  assuming you are in the root dir of the repo     bash th download download roi proposals lua   Datasets   To run the example code  the user can let the script handle the data download and setup    However  if the user already has downloaded the data to disk  it is advisable to manually setup the dataset before runing the script to avoid downloading the data to disk    To do this you can do the following    lua dbc   require  dbcollection manager  dbc add name  pascal voc 2007   data dir  path to dataset   task     file path       Even if you don t have the dataset  it is best to manually setup the dataset because this way you can specify the path where data will be stored on disk  Like the previous example  to setup a dataset s data you simply need to do the following    lua dbc   require  dbcollection manager  dbc load name  pascal voc 2007   data dir  save dir path       Note  If no path is provided  the dataset will be stored in the    dbcollection  dataset name  data   directory in your home path      Available datasets   The following datasets are available for training testing an object detector using this repo      Pascal VOC 2007   Pascal VOC 2012   Microsoft COCO       Note  For a list of all available datasets fro train test check  options lua       Train and test a model using the example code   This repository contains scripts for training and testing an object detector network using a pre trained network on ImageNet for feature extraction such as the alexnet or resnet      Note  several options are available for configuring the training testing parameters  see  options lua  for a complete set of available parameters       Training a network   To train a model run  th train lua   To change the default settings  use the input arguments of your choice  To see all available option s parameters do  th train lua   help  or check  options lua         You can select one of the following imagenet pre trained networks for feature extraction  AlexNet  ZeilerNet  VGG  16  19   ResNet  19  34  50  101  152  200   and GoogleNet v3        Snapshots of the network s training procedure are stored to disk other information such as the configuration file  loss logs and model parameters of the training procedure  default path is    data exp    You can change this directory by passing the   expDir new path to save the experiment option        Testing a network  mAP accuracy    To test a network s accuracy  run  th test lua  and define the   expID     dataset  and   expDir  input options  if changed  to compute the mean average precision      Note  Network evaluation  test  only uses a single GPU for inference      Scripts   In the  scripts   folder there are several pre configured example scripts for training and testing a network  To run a script just call them  like this    th scripts train test alexnet voc2007 lua   Running the demo   To run the basic demo code you ll first need to train a network model  After this is done  just simply run the demo on the terminal    lua qlua demo lua  expID  exp name   dataset  dataset name    After running the demo you should see something like this        Note  This image was taken from this  repo   In the demo script a random image is selected from the test set and displayed  To change the generated image just modify the   manualSeed  value      License   MIT license  see the LICENSE file  Pedestrian detector for Lua Torch7   Train test benchmark a pedestrian detector using lua torch7  These detector uses a modified  Fast R CNN  network   a pedestrian oriented roi proposal generator for detection      The available networks for feature extraction for use are the following      AlexNet   ZeilerNet   VGG16 19   Googlenet  inception v3    ResNet  18  32  50  101  152  200        Note  Some of these networks require GPUs with 10Gb  ram to be trained      For Region of Interest  ROI  proposal generation  this code has three methods for roi proposal generation      ACF   paper    code     LDCF   paper    code     EdgeBoxes   paper    code       Installation   Requirements   To run the code in this repository you ll need the following resources      Torch7   Fast R CNN module   dbcollection   Python    2 7 or   3 5    Matlab    2012a  for benchmark and roi proposal generation      Packages dependencies installation   To use this example code  some packages are required for it to work   fastrcnn  and  dbcollection     fastrcnn   To install the Fast R CNN package do the following      install all the necessary dependencies      bash luarocks install tds luarocks install cudnn luarocks install inn luarocks install matio luarocks install torchnet     download and install the package      bash git clone https   github com farrajota fast rcnn torch cd fast rcnn torch    luarocks make rocks       For more information about the fastrcnn package see  here       dbcollection   To install the dbcollection package do the following        install the Python module  Python  2 7 and   3 5     pip install dbcollection  0 1 7   or   conda install  c farrajota dbcollection  0 1 7       install the Lua Torch7 dbcollection wrapper      download the Lua Torch7 git repo to disk      git clone https   github com dbcollection dbcollection torch7     install the package   cd dbcollection torch7    luarocks make           For more information about the dbcollection package see  here       Getting started   Usage   To start using the code  clone this repo to your home directory    git clone   recursive https   github com farrajota pedestrian detector torch   If you clone the repo into a different directory  please make sure you modify  projectdir lua  and point to the new path before using the code    Data setup   The necessary data is available for download by calling the following command in the terminal    bash th download download all lua   This will download the following data      pre trained models on Imagenet   pre processed roi proposals on the caltech pedestrian dataset   algorithm scores of several methods for benchmark       Note  this data can also be downloaded manually by following the next steps      Download pre trained models and benchmark algorithm results   Several pre trained models are available to be used in this code  To download all networks  simply run the following command in the terminal    bash th download download pretrained models lua   For benchmark  other evaluation algorithms need to be downloaded before proceeding to evaluate the trained network  To download all available algorithms simply do the following    bash th download download extract algorithms lua   RoI Proposals   To generate region proposals  run  th generate rois lua  on the terminal  This will automatically generate the RoI proposals needed for training and testing the pedestrian detector      Warning  processing the region proposals takes around a week to process  For this reason  pre processed proposals are available for download      bash th download download proposals lua   Dataset   To run the code  first it is necessary to setup the dataset s data  This code uses the  dbcollection  package for data setup management    To setup a dataset do the following    lua dbc   require  dbcollection  caltech   dbc load name  caltech pedestrian                      data dir  save files to dir     This will download and pre process the dataset s data and store the all files to the selected path  If  data dir  is not defined or left empty  the data files will be stored in the  dbcollection   folder in your home directory in a folder with the dataset s name    In case you already have the necessary data files  you can manually set the directory path of the files by doing the following commands    lua dbc   require  dbcollection  caltech   dbc load name  caltech pedestrian                      data dir  path to dataset files       Note  All datasets used here can be downloaded via  dbcollection       To download and extract the relevant data  please run the following scripts   th download extract dataset lua  save dir  store dir path   and  download extract algorithms lua  save dir  store dir path    The   save dir  store dir path   allows for the user to save the downloaded data into another directory than the root dir of the code    Available datasets   The following datasets are available for training testing a pedestrian detector using this repo      Caltech Pedestrian Dataset     Train and test a model using the example code   This repository contains scripts for training and testing a pedestrian detector network using a pre trained network on ImageNet for feature extraction such as the alexnet or vgg      Note  several options are available for configuring the training testing parameters  see  options lua  for a complete set of available parameters       Training a network   To train a model run  th train lua   To change the default settings  use the input arguments of your choice  To see all available option s parameters do  th train lua   help  or check  options lua         You can select one of the following imagenet pre trained networks for feature extraction  AlexNet  ZeilerNet  VGG  16  19   ResNet  19  34  50  101  152  200   and GoogleNet v3        Snapshots of the network s training procedure are stored to disk other information such as the configuration file  loss logs and model parameters of the training procedure  default path is    data exp    You can change this directory by passing the   expDir new path to save the experiment option        Testing a network  mAP accuracy    To test a network s accuracy  run  th test lua  and define the   expID     dataset  and   expDir  input options  if changed  to compute the mean average precision      Note  The test script only uses a single GPU      Demo  detecting persons   To run the basic demo code you ll first need to train a network model  After this is done  just simply run the demo on the terminal    lua qlua demo lua  expID  exp name    This selects random images from the testing set and it should give you a glimpse of how the net performs    Benchmark evaluation   This repo contains scripts for benchmarking the results of your network against other top performing methods on the Caltech dataset  This requires that evaluation results of other algorithms have been downloaded in order to run    After this is setup  simply run  th benchmark lua  expID   exp name   and this will generate a series of plots which are stored in the experiment s folder in    data      The default name for the method is  OURS   To change to any other name just set a different name using   eval plot name  input option    License   MIT license  see the  LICENSE  file    Acknowledgements   The evaluation benchmarking code is an adaptation of  pdollar  toolbox   evaluation  code  Human Body Joints estimation for torch7   Train and test a human body joints estimator network using Lua Torch7 for single humans on a single image  This method is a modified version of the original  hourglass networks   For more information see our  paper     This code provides an easy way to train a network on a variety of datasets  all available through the  dbcollection  package  The available datasets for train test benchmark are the following      Dataset   Train   Test   Benchmark                                Frames Labeled In Cinema  FLIC     Yes   Yes   Yes      Leeds Sports Pose  LSP     Yes   Yes   Yes      MPII    Yes   Yes    No       COCO    Yes   Yes    No           Note    Only the FLIC and LSP datasets are evaluated benchmarked here  The MPII and COCO datasets have dedicated servers for this purpose on their websites      Network architecture   The network model used here for human body joint estimation is an enhanced version of  Newell s  method described in  his paper  with several modifications      replaced ReLUs with  RReLUs   use of  spatialdropout  between convolutions   more data augmentation  more rotation  scaling  colour jittering    use of wider feature maps  more kernels  as the image resolution decreases   replaced rmsprop optimization with  adam   additional tweaks to the basic auto encoder network     Results   FLIC dataset results   PCK 0 2    Observer Centric     Method   Elbow   Wrist   Total                               Sapp et al   CVPR 13    72 5   54 5   63 5     Chen et al   NIPS 14    89 8   86 8   88 3     Yang et al   CVPR 16    91 6   88 8   90 2     Wei et al   CVPR 16    92 5   90 0   91 3     Newell et al   arXiv 16    98 0   95 5   96 8      Ours      98 3     96 0     97 2      LSP dataset   PCK 0 2    Person Centric     Method   Head   Shoulder   Elbow   Wrist   Hip   Knee    Ankle   Total                                                              Wang et al   CVPR 13    84 7   57 1    43 7    36 7    56 7    52 4   50 8   54 6     Pishchulin et al   ICCV  13    87 2   56 7    46 7    38 0    61 0    57 5   52 7   57 1     Tompson et al   NIPS 14    90 6   79 2    67 9    63 4    69 5    71 0   64 2   72 3     Fan et al   CVPR 15    92 4   75 2    65 3    64 0    75 7    68 3   70 4   73 0     Chen et al   NIPS 14    91 8   78 2    71 8    65 5    73 3    70 2   63 4   73 4     Yang et al   CVPR 16    90 6   78 1    73 8    68 8    74 8    69 9   58 9   73 6     Rafi et al   BMVC 16    95 8   86 2    79 3    75 0    86 6    83 8   79 8   83 8     Yu et al   ECCV 16    87 2   88 2    82 4    76 3    91 4    85 8   78 7   84 3     Belagiannis et al   arXiv 16    95 2   89 0    81 5    77 0    83 7    87 0   82 8   85 2     Lifshitz et al   ECCV 16    96 8   89 0    82 7    79 1    90 9    86 0   82 5   86 7     Pishchulin et al   CVPR 16    97 0   91 0    83 8    78 1    91 0    86 7   82 0   87 1     Insafutdinov et al   ECCV 16    97 4   92 7    87 5    84 4    91 5    89 9   87 2   90 1     Wei et al   CVPR 16     97 8    92 5    87 0    83 9    91 5    90 8   89 9   90 5     Bulat et al   ECCV 16    97 2   92 1    88 1    85 2     92 2     91 4   88 7   90 7      Ours     97 7    93 0      88 9      85 5     91 5     92 0     92 1     91 5        Note  The network was trained with data from the MPII and LSPe datasets just like most of the methods on the benchmark      Installation   Requirements   To run the code in this repository you ll need the following resources      Torch7   Matlab    2012a  for running the benchmark code    Python    2 7 or    3 5  for  dbcollection     NVIDIA GPU with compute capability 3 5   12GB  ram or two 6GB  ram GPUs        Note  Here we used two 6GB GPUs to train the network  We recommend  at least  one GPU with 6GB ram for inference and one 12GB ram GPU for training the model      Also  you ll need to install the following packages    lua luarocks install display luarocks install cudnn luarocks install inn luarocks install matio luarocks install tds luarocks install torchnet   dbcollection   To install the dbcollection package do the following        install the Python module  Python  2 7 and   3 5     pip install dbcollection  0 1 7   or   conda install  c farrajota dbcollection  0 1 7       install the Lua Torch7 dbcollection wrapper      download the Lua Torch7 git repo to disk      git clone https   github com dbcollection dbcollection torch7     install the package   cd dbcollection torch7    luarocks make           For more information about the dbcollection package see  here       Data setup   dbcollection  contains the data annotations necessary to run this code  If the data is not setup  this package will download  extract and preprocess the data annotations and store them to disk  To specify where to store the downloaded extracted data use the   data dir  path    If left empty  and the data will be stored in    dbcollection  dataset  data      For more information on how to setup a dataset see the  dbcollection  repo    Getting started   After installing the necessary requirements  it is advised to setup the necessary data before proceeding  Since the code uses  dbcollection  for managing datasets  downloading setting up the data folders first is best if you desire to specify the dataset s directory manually  Then  to start training a network simply do  th train lua  expID  net name    To use a specific dataset  for example FLIC  specify the   dataset  input arg when running the script  th train lua  expID  net name   dataset flic     Most of the command line options are pretty self explanatory  and can be found in  options lua   The   expID  option will be used to save important information in a directory like  pose torchnet exp  dataset   expID     This directory will include snapshots of the trained model  training validations logs with loss and accuracy information  and other details of the options set for that particular experiment    Download Setting up this repo   To use this code  clone the repo into your home directory    git clone   recursive https   github com farrajota human pose estimation torch   If you clone the repo into a different directory  please make sure you modify  projectdir lua  and point to the new path before using the code    Training a model   To train a network you simply need to do  th train lua   This will train a network with the default parameters  To train a network with other options please see the  options lua  file or look in  tests   for some scripts that contain training procedures    Testing a model   When training a network  a small sample of the overall dataset is used to compute the current accuracy  To use the entire dataset to compute the total accuracy of a network  run  th test lua  expID  name exp   dataset  name exp    For the MPII dataset  the training set is split into two  train   val  and the evaluation is performed on the  val  set    Benchmarking against other methods   To benchmark a trained model on the FLIC or LSP datasets run  th benchmark lua   expID  name exp   dataset  name exp    This will process predictions of all body joints for all images of the selected dataset and store them to disk into two files   Predictions t7     Predictions mat   inside the folder of the experiment    Furthermore  for the FLIC and LSP datasets  this repo provides a comparison with other methods for body joint prediction whose predictions are made available online   lsp results are available here   for FLIC results I had to manually search some of them on github   To evaluate on the MPII and COCO datasets you will need to use their evaluation servers if you want to compare the results with other methods    Additional information   Accuracy metric   For convenience during training  the accuracy function evaluates the PCK metric by comparing the output heatmap of the network to the ground truth heatmap  This should provide a good enough performance measure when training a network on any dataset    Additional notes   Due to problems in cleaning temporary buffers  grad output buffers  with   clearState     in order to store models to disk  this option is hidden behind an input flag  For our setup  using   clearState    with the GPUs memory almost maxed out  this would cause crashes due to insufficient memory when re populating the buffers  In case of training networks that are smaller or have less parameters this should not be an issue  or if you have a GPU with 12GB  ram     Cite this paper    Inbook Farrajota2017      author  Farrajota  M      and Rodrigues  Jo   a o M  F      and du Buf  J  M  H        editor  Alexandre  Lu   i s A      and Salvador S   a nchez  Jos   e      and Rodrigues  Jo   a o M  F        title  Human Pose Estimation by a Series of Residual Auto Encoders       bookTitle  Pattern Recognition and Image Analysis  8th Iberian Conference  IbPRIA 2017   Faro  Portugal  June 20 23  2017  Proceedings       year  2017       publisher  Springer International Publishing       address  Cham       pages  131  139       isbn  978 3 319 58838 4       doi  10 1007 978 3 319 58838 4 15       url  https   doi org 10 1007 978 3 319 58838 4 15      License   MIT license  see the  LICENSE  file    Acknowledgements   This repo is based on  Newell s  code for training a human pose detector  Human Activity Recognition using Torch7     Recognize human activities of individuals using human body joints on video sequences    This method combines features from a ResNet 50 with human body joint prediction to improve performance in human action recognition applications  It uses a modifed  Stacked Hourglass Network  trained on the LSP MPII datasets to predict human body joints for a sequence of images      The proposed network takes as input a sequence of images with centered persons which are then processed by two parallel networks that produce image features and 2D heatmaps of human body joint which are then fed to a couple of LSTM layers to compute the classification score for a video sequence        Note  This was only tested on the UCF Sports dataset because it had bounding box annotations of humans available      Installation   Requirements     NVIDIA GPU with compute capability 3 5   2GB  ram    Torch7   dbcollection     Packages dependencies installation   To use this example code  some packages are required for it to work    bash luarocks install loadcaffe luarocks install cudnn luarocks install display   dbcollection   To install the dbcollection package do the following        install the Python module  Python  2 7 and   3 5     pip install dbcollection  0 1 7   or   conda install  c farrajota dbcollection  0 1 7       install the Lua Torch7 dbcollection wrapper      download the Lua Torch7 git repo to disk      git clone https   github com dbcollection dbcollection torch7     install the package   cd dbcollection torch7    luarocks make           For more information about the dbcollection package see  here       Getting started   Download setup this repo   To start using this repo you ll need to clone this repo into your home directory    git clone https   github com farrajota human activity torch   By default  this repo path points to    human activity torch   when running the scripts  If you want to use this repo with another path directory  you ll need to edit the  projectdir lua  file in the repo s root dir and set the proper path to where it was cloned    Next  the necessary data for this code to run is needed to be set  Download the pre trained models by ruining the script  download data lua  script in the  download   dir    th download download data lua   The pre trained models uses the  cudnn  library  so make sure this package is installed on your system before proceeding any further    Train   To train a network  simply run the  train lua  script to start optimizing the proposed network  ResNet50   Pose Hms   LSTM  on some default options  The default configurations are detailed in the the paper  TODO  insert paper link     To train a network  there are several input arguments to configure the training process  The most important ones are the following  the rest you can leave as defaults        expID  exp name    experiment id to store all the metadata  logs and model snapshots to disk under the  exp   dir in the repo s main folder     dataset  dataset name    indicates which dataset to train on  default  ucf sports      warning  for now  the ucf sports is the only available dataset to train test on     data dir  path to dataset files    Path to store the dataset  s data files in case you haven t configured the  ucf sports  previously in  dbcollection   Specify a path if you want to store the data files into a specific folder     expDir  path to folder    specifies which folder to store the experiment directory  By default  it uses the  exp   dir in the repo s main directory   optional     netType  net type name    specifies which network to train  Options  vgg16 lstm   hms lstm   vgg16 hms lstm   vgg16 convnet   hms convnet   vgg16 hms convnet     trainIters  num iters    Number of train iterations per epoch  default 300      testIters  num iters    Number of test iterations per epoch  default 100      nEpochs  num epochs    Total number of epochs to runh  default 15      batchSize  size    Mini batch size  default 4      seq length  size    Sequence length  number of frames per window   default 10       For more information about the available options  see the  options lua  file or type  th train lua  help     Note  In the  scripts   dir there are several scripts to train   test other networks with different architectures and configurations  Check them out to see more training configurations specs      Test   Evaluating a network is done by running the  test lua  script with some input arguments  To do so  you need to provide the following input arguments       expID  exp name    experiment id that contains the model s snapshots   logs     loadModel  path to network t7    if this flag is used  it bypasses the   expID  flag and loads a specific file     dataset  dataset name    indicates which dataset to test on  default  ucf sports       test progressbar false   displays text information per iteration instead of a progress bar  optional      test load best false   if true  it loads the best accuracy model  if exists  optional       The results of the test  when finished  are displayed on screen and stored in the folder of the network file with the name  Evaluation full log     Results     Method   Accuracy                       Rodriguez et al   1    69 2     Lan et al   2    83 7      Jones et al   3      93 5              PoseNet   LSTM   59 6     ResNet   LSTM   80 9      Resnet   PoseNet   LSTM     87 2       1    Rodriguez  M D   Ahmed  J   Shah  M   Action mach a spatio temporal maximum average correlation height filter for action recognition  In  CVPR  pp  1 8  IEEE  2008     2    Lan  T   Wang  Y   Mori  G   Discriminative figure centric models for joint action localization and recognition  In  ICCV  pp  2003 2010  IEEE  2011     3    Jones  S   Shao  L   Zhang  J   Liu  Y   Relevance feedback for real world human action retrieval  Pattern Recognition Letters 33 4   446 452  2012    License   MIT license  see the  LICENSE  file  RNN modules libraries benchmark on Torch7   This repo contains benchmark results for popular rnn modules architectures on three different libraries available for Torch7 on a simple task for word language model      cudnn   rnn   rnnlib     The evaluated rnn architectures are the following      RNN   LSTM   GRU     Requirements   To use this repository you must have  Torch7  installed on your system  Also  you ll need a NVIDIA GPU with compute capability 3 5   2GB  ram  and  CUDNN R5   installed in order to run this code    Furthermore  you ll need to install the following dependencies for torch7    bash luarocks install cudnn luarocks install optnet luarocks install rnn luarocks install rnnlib luarocks install torchnet     Note  Please make sure you have an up to date version of torch7 before running the benchmark script  To do this  go to your  torch   folder and run the    update sh  file      Getting started   Data setup   Download and setup the necessary data by running the following script    th download setup data lua   This script will download and extract the following datasets to disk      shakespear   linux kernel   wikipedia     Benchmarking the networks libraries   To evaluate the different rnn modules libraries tested here  run the main script    th scripts run benchmark lua   This will train and test several networks on three different datasets and plot all results into sveral graphs for each dataset    Results   When running the benchmark script  you should get the same results presented in these next graphs    All models have a sequence length of 50  a batch size of 64  2 layers  and were averaged over 10 epochs and over all 3 datasets      Warning  Results are currently being processed  Once they are finished  it takes several days   I ll upload them      Speed  batch ms      Forward backward Speed  batch ms      GPU Memory Usage  in MB      Test Loss Results   Below are the results of the loss of all rnn models tested on the Shakespear   Linux kernel and Wikipedia datasets      Shakespear   Linux kernel   Wikipedia                                                                                               License   MIT License pypackage         Python package to test integration with Travis and Appveyor for continuous integration and deployment    License   MIT dbcollection for Torch7     This is a simple Lua wrapper for the Python s  dbcollection  module  The functionality is almost the same  appart from some few minor differences related to Lua  namely regarding setting up ranges when fetching data    Internally it calls the Python s dbcollection module for data download process management  The  Lua Torch7 interacts solely with the metadata  hdf5  file to fetch data from disk    Package installation   Requirements   This package requires      Python s dbcollection package installed    Torch7   json   hdf5   argcheck     To install Torch7 just follow the steps listed  here     The other packages should come pre installed along with Torch7  but in case they don t  you can simply install them by doing the following    lua luarocks install json luarocks install hdf5 luarocks install argcheck   Installation   To install the dbcollection s Lua Torch7 API you must first have the Python s version installed in your system  If you do not have it already installed  then you can install it either via  pip    conda  or from  source   Here we ll use  pip  to install this package    bash   pip install dbcollection  0 2 6   After you have the Python s version installed in your system  get the Lua Torch7 s API via the following repository    Then  all there is to do is to clone this repo and install the package via  luarocks     bash   git clone https   github com dbcollection dbcollection torch7   Then  all there is to do is to install the package via  luarocks   bash   cd dbcollection torch7     luarocks make rocks     Usage   This package follows the same API as the Python version  Once installed  to use the package simply require  dbcollection        lua         dbc   require  dbcollection              Then  just like with the Python s version  to load a dataset you simply do       lua         mnist   dbc load  mnist               You can also select a specific task for any dataset by using the  task  option       lua         mnist   dbc load name  mnist   task  classification               This API lets you download extract most dataset s data directly from its source to the disk  For that  simply use the  download    method       lua         dbc download name  cifar10   data dir  home some dir               Data fetching   Once a dataset has been loaded  in order to retrieve data you can either use Torch7 s  HDF5  API or use the provided methods to retrive data from the  h5 metadata file    For example  to retrieve an image and its label from the  MNIST  dataset using the Torch7 s  HDF5  API you can do the following       lua         images ptr   mnist file read  default train images   img   images tr partial  1 1    1 32    1 32    1 3     labels ptr   mnist file read  default train labels   label   labels ptr partial  1 1               or you can use the API provided by this package       lua         img   mnist get  train    images   1  label   mnist get  train    labels   1              Documentation   For a more detailed view of the Lua s API documentation see  here     License   MIT license My website talks   Slides of talks I ve done so far  Kaggle   Here are some of my attempts on some  kaggle competitions       Note  The competitions were mostly used for learning benchmarking purposes  Feel free to check and or use any of my solutions      Competitions     Titanic  Machine Learning from Disaster    code     License   MIT License Titanic  Machine Learning from Disaster challenge   My attempt at the  Titanic  Machine Learning from Disaster  Kaggle competition  This repo contains the code to predict which passengers survived the Titanic shipwreck  The code is available via jupyter notebooks and its divided into two main notebooks      notebooks Data Analysis and Visualizations ipynb  has all the data analysis and data visualization code used for the Titanic dataset   notebooks Machine Learning with Scikit learn ipynb  has the code for building up models for predicting the survival label of the passengers of the Titanic test set and for building the predicted results for submission on Kaggle using  Scikit learn   notebooks Machine Learning with PySpark ipynb  has the code for building up models for predicting the survival label of the passengers of the Titanic test set and for building the predicted results for submission on Kaggle using  PySpark       Note  The Machine learning notebook requires that the data visualizations notebook has been run so that the processed data is available for use by this notebook      Requirements     Python3  3 6 recommended    jupyter   scipy stack   pandas  scipy  scikit learn  etc     pyspark  v2 3 0    docker  optional  recommended    docker compose  optional  recommended      Getting started   The code is available via jupyter notebooks for easier use    To run these notebooks  you need to start a jupyter server  Here  you can do it in two ways      a  run a local jupyter server or   b  run a self contained docker image      Run a local jupyter server   To start the jupyter server you must first have python   jupyter installed  The quickest way to accomplish this is by installing  anaconda     After installing anaconda  you should create an environment    bash   conda create  n py36 jupyter python 3 6 anaconda   This command will install the recommended version of CPython and the necessary packages to run the code    Finally  to start a jupyter server you simply need to run the following command    bash   jupyter notebook   Run a self contained docker image   To run the notebooks using docker  you first need to build the container s docker image  To do so  you just need to do the following        i  Build the container using a Makefile macro    bash   make build       ii  Run the container using a command    bash   docker image build  t jupyter spark custom         Then  to start the container you can        i  Run the container using a Makefile macro    bash   make run       ii  Run the container using a command    bash   docker run   rm  p 8888 8888  v   PWD  notebooks  home jovyan work   name jupyter kaggle titanic jupyter spark custom       Setting up the data   To run the cells in the notebooks  you must first download the data for the Titanic challenge  You can get it from  Kaggle  directly and you should put the  train csv  and  test csv  files inside the  notebooks data   directory      Note  The data needed to run the notebooks is not provided by this repo      License   MIT License Bug report for issue  3010   The code available in this repo reproduces the bug submitted in this  issue  3010   To run the code simple do the following steps      clone the repo into your machine     bash   git clone https   github com farrajota vuetify nuxt issue report git     cd to the dir and run the following commands     bash   cd vuetify nuxt issue report    npm install    npm run generate   Bug description   The issue revolves around using  v tabs  to route to different views with the  nuxt  framework    When attempting to generate static files using the latest version of vuetify   vuetify 1 0 0 beta 5   it terminates the process with an error  Using the stable version   vuetify 0 17 3   works correctly    Upgrading packages like  nuxt    vue  or any other package does not fix the issue  the error message clearly points that the problem come from the vuetify package itself       Note   npm run dev  and  npm run build    npm run start  work correctly  It only raises an error when running  npm run generate       Reproducing the error locally   Follow these steps to reproduce the error on your local machine      Fetch the nuxt template using vuetify     bash vue init vuetifyjs nuxt     cd to the project dir and install all dependencies     bash cd  proj     npm install     Install vuetify 1 0 0 beta 5     bash npm install vuetify 1 0 0 beta 5   save   3 5   Optional  Install the latest version of  nuxt   bash npm install nuxt 1 1 1   save     Add the following code to layouts default vue      template     v app dark           Add these 3 lines right after the first two tags in default vue          v tabs fixed tabs centered         v tab router to     stuff  v tab        v tabs              Attempt to generate static files     bash npm run generate   Error message      bash     nuxt generate                          100    Build completed in 15 899s   WARNING  Compiled with 1 warnings                                  20 40 05   warning   asset size limit  The following asset s  exceed the recommended size limit   300 kB   This can impact web performance  Assets    vendor c59b89ca6354e81abe7f js  432 kB    Hash  bba9e937db776460c056 Version  webpack 3 10 0 Time  15902ms                                    Asset       Size  Chunks   Chunk Names  layouts default 8a3bc2ed1e5d25810ede js    2 63 kB       0   emitted    layouts default    pages inspire b3be5d62bb9901dc1c9e js  697 bytes       1   emitted    pages inspire      pages index e9be0503ae250a5fff91 js    1 99 kB       2   emitted    pages index           vendor c59b89ca6354e81abe7f js     432 kB       3   emitted    big   vendor              app f2b6b62a4ff38d03ba68 js    27 9 kB       4   emitted    app         manifest bba9e937db776460c056 js    1 58 kB       5   emitted    manifest app 0ae0e6a1ad902bc0e6bf9464800f3ea5 css     275 kB       4   emitted    app                                 LICENSES  645 bytes           emitted     3 hidden assets   WARNING in asset size limit  The following asset s  exceed the recommended size limit  300 kB   This can impact web performance  Assets    vendor c59b89ca6354e81abe7f js  432 kB  Hash  55e1e27dd6103b323e8c Version  webpack 3 10 0 Time  7347ms              Asset    Size  Chunks             Chunk Names server bundle json  128 kB           emitted   Vue warn   Error in nextTick   TypeError  Cannot read property  firstChild  of undefined    found in                at layouts default vue                 ERROR   TypeError  Cannot read property  firstChild  of undefined       vuetify js 19678 VueComponent        vuetify nuxt issue report   vuetify  dist vuetify js 19678 23       vue runtime common js 1811 Array        vuetify nuxt issue report   vue  dist vue runtime common js 1811 12       vue runtime common js 1732 flushCallbacks      vuetify nuxt issue report   vue  dist vue runtime common js 1732 14       TypeError  Cannot read property  firstChild  of undefined     at VueComponent     home mf tmp2 vuetify vuetify nuxt issue report node modules vuetify dist vuetify js 19678 23      at Array     home mf tmp2 vuetify vuetify nuxt issue report node modules vue dist vue runtime common js 1811 12      at flushCallbacks   home mf tmp2 vuetify vuetify nuxt issue report node modules vue dist vue runtime common js 1732 14      at   npm ERR  code ELIFECYCLE npm ERR  errno 1 npm ERR  my proj2 1 0 0 generate   nuxt generate  npm ERR  Exit status 1 npm ERR  npm ERR  Failed at the my proj2 1 0 0 generate script  npm ERR  This is probably not a problem with npm  There is likely additional logging output above    npm ERR  A complete log of this run can be found in  npm ERR       home mf  npm  logs 2018 01 24T20 40 13 441Z debug log       License   MIT License House Prices  Advanced Regression Techniques challenge   My attempt at the  House Prices  Advanced Regression Techniques challenge  Kaggle competition  This repo contains the code to predict the house prices in dollars for a set of data  The code is available via jupyter notebooks and its divided into two main notebooks      notebooks Data Analysis and Visualizations ipynb  has all the data analysis and data visualization code used for the house prices dataset   notebooks Machine Learning with Scikit learn ipynb  has the code for building up models for predicting the survival label of the passengers of the Titanic test set and for building the predicted results for submission on Kaggle using  Scikit learn         notebooks Machine Learning with PySpark ipynb  has the code for building up models for predicting the survival label of the passengers of the Titanic test set and for building the predicted results for submission on Kaggle using   PySpark        Note  The Machine learning notebook requires that the data visualizations notebook has been run so that the processed data is available for use by this notebook      Requirements     Python3  3 6 recommended    jupyter   scipy stack   pandas  scipy  scikit learn  etc          pyspark  v2 3 0       docker  optional  recommended      Getting started   The code is available via jupyter notebooks for easier use    To run these notebooks  you need to start a jupyter server  Here  you can do it in two ways      a  run a local jupyter server or   b  run a self contained docker image      Run a local jupyter server   To start the jupyter server you must first have python   jupyter installed  The quickest way to accomplish this is by installing  anaconda     After installing anaconda  you should create an environment    bash   conda create  n py36 jupyter python 3 6 anaconda   This command will install the recommended version of CPython and the necessary packages to run the code    Finally  to start a jupyter server you simply need to run the following command    bash   jupyter notebook   Run a self contained docker image   To run the notebooks using docker  you first need to build the container s docker image  To do so  you just need to do the following        i  Build the container using a Makefile macro    bash   make build       ii  Run the container using a command    bash   docker image build  t jupyter spark custom         Then  to start the container you can        i  Run the container using a Makefile macro    bash   make run       ii  Run the container using a command    bash   docker run   rm  p 8888 8888  v   PWD  notebooks  home jovyan work   name jupyter kaggle house prices jupyter spark custom       Setting up the data   To run the cells in the notebooks  you must first download the data for the house prices challenge  You can get it from  Kaggle  directly and you should put the  train csv  and  test csv  files inside the  notebooks data   directory    You can also install and setup the the  kaggle api  in your system and then run  make download  in the terminal to automatically download the data to the correct folder      Note  The data needed to run the notebooks is not provided by this repo      License   MIT License pgAdmin 4  with Docker    This is a basic docker image for running pgAdmin 4 in a container  The default configuration is intended for playing with SQL datasets and it is not intended for production  it runs in  desktop  mode and authentication is disabled     Requirements     Docker   docker compose     Getting started   To run the pgAdmin4 docker image you just need to call  docker compose up   or  make up   to start the service with postgres and pgadmin4    Then  you must open a browser tab and go to  0 0 0 0 5050  or  127 0 0 1 5050  to start using the pgAdmin service      Note  If you start the service via  localhost 5050  you ll receive an error on the form  Query Tool Initialize Error  when trying to use the query tool  This is an issue with pgadmin v3 0 and  as soon as a fix is provided  I ll update this repository accordingly      Setting up a dataset   To create a dataset you first need to set up a server to connect to the postgres service  For that  you ll need to  1  set a server connection to the postgres service and  2 create a database  and  3  load data from a file     The following subsections will show you how to set up these steps in order to use pgAdmin for querying data    Set up a connection to the postgres service     Right click on the  Servers  in the left side bar and create a server          A popup window will appear  In the  General  tab  enter a name for the server  for example   Postgres10            In the  Connection  tab  add the following string to the Host name address   postgres           Set the password field to  postgres   optional  click the  Save password   checkbox to avoid typoping the password everytime you start pgadmin           Save the settings      Create a database     In the previously created server connection  right click on the  Databases  field and click on Create    Database          Give a name to the database and click on save        A database will be created and you should see a sucess message appear in the bottom right corner of the screen    Load restore data of a database   Loading data into a database can be done by following the next steps  Before doing so  you must copy the file of the database you want to load into a directory in the  root dir of this repository with the name  data    After you ve copied a data file into this directory  do the following steps to load the data to postgres      On the database you ve previously created  right click on its name under Databases and select the  Restore  option          A popup window will open  Click on the       icon in the right side of the  Filename  field          This will opean another window and you ll travel to the   home   directory  This is where the data file stored in the  data  dir is located  To visualize it  select the  All Files  option in the dropdown box in the bottom right corner            Select the file you want to load and click on  Save         Next  in the  Restore options  tab  click on the  Data  box and set it to  yes  and then click on  Restore             Note  You ll see in the bottom right corner a success message if the database restoration has been done successfully        Finally  right click on the database you ve restored and click on  Refresh  in order to see the changes done to the database        Querying a database   Querying a database is easy        Select the database you want to query        Right click on the database and select the  Query Tool  option            Note  If it is giving the error  Query Tool Initialize Error   you ll need to connect to pgAdmin via  0 0 0  5050  or  127 0 0 1 5050   pgAmin v3 0 is having an issuing when using  localhost 5050  to connect to the service        Write an SQL statement and click on the symbol to run it        Acknowledgements   This repository is based on  thaJeztah s  pgadmin4 docker     License   MIT License AutoML toolbox   PROJECT DEPRECATED     The  Auto matic  M achine  L earning  AutoML  toolbox is a collection of methods with a simple API which can assist you in your Data Science tasks by providing boilerplate code in a form of simple functions ready to be used  You can use it as your personal Data Science assistant   wizard for creating ETL processes and or a fully automated machine learning pipelines in a simple and quick way    This library is intended to be used as a testing playground for a bunch of wrapper methods to serve as high level APIs to a bunch of common tasks like      data profiling   cleaning missing values   detecting outliers   performing feature engineering   hyper parameter optimization   evaluating machine learning models   creating ensembles of such models   etc      Warning   This code base is in heavy development for now  Once it reaches  v0 1 0  you may then try it  but for now you are at your own risk    Installation   For now  to install this package you must build it from source  To do that  just run the following command in the terminal    bash python setup py install     Note  once this package reaches  v0 1 0  it will be possible to install it via pip      Key Libraries used   This toolbox integrates the following packages in its core for doing most of its work  Basically  you can think of this package as a wrapper for a bunch functions you would uneed like cross validation  hyperparameter optimization  etc   but with a nice  high level API      Numpy   Pandas   pandas profiling  data profiler    Scikit learn  collection of ML libs    xgboost  ML lib    lightgbm  ML lib    Hyperopt  hyperparam optim   bo    HpBandSter  hyperparam optim   hyperband   bo      Libraries to be integrated in the future     dask  distributed computing   big data    keras  DL lib    feature tools  automatic feature engineering    pygdf   GPU DataFrame      TODO   Funcionalities intended to be added to the toolbox       x  basic data profiler       automatic analysis   benchmarking and filling of missing values       automatic analysis   benchmarking and cleaning of outliers       automatic feature transformations   normalization       automatic feature engineering       automatic feature selection       automatic model selection       automatic model optimization  hyper parameter optimization        automatic model ensembling       pre defined parameter list of the most popular ML models in scikit learn       distributed computing  integrate Dask        pipeline generation     License   MIT dockerfiles   Compilation of Dockerfiles used by other repos of mine    License   MIT License Benchmark of Multiple Imputation using Chained Equations  MICE  algorithm   This repo contains a benchmark of the MICE algorithm regarding performance and execution time for imputing missing values on data  For this purpose  several variations of the MICE algorithm have been implemented using  LightGBM  instead of linear models for value imputation for accuracy and speed improvements  Here  four method are evaluate against the mean   mode value imputation procedure for multi dimensional data on 5 different datasets available on the  Scikit Learn  package  namely  boston house prices  iris  diabetes  wine and breast cancer      Vanila MICE   Value by value imputation over a set number of iterations    Fast MICE   Column by column imputation over a set number of iterations    Slow Fast MICE   Value by value imputation in the first iteration and column by column for the remaining iterations    Fast Slow MICE   Column by Column imputation in all iteration except the last one where value by value imputation is used for the remaining iterations   The procedure is available via a jupyter notebook in the  notebook   folder in this repo    TL DR   If you are just looking for the results of the benchmark  here they are      On average  Fast MICE is  12 0x  faster than Fast Slow   Slow Fast MICE and  56 0x  faster than Vanila MICE   On average  Fast Slow   Slow Fast MICE are  5 0x  faster than Vanila MICE     Boston house prices results     Iris results     Diabetes results     Wine results     Breast Cancer results     Requirements     Python3  3 6 recommended    jupyter   scipy stack   pandas  scipy  scikit learn  etc     docker  optional  recommended      Getting started   The code is available via jupyter notebooks for easier use    To run these notebooks  you need to start a jupyter server  Here  you can do it in two ways      a  run a local jupyter server or   b  run a self contained docker image      Run a local jupyter server   To start the jupyter server you must first have python   jupyter installed  The quickest way to accomplish this is by installing  anaconda     After installing anaconda  you should create an environment    bash   conda create  n py36 jupyter python 3 6 anaconda   This command will install the recommended version of CPython and the necessary packages to run the code    Finally  to start a jupyter server you simply need to run the following command    bash   jupyter notebook   Run a self contained docker image   To run the notebooks using docker  you first need to build the container s docker image  To do so  you just need to do the following        i  Build the container using a Makefile macro    bash   make build       ii  Run the container using a command    bash   docker image build  t jupyter scipy custom         Then  to start the container you can        i  Run the container using a Makefile macro    bash   make run       ii  Run the container using a command    bash   docker run   rm  p 8888 8888  v   PWD  notebook  home jovyan work   name jupyter benchmark mice jupyter scipy custom       License   MIT Digit Recognizer challenge   My attempt at the  Kaggle s Digit Recognizer  competition  This repo contains the code to classify digit images using Keras  Tensorflow and Pytorch deep learning frameworks  The code is available via jupyter notebooks and  depending on the deep learning library used  contains the following notebooks      notebooks keras mnist ipynb   solution using Keras   notebooks tensorflow mnist ipynb   solution using tensorflow  TODO    notebooks pytorch mnist ipynb   solution using pytorch  TODO      Requirements     Python3  3 6 recommended    jupyter   scipy stack   pandas  scipy  scikit learn  etc     Keras  2 2 4    Tensorflow  1 11 0    Pytorch  0 4 1    docker  optional      Getting started   The code is available via jupyter notebooks for easier use    To run these notebooks  you need to start a jupyter server  Here  you can do it in two ways      a  run a local jupyter server or   b  run a self contained docker image      Run a local jupyter server   To start the jupyter server you must first have python   jupyter installed  The quickest way to accomplish this is by installing  anaconda     After installing anaconda  you should create an environment    bash   conda create  n py36 jupyter python 3 6 anaconda   This command will install the recommended version of CPython and the necessary packages to run the code    Finally  to start a jupyter server you simply need to run the following command    bash   jupyter notebook   Run a self contained docker image   To run the notebooks using docker  you first need to build the container s docker image  To do so  you just need to do the following        i  Build the container using a Makefile macro    bash   make build       ii  Run the container using a command    bash   docker image build  t jupyter deeplearning custom         Then  to start the container you can        i  Run the container using a Makefile macro    bash   make run       ii  Run the container using a command    bash   docker run   rm  p 8888 8888  v   PWD  notebooks  home jovyan work   name jupyter kaggle mnist jupyter deeplearning custom       Setting up the data   To run the cells in the notebooks  you must first download the data for the house prices challenge  You can get it from  Kaggle  directly and you should put the  train csv  and  test csv  files inside the  notebooks data   directory    You can also install and setup the the  kaggle api  in your system and then run  make download  in the terminal to automatically download the data to the correct folder      Note  The data needed to run the notebooks is not provided by this repo      License   MIT License Sample Project  ETL pipeline   API service for predicting a user s gender from click stream data   This repo contains an exercise project for processing a click stream of users visiting a website in order to create a gender predicting service to infer what gender a visitor is  Here it is shown how the pipeline and gender API service are architected  developed and deployed on AWS using AWS SAM    The solution of this exercise uses a serverless data pipeline using AWS Kinesis  S3  Dynamodb  Lambda and API Gateway services  and the architecture is coded as a CloudFormation template for easier deployment to the AWS cloud    Challenge Overview   The challenge is the following  see the figure below   from a stream of event clicks of visitors on a fashion website  build a gender API service which predicts a visitor s gender by using a  clientID   The solution must be able to handle a couple of millions of click events per day and be able to cope with a substantial increase in traffic in the upcomming months  2x more       To determine the visitors  gender  the three following heuristics must be used      Last Gender Visited   Top Gender Visited   Top Gender Visited in the last 7 days  rolling window        Note  The goal of this challenge is not to design the most accurate model to predict a visitor s gender  so a simple model is sufficient for the purpose of this exercise      The goals to solve  this challenge is to develop an infrastructure to read the stream  process and persist the data  and return the predicted gender for any user  Random data should be generated to see the pipeline in action and any technology stack is allowed     Solution Overview   The solution to this exercise was designed using  serverless  services from AWS to store and process streams of data from visitor s page hits  This solution also provides a gender API service to fetch the gender of visitors via an identifier passed as a  GET  request    The architecture design of the pipeline in this repo can be seen in the image below  This type of architecture requires little maitenance while being capable of handling large loads of data by elasticaly scaling from dozens to thousands of requests per second without special oversight      Requirements   To deploy this architecture  it is required the following tools in order to set it up with the provided commands in this repo      Linux   Python 3 6    Docker   AWS CLI   AWS SAM   AWS account   Terraform v0 11 11   optional        Note  Ensure that the linux distribution you are using comes with Python 3 6  support  ubuntu 16 04  is a good choice       Python dependencies   The Python s lib dependencies are available via a  Pifile  and  requirements txt  in this repo    If you are using  pipenv  you can install the dependencies using    bash pipenv install   If you are using  virtualenv    conda  or the default python env  to install the requirements you need to do the following    bash pip install  r requirements txt   Installing docker   To install docker  follow this guide in the official docker documentation  https   docs docker com install linux docker ce ubuntu    Installing AWS CLI   Installing the aws cli is pretty straight forward  To do so  run the following command in a terminal    bash pip install awscli   Installing AWS SAM   Installing AWS Serverless Application Model  SAM  is simple and requires you to have docker  aws cli and pip installed in your machine  To install AWS SAM  you can follow the official install guide from AWS  https   docs aws amazon com serverless application model latest developerguide serverless sam cli install html   Getting an AWS account   If you do not have an aws account with programmatic access  you first need to create one before proceeding any further  To do so  follow the official guide here and create an account  https   aws amazon com premiumsupport knowledge center create and activate aws account    When getting access to AWS services  create an account for programmatic access  and console access as well  in order to be able to deploy the pipeline in the aws cloud  This short  youtube video  should help you create one in case you don t know yet how to do it    Getting started   Before proceeding to deploy the architecture  you will need to define two environment vars containing your aws s access key and secrets  To set them up  do the following commands in a terminal window    bash export AWS ACCESS KEY  your access key  export AWS SECRET KEY  your secret key    After this is done  in order to deploy and test the architecture  you need to build  package and deploy the architecture template to aws cloud  Afterwards  to really test the architecture  some dummy data needs to be generated in order to see the pipeline working as expected    To simplify these processes  several macros are available in the  Makefile  in the root of this repo    The next sections follows the 4 steps needed to start using and testing the pipeline      Building the source code   Packaging the code and generate a cloudformation template   Deploy the cloudformation template to aws cloud   Generate dummy data to test the pipeline     Building the source code   The first step to deploy the pipeline starts with building the lambda code stored in the following folders      kinesis firehose stream process    contains the code used to preprocess the data stream generated by Kinesis Firehose   process stream data    contains the code used to compress the stream data to  parquet  format  process the visitor s gender hits and update the dynamodb table state   daily process lambda    computes the daily page hits of all visitors on the previous day  computes the top gender of the last 7 days of visitors and updates the dynamodb table state   gender api lambda    retrieves the gender data of a visitor via a  clientid  from the dynamodb table     To to this  run the following command in the terminal    bash make build   This will build the lambda code and their lib requirements from these folders automatically to be deployed to AWS S3 so AWS Lambda can access these functions    Packaging the code   Next  the code and its dependencies need to be bundled and stored in a S3 bucket in order for AWS Lambda to access it  and to transform the  template yaml  to a valid cloudformation template    To package and generate the cloudformation template  run the following command in the terminal    bash make package   Deploying the code   Finally  the architecture can be deployed to the aws cloud by using the cloudformation template generated in the previous step    bash make deploy   This will take a few minutes for AWS to to spin up the services  You can check its progress by visiting the CloudFormation service in the AWS console  When the services and ready to run  the command shall stop and you should be able to run additional commands in the terminal    All in one command   You can accomplish the 3 previous commands with the following macro to build  package and deploy the pipeline with a single command    bash make setup all   Deploying using Terraform   The same stack can be deployed to AWS using Hashicorp s Terraform  For that  you need to first have terraform installed on your machine  and then you can do the following to deploy the architecture to AWS        initialize terraform   bash make terraform init       Package and store the code to S3   bash make terraform build package       Deploy the services to AWS   bash make terraform deploy       Afterwards  to terminate and clean the allocated services on AWS  run the following command    bash make terraform destroy       Generating dummy data   To see the pipeline working in its full potential  it would be best to generate data of some previous days in order to see the effects of computing the top gender in the last 7 days  To do so  run the following command in the terminal which generate some dummy data for the last 10 days  from the current time ofc     bash make generate dummy data   Generating streams of data   To generate streams of dummy data you can run the following command   bash make generate stream data   You can define the number of requests per second by using the  requests per second  input arg    bash make generate stream data requests per second 50   Cleaning up   After you tested and experimented with the pipeline  you can stop and delete the cloudformation stack with the following command    bash make delete stack     Note  S3 buckets containing files won t be deleted by default  so you need to manually delete them to clean your environment in case the buckets used contain objects      Unit tests   You can run the unit tests of the lambda functions used to process the stream data by running the following command in the terminal    bash make test   This will run all unit tests inside the  tests   folder      Note   pytest  module is required to run the tests      License   MIT Build and package code with custom libraries to aws lambda   Build and package code using custom libraries into an aws lambda compatible zip file with little to no effort  Additionally  you can also upload the packaged code to S3 using a simple command    Requirements     Docker   aws cli   optional    Dependencies listed in either  requirements txt  or  Pipfile lock     Pipfile     How to use   To build and package your code  all you need to do is to copy your code to the  code   folder and run the following command    bash   make build package   This will build and package your code targetting Python 3 7 as the run time into a  zip  file   lambda zip      If you wish to target another version for python  or to change the output name of the zip file  you can use the  version   and  filename   input arguments  respectively    bash   make build package version  version  filename  file name    Example   You want to build your code with Python 3 7 as a target  but you only have version 3 6 installed or you have been using Python 3 6 but want to package it using 3 7 with compatible libraries  Assuming you are using either a  requirements txt  or a  Pipfile lock  file to store the dependencies used in your code  to build and package your code painlessly  you need to do the following    bash   make build package version 3 7 filename example zip   This will generate the aws lambda compatible zip file   example zip  containing  all the specified libraries in  requirements txt  or  Pipfile lock  built targeting Python 3 7 as the runtime  If you wish to use another version of Python  all you need to do is change the target version from 3 7 to the version you want in the  make build package  macro    Uploading to S3   In order to upload the file to S3  it is required you to have a authentication configured for aws cli using either a config file      aws credentials   or environment variables  If you are using environment variables  you can set an account in the command line with the following commands    bash   export AWS ACCESS KEY ID  access key    export AWS SECRET ACCESS KEY  secret key    Then  to upload the packaged file to S3  run the following command in the terminal    bash   make upload filename  filename zip  bucket  bucket name    License   MIT Setting a connection pool using PgBouncer for PostgreSQL   Setting up a connection pool for PostgreSQL does not have to be a complicated process  Actually  it can be a reasonably simple procedure if you know enough  In this repo you ll find a simple tutorial on how to deploy PgBouncer alongside a Postgres database in a kubernetes environment      Note  This tutorial uses kustomize to configure and setup the deployment manifest          If you never used it before  it is a  simple and powerful tool          to customize kubernetes deployments      Requirements     kubernetes cluster  e g   miniconda  k3s k3d  kind    kubectl  1 16  kustomize    Linux   MacOS     Setting up the environment   resources       Start the kubernetes cluster  bash minikube start       Generate k8s deploy manifest  bash kubectl kustomize   deploy yaml       Create the k8s resources  bash kubectl create  f deploy yaml       Test the setup  bash kubectl  n database exec  it pg 0    bash  c  PGPASSWORD password user1 psql  U user1  h pgbouncer  p 5439  d sandbox  c   SELECT   from test         Destroy the k8s resources  bash kubectl delete  f deploy yaml       Clean remaining artifacts  bash rm deploy yaml       References     https   www pgbouncer org config html   https   gitlab com aztek io oss containers pgbouncer container   https   www cybertec postgresql com en pgbouncer authentication made easy    https   www depesz com 2012 12 02 what is the point of bouncing    https   www enterprisedb com postgres tutorials challenges setting maxconnections and why you should use connection pooler     License   MIT License Design Patterns   Send random design patterns to your slack for you or your teammates to see    The motivation for making this is due to the existence of dozens and dozens of design patterns one can  find in the wild  but you have no idea of their existence but still you would like to learn more about to enrich your knowledge and toolkit    This repo provides a tool to easily ingest this kind of information by providing a summarized view of the patterns which you can learn more about and share it with your friends      Note  This repo is slowly being updated but if you would like to contribute  feel free to make a pull request and I ll gladly review it      Requirements     Python 3 6    Slack API key   Slack channel     Usage   Slack   To send a message  first set  SLACK API KEY  and  SLACK CHANNEL  as environment variables with the api key and channel to send messages to    bash export SLACK API KEY  xxxx xxxxxxxx xxxxxxxxxx xxxxxxxxxxxxxxxxx  export SLACK CHANNEL   monitoring testing    Then  to send a message with a random pattern just run the following script    bash python slackbot send messages py   random   You should see some message like the following      Additionally  to generate a random list of messages that will not be repeated and to send a single message do the following    bash python slackbot send messages py    sends a message to slack python slackbot send messages py    sends a different message python slackbot send messages py    send another different one     Idea  Setup a cronjob to send random messages to slack every morning to your team to share this knowledge      Patterns   Cloud Architecture     Azure cloud design patterns     TODO     Add patterns    general architecture   Cloud architecture         References     https   github com DovAmir awesome design patterns cloud architecture     License   MIT License
43,chahuja,hilbert fpga   Implementation of 32 point hilbert transform on FPGA  verilog    fft16 v   implementation of 16 pt fft using lesser multiplications and a few more additions than standard fft           floatind point error reduced fft32 v   uses fft16 v to implement 16 pt fft hilbert v uses fft32 v to implement hilbert transform of the complete system   Preliminary analysis   absolute error in a random signal   10   3  when compared to matlab implementatino   Feel free to use  modify or distribute this code  No guarantees on its robustness  it might just crash your system  P edgeDetection   Detection of Edges using XOR operator   Matlab scripts   sxor m   gives edge image for black white images   bitplanes m   gives edge image for gray scale images   errors m   calculates PSNR and MSE jointCMF   Complex Matrix Factorization     nmfStandard m   Standard NMF Interations to minimize Eucledian distance with parameters as    nIter    Number of Iterations   fixedX    to keep X fixed while interating   X    to initialize X with given values   nmfWithPhase4 m   Mis named as nmfWithPhase  it is basically performing complex matrix factorization based on the paper http   arxiv web3 library cornell edu pdf 1411 6741v1 pdf It has a single dependency on nmfStandard m which it uses to perform Complex Matrix Factorization     Outputs   X    Complex Factorized Vector Sets   PxQ    H    Real Factorized Weights  QxR    Xc    Real Matrix which can be appropriatly transformed to get X  2Px2Q     Manga Download   This script could be rendered useful for downloading batch images of manga from  mangareader net   It is currently at a very basic level as one would have to give options to select a particular manga      URL of the first page in the manga    site      Number of Chapters    num chap      Folder to store the outputs    folder    Example  sh python manga download py   site http   www mangareader net death note 1 1   num chap 10   folder dn   If one would like to convert the images to a pdf  ImageMagik could be used to do so with a single command  sh convert   jpg manga pdf   At present  only python 2 7 is supported  It may be compatible with python 3  but I have not checked  Personal website   uses  https   github com bamos cv  to generate the contents of this webpage and some markups for icons from https   github com bamos bamos github io The Secret Ingredient s    Just some random recipes I tried and they surprisingly turned out well  Some of them are inspired from random cookbook or online recipes  some come from late night phone calls with grandma and very few are my own creation    Feel free to suggest changes  modifications or even contribute a whole new recipe    Lattice Recurrent Unit   Implementation of Lattice Recurrent Unit as described in this  paper   I encourage you to check out the  website  to get an overview of the results and observations described in the paper    The code has been written in  PyTorch  and has two key components      Language Model   Given a bunch of sentences  the model learns to predict the next character  word or more generally  token  conditioned on all the characters until the current time step   Check  class langModel  in  src model py     Lattice   A Lattice Network  unlike LSTM and GRU  supports distinct outputs along depth and time  Hence we implemented  class Lattice   in  src model py   which supports recurrent units with 2 different outputs  Batches with multiple length sequences are allowed if they are converted to  torch nn utils rnn PackedSequence      In addition   Lattice  also supports multiple layers within an RNN cell   class LRUxCell  and  class HIGHWAYxCell  in  src model py           Currently only works for  batch first True     I will make it  batch first  independent as soon as I get time       Lattice Language Model       Requirements   This code is written in Python 3 6 and requires PyTorch    v0 2   I would suggest using the  anaconda  environment provided in  env   as this would save the hassle of installing all the requirements    sh conda env create  n  env name   f env torch 0 2 0 cuda80 py36 pandas yaml   To active the environment run   sh source activate  env name    Alternatively  if you would like to save space or be adventurous  you could cherry pick and install the missing requirements    Usage   All the source files are in a sub directory  src   sh cd src   Dataset   I would suggest storing every dataset in a directory of its own as the code creates multiple meta data files essential to the training process    Training   A language model using various RNN units can be trained using  char py     sh python char py  data  path to data                    model  model name                    rnn size  rnn size                    num layers  num layers                    num unrolling  backprob time                  save dir  path to results                    cpk  checkpoint name    The models supported in this implementation are  lru    rglru    pslru    gru    lstm    highway   and  glstm     For example the training scripts can be called with the following command  sh python char py  data    dataset ptb ptb txt  model lru  rnn size 500  num layers 2  num unrolling 50  save dir save ptb lru  cpk m   Arguments   It would be useful to check out all the arguments of the training code by running   sh python char py  h   Sampling characters from a model   Weights for the best model  based on validation loss  are saved as a pickle file at the end of training  or after every epoch if   greedy save 1  is used   These weights are used to initialize a language model from which characters can be sampled    sh python generate py  load  path to weights   num sample 1000   Note  Weights are stored in   save dir  with a suffix of   weights p   GPU support   Using   cuda  gpu id   for  char py  and  generate py  gives the option of choosing a device on a multi gpu machine  If you wish to run train the model on a cpu  use   gpu id     1     Note  Currently multi gpu training is not supported    Other Implementations     A nice gist of the LRU Cell in  Tensorflow  by  simonnanty    Casper   some random functions classes which serve a utilitarian purpose in multiple projects PS  Hierarchy of Classes will change as I add more utilities    Requirements     pandas   tensorboard   pytorch     Installing   sh git clone https   github com chahuja pycasper git cd pycasper python setup py install   ProtoTyping   Name  When finding unique names becomes a bigger task than the task itself      Based on a Namespace of arguments and its values   Name  creates a unique name    If all arguments are too much for the length of the name  it is possible to provide a subset of those arguments   Read more in the docfile of the class Name     BookKeeper     Logging routine for deep learning      Results   Visualize results stored by BookKeeper   I would recommend using jupyter lab   notebook to see all the results in a pretty format  import pandas as pd from from pycasper results import walkthroughResults df  df all   walkthroughResults  save    args subset   exp    cpk    model    window hop                      res subset   train    dev    test    val key  dev   print df sort values by  dev      PS  Ideally  it would work best if you use  BookKeeper  to store the results  But  if you store results in a json format  with  train    test    dev  as dictionary keys storing lists of loss metrics across number of epochs and args for the experiment as a python Dictionary  this function would work    sh Results filename   name  res json Args filename   name  args args Both of them are json files Language2Pose Natural Language Grounded Pose Forecasting     Paper    Webpage     There are 5 steps to running this code   Python Virtual Environment and dependencies   Data download and preprocessing   Training   Sampling   Rendering     PS  The implementation of one of the baselines  proposed by Lin et al   1    was not publicly available and hence we make use of  our  implementation  of their model to generate all the results and animations marked as Lin et al  Due to the differences in training hyperparameters  dataset and experiments  the numbers reported for Lin et al  in our paper differ from the ones in the original paper   1      PS  This repo  at the moment  is functional at best  Feel free to create issues pull requests however you see fit       Python Virtual Environment   Anaconda is recommended to create the virtual environment  sh conda create  f env yaml source activate torch   To handle the logistics of saving loading models  pycasper  is used  sh git clone https   github com chahuja pycasper cd src  ln  s    pycasper pycasper   cd        Data   Download   We use KIT Motion Language Dataset which can be downloaded  here   sh wget https   motion annotation humanoids kit edu downloads 4 2017 06 22 zip mkdir dataset kit mocap unzip 2017 06 22 zip  d dataset kit mocap rm 2017 06 22 zip   Download Word2Vec binaries   Download the binary file  here  and place it in  src s2v   Pre trained Models   Download pretrained models  here  and place it in  src save   Preprocessing   sh python data data py  dataset KITMocap  path2data    dataset kit mocap   Rendering Ground Truths   sh python render py  dataset KITMocap  path2data    dataset kit mocap new fke  feats kind fke   Calculating mean variance for Z Normalization   sh python dataProcessing meanVariance py  mask   0    feats kind rifke  dataset KITMocap  path2data    dataset kit mocap  f new 8     Training   We train the models using a script  train wordConditioned py   Pardon the misnomer  initially it was supposed to be word conditioned pose forecasting but then I ended up adding sentence conditioned pose forecasting as well and was too lazy to change the filename     All the arguments  and their corresponding help texts  used for training can be found in src argsUtils py  PS  Some of them might be deprecated  but I have not removed them in case it breaks any of the other code that I might have written in the experimentation phase  Please raise an issue  or send me an email if you have any clarification questions about any of the arguments   It would be good to stick to the args used in the examples if you want to play with the models in the paper      JL2P  sh python train wordConditioned py  batch size 100  cpk jl2p  curriculum 1  dataset KITMocap  early stopping 1  exp 1  f new 8  feats kind rifke  losses    SmoothL1Loss     lr 0 001  mask   0    model Seq2SeqConditioned9  modelKwargs    hidden size  1024   use tp  False   s2v   lstm     num epochs 1000  path2data    dataset kit mocap  render list subsets render list  s2v 1  save dir save model   tb 1  time 16  transforms    zNorm         modelKwargs  need some explaination as they could vary based on the model   sh hidden size  size of the joint embedding use tp  use a trajectory predictor  1   False for JL2P models s2v  sentence to vector model   lstm  or  bert       Our Implementation of Lin et  al   1   sh python train seq2seq py  batch size 100  cpk lin  curriculum 0  dataset KITMocap  early stopping 1  exp 1  f new 8  feats kind rifke  losses    MSELoss     lr 0 001  mask   0    model Seq2Seq  modelKwargs    hidden size  1024   use tp  True   s2v   lstm     num epochs 1000  path2data    dataset kit mocap  render list subsets render list  s2v 1  save dir save model  tb 1  time 16  transforms    zNorm        This model has 2 training steps   train seq2seq py  uses a seq2seq model to first learn an embedding for pose sequences  Once the training is complete   train wordConditioned py  is called which optimizes to map from language embeddings to pose embeddings      Sampling   Sampling from trained Models   The training scripts will sample after the stopping criterion has reached  but if you would like to manually sample run the following script   sh python sample wordConditioned py  load  path to weights p     path to weights p   ends in   weights p   Using Pretrained Models   Make sure you have downloaded the pre trained models as described  here     JL2P   sh python sample wordConditioned py  load save jl2p exp 726 cpk jointSampleStart model Seq2SeqConditioned9 time 16 chunks 1 weights p     Our Implementation for Lin et  al   1   sh python sample wordConditioned py  load save lin et al exp 700 cpk mooney model Seq2SeqConditioned10 time 16 chunks 1 weights p       Rendering   After sampling  it would be nice to see what animation does the model generates  We only use the test samples for rendering      If possible  use a machine with many cpu cores  as rendering animations on matplotlib is painfully slow   render py  uses all the available cores for parallel processing    Using your trained model   sh python render py  dataset KITMocap  load  path to weights p   feats kind fke  render list subsets render list   Using pre trained Models       JL2P  sh python render py  dataset KITMocap  load save jl2p exp 726 cpk jointSampleStart model Seq2SeqConditioned9 time 16 chunks 1 weights p  feats kind fke  render list subsets render list       Our Implementation for Lin et  al   1   sh python render py  dataset KITMocap  load save lin et al exp 700 cpk mooney model Seq2SeqConditioned10 time 16 chunks 1 weights p  feats kind fke  render list subsets render list       References    1   Lin  Angela S   et al   1  Generating Animated Videos of Human Activities from Natural Language Descriptions   Learning 2018  2018   Demo Videos coming soon collection of colab notebooks PATS Dataset   PATS was collected to study correlation of co speech gestures with audio and text signals  The dataset consists of a diverse and large amount of aligned pose  audio and transcripts  With this dataset  we hope to provide a benchmark which would help develop technologies for virtual agents which generate natural and relevant gestures  For a complete overview check the following  link            License   This work is licensed under a  Creative Commons Attribution NonCommercial 2 0 Generic License     Relevant Paper s      Ahuja  Chaitanya  et al   Style Transfer for Co Speech Gesture Animation  A Multi Speaker Conditional Mixture Approach   ECCV 2020     website    code     Ahuja  Chaitanya  et al   No Gestures Left Behind  Learning Relationships between Spoken Language and Freeform Gestures  Findings at EMNLP 2020     code       Structure of the dataset   sh pats data     cmu intervals df csv     missing intervals h5     processed         oliver   speakers             XXXX h5             YYYY h5                       jon                             bee         noah     raw         oliver cropped             xxx mp3   The dataset consists of      cmu intervals df csv   list of all intervals and the relevant meta information  Similar to  Ginosar et  al  2019     missing intervals h5   list of all intervals that have incomplete set of features  For the sake of uniformity they are excluded from the benchmark tests    proceesed   h5 files containing processed features for pose  audio and transcripts for all speakers   raw   mp3 audio files corresponding for each interval which is useful during rendering      Processed Features   Heirarchy of the features in h5 files explained  To access a festure  both parent and child combine to give a key  For example   pose data    audio log mel 400  and so on     pose           data   XY coordinates of upper body pose relative to the neck joint  Joint order and parents can be found  here         normalize   same as data but the size of the body is normalized across speakers  In other words  each speaker is scaled to have the same shoulder length  This is especially useful in style transfer experiments where we would like the style of gestures to be independent of the size of the speaker         confidence   confidence scores provided by openpose with 1 being most confident and 0 being least confident     audio          log mel 400    Log mel Spectrograms extracted with the function  here         log mel 512   Log mel Spectrograms extracted with the function  here          silence   Using  VAD  we estimate which segments have voice of the speaker and which just have noise     text          bert   fixed pre trained bert embeddings of size 768        tokens   tokens extracted using BertTokenizer of  HuggingFace         w2v   Word2Vec features of size 300        meta   Pandas Dataframe with words  start frame and end frame   Raw Features   We provide links to original youtube videos in  cmu intervals df csv  to help download the relevant audio files  Rendering the generated animations with audio would require the raw audio and would be useful for user studies    Dataset Download   To download  processed  features of the dataset visit  here   Each of the 25 speakers is available as a single zip file except  noah  which had to be split to 2 files  Please run  cat noah zip     noah zip  to recombine the split files before unzipping them    To download  raw  features of the dataset run   sh python youtube2croppedaudio youtube2audio py    base path pats data      Path to dataset folder   speaker bee     Speaker Name  Optional   Downloads all speakers if not specified  interval path cmu intervals df csv  As raw audio files are downloaded from video streaming websites such as YouTube  some of them may not be available at the time of download  For the purposes of consistent benchmarking  processed  features should be used    Data Loader   As a part of this dataset we provide a DataLoader in  PyTorch  to jumpstart your research  This DataLoader samples batches of aligned processed features of Pose  Audio and Transcripts for one or many speakers in a dictionary format  We describe the various  arguments  of the class  Data  which generates the DataLoaders    DataLoader Examples   Ipython Notebook   Requirements     pycasper     sh mkdir    pycasper git clone https   github com chahuja pycasper    pycasper ln  s    pycasper pycasper       Create an  anaconda  or a virtual enviroment and activate it     sh pip install  r requirements txt   Arguments of class  Data   There are way too many arguments   research  for  Data   For most cases you might not even need most of them and can leave them as default values  We divide the arguments into  Essential    DataLoader Arguments    Modality Arguments    Sampler Arguments  and  Others     Essential     path2data  str    path to processed data e g   pats data processed    speaker  str or list    one or more speaker names  Find list of speakers  here     modalities  list    list of processed features to be loaded  Default    pose data    audio log mel 512    Find list of all processed features  here     fs new  list    list of frame rates for each modality in modalities  Default   15  15   Length of fs new    Length of modalities     time  float    length of window for each sample in seconds  Default  4 3  The default value is recommended  It results in 64 frames of audio and pose when fs new is 15    split  tuple or None    train  dev and test split as fractions  Default  None  Using None would use pre defined splits in cmu intervals df csv  Example use case of a tuple   0 7  0 1  represents the ratios of train and dev  hence test split is 0 2    window hop  int    number of frames a window hops in an interval to contruct samples  Default  0  Using 0 implies non overlapping windows  For  window hop    0  samples are created with the following formula   sample i i int time fs new 0    for i in range 0  len sample   window hop       DataLoader Arguments     batch size  int    Size of batch  Default  100    shuffle  bool    Shuffle samples after each epoch  Default  True   num workers  int    Number of workers to load the data  Defaut  0     Text Arguments     filler  int    Get  text filler  as a feature in the sampled batch  This feature is a tensor of shape  batch x time   where each element represents if the spoken work was a filler word or not  The list of filler words is the same as nltk s stopword list for english  Default  0  Use 1 to get the  text filler  feature    repeat text  int    If 1  the feature of each word token is repeated to match the length of its duration  For example if a word is spoken for 10 frames of the pose and or audio sequence  it is stacked 10 times  Hence the time dimension of pose audio and transcripts are the same  If 0  words tokens are not repeated  As each sample could have different number of words  the shorter sequences are padded with zeros  Extra features  text token duration  and  text token count  are also part of the sample which represent the duration of each token in frames and number of tokens in each sequence respectively      Sampler Arguments  Mutually exclusive unless specified      style iters  int    If value   0   AlternateClassSampler  is used as the sampler argument while building the train dataloader  This sampler is useful if two or more speakers are trained together  This sampler ensures that each mini batch has equal number of samples from each speaker  Value refers to the number of iterations in each epoch  Default  0    sample all styles  int    Can only be used with argument  style iters  If value   0  randomly selects value number of samples from each speaker to load  This is especially useful for performing inference in style transfer experiments  when the number of permutations of style transfer increases exponentially with the number of speakers  This argument puts an upper bound on the number of samples for each speaker  hence limiting the time to generate gestures for a limited number of inputs  Default  0    num training sample  int or None    if value   0  chooses a random subset of unique samples with cardinality of the set    value as the new training set  if value is None  all samples are considered for training  Default  None    quantile sample  float or int or None    Default  None    quantile num training sample  int or None    Default  None    weighted  int    If value   0   torch utils data WeightedRandomSampler  as the sampler argument while building the train dataloader  The weights are set to 1 for each sample  While  this is equivalent to a uniform sampler  this provides a possibility of being able to change the weights for each sample while training  Default  0      Others     load data  bool    If True  loads the hdf5 files in RAM  If False  files are not loaded and the dataloaders will not work as intended  Useful for quick debugging    num training iters  int or None    If value   0  changes the training sampler to sample with replacement and value is the number of iterations per epoch  If value is None  the sampler samples without replacement and the number of iterations are inferred based on the size of the dataset  Default  None      Render   Check this  repository  for rendering scripts    Creating your own dataloader   In case you prefer to create your own dataloaders  we would recommend checking out the  structure of the h5 files  and the last sections of the  Ipython Notebook   We have a class  HDF5  with many staticmethods which might be useful to load HDF5 files    Issues   All research has a tag of work in progress  If you find any issues with this code  feel free to raise issues or pull requests  even better  and I will get to it as soon as humanly possible  Mix Stage   This is the official repository for the paper  Style Transfer for Co Speech Gesture Animation  A Multi Speaker Conditional Mixture Approach     Chaitanya Ahuja   Dong Won Lee   Yukiko Nakano    Louis Philippe Morency     ECCV2020     Links   Paper    Demo Project Website    Dataset Website   Bibtex    sh  inproceedings ahuja2020style    title  Style Transfer for Co Speech Gesture Animation  A Multi Speaker Conditional Mixture Approach     author  Ahuja  Chaitanya and Lee  Dong Won and Nakano  Yukiko I and Morency  Louis Philippe     booktitle  European Conference on Computer Vision     year  2020      Overview     This repo has information on the training code and pre trained models     For the dataset  we refer you to     Dataset Website  for downloading the dataset    Dataset Repo  for scripts to download the audio files and other dataloader arguments     For the purposes of this repository  we assume that the dataset is downloaded to     data    This repo is divided into the following sections      Clone   Set up environment   Training   Inference   Rendering     This is followed by additional informational sections     Experiment Files     Inception Score for pose sequences   Clone   As the project website is also hosted on this repository  clone only the master branch    sh git clone  b master   single branch https   github com chahuja mix stage git   Set up Environment     pycasper        sh cd mix stage mkdir    pycasper git clone https   github com chahuja pycasper    pycasper   cd src ln  s       pycasper pycasper       create a symlink         Create an  anaconda  or a virtual enviroment and activate it     sh pip install  r requirements txt   Training   To train a model from scratch  run the following script after chaging directory to  src     sh python train py     cpk JointLateClusterSoftStyle4 G      checkpoint name which is a part of experiment file PREFIX   exp 1      creates a unique experiment number   path2data    data    path to data files   speaker    corden    lec cosmic    ytch prof    oliver         List of speakers   model JointLateClusterSoftStyle4 G      Name of the model   modelKwargs    lambda id   0 1   argmax   1   some grad flag   1   train only   1        List of extra arguments to instantiate an object of the model   note mix stage      unique identifier for the model to group results   save dir save mix stage      save directory   modalities    pose normalize    audio log mel 400         all modalities as a list  output modality first  then input modalities   fs new   15  15        frame rate of each modality   input modalities    audio log mel 400         List of input modalities   output modalities    pose normalize         List of output modalities   gan 1      Flag to train with a discriminator on the output   loss L1Loss      Choice of loss function  Any loss function torch nn   will work here   window hop 5      Hop size of the window for the dataloader   render 0      flag to render  Default 0   batch size 16      batch size   num epochs 20      total number of epochs   overfit 0      flag to overfit  for debugging    early stopping 0      flag to perform early stopping    dev key dev spatialNorm      metric used to choose the best model   num clusters 8      number of clusters in the Conditional Mix GAN   feats    pose    velocity    speed         Festures used to make the clusters   style iters 3000      Number of training iterations per epoch   num iters 3000    Maximum number of validation iterations per epoch   Scripts for training models in the paper can be found as follows     Mix StAGE     StAGE   Inference   Inference for quantitative evaluation   sh python sample py    load  path2weights       path to PREFIX weights p file  path2data    data    path to data   Sampling gestures with many to many style transfers   sh python sample py    load  path2weights       path to PREFIX weights p file  sample all styles 20      if value   0  samples  value  number of intervals in all styles    number of speakers   path2data    data    path to data   Pre trained models  UPDATE   March 17  2021    Download pretrained models and unzip them in the  src  folder     sh cd mix stage src wget  O pretrained zip https   cmu box com shared static gw9i4qvj2vykcq3krkkvq6nickb4chem zip unzip pretrained zip   Once you unzip them  all the pretrained models can be found in the  save pretrained models   For the multi speaker scenario in Table 1 and part of Table 2 of the paper  look look for the weights in  save pretrained models multi speaker   For the attribute level training  look for the weights in  save pretrained models attribute     An example of sampling gesture animations from a pretrained model    sh python sample py    load save pretrained models multi speaker exp 3659 cpk JointLateClusterSoftStyle4 G speaker     corden       lec cosmic     model JointLateClusterSoftStyle4 G note s2g gst mixgan15 weights p    path2data    data   We also release a script to extract the reported results from the pretrained models in  eccv2020 results ipynb  which requires the latest version of  pycasper     Rendering   sh python render py    render 20      number of intervals to render  load  path2weights       path to PREFIX weights p file  render text 0    if 1  render text on the video as well   path2data    data    path to data   Experiment Files   Every experiment multiple files with the same PREFIX    Training files     PREFIX args args   arguments stored as a dictionary   PREFIX res json   results for every epoch   PREFIX weights p   weights of the best model   PREFIX log log   log file    PREFIX name name   name file to restore value of PREFIX     Inference files     PREFIX    directory containing sampled h5 files and eventually renders   PREFIX cummMetrics json   metrics extimated at inference   PREFIX metrics json   metrics estimated at inference for every style transfer separately   PREFIX style pkl   style space conditioned gesture regions to compute t SNE plots   PREFIX histogram json   Histogram of each generator in conditional Mix GAN giving an idea about which set of generators were important for which style      Inception Score for pose sequences   To measure inception scores for pose sequences  or gestures   we refer you to the class  InceptionScoreStyle   Other cool stuff   If you enjoyed this work  I would recommend the following projects which study different axes of nonverbal grounding     AISLe     Language2Pose     Dialogue2Pose   Issues   All research has a tag of work in progress  If you find any issues with this code  feel free to raise issues or pull requests  even better  and I will get to it as soon as humanly possible  AISLe   This is the official repository for the paper  No Gestures Left Behind  Learning Relationships between Spoken Language and Freeform Gestures     Chaitanya Ahuja   Dong Won Lee  Ryo Ishii   Louis Philippe Morency     EMNLP Findings 2020     Links   Paper    Dataset Website  1    Bibtex    sh  inproceedings ahuja2020no    title  No Gestures Left Behind  Learning Relationships between Spoken Language and Freeform Gestures     author  Ahuja  Chaitanya and Lee  Dong Won and Ishii  Ryo and Morency  Louis Philippe     booktitle  Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing  Findings     pages  1884  1895     year  2020      Overview     This repo has information on the training code and pre trained models     For the dataset  we refer you to     Dataset Website  for downloading the dataset    Dataset Repo  for scripts to download the audio files and other dataloader arguments     For the purposes of this repository  we assume that the dataset is downloaded to     data    This repo is divided into the following sections      Clone   Set up environment   Training   Inference   Rendering     This is followed by additional informational sections     Experiment Files   Clone   Clone only the master branch    sh git clone  b master   single branch https   github com chahuja mix stage git   Set up Environment     pycasper     sh mkdir    pycasper git clone https   github com chahuja pycasper    pycasper ln  s    pycasper pycasper       Create an  anaconda  or a virtual enviroment and activate it     sh pip install  r requirements txt   Training   To train a model from scratch  run the following script after chaging directory to  src     sh python train py     weighted 400      argument to run AISLe for adaptive reweighting  to be used with  gan 1   the number refers to the number of iterations per epoch   cpk JointLateClusterSoftTransformer12 G      checkpoint name which is a part of experiment file PREFIX   exp 1      creates a unique experiment number   path2data    data    path to data files   speaker    oliver         Speaker   model JointLateClusterSoftTransformer12 G      Name of the model   note aisle      unique identifier for the model to group results   save dir save aisle      save directory   modalities    pose normalize    text tokens    audio log mel 400         all modalities as a list  output modality first  then input modalities   repeat text 0      tokens are not repeated to match the audio frame rate   fs new  15       frame rate of each modality   input modalities    text tokens    audio log mel 400         List of input modalities   output modalities    pose normalize         List of output modalities   gan 1      Flag to train with a discriminator on the output   loss L1Loss      Choice of loss function  Any loss function torch nn   will work here   window hop 5      Hop size of the window for the dataloader   render 0      flag to render  Default 0   batch size 32      batch size   num epochs 100      total number of epochs   min epochs 50      early stopping can occur after these many epochs occur   overfit 0      flag to overfit  for debugging    early stopping 0      flag to perform early stopping    dev key dev spatialNorm      metric used to choose the best model   num clusters 8      number of clusters in the Conditional Mix GAN   feats    pose    velocity    speed         Festures used to make the clusters   optim AdamW      AdamW optimizer   lr 0 0001      Learning Rate   optim separate 0 00003      Use a separate recommended optimizer and learning rate schedule for the language encoder BERT   Example scripts for training models in the paper can be found as follows      Ours   Ours w o G attn   Ours w o AISLe   Gesticulator  2      Speech2Gesture  3      Inference   Inference for quantitative evaluation   sh python sample py    load  path2weights       path to PREFIX weights p file  path2data    data    path to data   Pre trained models  UPDATE   March 17  2021    Download pretrained models and unzip them in the  src  folder    sh cd aisle src wget  O pretrained zip part aa https   cmu box com shared static 4c2a7fax036sniupxajf7mt35osabrc7 part aa wget  O pretrained zip part ab https   cmu box com shared static lpnhd91xf228bx6wugf034a13cltv162 part ab cat pretrained zip part     pretrained zip unzip pretrained zip   Once you unzip the file  all the pretrained models can be found in the  save pretrained models      An example of sampling gesture animations from a pretrained model    sh python sample py    load save pretrained models aisle lec cosmic exp 3233 cpk mmsbert lfiw no update3 speaker     lec cosmic     model JointLateClusterSoftTransformer12 G note mmsbert lfiw no update3 weights p    path2data    data   We also release a script to extract the reported results from the pretrained models in  emnlp2020 results ipynb  which requires the latest version of  pycasper     Rendering   sh python render py    render 20      number of intervals to render  load  path2weights       path to PREFIX weights p file  render text 1    if 1  render text on the video as well   path2data    data    path to data   Experiment Files   Every experiment multiple files with the same PREFIX    Training files     PREFIX args args   arguments stored as a dictionary   PREFIX res json   results for every epoch   PREFIX weights p   weights of the best model   PREFIX log log   log file    PREFIX name name   name file to restore value of PREFIX     Inference files     PREFIX    directory containing sampled h5 files and eventually renders   PREFIX cummMetrics json   metrics estimated at inference which are reported in the paper     References   sh  1    Ahuja  Chaitanya et al   Style Transfer for Co Speech Gesture Animation  A Multi Speaker Conditional Mixture Approach  ECCV 2020   2    Kucherenko  Taras  et al   Gesticulator  A framework for semantically aware speech driven gesture generation   ICMI 2020   3    Ginosar  Shiry  et al   Learning individual styles of conversational gesture   CVPR 2019    Other cool stuff   If you enjoyed this work  I would recommend the following projects which study different axes of nonverbal grounding     Mix StAGe     Language2Pose     Dialogue2Pose   Issues   All research has a tag of work in progress  If you find any issues with this code  feel free to raise issues or pull requests  even better  and I will get to it as soon as humanly possible
45,meetps,ISK   Industrial Stock Keeper     Minimalist Stock Keeper and Reminder Program written as a project for course CS 101    My first proper project  Very close to my heart    Very shitty and noob ly written code     Custom Icon Size Unity   This scripyt allows you to change the icon size on Ubuntu Unity systems  Vishynoid   This repository contains the source of our ITSP 2014  FaceBook Utilities   GraphAPI based Python scripts EE717   Assignments Solutions of Course EE717 Advanced Computing for Electrical Engineers CS228   My attempts at Prolog for Course Logic for Computer Science GMap Fest Scheduler   This a general Google Map based scheduling framework that can be incoporated into any event or college fest Android app  TechFest2k15   Android Application Source for TechFest 2015          Application Features   Map Navigation   View value your time and in a campus as big as 550 acres its hard to find your way to the exact event you don t want to miss  Hence we have a Google Map integration in the app with markers for all genres of events  utilities and much more which are dynamically updated    Event Notifications   We know how important it is to be at the right time at a must attend event   thus we have a remind feature for you   lest you miss it    Google Drive Schedule Support   No more app updates and no more large data consumption  Our Schedule is updated in real time on the app start and cached for you    Get Directions   Every event page has a Get Directions button so no more nano searching maps to find your event path    Open Source   We Share love   We share Code    Cache Optimisation   The Google Maps and App essentials are stored in the cache for you to cherish later    Have a Nice Visit      TF  MIT OCW Downloader   Download all video lectures from a MIT OCW course with a single command    Features     Intentionally detailed names  so that it will display and sort properly     on most interfaces  e g   MX Video  or VLC on Android devices     URL and lecture name filters to download only video files    Tested on Linux  and Mac  Python 2 7       Instructions     mit ocw dl  basically needs a link to the video lectures page of a MIT OCW course      which of course has video lectures  and a computer that runs python    Put the  download py  file in the location you want the videos to be downloaded    Get the  link  to the video lectures page of a MIT OCW course    Open up a shell wizard and navigate to the  location     Type  python download py link   Enjoy       Coming Up     mit ocw dl  only supports video content of the course presently   but attempts are on to make it download entire contents  lecture slides  assignments  solutions etc     Bugs Reporting and Contribution   Shoot a mail at meetshah1995 gmail com   Author   Meet Shah   Inspiration   coursera dl coursera CS416   Assignment Solutions for Course Computer Network and Security  CS 213   Assignment Solutions for Course Data Structures and Algorithms   EE 214   VHDL and Verilog Codes for Digital Lab  asc visualizer                           GUI based python scripts to scrape and visualize essential course statistics from ASC    Resources and Links for the Project       http   www tutorialspoint com python python gui programming htm   https   github com ropensci plotly     Keep adding to the database scilab sound   Sound Generation scripts written as a part of EE 210 Signals and Systems Course statistical computing interface   R based Statistical Computing Interface for easy statistical analysis and data visualization   A prototype application written in Django including basic features of displaying graphs from uploaded file data   OpenCPU R Computations etc    Features       User Registration system    Secure User Login System to prevent csrf attacks     Bar Graph output of uploaded data file   csv for now other formats support soon      Line Graph output of uploaded data file  csv for now other formats support soon     Mean of columns in the uploaded file displayed using R requests to OpenCPU Servers   Linear Regression coming soon       Installation     Clone this repository using  git clone  Repo link here    Set up a virual environment in the same directory using  virtualenv  environment name    Activate your virtualenv using  source   environment name  bin activate   Change your directory to statistical computing interface    Install all dependencies using  pip install  r dependencies txt   Collect the static files using  python manage py collectstatic   Start the Django server  python manage py runserver     Open up your browser and open up  127 0 0 1 8080 sciApp register   to see the application live      Why Django       Thousands of useful scientific and statistical python libraries can be harnessed which is not possible easily in PHP  Node js     Django is utilized by a lot of modern organizations as it provides a lot of scalibity features such as models   templateFillers etc which makes the interface future proof     No vulnerability to CSRF attacks    Code is in Python hence lesser lines of code    Statistical analysis is in pure python   hence future contribution from python community can be easy to get    sudoku solver   Sudoku Solver written in lunch break   Code Humor   Humor in the form of code   For the Geeks   By the Geeks   Characters Covered     Rahul Gandhi   Narendra Modi     Upcoming     Sadhu Yoginath   Smriti Irani   Arnub Goswami     Contributing   I would be glad to have your humor onboard too   EE 337   Assembly Codes for Microprocessors Laboratory CS 663 Digital Image Processing   Assignments   Assignment 1     Image Resizing  Interpolation   Contrast Stretching  Contrast Limited Adaptive  Adaptive and Normal Histrogram Equlization     Assignment 2     Image Sharpening   Bilateral Filtering   Patch Based Filtering     Assignment 3     Frequency Domain Image Filtering   Butterworth Filters   Harris Corner Detection   Mean Shift Image Segmentation     Assignment 4     Custom Singular Value Decomposition   Face Image Features vs Recognition Rate   Face Image reconstruction from Eigenspace   Untrained Reconstruction Rates     Assignment 5     PCA Based Image Denoising   Deblurring and Reflection Removal   Single Dimensional Image Retrieval   Eigenvectors abd Eigenvalues of Matrix Proofs and Problems     Authors and Contributors      meetshah1995    yashbhalgat   EE 340   Laboratory Files for course EE 340 Communications Lab  Convolutional Neural Networks for Object Recognition   This is a MATLAB implementation of a convolutional neural network to classify images from standard datasets done as a part of our course project for CS663 Digital Image Processing    Data Sets Used       MNIST  Hand written digits data  as available  here     STL 10  10 Class Categorized Objects  as available   here     CIFAR 10  10 Class Categorized Objects  as available   here     Note that the data set and its preprocessed variant is included in the  data  directory of the repository    Steps to Train and Test the CNN     Clone the Repository    Run the  cnntrain m  file with input  1  2  3  for different datasets    To visualize the Weight Filters and Pool Activation Features enable the  visualize  option      Team     Meet Shah   Yash Bhalgat    UFLDL solutions   Programming Exercises Solutions for the Stanford Unsupervised Feature Learning and Deep Learning Tutorial  EE 324   Solution codes and simulation files for EE324 Control Lab   ME 766   Assignment Solutions to course ME766 High Performance Scientific Computing    EE 352   C codes and relevant material for Digital Signal Processing Laboratory  EE 352   Authors  Navjot Singh  Meet Shah CS 736 Algorithms for Medical Image Processing   Assignments   Assignment 1     X Ray Computed Tomography   Radon Transform    X Ray Computed Tomography   Filtered Backprojection   X Ray Computed Tomography   Incomplete Data     Assignment 2     Diffusion Tensor Magnetic Resonance Imaging   Denoising a Phantom Magnetic Resonance Image using MAP algorithm with MRF based priors   Denoising a Magnetic Resonance Image of the Brain using MAP algorithm with MRF based priors     Assignment 3     Reconstructing a Phantom Magnetic Resonance Image using MAP Bayesian model and dynamic stepped optimization   Reconstructing a Magnetic Resonance Image of the Brain using multiple Markov Random fields in f domain     Assignment 4     Segmenting a Brain Magnetic Resonance  MR  Image using modified fuzzy c means  FCM    Segmenting a Brain Magnetic Resonance  MR  Image using EM  expectation maximization  with GMM for intensity and MRF model for labels     Authors and Contributors      meetshah1995    BijoySingh   EE 702 Computer Vision   Mini Projects     Shape from Shading   Multi Occular Stereo Vision      Assignments     Paper Review   Computing the Stereo Matching cost with Convolutional Neural Networks   Problem Setup   Video Frame Completion using Global Temporal Gradient Minimization     Authors and Contributors      meetshah1995    yashbhalgat   Filter Design Assignment   Personal Details     Name   Meet Pragnesh Shah   Roll Number   13D070003   Filter Number   82   m   7   q   0   r   7      Filter Specifications   Filter 1     Filter type  Band Pass    Passband tolerance   0 15  in magnitude     Stopband tolerance   0 15  in magnitude     Transition band   2 KHz on either side of band    Pass band type   equiripple    Stop band type   monotonic    Sampling frequency   100 kHz    Signal Bandlimit   45 kHz   Passband low limit  B l   18 kHz    Passband high limit  B h   28 kHz      Filter 2     Filter type  Band Stop    Passband tolerance   0 15  in magnitude     Stopband tolerance   0 15  in magnitude     Transition band   2 KHz on either side of band    Pass band type   monotonic    Stop band type   monotonic    Sampling frequency   100 kHz    Signal Bandlimit   45 kHz    Passband high limit  B h   18 kHz    Passband low limit  B l   28 kHz      Report made with Madako  RISC V           RISC V implementation and tools    Available tools       Pure Python RISC V 2 0 decoder    myHDL based decoder module     In Progress       RISC V core supporting RV32I CS 747 Intelligent and Learning Agents   Assignments   Assignment 1     Multi arm Bandits problem using Thompson Sampling     Assignment 2     Markov Decision Process Planning   Value Iteration   Policy Iteration     Assignment 3     SARSA Lambda    Trace accumulation   Trace replacement   EE 779 Advanced Topics in Signal Processing   Assignments Solutions   Assignment 1     Power Spectral Estimation using    Periodogram    Welch    Blackman Tukey     Assignment 2     Power Spectral Estimation using    AR model using autocorrelation    AR model using covariance    MUSIC    Minimum Norm      Assignment 3     Direction of Arrival estimation using    Array processing   Beamforming   Capon s Beamforming   Root MUSIC   ESPIRIT     Assignment 4     Least squares estimate using    SVD   Truncated SVD   Tikhonov Regularization     Assignment 5     PCA based face recognition   tf devanagri   Convolutional Neural Network to identify Devanagri characters implemented in Tensorflow    Data     n classes   104   Size   320x320   Format   Binary  PNG     Results and Preprocessing     94 6     0 07   accuracy on test data    30 epochs in 30 mins of training time on a 4 GB NVIDIA GT920M GPU and i7 CPU    10x downsampling to get 32x32 images    3 iterations of binary dilation to make characters thicker      Network   Architecture     Conv2d 7x7 1   Conv2d 5x5 32   Conv2d 5x5 64   Conv2d 3x3 128   Conv2d 3x3 256   Dense  256x2x2   Dense  1024   Softmax 104    n classes     HyperParameters     Learning Rate   0 003   Momentum   0 9   crnn music genre classification     Music Genre Classification using Convolutional Recurrent Neural Networks   Data     Download GENRE data from  Marsyas   Extract the tarball in the directory which contains the cloned repo      Running the code   CNN     python cnn py     CRNN     python crnn py   CS749 DGP tf 3dgan       Tensorflow implementation of 3D Generative Adversarial Network    This is a tensorflow implementation of the paper  Learning a Probabilistic Latent Space of Object Shapes  via 3D Generative Adversarial Modeling      Blog Post with interactive volume plots   Requirements     tensorflow  1 0   visdom  1 0 1  for mesh visualization    scipy   scikit image   stl  optional      One line installation   pip install scipy scikit image stl visdom   Data     Download the training data from the 3D Shapenet  website   Extract the zip and modify the path appropriately in  dataIO py     Usage   Launch  visdom  by running   python  m visdom server   To train the model  visdom will show generated chairs after every 200 minibatches    python 3dgan mit biasfree py 0  path to model checkpoint    To generate chairs   python 3dgan mit biasfree py 1  path to trained model    Some sample generated chairs                                                                                                                                                               Source code files     File        Description                                                                                                                                                                    3dgan mit biasfree py        3dgan as mentioned in the paper  with same hyperparams    3dgan py                     baseline 3dgan with fully connected layer at end of discriminator   3dgan mit py                 3dgan as mentioned in the paper with bias in convolutional layers   3dgan autoencoder py         3dgan with support for autoencoder based pre training   3dgan feature matching py    3dgan with additional loss of feature mathcing of last layers    dataIO py                    data input output and plotting utilities   utils py                     tensorflow utils like leaky relu and batch norm layer    Todo     Host the trained models   Add argparser based interface   Add threaded dataloader   Release the pytorch and keras versions of the GAN    Train for longer number of epochs to improve quality of generated chairs      Contributors      meetshah1995    khushhallchandra   madoko docs   Madoko Docs pytorch semseg         Semantic Segmentation Algorithms Implemented in PyTorch   This repository aims at mirroring popular semantic segmentation architectures in PyTorch             Networks implemented     PSPNet    With support for loading pretrained models w o caffe dependency   ICNet    With optional batchnorm and pretrained models   FRRN    Model A and B   FCN    All 1  FCN32s   2  FCN16s  and 3  FCN8s  stream variants   U Net    With optional deconvolution and batchnorm   Link Net    With multiple resnet backends   Segnet    With Unpooling using Maxpool indices     Upcoming     E Net   RefineNet     DataLoaders implemented     CamVid   Pascal VOC   ADE20K   MIT Scene Parsing Benchmark   Cityscapes   NYUDv2   Sun RGBD     Requirements     pytorch   0 4 0   torchvision   0 2 0   scipy   tqdm   tensorboardX     One line installation   pip install  r requirements txt   Data     Download data for desired dataset s  from list of URLs  here     Extract the zip   tar and modify the path appropriately in your  config yaml     Usage   Setup config file      yaml   Model Configuration   model      arch     options   fcn 8 16 32 s  unet  segnet  pspnet  icnet  icnetBN  linknet  frrn A B            Data Configuration   data      dataset     options   pascal  camvid  ade20k  mit sceneparsing benchmark  cityscapes  nyuv2  sunrgbd  vistas        train split        val split        img rows  512     img cols  1024     path         Training Configuration   training      n workers  64     train iters  35000     batch size  16     val interval  500     print interval  25     loss          name     options   cross entropy  bootstrapped cross entropy  multi scale crossentropy                  Optmizer Configuration optimizer      name   optimizer name   options   sgd  adam  adamax  asgd  adadelta  adagrad  rmsprop       lr  1 0e 3      optimizer keyarg1   value         Warmup LR Configuration     warmup iters   iters for lr warmup      mode    constant  or  linear  for warmup       gamma   gamma for warm up     Augmentations Configuration augmentations      gamma  x                                       gamma varied in 1 to 1 x      hue  x                                         hue varied in  x to x      brightness  x                                  brightness varied in 1 x to 1 x      saturation  x                                  saturation varied in 1 x to 1 x      contrast  x                                    contrast varied in 1 x to 1 x      rcrop   h  w                                   crop of size  h w       translate   dh  dw                             reflective translation by  dh  dw       rotate  d                                      rotate  d to d degrees      scale   h w                                    scale to size  h w       ccrop   h w                                    center crop of  h w       hflip  p                                       flip horizontally with chance p      vflip  p                                       flip vertically with chance p     LR Schedule Configuration lr schedule      name   schedule type   options   constant lr  poly lr  multi step  cosine annealing  exp lr        scheduler keyarg1   value     Resume from checkpoint   resume   path to checkpoint           To train the model         python train py   h     config  CONFIG        config                Configuration file to use       To validate the model         usage  validate py   h     config  CONFIG      model path  MODEL PATH                             eval flip     measure time      config              Config file to be used     model path          Path to the saved model     eval flip           Enable evaluation with flipped image   True by default     measure time        Enable evaluation with time  fps  measurement   True                         by default       To test the model w r t  a dataset on custom images s         python test py   h     model path  MODEL PATH      dataset  DATASET                     dcrf  DCRF      img path  IMG PATH      out path  OUT PATH       model path          Path to the saved model     dataset             Dataset to use   pascal  camvid  ade20k etc       dcrf                Enable DenseCRF based post processing     img path            Path of the input image     out path            Path of the output segmap       If you find this code useful in your research  please consider citing     article mshahsemseg      Author    Meet P Shah       Title    Semantic Segmentation Architectures Implemented in PyTorch        Journal    https   github com meetshah1995 pytorch semseg       Year    2017    Gpu cLUST setUP   Tools and Instruction Sets for TwinTitanX GPU cluster setup     Major Python Libraries     tensorflow  1 3 0   torch  0 2 0   numpy  1 13 1   scipy  0 19 1   keras  2 0 8   matplotlib  2 0 2   seaborn  0 8   scikit learn  0 19 0   pandas  0 20 3      Drivers and GPU Toolkits     NVIDIA Driver Version 375 36   CUDA Version 8 0   Cudnn Version 6 0      Monitoring Utilities     Ganglia   Ganglia NVIDIA Bindings   gmond python modules   Setup Tutorial         Miscellaneous     Session Management   tmux   screen       Remote Editing       rmate         Upgrade   New Module request   Open up a issue in this repo  stating why you need a update   new module  Usage   Installation   Installation on Laptop Workstation   curl  L https   github com meetps config raw master install sh   zsh  s laptop   Installation on Server   curl  L https   github com meetps config raw master install sh   zsh  s server   Updation   sudo   update sh    module name  dockerfiles   My public Dockerfiles
46,szagoruyko,libclsvm loadcaffe   Load Caffe networks in  Torch7  http   torch ch   Install torch first   There is no Caffe dependency  only protobuf has to be installed  In Ubuntu do    sudo apt get install libprotobuf dev protobuf compiler   In OS X    brew install protobuf   Then install the package itself    luarocks install loadcaffe   In Ubuntu 16 04 you need to use gcc 5   CC gcc 5 CXX g   5 luarocks install loadcaffe   Load a network       lua require  loadcaffe    model   loadcaffe load  deploy prototxt    bvlc alexnet caffemodel    ccn2         Models from Caffe  Model Zoo       Network    ccn2   nn   cudnn                                                             bvlc alexnet                 bvlc reference caffenet                 bvlc reference rcnn ilsvrc13                  finetune flickr style                   VGG CNN S                     VGG CNN M                     VGG CNN M 2048                     VGG CNN M 1024                     VGG CNN M 128                     VGG CNN F                     VGG ILSVRC 2014 16 layer                   VGG ILSVRC 2014 19 layer                   Network in Network Imagenet                   Network in Network CIFAR 10                   VGG16 SalObjSub                   AlexNex SalObjSub                    Binary Hash Codes                   Oxford 102 Flowers                   Age Gender                  MNIST LeNet                 Loading googlenet is supported by https   github com soumith inception torch For other models with non sequential structure check https   github com nhynes caffegraph   NN support means both CPU and GPU backends    You can also use Caffe inside Torch with this  https   github com szagoruyko torch caffe binding However you can t use both loadcaffe and caffe in one torch session    An example of using the package is in  examples mnist lenet lua   After running script to train lenet model in Caffe you can easily load and test it in Torch7 on CPU and GPU  with  cuda  as a first arguments    Some of ImageNet networks are validated to give reported accuracy in torch in https   github com szagoruyko imagenet validation torch   Rights to caffe proto belong to the University of California  imagine nn   Universite Paris Est Marne la Vallee IMAGINE LIGM torch neural network routines   Following modules are here for now    lua inn SpatialStochasticPooling kW kH dW dH  inn SpatialSameResponseNormalization  size   3    alpha   0 00005    beta   0 75   inn MeanSubtraction mean  inn SpatialPyramidPooling   w1 h1   w2 h2       wn hn    inn ROIPooling W H  setSpatialScale scale    Look at http   arxiv org abs 1301 3557 for  inn SpatialStochasticPooling  reference  this is fully working implementation    inn ROIPooling  is Spatial Adaptive Max Pooling layer for region proposals used in FastRCNN with bugfixes and 50 times faster in backprop  Set v2   false to use it s old version   inn ROIPooling  expects a table on input  first argument is features in NxDxHxW where N is number of images  second argument is bounding boxes in Bx5 where B is the number of regions to pool and 5 is image id   bbox  Image id is in  1 N  range  boxes are in  x1 y1 x2 y2     inn SpatialSameResponseNormalization  is a local response normalization in the same map in BDHW format  For details refer to https   code google com p cuda convnet wiki LayerParams Local response normalization layer  same map    inn MeanSubtraction mean   is done to subtract the Imagenet mean directly on GPU  Mean tensor is expanded to BDHW batches without using additional memory    inn SpatialPyramidPooling   w1 h1   w2 h2       wn hn     is a pyramid of regions obtained by using Spatial Adaptive Max Pooling with parameters   w1 h1       wn hn   in the input  The result is a fixed sized vector of size  w1 h1    wn hn  for any input dimension  For details see http   arxiv org abs 1406 4729   OBSOLETE modules   The difference with  inn SpatialMax Average Pooling  and  nn SpatialMax Average Pooling  is that output size computed with ceil instead of floor  as in Caffe and cuda convnet2   Also SpatialAveragePooling does true average pooling  meaning that it divides outputs by kW kH  inn SpatialMax Average Pooling kW kH dW dH  is equal to cudnn SpatialMax Average Pooling kW kH dW dH  ceil      inn SpatialCrossResponseNormalization  is local response normalization across maps in BDHW format  thanks to Caffe    For details refer to https   code google com p cuda convnet wiki LayerParams Local response normalization layer  across maps    inn SpatialMaxPooling kW kH dW dH     OBSOLETE  USE nn SpatialMaxPooling kW kH dW dH padW padH  ceil   inn SpatialAveragePooling kW kH dW dH     OBSOLETE  USE nn SpatialAveragePooling kW kH dW dH padW padH  ceil   inn SpatialCrossResponseNormalization size   alpha   0 0001    beta   0 75    k   1      OBSOLETE  USE nn SpatialCrossMapLRN with the same arguments torch caffe binding   A short binding to use Caffe as a module in Torch7  Has the same functionality as MATLAB bindings    You have to have installed and built Caffe  then do this    bash CAFFE DIR   path to caffe root   luarocks install caffe   Forward and backward are supported       lua require  caffe    net   caffe Net  deploy prototxt    bvlc alexnet caffemodel    test   input   torch FloatTensor 10 3 227 227  output   net forward input    gradOutput   torch FloatTensor 10 1000 1 1  gradInput   net backward input  gradOutput        Use can also use it inside a network as nn Module  for example       lua require  caffe    model   nn Sequential   model add caffe Net  deploy prototxt    bvlc alexnet caffemodel    test    model add nn Linear 1000 1         To load Caffe networks in Torch7 without having Caffe installed use this  https   github com szagoruyko loadcaffe InfiMnist torch   Torch7 ffi bindings for InfiMnist dataset  MNIST dataset with elastic deformations http   leon bottou org projects infimnist   To use it please download the archive from the link above and unpack somewhere  Then in torch do   lua require  infimnist  provider   infimnist InfiMnist  data      put your path to data folder here sample  label   provider getSample 34    Samples from 0 to 9999 are test digits  from 10000 to 69999 are train and above   elastic deformations  So to generate a deformed sample do   lua train sample  train label   provider getSample math random 70000 1e 9   cutorch rtc   Basic feature list      cutorch launchPTX function   apply kernels from cutorch     This package brings CUDA 7 runtime compilation to Torch  Linux or OS X with C  11 compiler required  Installation   luarocks install https   raw githubusercontent com szagoruyko cutorch rtc master cutorch rtc scm 1 rockspec  Then after requiring  cutorch rtc  you will get  launchPTX  function  which can run ptx code generated with NVRTC  and  cutorch apply  functions   lua require  cutorch rtc  t   torch randn 8  cuda   t apply1 x   x   0   0   x   That would be a simple ReLU implementation    Documentation   cutorch launchPTX   Runs compiled PTX   lua function cutorch launchPTX ptx  kernel name  arguments  gridDim  blockDim   Arguments     ptx   compiled PTX lua string    kernel name   name of kernel to run from the given PTX    arguments   lua table with CudaTensors as inputs and subtables in the form   int   n  to provide scalar arguments    gridDim   size of the grid table  has to have at least one value  others will be filled with ones    blockDim   size of block table  again has to have at least one value  others will be ones   PTX can be generated in runtime with https   github com szagoruyko nvrtc torch   Short example       lua local kernel      extern  C   global  void kernel float  a  int n      int tx   blockIdx x blockDim x   threadIdx x    if tx   n    a tx     2 f         local ptx   nvrtc compileReturnPTX kernel  local a   torch randn 32  cuda   local b   a clone   cutorch launchPTX ptx   kernel    a    int   a numel       1    32         apply1   Applies provided operator to a tensor   lua function CudaTensor apply1 self  op   op has to be a lua string assigning a value to variable  x   CUDA built in  device  functions can be used  see CUDA documentation for more information  Multiline ops supported  has to be separated with   Both contiguous and non contiguous tensors are valid  First call to any apply operation takes about 0 5s  then the compiled code is cached and other calls are fast    apply2   Applies provided operator using two tensors   lua function CudaTensor apply2 self  a  op   op has to use  x  and  y    self and a tensors  Can assign values to both tensors  See apply1 for properties    apply3   Applies provided operator using three tensors   lua function CudaTensor apply3 self  a  b  op   op has to use  x    y  and  z    self  a and b tensors  Can assign values to all three tensors  See apply1 for properties  nvrtc torch   Torch7 bindings for CUDA NVRTC  runtime compilation  library http   docs nvidia com cuda nvrtc index html   Requires CUDA 7 installation    Example of usage   lua local nvrtc   require  nvrtc  local ptx   nvrtc compileReturnPTX kernel   where kernel is a lua string with CUDA kernel code  Returned PTX can be loaded and ran with CUDA Driver API  More examples coming  cunn rtc   Runtime compiled Torch cunn modules Code for CVPR15 paper  Learning to Compare Image Patches via Convolutional Neural Networks    This package allows researches to apply the described networks to match image patches and extract corresponding patches    We tried to make the code as easy to use as possible  The original models were trained with Torch   http   torch ch   and we release them in Torch7 and binary formats with C   bindings which do not require Torch installation  Thus we provide example code how to use the models in Torch  MATLAB and with OpenCV http   opencv org   CREDITS  LICENSE  CITATION   Copyright   2015 Ecole des Ponts  Universite Paris Est   All Rights Reserved  A license to use and copy this software and its documentation solely for your internal research and evaluation  purposes  without fee and without a signed licensing agreement  is hereby granted upon your download of the software  through which you agree to the following  1   the above copyright notice  this paragraph and the following three paragraphs will prominently appear in all internal copies and modifications  2   no rights to sublicense or further distribute this software are granted  3  no rights to modify this software are granted  and 4  no rights to assign this license are granted    Please Contact Prof  Nikos Komodakis  6 Avenue Blaise Pascal   Cite Descartes  Champs sur Marne  77455 Marne la Vallee cedex 2  France for commercial licensing opportunities  or for further distribution  modification or license rights    Created by Sergey Zagoruyko and Nikos Komodakis  http   imagine enpc fr  komodakn    Please cite the paper below if you use this code in your research    Sergey Zagoruyko  Nikos Komodakis   Learning to Compare Image Patches via Convolutional Neural Networks   http   www cv foundation org openaccess content cvpr 2015 papers Zagoruyko Learning to Compare 2015 CVPR paper pdf  bib     InProceedings Zagoruyko 2015 CVPR      author    Zagoruyko  Sergey and Komodakis  Nikos       title    Learning to Compare Image Patches via Convolutional Neural Networks       booktitle    The IEEE Conference on Computer Vision and Pattern Recognition  CVPR        month    June       year    2015      Update 4   April 2017  dead links for models and datasets fixed   Update 3   July 2016  training code released   Update 2   February 2016  caffe models released   Update 1   January 2016  cudnn models removed because  cudnn convert  was out   Dataset   The original dataset website is down  you can still download the files here    http   icvl ee ic ac uk vbalnt notredame zip   http   icvl ee ic ac uk vbalnt yosemite zip   http   icvl ee ic ac uk vbalnt liberty zip   Models   We provide the models in Torch7 and binary format  The table from the paper is here for convenience    All models expect input patches to be in  0 1  range before mean subtraction    The models are not supposed to give outputs in  0 1  range  the outputs are not normalized     Train set   Test set   2ch   2ch2stream   2chdeep   siam   siam2stream                                                                yosemite   notredame   2 74    2 11    2 43   5 62   5 23     yosemite   liberty   8 59    7 2    7 4   13 48   11 34     notredame   yosemite   6 04    4 09    4 38   13 23   10 44     notredame   liberty   6 04   4 85    4 56    8 77   6 45     liberty   yosemite   7    5    6 18   14 76   9 39     liberty   notredame   2 76    1 9    2 77   4 04   2 82     Models in nn format can be loaded and used without CUDA support in Torch  To enable CUDA support  model cuda    call required    An archive with all models  binary and in torch format  is available at   https   s3 amazonaws com modelzoo networks cvpr2015matching networks tar gz   Torch   To install torch follow http   torch ch  Check torch folder for examples  Match patches on CPU       lua require  nn    N   76     the number of patches to match patches   torch rand N 2 64 64  float        load the network net   torch load    networks 2ch 2ch liberty t7       in place mean subtraction local p   patches view N 2 64 64  p add  p mean 3  expandAs p        get the output similarities output   net forward patches        Conversion to a faster  cudnn  backend is done by  cudnn convert net  cudnn   function    C   API   The code was tested to work in Linux  Ubuntu 14 04  and OS X 10 10  although we release all the source code to enable usage in other operating systems    We release CUDA code for now  CPU code might be added in the future  To install it you need to have CUDA with the up to date CUDA driver  those are separate packages     Install TH and THC    cd  tmp git clone https   github com torch torch7 git cd torch7 lib TH mkdir build  cd build cmake     make  j4 install cd  tmp git clone https   github com torch cutorch git  cd cutorch lib THC mkdir build  cd build cmake     make  j4 install   Clone and compile this repository it with    git clone   recursive https   github com szagoruyko cvpr15deepmatch cd cvpr15deepmatch mkdir build  cd build  cmake     DCMAKE INSTALL PREFIX    install make  j4 install   Then you will have  loadNetwork  function defined in src loader h  which expects the state and the path to a network in binary format on input  A simple example       c   THCState  state    THCState  malloc sizeof THCState    THCudaInit state     cunn  Sequential  Ptr net   loadNetwork state   networks siam siam notredame bin      THCudaTensor  input   THCudaTensor newWithSize4d state  128  2  64  64   THCudaTensor  output   net  forward input      output is 128x1 similarity score tensor       Only 2D and 4D tensors accepted on input    Again   all binary models expect input patches to be in  0 1  range before mean subtraction    After you build everything and download the networks run test with  run test sh   It will download a small test data bin file    MATLAB   Building Matlab bindings requires a little bit of user intervention  Open matlab make m file in Matlab and put your paths to Matlab and include lib paths of TH and THC  then run     make   Mex file will be created    To initialize the interface do   deepcompare  init    networks 2ch 2ch notredame bin     To reset do   deepcompare  reset     To propagate through the network    deepcompare  forward   A    A  can be 2D  3D or 4D array  which is converted inside to 2D or 4D array  Matlab is col major and Torch is row major so the array is transposed        dim   matlab dim   torch dim                      2d   N x B   B x N     3d   64 x 64 x N   1 x N x 64 x 64     4d   64 x 64 x N x B   B x N x 64 x 64     2D or 4D tensor is returned  In case of full network propagation for example the output will be 2D  1 x B  if input was B x 2 x 64 x 64    To set the number of GPU to be used  the numbering starts from 1     deepcompare  set device   2   Print the network structure    deepcompare  print     To enable splitting the computations of descriptor and decision parts in siamese networks we saved their binary parts      siamese network        Train Set   siam desc   siam decision                              yosemite   3 47 MB siam desc yosemite bin   1 00 MB siam decision yosemite bin     notredame   3 47 MB siam desc notredame bin   1 0MB siam decision notredame bin     liberty   3 47 MB siam desc liberty bin   1 00 MB siam decision liberty bin       siam 2stream network        Train Set   siam2stream desc   siam2stream decision                               yosemite   9 16 MB siam2stream desc yosemite bin   4 01 MB siam2stream decision yosemite bin     notredame   9 16 MB siam2stream desc notredame bin   4 01 MB siam2stream decision notredame bin     liberty   9 16 MB siam2stream desc liberty bin   4 01 MB siam2stream decision liberty bin     OpenCV   OpenCV example is here to demonstrate how to use the deep CNN models to match image patches  how to preprocess the patches and use the proposed API    Depends on OpenCV 3 0  To build the example do   cd build  cmake  DWITH OPENCV ON  DOpenCV DIR  opt opencv    make  j8  Here   opt opencv  has to be a folder where OpenCV is built  If you have it installed  you don t need to add it    DWITH OPENCV ON  will be enough    To run the example download the images    wget https   raw githubusercontent com openMVG ImageDataset SceauxCastle master images 100 7100 JPG wget https   raw githubusercontent com openMVG ImageDataset SceauxCastle master images 100 7101 JPG   and run it      build opencv example networks siam2stream siam2stream desc notredame bin 100 7100 JPG 100 7101 JPG   You have to use descriptor matching network in this example  Check the example code for explanation    CAFFE   Thanks to the awesome  ajtulloch s  torch2caffe  models were converted to CAFFE format  Unfortunatelly only  siam    2ch  and  2chdeep  models could be converted at the time  other models will be converted as missing functionality is added to CAFFE    Download link    https   s3 amazonaws com modelzoo networks cvpr2015networks caffe tar CUNN PRODUCTION   A simple CUDA C   wrapper over some cunn modules to enable easy embedding of Torch7 networks in C   projects  Only depends on TH and THC    The following modules supported for now   nn Sequential nn Concat nn Parallel nn SpatialConvolutionMM nn SpatialMaxPooling nn SpatialAveragePooling nn ReLU nn Linear nn SoftMax   I tried to follow Torch7 nn architecture  There is an abstract cunn  Module with forward   function and all the modules are inherited from it  so syntax has to be familiar to torch users       c   auto net   std  make shared     net  add std  make shared    net  add std  make shared  state    net  add std  make shared  state  2 2 2 2    net  add std  make shared  state 96 192 5 5    net  add std  make shared  state      THCudaTensor  input   THCudaTensor newWithSize4d state  1 3 224 224  THCudaTensor  output   net  forward input       clipp torch   Torch interface to OpenCLIPP   WORK IN PROGRESS     The goal of this package is bring to Torch  OpenCLIPP  image processing library with capabilities of running on CPU and GPU    Accepted formats      HW   HW1   HW4     Accepted tensors      torch CharTensor   torch ByteTensor   torch ShortTensor   torch IntTensor   torch FloatTensor   torch DoubleTensor     Supported transformations   image scale   THE IMPLEMENTATION IS DIFFERENT FROM image scale   in the following modes      bilinear   simple   cubic   lanczos2   lanczos3   supersampling   best     Supported filters       c       Gaussian blur filter   with sigma parameter       param Sigma   Intensity of the filer   Allowed values   0 01 10 ocipError ocip API ocipGaussianBlur ocipImage Source  ocipImage Dest  float Sigma         Gaussian filter   with width parameter       param Width   Width of the filter box   Allowed values   3 or 5 ocipError ocip API ocipGauss      ocipImage Source  ocipImage Dest  int Width         Sharpen filter       param Width   Width of the filter box   Allowed values   3 ocipError ocip API ocipSharpen    ocipImage Source  ocipImage Dest  int Width         Smooth filter   or Box filter       param Width   Width of the filter box   Allowed values   Impair     3 ocipError ocip API ocipSmooth     ocipImage Source  ocipImage Dest  int Width         Median filter      param Width   Width of the filter box   Allowed values   3 or 5 ocipError ocip API ocipMedian     ocipImage Source  ocipImage Dest  int Width         Vertical Sobel filter      param Width   Width of the filter box   Allowed values   3 or 5 ocipError ocip API ocipSobelVert  ocipImage Source  ocipImage Dest  int Width         Horizontal Sobel filter      param Width   Width of the filter box   Allowed values   3 or 5 ocipError ocip API ocipSobelHoriz ocipImage Source  ocipImage Dest  int Width         Cross Sobel filter      param Width   Width of the filter box   Allowed values   3 or 5 ocipError ocip API ocipSobelCross ocipImage Source  ocipImage Dest  int Width         Combined Sobel filter     Does SobelVert   SobelHoriz and the combines the two with sqrt V V   H H       param Width   Width of the filter box   Allowed values   3 or 5 ocipError ocip API ocipSobel      ocipImage Source  ocipImage Dest  int Width         Vertical Prewitt filter      param Width   Width of the filter box   Allowed values   3 or 5 ocipError ocip API ocipPrewittVert ocipImage Source  ocipImage Dest  int Width         Horizontal Prewitt filter      param Width   Width of the filter box   Allowed values   3 or 5 ocipError ocip API ocipPrewittHoriz ocipImage Source  ocipImage Dest  int Width         Combined Prewitt filter     Does PrewittVert   PrewittHoriz and the combines the two with sqrt V V   H H       param Width   Width of the filter box   Allowed values   3 or 5 ocipError ocip API ocipPrewitt      ocipImage Source  ocipImage Dest  int Width         Vertical Scharr filter      param Width   Width of the filter box   Allowed values   3 or 5 ocipError ocip API ocipScharrVert    ocipImage Source  ocipImage Dest  int Width         Horizontal Scharr filter      param Width   Width of the filter box   Allowed values   3 or 5 ocipError ocip API ocipScharrHoriz   ocipImage Source  ocipImage Dest  int Width         Combined Scharr filter     Does ScharrVert   ScharrHoriz and the combines the two with sqrt V V   H H       param Width   Width of the filter box   Allowed values   3 or 5 ocipError ocip API ocipScharr        ocipImage Source  ocipImage Dest  int Width         Hipass filter      param Width   Width of the filter box   Allowed values   3 or 5 ocipError ocip API ocipHipass        ocipImage Source  ocipImage Dest  int Width         Laplace filter      param Width   Width of the filter box   Allowed values   3 or 5 ocipError ocip API ocipLaplace       ocipImage Source  ocipImage Dest  int Width           Table of supported functions     Component name Status Function   Misc Done  ocipInitialize ocipUninitialize ocipChangeContext ocipSetCLFilesPath ocipGetErrorName ocipGetDeviceName ocipFinish ocipCreateImage done ocipSendImage done ocipReadImage done ocipReleaseImage ocipReleaseProgram    Conversions  ocipConvert ocipScale ocipScale2 ocipCopy ocipToGray ocipSelectChannel ocipToColor    Arithmetics  ocipPrepareArithmetic done ocipAdd ocipAddSquare ocipSub ocipAbsDiff ocipMul ocipDiv ocipImgMin ocipImgMax ocipImgMean ocipCombine ocipAddC ocipSubC ocipAbsDiffC ocipMulC ocipDivC ocipRevDivC ocipMinC ocipMaxC ocipMeanC ocipAbs ocipInvert ocipSqr ocipExp ocipLog ocipSqrt ocipSin ocipCos    Logic  ocipAnd ocipOr ocipXor ocipAndC ocipOrC ocipXorC ocipNot    LUT  ocipLut ocipLutLinear ocipBasicLut ocipLutScale    Morphology  ocipErode ocipDilate ocipGradient ocipErode2 ocipDilate2 ocipOpen ocipClose ocipTopHat ocipBlackHat    Transformations Done  ocipMirrorX ocipMirrorY ocipFlip ocipTranspose ocipRotate ocipResize ocipShear ocipRemap ocipSet    Filters Done  ocipGaussianBlur ocipGauss ocipSharpen ocipSmooth ocipMedian ocipSobelVert ocipSobelHoriz ocipSobelCross ocipSobel ocipPrewittVert ocipPrewittHoriz ocipPrewitt ocipScharrVert ocipScharrHoriz ocipScharr ocipHipass ocipLaplace    Histogram  ocipHistogram 1C ocipHistogram 4C ocipOtsuThreshold    Statistics  ocipMin ocipMax ocipMinAbs ocipMaxAbs ocipSum ocipSumSqr ocipMean ocipMeanSqr ocipStdDev ocipMean StdDev ocipCountNonZero ocipMinIndx ocipMaxIndx ocipMinAbsIndx ocipMaxAbsIndx    Thresholding  ocipThreshold ocipThresholdGTLT ocipThreshold Img ocipCompare ocipCompareC    Blob  ocipComputeLabels ocipRenameLabels    FFT  ocipIsFFTAvailable ocipFFTForward ocipFFTInverse    Integral  ocipIntegral ocipSqrIntegral    Proximity  ocipSqrDistance Norm ocipSqrDistance ocipAbsDistance ocipCrossCorr ocipCrossCorr Norm    ImageProximityFFT  ocipSqrDistanceFFT ocipSqrDistanceFFT Norm ocipCrossCorrFFT ocipCrossCorrFFT Norm    cifar torch   Newer version of this code is included in https   github com szagoruyko wide residual networks   The code achieves 92 45  accuracy on CIFAR 10 just with horizontal reflections    Corresponding blog post  http   torch ch blog 2015 07 30 cifar html   Accuracies      No flips   Flips                 VGG BN Dropout   91 3    92 45  NIN BN Dropout   90 4    91 9    Would be nice to add other architectures  PRs are welcome    Data preprocessing    bash OMP NUM THREADS 2 th  i provider lua   lua provider   Provider   provider normalize   torch save  provider t7  provider   Takes about 30 seconds and saves 1400 Mb file    Training    bash CUDA VISIBLE DEVICES 0 th train lua   model vgg bn drop  s logs vgg Torch7 OpenCV demos   Real time demos that use deep convolutional neural networks to classify and caption what they see in real time from a webcam stream    All demos use CPU  but it s trivial to fix them to work with CUDA or OpenCL    There s a  Docker image  to make installation   experiments easier    Otherwise      Quick install on OS X    bash brew instal opencv3   with contrib OpenCV DIR  usr local Cellar opencv3 3 1 0 share OpenCV luarocks install cv brew install protobuf luarocks install loadcaffe   In Linux you have to build OpenCV 3 manually  Follow the instructions in     https   github com VisionLabs torch opencv   https   github com szagoruyko loadcaffe     ImageNet classification   The demo simply takes a central crop from a webcam and uses a small ImageNet classification pretrained network to classify what it see on it  top 5 predicted classes are shown on top  the top one is the most probable    Run as  th demo lua   Example      Age Gender prediction   This demo uses two networks described here http   www openu ac il home hassner projects cnn agegender  to predict age and gender of the faces that it finds with a simple cascade detector    Run as  th demo lua video source  path to  haarcascade frontalface default xml     Where  video source  is  camera  or path to a video file  and the second argument is optional    IMAGINE Lab gives an example      NeuralTalk2 demo   This demo uses NeuralTalk2 captioning code from Andrej Karpathy  https   github com karpathy neuraltalk2   The code captions live webcam demo  Follow the installation instructions at https   github com karpathy neuraltalk2 first and then run the demo as    th videocaptioning lua  gpuid  1  model model id1 501 1448236541 cpu t7   Caption is displayed on top      Realtime stylization with texture networks   Check https   github com DmitryUlyanov texture nets     Credits   2016 Sergey Zagoruyko and Egor Burkov   Thanks to VisionLabs for putting up https   github com VisionLabs torch opencv bindings  Fast RCNN models in Torch 7 format   AlexNet and VGG 16 pretrained on ImageNet and finetuned on VOC 2007 converted to Torch 7 format    Original models can be found here https   github com rbgirshick fast rcnn tree master data   Files      imagenet alexnet features t7  and  imagenet alexnet top t7   AlexNet pretrained on ImageNet   imagenet vgg features t7  and  imagenet vgg top t7   VGG 16 pretrained on ImageNet   caffenet fast rcnn iter 40000 t7  AlexNet pretrained on ImageNet and finetuned on VOC2007   vgg16 fast rcnn iter 40000 t7  VGG 16 pretrained on ImageNet and finetuned on VOC2007     Download link   The files are in Yandex cloud storage    https   yadi sk d R6K 3Dk9nHXn7   Requirements   luarocks install cudnn luarocks install inn   Loading   Models were cleaned with  nn Module clearState  and  gradWeight    gradBias  removed so don t forget to run  unpack    to restore them if needed    lua require  cudnn  require  inn  net   torch load caffenet fast rcnn iter 40000 t7  unpack     Model printout   Finetuned   AlexNet   nn Sequential      1   nn ParallelTable       input             1   nn Sequential                 1   cudnn SpatialConvolution 3    96  11x11  4 4  5 5                2   cudnn ReLU               3   cudnn SpatialMaxPooling 3 3 2 2 1 1                4   cudnn SpatialCrossMapLRN               5   cudnn SpatialConvolution 96    256  5x5  1 1  2 2                6   cudnn ReLU               7   cudnn SpatialMaxPooling 3 3 2 2 1 1                8   cudnn SpatialCrossMapLRN               9   cudnn SpatialConvolution 256    384  3x3  1 1  1 1                10   cudnn ReLU               11   cudnn SpatialConvolution 384    384  3x3  1 1  1 1                12   cudnn ReLU               13   cudnn SpatialConvolution 384    256  3x3  1 1  1 1                14   cudnn ReLU                          2   nn Identity               output        2   inn ROIPooling    3   nn View    4   nn Linear 9216    4096     5   cudnn ReLU    6   nn Dropout 0 500000     7   nn Linear 4096    4096     8   cudnn ReLU    9   nn Dropout 0 500000     10   nn ConcatTable       input             1   nn Linear 4096    21              2   nn Linear 4096    84                output         VGG 16   nn Sequential      1   nn ParallelTable       input             1   nn Sequential                 1   cudnn SpatialConvolution 3    64  3x3  1 1  1 1                2   cudnn ReLU               3   cudnn SpatialConvolution 64    64  3x3  1 1  1 1                4   cudnn ReLU               5   cudnn SpatialMaxPooling 2 2 2 2                6   cudnn SpatialConvolution 64    128  3x3  1 1  1 1                7   cudnn ReLU               8   cudnn SpatialConvolution 128    128  3x3  1 1  1 1                9   cudnn ReLU               10   cudnn SpatialMaxPooling 2 2 2 2                11   cudnn SpatialConvolution 128    256  3x3  1 1  1 1                12   cudnn ReLU               13   cudnn SpatialConvolution 256    256  3x3  1 1  1 1                14   cudnn ReLU               15   cudnn SpatialConvolution 256    256  3x3  1 1  1 1                16   cudnn ReLU               17   cudnn SpatialMaxPooling 2 2 2 2                18   cudnn SpatialConvolution 256    512  3x3  1 1  1 1                19   cudnn ReLU               20   cudnn SpatialConvolution 512    512  3x3  1 1  1 1                21   cudnn ReLU               22   cudnn SpatialConvolution 512    512  3x3  1 1  1 1                23   cudnn ReLU               24   cudnn SpatialMaxPooling 2 2 2 2                25   cudnn SpatialConvolution 512    512  3x3  1 1  1 1                26   cudnn ReLU               27   cudnn SpatialConvolution 512    512  3x3  1 1  1 1                28   cudnn ReLU               29   cudnn SpatialConvolution 512    512  3x3  1 1  1 1                30   cudnn ReLU                          2   nn Identity               output        2   inn ROIPooling    3   nn View    4   nn Linear 25088    4096     5   cudnn ReLU    6   nn Dropout 0 500000     7   nn Linear 4096    4096     8   cudnn ReLU    9   nn Dropout 0 500000     10   nn ConcatTable       input             1   nn Linear 4096    21              2   nn Linear 4096    84                output         Imagenet pretrained   Alexnet features   nn Sequential      1   cudnn SpatialConvolution 3    96  11x11  4 4  5 5     2   cudnn ReLU    3   cudnn SpatialMaxPooling 3 3 2 2 1 1     4   cudnn SpatialCrossMapLRN    5   cudnn SpatialConvolution 96    256  5x5  1 1  2 2     6   cudnn ReLU    7   cudnn SpatialMaxPooling 3 3 2 2 1 1     8   cudnn SpatialCrossMapLRN    9   cudnn SpatialConvolution 256    384  3x3  1 1  1 1     10   cudnn ReLU    11   cudnn SpatialConvolution 384    384  3x3  1 1  1 1     12   cudnn ReLU    13   cudnn SpatialConvolution 384    256  3x3  1 1  1 1     14   cudnn ReLU     Alexnet top   nn Sequential      1   nn Linear 9216    4096     2   cudnn ReLU    3   nn Dropout 0 500000     4   nn Linear 4096    4096     5   cudnn ReLU    6   nn Dropout 0 500000      VGG 16 features   nn Sequential      1   cudnn SpatialConvolution 3    64  3x3  1 1  1 1     2   cudnn ReLU    3   cudnn SpatialConvolution 64    64  3x3  1 1  1 1     4   cudnn ReLU    5   cudnn SpatialMaxPooling 2 2 2 2     6   cudnn SpatialConvolution 64    128  3x3  1 1  1 1     7   cudnn ReLU    8   cudnn SpatialConvolution 128    128  3x3  1 1  1 1     9   cudnn ReLU    10   cudnn SpatialMaxPooling 2 2 2 2     11   cudnn SpatialConvolution 128    256  3x3  1 1  1 1     12   cudnn ReLU    13   cudnn SpatialConvolution 256    256  3x3  1 1  1 1     14   cudnn ReLU    15   cudnn SpatialConvolution 256    256  3x3  1 1  1 1     16   cudnn ReLU    17   cudnn SpatialMaxPooling 2 2 2 2     18   cudnn SpatialConvolution 256    512  3x3  1 1  1 1     19   cudnn ReLU    20   cudnn SpatialConvolution 512    512  3x3  1 1  1 1     21   cudnn ReLU    22   cudnn SpatialConvolution 512    512  3x3  1 1  1 1     23   cudnn ReLU    24   cudnn SpatialMaxPooling 2 2 2 2     25   cudnn SpatialConvolution 512    512  3x3  1 1  1 1     26   cudnn ReLU    27   cudnn SpatialConvolution 512    512  3x3  1 1  1 1     28   cudnn ReLU    29   cudnn SpatialConvolution 512    512  3x3  1 1  1 1     30   cudnn ReLU     VGG 16 top   nn Sequential      input     1      2      3      4      5      6     output     1   nn Linear 25088    4096     2   cudnn ReLU    3   nn Dropout 0 500000     4   nn Linear 4096    4096     5   cudnn ReLU    6   nn Dropout 0 500000      Credits   Converted by  szagoruyko 2015 imagenet validation torch   A modified subset of https   github com soumith imagenet multiGPU torch to allow easy testing of imagenet models    Also used to make sure that models converted from other frameworks are correct    This code was used in  An Analysis of Deep Neural Network Models for Practical Applications  report http   arxiv org abs 1605 07678   Evaluation    Model name  top 1  central crop  accuracy   test time  s    framework   trained by                                  NIN            0       155 66    caffe   authors    bn NIN         62 62   170 71   torch   me    inception v2   51 00      156 21       google    inception v3    78 53    536 67       google    bvlc alexnet   54 61   172 29   caffe   bvlc    bn alexnet     56 60   170 56   torch   me     VGG CNN S      63 06   175 31   caffe   VGG    VGG 16         70 62   738 82   caffe   VGG    VGG 19         70 74   898 13   caffe   VGG    ResNet 18      69 31   208 53   torch   facebook    ResNet 34      72 94   363 11   torch   facebook    ResNet 50      75 76   439 36   torch   facebook    ResNet 101     77 34   695 78   torch   facebook     Time is given for Titan Black    Networks     inception v2   https   github com soumith inception torch   inception v3   https   github com szagoruyko inception v3 torch   bvlc alexnet   https   github com BVLC caffe tree master models bvlc alexnet   bn alexnet   https   gist github com szagoruyko dd032c529048492630fc   bn nin   https   gist github com szagoruyko 0f5b4c5e2d2b18472854   VGG CNN S   https   gist github com ksimonyan fd8800eeb36e276cd6f9 file readme md   VGG 16   https   gist github com ksimonyan 211839e770f7b538e2d8 file readme md   VGG 19   https   gist github com ksimonyan 3785162f95cd2d5fee77 file readme md   iterm torch   Display files directly in iTerm2  Version 2 9 is required  More details in https   iterm2 com images html   Image example      It also works with graphs  See examples graphgen lua  downsampled       And with gnuplot    Fork description   This is a version of neural style implemented with autograd   100 lines less  as fast as the original    neural style   This is a torch implementation of the paper  A Neural Algorithm of Artistic Style  by Leon A  Gatys  Alexander S  Ecker  and Matthias Bethge    The paper presents an algorithm for combining the content of one image with the style of another image using convolutional neural networks  Here s an example that maps the artistic style of  The Starry Night  onto a night time photograph of the Stanford campus          Applying the style of different images to the same content image gives interesting results  Here we reproduce Figure 2 from the paper  which renders a photograph of the Tubingen in Germany in a variety of styles                Here are the results of applying the style of various pieces of artwork to this photograph of the golden gate bridge                              Content   Style Tradeoff   The algorithm allows the user to trade off the relative weight of the style and content reconstruction terms  as shown in this example where we port the style of  Picasso s 1907 self portrait  onto Brad Pitt                Style Scale   By resizing the style image before extracting style features  we can control the types of artistic features that are transfered from the style image  you can control this behavior with the   style scale  flag  Below we see three examples of rendering the Golden Gate Bridge in the style of The Starry Night  From left to right    style scale  is 2 0  1 0  and 0 5          Multiple Style Images   You can use more than one style image to blend multiple artistic styles    Clockwise from upper left   The Starry Night     The Scream    The Scream     Composition VII    Seated Nude     Composition VII   and  Seated Nude     The Starry Night            Style Interpolation   When using multiple style images  you can control the degree to which they are blended          Setup    Dependencies     torch7     loadcaffe   Optional dependencies    For CUDA backend      CUDA 6 5       cunn    For cuDNN backend       cudnn torch    For OpenCL backend       cltorch       clnn   After installing dependencies  you ll need to run the following script to download the VGG model   sh models download models sh  This will download the original  VGG 19 model   Leon Gatys has graciously provided the modified version of the VGG 19 model that was used in their paper  this will also be downloaded  By default the original VGG 19 model is used    If you have a smaller memory GPU then using NIN Imagenet model will be better and gives slightly worse yet comparable results  You can get the details on the model from  BVLC Caffe ModelZoo  and can download the files from  NIN Imagenet Download Link   You can find detailed installation instructions for Ubuntu in the  installation guide     Usage   Basic usage   th neural style lua  style image  image jpg   content image  image jpg    OpenCL usage with NIN Model  This requires you download the NIN Imagenet model files as described above    th neural style lua  style image examples inputs picasso selfport1907 jpg  content image examples inputs brad pitt jpg  output image profile png  model file models nin imagenet conv caffemodel  proto file models train val prototxt  gpu 0  backend clnn  num iterations 1000  seed 123  content layers relu0 relu3 relu7 relu12  style layers relu0 relu3 relu7 relu12  content weight 10  style weight 1000  image size 512  optimizer adam     To use multiple style images  pass a comma separated list like this     style image starry night jpg the scream jpg     Note that paths to images should not contain the     character to represent your home directory  you should instead use a relative path or a full absolute path    Options       image size   Maximum side length  in pixels  of of the generated image  Default is 512      style blend weights   The weight for blending the style of multiple style images  as a   comma separated list  such as   style blend weights 3 7   By default all style images   are equally weighted      gpu   Zero indexed ID of the GPU to use  for CPU mode set   gpu  to  1    Optimization options       content weight   How much to weight the content reconstruction term  Default is 5e0      style weight   How much to weight the style reconstruction term  Default is 1e2      tv weight   Weight of total variation  TV  regularization  this helps to smooth the image    Default is 1e 3  Set to 0 to disable TV regularization      num iterations   Default is 1000      init   Method for generating the generated image  one of  random  or  image     Default is  random  which uses a noise initialization as in the paper   image    initializes with the content image      optimizer   The optimization algorithm to use  either  lbfgs  or  adam   default is  lbfgs     L BFGS tends to give better results  but uses more memory  Switching to ADAM will reduce memory usage    when using ADAM you will probably need to play with other parameters to get good results  especially   the style weight  content weight  and learning rate  you may also want to normalize gradients when   using ADAM      learning rate   Learning rate to use with the ADAM optimizer  Default is 1e1      normalize gradients   If this flag is present  style and content gradients from each layer will be   L1 normalized  Idea from  andersbll neural artistic style     Output options       output image   Name of the output image  Default is  out png       print iter   Print progress every  print iter  iterations  Set to 0 to disable printing      save iter   Save the image every  save iter  iterations  Set to 0 to disable saving intermediate results    Layer options       content layers   Comma separated list of layer names to use for content reconstruction    Default is  relu4 2       style layers   Comman separated list of layer names to use for style reconstruction    Default is  relu1 1 relu2 1 relu3 1 relu4 1 relu5 1     Other options       style scale   Scale at which to extract features from the style image  Default is 1 0      proto file   Path to the  deploy txt  file for the VGG Caffe model      model file   Path to the   caffemodel  file for the VGG Caffe model    Default is the original VGG 19 model  you can also try the normalized VGG 19 model used in the paper      pooling   The type of pooling layers to use  one of  max  or  avg   Default is  max     The VGG 19 models uses max pooling layers  but the paper mentions that replacing these layers with average   pooling layers can improve the results  I haven t been able to get good results using average pooling  but   the option is here      backend    nn    cudnn   or  clnn   Default is  nn    cudnn  requires    cudnn torch  and may reduce memory usage     clnn  requires  cltorch  and  clnn      cudnn autotune   When using the cuDNN backend  pass this flag to use the built in cuDNN autotuner to select   the best convolution algorithms for your architecture  This will make the first iteration a bit slower and can   take a bit more memory  but may significantly speed up the cuDNN backend    Frequently Asked Questions   Problem   Generated image has saturation artifacts      Solution   Update the  image  packge to the latest version   luarocks install image   Problem   Running without a GPU gives an error message complaining about  cutorch  not found   Solution   Pass the flag   gpu  1  when running in CPU only mode   Problem   The program runs out of memory and dies   Solution   Try reducing the image size    image size 256   or lower   Note that different image sizes will likely require non default values for   style weight  and   content weight  for optimal results  If you are running on a GPU  you can also try running with   backend cudnn  to reduce memory usage    Problem   Get the following error message    models VGG ILSVRC 19 layers deploy prototxt cpu lua 7  attempt to call method  ceil   a nil value    Solution   Update  nn  package to the latest version   luarocks install nn   Problem   Get an error message complaining about  paths extname   Solution   Update  torch paths  package to the latest version   luarocks install paths   Problem   NIN Imagenet model is not giving good results     Solution   Make sure the correct   proto file  is selected  Also make sure the correct parameters for   content layers  and   style layers  are set   See OpenCL usage example above     Problem     backend cudnn  is slower than default NN backend   Solution   Add the flag   cudnn autotune   this will use the built in cuDNN autotuner to select the best convolution algorithms    Memory Usage   By default   neural style  uses the  nn  backend for convolutions and L BFGS for optimization  These give good results  but can both use a lot of memory  You can reduce memory usage with the following      Use cuDNN   Add the flag   backend cudnn  to use the cuDNN backend  This will only work in GPU mode    Use ADAM   Add the flag   optimizer adam  to use ADAM instead of L BFGS  This should significantly   reduce memory usage  but may require tuning of other parameters for good results  in particular you should   play with the learning rate  content weight  style weight  and also consider using gradient normalization    This should work in both CPU and GPU modes    Reduce image size   If the above tricks are not enough  you can reduce the size of the generated image    pass the flag   image size 256  to generate an image at half the default size      With the default settings   neural style  uses about 3 5GB of GPU memory on my system  switching to ADAM and cuDNN reduces the GPU memory footprint to about 1GB    Speed   Speed can vary a lot depending on the backend and the optimizer  Here are some times for running 500 iterations with   image size 512  on a GTX Titan X with different settings      backend nn  optimizer lbfgs   62 seconds     backend nn  optimizer adam   49 seconds     backend cudnn  optimizer lbfgs   79 seconds     backend cudnn  cudnn autotune  optimizer lbfgs   58 seconds     backend cudnn  cudnn autotune  optimizer adam   44 seconds     backend clnn  optimizer lbfgs   169 seconds     backend clnn  optimizer adam   106 seconds    Implementation details   Images are initialized with white noise and optimized using L BFGS    We perform style reconstructions using the  conv1 1    conv2 1    conv3 1    conv4 1   and  conv5 1  layers and content reconstructions using the  conv4 2  layer  As in the paper  the five style reconstruction losses have equal weights  Torch FFI bindings for NNPACK   NNPACK is a fast CPU implementation of convolution operations for training ConvNets    https   github com Maratyszcza NNPACK   The bindings are fully working and tested against  nn  version  Only single precision supported  Make sure you have AVX2 compatible Skylake Broadwell Haswell CPU    Limitations of NNPACK      there is no scale parameter on  accGradParameters  call     Installation   Follow installation steps at https   github com Maratyszcza NNPACK to generate  libnnpack so  and place where  LD LIBRARY PATH  can find it    Then do   luarocks install https   raw githubusercontent com szagoruyko nnpack torch master nnpack scm 1 rockspec   Conversion between nnpack and nn   Similar to  cudnn convert  in  cudnn torch  easy backend switching is supported  To switch to  nnpack  just do    lua nnpack convert net  nnpack    There will be no memory copy  just metatables will be swapped    Credits   Thanks to  Maratyszcza for adding the option to generate shared NNPACK library  Wide Residual Networks   This code was used for experiments with Wide Residual Networks  BMVC 2016  http   arxiv org abs 1605 07146 by Sergey Zagoruyko and Nikos Komodakis    Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance  However  each fraction of a percent of improved accuracy costs nearly doubling the number of layers  and so training very deep residual networks has a problem of diminishing feature reuse  which makes these networks very slow to train    To tackle these problems  in this work we conduct a detailed experimental study on the architecture of ResNet blocks  based on which we propose a novel architecture where we  decrease depth  and  increase width  of residual networks  We call the resulting network structures  wide residual networks  WRNs   and show that these are far superior over their commonly used thin and very deep counterparts    For example  we demonstrate that even a simple 16 layer deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks  including thousand layer deep networks  We further show that WRNs achieve  incredibly  good results  e g   achieving new state of the art results on CIFAR 10  CIFAR 100  SVHN  COCO and substantial improvements on ImageNet  and train  several times faster  than pre activation ResNets    Update  August 2019    Pretrained ImageNet WRN models are available in torchvision 0 4 and  PyTorch Hub   e g  loading WRN 50 2   python model   torch hub load  pytorch vision    wide resnet50 2   pretrained True    Update  November 2016    We updated the paper with ImageNet  COCO and meanstd preprocessing CIFAR results  If you re comparing your method against WRN  please report correct preprocessing numbers because they give substantially different results    tldr  ImageNet WRN 50 2 bottleneck  ResNet 50 with wider inner bottleneck 3x3 convolution  is significantly faster than ResNet 152 and has better accuracy  on CIFAR meanstd preprocessing  as in fb resnet torch  gives better results than ZCA whitening  on COCO wide ResNet with 34 layers outperforms even Inception v4 based Fast RCNN model in single model performance    Test error     flip translation augmentation   meanstd  normalization  median of 5 runs  on CIFAR    Network            CIFAR 10   CIFAR 100                                           pre ResNet 164     5 46       24 33 pre ResNet 1001    4 92       22 71 WRN 28 10          4 00       19 25 WRN 28 10 dropout   3 89         18 85   Single time runs  meanstd normalization     Dataset   network   test perf                                    CIFAR 10    WRN 40 10 dropout   3 8  CIFAR 100   WRN 40 10 dropout   18 3  SVHN        WRN 16 8 dropout    1 54  ImageNet  single crop    WRN 50 2 bottleneck   21 9  top 1  5 79  top 5 COCO val5k  single model    WRN 34 2   36 mAP   See http   arxiv org abs 1605 07146 for details      bibtex     INPROCEEDINGS Zagoruyko2016WRN      author    Sergey Zagoruyko and Nikos Komodakis       title    Wide Residual Networks       booktitle    BMVC       year    2016     Pretrained models   ImageNet   WRN 50 2 bottleneck  wider bottleneck   see  pretrained  for details  Download  263MB   https   yadi sk d  8AWymOPyVZns   There are also PyTorch and Tensorflow model definitions with pretrained weights at  https   github com szagoruyko functional zoo blob master wide resnet 50 2 export ipynb   COCO   Coming   Installation   The code depends on Torch http   torch ch  Follow instructions  here  and run    luarocks install torchnet luarocks install optnet luarocks install iterm   For visualizing training curves we used ipython notebook with pandas and bokeh    Usage   Dataset support   The code supports loading simple datasets in torch format  We provide the following      MNIST  data preparation script   CIFAR 10   recommended    data preparation script    preprocessed data  176MB    CIFAR 10 whitened  using pylearn2   preprocessed dataset   CIFAR 100   recommended    data preparation script    preprocessed data  176MB    CIFAR 100 whitened  using pylearn2   preprocessed dataset   SVHN  data preparation script     To whiten CIFAR 10 and CIFAR 100 we used the following scripts https   github com lisa lab pylearn2 blob master pylearn2 scripts datasets make cifar10 gcn whitened py and then converted to torch using https   gist github com szagoruyko ad2977e4b8dceb64c68ea07f6abf397b and npy to torch converter https   github com htwaijry npy4th    We are running ImageNet experiments and will update the paper and this repo soon    Training   We provide several scripts for reproducing results in the paper  Below are several examples    bash model wide resnet widen factor 4 depth 40   scripts train cifar sh   This will train WRN 40 4 on CIFAR 10 whitened  supposed to be in  datasets  folder   This network achieves about the same accuracy as ResNet 1001 and trains in 6 hours on a single Titan X  Log is saved to  logs wide resnet  RANDOM RANDOM  folder with json entries for each epoch and can be visualized with itorch ipython later    For reference we provide logs for this experiment and  ipython notebook  to visualize the results  After running it you should see these training curves      Another example    bash model wide resnet widen factor 10 depth 28 dropout 0 3 dataset   datasets cifar100 whitened t7   scripts train cifar sh   This network achieves 20 0  error on CIFAR 100 in about a day on a single Titan X    Multi GPU is supported with  nGPU n  parameter    Other models   Additional models in this repo      NIN  7 4  on CIFAR 10 whitened    VGG  modified from  cifar torch   6 3  on CIFAR 10 whitened    pre activation ResNet  from https   github com KaimingHe resnet 1k layers      Implementation details   The code evolved from https   github com szagoruyko cifar torch  To reduce memory usage we use  fmassa s  optimize net   which automatically shares output and gradient tensors between modules  This keeps memory usage below 4 Gb even for our best networks  Also  it can generate network graph plots as the one for WRN 16 2 in the end of this page    Acknowledgements   We thank startup company  VisionLabs  and Eugenio Culurciello for giving us access to their clusters  without them ImageNet experiments wouldn t be possible  We also thank Adam Lerer and Sam Gross for helpful discussions  Work supported by EC project FP7 ICT 611145 ROBOSPECT    Torch tutorials for Deep Learning  Tools and Methods workshop  July 4 5 6  Idiap    Worksop website  http   www idiap ch workshop dltm      MNIST autograd example shows usage of autograd to train neural networks and visualize t SNE embeddings of learned features   pretrained resnet shows how to load pretrain ResNet networks  visualize it and propagate a single image to predict it s classes   wide resnet multiGPU shows how to train state of the art networks on CIFAR using multiple GPUs on a single machine      Credits   Sergey Zagoruyko  Soumith Chintala  July 2016 PyTorch Examples   A repository showcasing examples of using pytorch   MNIST Convnets   Word level Language Modeling using LSTM RNNs   Imagenet 12 training with Residual Networks   Generative Adversarial Networks  DCGAN  functional zoo   Model definitions and pretrained weights for PyTorch and Tensorflow   PyTorch  unlike lua torch  has autograd in it s core  so using modular structure of  torch nn  modules is not necessary  one can easily allocate needed Variables and write a function that utilizes them  which is sometimes more convenient  This repo contains model definitions in this functional way  with pretrained weights for some models    Weights are serialized as a dict of arrays in  hdf5   so should be easily loadable in other frameworks  Thanks to  edgarriba we have  cpp parser  for loading weights in C      More models coming  We also plan to add definitions for other frameworks in future  probably  tiny dnn  first  Contributions are welcome    See also imagenet classification with PyTorch  demo ipynb   Models   All models were validated to produce reported accuracy using  imagenet validation py  script  depends on OpenCV python bindings     To load weights in Python first do  pip install hickle   then    python import hickle as hkl weights   hkl load  resnet 18 export hkl     Unfortunately  hickle py3 support is not yet ready  so the models will be resaved in torch pickle format with  torch utils model zoo load url  support  e g     python weights   model zoo load url  https   s3 amazonaws com modelzoo networks wide resnet 50 2 export 5ae25d50 pth     Also   make dot  was moved to a separate package   PyTorchViz   Folded   Models below have batch norm parameters and statistics folded into convolutional layers for speed  It is not recommended to use them for finetuning    ImageNet     model   notebook   val error   download   size                                                       VGG 16    vgg 16 ipynb    30 09  10 69   url coming   528 MB     NIN    nin export ipynb    32 96  12 29    url    33 MB     ResNet 18  fb     resnet 18 export ipynb    30 43  10 76    url    42 MB     ResNet 18 AT    resnet 18 at export ipynb    29 44  10 12    url    44 1 MB     ResNet 34  fb     resnet 34 export ipynb    26 72  8 74    url    78 3 MB     WRN 50 2    wide resnet 50 2 export ipynb    22 0  6 05    url    246 MB     Fast Neural Style   Notebook   fast neural style ipynb   Models      model   download   size                                 candy hkl    url    7 1 MB     feathers hkl    url    7 1 MB     wave hkl    url    7 1 MB     Models with batch normalization   Coming Attention Transfer   PyTorch code for  Paying More Attention to Attention  Improving the Performance of Convolutional Neural Networks via Attention Transfer   https   arxiv org abs 1612 03928  Conference paper at ICLR2017  https   openreview net forum id Sks9 ajex     What s in this repo so far     Activation based AT code for CIFAR 10 experiments    Code for ImageNet experiments  ResNet 18 ResNet 34 student teacher     Jupyter notebook to visualize attention maps of ResNet 34  visualize attention ipynb   Coming     grad based AT    Scenes and CUB activation based AT code   The code uses PyTorch  https   pytorch org   Note that the original experiments were done using  torch autograd   we have so far validated that CIFAR 10 experiments are  exactly  reproducible in PyTorch  and are in process of doing so for ImageNet  results are very slightly worse in PyTorch  due to hyperparameters     bibtex     inproceedings Zagoruyko2017AT      author    Sergey Zagoruyko and Nikos Komodakis       title    Paying More Attention to Attention  Improving the Performance of              Convolutional Neural Networks via Attention Transfer       booktitle    ICLR       url    https   arxiv org abs 1612 03928       year    2017     Requirements   First install  PyTorch   then install  torchnet     pip install git https   github com pytorch tnt git master   then install other Python packages    pip install  r requirements txt   Experiments   CIFAR 10   This section describes how to get the results in the table 1 of the paper    First  train teachers    python cifar py   save logs resnet 40 1 teacher   depth 40   width 1 python cifar py   save logs resnet 16 2 teacher   depth 16   width 2 python cifar py   save logs resnet 40 2 teacher   depth 40   width 2   To train with activation based AT do    python cifar py   save logs at 16 1 16 2   teacher id resnet 16 2 teacher   beta 1e 3   To train with KD    python cifar py   save logs kd 16 1 16 2   teacher id resnet 16 2 teacher   alpha 0 9   We plan to add AT KD with decaying  beta  to get the best knowledge transfer results soon    ImageNet   Pretrained model   We provide ResNet 18 pretrained model with activation based AT      Model   val error                          ResNet 18   30 4  10 8    ResNet 18 ResNet 34 AT   29 3  10 0     Download link   https   s3 amazonaws com modelzoo networks resnet 18 at export pth   Model definition   https   github com szagoruyko functional zoo blob master resnet 18 at export ipynb   Convergence plot      Train from scratch   Download pretrained weights for ResNet 34  see also  functional zoo  for more information     wget https   s3 amazonaws com modelzoo networks resnet 34 export pth   Prepare the data following  fb resnet torch  and run training  e g  using 2 GPUs     python imagenet py   imagenetpath   ILSVRC2012   depth 18   width 1                        teacher params resnet 34 export hkl   gpu id 0 1   ngpu 2                        beta 1e 3 openai gemm pytorch   PyTorch bindings for openai gemm    https   github com openai openai gemm   Installation   Clone original openai gemm and add it to PYTHONPATH  install pycuda    pip install pycuda   and follow instructions to install PyTorch on  http   pytorch org   No  neon  installation needed    Usage   The library defines  matmul  function similar to the one that works with neon   https   github com openai openai gemm blob master openai gemm py L14   which instead of neon matrices takes  torch cuda FloatTensor  or  torch cuda HalfTensor  as A  B and C  Neural Network Demos IMI   Presentation for IMI  ing nierie math matique et informatique  students at ENPC February 15 2017  There are 4 demos     Live ImageNet classification with OpenCV    Telegram bot for ImageNet classification    Live Fast Neural Style  Prisma like     Live face detection and Age Gender classification   The demos make use of     PyTorch  deep learning framework    OpenCV  fast computer vision library    Telegram  messenger with easy Python API for bot creation   This demos are mostly ported from  https   github com szagoruyko torch opencv demos PyINN   CuPy implementations of fused PyTorch ops    PyTorch version of  imagine nn   The purpose of this package is to contain CUDA ops written in Python with CuPy  which is not a PyTorch dependency    An alternative to CuPy would be  https   github com pytorch extension ffi   but it requires a lot of wrapping code like  https   github com sniklaus pytorch extension   so doesn t really work with quick prototyping    Another advantage of CuPy over C code is that dimensions of each op are known at JIT ing time  and compiled kernels potentially can be faster  Also  the first version of the package was in PyCUDA  but it can t work with PyTorch multi GPU      On Maxwell Titan X pyinn conv2d depthwise MobileNets are  2 6x faster than F conv2d    benchmark py   No longer the case   with new kernels PyTorch 0 3 0 is now  20  faster than pyinn    Installation   pip install git https   github com szagoruyko pyinn git master   Example   python import torch from torch autograd import Variable import pyinn as P x   Variable torch randn 1 4 5 5  cuda    w   Variable torch randn 4 1 3 3  cuda    y   P conv2d depthwise x  w  padding 1    or with modules interface    python from pyinn modules import Conv2dDepthwise module   Conv2dDepthwise channels 4  kernel size 3  padding 1  cuda   y   module x    Documentation   conv2d depthwise   Implements depthwise convolution as in  https   arxiv org abs 1704 04861  MobileNets  Efficient Convolutional Neural Networks for Mobile Vision Applications   CUDA kernels from https   github com BVLC caffe pull 5665   CPU side is done by  F conv2d     Equivalent to    python F conv2d input  weight  groups input size 1     Inputs and arguments are the same with  F conv2d   dgmm   Multiplication with a diagonal matrix    Used CUDA dgmm function  sometimes is faster than expand    In torch functions does  input mm x diag      Both left and right mutliplications are supported    Args      input  2D tensor     x  1D tensor   cdgmm   Complex multiplication with a diagonal matrix    Does  input mm x diag     where input and x are complex    Args      input  3D tensor with last dimension of size 2     x  2D tensor with last dimension of size 2   NCReLU   Applies NCReLU  negative concatenated ReLU  nonlinearity    Does  torch cat  x clamp min 0   x clamp max 0    dim 1   in a single fused op    Used in  https   arxiv org abs 1706 00388  DiracNets  Training Very Deep Neural Networks Without Skip Connections   Args      input  4D tensor   im2col and col2im   Rearrange image blocks into columns    The representation is used to perform GEMM based convolution    Output is 5D  or 6D in case of minibatch  tensor    Minibatch implementation is inefficient  and could be done in a single CUDA kernel  DiracNets   v2 update  January 2018     The code was updated for DiracNets v2 in which we removed NCReLU by adding per channel  a  and  b  multipliers without weight decay  This allowed us to significantly simplify the network  which is now folds into a simple chain of convolution ReLU layers  like VGG  On ImageNet DiracNet 18 and DiracNet 34 closely match corresponding ResNet with the same number of parameters    See v1 branch for DiracNet v1      PyTorch code and models for  DiracNets  Training Very Deep Neural Networks Without Skip Connections   https   arxiv org abs 1706 00388   Networks with skip connections like ResNet show excellent performance in image recognition benchmarks  but do not benefit from increased depth  we are thus still interested in learning  actually  deep representations  and the benefits they could bring  We propose a simple weight parameterization  which improves training of deep plain  without skip connections  networks  and allows training plain networks with hundreds of layers  Accuracy of our proposed DiracNets is close to Wide ResNet  although DiracNets need more parameters to achieve it   and we are able to match ResNet 1000 accuracy with plain DiracNet with only 28 layers  Also  the proposed Dirac weight parameterization can be folded into one filter for inference  leading to easily interpretable VGG like network    DiracNets on ImageNet     TL DR   In a nutshell  Dirac parameterization is a sum of filters and scaled Dirac delta function    conv2d x  alpha   delta   W    Here is simplified PyTorch like pseudocode for the function we use to train plain DiracNets  with weight normalization     python def dirac conv2d input  W  alpha  beta      return F conv2d input  alpha   dirac W    beta   normalize W     where  alpha  and  beta  are per channel scaling multipliers  and  normalize  does l 2 normalization over each feature plane    Code   Code structure         README md             this file       diracconv py       modular DiracConv definitions       test py                 unit tests       diracnet export ipynb    ImageNet pretrained models       diracnet py         functional model definitions       train py               CIFAR and ImageNet training code   Requirements   First install  PyTorch   then install  torchnet     pip install git https   github com pytorch tnt git master   Install other Python packages    pip install  r requirements txt   To train DiracNet 34 2 on CIFAR do    python train py   save   logs diracnets  RANDOM RANDOM   depth 34   width 2   To train DiracNet 18 on ImageNet do    bash python train py   dataroot   ILSVRC2012    dataset ImageNet   depth 18   save   logs diracnet  RANDOM RANDOM                     batchSize 256   epoch step  30 60 90    epochs 100   weightDecay 0 0001   lr decay ratio 0 1   nn Module code   We provide  DiracConv1d    DiracConv2d    DiracConv3d   which work like  nn Conv1d    nn Conv2d    nn Conv3d   but have Dirac parametrization inside  our training code doesn t use these modules though     Pretrained models   We fold batch normalization and Dirac parameterization into  F conv2d   weight  and  bias  tensors for simplicity  Resulting models are as simple as VGG or AlexNet  having only nonlinearity conv2d as a basic block    See  diracnets ipynb  for functional and modular model definitions    There is also folded DiracNet definition in  diracnet py   which uses code from PyTorch model zoo and downloads pretrained model from Amazon S3    python from diracnet import diracnet18 model   diracnet18 pretrained True    Printout of the model above    DiracNet     features   Sequential       conv   Conv2d  3  64  kernel size  7  7   stride  2  2   padding  3  3        max pool0   MaxPool2d kernel size  3  3   stride  2  2   padding  1  1   dilation  1  1   ceil mode False       group0 block0 relu   ReLU        group0 block0 conv   Conv2d  64  64  kernel size  3  3   stride  1  1   padding  1  1        group0 block1 relu   ReLU        group0 block1 conv   Conv2d  64  64  kernel size  3  3   stride  1  1   padding  1  1        group0 block2 relu   ReLU        group0 block2 conv   Conv2d  64  64  kernel size  3  3   stride  1  1   padding  1  1        group0 block3 relu   ReLU        group0 block3 conv   Conv2d  64  64  kernel size  3  3   stride  1  1   padding  1  1        max pool1   MaxPool2d kernel size  2  2   stride  2  2   dilation  1  1   ceil mode False       group1 block0 relu   ReLU        group1 block0 conv   Conv2d  64  128  kernel size  3  3   stride  1  1   padding  1  1        group1 block1 relu   ReLU        group1 block1 conv   Conv2d  128  128  kernel size  3  3   stride  1  1   padding  1  1        group1 block2 relu   ReLU        group1 block2 conv   Conv2d  128  128  kernel size  3  3   stride  1  1   padding  1  1        group1 block3 relu   ReLU        group1 block3 conv   Conv2d  128  128  kernel size  3  3   stride  1  1   padding  1  1        max pool2   MaxPool2d kernel size  2  2   stride  2  2   dilation  1  1   ceil mode False       group2 block0 relu   ReLU        group2 block0 conv   Conv2d  128  256  kernel size  3  3   stride  1  1   padding  1  1        group2 block1 relu   ReLU        group2 block1 conv   Conv2d  256  256  kernel size  3  3   stride  1  1   padding  1  1        group2 block2 relu   ReLU        group2 block2 conv   Conv2d  256  256  kernel size  3  3   stride  1  1   padding  1  1        group2 block3 relu   ReLU        group2 block3 conv   Conv2d  256  256  kernel size  3  3   stride  1  1   padding  1  1        max pool3   MaxPool2d kernel size  2  2   stride  2  2   dilation  1  1   ceil mode False       group3 block0 relu   ReLU        group3 block0 conv   Conv2d  256  512  kernel size  3  3   stride  1  1   padding  1  1        group3 block1 relu   ReLU        group3 block1 conv   Conv2d  512  512  kernel size  3  3   stride  1  1   padding  1  1        group3 block2 relu   ReLU        group3 block2 conv   Conv2d  512  512  kernel size  3  3   stride  1  1   padding  1  1        group3 block3 relu   ReLU        group3 block3 conv   Conv2d  512  512  kernel size  3  3   stride  1  1   padding  1  1        last relu   ReLU        avg pool   AvgPool2d kernel size 7  stride 7  padding 0  ceil mode False  count include pad True         fc   Linear in features 512  out features 1000      The models were trained with OpenCV  so you need to use it too to reproduce stated accuracy    Pretrained weights for DiracNet 18 and DiracNet 34    https   s3 amazonaws com modelzoo networks diracnet18v2folded a2174e15 pth   https   s3 amazonaws com modelzoo networks diracnet34v2folded dfb15d34 pth   Pretrained weights for the original  not folded  model   functional definition only    https   s3 amazonaws com modelzoo networks diracnet18 v2 checkpoint pth   https   s3 amazonaws com modelzoo networks diracnet34 v2 checkpoint pth   We plan to add more pretrained models later    Bibtex    inproceedings Zagoruyko2017diracnets      author    Sergey Zagoruyko and Nikos Komodakis       title    DiracNets  Training Very Deep Neural Networks Without Skip Connections       url    https   arxiv org abs 1706 00388       year    2017   Functional style transfer   Minimal reimplementation of  https   github com leongatys PytorchNeuralStyleTransfer  with PyTorch functional interface  PyTorchViz   A small package to create visualizations of PyTorch execution graphs and traces      Installation   Install graphviz  e g     brew install graphviz   Install the package itself    pip install torchviz   Usage   Example usage of  make dot       model   nn Sequential   model add module  W0   nn Linear 8  16   model add module  tanh   nn Tanh    model add module  W1   nn Linear 16  1     x   torch randn 1  8  y   model x    make dot y mean    params dict model named parameters            Set  show attrs True  and  show saved True  to see what autograd saves for the backward pass   Note that this is only available for pytorch    1 9       model   nn Sequential   model add module  W0   nn Linear 8  16   model add module  tanh   nn Tanh    model add module  W1   nn Linear 16  1     x   torch randn 1  8  y   model x    make dot y mean    params dict model named parameters     show attrs True  show saved True         Acknowledgements   The script was moved from  functional zoo  where it was created with the help of Adam Paszke  Soumith Chintala  Anton Osokin  and uses bits from  tensorboard pytorch   Other contributors are   willprice     soulitzer     albanD   1 bit Wide ResNet   PyTorch implementation of training 1 bit Wide ResNets from this paper    Training wide residual networks for deployment using a single bit for each weight  by  Mark D  McDonnell  at ICLR 2018   https   openreview net forum id rytNfI1AZ   https   arxiv org abs 1802 08530   The idea is very simple but surprisingly effective for training ResNets with binary weights  Here is the proposed weight parameterization as PyTorch autograd function       python class ForwardSign torch autograd Function        staticmethod     def forward ctx  w           return math sqrt 2     w shape 1    w shape 2    w shape 3      w sign      staticmethod def backward ctx  g       return g          On forward  we take sign of the weights and scale it by He init constant  On backward  we propagate gradient without changes  WRN 20 10 trained with such parameterization is only slightly off from it s full precision variant  here is what I got myself with this code on CIFAR 100      network   accuracy  5 runs mean    std    checkpoint  Mb                         WRN 20 10   80 5    0 24   205 Mb     WRN 20 10 1bit   80 0    0 26   3 5 Mb     Details   Here are the differences with WRN code  https   github com szagoruyko wide residual networks       BatchNorm has no affine weight and bias parameters   First layer has 16   width channels   Last fc layer is removed in favor of 1x1 conv   F avg pool2d   Downsample is done by F avg pool2d   torch cat instead of strided conv   SGD with cosine annealing and warm restarts     I used PyTorch 0 4 1 and Python 3 6 to run the code    Reproduce WRN 20 10 with 1 bit training on CIFAR 100    bash python main py   binarize   save   logs WRN 20 10 1bit  RANDOM   width 10   dataset CIFAR100   Convergence plot  train error in dash       I ve also put 3 5 Mb checkpoint with binary weights packed with  np packbits   and a very short script to evaluate it    bash python evaluate packed py   checkpoint wrn20 10 1bit packed pth tar   width 10   dataset CIFAR100   S3 url to checkpoint   https   s3 amazonaws com modelzoo networks wrn20 10 1bit packed pth tar WideResNets   RNNs with weight symmetry   PyTorch 0 3 code for  Exploring Weight Symmetry in Deep Neural Networks   https   arxiv org abs 1812 11027   We propose to impose symmetry in neural network parameters to improve parameter usage and make use of dedicated convolution and matrix multiplication routines  Due to significant reduction in the number of parameters as a result of the symmetry constraints  one would expect a dramatic drop in accuracy  Surprisingly  we show that this is not the case  and  depending on network size  symmetry can have little or no negative effect on network accuracy  especially in deep overparameterized networks  We propose several ways to impose local symmetry in recurrent and convolutional neural networks  and show that our symmetry parameterizations satisfy universal approximation property for single hidden layer networks  We extensively evaluate these parameterizations on CIFAR  ImageNet and language modeling datasets  showing significant benefits from the use of symmetry  For instance  our ResNet 101 with channel wise symmetry has almost 25  less parameters and only 0 2  accuracy loss on ImageNet    Main idea in two sentences   We only learn a fraction of weights for a Conv Linear layer  The other weights of that layer are generated dynamically by repeating the learned weights    Requirements   First install  PyTorch   then install  torchnet     pip install git https   github com pytorch tnt git master   To train SymmWideResNet on CIFAR10 with triangular symmetry  enter symmWideResNet directory  and run   bash python main py   width 1   depth 16   model resnet   dataset CIFAR10   symm type tri   To train SymmRNN  please check README md in symmRNN directory    Bibtex    article hu2018exploring    title  Exploring Weight Symmetry in Deep Neural Network     author  Hu  Shell Xu and Zagoruyko  Sergey and Komodakis  Nikos     journal  arXiv preprint arXiv 1812 11027     year  2018
48,hainm,Beerware License   Solvation Energy calculation with different Generalized Born solvent models   Those are Python scripts for my graduate work   those scripts were written about 6 years ago  and changed a bit this year   so the writing is very ugly  But they did what they were supposed to do    Aim    Calculate solvent energies with GB HCT  GB OBC  GB Neck  GB Neck2 model   you can do this with sander pmemd in AMBER too    Process AMBER topology and do a bunch of stuff  Please see the inside code    Check     GB GBClass py  and       Examples  Notebooks   I am gathering useful notebooks from internet   Cython       Memoryview Benchmarks      Memoryviews are as fast as raw pointer   https   jakevdp github io blog 2012 08 08 memoryview benchmarks              Projects that heavily use Cython     pytraj  https   github com pytraj pytraj   Cython QuantLib wrappers  https   github com enthought pyql   Hadoopy  https   github com bwhite hadoopy git         Speed comparison      2014  Matlab vs  Python Numpy Numba   http   blogs bu edu mhirsch 2014 11 speed of matlab vs python numpy numba         2014  Numpy  Cython  Fortran and OpenCL   http   ezietsman github io python 2014 09 06 parallel python on a gpu with opencl        Some Linear Algebra with Cython   http   nbviewer ipython org github carljv cython testing blob master cython linalg ipynb        2014  PERFORMANCE OF PANDAS SERIES VS NUMPY ARRAYS   http   penandpants com 2014 09 05 performance of pandas series vs numpy arrays         2014  Numpy vs C   and BLAS   http   stackoverflow com questions 7596612 benchmarking python vs c using blas and numpy        2013  Numba  NumPy and F2PY   http   combichem blogspot com 2013 04 fun with numba numpy and f2py html       my dotvimrc   my  vimrc file Test wrapping Fortran code by ctypes or cython pytraj demo    View the notebooks here   nbviewer   http   nbviewer ipython org github hainm pytraj demo tree master   Python projects using Fortran heavily     scipy   numpy    quippy   http   www jrkermode co uk quippy        Tips for wrapping Fortran amber things   my notes about Amber   nglview   bash nglview DOPC parm7  c DOPC rst7  j  AMBERHOME miniconda bin jupyter this folder has all the things that I don t know where to keep    Note for me  and for others too   so I don t need to remember or google     See  issues   https   github com hainm bag of tips issues    coding     font   Liberation Mono  12   builtin in scheme  black on light yellow   Text color  black   bg color  light yellow   Pallette  Linux console    pyspark       Use notebook remotely     PYSPARK DRIVER PYTHON ipython     PYSPARK DRIVER PYTHON OPTS  notebook   no browser   port 8889    bin pyspark         GCC   Install gcc5 3     trick  http   gcc gnu org wiki FAQ configure     Common Error       libgmp so 10  cannot open shared object file     locate libgmp so 10   export LD LIBRARY PATH  LD LIBRARY PATH  path to libgmp          How to use IPython  conda      in PHENIX     Upgrade pip   phenix python  m pip install pip   upgrade   Install readline   phenix python  m pip install readline   Install Ipython   phenix python  m pip install IPython   Alternate way is to use  conda   Run   phenix python  c  from IPython import start ipython start ipython     conda   phenix python  m pip install conda   install via conda   phenix python  m conda install package   update numpy    1 9 0   phenix python  m conda install numpy   To use  jupyter  notebook  create  py2  env with  anaconda  or  miniconda  distribution   man  too much work    Check  source phenix 2376     Install pytraj with PHENIX     cd  AMBERHOME AmberTools src pytraj    phenix python setup py clean    phenix python setup py install     Notes  does not work with pytraj in  AMBERHOME lib due to wrong Python version   although two Pythons are 2 7  ack    Crystal     cctbxwiki   xrayutilities     Install coot      bash conda install coot  c mw   update boots    1 59   conda install boost 1 59 0  c omnia   npm     401 error      fftools   force field related stuff  Just for backing my scripts    for testing  Nothing is interesting here  pymsmt   Python Metal Site Modeling Toolbox   It is developed by Pengfei Li in Prof  Kenne Merz s Research Group at Michigan State University  This software is free of charge and a version 1 0 Beta1 has been released in AmberTools15 package  It should be used with AmberTools15 due to it uses other code and files inside the packge  MSMs   Markov state models   Current Models   Src https   github com choderalab MSMs blob master src README md  Source  trajectory and prmtop files are converted from dcd and psf files from http   www ks uiuc edu Training Tutorials   Membrane Proteins Tutorial section  Introduction Steps       run  conda config   add channels https   conda binstar org ambermd  before build        Upload package to ambermd channel      login to you account in bash terminal  use your own account that has administration permission      bash     anaconda login     upload     bash     anaconda upload   user ambermd package tar bz2   See also  http   docs anaconda org using html UploadingPackagesToAnOrganization       Notes     There is no auto build for all packages yet         How    bash conda build cpptraj  conda build libcpptraj    run build sh pytraj    for py2 7  3 4  3 5   run build sh parmed    for py2 7  3 4  3 5  and so on        Remove package from anaconda   bash   anaconda remove ambermd package name       Package specific notes   pysander            For pysander  you need to have Amber built before building pysander    it requires the presence of libsander so and libsanderles so  as well as the sander h header file  inside the  AMBERHOME lib  and  AMBERHOME include  directories  These files are then copied to the lib and include directory for the build  and pysander is built against them  In order to generate a portable  standalone version of libsander so and libsander dylib  you need to configure Amber  without  the    with netcdf  flag  i e   you need Amber to build its own NetCDF   mdview   Trajectory viewer on Jupyter notebook   A fork from  mdtraj  sub package    This is my experiment  Not guarantee to make it work  but the overall aim is to build a package that can be integrated with  mdtraj    pytraj    mdanalysis    chemlab         License   Currently mdview license follows   mdtraj  license   https   github com mdtraj mdtraj blob master LICENSE   LGPLv2     Proposal    copied and lightly change from  mdtraj s website        python from mdview import TrajectoryView  enable notebook enable notebook     use mdtraj as backend   import mdtraj as md traj   md load fname  topology name  TrajectoryView traj  backend  mdtraj     use pytraj as backend   import pytraj as pt traj   pt load fname  topology name  TrajectoryView traj  backend  pytraj             Demo from other packages      http   istar cse cuhk edu hk iview    http   biasmv github io pv demo html     Why bother creating this        Developing a good GUI is very difficult  We should focus on a single package rather making different ones  mdtraj s view  chemview  pymol        And it s fun to collaborate        try to have BSD license      integrate with  jupyter notebook   https   jupyter org     why not using VMD  VMD is a beast and I really like it  But I love Python  it s hard to bring python to VMD      why not developing based on pymol  its license  not sure how to combine to jupyter notebook  many other things    make a light package  For example  if  pytraj  users want to use the trajectory view with jupyter notebook  they need to install  mdtraj  too  which instroduce additional level of complexity in installation      Current limitations of trajectove view in  mdtraj       mouse sensitivity  molecule rotation are not that great compareted to VMD  actually I don t see any program  even chimera  pymol  has that kind of mouse sensitivy like the one in VMD     don t have reprsentation for nucleic acid            Why naming  mdview       it s just temp name   I myself prefer short typing     Install       from conda       conda install mdview  c ambermd       from github     git clone https   github com mdview mdview   cd mview   python setup py install       Credits    For more details  please the license in each file      Hai Nguyen  patch for  pytraj   https   github com Amber MD pytraj     mdtraj s developers   https   github com mdtraj mdtraj   original python implementation   Jacob Kelley   Context js   https   github com jakiestfu Context js    Eli Grey   FileSaver js   https   github com eligrey FileSaver js     iview developers   https   github com HongjianLi iview     GLmol developers   https   github com biochem fan GLmol    mrdoob   three js   https   github com mrdoob three js    John Resig  jQuery   http   jquery org            An  IPython Jupyter  widget to interactively view molecular structures and trajectories  Utilizes the embeddable  NGL Viewer  for rendering  Support for showing data from the file system   RCSB PDB    simpletraj  and from objects of analysis libraries  mdtraj    pytraj    mdanalysis     Should work with Python 2 and 3  If you experience problems  please file an  issue     Table of contents     Installation   Usage   Interface classes   Changelog   License     Installation   From PyPI    pip install nglview    Note  The above will try to install  jupyter    traitlets  and  ipywidgets   as dependencies  If that fails install it manually  pip install jupyter     From Conda   conda install  c omnia nglview    Usage   Open a notebook   jupyter notebook    and issue   Python import nglview view   nglview show pdbid  3pqr      load  3pqr  from RCSB PDB and display viewer widget view   A number of convenience functions are available to quickly display data from the file system   RCSB PDB    simpletraj  and from objects of analysis libraries  mdtraj    pytraj    mdanalysis    ParmEd       Function                                   Description                                                                                                                                                     show structure file path                  Shows structure  pdb  gro  mol2  sdf  in  path             show pdbid pdbid                          Shows  pdbid  fetched from RCSB PDB                        show simpletraj struc path  traj path     Shows structure   trajectory loaded with  simpletraj       show mdtraj traj                          Shows  MDTraj  trajectory  traj                            show pytraj traj                          Shows  PyTraj  trajectory  traj                            show parmed structure                     Shows  ParmEd  structure    show mdanalysis univ                      Shows  MDAnalysis  Universe or AtomGroup  univ            API   Representations   python view add cartoon  protein   color  residueindex   view add surface  protein   opacity 0 3    Representations can also be changed by overwriting the  representations  property of the widget instance  view   The available  type  and  params  are described in the NGL Viewer  documentation     Python view representations           type    cartoon    params              sele    protein    color    residueindex                type    ball stick    params              sele    hetero             The widget constructor also accepts a  representation  argument       Python initial repr           type    cartoon    params              sele    protein    color    sstruc             view   nglview NGLWidget struc  representation initial repr  view       Properties      Python   set the frame number   view frame   100          Python   parameters for the NGL stage object   view parameters            percentages   dist  is distance too camera in Angstrom      clipNear   0   clipFar   100   clipDist   10        percentages  start of fog and where on full effect      fogNear   0   fogFar   100        background color      theme    dark           Multiple widgets   You can have multiple widgets per notebook cell    Python from ipywidgets widgets import Box w1   NGLWidget      w2   NGLWidget      Box children  w1 w2     License   Generally MIT  see the LICENSE file for details  MyList   List of things to remind me    3D viewer   ngl   chemview   pv   3dmol concept for amber phenix Python interface    Might need   python2 7  phenix only works with py2 version      phenix   https   www phenix online org      parmed   https   parmed github io ParmEd     pytraj   https   amber md github io pytraj     libsander   https   ambermd org  Non working code  just for my backup  if I even need them in the future  How        optinal  edit submit job sh      run  sbatch submit job sh     original decoy  has subfolder DecoyDiscrimination from github  just for copying files to   DecoyDiscrimination current folder   DO NOT   git clean  fdx  in this folder    Slides for AMBER16 meeting  March 2016      bash       jupyter notebook amber2016 update HaiNguyen ipynb  2  View  static HTML file       Note   monolayer xtc  and  monolayer pdb  were taken from mdtraj package       License  All notebooks  codes  figures are subjected to BSD license while trajecotry files are subjected  to GPL license     My backup for making tutorials for AMBER website credit belongs to  chemview   I adapt chemview test framework so  NGLView  can use it    How to test    bash git clone https   github com arose nglview cd nglview python setup py install git clone https   github com hainm nbtests jupyter notebook   port 8889   nightwatch PDZ ddG   predict ddG for PDZ domains docker build    t hainm pytraj build box juprog   Circle progress for Jupyter notebook   Basic Example      python   from time import sleep from juprog import CircleProgress   sequence   range 10  for x in CircleProgress sequence         fake long process     sleep 0 2          Install   Release   bash     pip install juprog   Development version   bash     pip install git https   github com hainm juprog   Acknowledgement   Use  progress circle  for displaying progress    arubenstein   kmb413   This is my temporary repo for AMBER methods  Sagar  Please check  Parse data  section       download   Sagar    git clone https   github com hainm mmgbsa decomp       For Hai  Run mmgbsa decomposition  Done       bash   n proteins   87   mpirun  n 87 python decomp mpi py   update   mmgbsa in if needed             Get energy for each residue for each structure      bash python   scripts get energy each snapshot py 1n7t07 wt   change 1n7t07 wt to other pdb folder too             Average energy  from 50 snapshots  for each residue in each protein          bash     pdb folder  res csv      tot   vdw    int   eel   pol   sas     int  Internal energy contributions      vdw  van der Waals energy contributions      eel  Electrostatic energy contributions      pol  Polar solvation free energy contributions      sas  Non polar solvation free energy contributions     tot  Total free energy contributions  sum of previous 5                Parse data   Sagar       bash cd 1be908 wt python    scripts parse data py residue number   update parse data py if needed   Expectation  This script will print out the energy for  residue number  for each component    tot  vwd  eel        Each array has length of n snapshots  rst7 files    to get a list of those snatshots  see below    and so on for other proteins             Get list of corrensponding rst7  coordinates  files   Sagar    bash sh scripts get rst7 filelist sh 1be908 wt       See also   Sagar    bash Section   Decomposition Data  in http   ambermd org doc12 Amber16 pdf  page 675        Get average energy per residue from 50 snapshots   Hai           bash    cd 1be908 wt    python    scripts process residue py      and so on for other proteins     TODO  better name         pymdgx     Playground with mdgx python interface  This repo is for developer    Nothing is serious yet    Examples      python import mdgx   prmtop    vAla3 prmtop  rst7    vAla3 rst7    with mdgx setup prmtop  rst7  as context        get energies and forces     mdgx ene  mdgx forces   context energy forces       set positions context positions   new positions          Install      bash amber python setup py install   prefix  AMBERHOME   or   make         Require     cython   numpy   AmberTools    16     Compare to sander energy and force   bash make test For pytraj energy testing  GNU LGPL v2    Wrapping some of AmberTools s programs for Jupyter notebook  For now  this is only for fun and you should not rely on this unstable API  Hopefully it s still useful    Install      bash git clone https   github com hainm jamber cd jamber python setup py install   If using ambertools   amber python setup py install         Require     AmberTools    16   nglview  optional   For visualization in Jupyter notebook     Example       notebooks       leap  python     from jamber import leap     command           source leaprc protein ff14SB     seq   sequence  ALA ALA ALA      saveamberparm seq seq prmtop seq rst7             leap run command        builder  python from jamber builder import build bdna build bdna seq  AAAAAA         build protein with given secondary structure         circleci   Build AmberTools with conda and docker  This is beta version      Update AmberTools version     bash       change v16 to v17     python scripts update ambertools version py 16 17       centos 5 derived image is used        Proposed usage    bash     conda install  c http   ambermd org conda  ambertools 16   current working version with python 3 5   conda install  c hainm ambertools 16   search   anaconda search ambertools           How to build     Make a git commit to this repo  Circleci  and travis  will do the rest  Built packages can retrieved from below url      bash     https   circleci com gh Amber MD ambertools conda build  build number  artifacts containers 0     Note  Need to replace  build number  by the commit number     e g      https   circleci com gh Amber MD ambertools conda build 153 artifacts containers 0     Or  build locally     by docker container  bash       update build sh if needed     sh build sh       by conda  bash conda build recipe           How continuous integration services are being used      travis   test building ambermini  full ambertools with GNU compiler  not use docker    circleci  test building ambermini with our   ambermd amber build box   docker image    Why  just for testing       Require   conda install flask conda build   Create a server   bash cd server python create database py   only do once python app py   Try to build and to install a dummy package and to send user info to database db   bash cd client conda build recipe conda install  conda build   output recipe    Check user database   http   localhost 8000 list amber win cross   Not in production yet     notes     mingw64 dll zip  https   groups google com forum   topic openblas users Xs0XLf K4CQ  wintrial   I know you re evil    Example   Install miniconda and some packages      bash bash install python sh   need to add  export PATH  HOME miniconda2 bin  PATH   to your  HOME  baschrc file         Install AmberTools      bash conda install  c hainm ambertools 16   yes   Note   hainm  channel will be replaced by something else    update  add   force   conda install  c hainm ambertools   force         Install     Require    ParmEd   tleap  optional          bash python setup py install   Usage      bash   pdb4amber   help   Some examples   simplest case  print output pdb   pdb4amber my pdb    simplest case  save output to file   pdb4amber my pdb  o out pdb   compat mode   pdb4amber  i my pdb  o out pdb   pipe   cat my pdb   pdb4amber  o out pdb   save to different formats  e g  mol2    pdb4amber my pdb  o out mol2   use  reduce  program to add hydgron   pdb4amber my pdb   reduce  o out pdb   process other formats  e g   cif    pdb4amber my cif   reduce  o out pdb   process from URL   pdb4amber https   raw githubusercontent com ParmEd ParmEd master test files 4LZT cif  o out pdb   fetch structure by its pdbid and process   pdb4amber 1tsu   pdbid   reduce  o out pdb   logfile   pdb4amber my pdb  o out pdb   logfile my log pdb4amber my pdb  o out pdb   logfile stdout       Test   bash py test  vs     Pull to amber repo      bash cd  AMBERHOME git remote add pdb4amber github https   github com amber md pdb4amber git fetch pdb4amber github git pull  s recursive  X subtree AmberTools src pdb4amber  X theirs   squash pdb4amber github master   diff   git diff pdb4amber github master master AmberTools src pdb4amber    stat   color     Linux     OSX    Aim  Testing AmberTools binary   Folder    HOME source code  HOME amber version  HOME miniconda  AMBERHOME bin amber run tests Run docker image as a server     start server     bash bash run hub sh       try       Paste  http   127 0 0 1 8000   to your web browser       Login with username  hello  and password 666666       add new user       Supposed you have a running server    bash   login to the docker container  jupyterhub    docker exec  it jupyterhub bash   add  new user  account   useradd  m  d  home new user   s  bin bash  G jupyterhub new user       Run jupyterhub without docker     setup    bash     require conda   source setup jupyterhub without docker sh jupyterhub           try       Paste  http   127 0 0 1 8000   to your web browser     Login with your username and password  from your account      Build docker image  developer    bash docker build  t ambermd amber jupyterhub     Tips     Use  ngrok  for port forwarding  safely       Disclaimer  I am not really sure if this is secure      Server   Download and unzip  ngrok  from https   ngrok com    Run    ngrok http 8000   You will see something like  http   xxxx ngrok io    localhost 8000     In another terminal  run  jupyterhub       User   Open web browser and paste  http   xxxx ngrok io   replace  xxxx  by actual address          User will see     Useful link     https   developer rackspace com blog deploying jupyterhub for education    Embarrassing parallel for beginner   Notes   package s name might be changed    Usage      python   make my script py filename   then run  mpirun  n 8 python my script py   change   n 8  to whatever core number   from map mpi import pmap   write your function with given command list   then call   data   pmap func  commands  if data is not None        do whatever       Install      bash pip install git https   github com amber md map mpi   require  numpy  mpi4py   conda install numpy mpi4py         Examples   Write code     cat my script py      python import subprocess from map mpi import pmap import time   def func sub commands       for cm in sub commands            fake expensive calculation         time sleep 1           subprocess check call cm split      commands     echo  hello   for   in range 8     you can also replace commands by another list of commands   e g   minimize 100 snapshorts in parallel   rst7 intput filename is inp  1  2  3      rst7   commands     sander  i min in  o min  i  out  p prmtop  c inp  i  rst7  r min  i  rst7  format i index    for index in range 100     pmap func  commands        Run code      bash   serial   time python my script py   parallel   time mpirun  n 8 python my script py       Output   serial      bash   time python my script py   hello   hello   hello   hello   hello   hello   hello   hello    real    0m8 268s user    0m0 173s sys 0m0 086s       parallel      bash   time mpirun  n 8 python my script py   hello   hello   hello   hello   hello   hello   hello   hello    real    0m1 496s user    0m1 431s sys 0m0 421s         Note   This is a demo package for this issue  https   github com jupyter widgets ipywidgets issues 2218       Install  pip install git https   github com hainm fullscreen jupyter nbextension install fullscreen   py   sys prefix jupyter nbextension enable  fullscreen   py   sys prefix       Acknowledgments     NGL   Fullscreen code   nglview   Fullscreen view and model       amber build box   To build docker image   Notes     The docker image will be automatically built in https   hub docker com r ambermd amber build box  if you make a commit here  So please make careful change  If you want to play with it  you should make another branch      Usage example     ambertools conda build   PolyTop   A topology builder for polymers   Click here to go to the web app    Features   Currently PolyTop only supports GROMACS  itp topologies and  pdb coordinates    Usage   This relies on the user already having coordinates   pdbs  and topologies   itps  for  each unit fragment of their molecule  You can get these at the  ATB     You may have to manually highlight the PDB file text and paste it into a text document  This  will get fixed      Load a PDB in with File   Load PDB   Load a topology for the current molecule with File   Load ITP  optional    Create  fragments  using the Fragment editor by selecting from the dropdown or picking the atom visually   Add molecules to your polymer by aligning fragments with fragments   Add multiple molecules at once   Save a polymer as a monomer  1 residue with all the atoms    Download the  pdb and the  itp   Undo and redo   Save your current molecules and fragments with File   Save Universe   Export an image with File   Export image   Keep an eye on your molecule with View   Show molecule information   Edit your molecule  delete atoms or move them to a different residue  with View   Show molecule editor   Rename your molecule by typing in the Name box     Error reporting   If something is happening that you think shouldn t  please raise an issue or  email me at   lily wang anu edu au  with the following    What you were trying to do   What it did   The current state  File   Save universe    What you can see in the Javascript console        Firefox   Ctrl Shift K        Chrome   Ctrl Shift I        Safari   Develop   Show Error Console  turn on Developer tools in Safari   Preferences   Advanced         Edge   F12   Console   There is likely an error message there in red  If there s a   arrow  try clicking on it and screenshotting the error trace       Browser and version   Molecule files     Thank you  3   To do     fix appearance of the unit alignment panel   follow Google guide to JS   fix width of molecule interaction on the stage where the dropdown and name are   accept all the coordinate formats NGL does   accept topology formats from other MD engines   fix z index placement of unit selection panels   document   add interface to customise visualisation past ball stick   add animation interface   highlight fragments in the side viewport when adding units   add user guide   make overlay panels move able     Authors     Lily Wang      lily wang anu edu au     License   This project is licensed under the MIT License   see the  LICENSE md  file for details   Acknowledgments   This project relies heavily on  NGL   Even the CSS is largely modified  from the NGL web app      NGL   PolyTop relies on NGL for the visualisation and coordinate handling logic    PolyTop s GUI is based largely off the NGL Web App GUI  which in turn is based off the three js editor  below        Selectize js    for the select menus in PolyTop   w2ui    for the GUI layout   three js   NGL relies on three js to interface with WebGL       sprintf js   Both NGL and PolyTop use sprintf to format text       jsfeat     from NGL    ESDoc    for documentation   Chroma js     from NGL    FlexiColorPicker     from NGL    Virtual DOM List   Font Awesome    for icons   JS Signals   Lightweight promise polyfill   pako   zlib port   Open Source PyMOL    screen aligned cylinder shader   VTK  Quadric shader code from the PointSprite Plugin   quadric surface center calculation   HyperBalls    hyperball stick shader   Chavent  M   Vanel  A   Tek  A   Levy  B   Robert  S   Raffin  B     Baaden  M   2011   GPU accelerated atom and dynamic bond visualization using hyperballs  a unified algorithm for balls  sticks  and hyperboloids  Journal of Computational Chemistry  32 13   2924 35   doi 10 1002 jcc 21861   TACCHEMZOO  TACC s Jupyter Notebook Environment for Computational Chemistry     Maintainer   Albert Lu   alu tacc utexas edu    Table of contents     Software   Tutorials   Examples   FAQ   Acknowledgment   Cite     Software   Platform and pre installed packages      Jupyter Notebook  5 5 0    Kernels  python3  Octave   Chemistry packages  ASE  GPAW  HOOMD Blue  Gromacs  LAMMPS  RDKit  AMBER     Gaussian     Gulp     NWChem     OpenKIM     Quantum ESPRESSO     Siesta     and VASP       no build in python interface     Pre installed python packages  matplotlib  numpy  pandas  pytables  scipy  sympy  Theano      For the full list of the software and details  please see  this page   Tutorials     Platform access via TACC Visualization Portal   Parallel Computing in Jupyter Notebook  1  ipyparallel     More  Tutorials   Examples     LAMMPS  Accelerate in phosphate   LAMMPS  Neb in neb hop1   LAMMPS  Peptide in peptide       More  Examples   FAQ   FAQ   Acknowledgment   Cite     Go to   Software    Tutorials    Examples    FAQ Usable but still  Work in progress    Codes are copied adapted from nglview and molstar  Will update the licenses    molstarview widget     molstarview  See the  dev  branch for now    Similar package s  that was bornt before  molstarview     https   github com janash pymolstar         Example     Load structure and trajectory      python from molstarview widget import MolstarView from nglview adaptor import SimpletrajTrajectory import nglview as nv   traj   SimpletrajTrajectory nv datafiles XTC  nv datafiles PDB  view   MolstarView   view add trajectory traj    struc   nv FileStructure  1tsu pdb   view add structure struc  view       Installation   To install use pip      pip install molstarview    For a development installation  requires  Node js  and  Yarn version 1        git clone https   github com molstar molstarview widget git   cd molstarview widget   pip install  e     jupyter nbextension install   py   symlink   overwrite   sys prefix molstarview   jupyter nbextension enable   py   sys prefix molstarview    When actively developing your extension for JupyterLab  run the command      jupyter labextension develop   overwrite molstarview    Then you need to rebuild the JS when you make a code change      cd js   yarn run build    You then need to refresh the JupyterLab page when your javascript changes    Acknowledgement     Thanks  nglview  for its code    Thanks  David Sehnal  for answering all questions about  molstar
49,virajprabhu,BranchAndBound   Implementation of Branch And Bound for 0 1 Knapsack and Travelling Salesman Problem  parallelized using OpenMP and MPI      Authored by Viraj Prabhu and Utkarsh Verma   Compiler Project Run Instructions  1   Import the project ImageResizer into Eclipse from the archive file provided  2   On line 32 of MasterAllocator java  change the third argument to the location of the bin folder of the project   3   Right click the project  and go to Run Configurations  4   Create a new Java Application Configuration for this project  select the MasterAllocator class as the Main class  5   Go to the arguments tab  and put the arguments in the following order      M N 6    Run using this configuration    Algorithm  The main method of the MasterAllocator class computes the number of slave processes to be spawned as  images m  where m is the threshold provided as a command line argument  we have found 1600 to be a suitable value for our size of dataset   and spawns them  Then it creates a server socket that listens for connections from the slave processes  On a successful connection  it sends a string containing names of 200  heuristically determined  images concatenated together and sends it     Having a reasonably large threshold m ensures that too many processes do not get spawned since process context switching is very time consuming  especially on a single system    The intuition behind sending 200 images is twofold  i   Some processes get scheduled more often and receive more CPU time  These processes should do more work  rather than dividing the  images equally among all processes at the beginning  ii  The amount of data sent and consequently I O blocking in the start of the program  when no other processes can run is reduced    The slave process connects to the server socket  receives the string and tokenizes it  It starts a threadpool  the size of which is min  cores   img rcvd N   where N is the threshold for the number of images per thread  It initializes the threadpool and passes successive string tokens to each thread which opens and resizes the image using the Graphics2D class and writes the resized image to the output directory  Once all string tokens are consumed  it tries to reconnect to the master and receive more images  When all images have been resized  the master sends a 0 which is recognized by the slave processes as the end of the program and they exit  after which the master decrements its count of the running processes  Once that reaches 0  the master process exits  pShuffle   pShuffle is an intelligent playlist organizer  README   Read Timer is a chrome extension built with JavaScript that provides an estimate of the amount of time it will take to read an article on a webpage    The present implementation starts off with a user s estimate of his her read speed to calculate the time  Speeds across websites are also tracked and used to refine this initial value and learn the user s actual read speed    Additionally  the user s current scroll position is used to provide an estimate of the time remaining in the article    The future scope of this extension could include        Providing a visualization of the progression of the user s read speed over time       Factoring in the complexity of the article while computing the estimate       Factoring in other intangible aspects such as user interest in the topic  nature of the article  for example  a comic  and so on        Contributing    1  Fork and clone the repo    2  Figure out something you d like to work on  say feature X  the milestones issues sections are generally good places to look   Do drop a comment saying you d like to work on such a feature    3  Choose a descriptive branch name for the feature  apart from master or develop    4  Checkout develop and create a new branch with this name  like so   git fetch upstream     git checkout develop     git merge upstream develop     git checkout  b your branch name   5  Make a commit to your feature branch with a descriptive commit message  like so    git commit  a  m    YOUR COMMIT MESSAGE HERE        git push origin   YOUR BRANCH NAME     6  Once done  create a pull request  You re done    Feel free to reach out in case of any doubts    Run instructions        Ensure you have the latest version of Google Chrome        Get the extension from the Chrome store at http   bit ly readtimer       Set your read speed estimate in the options tab       Click the icon while on a blog article to run the extension      A test driven JS assessment     This repo includes a set of tests that can be used to assess the skills of a candidate for a JavaScript position  or to improve one s own skills    I want to work on the tests  what do I do    To use the tests  you will need to install Node    you can do this via the  download page  or using  Homebrew  if you are on a Mac    Note that on Windows  there are some reports that you will need to restart after installing Node   see  12    You can clone or download this repo  Once you have done so  from the root directory of the repo  run    npm install npm start    You can then view the tests in your browser at  http   localhost 4444     When you visit that page  all of the tests should be failing  your job is to get the tests to pass  To do this  you ll need to refer to the tests in the files in the  tests app  directory  and edit the files in the  app   directory  Once you update a test  you can reload the test page in the browser to see whether it worked    You can also run  most of  the tests on the command line    npm test    The command line runner is a work in progress  contributions welcome      You can also develop with live reload and grunt watch if that s your thing    npm install  g grunt cli npm install grunt develop    I need help    There may be friendly folks willing to help you in  js assessment or  jshotline on freenode IRC    I want to contribute tests  what do I do    Submit a pull request  The tests are currently loosely organized by topic  so you should do your best to add tests to the appropriate file in  tests app   or create a new file there if you don t see an appropriate one  If you do create a new file  make sure to add it to  tests runner js   and to add a stub for the solution to the corresponding file in  app    Finally  it would be great if you could update the  answers  as well    Any substantial contributions will be duly credited in the readme  as well as of course in the git commit log    Data driven tests   If your tests need data that can be fetched via XHR  stick a   json  file in the  data  directory  you can access it at   data  filename  json     Available dependencies   The repo includes jQuery  Backbone  and Underscore  Do take advantage of these libraries when writing your solutions    I want to see the answers    First  bear in mind that looking up the answers is going to teach you a whole lot less than you ll learn by working on the tests  even if you occasionally get stuck  I d recommend only looking at the answers once you have the tests passing  to see if there s another way you could have approached the problem  When you re ready to look at the answers  you can find them  here   I ll do my best to keep them up to date    I hate   some technology you ve chosen    This repo uses  Mocha  and  Chai  for the tests themselves  It uses the BDD style for authoring tests  If this doesn t suit you  please fork away  or  better  submit a pull request that lets this be more flexible than it currently is    Todos   There are a number of things that would make this project better  check out the  issues  for details  pull requests welcome    License   Copyright   2012 2015 Rebecca Murphey      This work is licensed under a  Creative Commons Attribution ShareAlike 4 0 International License    quickmark    Use typeahead to bookmark efficiently  cmd ctrl   shift   b    todo      feature   display full path   deal with duplicate names      feature   add folder      feature   undo bookmark add    logic   better filtering      ui enhancements   dynamically resize popup height      ui enhancements   change icon on success   Kasper   This is a port of Ghost s default theme  Casper  for Jekyll   Feel free to fork  change  modify and re use it    How to use it   Simply clone this repository  and then run  jekyll serve  inside the directory    Kasper theme includes      Pagination   Rss   Google Analytics Tracking code   Code Syntax Highlight   Author s profile with picture   Disqus comments     Screenshots       Thanks   Most of the work has been already done by the Ghost team  I ve just ported Casper to Jekyll   I ve also added few things specific to Jekyll and some minor style changes    Copyright   License   Copyright  C  2013 Ghost Foundation   Released under the MIT License    Permission is hereby granted  free of charge  to any person obtaining a copy of this software and associated documentation files  the  Software    to deal in the Software without restriction  including without limitation the rights to use  copy  modify  merge  publish  distribute  sublicense  and or sell copies of the Software  and to permit persons to whom the Software is furnished to do so  subject to the following conditions    The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software    THE SOFTWARE IS PROVIDED  AS IS   WITHOUT WARRANTY OF ANY KIND  EXPRESS OR IMPLIED  INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT  IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM  DAMAGES OR OTHER LIABILITY  WHETHER IN AN ACTION OF CONTRACT  TORT OR OTHERWISE  ARISING FROM  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE  Pixyll   pixyll com     Pixyll is a simple  beautiful theme for Jekyll that emphasizes content rather than aesthetic fluff  It s mobile  first   fluidly responsive  and delightfully lightweight    It s pretty minimal  but leverages large type and drastic contrast to make a statement  on all devices    This Jekyll theme was crafted with  3 by  John Otander     4lpine           https   github com ee0703 pixyll zh cn     Getting Started   If you re completely new to Jekyll  I recommend checking out the documentation at  http   jekyllrb com  or there s a tutorial by  Smashing Magazine       git clone git github com johnotander pixyll git   cd pixyll   gem install bundler   If you don t have bundler installed   bundle install   If you don t want to install anything in your local machine  you can create a free development environment for this Pixyll project in the cloud on  Nitrous io  by clicking the button below          In the IDE  start Pixyll via  Run   Start Pixyll  and access your site via  Preview   3000     Verify your Jekyll version   It s important to also check your version of Jekyll since this project uses Native Sass which is  only supported by 2 0      Fork  then clone   Fork the repo  and then clone it so you ve got the code locally    Modify the   config yml   The   config yml  located in the root of the Pixyll directory contains all of the configuration details for the Jekyll site  The defaults are       yml   Site settings   title  Pixyll email  your email example com author  John Otander description   A simple  beautiful theme for Jekyll that emphasizes content rather than aesthetic fluff   baseurl     url   http   pixyll com    Build settings   markdown  kramdown permalink  pretty paginate  3       Jekyll Serve   Then  start the Jekyll Server  I always like to give the    watch  option so it updates the generated HTML when I make changes      jekyll serve   watch   Now you can navigate to  localhost 4000  in your browser to see the site    Using Github Pages   You can host your Jekyll site for free with Github Pages   Click here  for more information    A configuration tweak if you re using a gh pages sub folder   In addition to your github username github io repo that maps to the root url  you can serve up sites by using a gh pages branch for other repos so they re available at github username github io repo name    This will require you to modify the   config yml  like so       yml   Site settings   title  Repo Name email  your email example com author  John Otander description   Repo description  baseurl    repo name  url   http   github username github io    Build settings   markdown  kramdown permalink  pretty paginate  3       This will ensure that the the correct relative path is constructed for your assets and posts  Also  in order to run the project locally  you will need to specify the blank string for the baseurl     jekyll serve   baseurl        If you don t want the header to link back to the root url   You will also need to tweak the header include      site baseurl           html            site title                  include navigation html                         A relevant Jekyll Github Issue   https   github com jekyll jekyll issues 332   Contact Form   The contact form uses  http   formspree io   It will require you to fill the form out and submit it once  before going live  to confirm your email    More setup instructions and advanced options can be found at  http   formspree io   Disqus   To configure Disqus  set up a  Disqus site  with the same name as your site  Then  in   config yml   edit the  disqus shortname  value to enable Disqus    Customizing the CSS   All variables can be found in the   sass  variables scss  file  toggle these as you d like to change the look and feel of Pixyll    Page Animation   If you would like to add a  fade in down effect   you can add  animated  true  to your   config yml     AnchorJS   AnchorJS    A JavaScript utility for adding deep anchor links to existing page content  AnchorJS is lightweight  accessible  and has no dependencies   You can turn it on by toggling  enable anchorjs   Because it offers many ways for customization  tweaks should be done in   includes footer html   Default settings after turning AnchorJS on are       html        anchors options visible    always       anchors add  article h2  article h3  article h4  article h5  article h6             See  documentation  for more options    Put in a Pixyll Plug   If you want to give credit to the Pixyll theme with a link to  http   pixyll com  or my personal website  http   johnotander com  somewhere  that d be awesome  No worries if you don t    Web analytics and search engines   You can measure visits to your website either by using  Google Analytics  tracking embed or the more advanced  Google Tag Manager  container    For Google Analytics set up the value for  google analytics   it should be something like  google analytics  UA XXXXXXXX X     For Google Tag Manager set up the value for  google tag manager   it should be something like   google tag manager  GTM XXXXX      Do not  set both of above methods because this will cause conflicts and skew your reporting data    Remember that you need to properly configure the GTM container in its admin panel if you want it to work  More info is available in  GTM s docs     Your website is  by default  set to be allowed for crawling and indexing by search engines   Unless you made yourself a custom robots txt file   You can use front matter settings on each page to control how search engines will it  Sometimes you may want to exclude a particular page from indexing or forbid Google to store a copy of your page in its cache  It is up to you  Use the  meta robots  frontmatter key and assign values based on  this table   Some examples       yaml   exclude page from index   meta robots  noindex   allow indexing  disallow caching   meta robots  noarchive   allow indexing  disallow crawling links   meta robots  nofollow   disallow indexing  follow links   meta robots  noindex follow       In order to get more information about your website s status in search engines  you can register it in  Google Search Console  and or  Bing Webmaster Tools   Both these tools will ask you to authorize your website with them and there are couple of ways to do that  Pixyll supports verification via meta tags   just fill in values for  google verification  and or  bing verification  in   config yml   the verification strings and meta tags will then be added automatically    If search engine optimization is your thing  you can also set up  meta description  values for each page post  By default Pixyll uses  summary  to populate the   meta name  description  content         tag and falls back to  description  from   config yml  if  summary  is not present in page post s front matter  The  summary  is also used for generating Open Graph tags  Why would you want to use a dedicated variable for meta description  Because character limit to properly display this description in search results  as a snippet  is way smaller than in Open Graph  It is recommended to keep it at 155 160 characters  for more in depth info read  this article     And lastly   if you happen to write in language other than English be sure to change  og locale  in   config yml  to reflect it    Enjoy   I hope you enjoy using Pixyll  If you encounter any issues  please feel free to let me know by creating an  issue   I d love to help    Upgrading Pixyll   Pixyll is always being improved by its users  so sometimes one may need to upgrade    Ensure there s an upstream remote   If  git remote  v  doesn t have an upstream listed  you can do the following to add it    git remote add upstream https   github com johnotander pixyll git   Pull in the latest changes   git pull upstream master   There may be merge conflicts  so be sure to fix the files that git lists if they occur  That s it    Thanks to the following     BASSCSS   Jekyll   Refills   Solarized   Animate css     Contributing     Fork it   Create your feature branch   git checkout  b my new feature     Commit your changes   git commit  am  Add some feature      Push to the branch   git push origin my new feature     Create new Pull Request   deep image search Visdial RL PyTorch   PyTorch implementation of the paper    Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning  Abhishek Das   Satwik Kottur   Jos  Moura  Stefan Lee  and Dhruv Batra   https   arxiv org abs 1703 06585  ICCV 2017  Oral      Visual Dialog requires an AI agent to hold a meaningful dialog with humans in natural  conversational language about visual content  Given an image  dialog history  and a follow up question about the image  the AI agent has to answer the question    This repository contains code for training the  questioner  and  answerer  bots described in the paper  in both  supervised  fashion and via  deep reinforcement learning  on the Visdial 0 5 dataset for the cooperative visual dialog task of  GuessWhich          Table of Contents     Setup and Dependencies   Usage   Preprocessing VisDial   Extracting image features   Download preprocessed data   Pre trained checkpoints   Demo   Training   Evaluation   Logging   Benchmarks       Visualizing Results   Reference   License     Setup and Dependencies   Our code is implemented in PyTorch  v0 3 1   To setup  do the following      Install  Python 3 6   Install  PyTorch  v0 3 1  preferably with CUDA   running with GPU acceleration is highly recommended for this code  Note that PyTorch 0 4 is not supported    If you would also like to extract your own image features  install  Torch    torch hdf5    torch image     torch loadcaffe    and optionally  torch cutorch     torch cudnn    and  torch cunn   for GPU acceleration  Alternatively  you could directly use the precomputed features provided below    Get the source   git clone https   github com batra mlp lab visdial rl pytorch git visdial pytorch   Install requirements into the  visdial rl pytorch  virtual environment  using  Anaconda    conda env create  f env yml     Usage   Preprocess data using the following scripts OR directly download preprocessed data  below     Preprocessing VisDial   Download and preprocess VisDial as described in the  visdial  repo  Note  This requires  Torch  to run  Scroll down further if you would like to directly use precomputed features        cd data  python prepro py  version 0 5  download 1   To process VisDial v0 9  run      python prepro py  version 0 9  download 1  input json train visdial 0 9 train json      input json val visdial 0 9 val json   cd            This will generate the files  data visdial chat processed data h5   containing tokenized captions  questions  answers  and image indices   and  data visdial chat processed params json   containing vocabulary mappings and COCO image ID s     Extracting image features   To extract image features using VGG 19  run the following        sh data download model sh vgg 19 cd data   th prepro img vgg19 lua  imageRoot  path to coco images  gpuid 0       Similary  to extract features using  ResNet   run    sh data download model sh resnet 200 cd data th prepro img resnet lua  imageRoot  path to coco images  cnnModel  path to t7 model  gpuid 0   Running either of the above will generate  data visdial data img h5  containing features for COCO  train  and  val  splits    Download preprocessed data   Download preprocessed dataset and extracted features    sh scripts download preprocessed sh   Pre trained checkpoints   Download pre trained checkpoints    sh scripts download checkpoints sh   Demo   A demo of inference for question and answer generation is available in  inference ipynb     Training   The model definitions supported for training models are included in the  models   folder    we presently support the  hre ques lateim hist  encoder with  generative  decoding    The arguments to  train py  are listed in  options py   There are three training modes available    sl abot    sl qbot  for supervised learning  pre training  of A Bot and Q Bot respectively  and  rl full QAf  for RL fine tuning of both A Bot and Q Bot beginning from specified SL pre trained checkpoints   full QAf  denotes that all components of each agent  dialog component and or image prediction component  are fine tuned    For supervised pre training    python train py  useGPU  trainMode sl abot   For RL fine tuning    python train py  useGPU       trainMode rl full QAf       startFrom checkpoints abot sl ep60 vd       qstartFrom checkpoints qbot sl ep60 vd   Evaluation   The three types of evaluation    1  ranking A Bot s answers   2  ranking Q Bot s image predictions and  3  ranking Q Bot s predictions when interacting with an A Bot  are arguments  QBotRank    ABotRank  and  QABotsRank  respectively to  evalMode   Any subset of them can be given as a list to  evalMode     For evaluation of Q Bot on image guessing and A Bot on answer ranking on human human dialog  ground truth captions  questions and answers   the following command can be used    python evaluate py  useGPU        startFrom checkpoints abot sl ep60 vd        qstartFrom checkpoints qbot sl ep60 vd        evalMode ABotRank QBotRank   For evaluation of Q Bot on image guessing when interacting with an A Bot  the following command can be used  Since no human human dialog  ground truth  is shown to the agents at this stage  ground truth captions are not used  Instead  captions need to be read from  chat processed data gencaps h5   which contains preprocessed captions generated from  neuraltalk2   This file provides the VisDial 0 5 test split where original ground truth captions are replaced by generated captions    python evaluate py  useGPU        inputQues data visdial chat processed data gencaps h5        startFrom checkpoints abot sl ep60 vd        qstartFrom checkpoints qbot sl ep60 vd        evalMode QABotsRank   Logging   The code supports logging several metrics via  visdom   These include train and val loss curves  VisDial metrics  mean rank  reciprocal mean rank  recall 1 5 10 for the answerer  and percentile mean rank for the questioner   To enable visdom logging  use the  enableVisdom  option along with other visdom server settings in  options py   A standalone visdom server can be started using    python  m visdom server  p  port    Now you can navigate to  localhost  port   on your local machine and select the appropriate environment from the drop down to visualize the plots  For example  if I want to start a  sl abot  job which logs plots by connecting to the above visdom server  hosted at  localhost  port     the following command may be used to create an environment titled  my abot job   python train py  useGPU        trainMode sl abot        enableVisdom 1        visdomServer http   127 0 0 1        visdomServerPort  port         visdomEnv my abot job   Benchmarks   Here are some benchmarked results for both Agents on the VisDial 0 5 test split    Questioner   The plots below show percentile mean rank  PMR  numbers obtained on evaluating the questioner for the SL pretrained and RL full QAf settings  when evaluated on generated dialog  with the two agents interacting with each other along with being provided a generated caption instead of ground truth  based image retrieval    We have also experimented with other hyperparameter settings and found that scaling the cross entropy loss lead to a significant improvement in PMR  Namely  setting  CELossCoeff  to  1  and  lrDecayRate  to  0 999962372474343  lead to the PMR values shown on the right  The corresponding pre trained checkpoints are available for download and are denoted by a   delta  suffix    Note that RL fine tuning begins with annealing i e  the RL objective is gradually eased in from the last round  round 10  to the first round of dialog  Every epoch after the first one begins be decreasing the number of rounds for which supervised pre training is used  The following plots show the RL Full QAf model results at epoch 10  when annealing ends  as well as epoch 20    Note that RL fine tuning begins with annealing i e  the RL objective is gradually eased in from the last round  round 10  to the first round of dialog  Every epoch after the first one begins be decreasing the number of rounds for which supervised pre training is used  The following plots show the RL Full QAf model results at epoch 10  when annealing ends  as well as epoch 20  10 epochs of only RL            Answerer   The table below shows evaluation performance of the trained answerer on the VisDial answering metrics  These metrics measure the answer retrieval performance of the A Bot given image  human human  ground truth  dialog and ground truth caption as input  Note that the epoch number is consistent across A Bot and Q Bot  The SL pretraining epoch denotes the checkpoint from which the corresponding RL finetuning was started       Checkpoint   Epoch     MR     MRR      R1      R5     R10                                                                      SL Pretrain     60    21 94   0 432   33 21   52 67   59 23     RL Full QAf     10    21 63   0 434   33 29   53 10   59 70     RL Full QAf     20    21 58   0 433   33 22   53 09   59 64     Similar as above  we find better performance for the    Delta  hyperparameter setting  which downscales scales the cross entropy loss           Checkpoint      Epoch     MR     MRR      R1      R5     R10                                                                            SL Pretrain Delta     15    21 02   0 434   33 02   53 51   60 45     RL Full QAf Delta     10    21 61   0 410   30 38   51 61   59 30     RL Full QAf Delta     20    22 80   0 377   26 64   49 48   57 46     Visualizing Results   To generate dialog for visualization  run  evaluate py  with  evalMode  set to  dialog     python evaluate py  useGPU        startFrom checkpoints abot rl ep20 vd        qstartFrom checkpoints qbot rl ep20 vd        evalMode dialog        beamSize 5   This generates a json file  dialog output results results json   Now to visualize the generated dialog  run   cd dialog output  python  m http server 8000   Navigate to  localhost 8000  in your browser to see the results  The page should look as follows      Reference   If you use this code as part of any published research   please cite this repo as well as Das and Kottur et  al   Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning         misc modhe2018visdialrlpytorch    author    Modhe  Nirbhay and Prabhu  Viraj and Cogswell  Michael and Kottur  Satwik and Das  Abhishek and Lee  Stefan and Parikh  Devi and Batra  Dhruv       title    VisDial RL PyTorch      year    2018      publisher    GitHub      journal    GitHub repository      howpublished     url https   github com batra mlp lab visdial rl git       commit           inproceedings das2017visdialrl    title  Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning     author  Abhishek Das and Satwik Kottur and Jos  e M F  Moura and     Stefan Lee and Dhruv Batra     booktitle  Proceedings of the IEEE International Conference on Computer Vision  ICCV      year  2017          Acknowledgements   We would like to thank  Ayush Shrivastava  for his help with testing this codebase    License   BSD PyTorch Code for  SENTRY  Selective Entropy Optimization via Committee Consistency for Unsupervised Domain Adaptation   ICCV 2021    Viraj Prabhu  Shivam Khare  Deeksha Kartik  Judy Hoffman   Many existing approaches for unsupervised domain adaptation  UDA  focus on adapting under only data distribution shift and offer limited success under additional cross domain label distribution shift  Recent work based on self training using target pseudolabels has shown promise  but on challenging shifts pseudolabels may be highly unreliable and using them for self training may cause error accumulation and domain misalignment  We propose Selective Entropy Optimization via Committee Consistency  SENTRY   a UDA algorithm that judges the reliability of a target instance based on its predictive consistency under a committee of random image transformations  Our algorithm then selectively minimizes predictive entropy to increase confidence on highly consistent target instances  while maximizing predictive entropy to reduce confidence on highly inconsistent ones  In combination with pseudolabel based approximate target class balancing  our approach leads to  significant improvements over the state of the art on 27 31 domain shifts from standard UDA benchmarks as well as benchmarks designed to stress test adaptation under label distribution shift      Table of Contents     Setup and Dependencies   Usage   Train and adapt model   Download data   Pretrained checkpoints       Reference   License     Setup and Dependencies     Create an anaconda environment with Python 3 6   conda create  n sentry python 3 6 8                                    and activate   conda activate sentry   Navigate to the code directory   cd code    Install dependencies   pip install  r requirements txt     And you re all set up     Usage   Download data   Data for SVHN  MNIST is downloaded automatically via PyTorch  Data for other benchmarks can be downloaded from the following links  The splits used for our experiments are already included in the  data   folder   1   DomainNet   2   OfficeHome  3   VisDA2017   only train and validation needed    Pretrained checkpoints   To reproduce numbers reported in the paper  we include a a few pretrained checkpoints  We include checkpoints  source and adapted  for SVHN to MNIST  DIGITS  in the  checkpoints  directory  Source and adapted checkpoints for Clipart to Sketch adaptation  from DomainNet  and Real World to Product adaptation  from OfficeHome RS UT  can be downloaded from  this link   and should be saved to the  checkpoints source  and  checkpoints SENTRY  directory as appropriate    Train and adapt model     Natural label distribution shift  Adapt a model from   to   for a given    where benchmark may be DomainNet  OfficeHome  VisDA  or DIGITS   as follows      python train py   id  experiment id                      source  source                      target  target                      img dir  image directory                      LDS type  LDS type                      load from cfg True                     cfg file  config  benchmark   cfg file  yml                      use cuda True   SENTRY hyperparameters are provided via a  sentry yml  config file in the corresponding  config  benchmark   folder  On DIGITS  we also provide a config for baseline adaptation via DANN   The list of valid source target domains per benchmark are       DomainNet  real  clipart  sketch  painting      OfficeHome RS UT  Real World  Clipart  Product      OfficeHome  Real World  Clipart  Product  Art      VisDA2017  visda train  visda test      DIGITS  Only svhn  source  to mnist  target  adaptation is currently supported    Pass in the path to the parent folder containing dataset images via the    img dir  name of directory   flag  eg     img dir    data DomainNet     Pass in the label distribution shift type via the    LDS type  flag  For DomainNet  OfficeHome  standard   and VisDA2017  pass in    LDS type  natural    default   For OfficeHome RS UT  pass in    LDS type  RS UT    For DIGITS  pass in    LDS type  as one of  IF1    IF20    IF50   or  IF100   to load a  manually  long tailed target training split with a given imbalance factor  IF   as described in Table 4 of the paper     To load a pretrained DA checkpoint instead of training your own  additionally pass    load da True  and    id  benchmark name   to the script above  Finally  the training script will log performance metrics to the console  average and aggregate accuracy   and additionally plot and save some per class performance statistics to the  results   folder    Note  By default this code runs on GPU  To run on CPU pass     use cuda False   Reference   If you found this code useful  please consider citing    inproceedings prabhu2021sentry    title  SENTRY  Selective Entropy Optimization via Committee Consistency for Unsupervised Domain Adaptation     author  Prabhu  Viraj and Khare  Shivam and Kartik  Deeksha and Hoffman  Judy     booktitle  Proceedings of the IEEE CVF International Conference on Computer Vision     pages  8558  8567     year  2021      Acknowledgements   We would like to thank the developers of PyTorch for building an excellent framework  in addition to the numerous contributors to all the open source packages we use    License   MIT Code for  Active Domain Adaptation via Clustering UNcertainty weighted Embeddings   ICCV 2021    Viraj Prabhu  Arjun Chandrasekaran  Kate Saenko  Judy Hoffman   Generalizing deep neural networks to new target domains is critical to their real world utility  In practice  it may be feasible to get some target data labeled  but to be cost effective it is desirable to select a maximally informative subset via active learning  AL   We study the problem of AL under a domain shift  called Active Domain Adaptation  Active DA   We demonstrate how existing AL approaches based solely on model uncertainty or diversity sampling are less effective for Active DA  We propose Clustering Uncertainty weighted Embeddings  CLUE   a novel label acquisition strategy for Active DA that performs uncertainty weighted clustering to identify target instances for labeling that are both uncertain under the model and diverse in feature space  CLUE consistently outperforms competing label acquisition strategies for Active DA and AL across learning settings on 6 diverse domain shifts for image classification      Table of Contents     Setup and Dependencies   Usage   Train Active Domain Adaptation model   Download data   Pretrained checkpoints   Evaluation and plotting Results   Demo       Reference   License     Setup and Dependencies     Create an anaconda environment with  Python 3 6  and activate    conda create  n CLUE python 3 6 8 conda activate CLUE   Navigate into the code directory   cd CLUE    Install dependencies   Takes  2 3 minutes    pip install  r requirements txt      If running the demo  Install nb conda   conda install  c anaconda nb extensions nb conda       And you re all set up     Usage   Train Active Domain Adaptation model   Run  python train py  to train an active adaptation model from scratch  by passing it appropriate arguments    We include hyperparameter configurations to reproduce paper numbers on DIGITS and DomainNet as configurations inside the  config  folder  For instance  to reproduce DIGITS  SVHN  MNIST  results with CLUE MME  run    python train py   load from cfg True                      cfg file config digits clue mme yml                     use cuda False   To run a custom train job  you can create a custom config file and pass it to the train script  Pass    use cuda False  if you d like to train on CPU instead    Download data   Data for SVHN  MNIST is downloaded automatically via PyTorch  For DomainNet  follow the following steps  1  Download the original dataset for the domains of interest from  this link    eg  Clipart and Sketch  2  Run    python preprocess domainnet py   input dir  input directory                                     domains  clipart sketch                                     output dir  data     Pretrained checkpoints   At round 0  active adaptation begins from a model trained on the source domain  or from a model first trained on source and then adapted to the target via unsupervised domain adaptation  Checkpoints for reproducing DIGITS experiments have been included in the  checkpoints   directory  and those for reproducing DomainNet results on Clipart  Sketch can be downloaded at  this link   Note that checkpoints for models after active adaptation are not included    Evaluation and plotting Results   Run  python evaluate py  by passing it appropriate arguments  see file for instructions   It will pretty print raw results as well as save them as a figure in the  plots   directory  By default  it will generate a figure comparing CLUE   MME against a subset of representative Active DA and AL baselines and save it to the  plots   directory    Demo     Start a jupyter notebook with    jupyter notebook     and set the conda environment to adaclue   Run the Jupyter notebook  demo ipynb   which will walk you through    Loading SVHN  MNIST datasets and pretrained checkpoints   Label acquisition with baseline strategies and CLUE MME   Training  on CPU  with acquired labels   Plotting performance after one round of Active DA on SVHN  MNIST         Reference   If you found this code useful  please consider citing    inproceedings prabhu2021active    title  Active domain adaptation via clustering uncertainty weighted embeddings     author  Prabhu  Viraj and Chandrasekaran  Arjun and Saenko  Kate and Hoffman  Judy     booktitle  Proceedings of the IEEE CVF International Conference on Computer Vision     pages  8505  8514     year  2021      Acknowledgements   We would like to thank the developers of PyTorch for building an excellent framework  the  Deep Active Learning  repository for implementations of some of our baselines  and the numerous contributors to all the open source packages we use    License   MIT PyTorch Implementation of  Adapting Self Supervised Vision Transformers by Probing Attention Conditioned Masking Consistency    NeurIPS 2022    Viraj Prabhu   Sriram Yenamandra   Aaditya Singh  Judy Hoffman      equal contribution     Visual domain adaptation  DA  seeks to transfer trained models to unseen  unlabeled domains across distribution shift  but approaches typically focus on adapting convolutional neural network architectures initialized with supervised ImageNet representations  In this work  we shift focus to adapting modern architectures for object recognition    the increasingly popular Vision Transformer  ViT     initialized with modern pretraining based on self supervised learning  SSL   Inspired by the design of recent SSL approaches based on learning from partial image inputs generated via masking or cropping    either by learning to predict the missing pixels  or learning representational invariances to such augmentations    we propose PACMAC  a two stage adaptation algorithm for self supervised ViTs     inproceedings prabhu2022adapting    author     Prabhu  Viraj and Yenamandra  Sriram and Singh  Aaditya and Hoffman  Judy     title      Adapting Self Supervised Vision Transformers by Probing Attention Conditioned Masking Consistency     booktitle    Neural Information Processing Systems  NeurIPS      year       2022      This codebase is built upon the official  repository  of  MAE   Masked Autoencoders Are Scalable Vision Learners  paper    Installation instructions     Create a new conda environment and install requirements as follows   conda env create  f environment yml  n pacmac     Following MAE  this repo uses  timm  0 3 2   for which a  fix  is needed to work with PyTorch 1 8 1         Download MAE pretrained IN1k model  wget  nc https   dl fbaipublicfiles com mae visualize mae visualize vit base pth       Download OfficeHome DomainNet VisDA datasets inside data  directory    Step 1  In domain MAE pretraining   To pre train on a pair of domains from OfficeHome  eg  Clipart and Product   run the following on a node with 4 GPUs   python  m torch distributed launch   nproc per node 4 main pretrain py   batch size 128   epochs 800                     accum iter 2   model mae vit base patch16   input size 224   mask ratio 0 75                     data path data officehome                     output dir outputs officehome pretrain pretrain   SOURCE    TARGET                      log dir logs officehome pretrain pretrain   SOURCE    TARGET                      resume  RESUME PATH   no pin mem                     domains data OfficeHome txt source   SOURCE  cls txt                     target domains data OfficeHome txt source   TARGET  cls txt                     world size 4   blr 1e 4   no pin mem                     weight decay 5e 2   rand augs 3   rand aug severity 4   Step 2  Fine tune pretrained model on source data   To finetune a pretrained model using source data use the command below  Optionally  specify    target eval domains  to monitor accuracy on a target domain    python  m torch distributed launch main finetune py             accum iter 4   batch size 256   model vit base patch16             finetune   PRETRAINED PATH    epochs 100   blr 5e 5   layer decay 0 65             min lr 2e 4   weight decay 0 05   drop path 0 1             data path data officehome   nb classes 65             domains data OfficeHome txt source   SOURCE  cls txt             target eval domains data OfficeHome txt target   TARGET  cls txt             output dir outputs officehome finetune pretrained mae s t   SOURCE    TARGET              log dir logs officehome finetune pretrained mae s t   SOURCE    TARGET              target eval freq 10   rand augs 1   rand aug severity 2             source eval freq 10   ckpt save freq 50   Step 3  Adapt to unlabeled target domain   Finally  to adapt the source finetuned model to a target domain use the following command    python  m torch distributed launch main pacmac adapt py                                     accum iter 8   batch size 64   model vit base patch16                                     resume  FINETUNED PATH                                     epochs 100   blr 1e 4   nb classes 65   min lr 2e 4   weight decay 0 05                                     dist eval   data path data officehome                                     source domains data OfficeHome txt source    SOURCE   cls txt                                     target domains data OfficeHome txt target    TARGET   cls txt                                     output dir outputs officehome adapt    SOURCE      TARGET                                       drop path 0 1   log dir logs officehome adapt    SOURCE      TARGET                                       target eval freq 10   source eval freq 10   rand augs 1   rand aug severity 2                                     layer decay 0 65   ckpt save freq 50                                     warmup epochs 5   no pin mem                                     conf threshold 0 5                                     committee size 2                                     attention seeding True   We also provide commands for running our experiments on DomainNet shifts inside the  commands domainnet  directory    Pretrained checkpoints   We release ViT Base model checkpoints that are pretrained using MAE and DINO on the OfficeHome Clipart   rightarrow  Product shift        Pretraining method           Checkpoint       MAE   download  800 epochs        DINO   download  200 epochs        License   This project is under the CC BY NC 4 0 license  See  LICENSE  for details  PyTorch Code for Selective Self training by Probing Predictive Consistency for Unsupervised Domain Adaptation   Viraj Prabhu  Shivam Khare  Deeksha Kartik  Judy Hoffman   Many existing approaches for unsupervised domain adaptation  UDA  focus on adapting under only data distribution shift and offer limited success under additional cross domain label distribution shift  Recent work based on self training using target pseudolabels has shown promise  but on challenging shifts pseudolabels may be highly unreliable and using them for self training may cause error accumulation and domain misalignment  We propose Augmentation Consistency guided Self training  AUGCO   a UDA algorithm that probes the reliability of a target instance based on its predictive consistency under random image transformations  Our algorithm then selectively increases model confidence on consistent target instances  while reducing it on inconsistent ones  AUGCO achieves significant improvements over the state of the art on 27 31 domain shifts for classification adaptation  Further  AUGCO is directly extensible to other practical settings    to source free UDA  wherein source data is no longer available due to memory or compute restrictions  and to adapting semantic segmentation models  achieving strong performance in each       Table of Contents     Setup and Dependencies   Usage   Train and adapt model   Download data   Pretrained checkpoints       Reference   License     Setup and Dependencies     Create an anaconda environment with Python 3 6   conda create  n sentry python 3 6 8                                    and activate   conda activate sentry   Navigate to the code directory   cd code    Install dependencies   pip install  r requirements txt     And you re all set up     Usage   Download data   Data for SVHN  MNIST is downloaded automatically via PyTorch  Data for other benchmarks can be downloaded from the following links  The splits used for our experiments are already included in the  data   folder   1   DomainNet   2   OfficeHome  3   VisDA2017   only train and validation needed    Pretrained checkpoints   To reproduce numbers reported in the paper  we include a a few pretrained checkpoints  We include checkpoints  source and adapted  for SVHN to MNIST  DIGITS  in the  checkpoints  directory  Source and adapted checkpoints for Clipart to Sketch adaptation  from DomainNet  and Real World to Product adaptation  from OfficeHome RS UT  can be downloaded from  this link   and should be saved to the  checkpoints source  and  checkpoints SENTRY  directory as appropriate    Train and adapt model     Natural label distribution shift  Adapt a model from   to   for a given    where benchmark may be DomainNet  OfficeHome  VisDA  or DIGITS   as follows      python train py   id  experiment id                      source  source                      target  target                      img dir  image directory                      LDS type  LDS type                      load from cfg True                     cfg file  config  benchmark   cfg file  yml                      use cuda True   SENTRY hyperparameters are provided via a  sentry yml  config file in the corresponding  config  benchmark   folder  On DIGITS  we also provide a config for baseline adaptation via DANN   The list of valid source target domains per benchmark are       DomainNet  real  clipart  sketch  painting      OfficeHome RS UT  Real World  Clipart  Product      OfficeHome  Real World  Clipart  Product  Art      VisDA2017  visda train  visda test      DIGITS  Only svhn  source  to mnist  target  adaptation is currently supported    Pass in the path to the parent folder containing dataset images via the    img dir  name of directory   flag  eg     img dir    data DomainNet     Pass in the label distribution shift type via the    LDS type  flag  For DomainNet  OfficeHome  standard   and VisDA2017  pass in    LDS type  natural    default   For OfficeHome RS UT  pass in    LDS type  RS UT    For DIGITS  pass in    LDS type  as one of  IF1    IF20    IF50   or  IF100   to load a  manually  long tailed target training split with a given imbalance factor  IF   as described in Table 4 of the paper     To load a pretrained DA checkpoint instead of training your own  additionally pass    load da True  and    id  benchmark name   to the script above  Finally  the training script will log performance metrics to the console  average and aggregate accuracy   and additionally plot and save some per class performance statistics to the  results   folder    Note  By default this code runs on GPU  To run on CPU pass     use cuda False   Reference   If you found this code useful  please consider citing    article prabhu2021sentry    title  Selective Self training by Probing Predictive Consistency for Unsupervised Domain Adaptation     author  Prabhu  Viraj and Khare  Shivam and Kartik  Deeksha and Hoffman  Judy     year  2022      Acknowledgements   We would like to thank the developers of PyTorch for building an excellent framework  in addition to the numerous contributors to all the open source packages we use    License   MIT
50,leegao,See lua   An introspection library for Lua 5 1  5 2  5 3  and LuaJIT      lua     see  G    G 37                VERSION    Lua 5 3                     arg 1   assert              bit32 12            collectgarbage      coroutine 7   debug 16            dofile              error               getmetatable     io 14               ipairs              load                loadfile     math 35             next                os 11               package 8   pairs               pcall               print               rawequal     rawget              rawlen              rawset              require     see object  query   select              setmetatable        string 17   table 7             tonumber            tostring            type     utf8 6              xpcall              Demo               Installation   see lua  depends on either Lua 5 1 and above or LuaJIT  In addition  since  see lua  disassembles user functions automatically  it depends on the  bit32  library for byte stream manipulation    bash   luarocks install see   Usage   lua local see   require  see  see  G   print error       Documentation   Lua is a wonderful little language that lets you do a lot of cool stuff  However it s not very friendly to curious people  For example  let s say that we were just given a random library       lua     local parser   require  luainlua lua parser          and we want to see what s offered       lua     parser table  0x913e00         Wait wait wait  what is it with all these numbers  All I wanted to do is to know what s inside the  parser  table   Now there s an easy solution for these types of situations       lua     see parser   convert token       default action                          epilogue       grammar 84          ll1 83              prologue stream      Metatable     call this  str        Oooh  now that s fancy  Notice how tables are listed with their size and functions with their parameters   Even functions with variadic parameters         are listed correctly  Notice too that the metatables  are listed as well  In this case  the  parser  library contains a single    call  element that takes in a string  presumably the string to be parsed    Now  does this work with lists as well       lua     list      for i   1  200 do table insert list  i  end see list   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  18  19  20   21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40   41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60   61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80   81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99   100     101     102     103     104     105     106     107     108     109   110     111     112     113     114     115     116     117     118     119   120     121     122     123     124     125     126     127     128     129   130     131     132     133     134     135     136     137     138     139   140     141     142     143     144     145     146     147     148     149   150     151     152     153     154     155     156     157     158     159   160     161     162     163     164     165     166     167     168     169   170     171     172     173     174     175     176     177     178     179   180     181     182     183     184     185     186     187     188     189   190     191     192     193     194     195     196     197     198     199   200          Wow  it even lines up all of the columns for you    Let s go back to the  parser  example  Notice how there are  84  elements in the  parser grammar  table  We can actually  see  them as well via      lua     see parser  grammar   luainlua lua parser table lua        args maybe 1 3      args 4              assignment star 1 3   assignment 3        assignment or call group 1 5            binop 16   assignment or call maybe 1 3            assignment or call star 1 4   assignment or call 2                    block maybe 1 3     block star 1 3   block 2             exp  3              exp2  3             exp2 2   exp3  3             exp3 2              exp4 group 1 2      exp4 maybe 1 3   exp4 2              exp5  4             exp5 2              exp6  3   exp6 2              exp7 3              exp8 group 1 2      exp8 maybe 1 3   exp8 2              exp 2               exp stop star 1 4   exp stop 10   explist group 1 2   explist star 1 3    explist 2           field maybe 1 3   field maybe 2 3     field maybe 3 3     field 5             fieldsep 3   funcbody maybe 1 3                      funcbody 2          funcname star 1 3   funcname group 1 2                      funcname group 2 2   funcname maybe 1 3                      funcname 2          functiondef 2   label 2             level1 2            level2 2            level3 7   level4 2            level5 3            level6 4            level7 4   level8 2            namelist group 1 2                      namelist star 1 3   namelist 2          parlist group 1 2   parlist group 2 3   parlist star 1 4   parlist 2           primaryexp 3        retstat maybe 1 3   retstat maybe 2 3   retstat 2           root 2              stat group 1 group 1 2   stat group 1 maybe 1 3                  stat group 1 3      stat group 2 4   stat group 2 group 1 2                  stat group 2 maybe 1 3   stat group 3 2      stat group 4 2      stat maybe 1 3      stat star 1 3   stat 13             suffix 5            tableconstructor star 1 3   tableconstructor 2                      unop 4      see parser  grammar root  table 2        variable     root      see parser  grammar root 1     block        action  1        So we know that  parser grammar root 1       block   action   function  1      end      Now  imagine that I m debugging these grammars  I would like to know a little bit more about the   action  1   function  By selecting it  you can view its metadata       lua     see parser  grammar root 1  action function  1     home leegao distro install share lua 5 1 luainlua lua parser lua 421 423          In fact  if the source code is present  you can even view it directly      lua     see parser  grammar root 1  action sourcecode     GRAMMAR  grammar  root   1  action   function  1    return   1 end       Neato    What s more  you can also select the metatable using the   mt  field  like so      lua     see parser   convert token       default action                          epilogue       grammar 84          ll1 83              prologue stream      Metatable     call this  str      see parser  mt    call this  str    see parser  mt   call sourcecode    call   function this  str    local tokens        for    token in ipairs this prologue str   do     table insert        tokens        setmetatable          token             tostring   function self  return this convert self  end      end   local result   this ll1 parse tokens    return this epilogue result  end           We can also supply an optional query into  see  to highlight our results   For example  suppose that we want to only see functions related to tan in the mathematics library       lua     see math   tan    log             max             acos            huge   1  INF   ldexp     pi   3 1415926535898            cos              h          pow     deg                         cosh            sinh            random     randomseed      frexp           ceil            floor           rad     abs             sqrt            modf            asin            min     mod             fmod            log10           a 2         exp     sin             a       see math   tan   atan      function       native            On Ansi compatible terminals  you would see actual highlighting  whereas they are replaced by tags      on windows  Syntax highlighting rules for Lua    See http   leegao github com ACE Lua for a live demo  PARDON PRIVATE MANNING http   dl dropbox com u 25316665 eclipse zip laughing octo bot   test   I have no fucking idea Welcome to Lua  in Lua    Hey  I m glad you ve found me  I am a somewhat correct compiler for the  Lua 5 2  language into the Lua Bytecode format  but more than that  I hope that I can become a useful guide for those who are interested in language implementation    I grew out of a  month long sprint  from an initiative to complete a side project within the span of the November of 2015  but I wanted to keep tredding on    Installation   Make sure that you have Lua 5 2 with  luarocks  installed  Note that it s possible that you may have  luarocks  configured with  luajit   In this case  the LUA PATH ought to stay the same  so you can still use the  luainlua  package  you just won t be able to run  lua lua     bash git clone https   github com leegao LuaInLua git cd LuaInLua luarocks make   And voila  the package  luainlua  and the script  lua lua  will be installed    Usage   luainlua  comes with a scripted  interpreter    lua lua   as well as the entire compilation API    lua lua   lua lua filename lua  will compile and run  filename lua  using the luainlua compiler  In addition  you may pass in the   d  flag  at the very end  in order to print a human readable disassembly  like so   bash leegao DESKTOP 3RST9I3  mnt c Users leegao Documents IdeaProjects LuaInLua  lua lua testing hello world lua  d Level 0 Code 1        line 10        CLOSURE A r 0   Bx v 0   2        line 13        MOVE A r 1   B r 0   3        line 13        LOADK A r 2   Bx Kst 0   4        line 13        LOADK A r 3   Bx Kst 1   5        line 13        LOADK A r 4   Bx Kst 2   6        line 13        CALL A r 1   B v 4   C v 1   7        line 9         RETURN A r 0   B v 1   Constants 1       1 2       2 3       3 Upvalues 0        ENV    upval   0    Level 1    Code    1     line 10        GETTABUP A r 0   B v 0   C Kst 0  rk 256      2     line 10        VARARG A r 1   B v 0      3     line 10        CALL A r 0   B v 0   C v 1      4     line 10        RETURN A r 0   B v 1      Constants    1    print    Upvalues    0     ENV    upval   0     END OF DUMP     1       2       3   Additionally  if you don t specify a file to run   lua lua  will read from  stdin     Compilation API   You can directly invoke the  luainlua  compiler using the  luainlua luac  package  It exposes a single  compile  method that compiles source code  not file  into a quadruple of a lua function  its raw bytecode  in 5 2 compliant format   its internal representation in  luainlua   and a human readable dumper    lua local luac   require  luainlua luac  local func  bytecode  proto  dumper   luac compile  print  Hello World        Inspect the disassembly of bytecode dumper proto     Run func func     There s a wealth of internal APIs revolved around the compilation process   luainlua lua     as well as the bytecode format   luainlua bytecode     that can be useful for anyone who wishes to understand the Lua internals    Bootstrapping Test   The ultimate test of a compiler written in its own language is the bootstrap test  That is  can we use the compiler to compile itself  and then use the resulting compiler to compile an arbitrary program    Here is  hello world  in  lua lua  in lua    lua    lua lua luac   require  luainlua luac  func  bytecode  prototype  dumper   luac compile  print  Hello World     dumper prototype     Human readable disassembly func       ctrl   d       Level 0    Code    1        line 1         GETTABUP A r 0   B v 0   C Kst 0  rk 256      2        line 1         LOADK A r 1   Bx Kst 1      3        line 1         CALL A r 0   B v 2   C v 1      4        line 1         RETURN A r 0   B v 1      Constants    1       print    2       Hello World    Upvalues    0        ENV    upval   0    Hello World        Parser   In addition to the vanilla compiler   luainlua  also comes with its own recursive descent powered parser generator and its own implementation of  regular  regular expression  Both of these are within the  luainlua parsing    and the  luainlua ll1 ll1  packages  In addition  you can augment the vanilla  lua  grammar   luainlua lua grammar ylua   and recompile it by just requiring  luainlua generate parser     A sample grammar for the handmade parser generator is given here       file  experimental parser   require  luainlua parsing lex   require  luainlua parsing re       root     expr rexpr     eps    expr   PLUS  expr expr      consts  rexpr   ID  rexpr   FUN ID     expr   LPAREN  expr RPAREN  rexpr consts   NUMBER   STRING   TRUE   FALSE       code    local string stack      local function id token  return function      return  token       end end local function ignore      return end local function pop stack  return table remove stack  end local function push item  stack  table insert stack  item  end local tokenizer   lex lex     root               id  PLUS          fun   id  FUN               id  ARROW              id  LPAREN              id  RPAREN          true   id  TRUE          false   id  FALSE         re   s    ignore        re   d    id  NUMBER         re   d    d    id  NUMBER         re    a     a  d          id  ID              function piece  lexer  lexer go  string   push     string stack  end          string               function piece  lexer         lexer go  root        return   STRING   pop string stack       end        re      function piece  lexer         push pop string stack     piece  string stack      end               default action      function item      return item   end       prologue      function stream      local tokens          for token in tokenizer stream  do       table insert tokens  token      end     return tokens   end       convert      function token      return token 1    end       epilogue      function result      return result   end       quote     LPAREN  quote     RPAREN  quote  fun  FUN  quote      ARROW  quote     PLUS  quote  true  TRUE  quote  false  FALSE   consts    NUMBER           STRING            true             false    expr                    expr        consts   ID           expr    expr       fun  ID        expr  root               expr        This presents a lambda calculus esque language    Philosophy   Why Lua  If you re interested in language implementation  you typically see two types of tutorials out there depending on your background      If you re in college  chances are you ll find your local Programming Language department to be filled with OCaml Haskell enthusiasts pushing for compilers for mini OCaml Haskell Imp  You ll be shown the purple Dragon Book and the way out    If you re on your own  chances are you ll stumble across various tutorials on how to use Lex Yacc masquerading as  How to write your own compiler   guides  You ll learn about words  and then you get shown this weird parser language called BNF  and finally you get a grammar for some subset of C   Parsing is the hardest part of a compiler  they ll tell you   the rest is trivial and is subsequently left as an exercise for the reader   Finally  they ll tell you to go out and buy the purple Dragon Book      I ve tried both of these approaches  and neither worked  Obviously  the Dragon Book is to be blamed    My philosophy  don t read  just do  This is one of the core tenets of people who love to program  No matter how hard you try  you will rarely be able to have a full end to end perspective on a large and complex system  Stop falling into never ending rabbit holes and start iterating  There s no better way to learn about a domain than to get your hands dirty    So this is my sell for my grand ambition of getting free labor  Lua turns out to be a rather curious language  both from the language design perspective as well as the choice of tooling that it uses  No matter if you re a fresh developer or a seasoned Clang hacker  that s me    you re sure to find something fresh and interesting because the design choices made by Lua are somewhat unconventional  So come hack along and squash some bugs  Hopefully you ll gain some valuable insights that ll finally bridge you across the deep chasm between the  How to even Compilers for Dummies  and actually fleshing out your own language      Floating Point Hacks   Completely useless  but fun nevertheless      Equations for a  fast    method      Table of Contents     Floating Point Hacks   Usage   For Contributors   Pow   Exp   Log   Geometric Mean       Justification   Prelude   Arbitrary Powers   Exp   Differentiating the l2f and  f2l  functions    A Tale of Two Functions   Exp  redux        Log   Geometric Mean           This repository contains a set of procedures to compute numerical methods in the vein of the  fast inverse root method   In particular  we will generate code that     Computes rational powers     to an arbitrary precision    Computes irrational powers     to within 10  relative error    Computes   to within 10  relative error    Computes   to within 10  relative error    Computes the geometric mean   of an  std  array  quickly to within 10  error      Additionally  we will do so using mostly just integer arithmetic    Usage   You can use everything in  floathacks  by including  hacks h    include  floathacks hacks h  using namespace floathacks     Comment this out if you don t want your top level namespace to be polluted    For Contributors   This document is compiled from  READOTHER md  by  readme2tex   Make sure that you  pip install readme2tex   You can run   python  m readme2tex   output README md   branch svgs    to recompile these docs    Pow   To generate an estimation for    where   is any floating point number  you can run   float approximate root   fpow FLOAT 0 12345    estimate x      Since estimates of  pow  can be refined into better iterates  as long as  c  is  rational enough    you can also compute a more exact result via   float root   pow FLOAT 0 12345   n  x      where  n  is the number of newton iterations to perform  The code generated by this template will unroll itself  so it s relatively efficient    However  the optimized code does not let you use it as a  constexpr  or where the exponent is not constant  In those cases  you can use  consts  fpow x  c   and  consts  pow x  c  iterations   2   instead    float root   consts  pow x  0 12345  n      Note that the compiler isn t able to deduce the optimal constants in these cases  so you ll incur additional penalties computing the constants of the method    Exp   You can also compute an approximation of   with   float guess   fexp x      Unfortunately  since there are no refinement methods available for exponentials  we can t do much with this result if it s too coarse for your needs  In addition  due to overflow  this method breaks down when  x  approaches 90    Log   Similarly  you can also compute an approximation of   with   float guess   flog x      Again  as is with the case of  fexp   there are no refinement methods available for logarithms either    All of the  f     methods above have bounded relative errors of at most 10   The refined  pow  method can be made to give arbitrary precision by increasing the number of refinement iterations  Each refinement iteration takes time proportional to the number of digits in the floating point representation of the exponent  Note that since floats are finite  this is bounded above by 32  and more tightly  23     Geometric Mean   You can compute the geometric mean     of a  std  array float  n   with   float guess   fgmean 3    1  2  3        This can be refined  but you typically do not care about the absolute precision of a mean like statistic  To refine this  you can run Newton s method on    As far as I am aware  this is also an original method    Justification   Prelude   The key ingredient of these types of methods is the pair of transformations   and          takes a  IEEE 754  single precision floating point number and outputs its  machine  representation  In essence  it acts like unsigned long f2l float x      union  float fl  unsigned long lg   lens     x      return lens lg            takes an unsigned long representing a float and returns a  IEEE 754  single precision floating point number  It acts like   float l2f unsigned long z      union  float fl  unsigned long lg   lens     z      return lens fl              So for example  the fast inverse root method      union  float fl  unsigned long lg   lens     x      lens lg   0x5f3759df   lens lg   2    float y   lens fl     can be equivalently expressed as     In a similar vein  a fast inverse cube root method is presented at the start of this page      We will justify this in the next section    Arbitrary Powers   We can approximate any   using just integer arithmetic on the machine representation of    To do so  compute     where    In general  any value of  bias   as long as it is reasonably small  will work  At  bias   0   the method computes a value whose error is completely positive  Therefore  by increasing the bias  we can shift some of the error down into the negative plane and halve the error     As seen in the fast inverse root method  a bias of   0x5c416  tend to work well for pretty much every case that I ve tried  as long as we tack on at least one Newton refinement stage at the end  It works well without refinement as well  but an even bias of   0x5c000  works even better    Why does this work  See  these slides  for the derivation  In particular  the fast inverse square root is a subclass of this method    Exp   We can approximate   up to   using a similar set of bit tricks  I ll first give its equation  and then give its derivations  As far as I am aware  these are original  However  since there are no refinement methods for the computation of    there is practically no reason to ever resort to this approximation unless you re okay with 10  error      Here    is the  machine epsilon  for single precision  and it is computed by      To give a derivation of this equation  we ll need to borrow a few mathematical tools from analysis  In particular  while  l2f  and  f2l  have many discontinuities    of them to be exact   it is mostly smooth  This carries over to its  rate of change  as well  so we will just pretend that it has mostly smooth derivatives everywhere    Differentiating the  l2f  and  f2l  functions    Consider the function     where the equality is a consequence of the chain rule  assuming that  f2l  is differentiable at the particular value of    Now  this raises an interesting question  What does it mean to take a derivative of      Well  it s not all that mysterious  The derivative of  f2l  is just the rate at which a number s IEEE 754 machine representation changes as we make small perturbations to a number  Unfortunately  while it might be easy to compute this derivative as a numerical approximation  we still don t have an approximate form for algebraic manipulation    While   might be difficult to construct  we can fair much better with its sibling     Now  the derivative   is the rate that a float will change given that we make small perturbations to its machine representation  However  since its machine representations are all bit vectors  it doesn t make sense to take a derivative here since we can t make these perturbations arbitrarily small  The smallest change we can make is to either add or subtract one  However  if we just accept our fate  then we can define the  derivative  as the finite difference     where     Here  equality holds when   is a perfect power of    including fractions of the form       Therefore      From here  we also have     A Tale of Two Functions   Given    antidifferentiating both sides gives     Similarly  since   satisfies    we have     This makes sense  since we d like these two functions to be inverses of each other    Exp  redux    Consider     which suggests that      Since we would like    we can impose the boundary condition     which gives    However  while this method gives bounded relative error  in its unbiased form this is pretty off the mark for general purposes  it approximates some other     Instead  we can add in an unbiased form      where  empirically    gives a good approximation  Notice that the   we ve chosen is close to    which is what we need to transform   to    In particular  for all    the       and   relative error is always below 10       Log   In a similar spirit  we can use the approximation     to derive     Imposing a boundary condition at   gives    so we should expect     However  this actually computes some other logarithm    and we ll have to  again  unbias this term     where the   term came from the fact that the base computation approximates the 2 logarithm  Empirically  I ve found that a bias of   works well  In particular  for all    the       and   relative error is always below 10       Geometric Mean   There s a straightforward derivation of the geometric mean  Consider the approximations of   and    we can refine them as     Therefore  a bit of algebra will show that     which reduces to the equation for the geometric mean     Notice that we just add a series of integers  followed by an integer divide  which is pretty efficient      For more information on how the constant     is derived for the cube root  visit http   www bullshitmath lol     Equations rendered with  readme2tex   Yuck   Learning to implement an imperative language in the twenty first century    Imperative languages  What is this  1996    Why are you using Java for the Frontend  Are you a masochist    Roadmap      x  Specification of the  yuck  language     x  Turning text into trees     x  Turning trees into  ycode  programs     x  Interpreting  ycode  directly        A few linters and static analyzers        Simple peep hole optimizations        Gradually typing yucky code        Smell proofing yucky code  contracts and type refinements        Instrumentation and profiling support  or how I stopped worrying about the smell and learned to love yuck        JITing machine code directly from yuck code  a crash course on contemporary tracing dynamic compilers      Formal Specification   Yuck Grammar   Yuck is a simple imperative language  It is obnoxiously intuitive and natural for those who are familiar with the mainstream dynamic imperative languages of the twenty first century    At a first glance  Yuck has statements and expressions  Expressions are computations that outputs some value whereas statements do not    Within Yuck expressions  you ll find your usual binary operations such as the arithmetic operators              mod       logical operator  and  or   comparisons                etc   and a builtin range construct  a to b   Additionally  you have other compound expressions like unary operators for     and  not   function calls  f e  e     object instantiations  new Foo e    attribute selection  foo bar   list construction   a  b  c    table construction   k   v    anonymous functions  function x  y          and table list indexing  a e    As primitives  you have boolean  true  false   floats and ints  and strings like   Hello World    Finally  you also have variables like  x  y  foo bAr133     Every Yuck expression can serve as a statement as well  regardless of whether they have any effect or not  In addition  you can have variable declarations  either  var id   or  var id   e     function declarations  function foo           while statements  while e         for loops  for x in e         if statements   if e        or  if e       else          empty statements      and class declarations of the form   python class foo     var x    var y   bar    function meh                     While this is natural  as programming language developers  we should be a bit less wishy washy about all of this   Let s formalize this grammar in an extension of  BNF   In particular   we will allow constructs like       and    which denotes a production 0 or more times   a production 1 or more times  and a production 0 or 1 time     LL 1  Grammar     Here  the grammar we ve specified is mostly free of 1 lookahead conflicts  so it s amenable to a LL1 grammar with explicit conflict resolution  In particular  you will need to resolve conflicts for       at      since it doesn t know whether you want      or           You can resolve this by looking at the   next character and shifting to      if it s a      and   expr     expr     otherwise     at      which is the same problem as above for      versus             for the token  function   Here  we re not sure if we want to shift to an expression statement    function            or a function declaration  function id           While it s perfectly fine to just ignore the   first form  since it s effectively a NOP   we can resolve this easily by just looking at the next character  and shifting   to the expression statement production iff it s an open parenthesis        Within    For the token        it s not entirely clearly whether we should shift to the expression statement for a table or continue the  else        clause    Here  we ll just always shift to the else clause      Natural Grammar   For the sake of analysis  it s often easier to give a grammar specification that  while ambiguous  captures just the structure of our language  Here  we will give the specification of our language as an inductive class over the set of    expressions  and    statements    The expressions are given by     where   denotes binary arithmetic operators    denotes logical binary operators  and   denotes binary comparison operators    Similarly  the statements are given by     While this grammar may not be easily implementable using your everyday flavor of parser generators  it does have the advantage that it is compact and it gives you an inductive construction  We can take the structure defined here and use it to construct an operational semantic for this language to reveal the types of information that we will have to carry around in order to fully execute this program    Semantics   Simple Operational Semantics  Big Step    We will give the operational semantics in terms of inferences rules  Here  the sequent     says that if   all hold  then we can deduce    As we will see  it s very natural to specify the semantics of a language in terms of these inference rules    Let   denote the  execution  of a Yuck expression   in contexts    for local variables  and    for the heap of objects   Since expressions may  in general  have side effects  we also have to output the potentially altered contexts  Their semantics are given by     For statements  we also have a similar reduction   which outputs the next set of contexts for the next instruction    readme2tex   Renders LaTeX for Github Readmes       Make sure that pdflatex is installed on your system      readme2tex  is a Python script that  texifies  your readme  It takes in Github Markdown and replaces anything enclosed between dollar signs with rendered      In addition  while other Github TeX renderers tend to give a jumpy look to the compiled text           readme2tex  ensures that inline mathematical expressions are properly aligned with the rest of the text to give a more natural look to the document  For example  this formula   is preprocessed so that it lines up at the correct baseline for the text  This is the one salient feature of this package compared to the others out there    Installation   Make sure that you have Python 2 7 or above and  pip  installed  In addition  you ll need to have the programs  latex   and  dvisvgm  on your  PATH   In addition  you ll need to pre install the  geometry  package in      To install  readme2tex   you ll need to run   bash sudo pip install readme2tex   or  if you want to try out the bleeding edge    bash git clone https   github com leegao readme2tex cd readme2tex python setup py develop   To compile  INPUT md  and render all of its formulas  run   bash python  m readme2tex   output README md INPUT md   If you want to do this automatically for every commit of INPUT md  you can use the    add git hook  command once to set up the post commit hook  like so      bash git stash   include untracked git branch svgs   if this isn t already there   python  m readme2tex   output README md   branch svgs   usepackage tikz INPUT md   add git hook   modify INPUT md   git add INPUT md git commit  a  m  updated readme    git stash pop       and every  git commit  that touches  INPUT md  from now on will allow you to automatically run  readme2tex  on it  saving you from having to remember how  readme2tex  works  The caveat is that if you use a GUI to interact with git  things might get a bit wonky  In particular   readme2tex  will just assume that you re fine with all of the changes and won t prompt you for verification like it does on the terminal          You can uninstall the hook by deleting   git hooks post commit   See  python  m readme2tex   help  for a list of what you can do in  readme2tex     Examples    Here s a display level formula     The code that was used to render this formula is just       frac n   k  n k       n  choose k        Note  you can escape    so that they don t render    Here s an inline formula       It is well known that if    then        The code that was used to render this is    It is well known that if  ax 2   bx   c   0   then  x    frac  b  pm  sqrt b 2   4ac   2a       Notice that the formulas line up with the baseline of the text  even when the height of these two images are different    Sometimes  you might run into formulas that are bottom heavy  like    Here   readme2tex  can compute the correct offset to align this formula to the baseline of your paragraph of text as well    Tikz  Courtesy of http   www texample net     Did you notice the picture at the top of this page  That was also generated by     readme2tex  is capable of handling Tikz code  For reference  the picture     is given by the tikz code    begin tikzpicture   newcounter density   setcounter density  20       def couleur red       path coordinate   0 0   coordinate A                      60 6cm  coordinate B                      60 6cm  coordinate C        draw fill  couleur  thedensity   A      B      C     cycle       foreach  x in  1     15             pgfmathsetcounter density   thedensity 10           setcounter density   thedensity           path coordinate  coordinate X  at  A              path coordinate   A      B  coordinate pos  15  A                                  C  coordinate pos  15  B                                  X  coordinate pos  15  C            draw fill  couleur  thedensity   A    B    C   cycle         end tikzpicture     We can see a few other examples  such as this graphical proof of the Pythagorean Theorem      How about a few snowflakes      Usage   python  m readme2tex   output README md  READOTHER md     It will then look for a file called  readother md  and compile it down to a readable Github ready document    In addition  you can specify other arguments to  render py   such as        readme READOTHER md  The raw readme to process  Defaults to  READOTHER md       output README md  The processed readme md file  Defaults to  README GH md       usepackage tikz  Addition packages to use during   compilation  You can specify this multiple times      svgdir svgs   The directory to store the output svgs  The default is  svgs      branch master   Experimental  Which branch to store the svgs into  the default is just master      username username  Your github username  This is optional  and  render py  will try to infer this for you      project project  The current github project  This is also optional      nocdn  Ticking this will use relative paths for the output images  Defaults to False      htmlize  Ticking this will output a  md html  file so you can preview what the output looks like  Defaults to False      valign  Ticking this will use the  valign  trick  detailed below  instead  See the caveats section for tradeoffs      rerender  Ticking this will force a recompilation of all   formulas even if they are already cached      bustcache  Ticking this will ensure that Github renews its image cache  Github may sometimes take up to an hour for changed images to reappear  This is usually not necessary unless you ve made stylistic changes      add git hook  Ticking this will generate a post commit hook for git that runs readme2tex with the rest of the specified arguments after each  git commit       pngtrick  Ticking this will generate  png  files instead of  svgs  for the formulas      My usual workflow is to create a secondary branch just for the compiled svgs  You can accomplish this via   python  m readme2tex   branch svgs   output README md    However  be careful with this command  since it will switch over to the  svgs  branch without any input from you    Relative Paths   If you re on a private repository or you want to  for whatever reason  use relative paths to resolve your images  you can do so by using the combination   python  m readme2tex   branch master   nocdn   pngtrick        which will output  pngs  relative to your  README md     Due to security considerations  Github will not resolve  svgs  relatively  which means that private repositories will be locked out of the usual  svg  workflow  Using the    branch master   nocdn   pngtrick  combination will get around this restriction    Troubleshooting   Tikz   If your Tikz drawings don t show up  there s a good chance that you either don t have Ghostscript installed or  dvisvgm  isn t picking it up for whatever reason  This is most likely to happen on some installations of TexLive on OSX    Check to see if  ps  is included in the list when you run      bash   dvisvgm  l   bgcolor    background color special color      complete support of color specials dvisvgm    special set for embedding raw SVG snippets em         line drawing statements of the emTeX special set html       hyperref specials pdf        pdfTeX font map specials ps         dvips PostScript specials     tpic       TPIC specials       If not  try installing it  either  apt get    yum   or  brew    Furthermore  if you are on OSX  make sure to add the following to your     bash profile   bash export LIBGS  usr local lib libgs dylib   where   usr local lib libgs dylib  is the location where  libgs dylib  is installed    I m seeing weird formatting from time to time    Make sure that if you have a   p      p   tag somewhere  you leave at least one blank line after the closing tag    I ran    add git hook   but the post commit hook isn t running after committing    bash chmod  x  git hooks post commit   I raw  readme2tex  and got strange image srcs or got images that won t resolve   Try running  readme2tex  with   bash python  m readme2tex       username GITHUB USERNAME    project PROJECT NAME   I ran  readme2tex  and got a traceback somewhere    Unfortunately  this script still has a few kinks and bugs that I need to iron out  In the mean time  if the  pypi  releases aren t working for you  you should switch over to the development version to see if the bugs have been squashed    bash git clone https   github com leegao readme2tex cd readme2tex python setup py develop   Technical Tricks   How can you tell where the baseline of an image is    By prepending every inline formula with an anchor  During post processing  we can isolate the anchor  which is fixed at the baseline  and crop it out  It s super clowny  but it does the job    Caveats   Github does not allow you to pass in custom style attributes to your images  While this is useful for security purposes  it makes it incredibly difficult to ensure that images will align correctly to the text   readme2tex  circumvents this using one of two tricks      In Chrome  the attribute  valign offset  works for  img  tags as well  This allows us to shift the image directly  Unfortunately  this is not supported within any of the other major browsers  therefore this mode is not enabled by default    In every  reasonably modern  browser  the  align middle  attribute will vertically center an image  However  the definition of the vertical  center  is different  In particular  for Chrome  Firefox   and probably Safari   that center is the exact middle of the image  For IE and Edge however  the center is about 5 pixels  the height of a lower case character  above the exact center  Since this looks great for non IE browsers  and reasonably good on Edge  this is the default rendering method  The trick here is to pad either the top or the bottom of the image with extra spaces until the baseline of the formula is at the center  For most formulas  this works great  However  if you have a tall formula  like    you ll notice that there might be a lot of slack vertical spacing between these lines  If this is a deal breaker for you  you can always try the    valign True  mode  For most inline formulas  this is usually a non issue      How to compile this document   Make sure that you have the  tikz  and the  xcolor  packages installed locally    python  m readme2tex   usepackage  tikz    usepackage  xcolor    output README md   branch svgs    and of course   python  m readme2tex   usepackage  tikz    usepackage  xcolor    output README md   branch svgs   add git hook    For the  png  relative mode  use   python  m readme2tex   usepackage  tikz    usepackage  xcolor    output README md   branch master   nocdn   pngtrick      Regex Enumerator   Enumerate Regular Expressions the Fun Way    The regular expression   becomes    which tells us that there are   words of size   in this language          Regex Enumerator takes in a regular expression  and spits out a closed form formula for the number of   letter words in your language    Or how I learned to stop worrying and then lost my mind  TeX rendered using  readme2tex     Table of Contents     Regex Enumerator   Table of Contents   Introduction   Installation   Usage   Regular Expression Syntax   Library Functions   Caveat       Justification   Regular Expressions as Numerical Expressions   Rational Functions   Univariate Functions   Partial Fraction Decompositions   Fibonacci  Redux       Appendix   Library Architecture   Rationalizing Reduction   Proof that Regular Expressions generate Rational Functions   Exact Enumeration   Are there other ways to do this    Additional Examples               Introduction   Have you ever wondered about how many different strings you can form that fits your favorite regex    Yeah  chances are you probably haven t  But it s on your mind now    Here s one of my favorite regular expressions      It specifies the class of languages that are comma separated list of strings of zeros  For example   000  0  00000  belongs to this language  but  0    0  and  0  0   do not    Now  it might seem like a masochistic endeavor  but if you enumerate every possible word in this language  you ll find that there are 0 empty strings  1 single letter string  1 two letter string  2 three letter strings  3 four letter strings  and so on  This pattern looks like   0  1  1  2  3  5  8  13  21  34  55  89  144  233  377  610  987  1597  2584  4181         Why  that is the fibonacci sequence  How did it end up here of all places    Now  I could give you a combinatorial interpretation for this amazeballs result  but I still get shivers up my spine whenever I think back to my undergrad Combinatorics course  Instead  I ll give you a more general way to compute these enumerations as well as an algorithm that can do all of the tedious pencil pushin on your behalf    However  that s not the end of it  It turns out that this algorithm can also compute a closed form formula for this sequence      where   is the number of comma separated lists of size      Yes  that s right  there is an algorithm that can determine the closed form counting expression for  every   unambiguous  regular expression      Installation   You will need Python 2 7 or up  though it seems to be most stable on Python 3     bash git clone https   github com leegao RegexEnumerator git cd RegexEnumerator sudo python setup py develop   Note that you will need to install  numpy    scipy   and  sympy  in order to support solving a few linear equations and to translate numerically computed roots into algebraic forms  if they are available    To uninstall  run   bash pip uninstall RegexEnumerator   Usage   Regular Expression Syntax   We are using vanilla regular expression  so the standard                    variety  Note that for     and      we ve encoded them using just     and     instead            Here      denotes the  empty  transition   in formal languages  In effect  it acts as the identity element of concatenation  so that    For example  the regular expression of comma delimited language   can be encoded as  python e    0    or any other regular expression regex      e  e      e  e    format e   e    Library Functions   regex enumerate  offers a few library functions for you to use      enumerate coefficients   Returns an infinite generator counting the number of words of size   in your language    Since it depends on numerical approximations  you ll have to contend with round off and truncation errors         python   from regex enumerate import enumerate coefficients   from itertools import islice   print list islice enumerate coefficients   0 1  0     10         0 0  1 0  0 99999999999999989  1 9999999999999998  2 9999999999999996  4 9999999999999982  7 9999999999999982  12 999999999999998  20 999999999999993  33 999999999999986             exact coefficients   Uses a dynamic program to compute the same coefficients  Useful for validation   and pure computation  but does not reveal any algebraic structure within the problem         python   from regex enumerate import exact coefficients   from itertools import islice   print list islice exact coefficients   0 1  0     10         0  1  1  2  3  5  8  13  21  34            algebraic form   Computes the algebraic closed form counting formula of a regular expression         python   from regex enumerate import algebraic form  evaluate expression   from sympy import latex  pprint   formula   algebraic form   0 1 0        Normal Form   print formula      2 0 DiracDelta n    1 0 DiracDelta n   1    binomial n   1  1    3     Latex   print latex formula       2 0  delta left n right    1 0  delta left n   1 right      binom n   1  1     3     ASCII Unicode pretty print   print pprint formula                                                    n   1      2 0 DiracDelta n    1 0 DiracDelta n   1              3                                                    1      print evaluate expression formula  10       8         The magic behind this will be discussed in the next section  The   code looks like       Note that this differs from the above since we re enumerating   instead of        check on oeis   This will search https   oeis org for a potential combinatorial interpretation of your   enumeration         python   from regex enumerate import check on oeis   sequences   check on oeis   0    0    start 5    for oeis in sequences      print   s  https   oeis org  s     oeis name  oeis id       Fibonacci numbers  https   oeis org A000045     Pisot sequences E 3 5   P 3 5   https   oeis org A020701     Expansion of  1 x   1 x x 2   https   oeis org A212804     Pisot sequence E 2 3   https   oeis org A020695     Least k such that the maximum number of elements among the continued fractions for k 1  k 2  k 3  k 4   https   oeis org A071679     a n    Fibonacci n  mod n 3  https   oeis org A132636     Expansion of 1  1   x   x 2   x 18   x 20   https   oeis org A185357     Nearly Fibonacci sequence  https   oeis org A264800     Pisot sequences E 5 8   P 5 8   https   oeis org A020712     a n    s 1 t n    s 2 t n 1      https   oeis org A024595         In addition  regular expressions correspond to the family of rational functions  quotient of two polynomials   To see the generating function of a regular expression  try      python from regex enumerate import generating function from sympy import latex   print latex generating function   0 1  0         frac 1 0 z    1 0 z  2    1 0 z   1 0          which outputs     Caveat   There are many regular expressions that are ambiguous  For example  the regular expression     is inherently ambiguous  On encountering a  0   it s not clear which side of the bar it belongs to  While this poses no challenges to parsing  since we don t output a parse tree   it does matter in enumeration  In particular  the direct translation of this expression will claim that there are 2 strings of size 1 in this language      Justification   Now  all of this might feel a little bullshitty   Shameless plug  for more bullshitty math  check out http   bullshitmath lol  Is there any real justification for what you are doing here  Am I just enumerating a bunch of pre existing cases and running through a giant table lookup    Well  it s actually a lot simpler  at least the algorithm is  than that  However  there s a bit of a setup for the problem    Regular Expressions as Numerical Expressions   Let s rewind back to our first example  that of enumerating comma separated sequences of  x es      We ve seen above that this follows a fibonacci like sequence  Is there some way that we can derive this fact without brute force enumeration    Let s start with the sequence of  x es     This language  in an infinitely expanded form  looks like     Now  here s a trick  Let s pretend that our bar     is a plus sign      so that     This looks remarkably familiar  In fact  if you are working within a numerical field  then a little bit of precalculus would also show that     Could there be some connection here  Well  let s find out  To do this  let s equate the two expressions      so   and   if we pretend that each regular expression has a numerical value    In fact  this works for every regular expression  For any regular expressions   and for any letters   we have     As long as you don t need to invoke the axiom of multiplicative commutativity  this reduction works    For example  for the comma separated list example  we have     Note here that   is a variable  It might be tempting to try to simplify this further  Letting   denote the comma   we might try     But this requires a crucial axiom that we do not have      We do not have multiplicative commutativity  so we couldn t merge    since    no longer know whether this is   or         Now that we have this weird  compiler  taking us from regular expressions to numerical formulas  can you tell us what it means for a regular expression to take a numerical value    The answer  none  There is no meaning to assign a value of say   to    or that    It doesn t mean anything   it s just pure gibberish  Don t do it  except maybe values of   or    we ll get to that later    Okay  So why did we go on this wild goose hunt if their values don t even mean anything   It turns out that the value of a formula is not what we are interested in  these objects are compact and have nice algebraic properties  When we count things  we just care about how many objects there are that satisfies a certain property  When we count all words of  say  size 5 in a language  we don t care whether these strings are  000 0  or  0 0 0   The ordering of the letters in these strings are extraneous details that we no longer care about  Therefore  it would be nice to be able to forget these details  More formally  if the order of letters in a word doesn t matter  we would say that  we want the concatenation operator to be commutative   If there s a representational equivalence to the numerical  field   then the translation would be that  we want the multiplication operator to be commutative    This is a huge game changer  In the above example  we weren t able to fully simplify that ugly product of fractions precisely because we lacked this crucial axiom  Luckily for us  it now allows us to fully simplify the expression     Which tells us that our regular expression is isomorphic to the regular expression    That is  for each comma separated list  you can map it to one of the words in    In fact  not only are these two languages isomorphic  they are the same  A moment of thought reveals that this new regular expression also matches only comma separated list of sequences as well    That s a pretty cool trick to deduce equivalences between regular expressions  but is that all there is to it    It turns out that each of these translated numerical expressions also admit an infinite series expansion  in terms of its free variables   So     and in general  we have the multivariable expansion     where   is the coefficient attached to the   term    However  recall that each of the   corresponds to exactly one of the words in our language  Therefore  if there are 5 words of size 6 with just one comma in our language  the coefficient in front of   in the series expansion must be 5    Herein lies the key to our approach  Once we grant the freedom of commutativity  each of these regular expressions  generates  a numerical function with some infinite series expansion  The coefficients of the   term in this expansion is then the total count of all objects in this regular language that has  i   s   j   s  and  k   s    This approach is called the generating function approach within elementary combinatorics  It is a powerful idea to create these compact analytical  if a bit nonsensical  representations of your combinatorial objects of interest in order to use more powerful analytical tools to find properties about them    Rational Functions   We know that there s a translation for our regular expression     into some numerical field  We also know that this numerical formula admits a two variable infinite series expansion  The task at hand now is one familiar to most students of complex analysis  coefficient extraction  Given a function    how are we going to find the coefficients of      Before we tackle that beast  let s develop some more intuition about the functions that we will be working with  In general combinatorics  you may face complicated functions using an exotic variety of functions  differential forms  and even implicit functions that can t be expressed in some explicit form  So where do regular expressions sit on this spectrum    As it turns out  things are much nicer with regular expression  part of the reason they are called  regular   their regularities ensure that their algebraic properties are easier to analyze than general unbounded constructions   In particular if a regular expression has a translation     then we know for a fact that   is rational  What this means is that there s some pair of  polynomials    such that     The proof of this fact will be included in the appendix for interested readers  however that proof does not contribute much here  Polynomials are interesting in the context of infinite expansions  Since polynomials are already in the form     their infinite expansions are in fact finite  Now  the same cannot be said of    but a bit of algebra shows that the series expansion of this inverse is also computable      This form is particularly amenable for coefficient extraction  and a memoized version of this sits at the heart of the validation algorithm we use to test that the algebra for everything else is done correctly  See the appendix for a derivation of the dynamic program that can turn this into a somewhat fast coefficient extraction algorithm    Univariate Functions   Now  up to now  we ve been talking about multivariable functions    This makes sense since we need to parameterize our model on each of the letters in our alphabet  oh boy   In general however  multivariable coefficient extraction problems are prohibitively difficult  Not only that  the numerical tools needed to compute saddle points are outside the scope of this toy project  For more on general methods of multivariate enumeration techniques  check out  ACVS     The situation isn t so bleak within the rational function realm however  and while there is a straightforward extension of the traditional coefficient extraction technique to multivariable rational functions  I just never got to it  See  Stoutemyer08  for a brief summary of the multivariate partial fraction decomposition method  Just know that this isn t supported currently    Instead  we will only support the class of enumeration problems that counts the total number of words of a certain  singular  size in some language family  The trick here is to turn a blind eye on the fact that   and   are different variables  In order to do this  we just set them both equal to some other variable    Therefore  the Fibonnacci generating function above     Partial Fraction Decompositions   Now that we have a univariate rational function of the form     where   are mutually irreducible  that is  there isn t some other polynomial   that evenly divides both   and       There s a concept within polynomial algebra known as a partial fraction decomposition  This decomposition theorem tells us that     where    the variety  is the set of roots of   and   is its multiplicity    So for example  the rational function   has the partial fraction decomposition of     no matter what   is  To solve for    you can exploit the fact that     expanding the numerator and setting them equal to   will give you a linear system to solve  The details of how we are going to solve this linear system doesn t matter  it ll be taken care of for you under the hood by  numpy     Now  how does the partial fraction decomposition help us  Recall that     and in general  by way of the binomial theorem      which means that if      then the coefficients on   is     Bam   Closed form expression for any arbitrary regular expression    While this might seem super complicated  at the heart of this method  we re just using a very well known method to expand a rational function  This is in part why the functional part of this project that deals with computing this closed form is only a couple of lines long  It s actually a really simple idea    Fibonacci  Redux   Let s come back to our favorite example once more  Given the regular expression    we know that it has the generating function     where   are the roots of the quadratic equation    We know that this admits a partial fraction decomposition of     therefore   and    Solving this linear system     will yield the coefficients   and    and you ll find that     In addition  if you plot the generating function    as is in picture at the top of this page           you ll find that the singularities  the points where the graph suddenly jumps up and forms an infinitely tall column  are located exactly at where the roots of   are found  This isn t surprising  since by the fact that   is irreducible  the roots of the denominator must be non removable singularities  In fact  if all you cared about is the asymptotic exponential behavior  then there s a simple graphical method to compute the asymptotic complexity of enumerating your regular expression  Take   to be the root of the denominator that is closest to the origin on the complex plane  then     In addition  if you can figure out the multiplicity of    repeatedly divide out   until that column disappears   you can get an exact asymptotic characterization     Appendix   Library Architecture   It s definitely not too difficult to compute all of this by hand  but the math is really tedious and error prone  This is why this library exists  it automates away the boring parts  In particular  nothing really complicated is going on here      regex enumerate parse  has a Shunting Yard style stack based parser to convert a regular expression into a regex tree    regex enumerate transfer  translates a regex tree into its equivalent numerical expression tree  In addition  it includes   several algorithms for computations on polynomial rings and can simplify any induced numerical expression into a canonical form   of         where   is a small polynomial in general and   is irreducible  thus ensuring that all roots of   are non removable singularities   3   regex enumerate enumerate  has the exact dynamic programming algorithm referenced above as well as the    partial fraction decomposition algorithm to compute the closed form counting expression  4   regex enumerate nfa  contains a set of utility functions to work with the semiring of regular language finite automata     In particular  it will use a conflict free characterization of cycles in an automata to compile every regular expression    into some other non ambiguous expression  The resulting regex is usually significantly larger  in terms of state size   but    they are equivalent in terms of counting    Rationalizing Reduction   In general  it is not immediately obvious that the expression trees created from regular expressions are rational functions  However  if we already know  ahead of time  that a function is rational  we can rationalize the expression into the form      where   and   are not necessarily irreducible  through a pair of mutually inductive reductions    Suppose that the language of regular expressions of unreduced numerical expressions is given by     we would like to reduce an arbitrary expression   into some   where   are straightforward polynomials    To do this  let s start by defining the canonical class of polynomial expressions        and define the canonical form of    where     To construct this reduction    we need another inductive class of the simple ring of polynomials     defined by     with an associated reduction operator   that simplifies arithmetic on polynomials    Now  let us give the inductive definition of the reduction relations        For            For            This pair of reduction rules are implemented in  regex enumerate transfer down r  and  regex enumerate transfer down p  respectively    Proof that Regular Expressions generate Rational Functions   Theorem   The translation   given by     only generates rational functions    Proof   By structural induction on        Case           which is rational      Case           which is rational      Case           Now  by the induction hypothesis  both   and   are   rational  Since the sum of two rational functions is still rational  so too is        Case        Again  by the induction hypothesis  both   and      are rational  Since the product of two rational functions is still rational  so too is        Case        Again  we know that   is rational by the induction hypothesis  As   a result    is also rational  and the inverse of a rational function is still rational as long as that   function is not the zero function    While it is possible to construct zero via    they are   inherently ambiguous  and hence not in the proper domain of our analysis  Therefore    is rational    Since this covers all cases of    it must be the case that   is rational     Exact Enumeration   As mentioned before  we ll give an algorithm to compute the exact enumeration problem in cubic time  In essence  the input consists of three polynomials     where without loss of generality    is irreducible  We will consider the problem of finding    which is the coefficient of   in the infinite series expansion of    In particular  since it s easy to compute   since it is a polynomial  we will instead focus on the problem of computing    Furthermore  we can apply an identity transformation and consider     There are a few ways to do this  An easy way is to let    hence     By the binomial theorem  we know that     so      Since we can compute   from the solution of    computing this product forms the basis of our dynamic program  We will in turn focus on the problem of computing     where   and   are subproblems  Computing     in turn requires   time    Are there other ways to do this    Yes  In fact  one common transformation people do on regular expressions is compiling them down into some deterministic finite automata  A DFA is inherently ambiguity free since every path through a DFA must correspond to a different word in the language  otherwise there s some state   such that it can transition to two different states on the same input  which is impossible since DFAs are  well  deterministic  This then solves the problem of having to specify an unambiguous grammar  all DFAs are unambiguous    A DFA is a triple   where   gives the graph underlying the DFA and   labels each edge with  zero or more  letters it can transition on    Given a DFA  we can also solve the same problem  Suppose that   if    then we can construct a matrix     where   is the incidence matrix of    transposed   but tracking the in degree instead of just whether   is an edge    Then  we can calculate the count of all   letter words in   induced by the DFA via     where   is the first column of the identity  Given an eigenvalue decomposition  this will then reduce to solving a linear system fitting     This has several advantages  but numerical enumerations of the eigenvalues of a linear system is particularly sensitive to any perturbations  Nevertheless  this is an elegant approach that solves one of the biggest issues of the current technique  and all without doing more than simple linear algebra  plus NFA determinization  which is hard     Additional Examples      00 1     1 separated strings that starts with 0 and ends with 1     Its generating function is      For words of sizes up to 20 in this language  their counts are      1  0  1  1  2  3  5  8  13  21  34  55  89  144  233  377  610  987  1597  2584    Its closed form is      A list of OEIS entries that contains this subsequence      Fibonacci numbers  https   oeis org A000045   Pisot sequences E 3 5   P 3 5   https   oeis org A020701   Fibonacci numbers whose decimal expansion does not contain any digit 0  https   oeis org A177194   Expansion of  1 x   1 x x 2   https   oeis org A212804   Pisot sequence E 2 3   https   oeis org A020695   Least k such that the maximum number of elements among the continued fractions for k 1  k 2  k 3  k 4   https   oeis org A071679   a n    Fibonacci n  mod n 3  https   oeis org A132636   Expansion of 1  1   x   x 2   x 18   x 20   https   oeis org A185357   Numbers generated by a Fibonacci like sequence in which zeros are suppressed  https   oeis org A243063     Fibonacci numbers Fib n  whose decimal expansion does not contain any digit 6  https   oeis org A177247          1 11  00  1 11   0    1   complete 1 or 11 separated strings       Its generating function is      For words of sizes up to 20 in this language  their counts are      1  3  4  7  13  24  44  81  149  274  504  927  1705  3136  5768  10609  19513  35890  66012  121415    Its closed form is      A list of OEIS entries that contains this subsequence        Tribonacci numbers  https   oeis org A000073        000   111   22   33   44     complex root to        Its generating function is      For words of sizes up to 20 in this language  their counts are      1  0  3  2  6  6  13  12  24  24  39  42  63  66  96  102  138  150  196  210    Its closed form is      A list of OEIS entries that contains this subsequence      1  22   333   4444   55555     number of ways to make change give coins of denomination 1 2 3 4 and 5     Its generating function is      For words of sizes up to 20 in this language  their counts are      1  1  2  3  5  7  10  13  18  23  30  37  47  57  70  84  101  119  141  164    Its closed form is      A list of OEIS entries that contains this subsequence      Number of partitions of n into at most 5 parts  https   oeis org A001401     Number of partitions of n in which the greatest part is 5  https   oeis org A026811       11  22  33  44  55    5 compositions of n       Its generating function is      For words of sizes up to 20 in this language  their counts are      0  0  0  0  0  1  5  15  35  70  126  210  330  495  715  1001  1365  1820  2380  3060    Its closed form is      A list of OEIS entries that contains this subsequence        Binomial coefficient binomial n 4    n  n 1   n 2   n 3  24  https   oeis org A000332        11      all compositions of n       Its generating function is      For words of sizes up to 20 in this language  their counts are      1  1  2  4  8  16  32  64  128  256  512  1024  2048  4096  8192  16384  32768  65536  131072  262144    Its closed form is      A list of OEIS entries that contains this subsequence      Powers of 2  https   oeis org A000079   Expansion of  1 x   1 2 x  in powers of x  https   oeis org A011782   Zero followed by powers of 2  cf  https   oeis org A131577   Powers of 2  omitting 2 itself  https   oeis org A151821   Orders of finite Abelian groups having the incrementally largest numbers of nonisomorphic forms  A046054   https   oeis org A046055   a n    floor 2  n 1  2   https   oeis org A034008   Smallest exponent such that  1 3 a n  is divisible by 2 n  https   oeis org A090129   Pisot sequences E 4 8   L 4 8   P 4 8   T 4 8   https   oeis org A020707   Numbers n such that in the difference triangle of the divisors of n  including the divisors of n  the diagonal from the bottom entry to n gives the divisors of n  https   oeis org A273109     a n  2 A131577 n   https   oeis org A155559                                                                  number of ways to make n cents with US coins        Its generating function is      For words of sizes up to 20 in this language  their counts are      1  1  1  1  1  2  2  2  2  2  4  4  4  4  4  6  6  6  6  6    Its closed form is      A list of OEIS entries that contains this subsequence      Highest minimal distance of any Type I  strictly  singly even binary self dual code of length 2n  https   oeis org A105674   Number of ways of making change for n cents using coins of 1  5  10  25 cents  https   oeis org A001299   Number of ways of making change for n cents using coins of 1  5  10  25  50 cents  https   oeis org A001300   Number of ways of making change for n cents using coins of 1  5  10  25  50 and 100 cents  https   oeis org A169718   Number of ways of making change for n cents using coins of 1  5  10  20  50  100 cents  https   oeis org A001306   Repetition of even numbers  with initial zeros  five times  https   oeis org A130496   Number of ways of making change for n cents using coins of 1  5  10 cents  https   oeis org A187243     Coefficients of the mock theta function chibar q   https   oeis org A260984        00 1  00    list of 0 sequences       Its generating function is      For words of sizes up to 20 in this language  their counts are      0  1  1  2  3  5  8  13  21  34  55  89  144  233  377  610  987  1597  2584  4181    Its closed form is      A list of OEIS entries that contains this subsequence      Fibonacci numbers  https   oeis org A000045   Pisot sequences E 3 5   P 3 5   https   oeis org A020701   Expansion of  1 x   1 x x 2   https   oeis org A212804   Pisot sequence E 2 3   https   oeis org A020695   Least k such that the maximum number of elements among the continued fractions for k 1  k 2  k 3  k 4   https   oeis org A071679   a n    Fibonacci n  mod n 3  https   oeis org A132636   Expansion of 1  1   x   x 2   x 18   x 20   https   oeis org A185357   Nearly Fibonacci sequence  https   oeis org A264800   Pisot sequences E 5 8   P 5 8   https   oeis org A020712   a n    s 1 t n    s 2 t n 1      https   oeis org A024595   QuickType   Types without the extra baggage      QuickType is a blazing fast concurrent type resolution engine for large codebases that cannot afford to be rebuilt everytime you want to run static analysis  readme2tex tests                        1                                                                  381603 4                                                                                                                                                                                                                                                                                                                                     python3 c             Numpy                   main py                    h              n                                           t                                      s                               l                                            python main py  n 10000  t 1000  s 200  l 0 005                                                                                                                                                              softmax                                                                0  0 5                                 max epoch                                1    number train images                  x            c                                                                                                                                                                                                                                                                                                                                                                                               Python 3 6                                                NeuraNetwork py                                             initializeWeights                                                             0  0 5     train self  x values  t values  maxEpochs  learnRate  crossError                                                                       computeOutputs self  xValues                                      computeGradient self  t values  oGrads  hGrads                                                                              updateWeightsAndBiases self  learnRate  hGrads  oGrads                              crossEntropyError self  x values  t values                                      accuracy self  x values  t values                                                                                                                                                                                                                             100    14          0 9974      0 9725        200    17          0 9982      0 9837        300    22          0 9994      0 9821     Tiling 3 by N rectangles with Monomers and Dimers   Convoluted solution for the monomer dimer tiling problem    Suppose one is given pieces that may be one of the three forms  monomers     that are   squares  and dimers that are dominoes  either vertically     oriented    or horizontally     oriented    In how many ways can a   rectangle be covered completely and without overlap   tiled   by such pieces        For example  here s a   rectangular tiling      or this   rectangular tiling      There are various exact enumerations for this problem  e g  see http   algo inria fr libraries autocomb MonoDiMer html MonoDiMer html   but we d like to approach this with a combination of enumerative combinatorics and computer programming to ease the tedious analysis  In particular  we d like to reduce this problem into finding the singularities of a complex rational function      In the   example above  we can consider the tiling process as a sequence of constructions  We start with the   line at the beginning      Next  the natural thing to do would be to add the horizontal block at the top of the second column  However  for reasons that will become clear soon  let s artificially restrict our rules to allow the total gap at the end of the blocks to be at most one block wide  That is  we will always ensure that the contour edge of our work in progress  rectangle  will only have holes that are  shallow  enough to be filled by a monomer      Following that  we can add the horizontal block in the middle to get a contour with just a single protrusion in the middle      Adding the two monomers on the top and the bottom corners results in a smooth contour      Finally  adding in the final line completes the rectangle       You may have noticed above  but there s a set of rules that governs these transformations  For example  suppose we have a two prong node  there are two transformations we can make  First  we could add a single monomer and get a straight line contour on the rightmost edge      On the other hand  we could also add in a horizontal block to get a single central hump on the rightmost edge      If we don t care about what the blocks on the right most side are  we can actually characterize these operational rules by just specifying transformations of the right most contours  The above rules generates the following operational equations for the contours      As an another example  here are all of the equations that governs the behavior of L shaped edges        The above L contour generalizes for its reflection  and the C shaped contour generalizes to all contours with a single hole            We couldn t generalize the single hump in the middle contour in the above example  but we can enumerate their equations explicitly      Finally  let s enumerate the set of equations that starts off from the smooth contour        Finally  we will give the axiom that the empty board constitutes a single smooth column        At this point  the astute reader may recognize these operational rules as a grammmar involving the transformation of one type of contour into another  We were careful to ensure that no rule can be constructed as a combination of a sequence of two or more other rules  in other words  this grammar in unambiguous    We can present these rules in a tableau      Reading this table vertically  each column encodes one of the rules presented above  For example    and    three horizontal dominoes stacked together  will generate another    Similarly  appending a single   will create either a    a    or a      However  if we read each row horizontally  we get the set of transformation we can make to each of the contours that will eventually end up with the left most shape in that row  In fact  we can think of this table as a transition matrix   where     The above power denotes the enumeration of all shapes after applying   transformations    Now  we know that our initial state is an empty board    which is within the family of the   object  Therefore  after   transformations  we will have the following set of possible configurations      where the initial state is the first column of the identity       We know that   denotes the set of all configurations reachable from an empty initial configurations  Now      which allows us to compute the full set of reachable configurations  compressed as a set of rational equations      If we only care about the total number blocks that occurs within a particular tiling  then we don t really care if a domino is a   or an    we just care that each is a pair of monomers  Therefore  we can set     Finally  since we only care about rectangles  we just want to compute    This can be done by taking the first element of the full reachable configuration vector  that is     Giving this rational function a less cryptic name of    we can find that this is
51,wscullin,bgadmintools   Scripts and tools for Blue Gene administrators ACCA CS   Code samples for the ACCA CS talk workshop template   This repository is  Software Carpentry  and  Data Carpentry  s template for creating websites for workshops        Please  do not fork this repository directly on GitHub       Instead  please use GitHub s importer following  the instructions below      to copy this  workshop template  repository and customize it for your workshop        Please  do your work in your repository s  gh pages  branch       since that is what is      automatically published as a website by GitHub         Once you are done  please also  let us know  the workshop URL  If this is a self organised workshop  you should also  fill in the self organized workshop form   if you have not already done so   so we can keep track of all workshops  We build the list of workshops on our websites from the data included in your  index html  page  We can only do that if you  customize  that page correctly  and  let us know the workshop URL        If you run into problems  or have ideas about how to make this process simpler  please  get in touch   The pages on  customizing your website   the  FAQ   and the  design notes  have more detail on what we do and why  And please note  if you are teaching Git  please  create a separate repository  for your learners to practice in    Creating a Repository       Log in to GitHub       If you do not have an account  you can quickly create one for free       You must be logged in for the remaining steps to work        Go to  GitHub s importer         Paste the url of this repo as the old repository to clone       https   github com swcarpentry workshop template         Select the owner for your new repository       This will probably be you  but may instead be an organization you belong to         Choose a name for your workshop website repository      This name should have the form  YYYY MM DD site       e g    2016 12 01 miskatonic       where  YYYY MM DD  is the start date of the workshop        Make sure the repository is public        At this point  you should have a page like this      You can now click  Begin Import   When the process is done  you will receive a message like  Importing complete  Your new repository gvwilson 2016 12 01 miskatonic is ready   and you can go to the new repository by clicking on the name        Note   some people have had intermittent errors during the import process  possibly because of the network timing out  If you experience a problem  please re try  if the problem persists  please  get in touch     Customizing Your Website       Go into your newly created repository      which will be at  https   github com your username YYYY MM DD site       For example      if your username is  gvwilson       the repository s URL will be  https   github com gvwilson 2016 12 01 miskatonic         Ensure you are on the gh pages branch by clicking on the branch under the drop      down in the menu bar  see the note below           Edit the header of  index html  to customize the list of instructors      workshop venue  etc       You can do this in the browser by clicking on it in the file view on GitHub     and then selecting the pencil icon in the menu bar      Editing hints are embedded in  index html   and full instructions are in  the customization instructions         Edit   config yml  to customize certain site wide variables  such as   carpentry   to tell us which carpentry workshop this is    title   overall title for all pages    repository   so that URLs resolve correctly both locally and on GitHub    workshop repo   the URL of the workshop repository on GitHub  and  workshop site   the repository s GitHub Pages URL     Editing hints are embedded in   config yml   and full instructions are in  the customization instructions         Alternatively      if you are already familiar with Git      you can clone the repository to your desktop      edit  index html  and   config yml  there      and push your changes back to the repository        git clone  b gh pages https   github com your username YYYY MM DD site       You should specify   b gh pages  to checkout the gh pages branch because the imported  repository doesn t have a  master  branch    In order to view your changes once you are done editing  you must push to your GitHub repository        git push origin gh pages           When you are done editing      go to the GitHub Pages URL for your workshop and preview your changes      In the example above  this is  https   gvwilson github io 2016 12 01 miskatonic       The finished page should look  something like this         Optional  you can now change the README md file in your website s repository  which contains these instructions  so that it contains a short description of your workshop and a link to the workshop website        Note   please do all of your work in your repository s  gh pages  branch  since  GitHub automatically publishes that as a website     Note   this template includes some files and directories that most workshops do not need  but which provide a standard place to put extra content if desired  See the  design notes  for more information about these    Further instructions are available in  the customization instructions   This  FAQ  includes a few extra tips  additions are always welcome  and these notes on  the background and design  of this template may help as well    Checking Your Changes   If you want to preview your changes on your own machine before publishing them on GitHub  you can do so as described below        Install the software  described below       This may require some work      so feel free to preview by pushing to the website        Run the command          jekyll serve       and go to  http   0 0 0 0 4000  to preview your site  You can also run this command by typing  make serve   assuming you have Make installed         Run the command  python bin workshop check py index html      to check for a few common errors in your workshop s home page       You must have Python 3 installed to do this       If you have Make installed      you can also run this command by typing  make workshop check          Optional  Linking to Your Page   At the top of your repository on GitHub you ll see       No description or website provided    Edit       Click  Edit  and add        A very brief description of your workshop in the  Description  box  e g    Miskatonic University workshop  Dec  2016         The URL for your workshop in the  Website  box  e g    https   gvwilson github io 2016 12 01 miskatonic         This will help people find your website if they come to your repository s home page    Creating Extra Pages   In rare cases  you may want to add extra pages to your workshop website  You can do this by putting either Markdown or HTML pages in the website s root directory and styling them according to the instructions give in  the lesson template   If you do this  you  must  also edit   config yml  to set these three values        carpentry  is either  dc   for Data Carpentry    swc   for Software Carpentry       or  lc   for Library Carpentry   This determines which logos are loaded        title  is the title of your workshop  typically the venue and date         email  is the contact email address for your workshop      e g    gvwilson miskatonic edu         Note   carpentry  and  email  duplicate information that s in  index html   but there is no way to avoid this without requiring people to edit both files in the usual case where no extra pages are created    Installing Software   If you want to set up Jekyll so that you can preview changes on your own machine before pushing them to GitHub  you must install the software described below   Note  Julian Thilo has written instructions for  installing Jekyll on Windows          Ruby       This is included with Linux and Mac OS X      the simplest option on Windows is to use  RubyInstaller       You can test your installation by running  ruby   version       For more information      see  the Ruby installation guidelines         RubyGems       the package manager for Ruby       You can test your installation by running  gem   version         Jekyll       You can install this by running  gem install jekyll         You can check the formatting of your header by running  bin workshop check py   which is invoked by  make workshop check    You must have Python 3 installed in order to do this  and you will also need the  PyYAML  module    Setting Up a Separate Repository for Learners   If you are teaching Git  you should create a separate repository for learners to use in that lesson  You should not have them use the workshop website repository because        your workshop website repository contains many files     that most learners don t need to see during the lesson      and       you probably don t want to accidentally merge     a damaging pull request from a novice Git user     into your workshop s website while you are using it to teach        You can call this repository whatever you like  and add whatever content you need to it    Getting and Giving Help   We are committed to offering a pleasant setup experience for our learners and organizers  If you find bugs in our instructions  or would like to suggest improvements  please  file an issue  or  mail us
53,jinyangustc,Acme text editor   Files in this repo      Installation   sam editor language   An editing example using sam lang   Gossips about acme  settings and its creators   Remote editing with sshfs and acme   Some tips on sam   Some other notes      Wiki     9p wiki     Articles about acme      Russ Cox solving Advent of Code 2021 Day 2 using the Acme text editor   A Tour of Acme by Russ Cox  rsc      Topic  Mini review  The Acme editor on Debian on Windows   The Acme User Interface for Programmers  lists various resources about the acme editor    Ruben Berenguel  posted several blogs about acme    Brian Zwahr  shared his joys and pains during a 3 month experiment with acme in a series of blogs  9 episodes   from entry to   giving up   master       Jesper Louis Andersen  used acme for a long time  months at least  and talked about it in several posts    Editing remote files with Acme in Inferno OS       Configurations     LSP in Acme   fhs acme lsp   Ev Bogdanov s config   Michael Auchter s config    Jesper Louis Andersen s config   utilities   scripts   programs for acme  the programmes text editor     Other acme implementations     A graphical text editor    A is a text editor inspired by the Sam and Acme text editors for the Plan 9 operating system    Implemented in Go Lang    Simple Zettelkasten Setup in LaTex   A quick introduction of Zettelkasten can be found on  Wikipedia   For a more in depth discussion  S nke Ahrens has a nice book on the topic  1     There are many ways to implement such note taking approach and most of then use certain general purpose or specialized note taking software  such as  Bear app    Evernote    Obsidian    Zettlr    The Archive  and  Roam   But those tools are not essential in my opinion  given that the original system was implemented by Niklas Luhmann with pens  index cards and slip boxes  Luhmann s archive was digitalized and can be found  here   in German     I finished Ahrens s book  1  in the fragmented time when I was waiting my dog to calm down in crate training  I found the idea intriguing and wanted to try it  But I am little reluctant to invest in a new software at the moment  Instead of weighing the pros and cons of various software  I would like to start with something simple and easy to incorporate with my existing workflow  For example  I use Zotero for bibliography management and VSCode for coding and note taking    You can find many good advice online on how to start with plain text files and a text editor for Zettelkasten  Markdown is a popular choice of file format due to its simplicity and potential to be converted to other formats through software such as  pandoc   However  in my field  research papers are mostly typeset in LaTex and I would like to reuse my notes when writing  With sufficient configuration and plugins you may make Markdown work nicely with LaTex  But I decided to skip the hassle and directly note in LaTex    I quickly set up a simple system for this purpose  A few interesting decisions were made  First  LaTex instead of Markdown is chosen for the file format  The reason has been discussed  Second  the page size is limited to A6 instead of the common A4 or letter size  which better mimics Luhmann s original system  he used DIN A6 index cards  148x105mm  1  sec  3 1    Third  the file naming method follows the convention of the original system  A file name is just a number  bearing no meaning and just as a permanent unique identifier  And related notes will be named similarly  For example  a new note that is related to an existing note  1 tex  will be manually named  2 tex  or  1a tex   The next related note will be named  1b tex  or  1a1 tex  and so on  Fourth  linking is done manually through LaTex  hyperref  mechanism  facilitated only by a LaTex command   zl n   which is just   href file n pdf   emph n       A smaller page size is chosen because the limited space  hopefully  encourages precise and brief writing about things that matters  I also like that fact it mimics the original cards and maybe one day you can print them out  put them in a slip box  then you have a physical Zettelkasten system  which is pretty cool  although may not be useful     File naming and linking are done manually because  if I understand the idea correctly  they should be deliberate decisions from you when you review the notes  think about how they relate to your writing  develop ideas  arguments and discussions  It s simpler  more portable and software agnostic  well  it still relies on LaTex of course        1  S  Ahrens  How to take smart notes  One simple technique to boost writing  learning and Thinking for students  academics and nonfiction book writers  S nke Ahrens  2017    Assumptions     Notes are taken in LaTex and reside in one folder    Reviewing and traversing notes are done in PDFs which can be easily compiled    from LaTex sources with  latexmk   or other compiler with a build script    PDFs need to be in the same folder because  href  links are linked to PDFs in    the current directory  For example  in note  1 pdf  a link to note  1a pdf     will be   href file 1a pdf  1a      A bibliography file named  zotero library bib  resides in the same folder as    note LaTex source files      Of course  some of the assumptions can be relaxed with modifications in the class file  I encourage you to read the  cls  file and adjust according to your needs    Installation   Put the  jyzk cls  file in your note source directory    Or put  jyzk cls  to your local  texmf  directory    On mac    bash mkdir  p   Library texmf tex cd   Library texmf tex git clone git github com jinyangustc jyzk cls git   Commands provided by the class    zl n    creates a new link to the PDF file  n pdf      seealso   creates a  See also   at the bottom of the card in emphasis    Basic Setup   Folder structure   text       0index tex     1 tex     1a tex     2 tex     jyzk cls     slipbox        0index pdf        1 pdf        1a pdf        2 pdf     zotero library bib   New note   Create a new note with the following template       latex  documentclass jyzk   notetitle Your note title here   notedate Create time   begin document    Your note goes here       seealso  zl    end document        Optional Setup   VSCode snippet   A snippet that allows you type  nz   new zettel  to insert the template    json  New Zettel           prefix    nz         body                 documentclass jyzk                notetitle  1                notedate  CURRENT YEAR  CURRENT MONTH  CURRENT DATE  CURRENT HOUR  CURRENT MINUTE  CURRENT SECOND                begin document               0                 seealso   zl                 end document                  Open the command palette    Call  Preference  Configure User Snippets     Select  latex json     Insert the snippet      Zotero     Install  Better BibTex  plugin    Export your Zotero library in BibTex format and select  Keep Updated   Your  zotero library bib  file will be in sync with Zotero  Future    changes you make to the Zotero should be automatically made to the  bib  file        Example   Notes         Links   File links are highlighted in cyan  Click one will open the corresponding note  The link on the left header  showing the current note s id  will take you to the index card    Search   Search can be done either in the LaTex source files with your text editor or command line tools such as  grep  or  rg   Or in PDF files with system search tools  For example  on macOS      DeepSense Model for Vehicle Recognition    Version 1   Update 2022 08 24   generate train data py   from raw data to training  validation  and testing data in  train data 1sec   window 1s    train py   Training to detector  model stored in    log weight default 1sec h5    transfer to lite py   load    log weight default 1sec h5   then convert to lite model in    liteModel deepsense tflite     Replace the  train data  folder with the one in Dropbox   Copy the files in Dropbox folder  ShakeData GQ 2022 01 06 clean train data   to  train data   folder in this repo   train X both csv  train Y csv  val X both csv  val Y csv  test X both csv  test Y csv          Environment   Python 3 7 10   Run   pip install  r requirements txt   to install the required packages    Run the code   We save the model we trained in     log weight deepSense h5    to load the model from    log weight deepSense h5  and evaluate its performance  just run    python eval py  m   log weight deepSense h5    If you want to train the model by youself  you can run    python train py  m   model path       model path   is the file to store the trained model with a default value    log weight default h5  Illini Dancesport Utilities   Sync Event Service Space Request Confirmation with Google Calendar   cal py   Please refer to  Google Calendar Python Quickstart  for Authorization setup    Google Contact Sync   contacts py   Generate Rounds File   Preparation   Unzip announcement  mp3 s    bash unzip playlists announcements zip  d playlists    Generate a 30s silence  mp3     bash python rounds py silence   duration 30 mv silence mp3 playlists    Define playlists and generate rounds files   Define your playlist in a   ini  file  See files in  playlists  for examples    Then run  python3 rounds py run  your playlist  ini  to download  trim and concatenate songs into one round file    For example  running   bash python3 rounds py run playlists standard 1 ini   will produce a standard rounds file  standard 1 mp3  in  playlists
54,nikolaydubina,pintos   Were used as course projects in CS330 Operating Systems and Lab at KAIST Spring 2015   Official guidline   Coverage     Project 0   100     Project 1   60     Project 2   90     Project 3   95     Project 4   56    Note   All projects are separeted   master  branch is used only to account contributions  To see actuall code for each project  refer to corresponding branches  Mini Blog   Just a mini blog with basic functionality  Once I wanted to practise python and javascript  so I created this small static blog generator  You can use it  but it was created as an experiment and it will not be maintaned    Moreover  there are plenty of better solutions already  Have a look at       Jekyll      Octopress      Hugo   How to use      Create a post description file and move all necessary files into one folder   Run  python3 blog py   Commit and push updates     Structure   General idea is that each post is described as a sequence of  blocks    basic structure units with specific purpose    Implemented blocks   name          reg expression     description                                                    paragraph      p                lines will be merged into single paragraph  ignoring newline symbols image slider   img              lines in format    name      filename   will be added to list of pictures code           code             code will be automatically be recognized  using  highlight js  list           list             unordered list  each line starting with      hypehen  will be considered as entry raw            raw              iframe with source as listed html html file  there should be only one html htm file  all mentioned files will be moved copied into the same directory with html htm   Each regular expression is easily customizable in  postparser py   Requires  SASS  Python3   Used open source code      docopt   highlight js   Minimal LaTeX resume with visuals inspired by  Awesome CV       under 200LOC    Publications  section   3 colors of text  dark  normal  light    3 font sizes for content  plus one for heading and one for social links               Self contained  plain Go implementation of calendar heatmap inspired by GitHub contribution activity available via CLI and  calendarheatmap io     go install github com nikolaydubina calendarheatmap latest    echo         2020 05 16   8       2020 05 17   13       2020 05 18   5       2020 05 19   8       2020 05 20   5      calendarheatmap   chart png   Basic     Colorscales         UTF 8    SVG     Without month separator    Without labels    Without labels  without separator  go featureprocessing             Fast   simple  sklearn  like feature processing for Go      x  Does not cross  cgo  boundary    x  No memory allocation    x  No reflection    x  Convenient serialization    x  Generated code has 100  test coverage and benchmarks    x  Fitting    x  UTF 8    x  Parallel batch transform    x  Faster than sklearn in batch mode        go   go generate go run github com nikolaydubina go featureprocessing cmd generate  struct Employee   type Employee struct       Age         int      feature  identity       Salary      float64  feature  minmax       Kids        int      feature  maxabs       Weight      float64  feature  standard       Height      float64  feature  quantile       City        string   feature  onehot       Car         string   feature  ordinal       Income      float64  feature  kbins       Description string   feature  tfidf       SecretValue float64          Code above will generate a new struct as well  benchmarks  and  tests  using  google gofuzz      go employee    Employee     Age          22     Salary       1000 0     Kids         2     Weight       85 1     Height       160 0     City          Pangyo      Car           Tesla      Income       9000 1     SecretValue  42     Description   large text fields is not a problem neither  tf idf can help here too  more advanced NLP will be added later        var fp EmployeeFeatureTransformer   config       ioutil ReadAll  employee feature processor json   json Unmarshal config   fp    features    fp Transform  employee       float64 22  1  0 5  1 0039999999999998  1  1  0  0  0  1  5  0 7674945674619879  0 4532946552278861  0 4532946552278861    names    fp FeatureNames        string  Age    Salary    Kids    Weight    Height    City Pangyo    City Seoul    City Daejeon    City Busan    Car    Income    Description text    Description problem    Description help         You can also fit transformer based on data    go fp    EmployeeFeatureTransformer   fp Fit   Employee         config       json Marshal data      ioutil WriteFile  employee feature processor json   config  0644        This transformer can be serialized and de serialized by standard Go routines  Serialized transformer is easy to read  update  and integrate with other tools   json       Age identity           Salary minmax     Min   500   Max   900       Kids maxabs     Max   4       Weight standard     Mean   60   STD   25       Height quantile     Quantiles    20  100  110  120  150        City onehot     Mapping     Pangyo   0   Seoul   1   Daejeon   2   Busan   3       Car ordinal     Mapping     BMW   90000   Tesla   1        Income kbins     Quantiles    1000  1100  2000  3000  10000        Description tfidf            Mapping     help   2   problem   1   text   0          Separator               DocCount    1  2  2          NumDocuments   2         Normalizer               Or you can manually initialize it   go fp    EmployeeFeatureTransformer     Salary  MinMaxScaler Min  500  Max  900      Kids    MaxAbsScaler Max  4      Weight  StandardScaler Mean  60  STD  25      Height  QuantileScaler Quantiles    float64 20  100  110  120  150       City    OneHotEncoder Mapping  map string uint  Pangyo   0   Seoul   1   Daejeon   2   Busan   3       Car     OrdinalEncoder Mapping  map string uint  Tesla   1   BMW   90000       Income  KBinsDiscretizer QuantileScaler  QuantileScaler Quantiles    float64 1000  1100  2000  3000  10000        Description  TFIDFVectorizer        NumDocuments     2        DocCount           uint 1  2  2         CountVectorizer  CountVectorizer Mapping  map string uint  text   0   problem   1   help   2   Separator                  Benchmarks   For typical use  with this struct encoder you can get  100ns processing time for a single sample  How fast you need to get  Here are some numbers    0   C   FlatBuffers decode                                             200ps   4 6GHz single cycle time                 1ns        L1 cache latency                10ns        L2 L3 cache SRAM latency                20ns        DDR4 CAS  first byte from memory latency                20ns        C   raw hardcoded structs access                80ns        C   FlatBuffers decode traverse dealloc               100ns        go featureprocessing typical processing               150ns        PCIe bus latency               171ns        Go cgo call boundary  2015               200ns        some High Frequency Trading FPGA claims               800ns        Go Protocol Buffers Marshal               837ns        Go json iterator go json decode            1 s             Go Protocol Buffers Unmarshal            1 s             High Frequency Trading FPGA            3 s             Go JSON Marshal            7 s             Go JSON Unmarshal            9 s             Go XML Marshal           10 s             PCIe NVLink startup time           17 s             Python JSON encode or decode times           30 s             UNIX domain socket  eventfd  fifo pipes latency           30 s             Go XML Unmarshal          100 s             Redis intrinsic latency          100 s             AWS DynamoDB   DAX          100 s             KDB  queries          100 s             High Frequency Trading direct market access range          200 s             1GB s network air latency          200 s             Go garbage collector latency 2018          500 s             NGINX Kong added latency      10ms                  AWS DynamoDB      10ms                  WIFI6  air  latency      15ms                  AWS Sagemaker latency      30ms                  5G  air  latency     100ms                  typical roundtrip from mobile to backend     200ms                  AWS RDS MySQL PostgreSQL or AWS Aurora  10s                       AWS Cloudfront 1MB transfer time   This is significantly faster than sklearn  or calling sklearn from Go  for few samples  And it performs similarly or faster than sklearn for large number of samples       For full benchmarks go to   docs benchmarks   some extract for typical struct    goos  darwin goarch  amd64 pkg  github com nikolaydubina go featureprocessing cmd generate tests BenchmarkEmployeeFeatureTransformer Transform 8                                     62135674            206 ns op          208 B op        1 allocs op BenchmarkEmployeeFeatureTransformer Transform Inplace 8                             89993084            123 ns op            0 B op        0 allocs op BenchmarkEmployeeFeatureTransformer TransformAll 10elems 8                           5921253           1881 ns op         2048 B op        1 allocs op BenchmarkEmployeeFeatureTransformer TransformAll 100elems 8                           528890          20532 ns op        21760 B op        1 allocs op BenchmarkEmployeeFeatureTransformer TransformAll 1000elems 8                           53524         238542 ns op       221185 B op        1 allocs op BenchmarkEmployeeFeatureTransformer TransformAll 10000elems 8                           4879        2267683 ns op      2007048 B op        1 allocs op BenchmarkEmployeeFeatureTransformer TransformAll 100000elems 8                           475       23257147 ns op     20004876 B op        1 allocs op BenchmarkEmployeeFeatureTransformer TransformAll 1000000elems 8                           46      284763749 ns op    192004098 B op        1 allocs op BenchmarkEmployeeFeatureTransformer TransformAll 10elems 8workers 8                  1552704           7362 ns op         2064 B op        2 allocs op BenchmarkEmployeeFeatureTransformer TransformAll 100elems 8workers 8                  412455          29814 ns op        21776 B op        2 allocs op BenchmarkEmployeeFeatureTransformer TransformAll 1000elems 8workers 8                  63822         177183 ns op       213008 B op        2 allocs op BenchmarkEmployeeFeatureTransformer TransformAll 10000elems 8workers 8                  8704        1505994 ns op      2162707 B op        2 allocs op BenchmarkEmployeeFeatureTransformer TransformAll 100000elems 8workers 8                  800       15840396 ns op     21602323 B op        2 allocs op BenchmarkEmployeeFeatureTransformer TransformAll 1000000elems 8workers 8                  72      139700740 ns op    192004112 B op        2 allocs op BenchmarkEmployeeFeatureTransformer TransformAll 5000000elems 8workers 8                   9     1720488586 ns op       1040007184 B op        2 allocs op BenchmarkEmployeeFeatureTransformer TransformAll 15000000elems 8workers 8                  1    14009776007 ns op       3240001552 B op        2 allocs op    beta  Reflection based version   If you can t use  go gencode  version  you can try relfection based version  Note  that reflection version intrudes overhead that is particularly noticeable if your struct has a lot of fields  You would get  2x time increase for struct with large composite transformers   And you would get  20x time increase for struct with 32 fields  Note  some features like serialization and de serialization are not supported yet    Benchmarks      goos  darwin goarch  amd64      reflection pkg  github com nikolaydubina go featureprocessing structtransformer BenchmarkStructTransformerTransform 32fields 4                           1732573              2079 ns op             512 B op          2 allocs op      non reflection pkg  github com nikolaydubina go featureprocessing cmd generate tests BenchmarkWith32FieldsFeatureTransformer Transform 8                     31678317           116 ns op         256 B op          1 allocs op BenchmarkWith32FieldsFeatureTransformer Transform Inplace 8             80729049            43 ns op           0 B op          0 allocs op       Profiling   From profiling benchmarks for struct with 32 fields  we see that reflect version takes much longer and spends time on what looks like reflection related code  Meanwhile  go generate  version is fast enough to compar to testing routines themselves and spends 50  of the time on allocating single output slice  which is good since means memory access is a bottleneck  Run  make profile  to make profiles  Flamegraphs were produced from pprof output by https   www speedscope app     gencode       reflect     Reference     https   dave cheney net 2016 01 18 cgo is not go   https   github com json iterator go   https   benchmarksgame team pages debian net benchmarksgame fastest go html   https   github com shmuelamar python serialization benchmarks   https   shijuvar medium com benchmarking protocol buffers json and xml in go 57fa89b8525   https   gist github com shijuvar 25ad7de9505232c87034b8359543404a file order test go   https   google github io flatbuffers flatbuffers benchmarks html   https   www cockroachlabs com blog the cost and complexity of cgo    https   en wikipedia org wiki CAS latency   Go Machine Learning Benchmarks     Given a raw data in a Go service  how quickly can I get machine learning inference for it      Typically  Go is dealing with structured single sample data  Thus  we are focusing on tabular machine learning models only  such as popular  XGBoost   It is common to run Go service in a backed form and on Linux platform  thus we do not consider other deployment options  In the work bellow  we compare typical implementations on how this inference task can be performed      host  AWS EC2 t2 xlarge shared os  Ubuntu 20 04 LTS  goos  linux goarch  amd64 cpu  Intel R  Xeon R  CPU E5 2686 v4   2 30GHz BenchmarkXGB Go GoFeatureProcessing GoLeaves noalloc                              491 ns op BenchmarkXGB Go GoFeatureProcessing GoLeaves                                      575 ns op BenchmarkXGB Go GoFeatureProcessing UDS RawBytes Python XGB                    243056 ns op BenchmarkXGB CGo GoFeatureProcessing XGB                                       244941 ns op BenchmarkXGB Go GoFeatureProcessing UDS gRPC CPP XGB                           367433 ns op BenchmarkXGB Go GoFeatureProcessing UDS gRPC Python XGB                        785147 ns op BenchmarkXGB Go UDS gRPC Python sklearn XGB                                  21699830 ns op BenchmarkXGB Go HTTP JSON Python Gunicorn Flask sklearn XGB                  21935237 ns op   Abbreviations and Frameworks     Transport  Unix Domain Sockets  UDS   TCP  HTTP   Encoding  JSON   gRPC   raw bytes with fixed number of float64 IEEE 754   Preprocessing   go featureprocessing    sklearn   Model   XGBoost    Leaves   Leaves is XGBoost in native Go    Web Servers  for Python used  Gunicorn     Flask     Dataset and Model   We are using classic  Titanic dataset   It contains numerical and categorical features  which makes it a representative of typical case  Data and notebooks to train model and preprocessor is available in  data and  notebooks    Some numbers for reference   How fast do you need to get    200ps   4 6GHz single cycle time                 1ns        L1 cache latency                10ns        L2 L3 cache SRAM latency                20ns        DDR4 CAS  first byte from memory latency                20ns        C   raw hardcoded structs access                80ns        C   FlatBuffers decode traverse dealloc               150ns        PCIe bus latency               171ns        cgo call boundary  2015               200ns        HFT FPGA               475ns        2020 MLPerf winner recommendation inference time per sample               500ns        go featureprocessing   leaves               800ns        Go Protocol Buffers Marshal               837ns        Go json iterator go json unmarshal            1 s             Go protocol buffers unmarshal            3 s             Go JSON Marshal            7 s             Go JSON Unmarshal           10 s             PCIe NVLink startup time           17 s             Python JSON encode decode times           30 s             UNIX domain socket  eventfd  fifo pipes          100 s             Redis intrinsic latency  KDB   HFT direct market access          200 s             1GB s network air latency  Go garbage collector pauses interval 2018          230 s             San Francisco to San Jose at speed of light          500 s             NGINX Kong added latency      10ms                  AWS DynamoDB  WIFI6  air  latency      15ms                  AWS Sagemaker latency   Flash Boys  300million USD HFT drama      30ms                  5G  air  latency      36ms                  San Francisco to Hong Kong at speed of light     100ms                  typical roundtrip from mobile to backend     200ms                  AWS RDS MySQL PostgreSQL  AWS Aurora  10s                       AWS Cloudfront 1MB transfer time   Profiling and Analysis    491ns 575ns   Leaves   we see that most of time taken in Leaves Random Forest code  Leaves code does not have mallocs  Inplace preprocessing does not have mallocs  with non inplace version malloc happen and takes and takes half of time of preprocessing      243 s   UDS Raw bytes Python   we see that Python takes much longer time than preprocessing in Go  however Go is at least visible on the chart  We also note that Python spends most of the time in  libgomp so  call  this library is in GNU OpenMP written in C which does parallel operations       244 s   CGo version   similarly  we see that call to  libgomp so  is being done  It is much smaller compare to rest of o CGo code  as compared to Python version above  Over overall results are not better then  Likely this is due to performance degradation from Go to CGo  We also note that malloc is done       367 s   gRPC over UDS to C     we see that Go code is around 50  of C   version  In C   50  of time spend on gRPC code  Lastly  C   also uses  libgomp so   We don t see on this chart  but likely Go code also spends considerable time on gRPC code       785 s   gRPC over UDS to Python wihout sklearn   we see that Go code is visible in the chart  Python spends only portion on time in  libgomp so        21ms   gRPC over UDS to Python with sklearn   we see that Go code   main test   is no longer visible the chart  Python spends only small fraction of time on  libgomp so        22ms   REST service version with sklearn   similarly  we see that Go code   main test   is no longer visible in the chart  Python spends more time in  libgomp so  as compared to Python   gRPC   skelarn version  however it is not clear why results are worse      Future work         go featureprocessing   gRPCFlatBuffers   C     XGB       batch mode       UDS   gRPC   C     ONNX  sklearn   XGBoost        UDS   gRPC   Python   ONNX  sklearn   XGBoost        cgo ONNX  sklearn   XGBoost   examples   1         native Go ONNX  sklearn   XGBoost    no official support  https   github com owulveryck onnx go is not complete       text       images       videos     Reference     Go GC updates  2018   cgo performance  GopherCon 18   cgo performance  CockroachDB   cgo call to CPython  Datadog   cgo call to CPython  EuroPython 19   HFT latency   HFT FPGA latency   HFT FPGA 200 nanoseconds  2018   Google TPU latency   PCIe latency    Evaluating Modern GPU Interconnect  PCIe  NVLink  NV SLI  NVSwitch and GPUDirect   2019    Evaluation of Inter Process Communication Mechanisms    UNIX local IPC latencies   Cache and DRAM latency   MLPerf benchmarks   MLPerf benchmarks results  2020   Redis latency   Huawei WIFI6 latency   Verizon 5G latency   NGINX added latency   AWS Sagemaker latency   AWS Aurora latency   AWS Cloudfront transfer rates   ARIA Encryption Algorithm   Specification     ARIA is a general purpose block cipher algorithm developed by Korean cryptographers in 2003  It is an iterated block cipher with 128   192   and 256 bit keys and encrypts 128 bit blocks in 12  14  and 16 rounds  depending on the key size  It is secure and suitable for most software and hardware implementations on 32 bit and 8 bit processors  It was established as a Korean standard block cipher algorithm in 2004  ARIAKS  and has been widely used in Korea  especially for government to public services  It was included in PKCS  11 in 2007  ARIAPKCS      Specification     This implementation written for CS448 Introduction to Information Security course at KAIST in 2015 by Nikolay Dubina    Refer to specification for Copyright Notice and Licence of ARIA algorithm    Resources on Neuroscience   Basics     brain is not a  randomly  connected mesh  there is a lot of high and low level structure    grey matter    surface layer  cortex  mostly neurons   6 layers of neurons   white matter    insides  connections between neurons  moslty axons    current representation    vector at layer in network  what is being processed now   abeyant representation    stored knowledge  weights at layer of network    local coding    concept of  grandmother  is handled by a single neuron   distributed coding    it is distributed on many neurons  vector coding    connectomics    given brain is a graph  what are its properties  how does it differ in animals  how it comes into being  how it changes with time  Status of mapping  C elegans worm  Drosophilia fruit fly   fully mapped  pigeon  cat  monkey  human   good  but not full  A lot of structure of connectome is encoded in genes    plasticity    how neurons change their connection weights  structural plasticity   changing what is connected  functional plasticity   changing strength of connection    time  is important  as a lot of processing is oscilating potentiation  many recurrent loops  different types of neurons  neurons are either inhibitory or excitatory    brain development follows phases   neural plate  progenitor cells  neural fates  migrataion  synapse path finding  synapse connection forming  cells and synapse pruning      Books     Principles of Neural Science   Kandel   Fundamental Neuroscience   Squire  Academic Press   The Computational Brain    Patricia S  Churchland  Terrence J  Sejnowski  MIT Press   Changing Connectomes   Marcus Kaiser  MIT Press   Dynamic Patterns  The Self Organization of Brain and Behavior   MIT Press   Neuroscience of Mathematical Cognitive Development   Rhonda Douglas Brown  Springer   Chasing Men on Fire   Stephen G Waxman  MIT Press   The Ego Tunnel   Thomas Metzinger     Str ngmann Forum Reports       The Neocortex   MIT Press  2019   Emergent Brain Dynamics  Prebirth to Adolescence   MIT Press  2018   Translational Neuroscience  Toward New Therapies   MIT Press  2015     Courses     Neuromatch Academy   List of MIT Brain and Cognitive Sciences courses     Software   Labs     NITRC    large list of neuroimaging tools   LEAD DBS    model of deep brain stimulation as population level   VERTEX    model of electrical stimulation at neuronal level   SpikeInterface    a unified framework for spike sorting   MICrONS    machine intelligence from cortical networks  IARPA   SpikeGLX    used in neuropixels  Allen Institute   SMART    spatial registration tool  BICCN   HistoloZee    spatial registration tool  MRI visualization  histology  BICCN   cnpkg    cortical network simulator  optimized with CUDA  Seung Lab  MIT   fimpy    parallelizable processing of fluorescent imaging data  HDF5       many more software is available in labs and organizations links     Enterprise     Siemense     Hardware       Neuropixels   Allan Institute      Auto Surgery   Allan Institute  MIT     Siemens  fMRI      Braingate   BCI       Neurolutions   BCI     Labs      EPFL   BlueBrain    octocat     related to Neuralink      Braingate    Brown  MGH  Stanford  USA gov  USA hospitals     Ernst Str ngmann Institute for Neuroscience   Singer Lab     TUM   Portugues Lab    octocat      University of Pennsylvania   Kording Lab    octocat      Penn Image Computing   Science    keyboard      MIT  Princeton   Seung Lab    keyboard     octocat      MIT   brain   cognitive sciences department     Stanford   Cognitive   Systems Neuroscience    octocat      Brain Dynamics    octocat      Lee Lab     Carnegie Mellon   Cognitive Neuroscience     Computational Neuroscience     Neuro Tech   Engineering     Systems Neuroscience     Berkeley   Compuatation and Language    octocat      Perceptual Reality    octocat      Gallant Lab    octocat      Theunissen Lab    octocat      Bouchard Lab    octocat      OIST   Neural Coding and Brain Computing    octocat      Duke NUS   Neuroscience and Behavioural Disorders     KAIST   Cognitive Neuroscience and Neuroimaging     Brain Dynamics     Behavioral Genetics     Synaptic Brain Dysfunction     Neural Interoception     Sensory processing     Systems neuroscience     Companies       Neuralink    octocat         LVIS    Lee Lab  Stanford  visualization      Imaging Biometrics      Numenta    octocat         INSIGHTEC    neurosurgery  lesion with no incisions  ultrasound      AMRIT    David Feinberg  Berkeley  imaging      Neurolutions    BCI  improve arms for people after stroke  FDA   Organizations      INCF  International Neuroinformatics Coordinating Facility   octocat       BRAIN Initiative    keyboard       IEEE Brain      Allen Institute of Brain Science    octocat       BICCN  BRAIN Initiative Cell Census Network  Allen Institute of Brain Science   octocat     keyboard       NWB  Neuroscience without borders   keyboard       IARPA  Intelligence Advanced Research Projects Activity      SFN Sigapore  Society for Neuroscience Singapore chapter      KSBNS  Korean Society for Brain and Neural Sciences      AKNeuro  Assosiation of Korean Neuroscientists      KBRI  Korea Brain Research Institute      JNSS  Japan Neuroscience Society      hello                        I m currently working on backend    I m currently learning   Principles of Neuroscience     I m doing opensource  Kong     28    json iterator go     480   and lots  more       I m hosting  calendarheatmap io     go cover treemap io    I m talking at  seminars    KAIST Computer Science BSc  CKAD  4x AWS  1x GCP  5x NVIDIA  4x Redis  1x Terraform Certified     Ask me about Go  Python  Software  Data  ML  Neuroscience     How do I add JSON examples to  openapi yaml  from   json  files      Add to your  openapi yaml  annotation   source  filepath   like      yaml openapi  3 0 0 info    version  1 0 0   title  Swagger Marvelstore   paths     users      get        responses           200             description  A user object            content              application json                schema                  type  object                 properties                    id                      type  integer                     format  int64                     example  4                   name                      type  string                     example  Jessica Smith               examples                  basic                    value   source testdata user basic json          400             description  The specified user ID is invalid  not a number             content              application json                examples                  basic 400                    value   source testdata error 400 json          then run      go install github com nikolaydubina openapi inline examples latest   cat openapi yaml   openapi inline examples   openapi new yaml      which will produce      yaml openapi  3 0 0 info    version  1 0 0   title  Swagger Marvelstore   paths     users      get        responses           200             description  A user object            content              application json                schema                  type  object                 properties                    id                      type  integer                     format  int64                     example  4                   name                      type  string                     example  Jessica Smith               examples                  basic                    value    id  42  name   Nick Fury    source testdata user basic json          400             description  The specified user ID is invalid  not a number             content              application json                examples                  basic 400                    value    errors     message   resource not found   status  400  translation data   some translation identifier      source testdata error 400 json          which renders nicely as multiple examples that you can select     Why would anyone need this      Keep your OpenAPI up to date with values you use in tests   Can run multiple times   UNIX filter   Does not corrupt anything if fails at any stage   100  test coverage   Collect data about your dependencies            bash   go install github com nikolaydubina jsonl graph latest   go install github com nikolaydubina import graph latest   and get https   graphviz org download          Features      x  Go modules  runs tests  detects tests and benchmarks    x  git log    x  goreportcard com    x  codecov io    x  Analyzes README md    x  Checks if mentioned in Awesome lists    x  GitHub Stars       GitHub verified Organizations       reuse  go get  and  go list  to get code loaded by native Go routines           add yours here     Example     go mod graph   import graph  i gomod   jsonl graph  color scheme file    PWD basic json   dot  Tsvg   output svg     Output in  JSONL  graph    go mod graph   import graph  i gomod   id   golang org x net   can get gitstats  true  can get codecov  false              from   github com gin gonic gin   to   github com gin contrib sse     from   github com gin gonic gin   to   github com go playground validator v10     Pretty print with  jq   bash   go mod graph   import graph  i gomod   jq  f            id    github com gin gonic gin        can get git   true       can get codecov   true       can get goreportcard   true       can run tests   true       github url    https   github com gin gonic gin        git url    https   github com gin gonic gin        git last commit    2021 04 21        git last commit days since   3       git num contributors   321       codecov url    https   app codecov io gh gin gonic gin        codecov files   41       codecov lines   2036       codecov coverage   98 67       gotest has tests   true       gotest all tests passed   true       gotest num packages   6       gotest num packages with tests   4       gotest num packages tests passed   4       gotest package coverage avg   98 9       goreportcard average   0 99       goreportcard grade    A         goreportcard files   82       goreportcard issues   6       files has benchmarks   true       files has tests   true                 Notes   For GitHub you need to set to set in environment  GITHUB IMPORT GRAPH TOKEN  to your  personal GitHub token   It does not need any permissions at all  It is needed for higher quota of GitHub API calls    Related Projects     Graphviz  https   graphviz org  is a very popular tool for visualizing graph data  most of tools bellow use dot from it   Graphviz  https   graphviz org Gallery directed neural network html is nice example of dot format   Graphviz  http   magjac com graphviz visual editor online editor   Docs  https   awesomeopensource com projects dependency graph is a list of dependency visualization projects     Go  https   github com lucasepe modgv written in Go  converts go mod graph into graphviz  does not collect data  CLI  dot      Go  https   github com loov goda written in Go  analyses imports on its own  does not collect dta  CLI  dot     Go  https   github com adonovan spaghetti wirtten in Go  search and read details about selected package  web  not graphic   Go  https   github com psampaz go mod outdated written in Go  pretty prints built in Go tooling  not collecting  not dot  CLI    Go  https   github com firstrow go outdated written in Go  deprecated  collects data from GitHub  CLI   GitHub Tasks  https   github com moul depviz written in Go and JavaScript  collects data from GitHub Tasks  plots nice web UI   C    https   github com jmarkowski codeviz written in Python  C   headers analysis  does not collect data  CLI  dot     Python  https   github com thebjorn pydeps written in Python  looks for Python bytecode imports  clustering  does not collect data  CLI  dot     Python  https   github com naiquevin pipdeptree written in Python  looks for python modules locally  does not collect data  CLI  JSON and dot  Deprecated     JavaScript  https   github com auchenberg dependo written in JavaScript  does not fetch data  D3 js  CLI  HTML      JavaScript  https   github com pahen madge written in JavaScript  does not collect data  CLI  dot     JavaScript  https   github com sverweij dependency cruiser written in JavaScript  rules  does not collect data  CLI  dot     JavaScript  https   github com anvaka npmgraph an written in JavaScript  collects data  HTML  hosted in GitHub Pages     JavaScript  https   github com anvaka npmgraphbuilder written in JavaScript  collects data  module     JavaScript  https   github com dyatko arkit written in JavaScript  modules and dependencies  CLI  svg  puml     JavaScript  https   github com hughsk colony written in JavaScript  does not collect data  HTML  JSON     JavaScript  https   www npmjs com package node dependency visualizer written in JavaScript  does not collectdata  CLI  dot     Objective C   Swift  https   github com PaulTaykalo objc dependency visualizer written in JavaScript and Ruby  does not collect data  CLI  dot  HTML  D3 js      Java  https   github com arunkumar9t2 scabbard written in Kotlin  CLI  dot     PHP  https   github com mamuz PhpDependencyAnalysis written in PHP  does not collect data  code analysis  CLI  dot     Go   Python   Java   JavaScript   C    https   github com oss review toolkit ort written in Kotlin JavaSCript Python  collects data  analyses  analysis  downloading  reporting  used for licence scanning in open source  good architecture  a bit lacking support for Go  components may not be used separately     Code  https   github com aspiers git deps written in Python  analyses dependencies of commits in Git repository     JSONL Graph Tools     Convenient to use with  jq                       get https   graphviz org download      go install github com nikolaydubina jsonl graph latest       What is JSONL graph  Node has  id   Edge has  from  and  to           id    github com gin gonic gin        can get github   true       github url    https   github com gin gonic gin        git last commit    2021 04 21        git num contributors   321                       from    github com gin gonic gin        to    golang org x tools               Examples   Kubernetes Pod Owners   bash   kubectl get pods  o json   jq   items      to    kind          metadata name   from    metadata ownerReferences   kind          metadata ownerReferences   name      jsonl graph   dot  Tsvg   k8s pod owners svg     Large nodes and color scheme  bash   cat     id   github com gin gonic gin   can get git  true          id   github com gin contrib sse   can get git  true  can run tests  true             from   github com gin gonic gin   to   golang org x tools     from   github com gin gonic gin   to   github com go playground validator v10       jsonl graph  color scheme file    PWD testdata colors json   dot  Tsvg   colored svg     Small nodes or only edges  bash   cat     from   github com nikolaydubina jsonl graph graph   to   bufio     from   github com nikolaydubina jsonl graph graph   to   bytes     from   github com nikolaydubina jsonl graph graph   to   encoding json     from   github com nikolaydubina jsonl graph graph   to   errors     from   github com nikolaydubina jsonl graph graph   to   fmt           jsonl graph   dot  Tsvg   small svg     All Kubernetes Pod Owners with details      bash   add edges     kubectl get pods  o json   jq   items      to   metadata name  from   metadata ownerReferences   name     k8s pod owners details jsonl   add node details     kubectl get rs  o json   jq   items      id     metadata name     k8s pod owners details jsonl   kubectl get pods  o json   jq   items      id     metadata name     k8s pod owners details jsonl   flatten objects and render     cat k8s pod owners details jsonl   jq    as  in   reduce leaf paths as  path              path   map tostring    join         in   getpath  path        jsonl graph   dot  Tsvg   k8s pod owners svg         Rendering   Currently only Graphviz is supported  Follow progress of native Go graph rendering in  github com nikolaydubina go graph layout   Once it is ready  it will be integrated into this project  Go Recipes      Handy well known and  lesser  known tools for Go projects     Know some cool tool or one liner  Have a feature request or an idea    Feel free to edit this page or create an Issue Discussion            Contents     Testing     Make treemap of code coverage     Browse code coverage by file     Browse code coverage of Go code in terminal     Pretty print coverage of Go code in terminal     Make histogram of Go files per package     Run tests sequentially     Run tests in parallel     Detect goroutine leaks     Run tests with pretty output     Enrich  go test  output     Get packages without tests   Dependencies     Get Go version of current module     Get Go versions of upstream modules     Get directly dependent modules that can be upgraded     Get upstream modules without Go version     Get available module versions     Make graph of upstream modules     Make graph of upstream modules with  gmchart     Make graph of upstream packages     Scrape details about upstream modules and make graph     Scrape licenses of upstream dependencies     Explore upstream dependencies interactively     Use  go mod  directives     Analyze dependencies with  goda   Code Visualization     Make graph of function calls     Make graph of function calls in package     Make PlantUML diagram via  goplantuml     Make PlantUML diagram via  go plantuml     Make 3D chart of Go codebase   Static Analysis     Run default static analysis with  go vet     Run custom static analysis tool with  go vet     Run official static analyzers not included in  go vet     Rely on compiler for stricter Enums     Detect non exhaustive switch and map with  exhaustive     Detect usafe code with  go safer     Calculate cognitive complexity with  gocognit     Calculate age of comments     Detect mixing pointer and value method receivers with  smrcptr   Code Generation     Run  go generate  in parallel     Generate  String  method for enum types   Refactoring     Replace symbol   Errors     Pretty print  panic  messages   Build     Show compiler optimization decisions on heap and inlining     Disable inlining     Aggressive inlining     Manually disable or enable  cgo     Include metadata in binary during compilation with  ldflags     Make treemap breakdown of Go executable binary   Assembly     Get assembly of Go code snippets online     Get Go compiler SSA intermediary representation     View Go assembly interactively     Generate Go assembly in Go     Generate AST for code snippets   Execution     Run Go function in shell     Run simple fileserver     Monitor Go processes     Create 3D visualization of concurrency traces   Monitoring     Auto Instrument all functions with  go instrument     Auto Instrument all functions with  otelinji   Benchmarking     Run benchmarks     Table driven benchmarks     Generate benchmak CPU and Memory profiles     Visualize callgraph of profiles wiht  pprof     Visualize flamegraphs of profiles wiht  pprof     Visualize profiles online     Get delta between two benchmarks with  benchstat     Get summary of benchmarks with  benchstat     Continuous benchmarking     Continuous benchmarking with  gobenchdata     Continuous benchmarking with  benchdiff     Continuous benchmarking with  cob     Generate live traces using  net http trace     Generate traces using  go test     View traces with  go tool trace     Get wallclock traces     Get on off CPU profiles   Documentation     Make alternative documentation with golds     Read Go binary documentation in  man  format   Style Guide     Google     Uber     Testing       Make treemap of code coverage   Visualize distribution of code coverage in your project  This helps to identify code areas with high and low coverage  Useful when you have large project with lots of files and packages  This 2D image hash of your project should be more representative than a single number  Also available at https   go cover treemap io      nikolaydubina   go test  coverprofile cover out       go cover treemap  coverprofile cover out   out svg     Requirements  go install github com nikolaydubina go cover treemap latest       Browse code coverage by file   This is very helpful tool from the official Go toolchain  Similar visualization is integrated into VSCode and Goland  but can be used separately    go test  coverprofile cover out       go tool cover  html cover out         Browse code coverage of Go code in terminal   Interactively browse Go code coverage similarly to HTML provided by official Go toolchain  but in terminal      orlangure   go test  cover  coverprofile coverage out gocovsh gocovsh   profile profile out git diff   name only   gocovsh     Requirements  go install github com orlangure gocovsh latest       Pretty print coverage of Go code in terminal   It is similar to  go tool cover  html cover out  but in terminal  You can filter by functions  packages  minimum coverage  and more      nikandfor   cover     Requirements  go install github com nikandfor cover latest       Make histogram of Go files per package   Find when package is too big or too small  Adjust histogram length to maximum value    go list  json         jq  rc    ImportPath    GoFiles   length   tostring     join         perl  lane  print      x  20    F 1        x  F 1         F 1     t    F 0      Example                     18 github com gin gonic gin                      13 github com gin gonic gin binding                      1  github com gin gonic gin internal bytesconv                      1  github com gin gonic gin internal json                      11 github com gin gonic gin render   Requirements  https   stedolan github io jq download        Run tests sequentially   Use when you need to synchronize tests  for example in integration tests that share environment   Official documentation     go test  p 1  parallel 1             Run tests in parallel   Add  t Parallel  to your tests case function bodies  As per documentation  by default   p GOMAXPROCS  and   parallel GOMAXPROCS  when you run  go test   Different packages by default run in parallel  and tests within package can be enforced to run in parallel too  Make sure to copy test case data to new variable  why explained  here    Official documentation     go             for    tc    range tests           tc    tc         t Run tc name  func t  testing T                t Parallel                         Detect goroutine leaks   Refactored  tested variant of the goroutine leak detector found in both  net http  tests and the cockroachdb source tree  You have to call this library in your tests      fortytw2    https   github com fortytw2 leaktest      go func TestPoolContext t  testing T        ctx  cancel    context WithTimeout context Background    time Second      defer cancel       defer leaktest CheckContext ctx  t      go func         for           time Sleep time Second                           Run tests with pretty output   This wrapper around  go test  renders test output in easy to read format  Also supports JUnit  JSON output  skipping slow tests  running custom binary      dnephin    https   github com gotestyourself gotestsum   gotestsum   format dots     Requirements  go install gotest tools gotestsum latest       Enrich  go test  output   Add colors and enrich  go test  output  It can be used in CI pipeline and has lots of options      kyoh86   richgo test           Requirements  go install github com kyoh86 richgo latest       Get packages without tests   If code coverage does not report packages without tests  For example for CI or quality control      nikolaydubina   go list  json         jq  rc  select   TestGoFiles   length   0     ImportPath    Example  github com gin gonic gin ginS github com gin gonic gin internal json   Requirements  https   stedolan github io jq download    Dependencies       Get Go version of current module   For example  setup correct Go version automatically from  go mod  in CI    go mod edit  json   jq  r  Go   Requirements  https   stedolan github io jq download        Get Go versions of upstream modules   Use this when upgrading version of Go or finding old modules    go list  deps  json         jq  rc  select  Standard  true and  Module GoVersion  null      Module GoVersion  Module Path    join         sort  V   uniq   Example  1 11 github com ugorji go codec 1 11 golang org x crypto 1 12 github com golang protobuf   Requirements  https   stedolan github io jq download        Get directly dependent modules that can be upgraded   Keep your modules updated  Similar function is integrated in VSCode official Go plugin and GoLand    go list  u  m   go list  m  f     Indirect          all   grep   false    cut  d      f2    grep        Example  github com goccy go json v0 5 1  v0 7 3  github com golang protobuf v1 3 3  v1 5 2  github com json iterator go v1 1 9  v1 1 11        Get upstream modules without Go version   Find outdated modules or imports that you need to upgrade    go list  deps  json         jq  rc  select  Standard  true and  Module GoVersion  null     Module Path    sort  u   Example  github com facebookgo clock golang org x text gopkg in yaml v2   Requirements  https   stedolan github io jq download        Get available module versions   This works even if you did not download or install module locally  This is useful to check to which version you can upgrade to  what is the latest version  and whether there are v2  major versions recognized by Go toolchain    go list  m  versions github com google gofuzz       Make graph of upstream modules   For each module  the node representing the greatest version  i e   the version chosen by Go s minimal version selection algorithm  is colored green  Other nodes  which aren t in the final build list  are colored grey    official Go team   go mod graph   modgraphviz   dot  Tsvg  o mod graph svg     Requirements  https   graphviz org download  go install golang org x exp cmd modgraphviz latest       Make graph of upstream modules with  gmchart   Render in browser Go module graphs  Built with D3 js  Javascript  HTTP server in Go      PaulXu cn   go mod graph   gmchart     Requirements  go install github com PaulXu cn go mod graph chart gmchart latest       Make graph of upstream packages   Find unexpected dependencies or visualize project  Works best for small number of packages  for large projects use  grep  to narrow down subgraph  Without   deps  only for current module      nikolaydubina   go list  deps  json         jq  c  select  Standard  true     from   ImportPath  to   Imports       jsonl graph   dot  Tsvg   package graph svg     Requirements  https   stedolan github io jq download  https   graphviz org download  go install github com nikolaydubina import graph latest go install github com nikolaydubina jsonl graph latest       Scrape details about upstream modules and make graph   Find low quality or unmaintained dependencies      nikolaydubina   go mod graph   import graph  i gomod   jsonl graph  color scheme file    PWD basic json   dot  Tsvg   output svg     Requirements  https   graphviz org download  go install github com nikolaydubina import graph latest go install github com nikolaydubina jsonl graph latest       Scrape licenses of upstream dependencies   Collect all the licenses for checking if you can use the project  for example in proprietary or commercial environment    Google   go licenses csv github com gohugoio hugo   Example  github com cli safeexec https   github com cli safeexec blob master LICENSE BSD 2 Clause github com bep tmc https   github com bep tmc blob master LICENSE MIT github com aws aws sdk go https   github com aws aws sdk go blob master LICENSE txt Apache 2 0 github com jmespath go jmespath https   github com jmespath go jmespath blob master LICENSE Apache 2 0 github com gorilla websocket https   github com gorilla websocket blob master LICENSE BSD 2 Clause github com pelletier go toml v2 https   github com pelletier go toml blob master v2 LICENSE MIT github com spf13 cobra https   github com spf13 cobra blob master LICENSE txt Apache 2 0 github com kyokomi emoji v2 https   github com kyokomi emoji blob master v2 LICENSE MIT go opencensus io Unknown Apache 2 0 github com Azure azure storage blob go azblob https   github com Azure azure storage blob go blob master azblob LICENSE MIT github com yuin goldmark highlighting https   github com yuin goldmark highlighting blob master LICENSE MIT   Requirements  go install github com google go licenses latest       Explore upstream dependencies interactively   Useful in large refactorings  dependency breaking  physical layout changes     Alan Donovan   official Go team     Requirements  go install github com adonovan spaghetti latest       Use  go mod  directives   Tell Go compiler which versions of upstreams to include in your build  Tell all users of your module how to deal with versions of your module           Deprecated  use example com mod v2 instead  module example com mod   go 1 16   require example com other thing v1 0 2 require example com new thing v2 v2 3 4 exclude example com old thing v1 2 3 replace example com bad thing v1 4 5    example com good thing v1 4 5 retract  v1 9 0  v1 9 5            Analyze dependencies with  goda   This tool has extensive syntax for filtering dependencies graphs  It can work with packages and modules     Egon Elbre   goda graph     dot  Tsvg  o graph svg goda graph  cluster  short  github com nikolaydubina go cover treemap all    dot  Tsvg  o graph svg     Requirements  https   graphviz org download  go install github com loov goda latest   Code Visualization       Make graph of function calls   Visualize complex or new project quickly or to study project  Requires  main go  in module  Supports Graphviz output format  Has many options for filtering and formatting    official Go team   callgraph  format graphviz     dot  Tsvg  o graph svg recommend  grep  package class func of interest  recommend  grep  v Error since many packages report error recommend  adding  rankdir LR   to graphviz file for denser graph recommend  you would have to manually fix graphviz file first and last line     Requirements  go install golang org x tools cmd callgraph latest       Make graph of function calls in package   Quickly track which packages current package is calling and why      ofabry   go callvis       Requirements  go install github com ofabry go callvis       Make PlantUML diagram via  goplantuml   Generates class diagram in widely used format with the information on structs  interfaces and their relationships  Render   puml  files in for example  planttext com       jfeliu007   goplantuml  recursive path to gofiles path to gofiles2     Requirements  go get github com jfeliu007 goplantuml parser go install github com jfeliu007 goplantuml cmd goplantuml latest       Make PlantUML diagram via  go plantuml   Automatically generate visualization of classes and interfaces for go packages  Recommend recursive option  Render   puml  files in for example  planttext com       bykof   go plantuml generate  d    r  o graph puml     Requirements  go install github com bykof go plantuml latest       Make 3D chart of Go codebase   Fresh artistic perspective on Go codebase   GoCity  is an implementation of the Code City metaphor for visualizing source code   folders are districts  files are buildings  structs are buildings on the top of their files  This project has research paper   GoCity Code City for Go   at SANER 19  Also available at  go city github io       rodrigo brito     Requirements  go install github com rodrigo brito gocity latest   Static Analysis       Run default static analysis with  go vet   Official tool for static analysis of Go programs  with 27  static analyzers    official Go team   go vet             Run custom static analysis tool with  go vet   Standard  go vet  can be used to run custom analyzers binaries  Third party analyzers are supported  Lots of official analyzers not included by default into  go vet   Analyzer has to satisfy interface and command described here https   pkg go dev golang org x tools go analysis  Refer for https   pkg go dev golang org x tools go analysis passes for full list of official Go analyzers    official Go team   go install golang org x tools go analysis passes shadow cmd shadow go vet  vettool   which shadow        Run official static analyzers not included in  go vet   There are many analyzers not included in  go vet   These tools are experimental and may not work as expected  e g   usesgenerics  does not work   Refer to for full list https   pkg go dev golang org x tools go analysis    official Go team      go package main   import        golang org x tools go analysis multichecker     golang org x tools go analysis passes atomicalign   golang org x tools go analysis passes deepequalerrors   golang org x tools go analysis passes fieldalignment   golang org x tools go analysis passes nilness   golang org x tools go analysis passes reflectvaluecompare   golang org x tools go analysis passes shadow   golang org x tools go analysis passes sortslice   golang org x tools go analysis passes unusedwrite   golang org x tools go analysis passes usesgenerics         func main         multichecker Main          atomicalign Analyzer             checks for non 64 bit aligned arguments to sync atomic functions         deepequalerrors Analyzer         checks for the use of reflect DeepEqual with error values         fieldalignment Analyzer          detects structs that would use less memory if their fields were sorted         nilness Analyzer                 inspects the control flow graph of an SSA function and reports errors such as nil pointer dereferences and degenerate nil pointer comparisons         reflectvaluecompare Analyzer     checks for accidentally using    or reflect DeepEqual to compare reflect Value values         shadow Analyzer                  checks for shadowed variables         sortslice Analyzer               checks for calls to sort Slice that do not use a slice type as first argument         unusedwrite Analyzer             checks for unused writes to the elements of a struct or array object         usesgenerics Analyzer            checks for usage of generic features added in Go 1 18                     Rely on compiler for stricter Enums   For compile time blocking of  accidental arithmetics  implicit cast of untyped constants  all operators except      and         simply wrap into a struct in separate package and do not export field       go package color   type Color struct  c uint     var       Undefined   Color       Red         Color 1      Green       Color 2      Blue        Color 3                Detect non exhaustive switch and map with  exhaustive   This  go vet  compatible analyzer checks for exhaustive switch statemnts and map literals  It works for enums with underyling integer  float  or string types  struct based enums are not supported       nishanths   exhaustive             package token   type Token int   const       Add Token   iota     Subtract     Multiply     Quotient     Remainder     package calc   import  token    func f t token Token        switch t       case token Add      case token Subtract      case token Multiply      default            func g t token Token  string       return map token Token string          token Add        add           token Subtract   subtract           token Multiply   multiply         t          Example  calc go 6 2  missing cases in switch of type token Token  Quotient  Remainder calc go 15 9  missing map keys of type token Token  Quotient  Remainder   Requirements  go install github com nishanths exhaustive cmd exhaustive latest       Detect usafe code with  go safer   Find incorrect uses of  reflect SliceHeader    reflect StringHeader   and unsafe casts between structs with architecture sized fields  Reseach paper   Uncovering the Hidden Dangers Finding Unsafe Go Code in the Wild   presented at 19th IEEE International Conference on Trust  Security and Privacy in Computing and Communications  TrustCom 2020       jlauinger   go safer         Example       github com jlauinger go safer passes sliceheader testdata src bad composite literal   composite literal composite literal go 10 9  reflect header composite literal found composite literal composite literal go 10 9  reflect header composite literal found   github com jlauinger go safer passes sliceheader testdata src bad header in struct   header in struct header in struct go 16 2  assigning to reflect header object header in struct header in struct go 16 2  assigning to reflect header object header in struct header in struct go 17 2  assigning to reflect header object header in struct header in struct go 17 2  assigning to reflect header object       Requirements  go install github com jlauinger go safer latest       Calculate cognitive complexity with  gocognit   Congitive Complexity as defined in this tool can be more illustrative than Cyclometric Complexity  Research paper   Cognitive Complexity   a new way of measuring understandability    2021      uudashr   gocognit            Complexity Cyclomatic 4 Cognitive 7    Cognitive complexity give higher score compare to cyclomatic complexity  func SumOfPrimes max int  int               1     var total int     for i    1  i   max  i                  1  cognitive  1  nesting          for j    2  j   i  j                1  cognitive  2  nesting              if i j    0                     1                 continue OUT                                 total    i           return total        Complexity Cyclomatic 4 Cognitive 1    Cognitive complexity give lower score compare to cyclomatic complexity  func GetWords number int  string            1     switch number           case 1                              1  cognitive 0              return  one          case 2                              1  cognitive 0              return  a couple          case 3                              1  cognitive 0               return  a few          default               return  lots                Example    go binsize treemap   gocognit   21 main  BasicSymtabConverter  SymtabFileToTreemap basic converter go 23 1 12 symtab parseGoSymtabLine symtab go symtab parser go 37 1 11 main main main go 30 1 8 symtab EqSymbolName symtab symbol name parser go 12 1 7 symtab ParseSymbolName symtab symbol name parser go 32 1 7 symtab Test parseGoSymtabLine symtab go symtab parser private test go 5 1 4 symtab Test ParseSymbolName symtab symbol name parser private test go 5 1 3 main updateNodeNamesWithByteSize main go 99 1 3 main unique basic converter go 119 1 3 symtab  GoSymtabParser  ParseSymtab symtab go symtab parser go 14 1 2 fmtbytecount ByteCountIEC fmtbytecount format bytecount go 3 1   Requirements  go install github com uudashr gocognit cmd gocognit latest       Calculate age of comments   This go vet compatible tool analyses AST and git and collects details on how far comments drift from code they describe      nikolaydubina   go commentage  min days behind 360         Example  kubernetes pkg util ipset ipset go 283 1   CreateSet   doc last updated behind days 1336 83  kubernetes pkg util ipset ipset go 296 1   createSet   doc last updated behind days 1603 17  kubernetes pkg util ipset ipset go 320 1   AddEntry   doc last updated behind days 1578 10  kubernetes pkg util ipset ipset go 332 1   DelEntry   doc last updated behind days 1578 10  kubernetes pkg util ipset ipset go 340 1   TestEntry   doc last updated behind days 450 07    Requirements  go install github com nikolaydubina go commentage latest       Detect mixing pointer and value method receivers with  smrcptr   This  go vet  compatible linter detects mixing pointer and value method receivers for the same type      nikolaydubina   smrcptr            go type Pancake struct     func NewPancake   Pancake   return Pancake       func  s  Pancake  Fry        func  s Pancake  Bake            Example  smrcptr internal bakery pancake go 7 1  Pancake Fry uses pointer smrcptr internal bakery pancake go 9 1  Pancake Bake uses value   Requirements  go install github com nikolaydubina smrcptr latest   Code Generation       Run  go generate  in parallel   Official Go team  encourages  to run sequentially  However  in certain situations  such as lots of mocks  parallelization helps a lot  albeit  you should consider including your generated files in git  The solution bellow spawns multiple processes  each per pkg    grep  rnw  go generate   E  l    1    go       xargs  L1 dirname   sort  u   xargs  P 8  I   go generate          Generate  String  method for enum types   This is an official tool for generating  String  for enums  It supports overrides via comments    official Go team      go package painkiller     go generate stringer  type Pill  linecomment   type Pill int   const       Placebo Pill   iota     Ibuprofen     Paracetamol     PillAspirin      Aspirin     Acetaminophen   Paracetamol         Acetaminophen  var s string   Acetaminophen String           Requirements  go install golang org x tools cmd stringer latest   Refactoring       Replace symbol   I found this in announcement  notice  of Go 1 18 for changes to  interface    to  any   This can be useful for other refactorings too    gofmt  w  r  interface      any      Errors       Pretty print  panic  messages   Read  panic  messages easier  Need to redirect STDERR to this tool with  panic  stack traces  The tool has HTML output and does lots of deduplication and enhancements  Refer to examples in original repo    go test  v    pp     Requirements  go install github com maruel panicparse v2 cmd pp latest   Build       Show compiler optimization decisions on heap and inlining   Building with   m  flag will show decisions of compiler on inlining and heap escape  This can help you to validate your understanding of your code and optimize it    go build  gcflags   m  m    2  1   grep inline   Example        passengerfp go 25 6  cannot inline   PassengerFeatureTransformer  Fit  function too complex  cost 496 exceeds budget 80       passengerfp go 192 6  can inline   PassengerFeatureTransformer  NumFeatures with cost 35 as  method  PassengerFeatureTransformer  func   int   if e    nil   return 0    count    6  count      transformers OneHotEncoder  NumFeatures e Sex   count      transformers OneHotEncoder  NumFeatures e Embarked   return count         passengerfp go 238 43  inlining call to transformers   OneHotEncoder  FeatureNames   passengerfp go 238 43  inlining call to transformers   OneHotEncoder  NumFeatures       passengerfp go 151 7  parameter e leaks to  heap  with derefs 0    passengerfp go 43 11  make map string uint  escapes to heap       Disable inlining   Usually you may not need it  but can reduce binary size and even improve performance    go build  gcflags   l          Aggressive inlining   Usually you may not need it  but can improve performance  This includes mid stack inlining    go build  gcflags   l  l  l  l          Manually disable or enable  cgo   Disable  cgo  with  CGO ENABLED 0  and enable with  CGO ENABLED 1   If you don t   cgo  may end up being enabled or code dynamically linked if  for example  you use some  net  or  os  packages  You may want to disable  cgo  to improve performance  since complier and runtime would have easier job optimizing code  This also should reduce your image size  as you can have alpine image with less shared libraries        Include metadata in binary during compilation with  ldflags   You can pass metadata through compiler to your binary  This is useful for including things like git commit  database schema version  integrity hashes  Variables can only be strings    go build  v  ldflags   X  main Version v1 0 0   go build  v  ldflags   X  my pkg here Variable some string        go package main   var Version string   func main            Version here has some value                       Make treemap breakdown of Go executable binary   Useful for studying Go compiler  large projects  projects with C C   and  cgo   3rd party dependencies  embedding  However  total size may not be something to worry about for your executable      nikolaydubina   go tool nm  size  binary finename    go binsize treemap   binsize svg     Requirements  go install github com nikolaydubina go binsize treemap latest   Assembly       Get assembly of Go code snippets online   Use  godbolt org  to compile and see assembly of short Go code  You can check different platforms and compilers including  cgo   This tool is commonly used by C   community      mattgodbolt         Get Go compiler SSA intermediary representation   Check what does Go compiler do  Might be useful if you trying to optimize some code or learn more about compiler  https   golang design gossa      changkun    https   github com golang design ssaplayground         View Go assembly interactively   Understand how Go is compiled better      egonelbre    https   github com loov lensm     Requirements  go install loov dev lensm main       Generate Go assembly in Go   Write better quality Go assembly quicker in Go language itself  This tool conveniently generates stub for Go code to call your generated assembly  Used by Go core      mmcloughlin    https   github com mmcloughlin avo      go   go build ignore     build ignore   package main   import    github com mmcloughlin avo build    func main         TEXT  Add   NOSPLIT   func x  y uint64  uint64       Doc  Add adds x and y        x    Load Param  x    GP64        y    Load Param  y    GP64        ADDQ x  y      Store y  ReturnIndex 0       RET       Generate                 Generate AST for code snippets   Access Go core AST mechanism to generate AST       go package main   import        go ast       go parser       go token      func main         fs    token NewFileSet       tr       parser ParseExpr   3 1    5       ast Print fs  tr            Example          0   ast BinaryExpr        1     X   ast ParenExpr        2        Lparen         3        X   ast BinaryExpr        4           X   ast BasicLit        5              ValuePos         6              Kind  INT      7              Value   3       8                  9           OpPos        10           Op        11           Y   ast BasicLit       12              ValuePos        13              Kind  INT     14              Value   1      15                 16              17        Rparen        18           19     OpPos        20     Op        21     Y   ast BasicLit       22        ValuePos        23        Kind  INT     24        Value   5      25           26            Execution       Run Go function in shell   Run Go one liners  This tool will print to stdout the return of a function call      natefinch   cat README md   gorram crypto sha1 Sum echo 12345   gorram encoding base64 StdEncoding EncodeToString gorram net http Get https   google com   Requirements  go install github com natefinch gorram latest       Run simple fileserver   It takes one line to run HTTP file server in Go  Akin to famous oneliner in Python  python3  m http server  and  python  m SimpleHTTPServer   Run this file as usually  go run  filename         go package main   import  net http    func main     http ListenAndServe   9000   http FileServer http Dir                        Monitor Go processes   Monitoring memory of Go processes  forcing GC  getting version of Go of processes    Google   gops   Example  983   980    uplink soecks  go1 9    usr local bin uplink soecks 52697 52695  gops           go1 10   Users jbd bin gops 4132  4130   foops          go1 9    Users jbd bin foops 51130 51128  gocode         go1 9 2  Users jbd bin gocode   Requirements  go install github com google gops latest       Create 3D visualization of concurrency traces   Fresh artistic perspective on coroutines execution  There is no advanced functions and it is hard to analyze production systems  However  it could be interesting for educational purposes      divan     Requirements  go install github com divan gotrace latest patch Go compiler  available via Docker more instructions in original repo   Monitoring       Auto Instrument all functions with  go instrument   Automatically instrument all functions with Open Telemetry Spans by code generation  Inserts errors into Spans      nikolaydubina   find    name    go    xargs  I   go instrument  app my service  w  filename        Requirements  go install github com nikolaydubina go instrument latest       Auto Instrument all functions with  otelinji   Automatically instrument all functions with Open Telemetry Spans by code generation  Inserts errors into Spans  Supports custom templates and can be used for Open Tracing or any custom insertions      hedhyw   otelinji  w  filename input file go otelinji  filename input file go   input file go find    name    go    grep  v  vendor    git    test go    xargs  n 1  t otelinji  w  filename     Requirements  go install github com hedhyw otelinji cmd otelinji latest   Benchmarking       Run benchmarks   Start here  This is the standard tool for benchmarking  It can also do advanced features like mutex profiles  More flags are in Go  documentation  and  go help testflag     go test  bench    benchmem  benchtime 10s         Example    go test  bench    benchmem       goos  darwin goarch  arm64 pkg  github com nikolaydubina fpmoney BenchmarkArithmetic add x1 10                     1000000000             0 5 ns op           0 B op           0 allocs op BenchmarkArithmetic add x100 10                     18430124            64 6 ns op           0 B op           0 allocs op BenchmarkJSONUnmarshal small 10                      3531835           340 7 ns op         198 B op           3 allocs op BenchmarkJSONUnmarshal large 10                      2791712           426 9 ns op         216 B op           3 allocs op BenchmarkJSONMarshal small 10                        4379685           274 4 ns op         144 B op           4 allocs op BenchmarkJSONMarshal large 10                        3321205           345 8 ns op         192 B op           5 allocs op PASS ok      github com nikolaydubina fpmoney    62 744s       Table driven benchmarks   Similar to tests  Go supports table driven benchmarks  which is very helpful for fine gradation of meta parameters  More details in the Go  blog        go func benchIteratorSelector b  testing B  n int               setup here     b ResetTimer       for n    0  n   b N  n             err    myExpensiveFunc           if err    nil               b Error err                      func BenchmarkIteratorSelector b  testing B        for    q    range   int 100  1000  10000  100000            b Run fmt Sprintf  n  d   q   func b  testing B                benchIteratorSelector b  q                             Example  BenchmarkIteratorSelector n 100 10        297792          4265 ns op        5400 B op         13 allocs op BenchmarkIteratorSelector n 1000 10        31400         38182 ns op        9752 B op         16 allocs op BenchmarkIteratorSelector n 10000 10        3134        380777 ns op       89112 B op         24 allocs op BenchmarkIteratorSelector n 100000 10        310       3827292 ns op      912410 B op         32 allocs op       Generate benchmak CPU and Memory profiles   This is useful for identifying most time or memory consuming parts  Recommended to run for single benchmark at a time and with   count  or   benchtime  for better accuracy    go test  bench  my benchmark name   cpuprofile cpu out  memprofile mem out             Visualize callgraph of profiles wiht  pprof   Once you generate profiles  visualize them with  pprof   Both memory and CPU profiles are supported  Many options are available  Refer to the link you get in SVG to how to interpret this graph  More official documentation  blog    pkg doc     official Go team   go tool pprof  svg cpu out   cpu svg go tool pprof  svg mem out   mem svg         Visualize flamegraphs of profiles wiht  pprof   Latest versions of  pprof  can also render  Flamegraphs  for profiles  Make sure you set   http  to start webserver  Then it is available in  View   Graph  in at http   0 0 0 0 80    Google   pprof  http 0 0 0 0 80 cpu out     Requirements  go install github com google pprof latest       Visualize profiles online   You can also visualize profiles with online tools are aloso available https   www speedscope app  cpu           Get delta between two benchmarks with  benchstat   This is standard way to compare two benchmark outputs  Names of benchmarks should be the same  Generate benchmarks as per usual  You would get multiple tables per dimension  If no output  then pass   split  XYZ    If you do not see  delta   then pass   count 2  or more in benchmark generation  It is recommended to have alternative implementations in different packages  to keep benchmark names the same    official Go team   benchstat  split  XYZ  old txt new txt   Example     name                    old time op    new time op    delta JSONUnmarshal small 10     502ns   0      331ns   0     33 99    p 0 008 n 5 5  JSONUnmarshal large 10     572ns   0      414ns   0     27 64    p 0 008 n 5 5  JSONMarshal small 10       189ns   0      273ns   0     44 20    p 0 008 n 5 5  JSONMarshal large 10       176ns   0      340ns   0     93 29    p 0 008 n 5 5    name                    old alloc op   new alloc op   delta JSONUnmarshal small 10      271B   0       198B   0     26 94    p 0 008 n 5 5  JSONUnmarshal large 10      312B   0       216B   0     30 77    p 0 008 n 5 5  JSONMarshal small 10       66 0B   0     144 0B   0    118 18    p 0 008 n 5 5  JSONMarshal large 10       72 0B   0     192 0B   0    166 67    p 0 008 n 5 5    name                    old allocs op  new allocs op  delta JSONUnmarshal small 10      6 00   0       3 00   0     50 00    p 0 008 n 5 5  JSONUnmarshal large 10      7 00   0       3 00   0     57 14    p 0 008 n 5 5  JSONMarshal small 10        2 00   0       4 00   0    100 00    p 0 008 n 5 5  JSONMarshal large 10        2 00   0       5 00   0    150 00    p 0 008 n 5 5        Requirements  go install golang org x perf cmd benchstat latest       Get summary of benchmarks with  benchstat   Compare multiple benchmarks  Names of benchmarks should be the same  Generate benchmarks as per usual  You would get multiple tables per dimension  If no output  then pass   split  XYZ    It is recommended to have alternative implementations in different packages  to keep benchmark names the same    official Go team   benchstat  split  XYZ  int txt float32 txt fpmoney txt   Example     name   time op          int bench   float32 bench  fpmoney bench JSONUnmarshal small 10  481ns   2      502ns   0      331ns   0  JSONUnmarshal large 10  530ns   1      572ns   0      414ns   0  JSONMarshal small 10    140ns   1      189ns   0      273ns   0  JSONMarshal large 10    145ns   0      176ns   0      340ns   0    name   alloc op         int bench   float32 bench  fpmoney bench JSONUnmarshal small 10   269B   0       271B   0       198B   0  JSONUnmarshal large 10   288B   0       312B   0       216B   0  JSONMarshal small 10    57 0B   0      66 0B   0     144 0B   0  JSONMarshal large 10    72 0B   0      72 0B   0     192 0B   0    name   allocs op        int bench   float32 bench  fpmoney bench JSONUnmarshal small 10   6 00   0       6 00   0       3 00   0  JSONUnmarshal large 10   7 00   0       7 00   0       3 00   0  JSONMarshal small 10     2 00   0       2 00   0       4 00   0  JSONMarshal large 10     2 00   0       2 00   0       5 00   0        Requirements  go install golang org x perf cmd benchstat latest       Continuous benchmarking   Track how benchmarks change in codebase over time  This is accomplished by running benchmarks for git commits  storing results  and visualizing difference  Running benchmarks can be in GitHub Actions or locally  storage can be in same repository  master  or dedicated branch  or standalone servers  It should be straightforward to setup this manually  Example of GitHub Action  spec  and  blog  from   vearutop   and an example on how it produces a PR  comment           Continuous benchmarking with  gobenchdata   This tool uses  go test  bench  data in GitHub  It runs benchmarks  and uploads it as GitHub Pages for visualization  It is available as GitHub Action  gobenchdata   This is useful to see benchmark trends      bobheadxi    https   github com bobheadxi gobenchdata     Requirements  go install go bobheadxi dev gobenchdata latest       Continuous benchmarking with  benchdiff   Automates comparing benchmarks with  benchstat  of two git references  It is available as GitHub Action  benchdiff  which runs  benchstat  of HEAD vs base branch  This is useful to see how benchmarks change with PRs in CI      WillAbides     Requirements  go install github com willabides benchdiff cmd benchdiff       Continuous benchmarking with  cob   Automate comparing benchmarks with  benchstat  between  HEAD  and  HEAD 1   It can be used to block CI pipelines if benchmarks deteriorate  It reports output as text in CLI  This cane be useful in CI or in local development      knqyf263     Requirements  go install github com knqyf263 cob latest       Generate live traces using  net http trace   This will add endpoints to your your server  If you don t have server running already in your process  you can start one  Then you can point  pprof  tool to this data  For production  hide this endpoint in separate port and path  More details in documentation  trace    net http pprof        go package main   import        log       net http       net http pprof      func main         mux    http NewServeMux       mux HandleFunc   custom debug path profile   pprof Profile      log Fatal http ListenAndServe   7777   mux             Example  go tool pprof http   localhost 6060 debug pprof heap go tool pprof http   localhost 6060 debug pprof profile seconds 30 curl  o trace out http   localhost 6060 debug pprof trace seconds 5       Generate traces using  go test   Produce a trace of execution of tests in pacakge    go test  trace trace out         View traces with  go tool trace   You can view traces interactively in browser with standard Go tooling  This web tool also shows network blocking profile  synchronization blocking profile  syscall blocking profile  scheduler latency profile    go tool trace trace out         Get wallclock traces   This tool can be more illustrative of Go traces than standard Go traces      felixge    https   github com felixge fgtrace   Example    go package main   import        net http     github com felixge fgtrace         func main         http DefaultServeMux Handle   debug fgtrace   fgtrace Config        http ListenAndServe   1234   nil                  Get on off CPU profiles   This tool can be more illustrative of Go profiles than standard Go profiling      felixge    https   github com felixge fgprof   Example    go package main   import        log       net http         net http pprof     github com felixge fgprof         func main         http DefaultServeMux Handle   debug fgprof   fgprof Handler        go func             log Println http ListenAndServe   6060   nil                 code to profile                 Documentation       Make alternative documentation with golds   It has additional information like implementations of interface  promoted methods  The tool has nice minimalistic aesthetics     Tapir Liu    https   github com go101 golds   golds           Requirements  go install go101 org golds latest       Read Go binary documentation in  man  format   This tool fetches the repo s readme as a man page replacement      christophberger   goman  mypackage      Requirements  go install github com appliedgocode goman lates   Style Guide       Google   https   google github io styleguide go        Uber   https   github com uber go guide    Note   2022 06 11   first check this https   pkg go dev encoding json Decoder         This package was build for programmatic access of multiline JSON in Go   If you need CLI for JSON  I highly recommend  jq           bash   go install github com nikolaydubina multiline jsonl latest   For example  you want to parse input multiline JSONs    bash   echo         from   github com nikolaydubina jsonl graph graph        to   bufio        from   my id 1   to   my id 2     from   my id 5   to   my id 10    amount   123    from   my id 5   to   my id 10    amount     amount   123   currency    KRW             id    my id        number   123       nested              title    big title            nested level 2                  subtitle    some other thing                count   123                           multiline jsonl       Outputs shortened version  jsonl   from   github com nikolaydubina jsonl graph graph   to   bufio     from   my id 1   to   my id 2     amount  123  from   my id 5   to   my id 10     amount    amount  123  currency   KRW    from   my id 5   to   my id 10     id   my id   nested    nested level 2    count  123  subtitle   some other thing    title   big title    number  123    And with   expand  flag  json        from    github com nikolaydubina jsonl graph graph        to    bufio           from    my id 1        to    my id 2           amount   123       from    my id 5        to    my id 10           amount              amount   123           currency    KRW              from    my id 5        to    my id 10           id    my id        nested              nested level 2                  count   123               subtitle    some other thing                      title    big title              number   123     Here is example from https   github com nikolaydubina jsonl graph      go func NewGraphFromJSONL r io Reader   Graph  error        g    NewGraph     scanner    bufio NewScanner r  scanner Split multilinejsonl SplitMultilineJSONL   for scanner Scan         decoder    json NewDecoder bytes NewReader scanner Bytes         decoder UseNumber        var nodeEdge orNodeDataEdgeData     if err    decoder Decode  nodeEdge   err    nil           continue            node  edge  err    nodeEdge cast       if err    nil           return g  fmt Errorf  can not cast   w   err             switch       case node    nil          g AddNode  node      case edge    nil          g AddEdge  edge           return g  scanner Err              Features      x  No reflection    x  Simple Code    x  CLI    x  84  coverage     Reference     https   github com wlredeye jsonlines   reflection  no scanner Split  no multiline   https   github com neilotoole sq   no scanner Split  no multiline   https   github com emersion go jsonld   custom json tags  no multiline   https   github com qiangyt jsonlines2json   no scanner  no multiline   https   github com tylerstillwater jsonl   just a wrapper   https   github com Meromen JsonlParser   no multiline   https   github com go ap jsonld   no multiline   https   github com youpy go jsonl   no multiline   https   github com aaronland go jsonl   just a wrapper   svgpan   Pan and Zoom of SVG in your Go front end app in browser    This is port of JS  svgpan   As of 2022 05 21  Google has just took a bunch of JS code and  pasted  it as raw const string          Troubleshooting   If you are getting following error     go get github com nikolaydubina svgpan package github com nikolaydubina svgpan     imports syscall js  build constraints exclude all Go files in  usr local go src syscall js   Download like     GOARCH wasm GOOS js go get github com nikolaydubina svgpan Graph Layout Algorithms in Go   This module provides algorithms for graph visualization in native Go  As of 2021 11 20  virtually all graph visualization algorithms are bindings to Graphviz dot code which is in C  This module attempts to provide implementation of latest and best graph visualization algorithms from scratch in Go  However  given this is very complex task this is work in progress    Features      x  gonum Isomap    x  gonum Eades    x  Kozo Sugiyama layers strategy       Brandes K pf horizontal layers assignment  80  done        Graphviz dot layers algorithm  80  done     x  Gravity force    x  Spring force       Kozo Sugiyama Magnetic Force       Metro Style edges       Ports for edges       Spline edges       Collision avoidance  dot  edge path algorithm     Contributions   Yes please  These algorithms are hard  If you can  help to finish implementing any of above     If lots of contributions  I am ok to merge this into some org    References     Wiki Layered Graph Drawing    Handbook of Graph Drawing and Visualization    Roberto Tamassia  Brown  Ch 13  2013    A Technique for Drawing Directed Graphs    Emden R  Gansner Eleftherios Koutsofios Stephen C  North Kiem Phong Vo  AT T Bell Laboratories  1993    Fast and Simple Horizontal Coordinate Assignment    U  Brandes  Boris K pf  2002    Methods for visual understanding of hierarchical system structures   Sugiyama  Kozo  Tagawa  Sh jir   Toda  Mitsuhiko  1981    Graph Drawing by the Magnetic Spring Model   Kozo Sugiyama  1995     Pretty Treemaps     Looking to run this for Go coverage  Check https   github com nikolaydubina go cover treemap           bash   go install github com nikolaydubina treemap cmd treemap latest    echo   Africa Algeria 33333216 72 Africa Angola 12420476 42 Africa Benin 8078314 56         treemap   out svg     Adjusting size  bash         treemap  w 1080  h 360   out svg     bash         treemap  w 1080  h 1080   out svg     Imputing heat  bash         treemap  impute heat   out svg     Different colorscheme  bash         treemap  color RdYlGn   out svg     Tree Hue coloring when there is no heat          treemap  color balanced   out svg     Without color  bash         treemap  color none   out svg     Format   Size and heat is optional           delimitered path              Algorithms     Squarified  algorithm for treemap layout problem  This is very common algorithm used in Plotly and most of visualization packages    Squarified Treemaps   Mark Bruls  Kees Huizing  and Jarke J  van Wijk  2000   Tree Hue Color  algorithm for generating colors for nodes in treemap  The idea is to represent hierarchical structure by recursively painting similar hue to subtrees   Nikolay Dubina  2021     Contributions   Welcomed    References     Plotly treemaps  https   plotly com python treemaps    go colorful  https   github com lucasb eyer go colorful   D3 treemap is using Squerified  https   github com d3 d3 hierarchy   Interactive treemap  https   github com vasturiano treemap chart   Squerified in Rust  https   github com bacongobbler treemap rs   Squerified in JavaScript  https   github com clementbat treemap   Squerified in Python  https   github com laserson squarify   Treemap Go tool  https   github com willpoint treemap   Plotly color scales  https   plotly com python builtin colorscales   Plotly color scales source  https   github com plotly plotly py blob master packages python plotly  plotly utils colors colorbrewer py   Colorbrewer project  that is used in Plotly  http   colorbrewer2 org     Appendix A  Long Roots   When roots have one child multiple times it takes extra vertical space  which is very useful for narrow final dimensions      Can collapse them into one node    Long roots without collapsing somewhere deep inside     Long roots with collapsing somewhere deep inside     Appendix B  Less Illustrative Examples   Large dimensions and large tree  e g   github com golang go    bash         treemap  w 4096  h 4096   out svg     Go cover to Treemap     Useful when you have large project with lots of files and packages     New  Now available at https   go cover treemap io         go install github com nikolaydubina go cover treemap latest   go test  coverprofile cover out         go cover treemap  coverprofile cover out   out svg   github com gohugoio hugo       also available in 1080x360       and even 1080x180     github com gin gonic gin     github com go chi chi     github com nikolaydubina treemap     github com nikolaydubina go featureprocessing     Disclaimer   In all examples above I run  go test  coverprofile  my file          I did not do any special setup  Some projects may require additional steps to properly run tets and generate full coverprofile  What you see is  lower bound  of coverage for those projects  All profiles generated on  main  branch of each project in GitHub on 2021 12 07    Contributions   Welcomed  Add pretty color palettes  Add interesting examples    Reference     Official Go tool to make HTML from cover profile  https   github com golang go blob master src cmd cover html go L97   Official Go parser of cover profile  golang org x tools cover   https   github com golang tools tree master cover   Go SVG Treemap renderer with treemap  https   github com nikolaydubina treemap     Appendix A  Statements vs File for Size   You can see that structure and heat changes for  github com gohugoio hugo   Subtrees that look bad for files no longer look as bad for statements  Lots of red boxes for files become very small and unnoticeable  This can be because they contain non testable constructs like constants  It is more accurate to use statments  since heat is percentage of covered statements  and we compute heat by weighting sum by sizes of children  In short  you are more likely want to use statements for size    files    statements    Appendix B  Long Roots   It is common to have root and first few children to have only one child  Each takes margin and wastes space  We can collapse these into longer name  and use that space for visualizing higher depth of boxes  This is particularly useful for narrow dimensions  which makes feasible useful narrow dimension    1080x360 with root collapsing    1080x360 without root collapse    1080x180 with root collapsing    1080x180 without root collapse    Appendix C  Web UI   Web UI is maintained in dedicated repository  https   github com nikolaydubina go cover treemap web   This is to isolate web  WASM JS HTML  needed dependencies  like  syscall js  from minimal CLI package    It turns out interactive UI is very helpful  Brower can be utilized as effective input source for    changing dimensions of window    changing dimensions of SVG   drag and drop file   slider to increase granularity of treemap   Appendix D  Only Folders   Projects that have lots of files may benefit from not displaying files  but only folders  This is also useful when you want to see impact of immediate files in folder to overall folder heat  Hierarchical properties of size  sum of sizes of children  and heat  weighted heat of children  are preserved  Immediate children   go  files in folder are aggregated into single child      If there is only one     child  then it is skipped   suggested by  herlon214   files    only folders    only folders without aggregation  heat and size property not preserved     Go binary size SVG treemap     Make treemap breakdown of Go executable binary               go install github com nikolaydubina go binsize treemap latest   go tool nm  size  binary finename    go binsize treemap   binsize svg   Disclaimer   Should you be worried about executable binary size  In 2022  few seconds of cat videos or even a single image is tens of MBs  Transferring them over network is not a big deal either  So  probably  you should not worry too much about it  However  this tool can still be useful in couple of cases    You are studying compiler    You are investigating what 3rd party dependencies are getting included in binary    You are checking much data is getting embedded    You are estimating how much code is getting included by packages    You are researching which symbols included    You are doing  cgo     You are doing treemap visualizations     I build this in my spare time as another usecase for Go treemap tooling that I built before  Enjoy  Submit issues or PRs    Examples   github com gohugoio hugo   62MB  this famous example of large Go project     github com cockroachdb cockroach   71MB  this famous db is building with C       github com goccy go graphviz   6 5MB  this project has CGO and builds with lots of graphviz code in C     github com zalando skipper   36MB  is a large Go project  some builds can include C     Knowledge Base     What is  go itab       This is interface related code  Refer to this  article  by Russ Cox      What is  runtime pclntab   And why it is so big      As investigated Cockroach team  it is Go runtime structure for traces   reference    Past discussions in GitHub  thread  on why it is big and what to do about it  well  nothing     Known Issues and TODOs         Size slightly mismatches actual binary size  Including unknown does not help        Better symbol names parsing for C         identify go embed       color by type   increasing luminance  sys  user  c    go embed  etc         color by symbol type       heat by          Related Work     https   github com knz go binsize viz   this was an inspiration for current tool  However  instead of Python and D3 and Javascript  this tool is using single stack purely in Go and has test coverage  Arguably  the downside it is not interactive    https   github com jondot goweight   looks like it was working in the beginning  but as of 2022 01 22 it does not work anymore for me and there were reports dating back to 2020 01 23 for it to be not accurate      Reference     https   github com knz go binsize viz   https   github com jondot goweight   https   github com nikolaydubina treemap   https   github com nikolaydubina go cover treemap   https   github com golang go blob master src cmd nm doc go   https   linux die net man 1 c  filt   https   github com goccy go graphviz   https   research swtch com interfaces     Appendix A  Strange Output   C     CGO   You many need to demungle symtab file first  Install  c  flit   Then process symtab first  Note  c   support is work in progress      go tool nm  size  binary finename    c  filt   go binsize treemap   binsize svg   Appendix B  Large dimensions and lots of details   If you set dimensions very large you can see lots of details and navigate map    4096x4096 is recommended         but you can go much higher    Appendix C  Small dimensions and informative preview   You can generate small preview of project that fits for embedding in README for example    1024x256 is recommended   What is more efficient value or pointer method receivers    A  Why would you think struct is more efficient    You are making single call to fetch struct and its fields  Especially given that interce is a struct wrapper around pointer with type    B  Why would you think pointer is more efficient    You are not copying unnecessary data  only pointer is passed to function  Especially given that pointer is always same size    Result   Overall delta is negligible      small structs     structs few ns faster   Subject to codebase  but this is one of common situations  The effect is unnoticeable and dominated by business logic        deeply nested large structs     pointers is few hundred ns faster   You have to have very large and nested structs to observe this effect  This is only case when pointer is noticeably faster  Difference can be big  This is effect B        deeply nested small structs     structs is few ns faster   You have to have very deep nesting to observe this effect  This is effect A            GOMAXPROCS 1 go test  timeout 1h  bench    benchtime 10s  benchmem       goos  darwin goarch  arm64 pkg  github com nikolaydubina go bench receiver Benchmark BasicServiceStruct                        30910656           373 1 ns op         0 B op          0 allocs op Benchmark BasicServicePointer                       32115787           374 8 ns op         0 B op          0 allocs op Benchmark DepServiceLightStruct UpStruct            31909561           374 5 ns op         0 B op          0 allocs op Benchmark DepServiceLightStruct UpPointer           32234484           374 6 ns op         0 B op          0 allocs op Benchmark DepServiceLightPointer UpStruct           32067454           374 7 ns op         0 B op          0 allocs op Benchmark DepServiceLightPointer UpPointer          31839183           375 4 ns op         0 B op          0 allocs op Benchmark DepServiceLightStruct UpStructMany 10     30359691           394 3 ns op         0 B op          0 allocs op Benchmark DepServiceLightPointer UpPointerMany 10   30688312           393 1 ns op         0 B op          0 allocs op Benchmark DepServiceLightStruct UpStructMany 50     16043655           748 2 ns op         0 B op          0 allocs op Benchmark DepServiceLightPointer UpPointerMany 50   15883161           750 0 ns op         0 B op          0 allocs op Benchmark DepServiceStruct UpStruct                 31512231           377 9 ns op         0 B op          0 allocs op Benchmark DepServiceStruct UpPointer                31865308           377 4 ns op         0 B op          0 allocs op Benchmark DepServicePointer UpStruct                31695372           375 3 ns op         0 B op          0 allocs op Benchmark DepServicePointer UpPointer               32216876           374 4 ns op         0 B op          0 allocs op Benchmark DepServiceStruct UpStructMany 10          23444461           513 5 ns op         0 B op          0 allocs op Benchmark DepServicePointer UpPointerMany 10        30453177           392 7 ns op         0 B op          0 allocs op Benchmark EmptyServiceStruct                         2979396          4039 0 ns op         0 B op          0 allocs op Benchmark EmptyServicePointer                        2984328          4034 0 ns op         0 B op          0 allocs op   Why is the result this way    Looks like sturct poitner does not matter much to Go runtime  It must be using some lookup table based on type  regardless if it pointer or struct type  for methods of type  Similarly  interface must be resolved to concrete method too  Confirming this with Go source code is area of further research  happy little queue   97  covered  90LOC  80 000RPS  integration test  auto cleaning  lightweight         When your Go code does not panic  When your infra does not fail  When your data is small  When your data is temporary  When all you need is a happy little queue        go    once you have a redis connection rdb    redis NewClient                      github com go redis redis v8                  you can boot a lightweight worker worker    hq ReceiveWorker      Redis       rdb      Queue        my queue       PoolIdle    time Minute                recommended      PoolActive  time Millisecond   50      recommended       NumWorkers  10                         recommended      Batch       100                        recommended      Handler      handler                   interface   Handle message   byte  error     go worker Work              and send something sender    hq Sender      Redis   rdb      Queue    my queue       MaxLen  10      TTL     time Hour   4    sender Send   byte  my bytes         in redis it is single list    LLEN my queue       It is as fast as Redis  so  should  be around 80 000RPS    P S   happy  because optimistic   validate  simply      no reflection  no gencode  hierarchical and extendable  fast   100LOC  generics            This is convenient when you have custom validation and nested structures         go    Employee is example of struct with validatable fields and nested structure type Employee struct       Name          string     Age           int     Color         Color        custom func Validate       Education     Education    nested with Validate       Salary        float64     Experience    time Duration     Birthday      time Time     VacationStart time Time     func  s Employee  Validate   error       return validate All          validate OneOf  name   s Name   Zeus    Hera            validate OneOf  age   s Age  35  55           validate Min  age   s Age  10      same field validated again         s Color Validate            s Education Validate            validate Max  salary   s Salary  123 456           validate Max  duration   s Experience  time Duration 1  time Hour           validate After  birthday   s Birthday  time Date 1984  1  1  0  0  0  0  time UTC            validate Before  vacation start   s VacationStart  time Date 2024  1  1  0  0  0  0  time UTC                 Education is another custom struct type Education struct       Duration   int     SchoolName string     func  e Education  Validate   error       if  e Duration   17     5           return errors New  my special error             return validate All          validate Min     e Duration  10           validate OneOf     e SchoolName   KAIST    Stanford                 Color is custom enum type Color string   const       Red   Color    red      Green Color    green      Blue  Color    blue      func  s Color  Validate   error       switch s       case Red  Green  Blue          return nil     default          return fmt Errorf  wrong value  s   expected  v    s    Color               red                green                blue                            Example error message   validate  8 errors   name Bob  not in  Zeus Hera   age 101  not in  35 55   color wrong value orange   expected  red green blue    validate  1 errors    Berkeley  not in  KAIST Stanford    salary 256 99  higher than max  123 456   duration 10h0m0s  higher than max  1h0m0s   birthday 1984 01 01 00 00 00  0000 UTC  is not after  1984 01 01 00 00 00  0000 UTC   vacation start 2025 01 01 00 00 00  0000 UTC  is not before  2024 01 01 00 00 00  0000 UTC     Implementation Details   Printing error takes a lot of time   Thus  it is good to delay constructor of error message as much as possible  And sometimes user code does not need to print error at all and only  nil  check is performed  This is done by moving construction of error message in  Error  methods    It is advisable to avoid memory allocations and creation of structures  Such in case of success flow  we ideally will not have any memory allocations at all  This is why we make validators as functions and call them in chain  We do not delay nor wrap validation function calls  We use function arguments as storage for validation parameters  they are simple params and likely to be on stack which is fast  For example  for  OneOf  we are using variadic arguments  Other alternative is to use arrays since in Go they are on stack as well    We also hope Go compiler   can detect that argument to function is constant and inline it in assembly or stack   does not use expensive memory for variadic parameters   can inline functions   Defining custom validators with  switch  is expected to be even faster    Benchmarks     go test  bench    benchtime 10s  benchmem       goos  darwin goarch  amd64 pkg  github com nikolaydubina validate cpu  VirtualApple   2 50GHz BenchmarkEmployee Error Message 10                     3744121        3229 ns op        2376 B op         56 allocs op BenchmarkEmployee Error 10                            12533948         958 ns op         904 B op         23 allocs op BenchmarkEmployee Success 10                         100000000         115 ns op          80 B op          3 allocs op BenchmarkEmployeeSimple Error Message 10               9488436        1263 ns op         840 B op         25 allocs op BenchmarkEmployeeSimple Error 10                      44261380         270 ns op         344 B op          9 allocs op BenchmarkEmployeeSimple Success 10                   243491635          49 ns op          48 B op          2 allocs op BenchmarkEmployeeNoContainers Error Message 10        28089966         427 ns op         248 B op          9 allocs op BenchmarkEmployeeNoContainers Error 10               142881793          85 ns op          88 B op          3 allocs op BenchmarkEmployeeNoContainers Success 10            1000000000           4 ns op           0 B op          0 allocs op PASS ok      github com nikolaydubina validate   120 565s   Appendix A  Comparison to other validators   github com go playground validator   It uses struct tags and reflection  Binding custom validations require defining validation function with special name and using interface typecast then registering this to validator instance    It has instance of validator that is reused    Its speed is mostly few hundred ns and up to 1 s  Its memory allocation can be 0 and reaches up to few dozen    Appendix B  Wrapping validators into interface   Early version of this library was wrapping each validation operation into a  interface   Validate   error     In this approach  we already had in validators everything needed to format error message  which is why we were reusing them as error containers  However  there were few drawbacks    Code looked more verbose   go func  s Employee  Validate   error       return validate All          validate OneOf string  Name   name   Value  s Name  Values    string  Zeus    Hera             validate OneOf int  Name   age   Value  s Age  Values    int 35  55            validate Min int  Name   age   Value  s Age  Min  10      same field validated again         s Color          s Education          validate Max float64  Name   salary   Value  s Salary  Max  123 456           validate Max time Duration  Name   duration   Value  s Experience  Max  time Duration 1    time Hour           validate After Name   birthday   Value  s Birthday  Time  time Date 1984  1  1  0  0  0  0  time UTC            validate Before Name   vacation start   Value  s VacationStart  Time  time Date 2024  1  1  0  0  0  0  time UTC              Performance was slightly worse for error case  and much worse for success case     go test  bench    benchtime 10s  benchmem       goos  darwin goarch  amd64 pkg  github com nikolaydubina validate cpu  VirtualApple   2 50GHz BenchmarkEmployee Error Message 10                   3579223          3379 ns op        2761 B op         62 allocs op BenchmarkEmployee Error 10                           9361948          1277 ns op        1344 B op         34 allocs op BenchmarkEmployee Success 10                        25418672           474 ns op         552 B op         14 allocs op BenchmarkEmployeeSimple Error Message 10             8757170          1364 ns op         992 B op         28 allocs op BenchmarkEmployeeSimple Error 10                    30418941           394 ns op         504 B op         13 allocs op BenchmarkEmployeeSimple Success 10                  65194581           184 ns op         224 B op          6 allocs op BenchmarkEmployeeNoContainers Error Message 10      24971338           483 ns op         280 B op         10 allocs op BenchmarkEmployeeNoContainers Error 10              72736639           165 ns op         136 B op          5 allocs op BenchmarkEmployeeNoContainers Success 10            143333276           83 ns op          64 B op          2 allocs op PASS ok      github com nikolaydubina validate   124 950s   Appendix C  Binding validator functions in map to field names   It might appear that it is more efficient not to pass  name  of field in validator  Such it is tempting to run slice or map of functions  However  performance deteriorates with this approach  Likely this is due to compiler using stack or not efficiently inlining    Code sample     go func All vs map string error  error       errs    make map string error  len vs       for k  err    range vs           if err    nil               errs k    err                     if len errs    0           return errMultiple errs            return nil           func  s Employee  Validate   error       return validate All map string error           name             validate OneOf s Name   Zeus    Hera             age              validate OneOf s Age  35  55            age 2            validate Min s Age  10      same field validated again          color            s Color Validate             education        s Education Validate             salary           validate Max s Salary  123 456            duration         validate Max s Experience  time Duration 1  time Hour            birthday         validate After s Birthday  time Date 1984  1  1  0  0  0  0  time UTC             vacation start   validate Before s VacationStart  time Date 2024  1  1  0  0  0  0  time UTC                   Performance is worse across the board   goos  darwin goarch  amd64 pkg  github com nikolaydubina validate cpu  VirtualApple   2 50GHz BenchmarkEmployee Error Message 10                   2698065          4359 ns op        3866 B op         57 allocs op BenchmarkEmployee Error 10                           6993564          1714 ns op        2058 B op         21 allocs op BenchmarkEmployee Success 10                        14948445           810 ns op        1329 B op          7 allocs op BenchmarkEmployeeSimple Error Message 10             8243392          1460 ns op        1112 B op         26 allocs op BenchmarkEmployeeSimple Error 10                    33798837           356 ns op         496 B op          7 allocs op BenchmarkEmployeeSimple Success 10                  66953932           182 ns op          96 B op          3 allocs op BenchmarkEmployeeNoContainers Error Message 10      19754359           600 ns op         576 B op         10 allocs op BenchmarkEmployeeNoContainers Error 10              61285670           194 ns op         368 B op          3 allocs op BenchmarkEmployeeNoContainers Success 10            123293473           98 ns op          48 B op          1 allocs op PASS ok      github com nikolaydubina validate   128 263s   Reference     As of  2022 04 01   Go does not support generic arrays    Benchmarking Go errors   How does  creation    serialization   and  wrapping  changes with different errors    Notation    fmt Errorf  you wrap like this  fmt Errorf  my message  s with data  f   w   err      DelayedError  you wrap underlying error into field but do not call  e Err Error    immediately    errors New  you create error with string passed directly into struct  no fmt calls made    Results 1   fmt Errorf  wrapping calls  Error  immediately  Delaying  Error  call is not utilized 2   errors New  is very fast when no complex formatting  floats reflection iterables  is necessary 3   DelayedError  is 10x faster when  Error  calls are not done  since  Error  calls are lazily delayed 4   Error  call for  fmt Errorf  and for  DelayedError  takes same time   Benchmarks    go test  bench    benchtime 5s  benchmem       goos  darwin goarch  amd64 pkg  github com nikolaydubina go bench errors cpu  VirtualApple   2 50GHz BenchmarkWrap DelayedError float64 Message 2wrap 10         14211424           419 ns op         216 B op          8 allocs op BenchmarkWrap DelayedError float64 2wrap 10                 99927066            58 ns op          96 B op          2 allocs op BenchmarkWrap DelayedError float64 Message 5wrap 10          5403166          1097 ns op         736 B op         20 allocs op BenchmarkWrap DelayedError float64 5wrap 10                 38846422           153 ns op         240 B op          5 allocs op BenchmarkWrap DelayedError float64 Message 10wrap 10         2672426          2246 ns op        2072 B op         40 allocs op BenchmarkWrap DelayedError float64 10wrap 10                19889925           299 ns op         480 B op         10 allocs op BenchmarkWrap DelayedError float64 Message 50wrap 10          394473         14857 ns op       35814 B op        200 allocs op BenchmarkWrap DelayedError float64 50wrap 10                 4079131          1467 ns op        2400 B op         50 allocs op BenchmarkWrap fmt Errorf float64 Message 2wrap 10           13853347           430 ns op         176 B op          6 allocs op BenchmarkWrap fmt Errorf float64 2wrap 10                   13966632           429 ns op         176 B op          6 allocs op BenchmarkWrap fmt Errorf float64 Message 5wrap 10            5036449          1191 ns op         664 B op         15 allocs op BenchmarkWrap fmt Errorf float64 5wrap 10                    5040122          1189 ns op         664 B op         15 allocs op BenchmarkWrap fmt Errorf float64 Message 10wrap 10           2418132          2485 ns op        2048 B op         30 allocs op BenchmarkWrap fmt Errorf float64 10wrap 10                   2414413          2484 ns op        2048 B op         30 allocs op BenchmarkWrap fmt Errorf float64 Message 50wrap 10            353810         16856 ns op       38867 B op        150 allocs op BenchmarkWrap fmt Errorf float64 50wrap 10                    353611         16808 ns op       38867 B op        150 allocs op BenchmarkSingle fmt Errorf float64 Message 10               27763376           215 ns op          72 B op          3 allocs op BenchmarkSingle fmt Errorf float64 10                       30912550           192 ns op          72 B op          3 allocs op BenchmarkSingle fmt Errorf int Message 10                   46369344           129 ns op          51 B op          3 allocs op BenchmarkSingle fmt Errorf int 10                           56102511           106 ns op          51 B op          3 allocs op BenchmarkSingle errors New int Message 10                   100000000           53 ns op           4 B op          1 allocs op BenchmarkSingle errors New int 10                           163252323           36 ns op           4 B op          1 allocs op PASS ok      github com nikolaydubina go bench errors    149 863s Web UI using Go WASM for  go cover treemap   Fixed Point Decimals     To use in money  look at  github com nikolaydubina fpmoney               int64  inside   does not use  float  neither in parsing nor printing   as fast as  int64  in parsing  printing  arithmetics   3x faser  float   20x faster  shopspring decimal   30x faster  fmt   zero overhead   preventing error prone fixed point arithmetics   Fuzz tests  Benchmarks   JSON   200LOC        go var BuySP500Price   fp3 FromInt 9000    input      byte    sp500   9000 023      type Stocks struct       SP500 fp3 Decimal  json  sp500     var v Stocks if err    json Unmarshal input   v   err    nil       log Fatal err      var amountToBuy fp3 Decimal if v SP500 GreaterThan BuySP500Price        amountToBuy   amountToBuy Add v SP500 Mul 2       fmt Println amountToBuy     Output  18000 046       Implementation   Parsing and Printing is expensive operation and requires a lot of code  However  if you know that your numbers are always small and simple and you do not care or do not permit lots of fractions like   1234 567   then parsing and printing can be greatly simplified  Code is heavily influenced by hot path from Go core  strconv  package    It is wrapped into struct to prevent bugs    block multiplication by  fpdecimal  type  which leads to increase in decimal fractions and loose of precision   block additions of untyped constants  which leads to errors if you forget to scale by factor   Benchmarks   Parse    go test  bench    benchtime 5s  benchmem       goos  darwin goarch  arm64 pkg  github com nikolaydubina fpdecimal BenchmarkParse FP3Decimal small 10                             845515756             7 04 ns op           0 B op           0 allocs op BenchmarkParse FP3Decimal large 10                             278560885            21 43 ns op           0 B op           0 allocs op BenchmarkParse int strconv Atoi small 10                      1000000000             4 74 ns op           0 B op           0 allocs op BenchmarkParse int strconv Atoi large 10                       424242687            14 17 ns op           0 B op           0 allocs op BenchmarkParse int strconv ParseInt small int32 10             566976321            10 65 ns op           0 B op           0 allocs op BenchmarkParse int strconv ParseInt small int64 10             552894133            10 85 ns op           0 B op           0 allocs op BenchmarkParse int strconv ParseInt large int64 10             219031276            27 56 ns op           0 B op           0 allocs op BenchmarkParse float strconv ParseFloat small float32 10       344793511            17 43 ns op           0 B op           0 allocs op BenchmarkParse float strconv ParseFloat small float64 10       335880535            17 82 ns op           0 B op           0 allocs op BenchmarkParse float strconv ParseFloat large float32 10       129427171            46 40 ns op           0 B op           0 allocs op BenchmarkParse float strconv ParseFloat large float64 10       128508513            46 75 ns op           0 B op           0 allocs op BenchmarkParse float fmt Sscanf small 10                        20424795           295 6  ns op          69 B op           2 allocs op BenchmarkParse float fmt Sscanf large 10                         9479828           633 9  ns op          88 B op           3 allocs op PASS ok      github com nikolaydubina fpdecimal    194 558s   Print    go test  bench    benchtime 5s  benchmem       goos  darwin goarch  arm64 pkg  github com nikolaydubina fpdecimal BenchmarkPrint FP3Decimal small 10                            235701032            25 4 ns op           7 B op           1 allocs op BenchmarkPrint FP3Decimal large 10                            185768853            32 1 ns op          24 B op           1 allocs op BenchmarkPrint int strconv Itoa small 10                      457453576            13 1 ns op           3 B op           0 allocs op BenchmarkPrint int strconv Itoa large 10                      229820906            26 1 ns op          18 B op           1 allocs op BenchmarkPrint int strconv FormatInt small 10                 728307549            13 1 ns op           3 B op           0 allocs op BenchmarkPrint float strconv FormatFloat small float32 10      49801364           117 8 ns op          31 B op           2 allocs op BenchmarkPrint float strconv FormatFloat small float64 10      40938864           148 3 ns op          31 B op           2 allocs op BenchmarkPrint float strconv FormatFloat large float32 10      58160480            99 1 ns op          48 B op           2 allocs op BenchmarkPrint float strconv FormatFloat large float64 10      61878582            97 2 ns op          48 B op           2 allocs op BenchmarkPrint float fmt Sprintf small 10                      43542469           138 8 ns op          16 B op           2 allocs op BenchmarkPrint float fmt Sprintf large 10                      47824404           125 7 ns op          28 B op           2 allocs op PASS ok      github com nikolaydubina fpdecimal    194 558s   Arithmetics    go test  bench    benchtime 5s  benchmem       goos  darwin goarch  arm64 pkg  github com nikolaydubina fpdecimal BenchmarkArithmetic FP3Decimal add x1 10           1000000000             0 31 ns op           0 B op           0 allocs op BenchmarkArithmetic FP3Decimal add x100 10          181966545            32 75 ns op           0 B op           0 allocs op BenchmarkArithmetic int64 add x1 10                1000000000             0 31 ns op           0 B op           0 allocs op BenchmarkArithmetic int64 add x100 10               182298925            32 99 ns op           0 B op           0 allocs op PASS ok      github com nikolaydubina fpdecimal    194 558s   References     Fixed Point Arithmetic Wiki   shopspring decimal     Appendix A  Comparison to other libraries     https   github com shopspring decimal solves arbitrary precision  fpdecimal solves only simple small decimals   https   github com Rhymond go money solves typed number  currency   decodes through  interface    and float64  no precision in decoding  expects encoding to be in cents     Appendix B  Benchmarking  shopspring decimal   2022 05 28     go test  bench    benchtime 5s  benchmem       goos  darwin goarch  arm64 pkg  github com shopspring decimal BenchmarkNewFromFloatWithExponent 10                        59701516          97 7 ns op         106 B op           4 allocs op BenchmarkNewFromFloat 10                                    14771503         410 3 ns op          67 B op           2 allocs op BenchmarkNewFromStringFloat 10                              16246342         375 2 ns op         175 B op           5 allocs op Benchmark FloorFast 10                                    1000000000           2 1 ns op           0 B op           0 allocs op Benchmark FloorRegular 10                                   53857244         106 3 ns op         112 B op           6 allocs op Benchmark DivideOriginal 10                                        7   715322768   ns op   737406446 B op    30652495 allocs op Benchmark DivideNew 10                                            22   262893689   ns op   308046721 B op    12054905 allocs op BenchmarkDecimal RoundCash Five 10                           9311530         636 5 ns op         616 B op          28 allocs op Benchmark Cmp 10                                                  44   133191579   ns op          24 B op           1 allocs op Benchmark decimal Decimal Add different precision 10        31561636         176 6 ns op         280 B op           9 allocs op Benchmark decimal Decimal Sub different precision 10        36892767         164 4 ns op         240 B op           9 allocs op Benchmark decimal Decimal Add same precision 10            134831919          44 9 ns op          80 B op           2 allocs op Benchmark decimal Decimal Sub same precision 10            134902627          43 1 ns op          80 B op           2 allocs op BenchmarkDecimal IsInteger 10                               92543083          66 1 ns op           8 B op           1 allocs op BenchmarkDecimal NewFromString 10                             827455        7382   ns op        3525 B op         216 allocs op BenchmarkDecimal NewFromString large number 10                212538       28836   ns op       16820 B op         360 allocs op BenchmarkDecimal ExpHullAbraham 10                             10000      572091   ns op      486628 B op         568 allocs op BenchmarkDecimal ExpTaylor 10                                  26343      222915   ns op      431226 B op        3172 allocs op PASS ok      github com shopspring decimal    123 541sa   Appendix C  Why this is good fit for money    There are only  200 currencies in the world  All currencies have at most 3 decimal digits  thus it is sufficient to handle 3 decimal fractions  Next  currencies without decimal digits are typically 1000x larger than dollar  but even then maximum number that fits into  int64   without 3 decimal fractions  is  9 223 372 036 854 775 807  which is  9 quadrillion  This should be enough for most operations with money    Appendix D  Is it safe to use arithmetic operators in Go    Sort of       In one of iterations  I did Type Alias  but it required some effort to use it carefully    Operations with defined types  variables  will fail     go var a int64 var b fpdecimal FP3DecimalFromInt 1000       does not compile a   b       However  untyped constants will be resolved to underlying type  int64  and will be allowed      go const a 10000 var b fpdecimal FP3DecimalFromInt 1000       compiles a   b      also compiles b   42      this one too b    23       Is this a problem     For multiplication and division   yes  it can be  You have to be careful not to multiply two  fpdecimal  numbers  since scaling factor will quadruple  Multiplying by constants is ok tho    For addition substraction   yes  it can be  You have to be careful and remind yourself that constants would be reduced 1000x    Both of this can be addressed at compile time by providing linter  This can be also addressed by wrapping into a struct and defining methods  Formed is hard to achieve in Go  due to lack of operator overload and lots of work required to write AST parser  Later has been implemented in this pacakge  and  as benchmarks show  without any extra memory or calls overhead as compared to  int64     Appendix C  Print into destination   To avoid mallocs  it is advantageous to print formatted value to pre allocated destination  Similarly  to  strconv AppendInt   we provide  AppendFixedPointDecimal   This is utilized in  github com nikolaydubina fpmoney  package    BenchmarkFixedPointDecimalToString small 10     28522474            35 43 ns op       24 B op          1 allocs op BenchmarkFixedPointDecimalToString large 10     36883687            32 32 ns op       24 B op          1 allocs op BenchmarkAppendFixedPointDecimal small 10       38105520            30 51 ns op      117 B op          0 allocs op BenchmarkAppendFixedPointDecimal large 10       55147478            29 52 ns op      119 B op          0 allocs op   Appendix D  Quotes     Be Precise  Using floats to represent currency is almost criminal    Robert C Martin   Clean Code  p 301     Fixed Point Decimal Money             Be Precise   using floats to represent currency is almost criminal    Robert C Martin   Clean Code  p 301       as fast as  int64   no  float  in parsing nor printing   ISO 4217  currency   block mismatched currency arithmetics   does not leak precision   parsing faster than  int    float    string   Fuzz tests  Benchmarks  Generics   200 LOC        go var BuySP500Price   fpmoney FromInt 9000  fpmoney SGD    input      byte    sp500     amount   9000 02   currency    SGD        type Stonks struct       SP500 fpmoney Amount  json  sp500     var v Stonks if err    json Unmarshal input   v   err    nil       log Fatal err      amountToBuy    fpmoney FromInt 0  fpmoney SGD  if v SP500 GreaterThan BuySP500Price        amountToBuy   amountToBuy Add v SP500 Mul 2       fmt Println amountToBuy     Output  18000 04 SGD       Division   Division always returns remainder  Fractional cents can never be reached       go x    fpmoney FromInt 1  fpmoney SGD  a  r    x Div 3  fmt Println a  r     Output  0 33 SGD 0 01 SGD   a  r   x Div 5  fmt Println a  r     Output  0 20 SGD 0 SGD       Equality   Equality operator can be used to compare values    go x    fpmoney FromInt 3  fpmoney SGD  y    fpmoney FromInt 9  fpmoney SGD  fmt Println y    x Mul 3      Output  true   Cross Currency Protection   Akin to integer division by 0  which panics in Go  arithmetic operations on differnet currenices result in panic  Returning error in arithmetic operation would prohibit chaning of method calls  which is not convenient  It is better to stop execution  rather then corrupt value  Mismatched or missing currencies must be caught at testing or QA of your code    Two mechanisms to reduce panics are planned for future versions  1  package level var for enable disable currency check 2  package level var for fallback currency   Arithmetics  go x    fpmoney FromInt 10  fpmoney USD  y    fpmoney FromInt 10  fpmoney SGD  c    x Add y     panics   Equality  go x    fpmoney FromInt 10  fpmoney USD  y    fpmoney FromInt 10  fpmoney SGD  fmt Println y    x     Output  false   Ultra Small Fractions   Some denominatinos have very low fractions  Storing them  int64  you would get      BTC   satoshi  is  1 BTC   100 000 000 satoshi   which is still enough for   92 233 720 368 BTC     ETH   wei  is  1 ETH   1 000 000 000 000 000 000 wei   which is   9 ETH   If you deal with  wei   you may consider  bigint  or multiple  int64   In fact  official Ethereum code is in Go and it is using bigint   code        Given that currency enumn still takes at least 1B in separate storage from  int64  in struct and Go allocates 16B of memory for struct regardless  current implementation reserved padding bytes  It is sensible to use extra space our ot 16B to support long integer arithmetics  Implementing this is area of furthter research    Benchmarks     go test  bench    benchmem       goos  darwin goarch  arm64 pkg  github com nikolaydubina fpmoney BenchmarkArithmetic add x1 10                   1000000000             0 5 ns op           0 B op           0 allocs op BenchmarkArithmetic add x100 10                   12525424            51 9 ns op           0 B op           0 allocs op BenchmarkJSONUnmarshal small 10                    3610992           329 8 ns op         198 B op           3 allocs op BenchmarkJSONUnmarshal large 10                    2901363           412 4 ns op         216 B op           3 allocs op BenchmarkJSONMarshal small 10                      5032456           238 1 ns op         160 B op           3 allocs op BenchmarkJSONMarshal large 10                      4072776           295 5 ns op         176 B op           3 allocs op BenchmarkJSONMarshal Exact small 10               40404832            29 6 ns op         112 B op           1 allocs op BenchmarkJSONMarshal Exact large 10               28532677            41 6 ns op         112 B op           1 allocs op PASS ok      github com nikolaydubina fpmoney    62 744s   float32   old  and  fpmoney   new        benchstat  split  XYZ  float32 bench fpmoney bench name                    old time op    new time op    delta JSONUnmarshal small 10     502ns   0      338ns   1     32 63    p 0 008 n 5 5  JSONUnmarshal large 10     572ns   0      419ns   1     26 79    p 0 008 n 5 5  JSONMarshal small 10       189ns   0      245ns   1     29 12    p 0 008 n 5 5  JSONMarshal large 10       176ns   0      305ns   1     73 07    p 0 008 n 5 5    name                    old alloc op   new alloc op   delta JSONUnmarshal small 10      271B   0       198B   0     26 94    p 0 008 n 5 5  JSONUnmarshal large 10      312B   0       216B   0     30 77    p 0 008 n 5 5  JSONMarshal small 10       66 0B   0     160 0B   0    142 42    p 0 008 n 5 5  JSONMarshal large 10       72 0B   0     176 0B   0    144 44    p 0 008 n 5 5    name                    old allocs op  new allocs op  delta JSONUnmarshal small 10      6 00   0       3 00   0     50 00    p 0 008 n 5 5  JSONUnmarshal large 10      7 00   0       3 00   0     57 14    p 0 008 n 5 5  JSONMarshal small 10        2 00   0       3 00   0     50 00    p 0 008 n 5 5  JSONMarshal large 10        2 00   0       3 00   0     50 00    p 0 008 n 5 5        int    float32    fpmoney        benchstat  split  XYZ  int bench float32 bench fpmoney bench name   time op              int bench   float32 bench  fpmoney bench JSONUnmarshal small 10      481ns   2      502ns   0      338ns   1  JSONUnmarshal large 10      530ns   1      572ns   0      419ns   1  JSONMarshal small 10        140ns   1      189ns   0      245ns   1  JSONMarshal large 10        145ns   0      176ns   0      305ns   1    name   alloc op             int bench   float32 bench  fpmoney bench JSONUnmarshal small 10       269B   0       271B   0       198B   0  JSONUnmarshal large 10       288B   0       312B   0       216B   0  JSONMarshal small 10        57 0B   0      66 0B   0     160 0B   0  JSONMarshal large 10        72 0B   0      72 0B   0     176 0B   0    name   allocs op            int bench   float32 bench  fpmoney bench JSONUnmarshal small 10       6 00   0       6 00   0       3 00   0  JSONUnmarshal large 10       7 00   0       7 00   0       3 00   0  JSONMarshal small 10         2 00   0       2 00   0       3 00   0  JSONMarshal large 10         2 00   0       2 00   0       3 00   0        Appendix A   json Unmarshal  optimizations   Parsing is surprisingly slow  It is  6x of  float32     string     Use  json NewDecoder  and parse directly      BenchmarkJSONUnmarshal small 10           2030568          2977 ns op        1599 B op          38 allocs op BenchmarkJSONUnmarshal large 10           1956444          3106 ns op        1640 B op          39 allocs op         Make container struct and wrap int and ISO 4217 currency and copy values   BenchmarkJSONUnmarshal small 10           2776969          2160 ns op         430 B op           8 allocs op BenchmarkJSONUnmarshal large 10           2649692          2263 ns op         448 B op           8 allocs op   Two passes over string  find  amount  and find  currency    BenchmarkJSONUnmarshal small 10            686832          1732 ns op         198 B op           3 allocs op BenchmarkJSONUnmarshal large 10            657272          1820 ns op         216 B op           3 allocs op   Parsing just amount takes 400ns   BenchmarkJSONUnmarshal small 10              3339529           344 5 ns op         198 B op           3 allocs op BenchmarkJSONUnmarshal large 10              2686135           443 2 ns op         216 B op           3 allocs op   Package  github com ferdypruis iso4217 v1 2 0  does cast of string to currency through loop  But we have predefined currencies  we can rely on compiler for that  Optimizing this cast by avoiding mallocs and loops    As of  2022 06 17   package  github com ferdypruis iso4217 v1 2 1  uses map to cast currency  It is as efficient as switch case  Thanks  ferdypruis for the update    Appendix B  Other Libraries   github com shopspring decimal    fixed precision   faster printing parsing arithmetics   currency handling    github com Rhymond go money    does not use  float  or  interface    in parsing   currency is enum   github com ferdypruis iso4217    skipped deprecated currencies to fit into  uint8  and smaller struct size   Appendix C  Extra malloc in Printing   Even though  MarshalJSON  does exactly one malloc  using it with  json Marshall  package adds two more mallocs  This looks like penalty of reflect nature of  json  package and is unavoidable    BenchmarkJSONMarshal Exact small 10     40404832    29 6 ns op      112 B op        1 allocs op BenchmarkJSONMarshal Exact large 10     28532677    41 6 ns op      112 B op        1 allocs op   Appendix D  Strict Currency Enum   It is possible to rely on Go compiler to strictiy currency enum by wrapping into a struct  There is no performance penalty  Implementation is almost same  API is the same  but much safer    Go Benchmarks for Stream Processing   Is it efficient to chain  io Readers  in Go    How efficient it is compared to UNIX pipes    How efficient would be iterator      Tested Scenarios   ReaderSelector   This stream processor wraps  bufio Scanner  and writes    byte  to destination  It can be convenient to wrap for  io Reader  interface  It is efficient since    byte  is casted to  string   However  since it copies memory to output  chaining many of similar readers can cause overhead due to each of processors does copying      io Reader  interface   constant memory  memory does not increase with scaling input   600MB takes 40s   user cpu time  7s  is faster than  grep   26s    system cpu time  36s  is mush slower than  grep   0 25s    chaining readers is constant memory   chaining readers has linear processing time   chaining readers in Go has faster user cpu time  7s  then via UNIX pipe  11 1   4 7   15 8s        IteratorSelector   This steam processor wraps iterate calls  It performs reading and writing of bytes only once  Data is passed through function calls  Parsing and Printing is also minimized  since already parsed data structures are passed in chain of iterators  Data passed in iterator chain is either small enough to be on stack or pointers to same bytes buffer  This make it more efficient to chain many processors      uses wrappers for  io Reader  interface   constant memory   4x less mallocs than  io Reader  version   time is 10  faster tha  io Reader  version   time per processor composition is slightly cheaper   for this problem of filtering strings  it is generally same as  io Reader   For more complex usecases it may show greated benefits        Reader    functions    Reader   If each iterator just calls  Next  and there is no batching or utilizing multiple elements at once  then can just wrap as functions calls  No need for Iterator interface  Data can be passed through functions  Benchmarks and Examples for this is area of further research    Benchmarks UNIX Pipe   grep   time cat testdata colors basic 100000000 csv   grep  E  red green    grep green   testdata colors basic 100000000 csv out grep   cat testdata colors basic 100000000 csv  0 01s user 0 14s system 0  cpu 26 462 total grep   color auto  E  red green   26 10s user 0 25s system 99  cpu 26 463 total grep   color auto green   testdata colors basic 100000000 csv out grep  7 28s user 0 23s system 28  cpu 26 462 total   ReaderSelector  through UNIX pipe   time cat testdata colors basic 100000000 csv     reader selector  dict red green     reader selector  dict green   testdata colors basic 100000000 csv out reader selector   cat testdata colors basic 100000000 csv  0 02s user 0 46s system 1  cpu 40 874 total   reader selector  dict red green  11 15s user 7 63s system 45  cpu 40 876 total   reader selector  dict green     4 57s user 35 89s system 98  cpu 40 886 total   ReaderSelector  through Go wrapping   time cat testdata colors basic 100000000 csv     reader selector inline  dict1 red green  dict2 green   testdata colors basic 100000000 csv out reader selector inline   cat testdata colors basic 100000000 csv  0 02s user 0 43s system 1  cpu 42 902 total   reader selector inline  dict1 red green  dict2 green     7 67s user 34 88s system 99  cpu 42 906 total   Benchmarks Go   goos  darwin goarch  arm64 pkg  github com nikolaydubina go stream benchmarks BenchmarkReaderSelector n 100 10          261753          5158 ns op        9520 B op         14 allocs op BenchmarkReaderSelector n 1000 10          28534         41873 ns op       13872 B op         17 allocs op BenchmarkReaderSelector n 10000 10          2881        413503 ns op       93240 B op         26 allocs op BenchmarkReaderSelector n 100000 10          288       4194997 ns op      916545 B op         35 allocs op BenchmarkReaderSelector Chain l 2 n 100 10            233853          4809 ns op        9104 B op         10 allocs op BenchmarkReaderSelector Chain l 2 n 1000 10            27447         42102 ns op       20624 B op         15 allocs op BenchmarkReaderSelector Chain l 2 n 10000 10            2782        423778 ns op      162456 B op         24 allocs op BenchmarkReaderSelector Chain l 2 n 100000 10            282       4259467 ns op     1546913 B op         33 allocs op BenchmarkReaderSelector Chain l 4 n 100 10            138314          8013 ns op       17664 B op         18 allocs op BenchmarkReaderSelector Chain l 4 n 1000 10            17065         70732 ns op       29184 B op         23 allocs op BenchmarkReaderSelector Chain l 4 n 10000 10            1744        687230 ns op      171016 B op         32 allocs op BenchmarkReaderSelector Chain l 4 n 100000 10            174       6858673 ns op     1555472 B op         41 allocs op BenchmarkReaderSelector Chain l 8 n 100 10             71600         14792 ns op       34784 B op         34 allocs op BenchmarkReaderSelector Chain l 8 n 1000 10             9890        124288 ns op       46304 B op         39 allocs op BenchmarkReaderSelector Chain l 8 n 10000 10             998       1199137 ns op      188136 B op         48 allocs op BenchmarkReaderSelector Chain l 8 n 100000 10             99      11885847 ns op     1572594 B op         57 allocs op BenchmarkReaderSelector Chain l 16 n 100 10            34550         28969 ns op       69024 B op         66 allocs op BenchmarkReaderSelector Chain l 16 n 1000 10            5290        224309 ns op       80544 B op         71 allocs op BenchmarkReaderSelector Chain l 16 n 10000 10            535       2238667 ns op      222377 B op         80 allocs op BenchmarkReaderSelector Chain l 16 n 100000 10            52      22158313 ns op     1606832 B op         89 allocs op BenchmarkReaderSelector Chain l 32 n 100 10            18247         73484 ns op      137504 B op        130 allocs op BenchmarkReaderSelector Chain l 32 n 1000 10            2126        570957 ns op      149024 B op        135 allocs op BenchmarkReaderSelector Chain l 32 n 10000 10            217       5582361 ns op      290857 B op        144 allocs op BenchmarkReaderSelector Chain l 32 n 100000 10            20      55388562 ns op     1675312 B op        153 allocs op BenchmarkReaderSelector Chain l 64 n 100 10             8691        156855 ns op      274464 B op        258 allocs op BenchmarkReaderSelector Chain l 64 n 1000 10            1052       1130642 ns op      285984 B op        263 allocs op BenchmarkReaderSelector Chain l 64 n 10000 10            100      11122256 ns op      427818 B op        272 allocs op BenchmarkReaderSelector Chain l 64 n 100000 10            10     109649171 ns op     1812272 B op        281 allocs op BenchmarkReaderSelector Chain l 128 n 100 10            3831        285850 ns op      548387 B op        514 allocs op BenchmarkReaderSelector Chain l 128 n 1000 10            502       2287637 ns op      559908 B op        519 allocs op BenchmarkReaderSelector Chain l 128 n 10000 10            51      23123621 ns op      701739 B op        528 allocs op BenchmarkReaderSelector Chain l 128 n 100000 10            5     230775733 ns op     2086192 B op        537 allocs op BenchmarkReaderSelector Chain l 256 n 100 10            2095        577719 ns op     1096227 B op       1026 allocs op BenchmarkReaderSelector Chain l 256 n 1000 10            252       4983756 ns op     1107749 B op       1031 allocs op BenchmarkReaderSelector Chain l 256 n 10000 10            25      47707918 ns op     1249580 B op       1040 allocs op BenchmarkReaderSelector Chain l 256 n 100000 10            3     472256153 ns op     2634032 B op       1049 allocs op BenchmarkIteratorSelector n 100 10        297792          4265 ns op        5400 B op         13 allocs op BenchmarkIteratorSelector n 1000 10        31400         38182 ns op        9752 B op         16 allocs op BenchmarkIteratorSelector n 10000 10        3134        380777 ns op       89112 B op         24 allocs op BenchmarkIteratorSelector n 100000 10        310       3827292 ns op      912410 B op         32 allocs op BenchmarkIteratorSelector Chain l 2 n 100 10              286718          4097 ns op        4984 B op          9 allocs op BenchmarkIteratorSelector Chain l 2 n 1000 10              29946         39103 ns op       16504 B op         14 allocs op BenchmarkIteratorSelector Chain l 2 n 10000 10              3056        393257 ns op      158328 B op         22 allocs op BenchmarkIteratorSelector Chain l 2 n 100000 10              304       3928017 ns op     1542780 B op         30 allocs op BenchmarkIteratorSelector Chain l 4 n 100 10              206868          5320 ns op        5032 B op         11 allocs op BenchmarkIteratorSelector Chain l 4 n 1000 10              22987         52178 ns op       16552 B op         16 allocs op BenchmarkIteratorSelector Chain l 4 n 10000 10              2305        519023 ns op      158376 B op         24 allocs op BenchmarkIteratorSelector Chain l 4 n 100000 10              230       5187744 ns op     1542824 B op         32 allocs op BenchmarkIteratorSelector Chain l 8 n 100 10              143556          7879 ns op        5128 B op         15 allocs op BenchmarkIteratorSelector Chain l 8 n 1000 10              15661         77363 ns op       16648 B op         20 allocs op BenchmarkIteratorSelector Chain l 8 n 10000 10              1584        754097 ns op      158472 B op         28 allocs op BenchmarkIteratorSelector Chain l 8 n 100000 10              158       7657606 ns op     1542922 B op         36 allocs op BenchmarkIteratorSelector Chain l 16 n 100 10              94784         13286 ns op        5320 B op         23 allocs op BenchmarkIteratorSelector Chain l 16 n 1000 10              9658        125734 ns op       16840 B op         28 allocs op BenchmarkIteratorSelector Chain l 16 n 10000 10              943       1231283 ns op      158664 B op         36 allocs op BenchmarkIteratorSelector Chain l 16 n 100000 10              94      12263412 ns op     1543118 B op         44 allocs op BenchmarkIteratorSelector Chain l 32 n 100 10              25926         48515 ns op        5704 B op         39 allocs op BenchmarkIteratorSelector Chain l 32 n 1000 10              2548        471494 ns op       17224 B op         44 allocs op BenchmarkIteratorSelector Chain l 32 n 10000 10              254       4700952 ns op      159049 B op         52 allocs op BenchmarkIteratorSelector Chain l 32 n 100000 10              25      47805630 ns op     1543507 B op         60 allocs op BenchmarkIteratorSelector Chain l 64 n 100 10              12015         95875 ns op        6472 B op         71 allocs op BenchmarkIteratorSelector Chain l 64 n 1000 10              1266        947828 ns op       17992 B op         76 allocs op BenchmarkIteratorSelector Chain l 64 n 10000 10              128       9388938 ns op      159816 B op         84 allocs op BenchmarkIteratorSelector Chain l 64 n 100000 10              12      94434326 ns op     1544272 B op         92 allocs op BenchmarkIteratorSelector Chain l 128 n 100 10              5496        212939 ns op        8008 B op        135 allocs op BenchmarkIteratorSelector Chain l 128 n 1000 10              566       2091296 ns op       15432 B op        139 allocs op BenchmarkIteratorSelector Chain l 128 n 10000 10              56      20917019 ns op      161352 B op        148 allocs op BenchmarkIteratorSelector Chain l 128 n 100000 10              5     206931033 ns op     1545838 B op        156 allocs op BenchmarkIteratorSelector Chain l 256 n 100 10              2790        417856 ns op       11080 B op        263 allocs op BenchmarkIteratorSelector Chain l 256 n 1000 10              295       4167610 ns op       22600 B op        268 allocs op BenchmarkIteratorSelector Chain l 256 n 10000 10              27      41706057 ns op      164424 B op        276 allocs op BenchmarkIteratorSelector Chain l 256 n 100000 10              3     410400611 ns op     1548872 B op        284 allocs op   ReaderSelector Chain memory    ReaderSelector Chain time    IteratorSelector Chain memory    IteratorSelector Chain time    Time of reader and iterator for n 100000 and l 256    mdpage   This CLI tool generates one page Markdown lists with summary based on YAML    bash mdpage  page page yaml   README md    go instrument     Automatically add Trace Spans to Go methods and functions               This tool uses standard Go library to modify AST with instrumentation  Use this in your CI before compilation  It is also possible to track generated code  however comments will be missing  You can add new instrumentations by defining your own  Instrumenter  and invoking  Processor  like it is done in  main       No dependencies   400 LOC   OpenTelemetry  Datadog  NewRelic  etc       bash go install github com nikolaydubina go instrument latest   bash find    name    go    xargs  I   go instrument  app my service  w  filename      Functions and methods with  ctx context Context  in arguments  go func  s Cat  Name ctx context Context   name string  err error            will be instrumented with span  go func  s Cat  Name ctx context Context   name string  err error        ctx  span    otel Trace  my service   Start ctx   Cat Name       defer span End       defer func             if err    nil               span SetStatus codes Error   error               span RecordError err                            Example HTTP server  go instrument example  as it appears in Datadog     Features   Excluding and Including   To avoid instrumentation of function add comment directive anywhere in the file       go   instrument exclude SomeFunc SomeOtherfunc privateFunc       func  s Cat  Name ctx context Context   name string  err error        instrument exclude Name       To instrument only specific functions add comment directive anywhere in the file and pass   all false  in CLI       go   instrument include SomeFunc SomeOtherfunc privateFunc       func  s Cat  Name ctx context Context   name string  err error        instrument include Name       Errors   Functions that have named return  err error  will get spans with appropriate status and error recorded    go func  s Cat  Walk ctx context Context   err error            Comments   Comments will be stripped  This fits well if your next step is to compile    In Development         Dynamic error variable name       Creating error when return is not named       Detection if function is already instrumented       Span Tags arguments       Span Tags returns       Assigning  ctx  to     when  ctx  is not used in function   unused assignement  linter checks issue        Datadog native instrumenter       Keep comments  unsurmountable task without 3rd partty lib support  Go core has PR in progress to improve it in Go core  wait until that is merged       Motivation   It is laborious to add tracing code to every function manually  The code repeats 99  of time  Other languages can either modify code or have wrapper notations that makes even manual tracing much less laborious    As of  2022 11 06   official Go does not support automatic function traces  https   go dev doc diagnostics     Is there a way to automatically intercept each function call and create traces      Go doesn t provide a way to automatically intercept every function call and create trace spans  You need to manually instrument your code to create  end  and annotate spans      Thus  providing automated version to add Trace Spans annotation    Performance   Go Compiler Inlining   Since we are adding multiple functions calls  it affects Go compiler decisions on inlining  It is expected that Go will less likely inline    For example  can inline function  bash   go build  gcflags   m  m    internal example 2  1   grep OneLine internal example basic go 80 6  can inline OneLineTypical with cost 62 as  func context Context  int   int  error    return fib n   nil     bash go instrument  w  filename internal example basic go   Can not inline after instrumentation  bash   go build  gcflags   m  m    internal example 2  1   grep OneLine internal example basic go 132 6  cannot inline OneLineTypical  unhandled op DEFER       Appendix A  Related Work     https   github com hedhyw otelinji   Very similar to current project  This tool gracefully handles code comments  so that its output can be tracked with normal code in version control  Main difference current project focuses on minimal code and dependencies    https   github com open telemetry opentelemetry go instrumentation    in development  official eBPF based Go auto instrumentation   https   github com keyval dev opentelemetry go instrumentation   eBPF based Go auto instrumentation of  pre selected  libraries   https   developers mattermost com blog instrumenting go code via ast   Very similar  Instrumenting Go code for tracing      Appendix B  Other Languages   Java   Java runtime modifies bytecode of methods on load time that adds instrumentation calls  Pre defined libraries are instrumented  http  mysql  etc       Very short single line decorator statement can be used to trace selected methods    Datadog    java import datadog trace api Trace   public class BackupLedger      Trace   public void write List  transactions        for  Transaction transaction   transactions          ledger put transaction getId    transaction                     OpenTelemetry    java import io opentelemetry instrumentation annotations WithSpan    public class MyClass      WithSpan   public void myMethod                               Automatic instrumentation of all functions is also possible    Datadog supports wildcard for list of methods to trace      dd trace methods  Environment Variable  DD TRACE METHODS  Default  null  Example  package ClassName method1 method2      AnonymousClass 1 call  package ClassName     List of class interface and methods to trace  Similar to adding  Trace  but without changing code  Note  The wildcard method support       does not accommodate constructors  getters  setters  synthetic  toString  equals  hashcode  or finalizer method calls     bash java  javaagent  path to dd java agent jar  Ddd service web app  Ddd env dev  Ddd trace methods      jar path to application jar     Java Auto Instrumentation   Datadog Java Auto Instrumentation   Datadog Java Tracing Config   Datadog Instrumentation Business Logic   Javaassist     Python   Python monkeypatching of functions at runtime is used to add instrumentation calls  Pre defined libraries are instrumented  http  mysql  etc       Very short single line decorator statement can be used to trace selected methods    Datadog    python from ddtrace import tracer   class BackupLedger       tracer wrap       def write self  transactions           for transaction in transactions              self ledger transaction id    transaction       OpenTelemetry  python  tracer start as current span  do work   def do work        print  doing some work           Automatic instrumentation of all functions is also possible via monkeypatching  fidning stable library is pending       OpenTelemetry Python Instrumentation   Blog  Timescale  OpenTelemetry and Python  A Complete Instrumentation Guide   https   github com harshitandro Python Instrumentation     C       Only manual instrumentation    Rust     Very short single line decorator statement can be used to trace selected functions with well establisehd tokio framework       rust    tracing  instrument    pub fn shave yak  usize     Result     Box              rust    instrument    async fn write stream   mut TcpStream     io  Result            https   github com tokio rs tracing   https   opentelemetry io docs instrumentation rust   https   docs rs opentelemetry latest opentelemetry   https   github com open telemetry opentelemetry rust tree main examples   https   docs rs datadog apm latest datadog apm     Appendix C  Paths Not Taken   eBPF   With eBPF we can track latency  but we would not be able to assign errors to spans  Some platforms may not have access to eBPF    Wrapping internal functions   Benefit of wrapping is to keep original code without modifications  However  manual step for switching would still be requied  Given every single function is duplciated and is within same package  code will quickly become messy and hard to maintain by user    Wrapping exported functions   Typically  packages are failry big and performs lots of logic  Oftencase  business domains are split only in few large packages  Low level packages are already likely to be traced with standard tracing  MySQL   het http   etc   Thus  it is doubtful how much benefit would be from tracing only exported functions and only on import    Wrapping exported functions with separate package   This would lead to circular dependency failure  since some even exported functions in original package may be called withing same package  Thus  we would either skip those calls  or fail with circular dependency while trying to wrap those    Appendix D  Generating Many Spans   1 97K  spans  fibbonaci     3 7K  spans  go cover treemap     Appendix E  Directives   Orignal version was using  go instrument  directive  However  many members of Go community raised concern that it takes over reserved core Go toolchain directives  eg     go norace    Even though as of  2022 11 25  Go core does not use  go instrument   to respect community and Go core  leaving using    instrument   directive instead    Appendix E  Selectors   One of proposed solutions for selectors was to use regex    Specifically  first usecase proposed was to use     instrument exclude      instrument include  API      The issue with this is collision of two functions    A  exlude all and select specific   B  include all and exclude specific   Similarly  there is collision of subspace of functions for exclusion and inclusion    As of  2022 11 25    nikolaydubina does not know how to resolve this better  Thus  keeping simple map matching wiht  and  condition of overlaps  Example http server to illustrate  go instrument  with Datadog and OpenTelemetry integration    The repo is in un instrumented state    For instrumented versions check GitHub Tags  Releases  and Branches   How to make strict Enum in Go    To harden Enum with    compile time block of accidental arithmetics   compile time block of implicit cast of untyped constants   compile time block of all operators except      and        compile time block of creaing new values    zero overhead   Simply    wrap into struct   do not export field   make separate package      go package color   type Color struct  c uint     var       Undefined   Color       Red         Color 1      Green       Color 2      Blue        Color 3          There are still uncovered cases  albeit they are very unlikely and easy to spot    outside of package can swap enum values    if not separate pacakge  inside of package can override values   Why not  string        go type Color string   const      Red Color    red      Blue Color    blue          Passing strings in Go is done by referneces  String content is not copied in function calls nor assignments    Problems    string comparison can take up to  O N    which is very common operation on Enums    strings in struct fields result to more indirection as compared to  int  or  struct     if it is not wrapped into struct  it will also leak comparison and concatenation operators and casts    can be up to  4x     5x  slower   Why not  uint  and  iota        go type Color uint   const      Red Color   iota     Blue         This is very common way  It is also efficient    Problems     leaks all arithmetic operators    leaks cast    easily mixed with untyped constants in very many places  function returns  channles  switch  casts  assignemnts  etc    Benchmarks   All versions have zero mallocs and memory transfer    bash go test  bench    count 3   color   doc struct go test  bench    count 3   color int   doc int go test  bench    count 3   color string   doc string benchstat  split  XYZ  doc struct doc int doc string   name   time op                          struct       int          string EnumPassFunction call one 10            2 12ns   0   2 14ns   0    2 52ns   0  EnumPassFunction call one apple 10      2 37ns   0   2 44ns   0    8 62ns   0  EnumPassFunction Big call one 10        2 19ns   0   2 14ns   0    6 59ns   0  EnumPassFunction Big call one apple 10  2 39ns   0   2 38ns   0   11 19ns   0    What is Enum    Enum is a data type consisting of a set of named values  They are typically constants  They have comaprison and assignemnt operators  Underlying represtentation is free up to compiler  but typically is an integer  Enums are typically prevented from illogical operations such as arithmetic operations      C    interger  exposed  arithmetics permitted    C     integer  not exposed  some arithmetics permitted    C      integer  not exposed  not converted  some arithmetics not permitted    Go    integer  exposed  arithmetics permitted    Python    integer  arithmetics permitted    Java    not integer  not converted  arithmetics not permitted  internally integer    Rust    not integer  not converted  arithmetics not permitted  can be extended to integer   Swift    not integer  not converted  arithmetics not permitted  can be extended to interger     Related Tools     https   github com nishanths exhaustive   performs exhaustive checks in switch  does not check for type conversions  widely used  however does not support struct enums   https   github com loov enumcheck   focuses on performing exhastive checks  also does constant expressions validations  as of 2022 11 15  work in progress after 3 years     References     https   en wikipedia org wiki Enumerated type   https   go dev ref spec   https   www w3schools com java java enums asp   https   en cppreference com w cpp language enum   https   en cppreference com w c language enum   https   docs python org 3 library enum html   https   godbolt org     Assembly   Overall   int  and  struct   int    versions are same and very efficient  Version with  string   as expected  has much more code and jumps  presumably for string comparison logic with shortcuts    int   s main callOne pc101          CALL    runtime panicdivide SB          XCHGL   AX  AX         TEXT    main callOneApple SB   NOSPLIT ABIInternal   0 48         MOVQ    BX  main a 16 FP          FUNCDATA         0  gclocals IuErl7MOXaHVn7EZYWzfFA   SB          FUNCDATA         1  gclocals J5F 7Qw7O7ve2QcWC7DpeQ   SB          FUNCDATA         5  main callOneApple arginfo1 SB          FUNCDATA         6  main callOneApple argliveinfo SB          PCDATA   3   1         CMPB    SIB  R8B         JNE     main callOneApple pc23         XORL    AX  AX         XORL    BX  BX         MOVQ    BX  CX         MOVL     3  DI         RET   struct   s main callOne pc101          CALL    runtime panicdivide SB          XCHGL   AX  AX         TEXT    main callOneApple SB   NOSPLIT ABIInternal   0 48         MOVQ    BX  main a 16 FP          FUNCDATA         0  gclocals IuErl7MOXaHVn7EZYWzfFA   SB          FUNCDATA         1  gclocals J5F 7Qw7O7ve2QcWC7DpeQ   SB          FUNCDATA         5  main callOneApple arginfo1 SB          FUNCDATA         6  main callOneApple argliveinfo SB          PCDATA   3   1         CMPB    SIB  R8B         JNE     main callOneApple pc25         MOVBLZX main Purple SB   DI         XORL    AX  AX         XORL    BX  BX         MOVQ    BX  CX         RET   string   s main callOneApple pc0          TEXT    main callOneApple SB   ABIInternal   72 64         CMPQ    SP  16 R14          PCDATA   0    2         JLS     main callOneApple pc234         PCDATA   0    1         SUBQ     72  SP         MOVQ    BP  64 SP          LEAQ    64 SP   BP         MOVQ    R9  main c 128 FP          FUNCDATA         0  gclocals J YAdREO0hCD8EYeU6UDCw   SB          FUNCDATA         1  gclocals yROwgZmxcEjQO7qZUR29ZQ   SB          FUNCDATA         5  main callOneApple arginfo1 SB          FUNCDATA         6  main callOneApple argliveinfo SB          PCDATA   3   1         MOVQ    BX  main a 88 SP          MOVQ    CX  main a 96 SP          MOVQ    DI  main a 104 SP          MOVQ    SI  main a 112 SP          MOVQ    R8  main a 120 SP          MOVUPS  X15  main  r0 24 SP          MOVUPS  X15  main  r0 32 SP          MOVUPS  X15  main  r0 48 SP          MOVQ    main a 120 SP   CX         MOVQ    main a 112 SP   AX         CMPQ    R10  CX         JNE     main callOneApple pc105         MOVQ    R9  BX         PCDATA   1   1         NOP         CALL    runtime memequal SB          TESTB   AL  AL         JNE     main callOneApple pc170 main callOneApple pc105          MOVQ    main a 88 SP   DX         MOVQ    DX  main  r0 24 SP          MOVUPS  main a 96 SP   X0         MOVUPS  X0  main  r0 32 SP          MOVUPS  main a 112 SP   X0         MOVUPS  X0  main  r0 48 SP          MOVQ    main  r0 24 SP   AX         MOVQ    main  r0 32 SP   BX         MOVQ    main  r0 40 SP   CX         MOVQ    main  r0 48 SP   DI         MOVQ    main  r0 56 SP   SI         MOVQ    64 SP   BP         ADDQ     72  SP         RET main callOneApple pc170          MOVUPS  X15  main  r0 24 SP          MOVUPS  X15  main  r0 32 SP          MOVUPS  X15  main  r0 48 SP          LEAQ    go string  purple  SB   DI         MOVQ    DI  main  r0 48 SP          MOVQ     6  main  r0 56 SP          MOVQ    main  r0 24 SP   AX         XORL    BX  BX         MOVQ    BX  CX         MOVL     6  SI         MOVQ    64 SP   BP         ADDQ     72  SP         RET main callOneApple pc234          NOP         PCDATA   1    1         PCDATA   0    2         MOVQ    AX  8 SP          MOVQ    BX  16 SP          MOVQ    CX  24 SP          MOVQ    DI  32 SP          MOVQ    SI  40 SP          MOVQ    R8  48 SP          MOVQ    R9  56 SP          MOVQ    R10  64 SP          CALL    runtime morestack noctxt SB          MOVQ    8 SP   AX         MOVQ    16 SP   BX         MOVQ    24 SP   CX         MOVQ    32 SP   DI         MOVQ    40 SP   SI         MOVQ    48 SP   R8         MOVQ    56 SP   R9         MOVQ    64 SP   R10         PCDATA   0    1         NOP         JMP     main callOneApple pc0 main main pc0          TEXT    main main SB   ABIInternal   72 0         CMPQ    SP  16 R14          PCDATA   0    2         JLS     main main pc88         PCDATA   0    1         SUBQ     72  SP         MOVQ    BP  64 SP          LEAQ    64 SP   BP         FUNCDATA         0  gclocals g2BeySu wFnoycgXfElmcg   SB          FUNCDATA         1  gclocals g2BeySu wFnoycgXfElmcg   SB          MOVQ    main  stmp 1 SB   BX         MOVQ    main  stmp 1 8 SB   CX         MOVQ    main  stmp 1 16 SB   DI         MOVQ    main  stmp 1 24 SB   SI         MOVQ    main  stmp 1 32 SB   R8         MOVL     10  AX         LEAQ    go string  blue  SB   R9         MOVL     4  R10         PCDATA   1   0         CALL    main callOneApple SB          MOVQ    64 SP   BP         ADDQ     72  SP         RET   go commentage       How far behind are comments compared to code  Are they being updated  Inspired by  Clean Code  by Robert C  Martin  this  go vet  compatible tool analyses AST and  git  and collects details on how far comments drift from code they describe    bash go commentage         txt kubernetes pkg util ipset ipset go 283 1   CreateSet   doc last updated behind days 1336 83  kubernetes pkg util ipset ipset go 296 1   createSet   doc last updated behind days 1603 17  kubernetes pkg util ipset ipset go 320 1   AddEntry   doc last updated behind days 1578 10  kubernetes pkg util ipset ipset go 332 1   DelEntry   doc last updated behind days 1578 10  kubernetes pkg util ipset ipset go 340 1   TestEntry   doc last updated behind days 450 07  kubernetes pkg util ipset ipset go 356 1   FlushSet   doc last updated behind days 0 00  kubernetes pkg util ipset ipset go 364 1   DestroySet   doc last updated behind days 73 85  kubernetes pkg util ipset ipset go 372 1   DestroyAllSets   doc last updated behind days 0 00  kubernetes pkg util ipset ipset go 380 1   ListSets   doc last updated behind days 0 00  kubernetes pkg util ipset ipset go 389 1   ListEntries   doc last updated behind days 0 00    Requirements   You need to have  git  with version    2 37     bash go install github com nikolaydubina go commentage   Filtering   To narrow down output  filters can be specified  This allows interactive exploration  This also allows usage of this tool in CI  since when no diagnostic is printed then status code is 0    bash   go commentage  min days behind 100         go commentage  commit  min days behind 100         go commentage  min days behind 10  commit  min days behind 100         echo    0   Heuristics   Simple Age Difference   Measure of how far away in terms of days or commits last update of function body as compared to last update to associated doc comment    Weighted Age Difference     Warning  Work in Progress     Code changes happen at various rates  Comments and code can change one line or can change 90  of lines  It is useful to differentiate between updates    References     https   git scm com docs git blame   https   git scm com docs git rev list   https   github com nishanths exhaustive   https   github com kubernetes kubernetes     Appendix A  Full Output   This can be useful for debugging or exporting for post processing and further data visualization    bash go commentage  verbose  time  commit         txt kubernetes pkg util pod pod go 34 1   PatchPodStatus   last updated at 2022 08 02T13 58 08 08 00  doc last updated at 2020 02 27T06 05 33 08 00  doc last updated behind days 887 33 last commit 04fcbd721cd3  doc last commit b2528654797e  doc last commit behind 8786  kubernetes pkg util pod pod go 74 1   ReplaceOrAppendPodCondition   last updated at 2022 11 07T18 57 56 08 00  doc last updated at 2022 11 07T18 57 56 08 00  doc last updated behind days 0 00 last commit 4e732e20d05e  doc last commit 4e732e20d05e  doc last commit behind 0  kubernetes pkg util procfs procfs fake go 28 1   GetFullContainerName   last updated at 2015 11 14T07 47 25 08 00  doc last updated at 2017 04 04T14 16 34 08 00  doc last updated behind days  507 27 last commit fb576f30c838  doc last commit 932ece5cfd0f  doc last commit behind  10936  kubernetes pkg util procfs procfs unsupported go 34 1   GetFullContainerName   last updated at 2016 08 18T23 01 03 08 00  doc last updated at 2016 08 17T07 34 14 08 00  doc last updated behind days 1 64 last commit 5eef6b8d91a2  doc last commit a2824bb7a337  doc last commit behind 58  kubernetes pkg util procfs procfs unsupported go 40 1   PKill   last updated at 2016 08 18T23 01 03 08 00  doc last updated at 2016 08 18T23 01 03 08 00  doc last updated behind days 0 00 last commit 5eef6b8d91a2  doc last commit 5eef6b8d91a2  doc last commit behind 0  kubernetes pkg util procfs procfs unsupported go 46 1   PidOf   last updated at 2016 08 18T23 01 03 08 00  doc last updated at 2016 08 18T23 01 03 08 00  doc last updated behind days 0 00 last commit 5eef6b8d91a2  doc last commit 5eef6b8d91a2  doc last commit behind 0  kubernetes pkg util removeall removeall go 35 1   RemoveAllOneFilesystemCommon   last updated at 2021 06 04T06 38 37 08 00  doc last updated at 2021 06 04T06 38 37 08 00  doc last updated behind days 0 00 last commit 484eb0182224  doc last commit 484eb0182224  doc last commit behind 0  kubernetes pkg util removeall removeall go 115 1   RemoveAllOneFilesystem   last updated at 2021 06 04T06 38 37 08 00  doc last updated at 2021 06 16T00 40 17 08 00  doc last updated behind days  11 75 last commit 484eb0182224  doc last commit 01bb0f86b02b  doc last commit behind  1  kubernetes pkg util removeall removeall go 126 1   RemoveDirsOneFilesystem   last updated at 2021 06 04T06 38 37 08 00  doc last updated at 2021 06 16T00 40 17 08 00  doc last updated behind days  11 75 last commit 484eb0182224  doc last commit 01bb0f86b02b  doc last commit behind  1  kubernetes pkg util rlimit rlimit unsupported go 27 1   SetNumFiles   last updated at 2020 02 25T13 58 28 08 00  doc last updated at 2020 02 25T13 58 28 08 00  doc last updated behind days 0 00 last commit 4936cd476bf3  doc last commit 4936cd476bf3  doc last commit behind 0  kubernetes pkg util slice slice go 26 1   CopyStrings   last updated at 2017 06 23T11 41 18 08 00  doc last updated at 2015 01 23T06 12 37 08 00  doc last updated behind days 882 23 last commit f98bc7d45435  doc last commit f7e3cb12a6e7  doc last commit behind 19409  kubernetes pkg util slice slice go 37 1   SortStrings   last updated at 2015 01 23T06 12 37 08 00  doc last updated at 2015 01 23T06 12 37 08 00  doc last updated behind days 0 00 last commit f7e3cb12a6e7  doc last commit f7e3cb12a6e7  doc last commit behind 0  kubernetes pkg util slice slice go 44 1   ContainsString   last updated at 2017 04 07T08 14 16 08 00  doc last updated at 2017 04 07T08 14 16 08 00  doc last updated behind days 0 00 last commit 151770c8fde9  doc last commit 151770c8fde9  doc last commit behind 0  kubernetes pkg util slice slice go 58 1   RemoveString   last updated at 2017 11 23T23 00 35 08 00  doc last updated at 2017 11 23T23 00 35 08 00  doc last updated behind days 0 00 last commit e1312f2c00ed  doc last commit e1312f2c00ed  doc last commit behind 0  kubernetes pkg util tail tail go 38 1   ReadAtMost   last updated at 2022 10 20T15 13 28 08 00  doc last updated at 2016 12 08T04 56 06 08 00  doc last updated behind days 2142 43 last commit cc90e819bce9  doc last commit 2bb2604f0b0d  doc last commit behind 29079  kubernetes pkg util tail tail go 68 1   FindTailLineStartIndex   last updated at 2018 02 11T11 02 23 08 00  doc last updated at 2016 12 08T04 56 06 08 00  doc last updated behind days 430 25 last commit 7cfb94cbc576  doc last commit 2bb2604f0b0d  doc last commit behind 8120  kubernetes pkg util tolerations tolerations go 27 1   VerifyAgainstWhitelist   last updated at 2019 08 21T09 21 57 08 00  doc last updated at 2017 02 28T02 34 46 08 00  doc last updated behind days 904 28 last commit 5a50b3f4a2a2  doc last commit af5379485411  doc last commit behind 15293    Same Receiver Pointer  smrcptr        This  go vet  compatible linter detects mixing pointer and value method receivers for the same type       go type Pancake struct     func NewPancake   Pancake   return Pancake       func  s  Pancake  Fry        func  s Pancake  Bake            bash   smrcptr       smrcptr internal bakery pancake go 7 1  Pancake Fry uses pointer smrcptr internal bakery pancake go 9 1  Pancake Bake uses value   Why this is useful  Go has rules on how it can automatically select value and method receivers  which is complex and can lead to bugs  Official Go  wiki  and  style guide  recommends      Don t mix receiver types  Choose either pointers or struct types for all available methods      Requirements   bash go install github com nikolaydubina smrcptr latest   Features   Return Status Code   When issue is detected  related info is printed and status code is non zero  This is similar as other  go vet  and linters  This allows convenient use in CI    Constructor   It is also useful to detect if  construtor  functions that commonly start with  New     returns value that matches used in receivers    bash   smrcptr   constructor true   internal     smrcptr internal bakery pancake go 7 1  Pancake Fry uses pointer smrcptr internal bakery pancake go 5 1  Pancake NewPancake uses value smrcptr internal bakery pancake go 9 1  Pancake Bake uses value smrcptr internal bakery pancake go 14 1  Cake Fry uses pointer smrcptr internal bakery pancake go 16 1  Cake Bake uses value smrcptr internal bakery pancake go 23 1  Brownie Bake uses pointer smrcptr internal bakery pancake go 21 1  Brownie NewBrownie uses value smrcptr internal bakery pancake go 35 1  BadCookie NewBadCookie uses pointer smrcptr internal bakery pancake go 37 1  BadCookie Bake uses value   Existing Linters   staticcheck   As of  2022 11 30   it does not detect that pointer and value method receivers are mixed  Most relevant analyzser  ST1016  checks only name of method reciver    bash   staticcheck  checks ST1016           main go 9 18  methods on the same type should have the same receiver name  seen 1x  v   2x  s    ST1016    Using all analyzers does not detect it either    bash staticcheck  checks all       main go 9 18  methods on the same type should have the same receiver name  seen 1x  v   2x  s    ST1016    References     https   github com golang go wiki CodeReviewComments receiver type   https   golang org ref spec Method declarations   https   golangci lint run usage linters    https   github com dominikh go tools blob master stylecheck lint go L295   https   github com dominikh go tools tree master stylecheck testdata src CheckReceiverNamesIdentical   https   google github io styleguide go decisions receiver type
55,floydprice,bb meteor architect   This is a prototype applicatiob builder plugin for Meteor  I have high hopes for it but right now its just a few ideas i m testing  INSS Prototype Services   Some prototype service written in NodeJS designed to seed data from CSV files  in the data directory     Installation   Step 1   Install Meteor   Mac and Linux     curl https   install meteor com    sh     Windows   See meteor com install   Step 2   Clone repo and cd into directory   Step 3   run the meteor server     meteor     Availible Services   Service Status    http   localhost 3000   PostCode Lookup    http   localhost 3000 v1 postcodes  q PL14PD   Managed Parties    http   localhost 3000 v1 managed parties  q bank   Re seeding the database   If you have updated on of the data files you can simple reset the database and the app will automatically re seed upon restart     meteor reset   meteor   ebf   Generic electronic file repository with token based authentication    Readme conent TODO Digital Apprenticeship Service App     Static website with links to external DAS Pages     What is it    DAS Web App built using the GDS Design   govuk elements     Running this site locally   If you would like to clone the repository and run it locally  you will need  Node js   at least version v0 10 0     Clone this repository   git clone https   github com rajeshdigital gds apprenticeship service git    Install the required node modules   npm install    Run the app   npm start    Go to  localhost 3000  in your browser  TodoMVC with Flutter   An implementation of the  famous TodoMVC application   in flutter     Getting Started   Clone the repo  open it in your favorite editor and run the flutter    Or from the command line      bash     get the dependencies flutter pub get     run the project flutter run       DfE Express Health Check       Express middleware to provide health check endpoints
56,OrkoHunter,Modules   guess py    Assume a number and the computer will guess it out for you asking some finite questions    lfactor py    Returns the largest prime factor of an integer    say py    Usses  pyttx  to enunciate strings    python say py  Hello World          Morse Talk   Morse Talk is a Python library which deals with  Morse code   Installation   Using pip   sh pip install morse talk   Development version   sh git clone https   github com OrkoHunter morse talk git cd morse talk  python setup py install   Examples      python         import morse talk as mtalk             Encoding in morse      python         mtalk encode  Alpha Ranger 45 departed                                                                                                                                                     Encoding using binary pattern      python         mtalk encode  Alpha Ranger 45 knocked down   encoding type  binary    101110001011101010001011101110100010101010001011100000001011101000101110001110100011101110100010001011101000000010101010111000101010101000000011101011100011101000111011101110001110101110100011101011100010001110101000000011101010001110111011100010111011100011101              Decoding a code encoded in morse      python         code                                                       mtalk decode code   BOMB X PM              Decoding a binary pattern      python         bin   mtalk encode  Alpha Ranger 45 knocked down   encoding type  binary   mtalk decode bin  encoding type  binary    ALPHA RANGER 45 KNOCKED DOWN              Morse Code   Morse code is a method of transmitting text information as a series of on off tones  lights  or clicks that can be directly understood by a skilled listener  or observer without special equipment  The International Morse Code encodes the ISO basic Latin alphabet  some extra Latin letters  the Arabic numerals and a  small set of punctuation and procedural signals as standardized sequences of  short and long signals called  dots  and  dashes   or  dits  and  dahs    Because many non English natural languages use more than the 26 Roman letters   extensions to the Morse alphabet exist for those languages      International Morse code is composed of five elements      short mark  dot or  dit         dot duration  is one time unit long   longer mark  dash or  dah        three time units long   inter element gap between the dots and dashes within a character   one dot duration or one unit long   short gap  between letters    three time units long   medium gap  between words    seven time units long     My Dotfiles   Saving code in Google Drive is not cool    File Information     proxies   Making life out of proxy wall in the campus   95proxies   Put it inside   etc apt apt conf d    config   ssh socks settings in     ssh    environment   Place it in   etc    Governs the system proxy   scipts   Disable Touchpad   Bash script for disabling touchpad   fix touchpad   How to fix touchpad on a ASUS X550LD Laptop   sakis3g   The sakis3g script   For USB modems on Linux   vim   Place it in    vim runtime   zsh   Courtesy Oh My ZSH   nxcpy     Please refer to the  wiki  as the project is in alpha mode  Minesweeper   How to play   sh   wget https   raw githubusercontent com OrkoHunter Minesweeper master dist Minesweeper jar   java  jar Minesweeper jar   and play     For  Windows   simply download  this  jar file and run it with  java  jar Minesweeper jar     Please do report issues in case of a bug  gsoc FAQs   An unofficial  Google Summer of Code  Frequently Asked Questions    Read This before you proceed   If you want GSoC advice  please understand that there are many  many mentors who have been asked similiar questions over and over again  Hence  there is a GSoC Manual for you  It might be painful for you if you do not like reading texts on the internet  but you will have to develop the habit of reading guides  documentations  manuals etc  if you plan to contribute to Open Source  Also  the GSoC manual is written in a very simple and interactive manner  Hence  sit down  and read the entire thing  Just do it     closed book  https   google github io gsocguides student    Now  once you have read the entire thing      And you still have questions  mostly similar to  What does XYZ mean    do a Google Search  Spend time on the articles that pop up  read them    And you really need to know  my  opinion for some question you have  search them below  If you do not find your question here  please feel free to message email me      Note   I use this repository for personal use and send it to friends and acquantainces when asked for an advice related to GSoC    Questions     When do I start  Is it too late      Do first year undergraduate students qualify  I find this very hard    I get scared when I see seniors applying on the same project  What to do    But I just started with Open Source and X has more experience and knowledge  Then why would the mentor choose me    I do not know even a small fraction of technologies listed in the GSoC projects  How am I going to learn them all     How will GSoC help me in my career     Who are the mentors  Do they get paid    How much minimum coding experience do I need    I just know C  Is that enough    Why does Google pay if it gets nothing in return      How are the applications for GSoC selected  On what factors do they determine the skill set of a student    Can we contribute to the project s  only after getting selected    Is GSoC hard to do    Why do you recommend GSoC  and to whom        When do I start  Is it too late      It varies with the organizations  Few orgs have contributors who start way too early every year while some orgs are new and receives traction only when GSoC releases the confirmed org list for that year  But I do not think it s ever too late to apply for the next GSoC   I came to know about my organization  NetworkX under after Python Software Foundation  around a week after it was officially released   I was 6 days late for  my first comment   but mostly it s not late as long as GSoC has not officially released the list of organizations    Do first year undergraduate students qualify  I find this very hard    Yes  a good number of first years qualify for GSoC  Nothing will change if you just wait for one more year  start now  You surely can learn whatever your college will teach you related to GSoC  in days  weeks or months    I get scared when I see seniors applying on the same project  What to do    In GSoC  there are 2 types of applicants  One who are afraid  take your example if you are  and second  lets call them X  who will be applying for the second time  doesn t matter if they qualified the first time   Now  there are project ideas and mentors associated with those ideas  All the mentor is looking for is a trust worthy student who can understand a project idea really well and can do something about it  So  if the mentor feels that you and X both have the same level of understanding about the project  they shall consider you both  This understanding comes from the conversations you have with the mentor  The way you email them and ask questions  etc  Now  the mentor trusts both of you  and both of your proposals look very good  then how to choose one  They will go through your works  So  that s when you can take a lead  and create more number of good quality Pull Requests than X  and get it merged  Simple as that    But I just started with Open Source and X has more experience and knowledge  Then why would the mentor choose me    If you are really sure about that  then I will suggest   1  Don t choose the project idea which X has already chosen  You both are allowed to work under the same org but on different projects  You will have to go through the communication channel  its previous emails to know which project idea X is interested in  2  If you are an undergraduate student  do not choose research oriented organizations  Not always but a lot of mentors will be biased when it comes to choose between undergrads  postgrads and research scholars on research oriented projects  e g  Machine Learning  Deep Learning and Artificial Intelligence and many more  But this a subjective advice based on experiences  so you can go ahead and take risks    I do not know even a small fraction of technologies listed in the GSoC projects  How am I going to learn them all     It is alright that you feel like it  But understand that it is going to be a problem with every GSoC student  Hence you are expected to learn it gradually  Although you can learn the basic stuff in 1 2 days of reading the tutorial of the project  Most people actually do it on the first few days when the orgs are released for the program  So  dont worry and read a lot    How will GSoC help me in my career     This is debatable  More often than not  GSoC makes people more responsible for Software Development and Open Source  Plus  you will see and learn different aspects of working in teams and volunteering  A diverse range of careers depened on these skills and GSoC might be your first step in learning them  If you have the question  whether it will help you land a job  then you are right  Companies look for projects and experiences which you can talk about and did good in  GSoC will give you one such project and experience    Who are the mentors  Do they get paid    Mentors are people who volunteer to help a particular project organization grow  GSoC Mentors can be the founders of the project  early or past contributors or past year GSoC students themselves    How much minimum coding experience do I need    This also depends on the complexity of the projects you are going to choose  Generally  GSoC mentors already label the project ideas as Beginner  Intermediate and Advanced  So  you can choose the project ideas accordingly    I just know C  Is that enough    Yeah   There are a lot of organizations which use only C  But make sure you understand this language in depth and gain a little experience in creating softwares in C  And  just to add  it will take you very less time to learn languages like Python or Ruby to the point where you d be able to say  I know C and Python     Why does Google pay if it gets nothing in return      Because Google understands the core problem and Google has the money  There was very little incentive for young college students to get into open source  Now you can see so many Software Developers understanding the importance of Open Source since a very young age  Also  Google runs on a lot of open source softwares    How are the applications for GSoC selected  On what factors do they determine the skill set of a student      Your proposal and the depth in your understanding of the project idea    Your contributions to the project    Mentor s trust on you as a potential student which develops with interactions and how much smart questions you ask  There is no defined weightage to these factors and are subjective to each mentor      Can we contribute to the project s  only after getting selected    No  you can contribute to the project anytime  that is what open source is about  GSoC is just a program and has some rules  If you are selected in that program  Google pays you for the work you would have done anyway without taking any money    Is GSoC hard to do    GSoC needs dedication and patience  probably alike everything else  Keep your motivation strong and talk to peers and seniors if you feel inferior to anyone  It s not hard  it s worthwhile    Why do you recommend GSoC  and to whom      I am still not able to clearly understand the reason why I recommend it so much to college students who wish to pursue anything related to Software as their career or at least consider it as an option  I am no expert to tell you why Open Source is good  you can look it up online  But I can answer to this  Why are you promoting GSoC so much  Shouldn t you be promoting hardcore Open Source instead   This is where I differ and agree with the questioner at the same time  Yes  we should be promoting hardcore Open Source  But what would be the perfect strategy  Engineers with jobs  when they have an upfront deadline to ship a product and they need a simple NLP date time parser in Python  they don t go and write one library  they find it somewhere on GitHub  and use them to meet the deadline  And thenafter  when they create something useful  they return the favor to open source  There are so many projects being maintained by corporate giants like Google  Facebook  Microsoft and so on while they are open source and has wide users and contributors  So  the question is  why in this world  an 18 year old college student would do such thing  What is the value the kid sees  In a college  If you just promote hardcore open source  50 out of 1500 would come and 1 would stay  If you promote Google Summer of Code  500 out of 1500 would come and 30 would stay  And out of those 30  you can definitely be certain that more than 1 would continue doing open source  This is a model which Google founders created in 2005  Even they resorted to money  incentives  and glamour to get more students into open source  I still feel young to give this problem an other attempt  as of now           Denotes personal favorite questions   Got more questions    Create an issue   python freezing example       Install with  python setup py install       sh     foo   Foo was called      bar   Bar was called        Things to remember     pyinstaller  can freeze one python script into a stand alone binary executable which can be executed by adding      prefix    Add the binary into one of your   PATH  locations and the prefix is not required  That works as a program     setuptools  provides the  entry points  option for freezing a complete library  That s cool     Note from the author   I no longer work on this project  Maintaining a server and other extensions to store reminders and execute them on time takes a lot of effort  There are gigantic companies and startups working on this  It also made me hit my first burnout because I thought I could make everything on my own in a short duration of time  I m thankful I learnt this early    I agressively use Google Calendar which sends me all kinds of reminders aka  Pings      and thus instead of re inventing the wheel  I think I can work on new ideas  Plus  there exists quite a lot of tools which interface with Google Calendar from the command line and and they take care of NLP as well  Thus  I feel there is no  need  to revive the project    It was a good run  Thanks to the 300 registered and other anonymous users for giving chills to an 18 year old kid  on one of his first pet projects    April 20  2018   TLDR  Have a look at  gcalcli  or  khal         ping me   A Cross Platform personalized Ping   NOTE   The server is not being maintained right now   Because my GitHub student pack expired    The beauty of  ping me  is its command line interface  Get all of your reminders done just by a single line command on your favorite terminal screen   ping me  will  surely  ping you at that time  no matter you are online or not  It will get to you on your phone device  smart watch and even SMS in worst cases    Stay Lazy  Stay Updated     Installation   Current Release     0 3   Step 1    Installing package and dependencies  sh   pip install ping me  Make sure all the dependencies get installed properly while the installation    Step 2    Setting up cronjob  Not for windows  See step 3   sh   crontab  e  In the file  add the following lines     PATH  usr local sbin  usr local bin  usr sbin  usr bin  sbin  bin DISPLAY  0 0                     get ping      Make sure to leave a blank line at the end of the file                      Save and exit  The installation is complete    See  this  for some explanation of the crontab   Step 3    Chrome Extension   Download the chrome extension from  here     Log in with the credentials you used for ping me  And we are done    Note   ping me will work even if all chrome windows are closed    Usage   Use of flags   sh   ping me  d November 24 2015  t 14 30 Get up and eat   By default   d  and   t  accounts for  datetime today  and  00 00 hours  respectively  Go ahead and make experiments with the syntax    No flags  pure language   sh   ping me to get up and eat tomorrow afternoon   Add   v  flag at the end to see the verbose output    Project Status      X   ping me  identifies date  time and message using flags    X   ping me  asks for configuration on first request    X   ping me  stores the configuration on remote server    X   ping me  stores the message with datetime stamp on the server    X  Server activates the ping 50 seconds prior to its exact time    X  Server ready for a GET request    X   ping me  makes full use of natural language processing    X   ping me  notifies through chrome firefox extension    X   ping me  works on linux    X   ping me  works on windows    X   ping me  works on OS X        ping me  works on Android        ping me  sends texts to phone        ping me  ping me works on ios        ping me  works on Windows phone     Contribution    Please feel free to report bugs in the application  I ll try to fix them  Also  if either of   javascript   or android is your piece of cake  please contact me and we ll make things up real quick  Enjoy anyways          A Meta CLI toolkit     Your personal shell command keeper     Why    Writwick Wraj loves using the command line    Writwick googles   How to do X in terminal    and multiple forums and blog posts finally provide him the magical  command  for the rescue  Problem Solved     Fast forward couple weeks  Writwick has to do X in terminal  again  Wraj remembers solving this few weeks ago  Let him do a reverse i search with  Ctrl R   Nope  can  t remember sh t  Browser search history  25 web pages found matching  X   Argh    Writwik finally finds the solution  From this time Writwik starts writing the commands somewhere online for the future    Wait  why shouldn  t he keep the command in his terminal itself if this is only place where he  ll ever have use it    Features     Save a new command with a brief description   Search the saved commands using powerful patterns   Save the commands as a secret GitHub gist   Use  keep push  and  keep pull  to sync the commands between GitHub   gist and other computers      ProTip   Save the commands you usually forget in ssh sessions and sync it with your local machine    Installation     pip3 install keep    Use Python 3 6 or later    You can install pip3 using apt get as  sudo apt install python3 pip     Usage   Usage  keep  OPTIONS  COMMAND  ARGS        Keep and view shell commands in terminal only     Read more at https   github com orkohunter keep  Options     v    verbose  Enables verbose mode      help         Show this message and exit   Commands    edit          Edit a saved command    github token  Register a GitHub Token to use GitHub Gists as a backup    grep          Searches for a saved command    init          Initializes the CLI    list          Shows the saved commands    new           Saves a new command    pull          Pull commands from saved GitHub gist    push          Push commands to a secret GitHub gist    rm            Deletes a saved command    run           Executes a saved command    update        Check for an update of Keep     See the detailed usage and tutorial    Command line Completion   To enable command line completion  TAB completion  follow these steps for the shell of your choice   bash       Create a directory in your home directory called   bash   mkdir  p  HOME  bash        Copy  completion keep bash  to   HOME  bash keep   curl  SLo   HOME  bash keep   https   raw githubusercontent com OrkoHunter keep master completions keep bash         Add the following lines to   HOME  bashrc  file      f   HOME  bash keep           HOME  bash keep         zsh       Create a directory in your home called   zsh   mkdir  p  HOME  zsh        Copy  completion keep zsh  to   HOME  zsh  keep   curl  SLo   HOME  zsh  keep   https   raw githubusercontent com OrkoHunter keep master completions keep zsh         Add the following lines inside   HOME  zshrc  file   fpath   HOME  zsh  fpath  autoload  Uz compinit    compinit        Contribute   This is a very young project  If you have got any suggestions for new features or improvements  please comment over  here   Pull Requests are most welcome           Not a command line fanatic  Here are some resources for you       https   github com jlevy the art of command line   https   github com herrbischoff awesome osx command line   https   github com alebcay awesome shell   https   github com aharris88 awesome cli apps   opensoft   Open Soft is an application based programming competition which takes place every year at IIT Kharagpur in which teams from different Halls of Residence try to make the best out of a problem statement  https   wiki metakgp org w Open Soft   I ll push all of my related works in this repository as after all  the name has the word   open   in it       Clone the repository with   git clone   recursive https   github com OrkoHunter opensoft Dependencies   Let s dig  em out    Note   The script is at very initial stage with very simple crawling  Kindly report issues in case of bugs  believe me there s a lot of them     If you think we can do more from the data  you re most welcome to suggest implement it  My favorite fortune aggregator     For the love of Linux command line tool  fortune     See my Collection   tada              All generalizations are false                                                           including this one                                                                         Mark Twain   Pudd nhead Wilson s         Calendar                                                                   o o                                                                                    To quote from a sysadmin s personal notes   There are two types of Linux users           One who   Linux       One who wish Windows was made for programming          W  Shakespeare 1549 AD     I am so in love with Linux  I am personally very fond of the commands  fortune  and  cowsay   so much that every time when my terminal session starts  I spare 2 5 seconds with new fortunes  But there were many ones which I wanted to store somewhere and look over again  So  that s why you are reading this now         Read how this works  notebooks   Collection of personal IPython notebooks Tornado Web Server and Python 3 on Openshift   This git repository helps you get up and run quickly Tornado Web Server on Openshift     Running on Openshift   Create an account at http   openshift redhat com    Install the RHC client tools if you have not already done so     sudo gem install rhc    Create a python 3 3 application    rhc app create tornadopy3 python 3 3    Add this upstream repo    cd tornadopy3  git remote add upstream  m master git   github com rancavil tornado openshift quickstart git  git pull  s recursive  X theirs upstream master    Then push the repo upstream    git push    That s it  You can now checkout your application at     http   tornadopy3  youtnamespace rhcloud com    The structure of directories created are     tornadopy3        wsgi py        gitignore       README md       setup py       requirements txt       data        libs         openshift             action hooks             cron             markers        wsgi             application            openshift py            static             templates                  index html    The main file is openshift py  this contains the definitions of the handlers  You can change it the name  but you must be sure to change the import statement in application file     import tornado web  import os   class MainHandler tornado web RequestHandler         def get self              self render  index html       Put here yours handlers    handlers     r    MainHandler       Openshift uses WSGI to deploy the python applications  In the application file we will define     application   tornado wsgi WSGIApplication handlers    settings     application will be invoked by app py to run the application on wsgi  If CherryPy is not installed will be used wsgiref  single ways PotterScript    The one language that Harry always used  but never spoke of    About   PotterScript is an interpreted language  similar to Muggle world s  Python   which is believed to be developed by Harry Potter during his stay at Hogwarts School of Witchcraft and Wizardry    Installation   sh   git clone https   github com OrkoHunter PotterScript git   cd PotterScript    python3 setup py install   Make sure you use Python 3  Use  sudo  in case of permission issue    Usage   Start the interactive shell    sh   potterscript     Feature Requests   PotterScript is a fun toy project  If you re a Harry Potter fan  then please tell what would you expect from this    Link to submit feature request   Credits   This project is heavily inspired from  TrumpScript  and with good muggle intentions  GitHub Audio   Listen to music generated by events happening across GitHub   Imitation is the sincerest form of flattery      Oscar Wilde   But seriously   listen to wikipedia  by   slaporte  and   mahmoud  is the hotness    Use  ULTIMATE DREAM KILLER  to filter all events except closed PRs    Media Attention     The Next Web   Product Hunt     Installing dependencies   This application requires    node   npm   redis   Dependency Installation on OSX   bash   brew install nodejs   brew install npm   brew install redis   Dependency Installation on Linux   bash   sudo apt get update   sudo apt get install nodejs   sudo apt get install npm   sudo apt get install redis server   Running the Application   Install node packages   Navigate to the project directory and run   bash   npm install   Environment variables     export GITHUB OAUTH KEY  your github oauth key    Note  Without the GitHub oauth key the number of requests is throttled at 60 per hour  It can be increased to 5000 per hour by using an oauth key    Run Redis and Server   bash   redis server   In a separate window   bash   node server   Note  For production run  export NODE ENV  production   before starting the server  rasfeed github io   Website for rasfeed               Powered by  Electron   Muriel  is a cross platform app for watching a long series  TV  Documentaries  etc   in a random order such that no video is repeated until all the others have been watched already    Why would you use it    There are some TV series with over hundreds of episodes  After completing the whole thing once  and if we like it   we often watch one random episode anytime we feel like it  In this pseudo random process of ours  there is a high chance of recurrence of episodes in short duration  which ultimately makes us bored and we quit    So how does this work    Muriel keeps a shuffled order of episodes and doesn t let any episode repeat unless all the others have been watched the same number of times    Show me a cool GIF     How to use    Go to the  release page   download the one for your platform  Windows  Linux or Mac OS   Run  chmod  x Muriel  on the executable  Create a shortcut to the executable from where you want to use it    Confession 1   Sorry about the large release size  Electron packaging is not lightweight   Confession 2   Again sorry for no standalone executables or app  I couldn t figure out how to make one    Can I make changes and contribute    Yes please     sh   git clone https   github com OrkoHunter Muriel   cd Muriel   npm install   npm start   If you don t have npm  use  nvm  to install it  NQueens   sh   javac Greeter java Queens java   java Greeter     test pep8speaks   Testing the bot  pep8speaks Source Code   Source Code is a coding event in IIT Kharagpur s techno management fest Kshitij where you are given an executable file and have to guess the pattern and write the code for it    How to generate binaries    Use  pyinstaller   onefile question py Play Next   Play Next in YouTube   Chrome Extension     Have you ever felt lazy  Not talking about extremity but I don t like changing songs one after other when they finish playing  I repeat I don t like opening a new tab and then searching for a song after which I play it  only when  the current song ends playing   This chrome extension is a dark knight for people like us  Now you can add songs from whichever tab you want even when the current song is playing and like an obedient  queue  it will play each song one after other    Well words are deceiveing  lets see the extension in action     Features   You can add songs sequentially while remove songs from anywhere and would  heart  it always    Installation     Download the zip file from  here     Extract it    Type  chrome   extensions  in your Chrome Chromium browser    Tick the  Developer mode  and then click  Load unpacked extension     Select the directory where you had extracted the zip file  Done    Reset the  Developer mode  option to its initial value      Simplest Usage Example   Open a YouTube video  Something like this      Here you have songs suggestions in the right hand side  something like this      Click on the Play Next link to add it to queue      You can click on it again to remove it from queue      And on the upper right hand side  on clicking the extension icon you can see the queue      After the current song ends  the next song from queue will play  Something like this         Happy songs listening    Adding songs from another tab   Suppose you are listening to a song      And you want that some other song plays after this  so you open a new tab      And search for the song      You want to play this song      Just click on the  Play Next  link      After the current song ends in the initial tab      The song you have searched for will play      This is still in developmental phase  Feel free to contribute  Monitor URLS   Install and Start monitoring using  npm     npm install npm start       Important Routes defined in  index js        add  Send a POST request to the route with the body having a parameter  url   The response will contain the  id  associated with the url       responses  Send a POST request with  id  in the body  The response contains last 100 response times of the url     edit  Send a POST request with  id  and  url  in the body which Patches the existing URL in the database       stop  Send a POST request with  id  in the body and it removes the existing database records of the url associated with the id        Database scheme       Database name    monitor urls   Tabel 1    identity     id CHAR  128  PRIMARY KEY  url TEXT NOT NULL   Table 2    responses     id CHAR  128  PRIMARY KEY  delays TEXT cronjob   0 0 6 12 16 20       sh  home hunter workspace dctrends cronjob sh Machine Learning in Production   About the project   In Python  currently we have libraries like Theano  TensorFlow and PyTorch where you can write low level code for Deep Learning  then there are libraries like Keras  built on top of Theano and Tensorflow  and Lasagne  built on top of Theano  which gives us more ease to create a production ready model    We have tiny dnn in C   which has shown very promising speed up on CPU    But any how the user has to write code in one of the language and it hinders a person s reach to Deep Learning model  When nearly every Deep Learning Paper has a graphical representation of the model  and nearly every Deep Learning library creates a graph of the model before training  why can t we just let the user create a graph and run the model    But there is a compromise  the more high level abstraction you get to manipulating with the model becomes harder  but most of the deep learning model in production needs no manipulation as such    We propose to create an interface such that the user would be able to create a graph without writing code and then easily train it  For it we would create an extension over Keras library and using flask we would create a web interface    Why web interface    Because not only this makes the app platform independent  but we would be able to host it on a server and give the user flexibility to create a graph from all the possible platforms  and even from a phone    About the application   We would create abstraction over every  layer  defined in the Keras library  and firstly would like to target the sequential model of Keras   Note  Keras has two types of model  sequential and the functional model  We are targeting both the models but firstly the sequential model because it is easier to create a graph of sequential model  This is a rough breakdown of few of the layers  1  Input   User have to provide the dimension of the input specific to the data  2  Dense   Number of Units  Non linearity     3  Conv1D   4  Conv2D Similarly for others  see that we are providing enough flexibility to tweak with the layer parameters  We aim to cover every argument passed in the corresponding Keras API    Installation   Clone the repository  install the requirements using   pip install  r requirements txt   And then run the app using   python run py   Just when you thought  Python could not be more fun      1  Hello World      py         import  hello  Hello World  import  phello  Hello world              2  The classic               import this         The Zen of Python  by Tim Peters   Beautiful is better than ugly  Explicit is better than implicit  Simple is better than complex  Complex is better than complicated  Flat is better than nested  Sparse is better than dense  Readability counts  Special cases aren t special enough to break the rules  Although practicality beats purity  Errors should never pass silently  Unless explicitly silenced  In the face of ambiguity  refuse the temptation to guess  There should be one   and preferably only one   obvious way to do it  Although that way may not be obvious at first unless you re Dutch  Now is better than never  Although never is often better than  right  now  If the implementation is hard to explain  it s a bad idea  If the implementation is easy to explain  it may be a good idea  Namespaces are one honking great idea    let s do more of those        3  The missing line from the classic   The Zen of Python was introduced in  PEP 20   It is supposed to be 20 aphorisms  but only 19 of which have been written down    4  A simple life lesson      py         import this     love   this this is love True love is True False love is False False love is not True or False True love is not True or False  love is love    FML True             5  Comics  yeah       py         import antigravity             6  It s not a choice  it defines who we are      py         from  future  import braces   File      line 1 SyntaxError  not a chance             7  Origins   The name Python has nothing to do with the type of Snake    8  The confuscation   This is how the  this py  module looks  which prints the Zen of Python       py s      Gur Mra bs Clguba  ol Gvz Crgref   Ornhgvshy vf orggre guna htyl  Rkcyvpvg vf orggre guna vzcyvpvg  Fvzcyr vf orggre guna pbzcyrk  Pbzcyrk vf orggre guna pbzcyvpngrq  Syng vf orggre guna arfgrq  Fcnefr vf orggre guna qrafr  Ernqnovyvgl pbhagf  Fcrpvny pnfrf nera g fcrpvny rabhtu gb oernx gur ehyrf  Nygubhtu cenpgvpnyvgl orngf chevgl  Reebef fubhyq arire cnff fvyragyl  Hayrff rkcyvpvgyl fvyraprq  Va gur snpr bs nzovthvgl  ershfr gur grzcgngvba gb thrff  Gurer fubhyq or bar   naq cersrenoyl bayl bar   boivbhf jnl gb qb vg  Nygubhtu gung jnl znl abg or boivbhf ng svefg hayrff lbh er Qhgpu  Abj vf orggre guna arire  Nygubhtu arire vf bsgra orggre guna  evtug  abj  Vs gur vzcyrzragngvba vf uneq gb rkcynva  vg f n onq vqrn  Vs gur vzcyrzragngvba vf rnfl gb rkcynva  vg znl or n tbbq vqrn  Anzrfcnprf ner bar ubaxvat terng vqrn    yrg f qb zber bs gubfr       d      for c in  65  97       for i in range 26           d chr i c     chr  i 13    26   c    print    join  d get c  c  for c in s          The code for the Zen violates itself  It s not beautiful but ugly  not explicit but implicit  This would probably be the  only  module to go against the spirit of what it says itself      9  C C   anyone    From the Zen again   There should be one   and preferably only one   obvious way to do it    10  Naming identifiers can be unspeakably cool   Just when you thought that working in Python couldn t possibly  be  any more fun     py         from math import pi     pi area       r  2   r sum     knows Python   Python  in r sum  True             11  Picking a place for meetup       py         from antigravity import geohash   Your location  a date and that date s  or most recent  DJIA opening    geohash 37 421542   122 085589  b 2005 05 26 10458 68   37 857713  122 544543     This can generate a GPS coordinate in a region which is 1 longitude long and 1 latitude wide based on your location          12  The FLUFL   Friendly Language Uncle For Life from  PEP 401    BDFL Retirement      py         from  future  import barry as FLUFL 1    2 True 1    2   File      line 1     1    2          SyntaxError  invalid syntax     Recognized that the    inequality operator in Python 3 0 was a horrible  finger pain inducing mistake  the FLUFL reinstates the    diamond operator as the sole spelling          13  InPynite       py         infinity   float  infinity   hash infinity  314159 hash float   inf     314159     A hash is a fixed sized integer that identifies a particular value  On a closer look  the hash of infinity is 10 5 x pi  Interestingly  hash float   inf    yields  10 5 x pi in python3  whereas  271828 i e   10 5 x e in python2          14  types CodeType    Not for the faint of heart   If you start digging deep into Python s internals you will get a warning in  help  output for  types CodeType     py         import types help types CodeType      Help on class code in module builtins                                                              class code object                                                                             code argcount  kwonlyargcount  nlocals  stacksize  flags  codestring                            constants  names  varnames  filename  name  firstlineno                                   lnotab   freevars   cellvars                                                                                                                                                  Create a code object   Not for the faint of heart                                                                                                                                   Methods defined here                                                                                                                                                                                                                       15  Python 3 9 PEG parser     peg parser    is a keyword in python 3 9  will throw a syntax error if used    py         peg parser    File      line 1      peg parser        SyntaxError  You found it              Notes     Easiest hello world program in a language without calling any function   Each and every line is the philosophy of Python s design and is a supreme holy guide   Maybe just to show that there always should be a new line at the end of a file    Not an easter egg  a joke in the interpreter   It opens this  xkcd comic  which demonstrates how easy it is to do stuff with modules   This is to instantly close down any conversation about introducing curly braces to Python   Guido van Rossum is a big fan of  Monty Python s Flying Circus   It s a substitution cipher called  ROT13   In many languages there are two ways to do the same thing    no  and  no     The message has a hidden example in itself   Support for unicode character set for naming identifiers was added in Python3  Though  it is not explicitly preferred while writing code  it adds flavour to working with scientific formulas   The original code is  here  with the  xkcd comic  referenced  and maybe that s why this is also in the  antigravity  module   The  PEP 401  is an April Fools  Joke   The PEP s number is 401  i e  4 01 or April 1st  April Fools  Day   The PEP states that Guido van Rossum is stepping down  The new title given to him would be pronounced  BDEVIL   Benevolent Dictator Emeritus Vacationing Indefinitely from the Language  and Guido s successor will be Barry Warsaw  or as he is affectionately known  Uncle Barry  Uncle Barry s official title is  FLUFL   Friendly Language Uncle For Life   There are in jokes about the Parrot virtual machine and the  non existent  Python Secret Underground  possibly a throw back to   TINC  on USENET      Source     See  this answer     Add more   Please feel free to create a PR and add more  KOSS Python classes   Introduction to Programming with Python  and basic Open Source Software Development principles    Installation and Environment setup   It is highly recommended for beginners to install Anaconda  If you don t want to use it  please have  jupyter notebook  installed  Python 3       Download and Install Anaconda  Installation page   https   www continuum io downloads    Windows   https   repo continuum io archive Anaconda3 4 3 1 Windows x86 64 exe   Linux  Ubuntu    https   repo continuum io archive Anaconda3 4 3 1 Linux x86 64 sh   Run this script using  bash   See the  installation page       macOS  https   repo continuum io archive Anaconda3 4 3 1 MacOSX x86 64 pkg     If you do not have notebook or python setup in your compute 1  Go to https   notebooks azure com 1  Sign in  create a library 2  Create a new notebook in the library 3  In language  choose Python 3 6 4  Start the notebook    Target students   The initial classes are aimed for beginners  people who do not have any programming experience as well as people who have basic knowledge of C C    If you already have tried Python before  the first few classes are not good for you  But you can come and help tutor                 What     GitHub Monitor  is a minimalistic react native app which uses GitHub API to show the activities of a user    Why     I created it for two of my personal reasons       I have a little bot   pep8speaks  which I would love to keep track of on my phone    I wanted to make a working android ios app using react native      How     Insert my medium article here   Table of Contents     Installation   Available Scripts   npm start   npm test   npm run ios   npm run android   npm run eject     Installation   If you trust me enough  install it using the APK I ve genereated for the Android people   https   exp shell app assets s3 us west 1 amazonaws com android 2F 40himanshumishra 2Fgithub monitor 14d9387f 5f68 11e7 a73f 0a580a781a13 signed apk   If you do not  or you re crazy to be using an IPhone  this is for you        Fork  Clone this repository  run  npm install   Btw  there s something called  pnpm  in town  check it out    Install the expo app on your phone   Start the packager with  npm start   Read whatever gets printed on the console  and follow of course    Once you are done with scanning  and maybe resizing your terminal   shake your phone to get a developer options menu  Click on  Debug JS Remotely  and a browser tab will open on your computer    Check out the console of the tab and debug away       Available Scripts   npm start   Runs your app in development mode    Open it in the  Expo app  on your phone to view it    npm run ios   Like  npm start   but also attempts to open your app in the iOS Simulator if you re on a Mac and have it installed    npm run android   Like  npm start   but also attempts to open your app on a connected Android device or emulator  Requires an installation of Android build tools  see  React Native docs  for detailed setup     npm run eject   This will start the process of  ejecting  from Create React Native App s build scripts  You ll be asked a couple of questions about how you d like to build your project    Warning   Running eject is a permanent action  aside from whatever version control system you use   An ejected app will require you to have an  Xcode and or Android Studio environment  set up    Contribute   I suck at Javascript  And this has to be my first phone app  Please report issues or suggest new features  no matter how small  and I swear by you  I ll try to make it   Or convince someone else to do it instead       tutorials day 1 2017 07 10   tutorials day 2 2017 07 11   keynotes   talks   2017 07 12   2017 07 13   2017 07 14       lightning talks     scipy 2017 notes   Links and notes for SciPy 2017    What to know what happed last year      https   github com chendaniely scipy 2016 notes     Please help mainain the list  send a PR    Lists of tutorials  https   scipy2017 scipy org ehome 220975 493418    Proceedings  http   conference scipy org proceedings scipy2017    Videos of talks and tutorials  https   www youtube com playlist list PLYx7XA2nY5GfdAFycPLBdUDOUtdQIVoMf   BoFs     jupyter widget bof  https   gist github com mwcraig 176560759e889a3635cbaaa1fdef8bbc     Tutorials Day 1 2017 07 10     Tutorial Repository                                                                   Video                                                                                                                                                                                                                                                                                                    Software Carpentry Scientific Python Course Part 1  Beginner                      link        Cython for Data  Scientists  and Data Scientists  Intermediate Advanced            link        Numba  Tell Those C   Bullies to Get Lost  Intermediate                            link        Automatic Code Generation with SymPy  Advanced                                     link        Modern Optimization Methods in Python  Intermediate Advanced                           Software Carpentry Scientific Python Course Part 2  Beginner                                 Computational Statistics  Beginner                                                link        The Jupyter Interactive Widget Ecosystem  Intermediate Advanced                    link        HDF5 take 2  h5py   PyTables  Intermediate Advanced                                link        Interactive Data Visualization with HoloViews   Bokeh  Advanced                         Tutorials Day 2 2017 07 11     Tutorial Repository                                                                            Video                                                                                                                                                                                                                                                                                                             Introduction to Numerical Computing with NumPy  Beginner                                   link     Pandas for Data Analysis  Beginner                                                          link     Machine Learning with scikit learn Part One  Intermediate                                   link     Parallelizing Scientific Python with Dask  Intermediate                                    link     Parallel Data Analysis in Python  Intermediate                                             link     Anatomy of Matplotlib  Beginner                                                            link     scikit image  Image Processing for Python  Intermediate                                    link     Network Science and Statistics  Fundamentals and Applications  Intermediate                link     Machine Learning with scikit learn Part Two  Intermediate                                   link     Signal Processing and Communications Hands On Using scikit dsp comm  Intermediate          link   Keynotes     https   github com katyhuff 2017 07 12 scipy     https   www slideshare net GaelVaroquaux presentations       Talks   2017 07 12     Python   Tableau  Building an Interactive and Beautiful Data Visualization with TabPy   Chloe Tseng   https   github com tableau TabPy       Dataflow Notebooks  Encoding and Using Cell Dependencies   David Koop   https   github com dataflownb dfkernel   Working with notebook cells   iterermediate results   out  dictionary       global variables   Out  13         Persistent cell IDs       bqplot   Seamless Interactive Visualizations in the Jupyter Notebook   Dhruv Madeka   https   github com dmadeka SciPy2017       nbgrader  A Tool for Creating and Grading Assignments in the Jupyter Notebook   https   github com jhamrick nbgrader       Berryconda   Scientific Python on the Raspberry Pi   https   github com jjhelmus berryconda       MatchPy  A Pattern Matching Library   Manuel Krebber  RWTH Aachen University   https   github com HPAC matchpy   slides  http   slides com wheerd matchpy         MNE to see the brain at a millisecond time scale   Alex Gramfort   https   github com agramfort mne scipy2017   https   www slideshare net agramfort mne scipy conference 2017         2017 07 13     Composable Multiprocessing and Multithreading for Numeric Libraries   Anton Malakhov  Anton Gorshkov  Terry Wilmarth   https   github com IntelPython smp       Dash   A New Framework for Buildin User Interfaces for Technical Computing   Chris Parmer   It s like Shiny for Python    https   github com plotly dash   https   plot ly products dash    http   flask pocoo org    https   facebook github io react     widgets  are react components   https   plot ly dash plugins       Learn react  https   academy plot ly        Diversity and Inclusion at the SciPy Conference   Julie Hollek   https   github com jkru scipy diversity   https   www scientificamerican com article how diversity makes us smarter    http   www laurenbacon com how likely is an all male speakers list statistically        Make it Work  Make it Right  Make it Fast   Debugging and Profiling in Dask   James Crist   http   jcrist github io talks profile and debug dask slides html     https   github com jcrist talks tree master profile and debug dask   https   github com dask dask glm       Introducing JOSS  The Journal of Open Source Software   http   joss theoj org        Scientific MicroPython on Microcontrollers and IoT   Roberto Colistete Jr   https   github com micropython micropython       Creating Reproducible Experiments with ReproZip   Vicky Steeves   VickySteeves    R mi Rampin   remram44    https   reprozip org   https   vickysteeves gitlab io 2017 SciPy        Efficient Array Computing in C   with xtensor and Apache Arrow   Johan Mabille   Sylvian Corlau   https   github com QuantStack xtensor   http   xtensor readthedocs io en latest numpy html         2017 07 14     Dask  Advanced Techniques   Matthew Rocklin   http   matthewrocklin com slides scipy 2017 html     https   www youtube com watch v ZxNPVTHQwGo   http   matthewrocklin com slides scipy 2017 html         Terabytes at Your Fingertips  Interactive Big Data Coding with PySpark   Sam Penrose   https   github com SamPenrose decision tree coding   www sampenrose net       Scientific Analysis at Scale  a Comparison of Five Systems   Jake VanderPlas   https   speakerdeck com jakevdp image analysis at scale  Comparative Evaluation of Big Data Systems on Scientific Image Analytics Workloads   https   arxiv org pdf 1612 02485 pdf           A Python API for Earth   Sam Skillman   http   descartes labs python readthedocs io en latest    https   www descarteslabs com beta application       NEXT  Machine Learning  Crowdsourcing  and Cartoons   Scott Sievert   https   speakerdeck com stsievert scipy 2017 next         ChiantiPy     https   github com wtbarnes chiantipy talk scipy 2017         python packaging     https   github com python packaging tutorial python packaging tutorial         Scientific Programming with the SciPy Stack     https   github com scw scipy devsummit 2017 talk         Dashboards Visualizing Hundreds of Millions of Datapoints in 30 Lines of Python     https   github com philippjfr scipy talk 2017         Lightning talks     modeling simulation in python  http   greenteapress com wp modsimpy    screenlamp  toolkit for facilitating hypothesis driven  ligand based screening of large molecule libraries   https   github com psa lab screenlamp   biopandas  https   github com rasbt biopandas   figurefirst  http   flyranch github io figurefirst    synthpy  synthetic data in python   https   github com pambot synthpy   PlasmaPy  https   github com PlasmaPy PlasmaPy   PyART  The Python ARM Radar Toolkit  Atmospheric Radiation Measurement  ARM    http   arm doe github io pyart    Computational Experiments Survey  http   survey npimentel net en    ipython unittest  https   www google com search q ipython unit test ie utf 8 oe utf 8   burrito data  http   bit ly burritodata   burrito review  https   docs google com forms d e 1FAIpQLSdWAkbSKzHydtzJ AKHp rXKHeG80cZuUBjyEeMAUZXJsiFKQ viewform     burrito  https   github com srcole burritos       elegant scipy  https   github com elegant scipy elegant scipy     givaway  https   github com elegant scipy elegant scipy issues 308   picking the 25 people script gist  https   gist github com stefanv 87c866f3bda2a1f1f2d23380cdf4dd9f   html build copy  https   www dropbox com s cw49dk1gj6r389i elegant scipy 2017 07 13 15 18 54 zip dl 0       cykdtree  http   cykdtree readthedocs io en latest    cgal4py  http   cgal4py readthedocs io en latest    hi toby  https   github com tobsecret   bokeh genome explorer  https   github com tobsecret Bokeh genome explorer   tex2ipy  beamer to jupyter notebooks   https   github com prabhuramachandran tex2ipy   tex2ipy talk tex talk ipynb       ipyaml  https   github com prabhuramachandran ipyaml   https   github com aaren notedown   https   github com rossant ipymd       reproducible self publishing  pythontex  https   github com gpoore pythontex   vpython  navigable 3D displays and animations   http   vpython org    Matt Craig   MSUM Physics and Astronomy   http   www glowscript org    https   github com BruceSherwood vpython jupyter       Jupyter Lab  https   github com jupyterlab jupyterlab   Jason Grout  http   jasongrout org    clap clap       Data Package manaer  data retriever   http   www data retriever org    https   github com weecology retriever       cpDetect  Bayesian change point detection   https   github com choderalab cpdetect   BACnet  http   www bacnet org    https   github com BuildingRobotics pybacnet   http   bacpypes sourceforge net        script all the things  https   goo gl AqQd9v   datalore   https   www jetbrains com products html   http   www surveygizmo com s3 2911800 DataLore Private Beta Sign up       opsn source sustainability    https   www numfocus org    bit ly nf sustainability       pep8speaks   https   pep8speaks com    https   github com OrkoHunter pep8speaks   http   tinryurl com pythonjokes       clover health   https   www cloverhealth com en    Associate Software Engineer Program   https   www cloverhealth com en about us careers   We miss you Matt  https   twitter com jiffyclub       RDP   https   rdp cme msu edu    https   github com bio miga miga   http   enve omics ce gatech edu 3000        ipython 6 0 6 1   python 3 only   upgrade  pip  and  setuptools   ending py2 compatibility  https   www youtube com watch v 2DkfPzWWC2Q   https   github com Carreau pycon2017       gormless    dontusethiscode  https   twitter com dontusethiscode   domain modelling       firefly   https   github com Caltech IPAC firefly   stargazer    ipywidgets readthedocs io   firefly widgets  https   github com Caltech IPAC firefly widgets   cookiecutter templates for jupyter widgets       Pwang   mission accomplished   open source won    2010 talk  https   www youtube com watch v fK6E9tq KjM feature youtu be       Pump up the Jams   https   gist github com bmcfee ab75ca0f10d0a01d4fe67035d1e65990       Bus Factor  https   github com dpshelio busfactor   Hack your inbox  https   docs google com presentation d 1EaoHUxg3VNDqDq6LBD8ayawUVGVwMKkJMn 7SSH5ujk edit slide id p   dc audio   Listen to the melody created by your DC network   Run the server outside of the local network  Use  brahma py  from your local computer to stream data  Read https   orkohunter net blog peeking over dc search  Reading emails on slack   If your team works on a Slack workspace  and the team has to maintain a bunch of different email ids and wish to be notified of every email instantly on slack  there is a hack for you    How does it work    After setting up your Slackbot chat as a forward emailing address of your email account   Steps expalined below   this bot watches for messages in the Slackbot chat and whenever a message arrives with an attached email  It forwards it to a channel as a message    Installation     You can create a account for just emails as the emails appear in the DM of the user that sets up the app  If you want to avoid two notifications  one from the slackbot and one from this app  it is advised to use a secondary account to setup this app      Slack mail generation     Generate the email for forwarding to slack from   Preferences    Messages and Media    Bring emails into Slack      ex email    something   workspace  slack com   Setup forwarding in gmail     Gmail sends a email to slack with the link to confirm  This email shows up in the slackbot  Click on the link and accept forwarding    Send a sample mail to account to test whether it gets forwarded correctly  This email should show up in the slackbot dm      Creating the Slack App     Go to api slack com apps and create a new Slack app    Save the following details      App ID   Verification token   In   Sidebar    OAuth and Permissions   give the permissions for    files read    im history  in User Token Scopes   and  incoming webhook  in Bot Token Scopes   Then Install the app from   Sidebar    OAuth and Permissions       Deploying the server     Here we will deploy on heroku for simplicity  This can be deployed to any other hosting provider or the app can be self hosted if you choose to do so  Do note that though deployment on heroku is optional  deployment itself is not  If you have deployed the app elsewhere  please set the environment variables accordingly      For heroku ensure  You have the  heroku cli  installed  If you haven t logged in   heroku login   Then in the root of the repo    sh   heroku create   creates a new heroku app   git push heroku master main   deploys the master branch     Aside   One quick way to test that the deployment was semi successful is to visit the deployed URL and it should redirect to the github s repo  The app processes the POST requests and all GET requests are forwarded to the repo s page      Now time to configure the environment variables on heroku      Config Variable          Description                                                                                                                                                         APP ID                  You get this when you create the app                                  INCOMING WEBHOOK URL    You get this when you install the app on one channel                  TEAM ID                 ID of your slack workspace                                            USLACKBOT CHANNEL       The ID of the direct messaging channel between you and  slackbot      VERIFICATION TOKEN      You get this when you create the app                                 Note that the Slack IDs  for channel  users  files  are alphanumeric uppercase string of 9 characters  In browser  if you have opened the chat with the slackbot the URL will be of the format    https   app slack com client  TEAM ID   USLACKBOT CHANNEL     For more details on finding team and channel ID  see this question on stackoverflow     You can set these variables either on the Heroku Dashboard or via cli      heroku config set APP ID xxxxxxxx   Do note the link to which server is deployed  we will set it up in slack settings in a moment    Setup of Event Subscription     Navigate to   Sidebar    Event Subscription   and turn on    Put the link to the deployed server in  Request URL     Subscribe to  message im  in  Subscribe to events on behalf of users     Voil  the setup is complete    Notes      Slack creates a beautiful file for an email  But I could not find way to change file permissions which is shared privately with just the user  Hence  I have to customize and post the email  If there is a way to change file permissions from private to be shared in a team  that would be easy and the best way      Alternative   A PHP version of  email to slack by Mehdi Chaouch   Troubleshoot   The deployment doesn t work          Check if the permissions required are present        Check heroku logs        Check if the server deployment is live by sending a GET request     If these steps don t work please file an  Issue   blockchain hackathon unnati   We want to create an offline first mobile application which caters to the needs of rural farmers for soil testing  Our app will use GPS data to gather the location info and based on a learning model  it will predict the amount of fertilizer required for a crop  Furthermore  the app will be backed by an announcement system and bi directional online help desk  This app will adapt to the default language of the phone and support multiple regional languages like Hindi and Bengali  Even if it will use complex algorithms for recommending fertilizers  it will be quite lightweight so as to install on a low end phone  We will also avoid complicating the app so that user need not be familiar with the app or many features of the phone  We are highly conscious about the phone resources and will try to deliver the best results at minimum cost  VectorEntry   A Text Entry Mechanism for visually impaired   Installation   Go to https   github com OrkoHunter VectorEntry Keyboard releases and download the latest apk file  Install it in your android phone    Usage   All the numbers and the characters are divided into 4x3   12 groups  based on the T 9 mobile keypad layout    VectorEntry is a two step procedure to input one character    Step 1  Group Selection     Alphabets     Group    Flick direction type                                                                               Key 1    bottom to left                  a b c        Key 2    up                              d e f        Key 3    bottom to right                 g h i        Key 4    left                            j k l        Key 5    double tap                      m n o        Key 6    right                           p q r s      Key 7    top to left                     t u v        Key 8    down                            w x y z      Key 9    top to right                   Space         NA       right       and down           Backspace     NA       left       and down                          Key      Two Finger Tap and Left                      Key 0    Two Finger Tap and Up            Quit        Key      Two Finger Tap and Right       Select the group by the gesture associated with it  If you want to enter a number  select the group of that Key    Step 2  Select a character from the group   The next gesture determines which character to choose from the selected group  The directions have the following meaning  clockwise directions          Left flick   First   Top flick   Second   Right flick   Third   Bottom flick   Fourth       A group can have maximum 4 possible characters  For example  the group of Key 9 has  1   w    2   x    3   y  and  4   z  possibilities  To select  y   we will do the bottom flick    If no second move is made within five seconds  then the number of that key is inserted  For example  if a person does  up  flick and leaves for five seconds  the number  2  is entered    Example   To select  q   we first select the group of Key 7 by doing a bottom left flick  We then do an  up  flick because  q  is the second character in the group    jio hackathon backend   Backend for jio hackathon app   farmer s marketplace   Presentation Link   https   docs google com presentation d 13zwOn1wfzagIo2RLTcZ4Vlq9O1ymgyvH9NhfJJtLiWo edit usp sharing   Media Link   https   fu defcon007 com interiit  Hey  I m  OrkoHunter         Twitter         LinkedIn       Not interested in displaying the profile views         Latest blog posts     Your product either lives long enough to become a platform or it dies out   Aldous Huxley s world of perceptions   Why should one do Open Source in college    Live Asynchronously   Joining Spotify in Sweden   PEP8Speaks   Now helping 5 000 open source projects write neat and clean Python     You can read more on  orkohunter net blog     Other links     The OrkoHunter Podcast  on Youtube and Spotify            Emacs Prelude   Prelude is an Emacs distribution that aims to enhance the default Emacs experience   Prelude alters a lot of the default settings  bundles a plethora of additional packages and adds its own core library to the mix  The final product offers an easy to use Emacs configuration for Emacs newcomers and lots of additional power for Emacs power users    Prelude is compatible  ONLY with GNU Emacs 25 1    In general you re advised to always run Prelude with the latest Emacs   currently  26 1     You can support the development of Prelude via  PayPal    Salt    Patreon  and  Liberapay         Fast Forward   Assuming you re using an Unix like OS    BSD    GNU Linux    macOS    Solaris   etc   you already have Emacs 24 4  installed  as well as  git     curl  you can skip the whole manual and just type in your favorite shell the following command    bash curl  L https   git io epre   sh   You can now power up your Emacs  sit back and enjoy Prelude  forgetting about the rest of this manual    There are two environment variables you can use to control the source repository and the installation directory  To change the installation directory    bash export PRELUDE INSTALL DIR   HOME  emacs d     curl  L https   github com bbatsov prelude raw master utils installer sh   sh   To change the source repository    bash export PRELUDE URL  https   github com yourname prelude git     curl  L https   github com bbatsov prelude raw master utils installer sh   sh   Note that the installer will back up any existing   emacs  file or   emacs d  since it will unpack Prelude s code in   emacs d   If you re doing a manual install make sure you don t have a   emacs  file or back up your existing   emacs d  directory manually    Don t forget to adjust your  prelude modules el  file in your personal directory once the installation is done  By default most of the modules that ship with Prelude are not loaded    User Manual   Check out our  user manual  for more information    Known issues   Check out the project s  issue list  a list of unresolved issues  By the way   feel free to fix any of them and send me a pull request        Support   Support is available via several channels      Prelude s Google Group  emacs prelude googlegroups com   Prelude s Freenode channel    prelude emacs     Gitter     Contributors   Here s a  list  of all the people who have contributed to the development of Emacs Prelude    Bugs   Improvements   Bug reports and suggestions for improvements are always welcome  GitHub pull requests are even better        Cheers    Bozhidar vectorentry backend   Backend for the VectorEntry keyboard with features like correction and prediction   Hosted   https   vectorentry backend herokuapp com    How to run    Install dependencies   shell pip3 install  r requirements txt   and download nltk corpora      python         import nltk nltk download  wordnet               run the app      shell python3 app py   Open the URL in browser to interact with the server  Website for PEP 8 Speaks   Website  https   pep8speaks com   Project  https   github com OrkoHunter PEP8Speaks   Built with amazing  Launchaco   3 notify on broken links   A GitHub app that keeps watching your repositories for broken links and creates an issue if any are found   What is it      Install this GitHub app on your repositories and then forget about it    Configure what you want to be scanned   All of the source code in the repository   Maybe the pages on GitHub wiki as well   Use glob patterns for excluding some directories or files   The app will daily look for all the URLs listed in the repository   It will create an issue on your repository with the links and their status code   If an issue is already open  the app would not open another open  It will modify the existing one instead    Additional configurations of ignoring some specific domains or links can be added    The behavior logic of the app can also be configured      Development   The project will be a wrapper for a CLI tool designed to do the same job  Currently  this project is not under development  Created using  reveal js   Checked out to  tags 4 1 0   Use  live server  for hot reload  Getting Started with Create React App   This project was bootstrapped with  Create React App     Available Scripts   In the project directory  you can run    npm start   Runs the app in the development mode   Open  http   localhost 3000  to view it in your browser    The page will reload when you make changes   You may also see any lint errors in the console    npm test   Launches the test runner in the interactive watch mode   See the section about  running tests  for more information    npm run build   Builds the app for production to the  build  folder   It correctly bundles React in production mode and optimizes the build for the best performance    The build is minified and the filenames include the hashes   Your app is ready to be deployed    See the section about  deployment  for more information    npm run eject   Note  this is a one way operation  Once you  eject   you can t go back    If you aren t satisfied with the build tool and configuration choices  you can  eject  at any time  This command will remove the single build dependency from your project    Instead  it will copy all the configuration files and the transitive dependencies  webpack  Babel  ESLint  etc  right into your project so you have full control over them  All of the commands except  eject  will still work  but they will point to the copied scripts so you can tweak them  At this point you re on your own    You don t have to ever use  eject   The curated feature set is suitable for small and middle deployments  and you shouldn t feel obligated to use this feature  However we understand that this tool wouldn t be useful if you couldn t customize it when you are ready for it    Learn More   You can learn more in the  Create React App documentation     To learn React  check out the  React documentation     Code Splitting   This section has moved here   https   facebook github io create react app docs code splitting   Analyzing the Bundle Size   This section has moved here   https   facebook github io create react app docs analyzing the bundle size   Making a Progressive Web App   This section has moved here   https   facebook github io create react app docs making a progressive web app   Advanced Configuration   This section has moved here   https   facebook github io create react app docs advanced configuration   Deployment   This section has moved here   https   facebook github io create react app docs deployment   npm run build  fails to minify   This section has moved here   https   facebook github io create react app docs troubleshooting npm run build fails to minify BrowserStack 101  Tutorial    Learn how to use BrowserStack to run your selenium tests on multiple real devices and cross browser environments   Expected time  30 mins          Overview   BrowserStack  is a service to run tests for mobile and web apps in cloud  It provides access to several  real  mobile devices  cross platform and multiple browser environments  Essentially  it declutters the desk of a QA engineer   so that they can focus on writing their tests  and not carry around dozens of devices    In this tutorial  we are going to write a simple automation test using  Selenium in Node js   Then with some additional lines of code  we ll be able to run it on two mobile devices and three browsers using BrowserStack  We will use the  BrowserStack demo website  for the purpose of this tutorial to write some test workflows e g  login  add to cart and check offers    Pre requisites   1  Node js   Make sure you have Node js  and npm  installed   preferably the LTS version  If not  use  nvm  to install it  Run the following in your command prompt      bash curl  o  https   raw githubusercontent com nvm sh nvm v0 39 1 install sh   bash   nvm install   lts nvm use   lts       Check the  nvm docs  for more detailed instruction    2  Selenium Webdriver   Create a new empty directory for our test project  Now let s install the  selenium webdriver  package  which is our primary dependency for running our tests in a browser of our configuration    bash npm install selenium webdriver   3  Chrome Firefox webdriver   While writing the tests from scract  we would want to have a local selenium setup  This will save us a lot of time in debugging and writing the tests faster  Once  we are able to run our tests in one particular browser environment  we will then use the power of BrowserStack and run it on three browsers   Chrome  Firefox and Safari and two mobile devices   iPhone  iOS  and OnePlus  android     For the purpose of this tutorial  we will use chromedriver  Download the latest release of chromdriver from their  releases page   Choose the particular executable per your operating system  Linux  macOS  Windows  which you ll be using for this tutorial  and move it to a location in your  PATH   An example of using Chrome on Linux     bash wget https   chromedriver storage googleapis com 103 0 5060 53 chromedriver linux64 zip unzip chromedriver linux64 zip sudo mv chromedriver  usr local bin   We also need to make sure we have a Chrome browser installed  You can download Chrome directly from  their website   If you are on Linux  you can also install it with apt get    bash wget https   dl google com linux direct google chrome stable current amd64 deb sudo apt  y install   google chrome stable current amd64 deb   Note  Make sure that the major version of both the browser and chromedriver matches  or else you will get an error when creating the driver    If you want to use Firefox  you can download the latest files from  geckodriver  releases page     4  BrowserStack account and API keys   Head over to  BrowserStack Sign Up  page and create a new account for free  Once you are logged in  head over to your  profile page  and note down your  Username  and  Access Key   We will need them once we are about to run our tests on BrowserStack    That s all   let us begin      Tutorial   Step 1  Think about a test workflow  Sign In    Head over to  bstackdemo com  which is the demo website we will use for the purpose of this tutorial  There we can find a Sign in button on the top right corner      Click on it to open the login page  There is a list of demo usernames and a demo password which can be used to sign in to the website  Let s pick the username  demouser  and password  testingisfun99  for our purpose of this tutorial      Upon hitting login  we are now logged in to the site  We can verify this by finding our username  demouser  on the top right corner of the page  And the Sign In button has now been replaced with Logout      That s our first test  Let s write this workflow in Selenium    Step 2  Writing the test workflow in Selenium   Create a new directory and run  npm install selenium webdriver   If you have already run this  you should have the  node modules  directory   package json  and  package lock json     Create a new file called  testLogin js  with the following javascript code       js    File  testLogin js    Selenium test without using BrowserStack const webdriver   require  selenium webdriver      async function runTest          Initialize a webdriver to use a local browser instance   let driver   new webdriver Builder      forBrowser  chrome      build         Click the Sign In Button   await driver get  https   bstackdemo com      console log  Opening Sign In Page     const signInButton   await driver findElement webdriver By id  signin       await signInButton click        Explicit wait until the username input field loads  with a timeout of 100 seconds   await driver wait webdriver until elementLocated webdriver By css   username input     100        Enter username and password   const usernameField   await driver findElement webdriver By css   username input       await usernameField sendKeys  demouser   webdriver Key ENTER     const passwordField   await driver findElement webdriver By css   password input       await passwordField sendKeys  testingisfun99   webdriver Key ENTER     const submitButton   await driver findElement webdriver By id  login btn       await submitButton click        Verify logged in username   await driver wait webdriver until elementLocated webdriver By className  username     100     const usernameDisplay   await driver findElement webdriver By className  username      if  await usernameDisplay getText       demouser         console log  Successfully logged in        else       console log  Something went wrong  Log in failed           await driver quit      return      runTest          This might be a few too many lines of code  but essentially follows the steps we planned in step 1 and checks for a successful login  As of now  we have not used BrowserStack at all and are purely using selenium to run the test locally  Execute the test using   bash node testLogin js   Hopefully  you should see an selenium controlled browser window open up  it ll automatically follow the log in steps and close successfully with positive log messages in the command prompt    Congratulations  You have now written a successful selenium based test  Now let us use BrowserStack to run the test on different browsers and different operating systems    Step 3  Configure BrowserStack credentials   Head over to your  BrowserStack profile page  and note down your  Username  and  Access Key   Now  set them as environment variables  which is more secure than writing them in code  You can use the command prompt for this    bash export BROWSERSTACK USERNAME  my username   export BROWSERSTACK ACCESS KEY  my access key     Step 4  Using BrowserStack   Let us now tweak our code a bit to use BrowserStack for running our tests  instead of our local browser instance  Replace the entire  testLogin js  file with the following code      js    File  testLogin js    Selenium test using BrowserStack const webdriver   require  selenium webdriver      const browserstackServerUrl    http     process env  BROWSERSTACK USERNAME      process env  BROWSERSTACK ACCESS KEY    hub cloud browserstack com wd hub     async function runTest capabilities         Initialize a webdriver to use a local browser instance   let driver   new webdriver Builder      usingServer browserstackServerUrl     withCapabilities          capabilities         capabilities  browser        browserName  capabilities  browser        Because NodeJS language binding requires browserName to be defined         build         Click the Sign In Button   await driver get  https   bstackdemo com      console log  Opening Sign In Page     const signInButton   await driver findElement webdriver By id  signin       await signInButton click        Explicit wait until the username input field loads  with a timeout of 100 seconds   await driver wait webdriver until elementLocated webdriver By css   username input     100        Enter username and password   const usernameField   await driver findElement webdriver By css   username input       await usernameField sendKeys  demouser   webdriver Key ENTER     const passwordField   await driver findElement webdriver By css   password input       await passwordField sendKeys  testingisfun99   webdriver Key ENTER     const submitButton   await driver findElement webdriver By id  login btn       await submitButton click        Verify logged in username   await driver wait webdriver until elementLocated webdriver By className  username     100     const usernameDisplay   await driver findElement webdriver By className  username      if  await usernameDisplay getText       demouser         console log  Successfully logged in        await driver executeScript         browserstack executor    action    setSessionStatus    arguments     status   passed   reason    Successfully logged in                 else       console log  Something went wrong  Log in failed         await driver executeScript         browserstack executor    action    setSessionStatus    arguments     status   failed   reason    Something went wrong  could not log in                   await driver quit      return      const baseCapabilities        realMobile    true      build    Browserstack demo site       const capabilities1          baseCapabilities     name    Test 1      device    iPhone 13 Pro Max      osVersion    15      browserName    Safari      const capabilities2          baseCapabilities     name    Test 2      device    iPhone 13 Pro Max      osVersion    15      browserName    Chrome       const capabilities3          baseCapabilities     name    Test 3      device    OnePlus 9      osVersion    11 0      browserName    Chrome       const capabilities4          baseCapabilities     name    Test 4      device    OnePlus 9      osVersion    11 0      browserName    Firefox       runTest capabilities1   runTest capabilities2   runTest capabilities3   runTest capabilities4         Step 5  Exploring the build on BrowserStack   We ll deep dive into the code in a bit  but let s first run it and see it in action  Execute the script using   bash node testLogin js   Open your  BrowserStack automate dashboard   On the left bar  find  All Builds  and click on the latest build  Shortly  if everything worked out well  you should see all 4 tests passed on different browsers and devices      Click on one of the test  and open the details page  It will look something like this      There are four main sections on this page  On the top most  you can find some basic details of the test e g  Device used  user and the test result  Below it  you can see a video recording of the entire test in action  on a real mobile device  pretty amazing  right    Just below the recording  you can find the input capabilities we provided for the test  along with a long list of device capabilities    Note  Capabilities are basically settings or options to configure the test    On the right hand side  you can find the logs and steps in detail  as well as other information like network logs   which are super useful for debugging    Step 6  Understanding BrowserStack related changes   Congratulations  You have now run a multi device multi browser automated test on BrowserStack  Now let s look at the changes in the Selenium code we had to make for it to happen  There are mainly three changes to look at    Change 1  Use BrowserStack remote server for the webdriver      diff       const browserstackServerUrl    http     process env  BROWSERSTACK USERNAME      process env  BROWSERSTACK ACCESS KEY    hub cloud browserstack com wd hub         async function runTest         async function runTest capabilities        let driver   new webdriver Builder        forBrowser  chrome        usingServer browserstackServerUrl       withCapabilities            capabilities           capabilities  browser        browserName  capabilities  browser        Because NodeJS language binding requires browserName to be defined             build          We modified the  runTest  function to accept an argument called  capabilities  here  We will provide options like device name  browser name  etc  using  capabilities   We have defined a  browserstackServerUrl  using the BrowserStack username and Access Key set in the environment variable  We removed the local browser setting and added a  usingServer  call to use the BrowserStack remote server  We have also added a  withCapabilities  call to include the options we will provide for the test    Change 2  Set the test session result   diff     if  await usernameDisplay getText       demouser           console log  Successfully logged in          await driver executeScript           browserstack executor    action    setSessionStatus    arguments     status   passed   reason    Successfully logged in                     else         console log  Something went wrong  Log in failed           await driver executeScript           browserstack executor    action    setSessionStatus    arguments     status   failed   reason    Something went wrong  could not log in                       This is to tell BrowserStack that all our tests have passed  Thus triggering some follow up workflows like notify someone on Slack or start a new CI CD pipeline    Change 3  The capabilities   multiple browser and device settings   diff   runTest      const baseCapabilities          realMobile    true        build    Browserstack demo site           const capabilities1            baseCapabilities       name    Test 1        device    iPhone 13 Pro Max        osVersion    15        browserName    Safari          const capabilities2            baseCapabilities       name    Test 2        device    iPhone 13 Pro Max        osVersion    15        browserName    Chrome           const capabilities3            baseCapabilities       name    Test 3        device    OnePlus 9        osVersion    11 0        browserName    Chrome           const capabilities4            baseCapabilities       name    Test 4        device    OnePlus 9        osVersion    11 0        browserName    Firefox           runTest capabilities1     runTest capabilities2     runTest capabilities3     runTest capabilities4     For each test  we have mofied the type of the device used  its operating system version and the browser used  We are also running all the tests asychronously in parellel  You can find the list of all devices available in the  reference documentation     Step 6  Checkout additional workflows  Add to cart and Offers    This repository contains two more example workflows using the same demo site    testAddToCart js  and  testOffers js   You can check them out to seek more inspiration on how to test different aspects of a website  However  the code responsible for BrowserStack integration is exactly the same as  testLogin js     Next Steps     Checkout BrowserStack  getting started  guides   Learn more from  Selenium documentation     Feel free to suggest changes and improvements
57,USERNAMES,READMES
58,tophtucker,meltingtime   How long will it take my ice cream to melt    CREDITS   Bootstrap  obviously  http   twitter github io bootstrap    Weather data from Forecast io  https   developer forecast io    Solar calculations adapted from this NOAA calculator  http   www esrl noaa gov gmd grad solcalc calcdetails html   Snowflake script from dmolsen  https   github com dmolsen CSS3 Snowflakes   Ice cream cone by Megan Strickland  2012   via The Noun Project  http   thenounproject com noun ice cream cone  icon No1653   Background aesthetic loosely inspired by the iOS game  Scoops   https   itunes apple com us app scoops ice cream fun for everyone id291591378   Moon graphic  http   www webweaver nu clipart moons shtml   Barn adapted from Shutterstock  http   image shutterstock com display pic with logo 216823 216823 1282137614 2 stock vector cartoon red barn vector illustration 59286142 jpg   Favicon by Alexi Robbins   Marketing consulting by Tommy Cabrera and Elizabeth Maybank   Organic chemistry consulting by Bobby Shaw   Physics consulting by Drew Peck   Government major consulting by Willy Tucker   Thanks to Linda Kinstler for introducing me to the Anderson Alley ice cream shop   And thanks to everyone else who s had suggestions     Someone showed me Sleeves after I showed them this  very similar idea  http   gotsleeves appspot com    And I saw this midway through development   which kinda made me think  Oh no  I m building a weather app too   http   blog artlogic com 2013 07 02 the new ui design playground  But no  guys  It s an ice cream app  Just keep telling yourself that  bowdointweeps   Bowdoin alumni sorted by number of Twitter followers    Uses abraham s twitteroauth php library  https   github com abraham twitteroauth   The way this works is currently very low tech  I run  generate php  and manually save the html output as the static  index html   Yeah yeah  I ll improve    generate php  takes an array of Twitter lists  fetches all members  orders   de dupes  That s the gist of it    Some historical data is in   archives   again as staic html  Gonna get a database solution up and running so we can track rankings over time       Then I saw in my dream  that when they were got out of the wilderness  they presently saw a town before them  and the name of that town is Vanity  and at the town there is a fair kept  called Vanity Fair  it is kept all the year long    these pilgrims set very light by all their wares  they cared not so much as to look upon them  and if they called upon them to buy  they would put their fingers in their ears  and cry  Turn away mine eyes from beholding vanity  and look upwards  signifying that their trade and traffic was in heaven   http   www gutenberg org files 131 131 h 131 h htm   http   www bloomberg com whatiscode   LICENSE     This repository contains a variety of content  some is owned by Bloomberg Finance LP  and some is from third parties  various Javascript libraries     The third party content is distributed under the license provided by those parties    The Javascript content owned by Bloomberg Finance LP is distributed under the Apache 2 license  the text of this license can be found in the  LICENSE  file    The article text  contained in the  index html  file  is licensed under the Creative Commons Attribution NonCommercial NoDerivatives 4 0 International license  For the full text of the license  please see  the Creative Commons site   Tutorial   This is my tutorial repository  Wayback Machine for tophtucker com since 2001   Current incarnation is  v8  since fall 2016  It renders a simple HTML page in the background  then hides it and clones every letter and has them all wandering the page with Perlin noise  Cute lil particles in the background illustrate each page in different ways  When you go from one page to another  the letters transition so that it  like  re uses all the A s and B s and C s and transitions them to their new spot  Attempting an SVG only rewrite      30  fewer characters in main js file   no dependencies except D3   fast    crisp    Futura renders correctly  as originally intended in Neary s   and so much crisper    no weird blurry artifacts around thick sharp lines     To me the code is also more legible  but  well  that s just cuz I wrote it  lol  I ll let others be the judge of that  Preferably an other who s apt to actually be developing charts for us  to do     shoot letters w  up down arrows     include cursor in force nodes     rewrite in canvas lol     refine game mechanics    better bomb and attractor         better letter expiration  like after a long time     play with other mechanics       ripped from  socketio chat example   https   github com socketio chat example   This is the source code for a very simple chat example used for the  Getting Started  guide of the Socket IO website    Please refer to it to learn how to run this application    You can also spin up a free Heroku dyno to test it out      Besides  the application is deployed on  Now   https   socketio chat example now sh  Meditation XVII     https   en wikisource org wiki Meditation XVII     https   archive org details devotions00donn page 98 mode 2up     https   en wikipedia org wiki John Donne     https   www christianitytoday com history people poets john donne html   pathstuff   Helpers for SVG path commands   See and run the original  Observable notebook  for documentation and examples  Install  Docco    sudo npm install  g docco   Build site   docco index js md menu Errors from using different copies of the Runtime   Here are two notebook embeds  Notebook Embed 1 was downloaded to a local folder  and uses that local copy of the runtime  Notebook Embed 2 is hotlinked to the module served by the API  and uses the CDN s copy of the runtime    The first time  it loads fine  The second time  presumably cached   we get  spectralDecompositionsAll   RuntimeError  invalid module  in Notebook Embed 1    This can be fixed by changing either notebook embed to use the same copy of the runtime   either one of the following two lines       js import  Runtime  Inspector  from  https   cdn jsdelivr net npm  observablehq runtime 4 dist runtime js   import  Runtime  Inspector  from    spectral decompositions of natural images runtime js         The same problem occurs if you re using two different local copies of the runtime    It does seem to depend somewhat on the contents of the cells you re embedding  I haven t been able to reconstruct the failure with fresh minimal notebooks yet  opt scroll   hold opt to rewind scrolling   doesnt work yet Based on https   observablehq com  tophtucker classic research in data visualization
59,dfridman1,Design of Computer Programs   Prerequisites and Requirements   This course is intended for experienced Python programmers  students should be familiar with the Python syntax  as well as familiar with the following programming concepts  data structures  basic algorithms  and lambda functions   Syllabus   Lesson 1  Winning Poker Hands   Steps of the design process  Developing for clarity and generality  Arguments for program correctness  Experimentation and simulation   Design tradeoffs  Simplicity and Clarity  Decomposition and composability    Lesson 2  Back of the Envelope   Back of envelope calculations  When to use brute force and when to be clever  The Zebra puzzle  Generator expressions  Permutations and combinations  Cryptarithmetic  Recursive and wishful thinking  Longest palindrome substring algorithm    Lesson 3  Regular Expressions  other languages and interpreters   Defining the language of regular expressions  Interpreting the language  Defining the set of strings matched by a regular expression  Other languages    Lesson 4  Dealing with complexity through search   Search  finding your way with a flashlight or boat  pouring water  Analyzing the efficiency of an algorithm  Recurrence relations  Matching data types with algorithms    Lesson 5  Dealing with uncertainty through probability   Probability  the game of Pig  Maximizing expected utility to optimize strategy    Lesson 6  Word Games   Managing complexity  Large sets of words  Appropriate data structures  Word games    Lesson 7  Conclusion   Final exam project Bura  iOS Card Game    iTunes   link   Typical game played by Russian inmates and ex convicts  Bura is popular card game played from 2 to 6 people using a deck of 36 cards  Unconventional among point trick games  Bura allows the possibility of leading a number of cards at once given their suits are identical    How to play    The game is played using 36 card deck  At the beginning 3 cards are dealt out to each of the players and the card  determining the trump suit  is displayed  A valid move for a player  initiating a move  consists of any combination of cards of the same suit  a combination of 3 aces  one of which is of trump suit  is also valid  and is called a golden bura   A player  responding to his opponent  has 2 options  either beat his cards  take a bribe  or give up the corresponding number of any cards in his hand  In the case when a player gives up cards  his opponent is said to have taken a bribe  After that cards are dealt out to players until each of them has 3 cards in hand  A player  who has taken the last bribe  initiates the next move  The game finishes when there are no cards left in deck and players  hands  The score of each player is calculated according to his cards  points in bribes  following the rule  Ace   11 points  10   10 points  King   4 points  Queen   3 points  Jack   2 points  and 6  7  8  9 have no value  In this game the order of ranks  ascending  is  6  7  8  9  Jack  Queen  King  10  Ace  Lex   Yacc   This repo contains an implementation of tools for lexical  tokenizing  and syntax analyses  parsing and AST construction   Below follow short overviews of lexer and parser    Lex Overview   The token names must be specified in a tuple named  tokens      User can optionally specify possible states in a tuple  each state is itself a 2 element tuple consisting of a state name and its type   exclusive   inclusive    NOTE  the state   INITIAL    exclusive   is implicitly defined  If the state is inclusive  then all of the token rules defined for this state will be added to the rules of the active states  whereas  if the state is exclusive  then it will be the only active state    Token rules can be defined in 2 ways  either by defining a global function or a global variable with regex value  The naming of the token rules must conform to the following    t TOKENNAME  for the INITIAL state    t STATENAME TOKENNAME  for all other states    NOTE  if the token name  part after prefix  t    for the INITIAL state is not present in the tuple  tokens  AND the token rule  function  return value is not None  then an error is raised    For example  suppose we have    tokens     INT    STRING      version 1 def t INT t       r  0 9        t value   int t value      return t    version 2 def t INT INVALID t       r  0 9        t value   int t value      return t    Then the version 1 is valid since token  INT  exists  whereas version 2 is invalid because token name  INT INVALID  is undefined  This problem can be overcome by explicitly setting token type  For example    def t INT VALID t       r  0 9        t value   int t value      t type    INT      return t    If we use a global variable to define a token rule  ie   t int   r  0 9       then under the hood it will be transformed to the following    def t int t       r  0 9        return t    The implicit argument  t  to the token rule is an instance of the class LexToken  which has the following attributes    type  token name    value  lexeme matched    lexpos  index in the input string  0 based     lineno  line number    lexer  reference to the current lexer    When the function is invoked   value  attribute is automatically set to the lexeme matched   type  is set to the part of function name after prefix  t STATENAME   or  t   for  INITIAL  state   lexpos is set to the start index in the input string and lexer is set to the current lexer  Attribute  lineno  is also automatically set to the value of  lineno  of lexer itself  which has to be managed by the user  It can be achieved this way    def t newline t       r  n      t lexer lineno    1    Suppose we have the following    tokens     INT    STRING   states      COMMENTS    exlusive        and we want to enter a  COMMENTS  state when we see      and exit it on       We can accomplish this    def t COMMENTS comment t       r           t lexer begin  COMMENTS    def t COMMENTS commentend t       r           t lexer end  COMMENTS      A special token rule is reserved for each state    t STATENAME error    t error   for  INITIAL  state   In this case the implicit parameter is an instance of  LexError  which has the same attributes as LexToken  but instead of  type  it  has  error msg   Also the  value  is set to the value of the input string lexpos    Example usage    def t error t       print  invalid char   r    t value 0      t lexer skip 1     1 char advance in input to avoid infinite loop     return t    Characters to be ignored can be specified in  t STATENAME ignore    t ignore  for  INITIAL  state  Hence  if we want to ignore whitespace  we can do the following    t ignore   r   t     the name t ignore  is arbitary    To initialize a lexer    import lex  lexer   lex lex   lexer input  input string   while True      token   lexer token       if token is None          break       otherwise  do something with token    An example of using a lexer for tokenizing JSON formatted strings  see  json lex py     Yacc Overview   Our parser will need an object created by the lexer  ie  lex lex     Suppose we have the specification for the lexer in a file  mylexer py     Let s consider a simple language of function definitions conforming to the following CFG  context free grammar     DEF               string lparen ARGS LIST rparen equal EXP SUM ARGS LIST         NONEMPTY LIST   epsilon NONEMPTY LIST     string comma NONEMPTY LIST   string EXP SUM           EXP EXP               EXP plus string   string     myfunc x  y  z    x   z  is a valid sentence of the grammar  In our specification all lower case symbols are terminals  tokens defined in  mylexer py    whereas upper case symbols constitute non terminals  case distinction is irrelevant   here it is used for convinience only     A production rule must be defined as a function with a name starting with  p    Different productions for the same nonterminal may be defined within the same function  alternatives must be separated by        or as separate functions  Epsilon is represented as an empty string  ie  LIST      empty list  or  LIST   ARG comma LIST       The production rules for the above grammar may look like    import yacc from mylexer py import lexer  tokens    NOTE  tokens must be imported  def p func def p        DEF   string lparen ARGS LIST rparen equal EXP        p 1    string  function name        p 3    ARGS LIST       p 6    EXP  body of the function      p 0      function   p 1   p 3   p 6    def p args list p          ARGS LIST   NONEMPTY LIST                            p 0      args       if len p     1 else   args   p 1    def p args nonempty p        NONEMPTY LIST   string comma NONEMPTY LIST      p 0     p 1     p 3   def p args nonempty one p        NONEMPTY LIST   string      p 0     p 1    def p exp sum p        EXP SUM   EXP      p 0      sum   p 1    def p exp p        EXP   EXP plus string   string      if len p     2          p 0     p 1       else          p 0    p 1     p 3      To run a parser    parser   yacc yacc   text to parse    myfunc x  y  z    x   z  tree   parser parse text to parse  lexer  if tree is not None      print tree else      print  Parsing failed      The result AST is   function    myfunc     args     x    y    z       sum     x    z        NOTE   parse method  from above takes a key argument  tokenfunc   By default   tokenfunc  is   tokenfunc   lambda lextoken  lextoken value    A more sophisticated example  JSON parsing  of using yacc is in  json yacc py   Solver for a puzzle game called Bloxorz Scrabble Solver   This repo contains a solver for a word game  Scrabble     Here s a short overview of the the functionality        The functions for reading string representations of both the board and bonus board lie in the ScrabbleParse hs module        Game represention and functions for showing game board   showScrabbleBoard  being most helpful  reside in the Scrabble hs module        The Dictionary for the game is created using function  makeDictionary  from module Dictionary hs       The functions for finding the best play available as well as updating the board with a new play are contained in the module ScrabbleSolver hs        ScrabbleAI hs contains functions for generating  and showing  plays until the player runs out of tiles  or no solutions can be found       Parser combinator library for Python     A recursive descent parser based on functional combinators  API closely follows that of Parsec  a parsing library for Haskell   The library provides the user with a wide range of combinators for building more complex parsers    Installation     Open up your terminal and run   pip install parsefunc   Example     Suppose we would like to parse and evaluate arithmetic expressions adhering to this simple context free grammar   EXPR      TERM   EXPR   TERM     TERM      FACTOR   TERM   FACTOR     FACTOR    integer     EXPR     Now  we can see that EXPR is just a sum of TERM s and TERM is a product of FACTOR s  High level of abstraction allows us to describe the parsers in terms of what they actually are rather than hardcoding them    Let s first define parsers for     and      They would be  plus  mult   char       char       To parse an expression  we can write   def parseExpr s           return sepBy parseTerm  plus  s    This returns a list of values returned by parser  parseTerm   Since our goal is to write an evaluator  we can add a  syntax tree f   decorator  which takes a value returned by the parser and applies  f  to it  Since we have a list of summands  we can apply  sum  to it to get     syntax tree sum      def parseExpr s           return sepBy parseTerm  plus  s    parseTerm  is almost identical to  parseExpr   but with multiplication replacing summation  Here s code for it     syntax tree product      def parseTerm s           return sepBy parseFactor  mult  s    where  product   lambda factors  reduce lambda x  y  x   y  factors  1      Finally  to parse a  FACTOR   we define    Parser     def parseFactor s           return  parens parseExpr    integer  s    Note  Parser  decorator instead of  syntax tree   When defining a combinator with a  def  keyword  it is  necessary  to decorate it with one of above decorators to make it an object of class  Parser     Parser  is equivalent to   syntax tree lambda x  x      Now that we have the description of our small language defined  it is time to call a top level function  parse   which takes a parser  text to parse  and optionally the  source name     input    5  4 3   2 4 3        82     print parse parseExpr  input  sourceName  arithm py        82   Note that the above implementation does not allow spaces between tokens  For a more rigorous treatment of evaluating arithmetic expressions  please go  here     Navigation     Please consult  combinators py    char py  and  tokens py  for a wide range of combinators available    examples  directory contains 2 examples of using the library    an extension to the arithmetic evaluator example laid out above   json parser   Notes   A more comprehensive documentation is soon to be released  Dota 2 Winner Prediction Digit Recognizer  Kaggle    A solution to a digit recognizer problem on Kaggle  Achieves accuracy of 0 97571  ranked 404th  on public leaderboard    Model used  3 layer Neural Network with ReLU activation function  500  neurons  in 1st hidden layer and 250  neurons  in 2nd hidden layer  Neural Style Transfer   This is a PyTorch implementation of  A Neural Algorithm of Artistic Style               Usage     run default sh   For a full list of arguments  run   python main py   help Conditional DCGAN   This repository contains a Pytorch implementation of Conditional DCGAN tested on 2 datasets    MNIST   CIFAR 10       Note  that the model architecture is not precisely the same as in the original DCGAN paper  Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks     Installation   Clone the repo by running   shell script git clone https   github com dfridman1 Conditional DCGAN git DCGAN Pytorch   After cloning the repository create a virtual environment and install prerequisite packages    shell script cd DCGAN Pytorch virtualenv venv  p python3 7  source venv bin activate   install sh   Training   train py  script launches training and dumps generated images to a specified folder  In order to see the command line arguments the script takes  run   shell script python train py   help   Here is a description of the arguments     Required            experiment dirpath  str            experiment name  str    the generated images are saved to  experiment dirpath experiment name           dataset  one of mnist  cifar10            dataset dir  str    the dataloader will try to fetch the dataset from this directory  if not found  the dataset will be downloaded to this directory    Optional            gpu id  int    if specified  the training takes place on  cuda gpu id   on CPU  otherwise          image size  int    the size of the generated images  if not specified  the size will be as in the dataset the model is trained on  28 for MNIST  32 for CIFAR 10           train iters  int            lr  float            show every  int    images will be generated and saved every  show every  iterations          z dim  int    the size of the noise vector passed to the generator          k  int    for every training iteration of the generator   k  iterations will be run for the discriminator          no conditional  flag    the vanilla  non conditional  DCGAN will be trained          l2 loss  flag    instead of binary cross entropy  L2 loss will be optimized          no label smoothing  flag    An example run   shell script python train py   gpu id 0                     dataset mnist                     dataset dir data                     experiment dirpath experiments                     experiment name my first experiment                     train iters 2000                     show every 100                     l2 loss   The generated images will be dumped to  experiments my first experiment   funparse
60,shafi,pnc to qbo   Convert PNC Bank CSV statement exports to Quickbooks import format
61,jgehring,peprserv   A repository statistics server for pepper   peprserv is a small web server that provides statistics and graphs for source code repositories  It is implemented as a report script for   pepper  and serves the output of other reports via HTTP    Current features include individual report configuration  caching of report output and answering to HTTP HEAD requests  A live demo using  the official Git repository  is available at  http   jgehring net 9000     Dependencies     pepper   version 0 3 1 or higher   Xavante   optional   lua zlib  for output     compression      Quick start   Install the dependencies  cd to the top level peprserv directory and run     pepper peprserv   config config lua   show index  REPOSITORY    where   REPOSITORY  is the path a source code repository  Afterwards  point your browser to  http   localhost 8080 and check your console for possible progress output    Configuration   The script can be customized with various command line arguments  accessible by passing    help   For full control  you might want to use a configuration file  The example configuration included in the package   config lua   should provide a good starting point    While you re setting up the server  you can use the    show index  flag to make the server provide a small HTML page that can be used to test the current configuration    Usage   The program is a web server using HTTP  so you need a browser or something similar to get data from it  Example given  if a report named  loc  is offered  it is available at   r loc   However  the intended use case is acting as a backend for web sites  providing on demand repository statistics   Thus  the program offers a small API containing the following  functions   i e  URLs        list  returns a JSON formatted list with all offered report      scripts  JSONP via   callback   is supported  too     r  REPORT  runs the respective report and returns its output     Individual report arguments can be append to an URL  like in   r loc branch next    Arguments inside the URL take precedence over those specified in the configuration file    License   peprserv   A repository statistics server for pepper Copyright  C  2012 Jonas Gehring   This program is free software  you can redistribute it and or modify it under the terms of the GNU General Public License as published by the Free Software Foundation  either version 3 of the License  or  at your option  any later version    This program is distributed in the hope that it will be useful  but WITHOUT ANY WARRANTY  without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE   See the  GNU General Public License for more details    You should have received a copy of the GNU General Public License along with this program   If not  see  http   www gnu org licenses                                                                           AUTOGRADPP   This is an experimental C   frontend to pytorch s C   backend  Use at your own risk    How to build      git submodule update   init   recursive   cd pytorch   On Linux    python setup py build   On macOS  may need to prefix with  MACOSX DEPLOYMENT TARGET 10 9 CC clang CXX clang    when using anaconda    LDSHARED  cc  dynamiclib  undefined dynamic lookup  python setup py build   cd     mkdir  p build  cd build cmake     DPYTHON EXECUTABLE FILEPATH   which python     helpful if you use anaconda make  j       Stuff     Check out the  MNIST example   which tries to replicate PyTorch s MNIST model   training loop   The principled way to write a model is probably something like   AUTOGRAD CONTAINER CLASS MyModel         This does a 2D convolution  followed by global sum pooling  followed by a linear   public    void initialize parameters   override       myConv    add Conv2d 1  50  3  3  stride 2  make     conv        myLinear    add Linear 50  1  make     linear          variable list forward variable list x  override       auto v   myConv   forward x       v   v mean  1  mean  1       return myLinear  forward  v         private    Container myLinear     Container myConv         Some things are not implemented    Batchnorm   Only SGD and Adam are implemented  the rest of the optimizers are just copying Python code from PyTorch over    Some things to be careful of    Variable detach does not do what you think it does  Better to do Variable old data      Otherwise  everything else works  There may be breaking API changes
62,spk921,PredNet with torch7     Work in E lab with Eugenio Culurciello       Multi GPU Training    Work in E lab add lastlayer and fine tune option  Add inplace color augmentation        Enet with torch7    Work in E lab with Eugenio Culurciello  Abhi  and Adam     Open Image Dataset downloader   Downloads images from the Google Open Image Dataset  https   github com openimages dataset   To install dependency   Needs python    3 5   See requirement txt for python pacakges   sh install sh   Have to upgrade pip3 when fail init pip3 in ubuntu   sudo  H python3  m pip install   upgrade pip   First Downloading url and lebel files   sh download sh   To download images from given csv file   Do python3 getImages py with follow option options   1st option   dict csv   2end option  Your classes csv   3rd option   path to images csv i e  images 2016 08 validation images csv   4th option   path to labels csv i e  machine ann 2016 08 validation labels csv   5th option   Number of Max images per class   6th option     Number of Threads   6th option   Target folder to download   To test   sh run sh   note  labels csv and images csv must sync i e  train labels csv train images csv note  class  scissors is not in validation dataset it s only at trainSet
64,scott-joe,Goose   Concept project to facilitate peaceful demonstration for political causes  The idea is to get people more involved and allow them better communication channels to make their voices heard  cleanup crew   Code for America   Keep Indy Beautiful distributed workforce city cleanup concept   Description   People sign up and register as effort for their neighborhood and earn a clean up flash mob for their neighborhood  If people actually participate and help other neighborhoods before their own gets a clean up day    Volunteers would register and pre schedule for  n  cleanups  Saturdays spent in other neighborhoods where they ve been checked in for full contribution  Their contribution builds up effort for their own neighborhood    Ideally  this would increase the number of workers ready and willing to work on any given Saturday  increasing the amount of work done on any given Saturday in less time    The underlying goal behind this is two fold  1  Neighborhood cleanups 2  Cross neighborhood familirity   Indianapolis is a very splotchy hot and cold distribution of socio economic groups  They tend to isolate among themselves  I would like to extend the community spirit within each neighborhood across neighborhoods    Maybe each individual simply works up for their neighborhood  So some people could work as often as they like  Neighborhoods would likely need a project coordinator to assess the needs of the neighborhood before going in to figure out what supplies and how many people they d need  This represenative should also be able to rally their own neighborhood to help and attend meetings of captains to work on core needs and alignment    It would be nice to have small groups from each team come together so they don t feel so isolated  but I d prefer not to have entire neighborhoods descending onto other neighborhoods  Part of the idea is to develop bonds between people in different neighborhoods    No one knows what neighborhood is up next so they can t ditch out of going somewhere they don t want to go  No one will ever go anywhere unsafe    Need sponsors to supply  equipment  checked in and out to people on duty who don t bring their own   consumable materials like mulch  food and bev for volunteers  team shirts  vegitation being planted  etc  deck monsters   Spawn monsters  equip them with cards  and send them into the ring to fight  This repo contains the core code for the game engine  Typically it would be hooked into another system via an adapter that instantiates the game object and makes it available in that system  For example  the game was originally written to be played within Slack  Basic homepage for the Ansible Network slack emoji   A repo for emoji Software Engineer s Portfolio   A heavily modified Gatsby template from HTML5 UP   Gatsby js V2 starter based on the Stellar site template  designed by HTML5 UP  Check out https   codebushi com gatsby starters and themes  for more Gatsby starters and templates    Preview   https   gatsby stellar surge sh    Installation   Install this starter  assuming Gatsby is installed  by running from your CLI     gatsby new gatsby starter stellar https   github com codebushi gatsby starter stellar   Run  gatsby develop  in the terminal to start the dev site    Evolving ToDo App   I wanted to try out React Hooks with something from  the ground up  with CRA and trying out the newer Github features along the way  I ve been following Egghead courses on React and other technologies to solidify a lot of  on the job  experience into something a little more complete  At a product company  it can takes years to realize you re behind the curve  As a consultant  I ve effectively had 4 jobs projects at 3 large international companies  That along with semiannual bench time to spend on education enabled me to update my skills faster than before    Tech Used   Scaffold      x  Create React App     View      x  Function Components    x  Custom hooks     Store      x  useState hooks     Testing      x  Jest     Static Analysis         ESLint    x  Prettier       TypeScript     Unit Testing      x  React Testing Library     Integration Testing         Cypress     Functional Testing         Lambda Test       Selenium         Persistence      x  LocalStorage       MongoDB       MySQL       Postgres     API         REST       GraphQL     Cacheing         Redis     Deployment         Static files       Static files   API       Docker       CRA Readme   Create React App readme has been moved  here     Make it  work     Make it  useful     Make it  beautiful         Learning       Redux  vanilla    Redux Thunks  vs Sagas    React Router v6   NestJS   Yarn3   Monorepos   Azure Functions   Sqlite   Typescript  more    gym express Screeps   Typescript   I m working on Screeps in TypeScript    Basic Usage   You will need      Node JS   10 x    12 x    A Package Manager   Yarn  or  npm     Rollup CLI  Optional  install via  npm install  g rollup       Download the latest source  here  and extract it to a folder    Open the folder in your terminal and run your package manager to install the required packages and TypeScript declaration files       bash   npm   npm install   yarn   yarn       Fire up your preferred editor with typescript installed and you are good to go    Rollup and code upload   Screeps Typescript Starter uses rollup to compile your typescript and upload it to a screeps server    Move or copy  screeps sample json  to  screeps json  and edit it  changing the credentials and optionally adding or removing some of the destinations    Running  rollup  c  will compile your code and do a  dry run   preparing the code for upload but not actually pushing it  Running  rollup  c   environment DEST main  will compile your code  and then upload it to a screeps server using the  main  config from  screeps json     You can use   cw  instead of   c  to automatically re run when your source code changes   for example   rollup  cw   environment DEST main  will automatically upload your code to the  main  configuration every time your code is changed    Finally  there are also NPM scripts that serve as aliases for these commands in  package json  for IDE integration  Running  npm run push main  is equivalent to  rollup  c   environment DEST main   and  npm run watch sim  is equivalent to  rollup  cw   dest sim     Important  To upload code to a private server  you must have  screepsmod auth  installed and configured    Typings   The type definitions for Screeps come from  typed screeps   If you find a problem or have a suggestion  please open an issue there  Meal Planner   Purpose   Web client to the Meal Planner app      Monorepo   This is a monorepo for a React PWA and NestJS middleware    Purpose   To help cut down on the effort going into meal planning each week  Yes I realize this is way more work than meal planning  but I like doing this and I don t like meal planning  Also gives me a good reason to try new things    Running   yarn yarn client start dev   Test   yarn test   Roadmap       Convert to GH Project      x  App bar    x  Side nav drawer    x  Redux    x  Consumer layer    x  Stub HTTP routes for recipes       Connect to SQLite db       Create new recipes  basic MD        Recall and display recipe list       Recall and display single recipe       Create recipes with metadata         create better interface for the db client   add way to check off stuff you know you have and end up with a list of things to check for before you go or order groceries   recipe can add link to source   recipe can add photos   recipe can be added from pinterest   pulls pin s title   pull s link for recipe   asks you to fill out the rest       verify items exist before attempting to collect   set frequency of each recipe   set frequency of each meal   flag recipe as public shared community   comes from moderator   suggested by user   comes from trusted source domain       search recipe for keywords in ingredients list to tag proteins and other things like  contains nuts  or  contains dairy
65,LukasMosser,ipython notebooks   Collection of ipython notebooks I have made over the years  Includes notebooks on geoscience  petroleum engineering  data science and more  Tracer   Agile Geoscience Hackathon Vienna 2016   Dario  Havard  Lukas MSc Thesis   Lukas Mosser MSc Thesis repository pykowski pydca   Decline Curve Analysis in Python Jupetro   A collection of oil gas related Jupyter notebooks   To start an interactive session of this repository click here     License   All notebooks are published under the GPL3 license    Created by Lukas Mosser 2016  MPyS   https   travis ci org LukasMosser MPyS svg branch master   PorousMediaGAN   Implementation and data repository for  Reconstruction of three dimensional porous media using generative adversarial neural networks   Authors   Lukas Mosser   Twitter   Olivier Dubrule   Martin J  Blunt   Department of Earth Science and Engineering  Imperial College London   Results   Cross sectional views of the three trained models   Beadpack Sample      Berea Sample      Ketton Sample     Methodology     Instructions   Pre requisites     To run any of the  jupyter  notebooks follow instructions  here  or install via pip   bash pip install jupyter   In addition we make heavy use of  pandas    numpy    scipy  and  numba   We recommend the use of  anaconda   For numba instructions  you can find a tutorial and installation guideline  here     For the torch version of the code training and generating code please follow the instructions  here   In addition you will need to have installed torch packages  hdf5  and  dpnn   bash luarocks install hdf5 luarocks install dpnn   For the pytorch version you will need to have installed  h5py  and  tifffile   bash pip install h5py pip install tifffile   Clone this repo  bash git clone https   github com LukasMosser PorousMediaGAN cd PorousMediaGAN     Pre trained model  Pytorch version only    We have included a pre trained model used for the Berea sandstone example in the paper in the repository    From the pytorch folder run  generate py  as follows  bash python generator py   seed 42   imageSize 64   ngf 32   ndf 16   nz 512   netG  path to generator checkpoint  pth   experiment berea   imsize 9   cuda   ngpu 1  Use the modifier    imsize  to generate the size of the output images      imsize 1  corresponds to the training image size Replace   path to generator checkpoint  pth  with the path to the provided checkpoint e g   checkpoints berea berea generator epoch 24 pth  Generating realizations was tested on GPU and CPU and is very fast even for large reconstructions    Training   We highly recommend a modern Nvidia GPU to perform training   All models were trained on  Nvidia K40  GPUs   Training on a single GPU takes approximately 24 hours   To create the training image dataset from the full CT image perform the following steps    Unzipping of the CT image    bash cd   data berea original raw   unzip using your preferred unzipper   unzip berea zip    Use  create training images py  to create the subvolume training images  Here an example use  bash python create training images py   image berea tif   name berea   edgelength 64   stride 32   target dir berea ti  This will create the sub volume training images as an hdf5 format which can then be used for training      Train the GAN   Use  main py  to train the GAN network  Example usage  bash python main py   dataset 3D   dataroot  path to training images    imageSize 64   batchSize 128   ngf 64   ndf 16   nz 512   niter 1000   lr 1e 5   workers 2   ngpu 2   cuda        Additional Training Data   High resolution CT scan data of porous media has been made publicly available via the Department of Earth Science and Engineering  Imperial College London and can be found  here   Data Analysis   We use a number of jupyter notebooks to analyse samples during and after training    Use  code notebooks Sample Postprocessing ipynb  to postprocess sampled images       Converts image from hdf5 to tiff file format       Computes porosity   Use  code notebooks covariance Compute Covariance ipynb  to compute covariances       To plot results use  Covariance Analysis ipynb  and  Covariance Graphs ipynb  as an example on how to analyse the samples    Image Morphological parameters   We have used the image analysis software  Fiji  to analyse generated samples using  MorpholibJ    The images can be loaded as tiff files and analysed using  MorpholibJ Analyze Analyze Particles 3D     Results   We additionally provide the results used to create our publication in  analysis     Covariance S2 r    Image Morphology    Permeability Results  The Jupyter notebooks included in this repository were used to generate the graphs of the publication    Citation   If you use our code for your own research  we would be grateful if you cite our publication  ArXiv    article pmgan2017      title  Reconstruction of three dimensional porous media using generative adversarial neural networks       author  Mosser  Lukas and Dubrule  Olivier and Blunt  Martin J        journal  arXiv preprint arXiv 1704 03225       year  2017      Acknowledgement   The code used for our research is based on  DCGAN  for the  torch  version and the  pytorch  example on how to implement a  GAN    Our dataloader has been modified from  DCGAN     O  Dubrule  thanks Total for seconding him as a Visiting Professor at Imperial College  OklahomaProductionData   A repository of machine learnable formatted oklahoma o g production data  pmgan   Pytorch implementation of porous media gan with automatic monitoring of properties GeoGAN  Conditioning of three dimensional generative adversarial networks for pore and reservoir scale models   Authors    Lukas Mosser    Olivier Dubrule    Martin J  Blunt   Department of Earth Science and Engineering  Imperial College London     This is the code repository accompanying the publication    Conditioning of three dimensional generative adversarial networks for pore and reservoir scale models     ArXiv     Datasets and pre trained models   Ketton Limestone Dataset   We provide two pre trained GAN models  The first one is trained on the Ketton limestone training image presented here   If you decide to use this dataset for your own work  please consider citing the following works    Stochastic reconstruction of an oolitic limestone by generative adversarial networks   ArXiv     Dynamic reservoir condition microtomography of reactive transport in complex carbonates   Article     Due to their size we provide the necessary files via a  Google Drive     Results     The figure above shows two samples  b c  obtained by a conditioning a generative adversarial network to three orthogonal cross sections of the Ketton training image a    Due to the stochastic nature of the optimization procedure the resulting images have distinctly different features away from the conditioning data      Maules Creek Dataset   We have trained a generative adversarial network on the Maules Creek alluvial aquifer training image      The required model checkpoints are included in this repository   If you choose to use the Maules Creek training image  please consider citing their originators at  trainingimages org     Results     We have conditioned 1024 realizations of the Maules Creek alluvial aquifer model and present mean and standard deviation maps of the resulting ensemble   The resulting images show that each realization honors the data at the well exactly and an ellipsoidal region of influence can be observed      Requirements   The conditioning tool is based on the following libraries     Python 2 7     Pytorch 0 3     Scikit Learn     tqdm     numpy     matplotlib   We recommend using the  anaconda  distribution to install the required dependencies    Development   Currently the code has limited object oriented design and is usable as demonstrated by the accompanying jupyter notebooks   We welcome contributions and suggestions for the improvement and development of the tool      Required Hardware   We recommend at least 16 GB of RAM and a modern CUDA capable NVIDIA graphics processor   All experiments apart from network training have been performed on an NVIDIA 960M   Training of the generative networks was performed on 8xNVIDIA K40 GPUs      Support   The software is provided as is   If you have any questions please feel free to contact us via   email   or   twitter                Geo Deadlines        A community driven collection of upcoming deadlines for earth science   engineering related conferences       geodeadlin es   Hosted with  heart  by   porestar       How to Contribute     Create a fork of the repository to your own github account      Update   data conferences yml  with the correct attributes      Send a pull request          If you are not able to do this  please get in touch via  email  or  twitter       Disclaimer   This initiative is inspired by Abhishek Das for  aideadlin es    The original code was created by   abhshkdz   dask nlmeans   Non Local Means on Dask geolink dataset   Analysis notebooks for the geolink well log dataset   All data and analysis CC by SA 4 0    All credit for data goes to  GEOLINK S2   Data is available to download  here   Install the necessary dependencies using conda      conda create  n geolink   file requirements txt Automatic Seismic Interpretation in Pytorch   Pytorch implementations of  Automatic Seismic Interpretation  approaches and publications   This repository is a collection of implementations of automatic seismic interpretation models and publications in Pytorch   The main purpose is purely educational and investigative    All code comes as is  with no guarantees  All credit for original work to the authors of respective publications and implementations    Currently implemented models    MalenoV   Ported from the original implementation in Keras by  bolgebrygg  GitHub   If you find this repository useful feel free to cite the repository  the original author s works or drop me a tweet   porestar     Please respect the licenses of the datasets  the original authors work and of this repository  Thank you SPE 1 Day Short Course  How to get started in Deep Learning    Automatic Seismic Interpretation Demo   This repository contains a demo application for automated seismic interpretation using convolutional neural networks  The accompanying notebook was created for the SPE 1 Day Short Course on How to get started with Deep Learning    You can run the notebook online in a google hosted virtual machine using the following link    https   colab research google com github LukasMosser SPE GS DL blob master SPE 1Day Course How To Get Started With Deep Learning ipynb   All data is provided as is without any liability  All Credit for data creation is attributed to Michael Steventon  LinkedIn     If you find this useful in your work  please remember to give credit where due according to the provided license  Reproducing Roeth and Tarantola using Modern Tools   Blog Series   This repository holds the code used to build the blog series around the seminal work of Roeth and Tarantola 1994  The blog is hosted on my  website     The first part of the series covers the creation of a generative model  using Pytorch distributions to create layered earth models    Planned Blog Posts     Part 1   Layered Earth Generative Modeling with Pytorch   Part 2   Synthetic Waveform Modelling using Devito   Part 3   Training Neural Networks to perform inversion using Pytorch   Part 4  Roeth  Tarantola and beyond        Stay tuned  DeepFlow   History Matching in the Space of Deep Generative Models   Authors   Lukas Mosser    Olivier Dubrule    Martin J  Blunt     Pytorch  implementation of  DeepFlow  History Matching in the Space of Deep Generative Models   Model Architecture     The model architecture consists of two parts  the generative adversarial network  implemented in Pytorch  and the forward solver   MRST    The coupling between the two is implemented in  deepflow mrst coupling PytorchMRSTCoupler  and defines a fully differentiable computational graph    Traversing the latent space while  History Matching      A visualization of the intermediate geological models obtained during the optimisation process    Interpolation between MAP solutions     Interpolation in latent space between three MAP estimates shown in the publication  Figure 9a b    Usage   To perform the inversion using the available pre trained generator network use  run deepflow sh  This requires a current version of Matlab or Octave available on the PATH   Interpolation is performed by running  interpolation py  using the example bash file  run deepflow interpolation sh     Trained Models   Pre trained models are available in the   checkpoints  directory    Results and Data   A subset of the results is available in this  Google Drive  The full dataset of the computations is multiple terrabyte in size and cannot be shared   Computing each run was made reproducible by setting the run number   seed command line argumen   Computations were performed on Imperial College CX1 supercomputing facilities  Total duration  3 days wall time on 100 servers   4 cores each      Matlab   Octave Compatibility   The reservoir simulator that solves the two phase flow problem and provides gradients via the adjoint  MRST  requires a Matlab license to run  but should be fully compatbile with  GNU Octave   Citing    ARTICLE 2019arXiv190505749M         author     Mosser   Lukas and  Dubrule   Olivier and  Blunt   Martin J            title     DeepFlow  History Matching in the Space of Deep Generative Models          journal    arXiv e prints        keywords    Computer Science   Machine Learning  Computer Science   Computer Vision and Pattern Recognition  Physics   Computational Physics  Physics   Geophysics  Statistics   Machine Learning            year    2019           month    May             eid    arXiv 1905 05749           pages    arXiv 1905 05749           archivePrefix    arXiv          eprint    1905 05749           primaryClass    cs LG          adsurl    https   ui adsabs harvard edu abs 2019arXiv190505749M         adsnote    Provided by the SAO NASA Astrophysics Data System      Acknowledgements   The author would like to acknolwedge the developers of the  Matlab Reservoir Simulator Toolbox    If you use their software  please acknowledge them in your references   O  Dubrule would like to thank Total for seconding him as a visiting professor at Imperial College London    License   MIT         Seismic NIST   Table of Contents      What the data looks like   what the data looks like     Why Seismic NIST was created   why seismic nist was created     Get the Data   get the data     Benchmarks and Results   benchmarks and results     Generating the data   generating the data     Contributing   contributing     Citation   citation     License   license       Seismic NIST  is a dataset of acoustic  seismic  waveforms and their underlying  velocity profiles   The dataset is inspired by the work of  Roeth and Tarantola 1994  where the authors tried to perform  seismic inversion  from raw acoustic waveforms at various levels of noise  Here we provide a reference dataset of such waveforms  The machine learning task to be solved is a regression problem of predicting synthetic  p Wave velocity  profiles from given acoustic waveforms  The data can be generated completly from scratch using  torch  and libraries from the  devito  project   The dataset is named after the outstanding deep learning benchmark  MNIST  by  Yann Le Cun   and is inspired by other projects such as  FashionMNIST  and  KMNIST     What the data looks like   The dataset consists of 750 waveforms generated from 9 layer earth models of acoustic p wave velocities   The training set consists of 600 waveforms and the test sets consist of 150 waveforms   There are three test sets   SNIST 0  SNIST 1 and SNIST 2   The number corresponds to the level of noise added to the test set i e  SNIST 0 has no noise added  SNIST 1 adds 1 sigma of noise  and SNIST 2 has 2 sigma of noise added  The noise is Gaussian uncorrelated noise   Each waveform consists of 20  traces  according to 20  offsets  sampled at 8 ms time intervals   p Wave velocities  are capped at 4000  m s    Here s what the waveform amplitudes and some of the velocity profiles  ground truth   black  look like        Why Seismic NIST was created   The dataset was largely inspired by discussion on the  software underground  slack channel and by Agile Geoscience s  blog post  on benchmark studies in the machine learning   geoscience domain     While the realism and usefulness in terms of real seismic applications is limited  this benchmark may serve as a reference on what a realistic benchmark should include  Hence  this benchmark is very much a platform or sandbox as not  m any reference benchmarks exist in the seismic deep learning domain  It is up to the community to shape what we want out of such a reference benchmark and I hope to provide here a starting point for such a discussion   If you would like to contribute or would like to raise an  issue  please do so and join the discussion on the  slack channel     Get the Data   The data comes prepackaged as   npy  files  Which you can either download manually or use the existing  torch dataset  implementation found in  utils snist       File              Examples   Size   Download  NumPy format                                                                        Training Amplitudes   600               13 MB    train amplitudes npy      Training Velocities   600               21 KB    train velocities npy       Testing Amplitudes 0  no noise   SNIST 0     150               3 MB   test amplitudes npy       Testing Amplitudes 1  1 sigma noise   SNIST 1     150               3 MB   test amplitudes noise 1 npy       Testing Amplitudes 2  2 sigma noise   SNIST 2     150               3 MB   test amplitudes noise 2 npy       Testing Velocities    150              5 KB    test velocities npy      Pytorch Datasets   The following is an example on how to use the provided dataset in  torch   All the data will automatically be downloaded   in this case   from the directory and is ready for training   You can try it out on         python from snist dataset import SNIST   snist train   SNIST       train True  download True  snist 0 test   SNIST       train False  download True  noise 0  snist 1 test   SNIST       train False  download True  noise 1  snist 2 test   SNIST       train False  download True  noise 2        Benchmarks and Results   A reference implementation  is provided  and here we collect the performance of methods that have been evaluated on the SeismicNIST dataset   If you wish to contribute to this list please raise a  pull request  and provide a link to a repository where your results can be reproduced         Model                              SNIST 0   SNIST 1   SNIST 2   Credit   Link                                                                                     1 Hidden Layer Benchmark        242 42  m s    287 98  m s    428 59  m s      porestar     Generating the data   The data can be reproduced by running  make build  in the  data generation  directory   This will run three scripts     generate velocities py   creates the velocity models based on the paper by Roeth and Tarantola    generate amplitudes sh   runs a docker container of  devito  and runs the forward model on the created velocities    generate noisy test set py   creates the noisy SNIST versions SNIST 1 and SNIST 2   Contributing   If you would like to contribute or would like to raise an  issue  please do so and join the discussion on the  slack channel     Citation   If you use SNIST in your work we would appreciate if you could cite the dataset throught the DOI    software lukas mosser 2022 6622823    author          Lukas Mosser     title            SNIST  A Benchmark for Seismic Velocity Inversion                     from Synthetics      month          jun    year           2022    publisher       Zenodo     version         v1 0     doi             10 5281 zenodo 6622823     url             https   doi org 10 5281 zenodo 6622823      License       MIT License   Copyright  c  2019 Lukas Mosser   Permission is hereby granted  free of charge  to any person obtaining a copy of this software and associated documentation files  the  Software    to deal in the Software without restriction  including without limitation the rights to use  copy  modify  merge  publish  distribute  sublicense  and or sell copies of the Software  and to permit persons to whom the Software is furnished to do so  subject to the following conditions    The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software    THE SOFTWARE IS PROVIDED  AS IS   WITHOUT WARRANTY OF ANY KIND  EXPRESS OR IMPLIED  INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT  IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM  DAMAGES OR OTHER LIABILITY  WHETHER IN AN ACTION OF CONTRACT  TORT OR OTHERWISE  ARISING FROM  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE      Stochastic seismic waveform inversion using generative adversarial networks as a geological prior   Authors   Lukas Mosser    Olivier Dubrule    Martin J  Blunt     Pytorch  implementation of  Stochastic seismic waveform inversion using generative adversarial networks as a geological prior   Model Architecture   The model architecture consists of two parts      the generative adversarial network  implemented in  Pytorch        the acoustic wave equation forward solver implemented in    Devito     The coupling between the two defines a fully differentiable computational graph    Movie representation of samples from the prior     Movie representation of samples from the posterior  27 sources      Usage   To perform the inversion using the available pre trained generator network use  apps main paper version revised py   Sorry for the long name  but older versions of the code were kept for reference purposes    Trained Models   Pre trained models are available in the   checkpoints  directory    Results and Data   The resulting datasets are available in this  Google Drive     Each run was made reproducible by setting the run number   seed command line argument   Computations were performed on Imperial College CX1 supercomputing facilities   Total duration  12 hours wall time on 32 core nodes   50 nodes simultaneously      Figures from paper   The figures from the paper can be reproduced using  notebooks Paper Figures ipynb    All figures are located in  results figures   Devito Optimizations   The library used to represent the forward solver has a number of optimizations that allow it to parallelize across cores and nodes using MPI   We suggest the following environment variables be set to maximize for performance    DEVITO OPENMP  1   DEVITO DLE  advanced  DEVITO LOGGING  INFO  DEVITO ARCH  gcc    An example bash script used to perform the numerical computations on Imperial s CX1 cluster can be found in  scripts cluster run sh   Citing    article mosser2018stochastic    title  Stochastic seismic waveform inversion using generative adversarial networks as a geological prior     author  Mosser  Lukas and Dubrule  Olivier and Blunt  Martin J     journal  arXiv preprint arXiv 1806 03720     year  2018      Acknowledgements   The author would like to acknolwedge the developers of the  Devito    If you use their software  please acknowledge them in your references   O  Dubrule would like to thank Total for seconding him as a visiting professor at Imperial College London    License   MIT Londonhack 2019   Pytorch Tutorial   Tutorial Notebooks for the Pytorch Tutorial at the London Hack 2019   Disclaimer   This material is not intended to be a full course on machine  deep learning  or neural networks  and is meant to introduce basic Pytorch functionality based on a number of examples  Pre requisites are         Basic Linear Algebra   Experience with Python Programming and the scientific python stack  Numpy  Matplotlib       is recommended    Some familiarity with Neural Networks  Optimization  Convolutional Neural Networks and their concepts      All code is meant to be run on Google Colab and was built on Pytorch 1 0    Course Material     Session     Exercise  Colab    Solutions  Colab                                                                   Getting Started  Google Colab and Logistics    Exercise            Session 1  Pytorch  Automatic Differentiation  Neural Nets    Exercise             Solutions             Session 2  Training Deep Neural Networks    Exercise             Solutions             Session 3  Convolutional Neural Networks    Exercise             Solutions             Project  The Seismic NIST Dataset    Dataset     Benchmark    Neural Rock   Do Machines See Rocks Like Geologists Do    Authors   Gregor Baechle    George Ghon    Lukas Mosser   Carbonate Complexities Group   2020   Introduction   This project aims to investigate the ability of neural networks to classify carbonate rocks from thin sections for  various carbonate classification schemes  More importantly  we seek to understand whether neural networks use similar visual and textural features to classify each image     To investigate this we use the Gradient Class Activation Maps  GradCAM   Distill Pub Article  to show highlighting features where a neural network is  looking  in an image to make its decision on which carbonate class to predict    These class activation maps are dependent on the architecture and weights of a model and we therefore provide  here pre trained models and code infrastructure to train various convolutional networks and a viewer application  to visualize the CAM maps and the predictions of each network    Due to the extremely small dataset of  80 images  a transfer learning approach was used to train ImageNet pretrained models  Because each image has dimensions of   3000 x 2000 pixels  we randomly extract patches at 224x224 pixels and apply feature preserving data augmentation to regularize model training and to  hopefully  prevent overfitting  Regardless  results should be evaluated on the test splits of the datasets  indicated in the viewer app for each model      Network types   We provide pretrained ResNet18 and VGG11 models that either use ImageNet pretrained activations in the feature  extractor or have been fine tuned by training of the feature extractor with a very small learning rate    Neural Rock Application   We provide a viewer application that allows inspection and visualization of the results  To run the application first  install  Docker   and  Docker Compose     Once finished start the application by calling   bash docker compose up  d  and navigating to the viewer at  localhost viewer     You should be greeted by the following interface      Here you can switch between different carbonate classification schemes  Labselset Name   different CNN architectures  Model Selector   whether to use a frozen or a trained feature extractor   Frozen Selector   and the network layer to visualize for CAM maps  Network Layer Number   You can select the class you want to activate the network  Class Name   and finally a selection of all the images in the dataset with an indication on whether they were used in the training set  or not  as well as their  ground truth label  as identified by a carbonate geologist    A histogram of the predictions for the network is given below      Once you have finished working with the application you can shut down the Docker container   bash docker compose down   If you wish to inspect the logs while the application is running run in a terminal   bash docker compose logs  t  f  which will show you a running log of the application status while you work with it    The viewer builds on  Panel    Holoviews   and  Bokeh   Docker Images are hosted on  Dockerhub     API Specification   Interested viewers can also access the api that runs behind the scenes to serve model predictions  Navigate in your browser to  http   localhost 8000 docs  once the app is  running to see the OpenAPI specification  The API is built on  FastAPI   Deploying on AWS   For sharing we have used  Hashicorp Terraform  to provide an Infrastructure as Code that will  deploy a worker on AWS EC2  This allows us to reduce manual work for spinning up a machine that serves the model   Integrating an  Ansible Playbook  could be considered future work    Model Training   To load a notebook for training a model in Google Colab  follow this link      Update May 2021   There seems to be an issue with Colab crashing due to incompatibility with Pytorch Lightning Training of individual models can also be performed via  train train py   We have made use of  Weights And Biases  to organize all our ML experiments  The dashboard for all model training runs executed as a sweep can be found  here       To make training on google colab efficient we preload the entire dataset onto the GPU as to keep hard disk and cloud storage latency to a minimum    Dataset and Weights   The dataset and model weights will be released in the coming weeks  but are included in the docker images  so  you are ready to run if you wish to play with the application locally    Dataset Augmentation   We make use of heavy dataset augmentation to ensure the network focuses on textural features  We therefore perform colorspace jittering in HSV space as a data augmentation  Here a batch of images as the network sees them at training time     During prediction time we only crop and resize the images  but do not perform any color jittering as to preserve the image dataset color distribution  Here a batch of images as seen during the validation step     Future Work   Some initial testing has been done to incorporate  Captum  to provide other model interpretability methods for CNNs  but there is no time left in the project to implement this currently    In terms of deployment  there is much room for improvement as the application does not  scale  currently   The process of building a well scaling application that builds on machine learning beyond the  How to deploy your sklearn model on AWS Lambda Tutorial   is a non trivial task especially if you can t make use of good inference libraries that take care of a lot of that work for you  In our case  RAM requirements are quite high due to the need for backpropagation to obtain the CAM maps  That puts special burden on deployment infrastructure   Nevertheless  one could design a better system to scale out the API using AWS ECS or similar approaches  maybe even Lambda type functions   Definitely something to learn for the future       Unit and Integration Tests  The cake was a lie      Data Acknowledgment   This research used samples and data provided by the following dissertation    Baechle  Gregor  2009    Effects of pore structure on velocity and permeability in carbonate rocks  dissertation at Mathematisch Naturwissenschaftliche Fakult t of Eberhard Karls University Tuebingen  http   hdl handle net 10900 49698   The image data for the PhD has been acquired while conducting research at the University of Miami  Comparative Sedimentology Laboratory    Credit and Thanks   If you find this useful feel free to credit where appropriate   A detailed publication on the outcomes of our findings is in the works    We also wish to thank the organizers of the  Full Stack Deep Learning Course  for an excellent programme  and for providing an incentive to create and share this work     Libraries and Articles that have contributed to this repository    Full Stack Deep Learning    pytorch grad cam by JacobGil    Terraform with Nana    Distill Pub   Pytorch    Google Colab    Pytorch Lightning    Weights And Biases    Captum    Panel    Holoviews    Bokeh   PyWeka   Image Segmentation in the style of the famous ImageJ plugin  Interactive Weka Segmentation  A Github Pages template for academic websites  This was forked  then detached  by  Stuart Geiger  from the  Minimal Mistakes Jekyll Theme   which is   2016 Michael Rose and released under the MIT License  See LICENSE md    I think I ve got things running smoothly and fixed some major bugs  but feel free to file issues or make pull requests if you want to improve the generic template   theme    Note  if you are using this repo and now get a notification about a security vulnerability  delete the Gemfile lock file    Instructions     Register a GitHub account if you don t have one and confirm your e mail  required     Fork  this repository  by clicking the  fork  button in the top right     Go to the repository s settings  rightmost item in the tabs that start with  Code   should be below  Unwatch    Rename the repository   your GitHub username  github io   which will also be your website s URL    Set site wide configuration and create content   metadata  see below    also see  this set of diffs  showing what files were changed to set up  an example site  for a user with the username  getorg testacct     Upload any files  like PDFs   zip files  etc   to the files  directory  They will appear at https    your GitHub username  github io files example pdf      Check status by going to the repository settings  in the  GitHub pages  section    Optional  Use the Jupyter notebooks or python scripts in the  markdown generator  folder to generate markdown files for publications and talks from a TSV file      See more info at https   academicpages github io    To run locally  not on GitHub Pages  to serve on your own computer      Clone the repository and made updates as detailed above   Make sure you have ruby dev  bundler  and nodejs installed   sudo apt install ruby dev ruby bundler nodejs   Run  bundle clean  to clean up the directory  no need to run    force     Run  bundle install  to install ruby dependencies  If you get errors  delete Gemfile lock and try again    Run  bundle exec jekyll liveserve  to generate the HTML and serve it from  localhost 4000  the local server will automatically rebuild and refresh the pages on change      Changelog    bugfixes and enhancements   There is one logistical issue with a ready to fork template theme like academic pages that makes it a little tricky to get bug fixes and updates to the core theme  If you fork this repository  customize it  then pull again  you ll probably get merge conflicts  If you want to save your various  yml configuration files and markdown files  you can delete the repository and fork it again  Or you can manually patch     To support this  all changes to the underlying code appear as a closed issue with the tag  code change     get the list  here   Each issue thread includes a comment linking to the single commit or a diff across multiple commits  so those with forked repositories can easily identify what they need to patch  Digital Rocks Data   Make downloading and using Digital Rock Data great again      Quick Start   Installation from Github using pip   bash pip install drd   Loading an Image Dataset      python from drd datasets eleven sandstones import load eleven sandstones   xarray DataArray with image data   img   load eleven sandstones  Berea    Berea 2d25um grayscale raw      Plot average over z dimension   img mean dim  z   plot         About   Digital Rock Images are three dimensional datasets of rocks and other porous media   These are typically acquired using three dimensional imaging techniques such as  Micro Computer Tomography  MicroCT        They represent a rich dataset that form a basis for characterization of physical processes involving porous media      Purpose of the  drd  library   Digital Rock Images are scattered throughout the web on various hosting sites such as the  Digital Rocks Portal    Zenodo   or university specific sites  This library aims to make downloading these datasets easy through a python interface so they can be used in automated image processing workflows   reproducible research  or data science and machine learning worfklows      Furthermore  these images are associated with metadata about their spatial dimensions which should be considered when loading these image datasets   The library therefore requires these metadata to be available and creates an  xarray  DataArray which can keep spatial scale information when loading an image dataset      Each dataset is linked in this library i e  no hosting is done by the library itself      Available Datasets   Digital Rocks Portal      Eleven Sandstones Dataset        Berea       Bandera Brown       Bandera Gray       Bentheimer       Berea Sister Gray       Berea Upper Gray       Buff Berea       Castle Gate       Kirby       Leopard       Parker   Imperial College London     MicroCT Images of Sandstones and Carbonates 2015    MicroCT Images of Sandstones and Carbonates 2009   Contributing   Authors are encouraged to contribute their own datasets using the correct metadata   See  drd datasets eleven sandstones py  for an example implementation   Please add corresponding tests and an example to your pull request      Creation   This package was created during the Transform 22 software sprint  Order Agnostic Autoregressive Diffusion Models for Geostatistical Applications   Reference     A live version of this is released on  Curvenote Curvespace  under  this link     Introduction   This is a short introduction to the reasoning behind this work    The introductory notebook provides a full length description and implementation of the methods    Introductory Notebook     Geostatistical Modeling   Geostatistical models are critical for applications such as mineral resource estimation   storage modeling of CO2  and many other geospatial tasks    Sequential indicator simulation  SIS   See  Gomez Hernandez Srivastasa  2021  for an excellent review  is an autoregressive model for categorical  properties that has found widespread adoption due to its flexibility and ability to incorporate existing data    These features make SIS able to generate stochastic realizations honoring existing observations     Deep  Autoregressive Generative Models   Autoregressive models  See  Kevin Patrick Murphy  s new book  Probabilistic Machine Learning  Advanced Topics  chapter 22 for an introduction  using  deep  neural networks have shown a large potential to represent complex data distributions  In many cases  so called causal convolutions require data to be generated in very specific patterns  top down  left to right  which does not allow for sampling of realizations with conditioning data at various locations    A recent method called order agnostic  autoregressive diffusion models by Hoogeboom et al   allows for arbitrary ordering of the generation steps     New possibilities for geostatistical modeling with  deep  generative models   This opens up the ability to incorporate spatially distributed conditioning data to generate geostatistical realizations that honor data     Furthermore  the model parameterizes a categorical distribution which allows us to directly compute the  entropy i e  uncertainty distribution given the conditioning data    I hope that these connections between  deep  autoregressive models and sequential geostatistical methods also interest the reader  and spurns further research at the intersection between the fields of geostatistics and machine learning    Disclaimer and a note on publishing   Right now this is a few notebooks  some code  and some models  I do not have funding necessary to publish in a proper journal  but may consider publishing through  Curvenote       Please if you find this useful or interesting do consider referencing the repository anyway    As such this article is not peer reviewed  but I am happy to receive comments and will acknowledge these    Models have been trained on my own cost via  Google Colab Pro    Models are hosted on  hug face   Huggingface  Model Repositories and Monitoring was done with  Weights Biases     Installation   Installation can be performed via pip   bash pip install git https   github com LukasMosser order agnostic diffusion geostats main   Demos   The following demos are available and deployed on Huggingface Spaces      Description                       Demo Link                                                                                                                                                                                                                       Conditional Channels Generation    Huggingface Spaces Link      Conditional MNIST Generation       Huggingface Spaces Link         Models   A few pre trained models are available via huggingface model repositories       Model Description              Huggingface Model Hub Link                                                                         Weights   Biases Logging Run                                                                                                                                                                                                                                                                                                                                                                                                                                                Channels Dataset at 64x64 px    Huggingface Model Hub Link                Weights   Biases Monitoring       MNIST Dataset at 32x32 px       Huggingface Model Hub Link                Weights   Biases Monitoring                  Notebooks   These notebooks are intended to be run on Google Colab      Description                                                  Google Colab Link                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Introduction and Walkthrough  Start Here                           Train MNIST and Channel Models with Google Colab                   Conditional Channel Image Generation on Google Colab        Log Likelihood Evaluation Demo        Acknowledgments   I would like to thank Emiel Hoogeboom   Website     Twitter   for clarifications via email on understanding the methodology of the ARDM approach    There exists an excellent official implementation by Emiel and his co authors here   Official Implementation Github   Furthermore  thanks to  Eric Laloy  and colleagues for making their  channel training image  available online    Finally  a huge thanks to the ML community for making available libraries such as  hug face  huggingface hub  diffusers  accelerate  pytorch  and many more without this work couldn t exist    Please consider citing their work and providing proper attribution    Reference   If you ve found this useful please consider referencing this repository in your own work   software lukas mosser 2022 6961205    author          Lukas Mosser     title            LukasMosser order  agnostic  diffusion  geostats                      Initial Release      month          aug    year           2022    publisher       Zenodo     version         v0 0 2  zenodo     doi             10 5281 zenodo 6961205     url             https   doi org 10 5281 zenodo 6961205      License   Apache License                            Version 2 0  January 2004                         http   www apache org licenses  Tutorial on using GANs for finding probabilistic solutions to an ill posed inverse problem   Lukas Mosser   2022   Curvenote Page Rendered Here   Description   A short tutorial on using a GAN to find solutions to an ill posed inverse problem    The inverse problem is taken from the CT imaging field and uses a radon transform to emulate imaging in a CT scan     The code for the forward problem is provided through the library  torch radon    This library also allows for computing a gradient of the likelihood with respect to the underlying image    The image space is represented by a pre trained GAN  either from MNIST or the Channels dataset     The solutions found here are not Bayesian as we find different solutions around the MAP using a gradient based approach     Nevertheless  the tutorial aims to give a self contained example of how to combine a GAN with a forward problem to find solutions to an ill posed inverse problem    The examples with either alot of noise  or few projections do not allow for the geometry of the ground truth number or channel system to be determined   The solutions obtained will then have a variety of solutions as many different geometries maximize the a posteriori probability    Try it out here
66,esafak,niML   niML is eventually going to be a machine learning library for nimrod  Currently it only provides numerical optimization functionality    Example   Let s find the minimum of the function y x     x x 0  2 for x 0    2 1  using gradient descent and Newton s method  For this we will have to pass the gradient and Hessian of the function       nimrod import niml opt  niml vec   echo opt gd proc x  array 2  float     auto   2  x  2 0  1 0     1 0  0 0   echo opt nm proc x  array 2  float     auto   2  x  2 0  1 0       proc x  array 2  float     auto   array  2 0  2 0     1 0  0 0    The result is    1 9999976054757174e 00  9 9999760547571737e 01    2 0000000000000000e 00  1 0000000000000000e 00        To do roadmap   There s obviously a lot to do  and you can help      Comment the code    Write unit tests    Create a package    File bug reports    Write ML routines    Add multi threading    Integrate  PLASMA   MAGMA       License   This software is released under the  GNU GPL v2 0  license
67,kokes,Just for nostalgic purposes  I created this at a time when I still coded sites  but interacted quite heavily with non technical people higher up  Their requirements  often bypassing designers  were of the kind  can you move this ten pixels to the right   and there was a lot of iteration  I thought it d be handy to have a tool that would allow anyone to tinker with basic CSS without imposing any limits on the developer or the user  This predates preprocessors shenanigans and other recent fades in the coding sphere  as is obvious from the technical specs    Check the  original README  for more info  This is here just for nostalgic reasons  The original goal was to bring data analysis to the browser while still retaining usability on the desktop through Node  I never had time make anything of it  but hey  it was fun    dtape   Just playing at this point   make    node dtape js experiments with the  feather file format   so far only the go folder has anything in it Struktur ln  fondy   Loni jsem  se hrabal  v datech od MMR  konkr tn  v tabulce projekt  za obdob  2007 2013  A jeliko  to  nikoho moc nezaj malo   rozhodl jsem se to ud lat znovu a po  dn ji    V e je uvedn  je na z klad   dat na webu MMR   Rozd l od lo sk ch hr tek je v tom   e MMR v mezi ase dodalo mnohem  ist   tabulku  D  ve  lov k musel p rovat projekty na vl dn  instituce  NACE kategorie atd  Te  to za n s ud lalo ministerstvo  tak e m  eme stratifikaci prohl  et mnohem snadn ji    Pro je t  lep   kategorizaci jsem sehnal data ze syst mu  ARES MF R   kterou pr v  p ruji s daty MMR  Konkr tn  jde o data z RES  Registr ekonomick ch subjekt     asem nejsp   dod m napojen  na Obchodn  rejst  k    Jsou vesm s t i m sta  kam se budete cht t pod vat      V t ina  ist n  a anal z tady je ve form  tzv   Jupyter Notebook    uvid te je jako soubory s koncovkou   ipynb     Ve slo ce  vstupy  uvid te  z  eho se to tu skl d   Jsou tam zdrojov  data od MMR  p r zdroj  z MF R a tak    Ve slo ce  vystupy  jsou v echny mo n  v stupy z Jupyter Notebook   jde o zpracovan  data v trochu fin ln j   podob       St  nosti  do mailu  nebo na  twitteru   N vrhy na nov  vstupy a v stupy samoz ejm  pos lejte takt    The Guardian published a set of  programming tests  it uses in their engineer recruiting  I thought they were quite intriguing  so I gave them a go  I did not lookup any patterns or pre existing solutions to these problems  I m just attacking it as I know best    Just so you know  I don t have a compsci background  so the algorithms probably won t be ideal in places  Also  it s not always clear what the constraints are   ie  what the cpu disk ram network situation is  if we re doing cold or warm starts etc  But hey  I m just playing around    All the problem sets inherit the license from the original repo  My code carries no license  you re free to do whatever you want with it  Stenoprotokoly z PSP  R     elem tohodle repa je uk zat si  jak rychle a snadno p eklopit stenoprotokoly ze Sn movny do hledac ho enginu  Z nuly m  ete m t pln  funk n  fulltextov  hled n  b hem 10 minut    Zat m to funguje jen pro nejnov j   data ze sou asn  Sn movny a jen na jednom vyhled vac m enginu  ale oboj  jde snadno zm nit    Instalace   P edpokl d m ur itou technickou znalost  kdybyste m li n jak  konkr tn  dotazy  pi t  mi  e maily  nebo  tweety   Ide ln  v t   z ludnosti ne    co je Solr     ale zas ne n jak  zv rstva  jsem p eci jen ekonom       Jak jsem psal  je to postaven  na konkr tn  hledac  technologii  konkr tn  jde o  Solr   co  je engine predatuj c  Elastic  dne n  hip hledadlo  funguje p kn  a rychle  ale j  si ho vybral  proto e se mi jm no l bilo v c jak Elastic  Uk  eme si ale oba p  stupy    Na pozad  si tedy dejte instalovat Solr  J  jsem d t  Macintosh   tak e pro m  to je  brew install solr   pro ostatn  syst my dohledejte informace na webu projektu  P edpokl d m   e Python m te  bude sta it z kladn  sada knihoven    lxml     Cel  proces jak se dostat od zdrojov ch dat a  po zpracovan  projevy  je popsan   zvl    v notebooku     Hled n    R d si nech m poradit  s m hled m teprv p r dn   Nasypal jsem data do Solru i Elastiku  oboj  funguje kr lovsky  Solr jsem naplnil p es dva p  kazy  nap ed jsem mazal p vodn  data       solr delete  c steno   solr create core  c steno   post json  c steno    Hotovo  v z kladu m te i prohl  edlo  tak e hur  na  localhost 8983 solr steno browse  a m  ete hledat  M  ete i snadno p idat facety  tedy agregace podle jednotliv ch polo ek  P es  facet field  m  ete snadno filtrovat v levy o Hitlerovi podle  e n ka a nebo sch ze      Par da  m te v z sad  hotovo  Solr funguje p es HTTP API  tak e ho m  ete napojit na svoji aplikaci bez n jak ch v t  ch pot       Elasticsearch   Pokud byste cht li data nahodit do Elastiku  tak je to samoz ejm  mo n   J  ud lal n sleduj c  pomoc   ofici ln ho bal ku        python import json import glob from elasticsearch import Elasticsearch   fns   glob glob  json   json     es   Elasticsearch     for fn in fns      print fn      with open fn  as f          docs   json load f      for d in docs          res   es index index  steno   doc type  projev   id d  id    body d    res   es search index  steno   q  Bohuslav Sobotka   res  hits    total         Asi to bude cht t pou  t bulk m d v subbal k   helpers   ale to u  si porad te      Hotovo  tolik asi k tomu  Prohrab registrem smluv   Stahov ni   stahuj py    konverze do CSV   tabulkuj py    pak tu bude p r dokument  s n jak m pr chodem t hle dat  Data z Port lu ve ejn  spr vy  P r z kladn ch informac       predatuje sou asn   Registr smluv   obsahuje 50 tis c smluv za dohromady 100 miliard   kontraktor  je cca 12 tis c  m te tu i informace z Registru ekonomick ch subjekt   den vzniku z niku  NACE  ESA k d         vkladatel   objednatel   bohu el nejsou pln  identifikov n    d vod je ten   e sice je u vkladatel  uvedena datov  schr nka  ale nepoda ilo se mi ji namapovat na I  u v ech zadavatel   A to zejm na proto e vkl daly odbory v r mci zadavatel    a ty maj  svoje vlastn  DS  Ve ejn  datab ze DS  zd  se  obsahuje pouze DS ve ejn ch subjekt  jako celk   Kdyby to um l n kdo napravit  dejte mi v d t    data jsou poskytovan   as is   nen  tam   dn  deduplikace   i t n  I   nic   kdybyste si data cht li stahovat sami  n sledujte skripty tady a adaptujte si je dle pot eby   j  p vodn  XML neskladoval  proto e sou  st  t ch soubor  byly i p  lohy  tak e to  lo do des tek gigabajt    v echny smlouvy jsou ve  vystupy smlouvy csv   p vodn  XML konvertovan  do JSONu je ve  vstupy   client side rendering of jupyter notebooks   tl dr  Render Jupyter notebooks straight in the browser  without a back end converter  Can be used as a library  Or if you re on macOS  you can even fire it up in Quick Look  see  ipynb quicklook     I often want to read through my Jupyter notebooks  but I rarely have my Jupyter instances running in the right folders  I can t quite use the  online nbviewer   because I don t have a public URL for these  so I resort to running dummy Jupyter instances or uploading my file as a one time gist on Github  one I have to delete thereafter   One last possibility is  nbconvert  in the command line    I thought it could be easier and more lightweight  So I hacked together this client side rendering of Jupyter notebooks  All you need is a browser that renders an HTML file  dependent on some JavaScript and CSS  both of which can be inlined   You simply drag and drop an   ipynb  file and it renders  It s rather fast and it supports most notebook features  I tried supporting the previous version  v3   since there are tons of examples in this version out there    Try a live demo   NEW  Rendering Github notebooks   You can now render notebooks hosted on Github  You can copy and paste their URL in the viewer  linked above  or you can save this following link as a bookmark    javascript  function   location href  https   kokes github io nbviewer js viewer html   btoa location href          Clicking this while on Github  looking at a notebook  will launch our nbviewer with this notebook rendered here instead  You ll also get a permanent link for you to share    Usage   There are two ways one can use this  You can use the library itself  there is just a single public method  you call  nbv render data  target    where  data  is the JSON representation of your Jupyter notebook and  target  is the node where the notebook is to be rendered    Or you can use  the demo   or a local copy   which is just a simple wrapper of the library  with dropzones and other basic features  There is no data being transferred anywhere  so feel free to bookmark it and use it    Tech details   It s rather simple at this point  all the DOM manipulation is written in vanilla JavaScript  Markdown rendering goes through  marked js   syntax highlighting is administered by  Prism js   The example implementation leverages a few goodies from modern web design  like File API or drag drops  so a fairly modern browser is necessary    Showcase     Contact   Drop me an email  ondrej kokes gmail com  or tweet at me    pndrej   if you have any questions or suggestions  Contributions welcome  P vodn  se tu skladovaly skripty na zpracov n  datab ze CEDR  kdy  je t  vdata byla tzv  linked data  kter  ne ka d  um l zpracovat  MV R v mezi ase vydalo datasety v CSV  tak e je m stn  postup zbyte n   m  ete j t rovnou ke zpracov n  v dal  ch n stroj ch  J   zvl    v repu  ukazuju  jak na to  Hodn  z bavy      MF R nab z  kompletn  datab zi dotac   CEDR  je ale pon kud slo it  na technick  a metodologick  zpracov n   Zde je rychl  p ehled jak z N3 dump  vytvo it CSV soubory  kter  jde d l jednodu e zpracovat    Prvn  je t eba st hnout v echny archivy s daty  kr tk  popis je ve slo ce  stahuj   Pro pohodlnost jsem v echno p epak  oval do  tar gz   ale pokud m te dost m sta  doporu uju rozbalit a pracovat s texty samotn mi  krapet to zrychl  parsov n   jde ale o asi 30 GB dat     Data samotn  jsou ulo en  jako klasick  trojvlastnosti  p edm t  predik t  objekt   kter  budeme parsovat a kategorizovat podle p edm tu  u dotac  je p edm t dotace  u rozhodnut  to je rozhodnut  atd    Probl mem p i tomto parsov n  je   e jedna z vlastnost   sloupc  v na em CSV  m  e b t a  na konci cel ho dumpu  tak e mus me celkem hodn  informac  dr et v pam ti  ne  se n m sejdou v echny vlastnosti    Ka dop dn  je t eba m t v hlav  Obr zek 3 ze strany 12 v  technick  dokumentaci CEDR   to je naprosto kl  ov  pro pochopen  vztah  mezi datasety  My budeme exportovat dva typy CSV soubor   Data samotn  a pak vztahov  soubory   kde je napojen  jednotliv ch tabulek mezi sebou  Jakmile vyexportujete jednotliv  tabulky z  n3  do  csv   bude t eba je spojovat  na z klad  jejich unik tn ch identifik tor    to u  nech m na v s  ale t eba pomoc  knihovny  pandas  to je trivi ln     P r informac  bokem      Vlastnosti v dan ch tabulk ch nejsou v dy kompletn    tj  n kter  vlastnosti nejsou p  tomny pro v echny   dky  Nap   je 1577627 dotac   ale jen 294098 m  n zev projektu  Podrobn  statistiky toho  co je a co nen   najdete  tady   J  vytahuji jen n kolik m lo vlastnost   v t inou ty  kter  jsou ve v ech z znamech   je ale mo n  tento seznam sloupc  roz   it  sta   zm nit onen  ovl dac  soubor   B hem parsov n  se ukazuje  kolik   dk   n3  souboru se u  zpracovalo  U t ch v t  ch dataset  to jde do des tek milion   podrobn j     sla  tutaj     Cel  to b    na Pythonu 3 bez jak chkoliv extern ch knihoven  Na sta en  a konvertov n  p vodn ch dat budete pot ebovat n co jako  wget  a  arepack   Na Unixech to existuje b  n   na Windows sta   Cygwin  nebo ten nov  bash ve Windows 10     Nepou  v m RDF knihovny  proto e jsou tuze pomal   by  korektn j    a j  pot eboval n co podobn  rychl  jako IO    Ano  jde to exportovat pomoc   SPARQL  samotn ho  ale na to je pot eba b   c  SPARQL server  Ten od MF R n m tohle ned  a provozovat n co vlastn ho je pomalej   ne  pustit jeden kr tk  skript nad dumpem     asem nejsp   bude MF R poskytovat tyto CSV dumpy samo  do t  doby se tyhle k dy mohou hodit    Co se t  e licenc   tak dejme tomu  e voln   bez z ruk a atribu n   Tak e si to pou  vejte dle libosti  ale za nic nem  u a jestli z toho n co vznikne  tak nalinkujte tohle repo      St  nosti na   kondrej  nebo  email   Playing with the http stack in golang  mostly an educational thing   go run tapi go  and open the browser to localhost 7678   fix     open browser   embed HTML in Go code   proper httptrace timing   handle redirs   allow repeated keys  key   v1  v2  v3     send errors in JSON   syntax highlight JSON    pretty print HTML   tests using httptest   gzip compression   treat timeout properly   atob decoding utf8 problem   Investi n  pob dky   Na te  seznam investi n ch pob dek  a m rn  jej o ist   v sledek je v  data csv   Hlavn  dodan  informace jsou I O spole nost    ofici ln  datov  zdroj je obsahuje jen u asi 10   polo ek  Tady jsou dopln ny na z klad   slovn ku   Bylo by fajn  kdyby mi to n kdo p ekontroloval  je to dohledan  jen na z klad  n zvu a ob as okresu    P r v c  je t eba o et it     dodat  jak se ud lala tabulka se  z kladn m infem z RES   o et it m s c obdr en  pob dky   z textu ud lat   slo  ob as tam je cel  datum nav c   MV zve ejnilo spoustu stovek tis c majitel  datov ch schr nek  mezi nimi velkou   st soukrom ch dr itel   Vznikla tak nejsp   nejv t   datab ze pr vnick ch osob v   z kladn ch  daj   Jeliko  jde o velk  XML  napsal jsem si parsery na zpracov n  do CSV    Python  verze z vis  na  lxml  a standardn ch knihovn ch  Za b hu na  t  XSD specifikaci  tak e by m la b t rezilientn  v  i zm n m  Go verze m  zakodovanou sou asnou podobu specifikace  tak e se mo n  rozsype  Je ale n sobn  rychlej   ne  Python  verze  Vyb rej dle pot eby    Parsery nejsou n jak echt o et en   za spr vnost v stup  neru  m  Ministerstvo vnitra vydalo CEDR  registr dotac    jako otev en  data   tak e si je m  e kdokoliv zpracovat  Bohu el samotn  publikace nesta    je t eba data m rn  p edzpracovat  Pokud by se v m do toho necht lo  m m tu sadu shell skript   kter  tu n mahu ud laj  za v s a vy skon  te s datasetem v maxim ln  neoptimalizovan  sad  tabulek v postgresql    Pot ebovat budete konzoli   wget    7z    gzip    awk    sed  a  pg  samotnou  Kdybyste si to cht li uzp sobit pro vlastn  rela n  datab zi  sta   upravit  pq sql  a dan  nahr vac  mechanismus v  load sh     Hodn  z bavy p eju  V clav ky   V m di ch se nez  dka kdy objev  v rok typu  zni en   roda je velk  jako sedm V clavsk ch n m st   nebo  za tolik pen z by se dalo koupit 200 f bi     a mn  p i lo   e tyto v roky jsou  asto trochu pah lovit    e si pod nima  lov k moc nep edstav   tak jsem se sna il vytvo it kalkula ku  kde by bylo v ce podobn ch v po t   aby si  lov k vybral  co mu je nejbli       Akceptujeme p  sp vky   a  u  formou issues nebo p  mo pull request   Sv  zm ny m  ete snadno otestovat lok ln   sta   repozit   vyklonovat a pustit p  mo v prohl  e i   nen  t eba   dn ch bundler   i instalac     Master v tev je viditeln  na  vaclavaky cz   taky m me kv z na  vaclavaky cz kviz html     V voj   Zp sob p id v n  nov ch p epo t  je v  pull request template md   co  se v m i uk  e  kdy  otev ete PR    Co se t  e testov n   m me tu trochu d evn  podobu  P ed za  tkem pr ce si pus e  node test js master   co  v m vytvo   master snapshot  Po zm n ch sta   pustit  node test js   bez argumentu   co  v m vytvo   nov  snapshot  Kdy  ty dva soubory porovn te  m ly by reflektovat jen va e zm ny   v p  pad  nap   refaktorov n  lze tak snadno ov  it   e se nic nem n      Ve snapshotech nen  v e  je tam jen hlavn  funkce  kter  d l  drtivou v t inu pr ce   ARES open data   tl dr  Ve ejn  rejst  k je nov  publikov n jako jeden bal k  nen  nutn  stahovat ho po kusech z API  Je to ale st le XML  tak jsem to p ekonvertoval do CSV     vod   Sou  st  Administrativn ho registru ekonomick ch subjekt    ARES   je nov  i sekce  otev en ch dat   Prvotn  n hled dat jsem komentoval  na twitteru   tady je k d  kter m jsem vyextrahoval v t inu informac  ze zdrojov ch XML soubor  do pou iteln j  ho CSV    V echen k d je v jednom Python m souboru  pot ebujete z kladn  instalaci Pythonu 3 a bal k  lxml   V k du je t eba jen upravit slo ku se vstupy a v stupy hned naho e  pou ijte      pokud m te k d a data pohromad    zbytek by m l b  et jak je    Skript vytvo   t i soubory      udaje csv  se z kladn mi  daji o podnic ch    fosoby csv  s fyzick mi osobami  spole n ci  jednatel   prokuristi atd      posoby csv  s pr vnick mi osobami      Berte na pam     e a koliv poj c  sloupec mezi soubory je I O  nen  to v  udaje csv  sloupec s unik tn mi hodnotami  proto e podniky mohou m t od t pn  z vody  kter m jsem p i adil stejn  I O  trochu to pak d l  bordel p i kombinaci dat  to mus m je t  vy e it     Data konvertovan  berte jak jsou  neru  m bohu el za nic  budu ale r d za reporty jak chkoliv chyb     Data   Zat m jsem do dat p  li  nezabru oval  to teprve p ijde  Alespo  p r z kladn ch v c  k  prvn  verzi dat   prosinec 2017     Zdroje dat   Obchodn  rejst  k                           764708 Spolkov  rejst  k                           136783 Rejst  k spole enstv  vlastn k  jednotek     72964 Rejst  k obecn  prosp  n ch spole nost        3178 Nada n  rejst  k                              2708 Rejst  k  stav                                 888    Po et subjekt    98214 pr vnick ch osob  z toho 762575 st le existuj c ch    Anga ovan  osoby   4 7 milionu fyzick ch a 200 tis c pr vnick ch osob  resp  vztah   nejde o unik tn  osoby     Kuk tko   Trochu jsem to zautomatizoval  tak e sta   pustit  proc load sh   co  zpracuje data do CSV a z rove  nah z  data do SQLite datab ze  bin rka  sqlite3  pot eba   Pak tam je je t   serve py   co  v m na  http   localhost 8089  zobraz  prohl  e    video tutaj      Net eba nic krom  SQLite a Pythonu 3    dn  extern  knihovny to nepot ebuje  webappka je jeden HTML soubor bez jak chkoli z vislost   P e roubovat na Postgre to p jde snadno  krom  collate nocase  a   import  v  load sh  nepou  v m nic specifick ho pro SQLite    T eba dod lat   Je to zat m prvn  n st el  p r v c  chyb       vazby fyzick ch a pr vnick ch osob na od t pn  z vody nejsou jednozna n    vy lenit je ze seznamu firem pry    kam pak ale s jejich PO a FO    budem pak ud lat PK nad I O   p epsat info o form tech z twitteru sem   spousta firem  zejm  ministerstva  nen  v OR  ale m  ou b t vlastn ky   tak e ty by m lo j t dohledat   pole vznikFunkce  vznikClenstvi  ZpusobJednani   innosti   cross check proti  specce   co v bec d v  smysl p id vat  nap   specka m  elementy  PravniForma  nebo  StatusVerejneProspesnosti   ale v datech nejsou  stejn  tak data narozen   rodn    sla atd     zastoupen  pr vnick ch osob  zano en  element  Zastoupeni       Kontakt   St  nosti a tak sm  ujte na  email  nebo  twitter   A nebo rovnou pos lejte pull requesty  Reimplementation of Python s file IO  using SQLite for storage  This is  not  an OS level implementation like FUSE  this is just a Python library meant to simulate a file system    Why      A single database file is very portable  copying many small files may be a bit problematic when using external or network storage    Using a database will allow for a relatively simple versioning mechanism    Sometimes you want an in memory filesystem to mock stuff    Hey  it was fun      This is not about performance  it s performing fairly poorly at the moment   more about usability and portability    Usage   The library itself is meant to be single file  so that it s easy to embed  It s meant to be used just like  open  in core Python        with sqlfs fs  filesystem db   as fs      with fs open  foobar txt    w   as fl          fl write  this is my new content           fl seek 0          fl write  and overwriting     with sqlfs fs   memory    as fs      with fs open  foobar txt    w   as fl          fl write  my in memory filesystem          Implementation   It s fairly straightforward  each file is saved into one or more slices  its size is user defined   Say you define a slice to be 1024 bytes  a file of one megabyte will occupy a thousand slices  Each file could have been saved into a single blob in one row  but as the files should be seekable and appendable  I didn t want to rewrite one giant blob when a single byte is changed    The whole thing is in Python 3  there are zero dependencies beyond the standard library  This way it s easily embedded  Tady najdete  naprosto neofici ln  kv z  na z klad  v rok  ze serveru  Demagog cz   M  ete si otestovat  jak moc dob e byste zvl dali fact check politick ch diskus     Data jsou sta en  krapet na hulv ta  tak e bych poprosil o nereplikov n  t to   sti moj  pr ce   commitnul jsem i raw data  tak m  ete pou  t ty  Stahovac  skripty jsou zde sp   pro  plnost  Lep   zp sob z sk n  dat by bylo po  dat lidi z Demagoga p  mo  to se st le nab z   Ned l m si samoz ejm  n rok na   dn  obsah  kter  jsem z jejich serveru z skal  Je to v e jen za   elem vytvo en  tohoto kv zu    V ka d m p  pad  doporu uji zv  it p isp n  serveru Demagog cz   na jejich webu  najdete informace o jejich financov n  spole n  s informac  o tom  jak m  ete p isp t i vy      Kv z je tutaj   Z technick ho hlediska to nen  nic velk ho  Jde o jednoduch  skript  kter  l n  na  t  partitionovan  data pomoc  HTTP requestu  tak e je to 100  klientsk    V Local Storage ukl d  vid n  v roky  aby se  lov ku nezobrazoval ten sam  znova  Je tam n jak  z kladn  nastaven   Pou  v  to jen a pouze  ist  JavaScript  nen  to zbabelovan   tak e to na spoust  konfigurac  nepob     prost  takov  dom c  sl tanina    Co by cht lo dod lat      P id n  data vy  en  ka d ho v roku   ob as je to nutn  pro pochopen  souvislost    bohu el toto datum nen  ve v pisu v rok   musel bych d lat request na ka d  v rok zvl     co  by nebylo moc  etrn   Proto  ek m na dump p  mo z datab ze    Filtrov n  na z klad  autora v roku   k d je vesm s hotov   a zakomentovan    ale nar    to na nedokonal  na  t n  dat  asynchronost v synchronn m k du   ale i kdyby to fungovalo  tak to bude pomal   tak e by to cht lo sp   p epartitionovat vedle podle autor   resp  stran     Opravit na  t n  dat  viz v  e     Opravit relativn  odkazy ve vysv tlen  hodnocen    ob as je vlo en obr zek  kter  m  adresu   data img super graf png   kter  ale logicky nem m u sebe   Proto by bylo dobr  proj t ten HTML stub a v echny relativn  adresy nahradit za absolutn       St  nosti  v tky  probl my a jak koliv dal   emoce m  ete sd let na mojem  e mailu  nebo  twitteru   tl dr  When trying to replicate a complex project  don t think it s just about recreating the visible API and calling it a day  Most of the logic is underneath  in unexported fields and methods  in inherited logic and careful optimisations  But hey  it s fun even when you fail miserably    We all have some pet projects that we hope to finish some day  This one was a bit odd in that I kinda knew that I would fail eventually  But I had tons of fun working on this anyway  This is a sort of  lessons learned  document    The premise was fairly innocuous   I would work with  pandas  on a daily basis for many years  but its fairly heavyweight nature would bite me from time to time  While it s trivial to install on a desktop  it would sometimes be problematic on remote servers  in minimal Docker images  on ARM etc  I wondered if there was a way to overcome this    Now  pandas is not really that heavyweight  The number of required dependencies is fairly short  with  numpy  being the biggest one  and fairly essential one  as it is used for storage   But once you replace the storage engine  you can solve most of the remaining problems with pure Python  pandas uses Cython in quite a few places for performance reasons   which is fairly well equipped  we re talking  csv    json    urllib    itertools    gzip  and other goodies in the standard library  And me being quite a fan of pure Python solutions  I decided to give this   pandas in pure Python and  what the heck  in a single file   project a go    The idea was that you d  curl  a single file from GitHub and off you d go  I imagined that a small subset of pandas  API would serve one well  Now let s see what we re up against          git clone   depth 1 https   github com pandas dev pandas   loc pandas     Language             Files        Lines        Blank      Comment         Code   Python                 589       332060        63990        21213       246857  reStructuredText        45        39548        11076            0        28472  HTML                    16        18977         2323          201        16453  Plain Text              49        17088         4879            0        12209  C                       11        12234         1720         1223         9291  Autoconf                13         5206          974          250         3982  C C   Header            28         5293          845         1033         3415  Bourne Shell            32          974          230          112          632  Markdown                 6          352           72            0          280  YAML                     8          372           51           43          278  JSON                     3          128           15            0          113  Batch                    4          103            6            0           97  INI                      1           82           11            0           71  Makefile                 2           38           10            0           28  Toml                     1            9            0            0            9  ASP NET                  2            2            0            0            2     Total                  810       432466        86202        24075       322189         Yikes  luckily we re not rebuilding all of pandas  just some of its features  It took me maybe a couple dozen hours to get something together  here s what I learned    Don t start with the high level stuff   How do people get data into pandas  Quite a few use  read csv     so I started with that  notice it s no longer in the code here   After building it for a few hours  adding compression support  HTTP handling  tests      I realised I didn t have any data structures to save the data into  I hadn t built any DataFrames or Series  This was a very early error    If you re recreating the visible API  you will fail   This is a big one    I built a constructor for a  Series  object and then a few methods  After implementing my favourite ones  I started remembering what other ones there were  So I went to the documentation and felt like parsing it    then it dawned on me   I can use  dir     The premise was that you create an object and then call  dir  on it to get all the methods it implements  So I d do things like  pd Series list  abc    str   dir      or  pd to datetime pd Series   2017 11 20     dt   dir      and I d get all the methods that needed implementing  But that turned out to be a very wrong approach    If you try and recreate everything that s visible  you will fail to capture the essence of productive programming   inheritance  or reuse  depending on context   A whole host of methods were meant to be implemented for  Series   or even  Index    and then reused in  DataFrame s and I wasn t capturing this logic    Pandas  API surface isn t giant because there are a lot of methods implemented  well  there are  but way fewer than one might think   but mainly because it leverages commonalities between different objects that share some properties    Python s STL is amazing   OK  I already knew that going into this  but working with it only reaffirmed this simple fact  I already listed some of the libraries above  but then there is  unittest    datetime   the underused  functools  etc  Whenever I teach someone Python  I urge them to give the standard library a go first  You get great mileage out of that built in beast and you spare yourself a world of pain when porting your code or maitaining it as your random libraries change their APIs willy nilly    There are more magic methods than I knew   I knew a few  like    getitem    or    len     but I got to know about love    repr     a suite of algebraic methods     add        div    and the like   and odd ones like    invert    or    contains       Utilising magic methods allows for hiding a lot of logic that would otherwise go into named methods and awkward APIs  Just imagine having to do  df add dd   instead of plain  df   dd   which is fairly easy to implement  It goes beyond just         you can implement    lt        gt    etc  to evaluate comparisons  methods for bit shifting  binary operators  there s    del    to handle object deletion   e g  upon garbage collection when a variable is redefined or simply when a program exits  though be careful  builtins are deleted before    del    gets triggered  so you may want to use    exit    in a  with  block instead     Anywho  magic methods are excellent and you should learn more about them  see e g   this epic post     Hard to view  copies in most places   Being quite used to manage some of my memory in Go  be it by pointers or slices  sort of views of underlying arrays   This is done in pandas by utilising numpy storage  which  in turns  uses dense byte arrays and views over chunks of them    Sadly  because I was working on a pure Python solution  I could mimick this only by using  bytearray   but that would require a lot of code around it to coerce data back and forth  In the end  I decided to go the memory inefficient  but very usable and friendly way   by just using lists of Python primitives    Composability is key   While it was not trivial to get the first iteration out the door  I was amazed by how much I could add by writing very little code  I had an internal   apply  method on Series  which I would then feed a bunch of Python s built in string methods to generate much of pandas    str  API  Same goes for   dt  and the  datetime  library    Purity calls for cautious testing   Building something using just Python s standard library  I had to ensure that the code was not in any way inferered with and my age old desktop installation of Python was the obvious culprit  A simple solution would be a virtual environment  but  for some reason  I have never been a fan  Wanting a bit more isolation without much work  I opted for Docker  Sure  it s a heavy dependency  but it was only optional  I built the   test py  file to be a self contained program  which allowed for argument to be passed  so you could just test the thing using your existing installation or in Docker  and virtual environments were planned     I opted for one of the simplest distros  I went with  alpine   By default  it weighs just a couple megs  when bundled with Python  it stops somewhere shy of 100 megs  Not great  but you can t get it much lower than that    The resulting command was very basic  Just run the base  python alpine  image  inject my code into it and run the   test py  file and they self destruct  Easy peasy and it runs in seconds    docker run  it   rm  v   PWD   usr src upandas  w  usr src upandas python alpine python upandas test py local   Other tidbits     devdocs io  are great  they are a  blazing fast  source of documentation that I reviewed constantly    Custom exceptions are cool  Sure  I used a lot of  NotImplementedError   but then I wrapped this around into  WontImplement  to signify some parts of the API there just not possible to recreate  In general  I prefer  NotImplementedError  over a bunch of TODOs  because at least the program crashes    Chaining is awesome  Every developer using pandas knows this  you can call things like  pd read csv        loc      head   T drop       and you can just keep going  This can be easily implemented by just returning the object back upon each method call     esk  otev en  data   C lem tohoto repozit  e je sjednotit p  stup k otev en m dat m  kter  se t kaj  finan n ch v daj  st tu  Jde n m o shrom  d n  dat v konzistentn m form tu a p id n    seln k   kter  rozkl  uj  vztahy st tu a firem  p  padn  organiza n ch slo ek st tu    P ed n kolika lety se st t za al v c a v c otev rat v  i ve ejnosti  ale  asto tak  inil pouze form ln   Do ve ejn  sf ry se dostala data  ale  asto ve form tech  patn  zpracovateln ch  s chybami  p  padn  oboj   Komunit  tak  asto trvalo dlouhou dobu  ne  data zpracovala  na n kter  datasety se v bec nedostalo    I kdy  prob hlo samotn  zpracov n    asto se nedostalo na kl  ovou   st   f zi jednotliv ch dataset   aby se na ve ejnost dostaly p edem nezn m  skute nosti  I to je jeden z   el  t to pr ce    A koliv se tu nenach z  p  li  mnoho k du  v  te   e tato pr ce stoj  na letech n mahy a komunikace s   ady  na nekone n ch diskus ch s lidmi ze v ech sf r st tn  spr vy i soukrom ch   astn k   I p es tyto n strahy se n m za ta l ta poda ilo vytvo it dialog mezi u ivateli a poskytovateli dat a jen douf me   e se n m poda   jej udr et      el repozit  e   Najdete zde n kolik sad k d   v tuto chv li nejsou n jak zna n  se  erovan   jde o sp  e samostatn  skripty   asem snad vymysl me n jakou koherentn  orchestraci      ely skript  budou zhruba n sleduj c       Sta en  dat   v t inou jsou definovan  adresy  odkud se daj   erstv  data strojov  st hnout  ob as je t eba ru n ho z sahu  zejm  u evropsk ch dotac  od MMR     Konverze dat a prvotn   i t n   Po tomto kroku bude ka d  dataset v CSV  kter  pak  lov k m  e nahr t prakticky kamkoliv  v etn  Excelu    Export dat z CSV do datab ze  v na em p  pad  PostgreSQL  M  ete si zvolit svoji vlastn   my jsme n   syst m index   datov ch verifikac  a dal  ch mechanism  postavili nad PostgreSQL      Posledn m aspektem je aktualizace dat  V tuto chv li m me hotov  jednor zov  nalit  v ech dat  postupn  prom  l me  jak by se ide ln  dal prov d t update    Ji  v tuto chv li ale funguje z kladn  princip tohoto projektu   jakmile  lov k dostane datasety do jednoho syst mu datab ze  m  e se dotazovat nap      nap  klad seznam zak zek pro firmu  kter  spl uje n jak  krit ria na z klad  informac  z ARES a je propojena s ur it mi politicky aktivn mi lidmi    Lok ln  spu t n      elem projektu je  aby se dal snadno pou  t nejen autorem  Pro z kladn  pou it  v m posta   Python  3 7   a nic jin ho  Sta   si nainstalovat p r z kladn ch z vislost  a m  ete data nahr t do CSV nebo i datab ze   podporovan  je SQLite  vestav n  do Pythonu  nebo PostgreSQL    sh python3  m venv  venv    venv bin activate pip3 install  r requirements txt python3 main py   all   partial   Tato sekvence p  kaz  nainstaluje pot ebn  z vislosti do virtu ln ho prost ed  a zpracuje v echna data do CSV    Selektivn  zpracov n  jde ud lat pomoc  specifikace datasetu jako pozi n ho argumentu   sh python3 main py   partial ares volby   A nahr n  do datab ze se   d  argumentem    connstring   P i specifikaci datab ze prob hne v e   sta en  dat  konverze do CSV a nahr n  do datab ze  Bez specifikace datab ze skon  te u CSV       sh   python3 main py   connstring sqlite    soubor db   partial ares volby python3 main py   connstring postgresql   localhost data   partial ares volby       N zev sch matu u Postgresu   i tabulky u SQLite lze prefixovat parametrem    schema prefix     sh python3 main py   connstring postgresql   localhost data   schema prefix opendata    partial ares volby   Spu t n  v Docker kontejneru   Pokud si nechcete lok lne instalovat Python  m  ete vyu  t p ilo en   Dockerfile   vytvo it si Docker image a spustit v e v kontejneru    V echna generovan  data v etn  sta en ch zdrojov ch soubor  se zapisuj  do slo ky   data  v kontejneru  Zpracovan  CSV v stupy najdete v   data csv     Tvorba Docker image   sh docker build  t kokes od     Spu t n    sh docker run  it   rm  v  PWD  data kokes od   partial ares volby   Dom nov  znalost a kvalita dat   Ne  se dostaneme k dataset m samotn m  je t eba zm nit kl  ov  p edpoklad pro spr vnou interpretaci dat  t m je dom nov  znalost  tedy pochopen  dan  problematiky na v cn   rovni  ne pouh  technick  zpracov n  dat     lov k mus  pochopit  pro  je n co v IS ReD a ne v DotInfo a naopak  V t ina dat obsahuje jist  informace o platnosti dat  datum podpisu nen  to sam  jako datum  erp n   Kdy  pak  lov k informace p ruje nap   s obchodn m rejst  kem  jsou tyto atribuce kl  ov     Je tu t   v cn  p ekryv  kdy dotace by m ly m t sepsan  smlouvy a pokud byly uzav eny v ur itou dobu  budu i v registru smluv  ale nemus       V neposledn   ad  jsou v echny datasety zat  eny jistou chybovost   Neznamen  to   e bychom m li rezignovat na jist  anal zy  sp    e bychom m li b t extr mn  opatrn  v tom  co z dat vy teme    Form t dat   P i zpracov n  dat se   d me jedn m hlavn m principem   sna  me se nem nit strukturu dat nad nezbytnou mez  To znamen    e bereme v echny sloupce ze zdrojov ch dataset   a  na extr mn  p  pady nem n me obsah dat  i tak jen kv li p rov n  dat   tedy I O     Jde n m o to  aby  lov k mohl v dy dohledat prim rn  zdroj  co  se mu v p  pad  na   manipulaci s daty nepoda      I p es na i zna nou snahu se m  e st t   e p i na em zpracov n  dat n co zm n me  i sma eme  Jakoukoliv takovouto chybu n m pros m hla te  pokus me se ji opravit v co mo n  nejkrat  m term nu  Je d le it  zd raznit   e  nejsme autory   dn ch t chto dat  pouze je zp  stup ujeme ve ejnosti     Datasety   Pl nujeme zde zapojit dva typy dataset    transak n  a klasifika n   by  toto rozd len  nen   ist   budou zde jist  p ekryvy      IS ReD    centr ln  evidence dotac   d  ve zn m  jako CEDR  je jeden z v t  ch dataset   obsahuje dotace pro soukrom  i ve ejn  subjekty a tento dataset sah  a  do roku 1999  Hlavn  nev hodou je absence metadat u velk    sti z znam   Aktualizov n je kvart ln     Dotace EU    medi ln  asi nejprop ran j   t ma  dataset je a  p ekvapiv  p  mo ar   jde o jednu tabulku  resp  dv   jednu pro ka d  rozpo tov  obdob   Dataset spad  pod MMR  aktualizov n je m s  n     DotInfo    t et  informa n  syst m pro dotace  bohu el zat m nen  jasn   co je ve kter m  Puristicky vzato by Dotace EU m l b t subset CEDR ReD a DotInfo by nem lo existovat  Bohu el je DotInfo do velk  m ry p ekryvem CEDR ReD  ale ne  pln m  Je t   mnohem krat    sah  jen do cca roku 2011    Ve ejn  zak zky    shrom  d n  dat z n kolika syst m  zad v n  ve ejn ch zak zek  pro n s doposud nejm n  prostudovan  dataset  s n m budeme pot ebovat nejv ce pomoci  Je asi nejv ce o emetn  co se t  e rozkl  ov n  slo it  struktury dat    Registr smluv    od l ta 2016 maj  ve ejn  subjekty povinnost zve ej ovat smlouvy nad 50 tis c K  hodnoty  tento revolu n  z kon dramaticky zv  il transparentnost ve ejn ho utr cen   Zp  stupnil informace o v daj ch mimo ve ejn  zak zky  ke v em v daj m t   p idal samotn  smlouvy  by  m sty za ern n   Dataset pat   pod MV R a je aktualizov n denn     Monitor st tn  pokladny    jeden z nej it  ch dataset  st tu nab z  pohled do vy  tov n  jednotliv ch subjekt  st tu  a  u  jde o ministerstva nebo obce  Zat m dataset nem me zpracovan   pl nujeme jej pou  t na obohacen  informac  o ve ejn ch subjektech   Nap   u smlouvy na 1 miliardu  lov k uvid   kolik procent z ro n ho rozpo tu to je     ARES    nechvaln  zn m  administrativn  rejst  k ekonomick ch subjekt  nab z  vhled nejen do pr vnick ch subjekt   eska  Kl  ov  jsou z kladn  informace o subjektech  mo n  vyu ijeme i obchodn  rejst  k  Nov  otev en  data ARES n m bohu el moc platn  nebudou  v ce v README  TODO   Tento dataset bude bohu el jedin   kter  nejde voln  st hnout najednou    PSP    Poslaneck  sn movna Parlamentu nab z  velmi zaj mav  datasety pro dal   zpracov n   u n s najdete dva hlavn    informace o osob ch  a nejen poslanc ch  ale i sen torech nebo  lenech vl d  a zpracov n  stenoprotokol     Volby    statistick    ad ji  n jak  p tek nab z  otev en  data  se kter mi se celkem snadno pracuje  ale nejsou p ipraven  k analytice hned po sta en   Nav c se jejich form t m nil v  ase  tak e se sna  me toto unifikovat    Justice    data od Ministerstva spravedlnosti obsahuj  informace o jednotliv ch ekonomick ch subjektech  jde o export z ve ejn ch rejst  k   jak jsou mj  dostupn  na webu  Justice     SZIF    data od  St tn ho zem d lsk ho interven n ho fondu  obsahuj  informace o p  jemc ch dotac   v etn  rozd len  na n rodn  a evropsk  zdroje     Identifikace podnik    Jedn m z hlavn ch z sah  do dat je nahrazen  identifikace podnik  na imi  vlastn mi  daty  konkr tn  daty z ARES  Probl mem je   e m sto odkazov n  do ARES se ka d  z poskytovatel  dat sna   tvo it si vlastn  datab zi podnik  a v on ch datech jsou  asto chyby  Z d vodu konzistence a kvality dat proto pou  v me v t inou pouze I O podnik  a d l p eb r me informace z ARES  esviewer js   tl dr  This lets you search an Elasticsearch index without having to install anything but Elasticsearch  You just point your browser to an HTML file and off you go  It s not secure  not one bit  It looks like this      This tool addresses a frequent issue I have  I usually have a dataset in Elasticsearch and I want to search through it  No Kibana  no nothing  I would usually just read the JSON of  http   localhost 9200 index  search q my query   but that s not a terribly friendly interface  Also  I hate dependencies  so I came up with this    There is a single HTML file that talks to Elasticsearch s REST API  that s pretty much it  There is just one caveat  modern browsers don t let websites talk to APIs that have a different hostname  or even a different port of the same hostname   unless said server is set up with  CORS     There are at least two ways you can CORSify your Elasticsearch server  You can turn on CORS in your ES settings  usually   etc elasticsearch elasticsearch yml  on Unix systems     http cors enabled  true http cors allow origin      http cors allow methods  GET  POST   Or if you re using something like nginx  you can put a proxy in front of your ES cluster and set it to add CORS while it s passing requests through  you ll also move your cluster to a different port by doing this         server       listen 8080      location       add header  Access Control Allow Origin           add header  Access Control Allow Methods   GET  POST  OPTIONS       add header  Access Control Allow Headers   DNT X CustomHeader Keep Alive User Agent X Requested With If Modified Since Cache Control Content Type   proxy pass http   localhost 9200               You should never use either of these solutions in production   This is just a toy script that I use on my laptop to browse my data    I ve also used this on a VPS and because I usually don t want to deal with nginx  I use the first method and then allow my home IP to access ports 80 and 9200 of my remove server  those being the webserver and Elasticsearch  respectively   I use Ubuntu  so I can tweak  ufw  in the following way  Note that using SSH tunnels is probably better    sudo ufw allow 22  sudo ufw allow from my ip address to any port 9200  sudo ufw allow from my ip address to any port 80  sudo ufw enable    In case you go for the nginx option  you can filter IPs in the CORS rule above  so you get both CORS and basic security in one  whereas here I have to modify both ES and ufw  It s a matter of preference    I really enjoy using this script  because it requires very little setup   and I usually have CORS enabled locally anyway  so I just launch the HTML file and I m done  The file has a few options you can modify    const endpoint    http   possibly a remote address 9200  search      set your Elasticsearch endpoint const pglen   5     set number of items per page const render keys     restrict    to    just    some    keys     As always  it comes with no guarantees  pull requests and issue submissions are welcome  Cross reference Netflix shows with IMDB ratings   I wanted to cross reference Netflix shows with IMDB to see what s what  I know there are browser extensions to do this in a way  but I wanted to see it all at once    Netflix shows   First I needed a list of shows  I already had that from a different side project   you just need to go to a listing   here s  a subsite with all shows with given subtitles   You scroll all the way to the bottom  while waiting for each new page to be downloaded  and once you reach the end  you open the developer console and run   javascript document querySelectorAll  div boxart container p fallback text   forEach x    console log x textContent     This gives you a list of shows listed on the current page  given current DOM  this may change in the future   Copy and paste this from the console  clean it up a little and save it as a line delimited list    From then on  it s up to you  what you want to do  Here s how you get it to Postgres    Postgres tables      sql create schema if not exists netflix    drop table if exists netflix shows  create table netflix shows       title varchar      primary key title       drop table if exists netflix imdb ratings  create table netflix imdb ratings       tconst varchar not null      averageRating numeric 4 2  not null      numVotes int not null      primary key tconst       drop table if exists netflix imdb title akas  create table netflix imdb title akas       titleId varchar not null       ordering int not null      title varchar not null      region varchar       language varchar       types varchar       attributes varchar      isOriginalTitle bool      primary key titleid  ordering       create index imdb title on netflix imdb title akas title         IMDB data   IMDB offers some of its data for  non commercial use   it s quite easy to get it in the database  It s tab delimited and uses   N  for nulls       sh curl  O https   datasets imdbws com title akas tsv gz curl  O https   datasets imdbws com title ratings tsv gz   gunzip  c title ratings tsv gz   tail  n  2   psql  c  copy netflix imdb ratings from stdin delimiter E  t  NULL   N   gunzip  c title akas tsv gz   tail  n  2   psql  c  copy netflix imdb title akas from stdin delimiter E  t  NULL   N         Then you insert all your Netflix shows info   sh cat shows txt   psql  c  copy netflix shows from stdin    And you re all done    Basic joins   There are tons of way you can combine these datasets  I chose this trivial exact lookup based on the show s name  breaking conflicts by the number of votes on IMDB    sql with imdb data as       select titleid  ordering  title  averagerating  numvotes from netflix imdb title akas ttl     inner join netflix imdb ratings rt on rt tconst   ttl titleid    joined as        select            row number   over partition by title order by numvotes desc  as rn     from netflix shows     left join imdb data using title    select   from joined where rn   1   You could also choose some fuzzy lookups  perhaps using n grams  you just index the given columns using  pg trgm  and off you go  you can search by similarity  Katalog nejen otev en ch dat   St edobod  esk ch otev en ch dat je  NKOD   n rodn  katalog otev en ch dat  Je v n m spousta zaj mav ch informac   ale pro takov  ten letm  p ehled o tom  jak  data jsou k dispozici  je celkem obt  n  pou iteln     Nechceme tedy NKOD replikovat  jde n m o trochu obecn j   p ehled  aby si ka d  mohl dohledat data pro jednotliv  oblasti z jmu      Katalogy   Administrativni informace   Dotace   Smlouvy   Zak zky   Faktury   Ostatn  v daje   Rozpo ty   Metainfo o st tu   Legislativa   Region ln  data   Zdravotnictv    Geodata   Ostatn      Katalogy   N rodn  katalog otev en ch dat  NKOD   je katalog v ech katalog   jednotliv    ady a instituce si ale ob as zalo   vlastn  katalog  lok ln  katalog otev en ch dat  LKOD   zde je v pis n kter ch z nich      Ministerstva   Ministerstvo dopravy   obsahuje i data pro St tn  fond dopravn  infrastruktury    zen  letov ho provozu nebo Dr  n ho   adu       Ministerstvo financ    m  i data z Gener ln ho finan n ho  editelstv   Gener ln ho  editelstv  cel a dal  ch       Ministerstvo kultury   Ministerstvo obrany   Ministerstvo pr ce a soci ln ch v c    asi nem     Ministerstvo pro m stn  rozvoj   krom MMR je tam i CzechTourism nebo St tn  fond rozvoje bydlen        Ministerstvo pr myslu a obchodu   Ministerstvo spravedlnosti   Technicky vzato jde o  Otev en  data  esk  justice       Ministerstvo vnitra   nem   asi fakt ne  maj NKOD   Ministerstvo zahrani n ch v c    nem     Ministerstvo zdravotnictv    v etn  dat z krajsk ch hygienick ch stanic  N rodn ho  stavu du evn ho zdrav  nebo Koordina n ho st ediska transplantac         Ministerstvo zem d lstv    nem     Ministerstvo  ivotn ho prost ed    Obsahuje i data z  HM   Spr vy jeskyn   Spr vy NP  umava a dal  ch p idru en ch organizac        Ministerstvo  kolstv   ml de e a t lov chovy   M sta   Hlavn  m sto Praha   nejen m sto  ale i Oper tor ICT  jednotliv  m stsk    sti  M stsk  knihovna atd        Brno   Ostatn     esk  telekomunika n    ad   T       Administrativni informace   K transak n m dat m  dotace  smlouvy  zak zky       je t eba doplnit data o smluvn ch stran ch  proto e tato data jsou v transak n ch datasetech zpravidla nedosta uj c   Neexistuje jedno centr ln   lo i t   je n kolik zdroj  t chto informac   z le   na tom  co  lov k po aduje      N kter  informace o n kter ch fyzick ch  pr vnick ch a ve ejn ch entit ch jde z skat z  export  datov ch schr nek   Org ny ve ejn  moci maj  sice DS povinn   ale u priv tn ch subjekt  to tak nen   tak e v datech nejsou zdaleka v echny    Dobr  pro p ehled o org nech ve ejn  moci  p  padn  jako zdroj pro mapov n  z adres datov ch schr nek na I O  i naopak    Administrativn  registr ekonomick ch subjekt   ARES    Historicky nejpodstatn j   dataset pro administrativn  data  do dne ka m  svou relevanci    V sekci  XML slu by  najdete popis  ady endpoint   ide ln  pro z sk n  informac  o n kolika m lo subjektech  Nejd le it j   je OR  obchodn  rejst  k    daje z Justice   RES  registr ekonomick ch subjekt    z kladn   daje od  esk ho statistick ho   adu  a R P   ivnostensk  rejst  k     API maj  limity v   dek des tek tis c dotaz  denn   tak pozor na to  proto e m  ete b t snadno zablokov ni    V sekci  otev en  data  je relativn  nov   bulkov  export  obchodn ho rejst  ku  Obsahuje skoro v e  co by  lov k pot eboval o pr vnick ch osob ch   chyb  historie n zv  subjekt  a data narozen  fyzick ch osob  jednatel   spole n k  atd      MF R tento registr provozuje  ale data jen poskytuje d l  nejsou v jeho vlastnictv     Otev en  data Ve ejn ho rejst  ku a Sb rky listin   Ministerstvo Spravedlnosti poskytuje export dat z webu Justice cz  zejm  z rejst  ku pr vnick ch osob  Cokoliv vid te na webov  verzi rejst  ku  to si m  ete st hnout v XML v bulkov  form     Pro aktu ln  informace sta   st hnout data pro sou asn  rok a v echny rejst  kov  soudy a pr vn  formy  Bohu el nejde st hnout v e najednou n jak jednodu eji    Informace o zanikl ch subjektech je trochu t     z skat  proto e firma zanikl  v roce 2009 bude naposledy v datasetu pro rok 2009  tak e  lov k mus  st hnout data pro v echny roky  aby z skal informace o v ech zanikl ch subjektech  Tato limitace se net k  export  ARES v  e  tam je snadn  z skat informace o zanikl ch subjektech    Oproti ARES  lov k z sk  informace o akcion   ch  insolvenc ch a dal  ch metadatech    Tento dataset bude v budoucnu jedin  nutn  pro identifikaci smluvn ch stran  v tuto chv li m  st le n kolik z drhel       Dotace     DotInfo   Ze syst mu DotInfo existuje  jeden export  z roku 2017   TODO  vysv tlit  pro  bohu el tenhle dataset existuje   IS ReD   obsahuje CSV exporty pro dotace  rozhodnut  nebo p  jemce   je mo n  dohledat informace v   seln c ch   doporu uji  diagram  pro lep   pochopen  rela n ho modelu   n stupce syst mu CEDR III   MS2014   a  Seznam operac  p  jemc    Dva datasety od MMR ohledn  evropsk ch dotac   tedy vy   ch des tk ch miliard ro n     MS2014  jsou otev en  data p  mo z informa n ho syst mu pro spr vu dotac   obsahuj  strukturovan  data o dotac ch pro obdob  2014 2020    Druh  dataset  Seznam operac   obsahuje data pro obdob  2007 13 a 2014 20  jde ale o celkem zvl  tn  strukturovan  Excely  kter  se nav c v  ase m n   Tak e pro nahl  en  dobr   ale pro analytiku je lep   export z MS2014     CzechInvest   ud len  investi n  pob dky   star   data neobsahuj  I O informace  tak pozor na to   St tn  zem d lsk  inverven n  fond  SZIF    Fond operuje s 30 40 miliardami ro n   na webu jsou jednotliv   adatel  k dohled n     Existuj  XML exporty pro posledn  dva roky dat      Smlouvy     Registr smluv   Jde o p elomov  informa n  syst m  kam maj  tis ce ve ejn ch subjekt  povinnost publikovat skoro v echny smlouvy p esahuj c  hodnotu 50 tis c K   jsou v jimky mj  z d vod  bezpe nosti  i obchodn ch tajemstv      Poskytuje otev en  data  na denn  b zi ve form tu XML    Syst m lze pou  vat nap  mo  zpr cov n m dat nebo p es  Hl da e st tu   nejzn m j  ho zpracovatele t chto dat  kde jsou krom smluvn ch dat prolinkov ny dal   datasety pro lep   kontext a analytiku    Ad hoc smluvn  data   P ed   innost  Registru smluv publikovaly n kter  subjekty smluvn  informace z vlastn ho popudu    V hodou t chto dat je   e smlouvy  asto predatuj  vznik Registru smluv   do registru toti  subjekty vkl daj  jen nov  smlouvy  p  padn  star  smlouvy  pokud je nov  smlouvy roz i uj     dn  d vkov  vkl d n  star ch smluv se ale nekon      P  klady export    Ministerstvo kultury    data pro 1994 2019   Ministerstvo pro m stn  rozvoj a jeho p idru en  organizace   TODO  dal   instituce         Zak zky     vestnik  jak se li       profil zadavatele   vsechny zakazky      Faktury   Neexistuje centralizace faktur  je na jednotliv ch   adech  i jin ch entit ch  jestli sv  faktury zve ejn   Tato data jsou  asto cenn j   ne  smlouvy nebo zak zky  proto e obsahuj  re ln   traty a jejich metadata jsou kvalitn j   ne  nap   u registru smluv      Ministerstva   Ministerstvo dopravy   obsahuje i data pro St tn  fond dopravn  infrastruktury  Dr  n  inspekci   editelstv  silnic a d lnic a dal   entity       Ministerstvo financ    port l obsahuje i faktury    adu pro zastupov n  st tu ve v cech majetkov ch       Ministerstvo kultury   Ministerstvo obrany   Ministerstvo pr ce a soci ln ch v c    nemaj     Ministerstvo pro m stn  rozvoj   Krom MMR jsou tu faktury i agentury CzechTourism  St tn ho fondu rozvoje bydlen  nebo Centra pro region ln  rozvoj       Ministerstvo pr myslu a obchodu   Odkaz vede na jednor zov  export z  nora 2019  pro aktu ln j   data je t eba na rozcestn k  vizte seznam LKOD v  e         Ministerstvo spravedlnosti   Na stejn m webu jsou i faktury soud   st tn ch zastupitelstv   v ze sk ch slu eb  justi n  akademie a dal  ch org n   esk  justice       Ministerstvo vnitra   nem     Ministerstvo zahrani n ch v c    nem     Ministerstvo zdravotnictv     na sv m port lu MZ R nejsou jen faktury ministerstva  ale i z dal  ch entit   nap   krajsk ch hygienick ch stanic  N rodn ho  stavu du evn ho zdrav  nebo Koordina n ho st ediska transplantac     Ministerstvo zem d lstv    zd  se   e nem    Dle z kona o svobodn m p  stupu k informac m  106 1999 Sb   ministerstvo n kter  data  poskytlo   ale systematicky nic nevyd v        Ministerstvo  ivotn ho prost ed    Obsahuje i faktury  pro dal   p idru en  organizace   nap   pro  eskou geologickou slu bu nebo Agenturu ochrany p  rody a krajiny        Ministerstvo  kolstv   ml de e a t lov chovy   nem     Samospr vy   Hlavn  m sto Praha   obsahuje nejen data pro magistr t  ale i pro n kter  m stsk    sti a m stsk  podniky       Ostatn     esk  telekomunika n    ad   T     IPR    Institut pl nov n  a rozvoje     Ostatn  v daje      esk  spr va soci ln ho zabezpe en    SSZ     SSZ nab z  p ev  n  p ehledov  datasety   po ty penzist   OSV   pr m rn  mzdy  pr m rn  d chody  statistiky pracovn  neschopnosti  typy d chod  atd      Rozpo ty     Monitor  St tn  pokladny je  aplikace  pro rozklik v n  rozpo t  a dal  ch   etn ch informac  o spoust  slo ek st tu   m st  obc   p  sp vkov ch organizac    kol atd    Datov  katalog    v echna data z Monitoru jde exportovat jako CSV    P  klad detailu dat    uk zka dat z Uhersk ho Hradi t    m me tu rozvahu    etn  z v rky  seznam p  sp vkov ch organizac  atd    Skv l  vysv len  struktury dat  od Petra Bouchala   CityVizor    p vodn  projekt z Ministerstva financ  se p esunul pod spolek  Otev en  m sta  a jde mu o vizualizaci rozpo t  samospr vn ch jednotek   Hlavn  rozd l proti Monitoru je ten   e Monitor m  rozpo ty na  rovni rozpo tov ch kapitol  nap   odvoz odpadu   ale nem te tam jednotliv  faktury  pr b  n  pln n   informace o dodavatel ch atd  To je p esn  mezera  kterou vypl uje CityVizor    Praha m  vlastn  instanci CityVizoru      Metainfo o st tu   TODO  prolinkovat toto n jak s admin informacemi v  e  Aby  lov k nemusel scrollovat mezi nima  oboj  pat   pod stejnou podkategorii     Org ny ve ejn  moci    asto je t eba identifikovat slo ky st tu  a  u  pro kategorizaci dat  jdou finance od soukromn ka st tu nebo mezi soukromn ky atd   nebo t eba pro adresnou komunikaci  Bohu el neexistuje jeden autoritativn  zdroj    Seznam org n  ve ejn  moci  OVM  je mo n  z skat z  exportu datov ch schr nek   Otev en  data Czech POINTu  maj  t   seznam org n  ve ejn  moci   Registr pr v a povinnost  m   webov  n hled  a  JSON export  t chto dat   Data Poslaneck  sn movny a Sen tu   Jde o sadu dataset   kterou na webu nikdy nenajdete  je ale velmi cenn     Jde o denn  aktualizovan  soubory  ve form tu podobn  CSV  jejich zpracov n  je celkem snadn   jen pozor  jsou normalizovan   tak e budete ob as joinovat p es n kolik tabulek    Obsahuje mj    Hlasov n  ve Sn movn   od vzniku  esk  republiky    Stenoz znamy   Tisky ze Sn movny i Sen tu   Pl ny sch z    Interpelace       Volby    esk  statistick    ad nab z  data z voleb jako otev en  data  m  to v ak n kolik z drhel     Star   data jsou zpravidla v jin m form tu ne  ta sou asn   nap   FoxPro vs  XML vs  CSV   tak e pro del    asov   ady mus   lov k trochu pracovat    Otev en  data neobsahuj  informace o historicky v ech volb ch v  esk  republice  pln  pokryt  je a  cca od roku 2004  Pro star   informace mus  j t  lov k na web  volby cz  a dohledat  daje tam    Kandid ti ani zvolen  zastupitel  nemaj    dn  unik tn  identifik tor  celkem  patn  se tedy mapuj  nap   na anga ovan  osoby z ARES nebo Justice  nem me toti  ani datum narozen   jen v k osoby  kter  nen  platn  k n jak m ur it mu datu    Centr ln  registr ozn men   je informa n  syst m zalo en pro   ely z kona o st etu z jm     Obsahuje data o ve ejn ch  initel ch  soudci  zastupitel   poslanci        zejm na pak jejich majetkov  pom ry    astnictv  ve firm ch a funkce  lenstv     Syst m nem  datov  export nebo ve ejn  API   k nahli en  je ale i tak u ite n     registr prav a povinnosti   wikidata    s  t n       Legislativa     psp cz o tvorb    eklep  veklep   bude elegislativa  esb rka   z kony pro lidi   ASPI      Region ln  data     golemio   data brno cz     Zdravotnictv       ZIS  viz hackathon 2019  ale bude toho i v c    S KL   m  n co ministerstvo      Geodata      UZK   m stsk    katastr   IPR prazsky model     Ostatn        ad pr myslov ho vlastnictv    PV     denn  exporty v XML   wikidata   portal gov cz   politicke finance   rozhlas data     edn  desky   insolvence   N co z  S    NB  ARAD     https   data gov cz wishlist    pg flame js   postgres plan flame graphs   Paste your execution plan  get a flame graph out  This project is heavily inspired by  pg flame   the one big difference is that we don t do any server side data manipulation  everything happens on the client    Try a live demo   There are still quite a few rough edges  These will eventually morph into issues      we might not be parsing InitPlan nodes correctly   we don t support cost based plans  only those with explicit timing   EXPLAIN ANALYZE     there are some rounding issues  you can see it in the example    any errors are emitted in the console  not in the UI     Contributions welcome  Politici na wikidatech   St v  se mi  asto   e pot ebuji zjistit n jak  re lie ohledn   esk  politiky  Kdo byl kdy p edsedou  eho  jak star  byl kdo p i zvolen   kolik jsme m li ministr  n  eho atd  Data k t mto skutk m jsou relativn  voln  dostupn   v t inou ale jen jako psan  text   asto na Wikipedii    Existuje v ak sestersk  projekt Wikipedie s n zvem  Wikidata   kter  m  za c l zhmotnit wikipedick  informace jako strukturovan  data   jako opravdov  dataset  kde v me   e Nicholas Cage je herec   lov k  mu    e se narodil tehdy a tehdy a hr l v t chto konkr tn ch filmech  Kdy  pak zad l do vyhled va e Wikidat   e chci v echny herce 50   kte   se narodili v Kalifornii  dostanu v echny relevantn  herce v etn  Nicholase Cage    Podobnou ambici m m i pro prostor  esk  politiky  Chci m t strukturovan  data o tom  kdo byl kdy poslancem  p  padn  kdo koho nahradil v pr b hu volebn ho obdob   Kdo byl kdy sen torem za jak  obvod  kdo kdy vedl sn movnu nebo sen t  Um el n kdo n kdy ve vysok   stavn  funkci  Atd  atd    C lem tohoto repa je tedy n sleduj c       Sb rat skripty  kter  pomohou s p  pravou dat pro nahr n  do Wikidat   V tuto chv li m m k d pro stahov n  dat z Poslaneck  sn movny  obsahuje i data za Sen t a spoustu dal  ho  a z Wikidat  to v e ulo   to SQLite datab ze pro maxim ln  p enositelnost  Jedin  z vislost tu je Python 3  bez extern ch bal  k      Napsat testy takov m zp sobem   e budeme moci st hnout data z extern ho zdroje a porovnat je s Wikidaty a pouk zat na nesrovnalosti    C l je takov    e jednor zov  dohrajeme historick  data a pak d l   zm ny u  budou prob hat ru n   jak se budou d t    Poukazovat na nesrovnalosti  i chyb j c  data  kter  budeme muset doplnit ru n   proto e k nim nejsou dobr  zdroje  nap    m sto p edsednictva stran nebo  lenstv  stran     Poukazovat na nejasnosti ohledn  datov ch model  Wikidat   jak spr vn  zapsat ur itou funkci  Jak zlep it kvalitu n jak ch dat  aby byla podle pravidel  To se pak konzultuje s Wikipedisty na jin ch platform ch    Bl  e skloubit Wikidata s Wikipedi   aby data vypln n  na Wikidatech byla automaticky propsan  v textu Wikipedie  abychom krom systematick  datab ze m li i bli    propojen  s popul rn  datab z   a t m p dem m n  nutn ch  prav na obou stran ch      V echny dosud zn m  probl my sledujeme v  issues   jist  se jich je t  spousta objev     P isp vat do Wikidat systematicky a spr vn  nen  jednoduch   ocen me ale i d l    pravy  metodologick  rady  i pouk z n  na chyby  Zde je p r odkaz  na n stroje  kter  pou  v me      Every politician    WikiProjekt  kter  trackuje pr b h vypln n  politick ch dat na Wikidatech   Open Refine    n stroj pro velmi rychl   i t n  dat  kde se daj  i snadno propojit  daje se z znamy z Wikidat   QuickStatements    rychl   pravy wikidat  v t inou na  rovni p idat odebrat statement  h   se zde pracuje s r zn mi detaily   lov k trochu mus  v d t  co d l    TABernacle    tabul rn  prohl  e ka a editor wikidat   TODO    licence   na kod a na otazky  MIT  CC0     searcher  abychom se vyhnuli duplicitam    index json    GA   generovani testu o x otazkach   moznost vybrat si narocnost   pridat navod jak pridavat otazky   pridat navod jak to pustit lokalne  treba python3  m http server    google form vcetne consentu   admin tooling  rozlozeni otazek do temat  do slozitosti  slozitost a temat  jak casto je vyplnen zdroj atd     testuj na telefonech smda   small medium data analysis   This project has been driven by my multi year effort to streamline one of my recurring tasks     I have a 200MB CSV file at hand and I want to look through it   filters  aggregations  charts  exports  I don t want to set up infrastructure or write much code to do so     All the solutions I ve used  they are superb  are pretty hard to set up for newcomers  they often require a lot of infrastructure  or they only work for data of certain size  I wanted to overcome these issues  hence this project  The tool in question does not handle everything  far from it  it s meant to serve as a tool to quickly grep a given dataset without worrying about databases  schemas  available RAM  virtual environments  Docker etc    Deployment simplicity is definitely at the heart of this project  a lot of effort was invested in ease of use and it definitely affected the core architecture and functionality     Usage   Go to  releases   download a version for your operating system and architecture  unpack it and launch it  A local webserver will be launched  you interact with that through your web browser  that s it    There are multiple ways you can run smda from source  You ll need  make  and either the  Go compiler  or  Docker       make run  builds it on the fly and launches it  You can run  DEV 1 make run  if you want web assets  HTML  CSS  JS  served from disk   useful for frontend development  This requires the Go compiler    make tests  runs tests   make build  builds a static binary that you can then launch  Again  the Go compiler is needed    make build docker  will build the binary from within Docker and result in a Docker image  The entrypoint is already set up  but you ll need to forward ports  e g  by running  docker run   rm  it  p 8822 8822 kokes smda       Main ideas   There are essentially three major things we want to address in smda      We face issues in terms of resource utilisation  namely RAM  In order to keep that in check  we re leveraging the fact that nowadays disks are fairly speedy  both in terms of throughput and latency  So all the data imported is processed and stored efficiently on disk   in a binary and compressed format  rather similar to Parquet or ORC     Existing tools mostly require you to specify a schema beforehand  While that is necessary for proper data engineering  it does get in the way when completing simple data exploration tasks  For this reason we employ schema inference  so that you don t have to hand pick your data types    Many tools require fairly heavy software stacks in order to function  Language runtimes  package managers  compilers  task schedulers      We wanted to keep things simple  so the whole thing here is a single binary  It has major costs associated with it  but the deployment simplicity is worth it      There are a few other things we re addressing  but they are out of scope for this README  we ll elaborate on those in the docs at some point    We ll also have a note on the inspirations that influenced the design of smda  but for now we ll resort to a short list   SQLite    Datasette    ORC    PostgreSQL    Trino   and  OpenRefine  to name just a few    Ultimate goals   The goal was to create a data exploration tool  and while the basis of that is committed here  there are a few major components we d like to add in order to consider it  feature complete  in a 1 0 sense      JOINs and other SQL functionality   at this point we support basics SQL and while we re not aiming to support everything in the standard  it s HUGE   we d like to get some commonly used features  most notably JOINs    Charting   exploratory data analyses rely on charting  it s pretty much essential to understand your data  And while we have some experience integrating client side charting in web apps  we just haven t been able to squeeze it in just yet    Object storage   we d like to decouple compute and storage   it s something we thought from day one  both the overall architecture and on disk binary format are amenable to this  The ultimate aim here is to make the compute layer so thin that it could be launched from a Lambda function      Closing notes   At this point the tools is in flux  the APIs  both REST and Go  keep changing  the binary format may be overhauled at some point  the code base and tooling is changing as well  For these reasons  the tool is to meant to be integrated into larger systems  it s meant as an ad hoc data exploration tool    If you have any bug reports  objections  questions  or proposals  you can  file an issue    e mail me   or ping me  on twitter   My personal knowledge base   This is a collection of articles  talks  books  and other resources I ve come across over the years  The vast majority should be stuff I actually read or watched  it s not just a bookmarking service    This document is very much a work in progress  most of my links are still to be added  At one point I ll probably split it into multiple documents    Development   Performance     Instructions can be essentially free if you re waiting for your memory to catch up   dotGo 2016   Damian Gryski   Slices  Performance through cache friendliness   CppCon 2016  Timur Doumler  Want fast C    Know your hardware     CppCon 2015  Chandler Carruth  Tuning C    Benchmarks  and CPUs  and Compilers  Oh My     Catching pandas performance regressions using snakeviz   flamegraph  and a line profiler   Maintaing Performance by Tom Augspurger   A user name length can change performance  and other random perf bits   A super fun talk     Performance Matters  by Emery Berger   CppCon 2019  Chandler Carruth  There Are No Zero cost Abstractions    This is a classic   Why is GNU grep so fast   It does very little work and it does this to as few bytes as possible  These advice apply to a much wider number of situations      Measure everything  things can be counter intuitive  Really cool simple demos on perf tracking   GothamGo 2019    Slice Recycling Performance and Pitfalls  by David Golden       Two topics here  First  various crazy optimisations compilers can do for you these days  e g  detect you re implementing POPCNT   second  infrastructure behind the de facto standard Godbolt tool  it s actually called Compiler Explorer  who knew     how it handles security  caching  deployment etc         CppCon 2017  Matt Godbolt  What Has My Compiler Done for Me Lately  Unbolting the Compiler s Lid        Assembly gets used in Go in places where performance is deemed more important than maintainability and other niceties       Michael McLoughlin s Geohash in Golang Assembly  goes in depth in this topic  describing in detail how and why to implement an assembly powered implementation of a simple ish algorithm    dotGo 2019   Michael McLoughlin   Better x86 Assembly Generation with Go    assembly is hard to get right  having higher level tools that allow you to generate it instead seems super helpful  You still need to know what you re doing  but you have fewer opportunities to shoot yourself in the foot      Languages   C C     I ve only written tiny bits of C   and I was quite surprised how it differs from the C C   code I saw years and years ago  And here s  Kate Gregory talking  about how modern C   should be taught as exactly that  modern C      Python   Python wheels are still magic to me  but they are a tiny bit less magic now   thanks to this PyCon talk     There s a walrus operator coming in Python 3 8  Do you not know   Here s not only what it is   but also a reminder that we should be nice to other people on the internet  and what s changing in terms of Python governance    I teach Python and I often struggle when explaining decorators   this talk  is a great overview of not just what they are  but also  why  they work    A well paced talk  on setup py and PyPI    I knew that CPython can efficiently concatenate strings  despite their immutability  I never knew why  thanks for this explanation  https   blog ganssle io articles 2019 11 string concat html   Raymond Hettinger is a core Python developer  he s quite known for being a great speaker and educator  Here s  a talk on Python dictionaries   how they came up to be  how their implementation changed over the years and what it means for their users    Raise your hand if you only use IPython for syntax highlighting  easier help dialog  and multiline support  Turns out  it has a lot more features     Mypy  has generics and interfaces     Go   It s a bit annoying that Go can t mock  so you have to do all this  interface kungfu   But at least it s more explicit    A very nice and clear explanation of  how maps are implemented in Go     A frustrating thing about inlining in Go is that there s a fairly arbitrary cost model and you sometimes end up fighting it  lookup George Tankersley s talks on YouTube   There have been tons of discussions about whether or not it should be user configurable  if inlining hints should happen at the call sites or function definitions etc  It s quite a nice discussion that helps people understand the toolchain  It also goes to show that while a self hosted build system is nice  you forgo decades of gcc llvm optimisations  https   github com golang go issues 17566   JavaScript     A great talk by the creator of Node js   What he regrets and what he d do differently      WebAssembly   I ve been following the developments of WebAssembly a bit over the past few years  but I haven t seen such a convincing and clear explanation as to why it s a big deal  This is a great presentation in terms of clarity  content  and form  Thanks   callahad  https   www youtube com watch v TGo3vJVTlyQ   Internals   Architecture   I love to listen to data architecture descriptions and I love it ever more when the bottom line is  we just use Postgres  usually heavily partitioned   There are a few podcast episodes on these  there s  Outlier    Pex  or  Heap     Firecracker   isolation is not just about Docker  because security and performance are two major areas of research in container isolation  Firecracker is an OSS offering from Amazon  which they already use in Lambda and the main ruse is that it allows for extremely low latency and low utilisation of resources  https   www amazon science publications firecracker lightweight virtualization for serverless applications   A periodic reminder that  You are not Google     A nice overview of how Tailscale works  Reminds me of using Hamachi back in the day  worked like magic  https   tailscale com blog how tailscale works    The Amazon Builders  Library is a nice resource  I hope they add more content over time  Here s a bit of it  in video form   I liked the shuffle sharding explanation   It has text form  but I only watched this    It s not all just about velocity  features and such  it s also about good design   Here s a Google talk  by John Ousterhout about that very topic  He also  has a new book about that   it s decent but fairly high level     One of the main skills you need to have is evaluating new technology   https   youtu be d5bNZX8tpiI   PID loops at AWS  and other fun stuff  Never knew this whole area even existed    Data structures   There are tons of interesting data structures and algorithms  Nicholas Ormrod covers a few of them in  this great CppCon talk   I highly recommend the last part where he talks about  HyperLogLog   an algorithm which is very close to magic  Oh and I also gave a talk on HyperLogLog and similar algorithms  at PyData Amsterdam     Being a big fan of Bloom filters  I really enjoyed this paper on Cuckoo filters  They are better in many ways  one only has to be careful about insertion performance at high occupancy  https   www cs cmu edu  dga papers cuckoo conext2014 pdf   I dislike all things Java JVM  but I m always astonished at Elasticsearch s performance   In this video   Adrien Grand describes at length what sort of data structures and algorithms are used in Elasticsearch and Lucene    This short talk on CRDTs is not only good in terms of explaining the basics of conflict free data structures  but the speaker also highlights good UX in data applications  something that is grossly overlooked  https   www youtube com watch v DEcwa68f jY   Locks and how to avoid their performance penalties   A great talk by Kavya Joshi     Parallelism   This is a nice and concise overview of parallel computing primitives in Python  Be it processes  threads  workers or coroutines  https   youtu be 0RaotdCa j0   Raft   how does one achieve consensus across a distributed system  Where do we keep the truth and how do we handle failures  Paxos used to be the answer  but it s a notoriously hard problem and this implementation was quite difficult to work with  Raft is a newer and simpler solution to this coordination problem and it s widely used in distributed systems  https   raft github io raft pdf   Multiprocessing is scary  here are some great tips on how not to go crazy https   www youtube com watch v 5dMOYf0b 20   software development   It s been a while since I needed gdb  but it s good to see I used maybe half a percent of its functionality  https   www youtube com watch v PorfLSr3DDI feature share   Code reviews are essential  but that doesn t mean we have to do PEP8 and other code quality checks by hand   Automate these away as much as possible    Dependency management is hard  Here are  a few notes from the creator of pipenv   I m still not entirely sold on pipenv  but it s a decent tool for the job    We need to learn from mistakes we or others have made  Here s a post on reading postmortems  https   danluu com postmortem lessons    Things I Learnt The Hard Way  in 30 Years of Software Development    Crash early and crash often for more reliable software   I expected a dry talk on code reviews and it ended up being  a really nice overview of culture  soft skills  helping newcomers  dealing with anxiety  GTD etc     Forget PEP8  version control  reviews or tests  No bash  subprocess run all the things  just good old sloppy code that  solves your problem    Super fast and excellent     TDD has become almost a religion  Here s a great talk on  where it all went wrong   Ian Cooper covers the origins   like red green refactor   but also the bad practices we ve come to adopt  the most prominent one being that we tend to test implementations rather than behaviour  Once you factor in your implementation details into your test  you ve deviated from the original idea of TDD    I can highly recommend all of Kyle Kingsbury s talks  He built this testing suite  Jepsen  which verifies data stores and checks them for consistency guarantees  data loss  split brain  dirty reads      Oh and  this talk  includes zings like  Turns out the write ahead log wasn t write ahead   If you re not fluent in this terminology  I can highly recommend  Martin Kleppmann s book     Writing system software  code comments   The Skills Poor Programmers Lack   Legacy software   With Python 2   about to be   deprecated  there are a few talks about migration to Python 3  There s Facebook s take  from PyCon 2018  and Instagram s  from PyCon 2017   Oh and the most recent now   how  Pinterest did the same    it s a bit more about the code differences rather than how they actually migrated    Our journey to type checking 4 million lines of Python https   blogs dropbox com tech 2019 09 our journey to type checking 4 million lines of python    Rewriting software always leads to broken deadlines  new bugs everywhere  reinventing the wheels etc  I ve done this a few times and it was always the same  Here s  a good overview of how a lot larger rewrites turned out     Lessons learned from rewriting code in my 10  years as a developer http   huseyinpolatyuruk com 2019 02 04 lessons learned from rewriting code in my 10 years as a developer    Legacy code is a pain in the ass  but I somehow like it  This is a great overview of tips and tricks https   www youtube com watch v YsMUlNGF1no Git scraping elektro nabij  ek pro auta   Pozn mky    stahujem data od jednotliv ch poskytovatel   tak e pokryt  celkem n zk   by  pou  v me ty nejv t      cht l jsem pou  t  fDrive   ale nejde tam rozli it mezi  esk ma a zahrani n ma  a s polygonama si hr t nechci    cht l jsem pou  t  nabijto cz   ale vypad  to celkem neaktualizovan    r d bych     spo  tal nab jec  body  ne jen nab je ky Taky  esk  otev en  data   Forknul jsem  od   kde mi p i lo   e zav z  n jak  data  kter  nejsou  pln   nebo v bec  st tn   ale cht l jsem je m t verzovan   Tak e je tam ma u a p esouv m sem  V t ina lid  by m la d l j t do  kokes od   tady jsou jen drobky    Co tu je      wikidata   p r metadat k politik m  sta eno z wikidat   icij   data z  icij   upv   data z  upv   to je technicky vzato st tn   ale dost se to nehodilo   Blokovan  dom ny   Sdru en  CZ NIC za alo publikovat dokument s blokovan mi dom nami  Zde budeme tento dokument archivovat i s jeho zm nami    https   www nic cz page 4315 sdruzeni cznic zacalo zverejnovat seznam jmen domen ktere vyradilo z dns  Recognised overseas pension schemes   Check the current list of  QROPS  and see if any new institutions have been added  Dopravn  p estupky v Praze    plne z kladn  nahr n  dat o dopravn ch p estupc ch  asi by to cht lo      vy istit ulice a korelovat je s datab z  ulic    ZK  Geoport l         zbooleanovat osobu firmu   do istit z konnou  pravu a prolinkovat s textem z kona   zareportovat n jak  z kladnosti     V tuto chv li to vyplivne p  kaz pro nahr n  dat do Postgresy  Tj  sta     python3 main py sh load sh
68,fairmiracle,Non negative Matrix Factorization code   C and CUDA implementations of NMF    Refererence   Lin  Chih Jen   Projected gradient methods for nonnegative matrix factorization   Neural computation 19 10  2007   2756 2779      Bioconductor page http   bioconductor org packages MODA   Installation   Install the stable version from Bioconductor       try http    if https    URLs are not supported   source  https   bioconductor org biocLite R   biocLite  MODA         Install the developer version from github  library devtools  install github  fairmiracle MODA     Refererence   MODA  MOdule Differential Analysis for weighted gene co expression network  http   www biorxiv org content early 2016 06 03 053496 EAModules   A package for modules identification using evolutionary algorithms    Code structure     EAModules  core functions of evolutionary algorithms for module identification    functions  core functions of evolutionary algorithms including SA  GA and MA    utils  utilities for supporting core functions like connected component finding and fitness    R  R code of genetic algorithm  modified from COSINE package        data  benchmark and real world data    examples  example scripts to call functions  including supplementary files used in the paper      To construct PPI network  check another package https   github com fairmiracle PPINet    Reference   Active module identification in intracellular networks using a memetic algorithm with a new binary decoding scheme   BMC Genomics  2017 18 Suppl 2  209   Link   For any questions  please contact Dong Li at dxl466 cs bham ac uk      Bioconductor page http   bioconductor org packages AMOUNTAIN   Installation   Install the stable version from Bioconductor       try http    if https    URLs are not supported   source  https   bioconductor org biocLite R   biocLite  AMOUNTAIN         Install the development version from Github  library devtools  install github  fairmiracle AMOUNTAIN     Compile on Linux   Make sure GSL in installed  type  gsl config  in terminal    To compile C code   git clone https   github com fairmiracle AMOUNTAIN git cd AMOUNTAIN gcc  c src AMOUNTAIN c  fPIC  std c99 gcc  shared  o src AMOUNTAIN so AMOUNTAIN o  lgsl  lgslcblas rm AMOUNTAIN o   To use C version functions in R   source  R AMOUNTAIN R   dyn load paste  src AMOUNTAIN    Platform dynlib ext  sep        source  R AMOUNTAINC R    Here is a table of C version functions and pure R functions      C version       Pure R      Brief description                                                                                                  CGPFixSS      moduleIdentificationGPFixSS    Module identification on single network      CGPFixSSTwolayer     moduleIdentificationGPFixSSTwolayer    Module identification on two layer network      CGPFixSSMultiLayer      moduleIdentificationGPFixSSMultilayer    Module identification on multi layer network     Compile on Windows   It is not that straightforward to compile with GSL under Windows  Someone has created GSL Windows DLL and headers for both 32 and 64 bit in https   code google com archive p oscats downloads  Extract gsl 1 15 dev win32 zip and gsl 1 15 dev win64 zip into two directories      C  GSL i386   C  GSL x64     and set the environment variable  LIB GSL  as   C  GSL  instead of  C  GSL   Finally add  C  GSL x64 bin  to the  Path  in case missing the DLLs  Then the source can be compiled    Refererence   Dong Li  Shan He  Zhisong Pan  Guyu Hu  Active modules for multilayer weighted gene co expression networks  a convex optimization approach  biorxiv 2016  http   www biorxiv org content early 2016 06 03 056952 Datasets   A personal collection of datasets converted to uniformed formats  They can be used directly by most  DMLC projects   The copyrights of these datasets belong to the original authors    Text classification   All are converted into the  LIBSVM format       name   class    1  1    training   testing   feature   feature group                                                                  CriteoKaggle    2   3 9 1   4 584   10 7    6 042   10 6    3 429   10 7 K   39      CriteoTera    2       2   10 9        8   10 8    39      CTRa    2   1 1   2 238   10 5    6 355   10 4    1 314   10 7     200      CTRb    2   8 6 1   1 645   10 5    4 772   10 4    1 742   10 7     100     Avito     Avazu     Image classification   All are converted into the  recordio format     name   class   image size   training   testing                                                         CIFAR10    10   28   28   3   60 000   10 000     ILSVRC12   1 000   227   227   3   1 281 167   50 000   Machine Learning   node2vec  Scalable Feature Learning for Networks http   www kdd org kdd2016 papers files Paper 218 pdf   Deepwalk  Online learning of social representations http   perozzi net publications 14 kdd deepwalk pdf   Discovering Structure in High Dimensional Data Through Correlation Explanation http   papers nips cc paper 5580 positive curvature and hamiltonian monte carlo   Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification  JMLR   http   www public asu edu  huanliu papers ijcai07 pdf https   users cs fiu edu  chens PDF WISE2015 Ha pdf   Regularization for supervised learning via the  hubNet  procedure http   arxiv org pdf 1608 05465 pdf   A stochastic gradient method with an exponential convergence rate for finite training sets https   papers nips cc paper 4633 a stochastic gradient method with an exponential convergence  rate for finite training sets pdf   Accelerating stochastic gradient descent using predictive variance reduction https   papers nips cc paper 4937 accelerating stochastic gradient descent using predictive variance reduction pdf   Computational biology   A large scale evaluation of computational protein function prediction http   www nature com nmeth journal v10 n3 full nmeth 2340 html 3FWT ec id 3DNMETH 201303 message global remove WT ec id NMETH 201303H   A comparison of the functional modules identified from time course and static PPI network data https   bmcbioinformatics biomedcentral com articles 10 1186 1471 2105 12 339   Continuous time Bayesian networks identify Prdm1 as a negative regulator of TH17 cell differentiation in humans http   www ncbi nlm nih gov pmc articles PMC4791550    Detection of gene communities in multi networks reveals cancer drivers http   www nature com articles srep17386   The structure and dynamics of multilayer networks http   www sciencedirect com science article pii S0370157314002105 PPINet   Contructing protein protein interaction network from multiple data sources      Gene expression profiles  to filter gene set in network   Ensembl  to match gene symbols and protein id   STRING  one of the most popular PPI databases  edges are donated by Ensembl Protein ID   BioGRID  one of the most popular PPI databases  edges are donated by OFFICIAL SYMBOL     The basic idea is to determine a significantly expressed gene set based on expression profiles  and match the gene symbols with associated protein ids from popular PPI databases  as the following figure shows     Contructing Multilayer  dynamic  protein protein interaction network   A Multilayer PPI network consists of several layers  each layer is a PPI network  There are two ways to construct Multilayer PPI      The structures of each layer are exactly the same as constructed above  but the vertecies weights are dertermined by gene expression level   The structures of each layer are also determined by vertecies weights  e g  gene expression level      Reference   If you are using this package  please cite the following paper    Active module identification in intracellular networks using a memetic algorithm with a new binary decoding scheme   BMC Genomics  2017 18 Suppl 2  209   Link Modules identification for weighted biological networks   The R Code for ShanHeLab at Disease Module Identification DREAM Challenge  see https   www synapse org   Synapse syn7136340 wiki 403063   Depends  R     3 1 0   igraph  Matrix   License  GPL     2    Files     Comdetect R  Recursive community detection based on igraph package    AMOUNTAIN R  Modified from AMOUNTAIN package    WeightedMethod R  Multiple modules extraction using AMOUNTAIN    util R  Some additional functions such as rank modules     Usage   Put the folder  DREAMChallengeCode  at the same level of data folder for both subchallenges  Make a new folder  result  to save the modules  and two subfolders named  result subchallenge1    result subchallenge2     Subchallenge 1    r  source  DREAMChallengeCode Comdetect R   filename  subchallenge1 1 ppi anonym v2 txt  excuteigraph filename  fastgreedy    It taks a while for network 1 using fast greedy  And also make sure the memory is large enough for holding large matrix and deep recursive procedure  Then the preliminary result would save under  result subchallenge1   We can see four modules are larger then expected  which needs further division    r  net   read delim filename header   FALSE  x   net  1  1 y   net  2  1 W   sparseMatrix i   x  j   y  x   net  3   symmetric   TRUE  savefile paste  result   filename sep     ForAtom W savefile  result subchallenge1 1 ppi anonym v2 txt Atomsize 109   ForAtom W savefile  result subchallenge1 1 ppi anonym v2 txt Atomsize 159   ForAtom W savefile  result subchallenge1 1 ppi anonym v2 txt Atomsize 338   ForAtom W savefile  result subchallenge1 1 ppi anonym v2 txt Atomsize 467    Now all clusters modules  are saved in  result subchallenge1 1 ppi anonym v2 txt   We need to assign the module score  defined as the sum of edges weights  to each module and rank them from highest to lowest  In previous submission we selected part of them to avoid too many false postive ones    r  source  DREAMChallengeCode util R   modulesid  result subchallenge1 1 ppi anonym v2 txt   reassignScore filename  result subchallenge1 1 ppi anonym v2 txt    Now the modules are ranked  For other methods like Louvain  just modify the following line    r  excuteigraph filename  louvain     We also use AMOUNTAIN in subchallenge1  Take network 6 for example  10 modules sized between 3 100 can be identified by    r  source  DREAMChallengeCode WeightedMethod R   filename    subchallenge1 6 homology anonym v2 txt  savefile   paste  result   filename sep     excutescrpt filename 10 savefile 3 100   It takes much longer time by using this optimization based method  Clique with equal edges weights may not be seperatable by this method  Further division is required in this case  Since the module score is already stored in the file  we only need to put the module id ahead and rank them    r  source  DREAMChallengeCode util R   modulesid  result subchallenge1 6 homology anonym v2 txt   reassignScore4  result subchallenge1 6 homology anonym v2 txt     The optimization based approach is still in development  The further plan includes reimplementing by C  See AMOUNTAIN package  https   github com fairmiracle AMOUNTAIN  for the latest news    Subchallenge 2   As discribed in the writing up  the overall network is constructed from a sum matrix of all network  filtered out by a threshold      r  source  DREAMChallengeCode Comdetect R   source  DREAMChallengeCode util R   W1   returnW  subchallenge2 1 ppi anonym aligned v2 txt  c 21115 21115   W2   returnW  subchallenge2 2 ppi anonym aligned v2 txt  c 21115 21115   W4   returnW  subchallenge2 4 coexpr anonym aligned v2 txt  c 21115 21115   W5   returnW  subchallenge2 5 cancer anonym aligned v2 txt  c 21115 21115   W6   returnW  subchallenge2 6 homology anonym aligned v2 txt  c 21115 21115   W6   W6 max W6    filename  subchallenge2 3 signal anonym aligned directed v3 txt  net   read delim filename header   FALSE  x   net  1  1 y   net  2  1 W3   sparseMatrix i   x  j   y  x   net  3  dims   c 21115 21115   W3 lower tri W3     t W3  lower tri W3   W3   W3 max W3    W   W1 W2 W3 W4 W5 W6 W W 0 5    0 g   graph from adjacency matrix W mode  undirected  weighted TRUE  V g  name 1 length V g   savefile    result subchallenge2 sub2 txt  recursiveigraph g savefile  fastgreedy         Further division and reassign module scores      r  ForAtom W savefile  result subchallenge2 sub2 txt Atomsize 122   ForAtom W savefile  result subchallenge2 sub2 txt Atomsize 127   ForAtom W savefile  result subchallenge2 sub2 txt Atomsize 334     source  DREAMChallengeCode util R   modulesid  result subchallenge2 sub2 txt   reassignScore2 W  result subchallenge2 sub2 txt       Short Tutorials   A series of short tutorials given by me at our group meeting in School of Computer Science  University of Birmingham  The aim is to provide  practical    transparent  and  reproducible  technical reports related to computational biology and machine learning    This repository is used for markdown sources and additional code  Click my  personal page  for easy read documents      Talk is cheap  Show me the code    Linus Torvalds   ModuleExtraction   A package for modules extraction using a continuous optimization approach    Code structure     ModuleExtraction  core functions    R  R code of the core functions        Example  Code for reproducing the results and figure 1 3 in the paper      Reference   Extracting active modules from multilayer PPI network  a continuous optimization approach  In preparing    For any questions  please contact Dong Li at dxl466 cs bham ac uk
69,mgutz,nanoc3 blog   This is a  nanoc3  blog starter kit  FreeBSD licensed    View this project on  nanoc3 blog mgutz com     Features     Uses the appropriate filter based on the file extension   erb    ERB   haml    HAML   md   markdown    BlueCloth   sass    SASS   Uses the filesystem unified datasource  so metadata file or header is optional    Rolls archives articles to front page    Generates tag pages    Minimalist styling    Uses SASS   DISQUS integration      Prerequisites   Know how to use  terminal  Ruby  HAML  SASS  and  Markdown     Installation   From the command line     gem install bundler   git clone git   github com mgutz nanoc3 blog git your blog   cd your blog   bundle install    Previewing the Site   Compile the site     cleans output  directory   rm  rf output  works just as well   rake clean    compiles content   and copies static   to output    nanoc compile    runs preview server and autocompile on page refresh   nanoc aco    Preview the site on  http   localhost 3000   Customizing Existing Content   Edit these two files    content about md content sticky md   box content on front page    Delete articles to remove them from site  Then  recompile and restart server    To change the front page   content index haml   To change the site layout   layouts default haml   To style the site   content assets style sass   Adding Content   Edit the Markdown file created by running this command      filename is written to console   rake create article title  Hello world     View your page after compiling or refresh in preview mode    Adding Static Files   Put static files into the  static   folder instead of  content     static    is copied to the  output   folder on compile and preview    Configuration   Edit these files    config yaml atom xml erb    Commenting   DISQUS comment service allows users to post comments on your static site  As such  one must register your site on  DISQUS  to use their service  Once registered  simply uncomment and adjust  disqus shortname  in  config yaml   Uncommenting this setting enables comments in articles    Deploying   Copy  output    to the public folder of your web server    Or  if you use rsync      configuration is in config yml rake deploy rsync    Naming Conventions   Hyphens in file names are converted to subdirectories in the output  You decide how you want to organize your posts       e g  These files render to the same output file  2010 01 01 post haml     2010 01 01 post html 2010 01 01 post haml     2010 01 01 post html 2010 01 01 post haml     2010 01 01 post html    Files may use Rails naming conventions  in which the first extension is retained for the output file and the second determines the template processor    sitemap xml erb     generate sitemap xml using erb processor    If a single extension is used  then the files are assumed to be CSS and HTML     sass      css         html  sbt console template   Get started with Scala and use Intellij IDEA  tested on 11      Scala 2 9 1   ScalaTest 1 6 1     Using   Run the app   sbt run    Run tests   sbt test    Convert to IDEA project       Start sbt   sbt       Within sbt  generate IDEA project     gen idea exit         Open the project in IDEAj from the menu  File    Open Project         Vim gVim Colorschemes   Styled to have     Less distracting splitters   Low contrast line numbers and comments   Unbusy NERDTree     Highly recommend using my custom tabline if you use tabs  Insert this line in   vimrc   let g mgutz tabline 1    GUI Colorschemes     chance of storm   off black with bright colors    screenshot   idle   off white theme based on Python idle  fruidle     screenshot   underwater mod   darkish blue with bright colors    screenshot     Terminal GUI Colorschemes   These are optimized for 256 color terminals and GUI  They should look identical on both  unless terminal uses a custom color palette      t256   black background  black works best on transparency enabled terminals      Generated Colorschemes   The combined terminal GUI colorschemes   t256  for example  are generated from CoffeeScript sources    To customize    npm install npm run build  Kohana WP   WordPress 3  plugin to execute a Kohana MVC framework route and inject the output result into a WordPress page  post or widget    Kohana WP Home    Not yet live    Licensed under  GPL2  to meet the requirement for a WordPress plugin  Frankly  if you need it to use it for commercial purposes just ask   I hope if you find benefit in using the plugin  you consider contributing back to the community    Description   Kohana WP allows developers to use Kohana MVC framewor5k to efficiently and happily build  web applications  Kohana WP is for developers experienced in HTML  Javascript and PHP    Installation   Install through Plugins in WordPress Admin   Plugin is not yet released  Please follow instructions in next section for now    Develop Using Edge Version   Good idea to start with a new database when I change the admin  Settings in DB could be in an invalid state        Install plugin and examples   cd WORDPRESS SITE wp content plugins git clone git   github com mgutz kohana wp git cd    git clone git   github com mgutz kohana wp examples git kohana        Copy and paste entries in  docs htaccess example  entries into  WORDPRESS SITE  htaccess         Default Stack   One of the good things about Kohana MVC is choices  One of the bad things about Kohana MVC is choices    Kohana WP has a default stack      Template Engine   Mustache  code behind class is optional    ORM   TBD  needs to be lightweight data mapper since WordPress has a schema already   Generators   Generators will be web based for a basic application and CRUD generator for Custom Post Types   Testing Framework   TBD     Things to Ponder       The biggest hurdle  beside WordPress  non object oriented framework is the concept of application spaces     Normally  there is    one application using Kohana MVC  Kohana WP allows multiple applications to coexist within    WordPress and each applicatoin is dynamically bootstrapped as needed  You MUST use  app url       controller url  when creating links    to an action or static asset  Kohana MVC applications are at the mercy of WordPress     URLs may change through SEO plugins  user customization etc  Pages may be moved        WordPress path constants do not end with      Kohana path constants do        Directory Structure   Directory structure for applications follows the convention of Kohana MVC applications     WORDPRESS SITE      wp content          kohana              sites                           non member end user tier  premium  is another internal tier at my startup                  all                         selectable apps for this tier                     app name 1                          application                                  classes         controllers  models                                                         modules             app modules  all are loaded  suffix with  off to disable                              auth off                              mustache off                              db                          public              static assets                         system              Kohana MVC framework  optional but recommended                          views               templates and code behind classes                                             app name 2                  default                     default apps for this tier         plugins              kohana wp                       the plugin                 application                 classes to integrate with WordPress                 modules                     custom controller  views and helpers to faciliate creating applications                 system                      default Kohana MVC framework    Constants   WORDPRESS SITE                              ABSPATH     wp content                              WP CONTENT DIR         kohana                              KOHANA APPS ROOT             sites                                        all                                          app name 1              DOCROOT                         application         APPPATH                              classes                          modules             MODPATH                         public                          system              SYSPATH                                             app name 2                  default                          plugins              kohana wp                       KWP DOCROOT                 application                 KWP APPPATH                     classes                  modules                     KWP MODPATH                 public                  system                      KWP SYSPATH    RoadMap   Aug 2010   Version 0 1 concentrate on the default stack  ORM  default teplate engine  multiple applications   WIP  Examples       sites all mustache pizza       Multi step form within a single WordPress page using Mustache templates  To view   add  mustache pizza order  exec route to Kohana WP Integration options on page        sites all php pizza            Multi step form within a single WordPress page using PHP views  To view  enter  php pizza order  as exec route        sites all skeleton             Skeleton app for building a new app using Mustache views  To view  enter  skeleton welcome  as exec route        Mustache   Mustache templates may be used by default by deriving from Controller Mustache  In most cases  a code behind class is not required  Function Metacity   GTK2 Theme   Clean  Gray  Function al metacity theme    Got tired of customizing the colors of Nub whenever I switched wallpapers  Gray goes with everything    Installation   If you have a version    0 4 7  remove   rm  rf    themes  Function  Lambda  Fn      To install  replace tar file with appropriate file       install locally  tar xfz Lambda   version   tar gz  C    themes    OR     install globally sudo tar xfz Lambda   version   tar gz  C  usr share themes    Changelog   0 1 1   Hide menu underscores  mnemonics     Reduce size of expander arrow in treeviews   Selected text to white   Flat popup menu style   Lots of color tweaks   0 1 2   Increased contrast of focused unfocused title bars   Lowered brightness  was too bright    Added panel background   reduced size of maximize icon   0 1 3   Vastly improved Gnome panel     inset active window buttons     inset title text     2px taller   0 1 4   New GTK Theme based on Radiance   Added powder blue color to selection progressbar scrollbar   Lighter panel bg   0 1 5   Lighten TreeView ListView headers   Bluer radio button checkbox   Use blue when pressing scrollbar   Toolbar toggle button depressed  comboboxes seem to use togglebutton shading  bug in Murrine     0 1 6   Fix selected menu items always using black instead of Appearance s selected text color   0 1 7   Set default terminal colors   Use shade of selected bg on toggle buttons   Diagonal hashes on progress bar   0 1 8   Reduced width of scrollbars   Adjusted colors of windows list on panel   Lowered brightness of blue selection color a tad   Consistent toggle button   Flatter comboboxes  everything looks like a button in most gnome themes    0 1 9   Tooltips color white on black   Chiclet buttons for comboboxes toggle buttons   0 2   Cleaned up tabs   0 2 2   x close button   0 2 3   x glyph fix   0 2 4   red hover over close button   0 2 5   base comboboxes  etc on windows bg not input bg   0 2 6   raven  more conrast active inactive window bars   0 2 7   raven  odd even listview colors  doesn t fix nautilus though    raven  select text color   0 2 8   removed hidden window menu button   0 2 9   Rounded corners   0 3 0   Create a seperate Lambda theme set for rounded corners   0 3 1   Strange bug in Appearance Preferences  Switching between Function and Lambda themes does not always change the theme correctly  I m guessing because I m sharing GTK themes  Workaround is to choose another theme like Clearlooks and then Function or Lambda      Improved highlighting to lessen the effect of jagged round corners   Keep square corners on utility windows  GIMP toolboxes     Change nautilus side pane bg to be consistent with open file dialog     0 4 1   Ubuntu 10 10 fixes   0 4 2   X was too faint  Trying red close    0 4 3   Silky smooth metacity corners      Added outline  Windows were melding into light backgrounds    Much better round corners    Improved close button hover      0 4 4     Reduce size of close button by 1px  X glyph was 1 pixel off center      0 4 5     Tweaked outline   Fixed shaded title bar   Improved close button      0 4 6     Minimize looks like an underscore  Increased margin a little    Increased size of close button      0 4 7     Use square border on maximize   GIMP toolbox shade   Use small x for utility windows  GIMP toolbox      0 4 8     Simplified core   Split archives     0 4 9     Moved Function Dot and Lambda Dot to Extras archive   Slightly brighten Raven panel background      0 5 0     Rounding looks more diagonal after Ubuntu 10 10 fixes   Changed SO X shade button     0 5 1     Improved rounding antialiasing on smooth theme corners   Add outline on utility dialogs  GIMP toolboxes    Remove white line under utility dialogs when shaded     0 5 2     Slightly increased smooth focused window gradient   Slightly darkened top bar separator   Changed shade button on SO X themes   SO X Bar Theme   shade button could be improved     0 5 3     Prelight hover on unfocused utility windows   Improved shade button on gloss themes   Slight changes to window button spacing     0 5 4     Window buttons vertical center off by 1 pixel      0 5 5     Darkened terminal selected tab   Tweaked shade button yet again   Status bar resize gripper in Faenza style     0 5 6   IO theme  I find  as a web developer  using a very dark theme strains the eyes more than using a light scheme since I use the browser so much  Going from a black  dark theme to browsing white pages is cause for retina burn      Adjusted shade button again    Adjusted red hover color on leaf theme    Lightened unselected tabs a tad   Reduced status bar window resize gripper      0 5 7     Added left variants for  SO X X  SO X Bar  SO X IO   Repackage into left and right variants   Slight gloss on IO controls   Switched to bzip for archiving     0 5 8   Some usability changes      BorderWidth constant in metacity theme  set value of   3 if you re having problems resizing    More contrast between inactive active tab   Replace Raven with IO as core coding theme     Other fixes      Move Raven to Extras archive   Removed padding in treeviews  scrollwindows     0 5 9     Installer  Verifies Murrine is installed  options for border width and left controls    Simplified menu listview treeview highlights   Slight adjustment to button dimensions   Backbone Examples from Knockout   This project contains  Knockout  examples ported to   Backbone   The motivation  is to learn enough about each to determine which framework best suits my  style    Opinion   My initial impression is Knockout is the more elegant framework as of this writing  However  almost everything  jashkenas    the author of Backbone  has created has been excellent  Backbone s markup is cleaner  which facilitates integrating creative assets from designers  Backbone s  synchronization with RESTful services  could also be a plus  We ll see    Knockout s examples have too much inline javascript in data attributes  Perhaps that is intentional to keep the examples concise  Not sure I like that  Who knows  I m un learning a lot of things and that may be one of those compromises which makes code simpler at the expense of  architectural  correctness    Pre requisites     express    awesome web framework    ejs     Install both via  npm   Examples Ported     Hello World   Click Counter   Simple List     Run It   node app js    TODOS     Use Docco   Create a Pretty Examples Site   node settings   Simple  hierarchical environment based app settings    Installation   npm install settings    Usage   Configuration file  config js   module exports       common        storage          host   localhost         database   server dev         user   qirogami user         password   password                   Rest of environments are deep merged over  common      development        test        storage          database   server test         password   foo               production         storage          password   secret                  Application file  app js   var Settings   require  settings    var config   new Settings require    config        inherited from common assert equal config storage host   localhost       specific to test assert equal config storage password   foo       Environments   The environment to use is based on  highest precedence first         forceEnv  property in config file      config environment js exports forceEnv    production           NODE ENV  environment variable   NODE ENV production node app js        env  option passed to constructor    new Settings file   env   test            Application Defaults   Property defaults may be preset in code    var settings   new Settings file     defaults       framework         views   app views              assert equal settings framework views   app views       Hacking on the source   To compile and test   npm install bake bash  g npm install  d bake test    Notes   globalKey  option has been removed  Do this instead   global APP   new Settings file      Credits   jQuery library for  support extend js  from  FGRibreau   License   Copyright  C  2010 by Mario L  Gutierrez  mario mgutz com   Permission is hereby granted  free of charge  to any person obtaining a copy of this software and associated documentation files  the  Software    to deal in the Software without restriction  including without limitation the rights to use  copy  modify  merge  publish  distribute  sublicense  and or sell copies of the Software  and to permit persons to whom the Software is furnished to do so  subject to the following conditions    The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software    THE SOFTWARE IS PROVIDED  AS IS   WITHOUT WARRANTY OF ANY KIND  EXPRESS OR IMPLIED  INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT  IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM  DAMAGES OR OTHER LIABILITY  WHETHER IN AN ACTION OF CONTRACT  TORT OR OTHERWISE  ARISING FROM  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE  Cake   Cake  is an enhanced version of  cake  and is 100  backwards compatible  as of this writing     jaskenas  wants the  cake  in the  coffee script  distribution to remain light  and simple  Fair enough  Let there be Cake    Enhancements Over cake   TODO section contains enhancements in the works        Asynchronous Tasks   task  generate    options  done         generateCoffeeScript  src    done        Task Dependencies  async aware    task  all    Run all tasks     clean    generate    test          Multi target Invoke  Serially calls each task  async aware    invoke  task1    task2    taskn          doStuff          Installation   For now  use repository   git clone git   github com mgutz coffee cake git cd cofee cake npm link    To test   cd test Cake test    TODO        TODO  File task helpers   task  app js      file    public jss app js       scripts   find  lib scripts       js       recursive     if outdated file  scripts          concatenateFiles file  scripts         TODO  Toolbelt Helper Functions     outdated target  dependenciesList    prependToFile file  text    appendToFile file  text    writeToFile file  text    gsubFile file  pattern  replacement    concatenateFiles file  fileList    removeDirectory dir   recursive  safe     makeDirectory dir   recursive  safe     copyDirectory dir   safe     Bake   Simple Bash build project utility in the style of rake    Not trying to reinvent wheel  Most node project Makefiles are just Bash scripts    Installation   npm install bake bash    Usage   Display tasks   bake    Run task   bake  task     Example Bakefile      sh function private       echo in private       Builds the project   function build         ensures clean is called only once     bake invoke  clean      bake ok building           clean will not run again     bake invoke  clean        Cleans the project   function clean       bake ok  cleaning          private       Renders hello template sh   function render template       bake ok  compiling template          bake render template hello template sh   cat     bake ok  coffe   compiled      function on task not found       echo  Task not found  1          Rules     bake  searches the current and parent directories for a  Bakefile  to run    Tasks are defined as normal Bash functions    A task description is simply a comment that starts with      and precedes   a function      Functions   Prints a red error message    bake error  action   description   example  bake error  compiling   src lib test coffee     Prints a plain message    bake log  action   description   example  bake log  bake   Processing bakefile        Prints a green ok message    bake ok  action   description   example  bake ok  compiling   compiled src lib test js     Prints a cyan info message    bake info  action   description   example  bake info  bake   built project in 700ms     Invokes a task only once    bake invoke  function name   example  bake invoke  clean     Determines if target is older than reference  returning 1 if outdated    bake outdated  target   reference   examples   bake outdated build src    return 1        skip rest of task outdated build src    invoke  compile     compile if outdated    Renders a heredoc file template   bake render template template sh   newfile txt    Run a dynamic task when task   1  is not found  For example  to run a test as the first argument to  bake   add this to  Bakefile   sh function on task not found           f test  1 js       mocha test  1 js    return 0     return 1   Install   npm install mgutz colors    Using   General usage   var color   require  mgutz colors   color  console log color  red on black    red black        Style format is  color attributes backgroundColor attributes    red                red  red b              red bold  red u              red underline  red bh             red bold high intensity  red white          red on white  red b white h      red bold on white high intensity  colors  black  white  red  yellow  green  cyan  blue  magenta  attributes  b bold  h high intensity  u underline    Flexible way to require   var colors   require  mgutz colors        color   colors color     Be lazy and define color functions   var bc   colors fn  black cyan    console log bc  this is black text on cyan        Turn off colors easily   colors plain   true  console log bc  this is plain now        Get color code of high intensity red on black background      use ansi escape sequences directly for speed  like logging  var redOnBlack   colors ansi  red h black   console log redOnBlack   message   colors reset      Output all color combinations in terminal   npm test    License    The MIT License    Copyright  c  2012 Mario Gutierrez  mario mgutz com   Permission is hereby granted  free of charge  to any person obtaining a copy of this software and associated documentation files  the  Software    to deal in the Software without restriction  including without limitation the rights to use  copy  modify  merge  publish  distribute  sublicense  and or sell copies of the Software  and to permit persons to whom the Software is furnished to do so  subject to the following conditions    The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software    THE SOFTWARE IS PROVIDED  AS IS   WITHOUT WARRANTY OF ANY KIND  EXPRESS OR IMPLIED  INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT  IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM  DAMAGES OR OTHER LIABILITY  WHETHER IN AN ACTION OF CONTRACT  TORT OR OTHERWISE  ARISING FROM  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE  Func d   Template engine in the style of Builder  Markaby  Erector      Blocks   Layouts   Mixins   Just functions   Partials   Safe HTML     Installation   npm install funcd    Features   Using   Funcd   require  funcd      Layouts and partials   layout         doctype 5    html         head           script src   js jquery js         block  page scripts       body           block  body   footer    text        div id   footer   text  page    name        extends layout     block  body           h1  Simple Page       div  Hello   name        render footer  page1   html   Funcd render page   kitty       Mixins   mixins     info   block          div class   info   block  template         info         div  bah      div class  info   div bah  div   div  Funcd render mixins  mixins  template    Safe HTML      a 1  lt  2  a  Funcd render     a  1   2      a  i apple  i   a  Funcd render     a  raw   i apple  i       Render from files     test coffee module exports    name  city        div name         city     div foo San Diego  div  Funcd renderFile      dirname  test coffee    foo    San Diego     Licensed Under MIT License   See the file LICENSE  Luv   Build utility like Rake for  luvit   Installation   No npm like utility yet      git clone https   github com mgutz luv git    symlink the script ln  sf  pwd  luv bin luv   bin    Usage   Create a  Luvfile lua  in your project s root directory       lua    Specify task depencencies in 3rd argument  task  build    Builds the project      clean    function     print  build your project here   end       All tasks must have  name    description  and  action   task  clean    Cleans the project    function     print  cleaning   end        See all tasks   luv    Run a task   luv build    Specify task options    lua    Encode task options in the description  lapp  style local desc      Creates distribution    a   archive name  string   Archive name     task  dist   desc    build    function self    print  Creating distribution    self args  archive name    end  Mapper   Mapper makes 80  of data access easy and provides unobtrusive access to SQL for the 20  complicated  speed critical tasks    Motivation   Wanted a lightweight data mapper that is fast and likes SQL    Install   To use mapper in your project     For Ubuntu sudo apt get install libmysqlclient dev  npm install mapper    To run Backbone or AngularJS Example   git clone git   github com mgutz mapper git cd mapper npm install  d make test                     creates necessary database and config json node example app js    then browse  http   localhost 3000   TODO   Connection pooling   adding SOON    Quickstart   Conect to Database   var Mapper   require  mapper    var conn     user   grace   password   secret   database   app dev         set verbose flag to trace SQL    set strict to be warned of invalid columns in JSON objects Mapper connect conn   verbose  true  strict  false       Define Data Access Objects      Table name and optional primary key var Comment   Mapper map  Comments       Post   Mapper map  Posts    id       Define Relationships   Post hasMany  comments   Comment   postId    Comment belongsTo  post   Post   postId       Create   var insertId      These are equivalent  where first is more SQL like Post insert   title   First Post     exec function err  result        insertId   result insertId      Post create   title   First Post     function err  result               Retrieve      Select inserted post Post where   id  insertId    one function err  post        assert equal post title   First Post          Post findById insertId  function err  post               Update      update inserted post Post    update                              optional since set   is used    set   title   New Title        where   id  insertId       exec function  err  result        assert equal result affectedRows  1             if doc has id set  then save is simple  Note     pluck only the columns you want updated Post save doc  function err  result               Delete      delete all posts with a specific title Post delete   where   title   New Title     exec function err  result        assert equal result affectedRows  1        Post deleteById insertId  function err  result          Gets the first page of posts and populate comments property with the second page of comments for each post retrieved    Post    select  id    title    excerpt      page 0  25     order  id DESC      load  comments   function c        c select  comment    createdAt         order  id DESC         page 1  50           all function err  posts           boo yah           OR  if you prefer SQL   var sql     SELECT id  title  excerpt FROM  Posts                ORDER BY id DESC LIMIT 0  25    Post all sql  function err  posts      Post load  comments   function c        c sql  SELECT comment  createdAt FROM Comments ORDER BY id DESC LIMIT 1  50         in posts  function err           boo yah               SQL goodness   Executing multiple statements in a series   Mapper client execSeries     SELECT   FROM posts WHERE author        1         SQL may be separated by        SELECT         FROM comments WHERE author        1      function err  results           posts are in results 0  0  n         comments are in results 1  0  n            Executing multiple statements in parallel   Mapper client execParallel     SELECT   FROM posts WHERE author        1      SELECT   FROM comments WHERE author        1     function err  results              Benchmarks   Time for 100 000 iterations alternating between insert and select  See  test bench  or run  make bench     time node test bench testMysql js  mysql 2 0 0 alpha3   real        1m27 239s user        0m58 506s sys         0m3 288s  time node test bench testMapperDao js  real        0m30 701s user        0m11 346s sys         0m4 403s  time node test bench testLibMysql js  real        0m26 044s user        0m8 207s sys         0m3 784s  time node test bench testMongo js  just for fun   real        0m41 771s user        0m30 830s sys         0m2 910s    The takeaway is  mysql libmysqlclient  is a much faster driver than the widely used  mysql  driver  Mapper  which is based on  mysql libmysqlclient  adds overhead yet outperforms the raw  mysql  driver    Even more surprising is Mapper is faster than MongoDB using the official MongoDB driver for node js    Implementation Best Practice   A simple approach  without over engineering your project  is to maintain 3 distinct layers in your code      Data Access Objects  DAO    Responsible for interacting with the database     There should be 1 DAO for each table used by project    Models   A model uses one or more DAO adding business logic  validations as needed    Resources or Services   This layer should only use models never DAO      On a more complex project where a few tables might be better stored in Redis for example  insert a Repository layer between DAO and models to insulate models completely from low level data access  mygrate   Database migrations for MySQL  PostgreSQL and SQL Server database using plain SQL files  The end goal is to have migration scripts that can be run by a DBA    Installation   npm install  g mygrate    Walkthrough   For a project without migrations  mygrate creates a  migrations  directory and  migrations config js  example which MUST BE edited for your database  Run one of the following commands   mygrate init postgresql               Postgresql mygrate init mssql                    Sql Server mygrate init mysql                    MySQL    Next step is to create the database in the config if it does not already exist  Mygrate will prompt for root user and password  Run one of the following   mygrate createdb                      creates development database NODE ENV test mygrate createdb        creates test database    To create a set of migration scripts  run the following command changing  add tables  to describe your migration   mygrate new add tables    That command creates  migration TIMESTAMP add tables  down up  sql   Edit  up sql  which is run by  mygrate up  command  Edit  down sql  which is run by  mygrate down  command    To run migrations  do any of the following   mygrate up                             migrate all scripts mygrate down                           down 1 migration mygrate down 2                         down 2 migrations mygrate down all                       down all migrations mygrate down TIMESTAMP some script     down to migration before this one mygrate last                           down  if needed  then up last dir    To view migrations applied to the database   mygrate    To target specific environments   development  is default   NODE ENV test mygrate up    Compiling   make compile  Mapper example   Mapper DAO example  Yup another svbtle obtvse clone cuz I m simple like that    Install   Install simple Bash build tool to run  Bakefile   npm install bake bash  g npm install  d    See all tasks   bake    Create database  ensure MySQL is running   put in root password   bake create db    Run server   bake run    Passwords   Admin account   user  admin password  password    Credit   Modified from  gorekee obtvse node  et  al  THIS PROJECT IS NO LONGER MAINTAINED  Please seek other alternatives      the project cannot be updated on npm due to camelCased title   i no longer do Windows   both iojs and node js v0 12 have execSync function   older node users can try sync exec from npm  suggested by one of the commentors      execSync   Executes shell commands synchronously    WARNING  For dev machine shell scripting only   DO NOT USE  for production servers    Install   Windows  requires Python and Visual Studio 2012  Express  installed for node to build  See  node gyp installation   Pre built binaries for node v0 8 and node v0 10 are packaged  They should work and if not try manually building    npm install execSync    Sometimes a manual build is necessary on Windows even with all the tools in place  replace Visual Studio version with  2010  or  2012  based on the version installed    npm install node gyp  g node gyp rebuild   msvs version 2012    Usage   Require it   var sh   require  execSync       Run  does not capture output    var code   sh run  echo  USER  echo some err 1  2  exit 1    console log  return code     code      Use the less efficient  exec  if you need output   exec  is just redirection trickery around  run     var result   sh exec  echo  USER  echo some err 1  2  exit 1    console log  return code     result code   console log  stdout   stderr     result stdout      Notes   In  nix and OSX version commands are run via  sh  c YOUR COMMAND   In  Windows  commands are run via  cmd  C YOUR COMMAND   License   Copyright  c  2012  2013 Mario Gutierrez mario mgutz com   See the file LICENSE for copying permission  IntelliJ Hello Project   Example vert x 1 1 0 final Intellij project to run debug    Installation     Clone the repository     sh     git clone git   github com mgutz vertx web hello git       Make Project  Ctrl F9        Chose  hello  configuration       Run or Debug       Browse http   localhost 8080     Mapper pg   A lightweight PostgreSQL data mapper that likes SQL    Install   To install   npm install mapper pg   save    To test it   npm install  d make test    To run Backbone example   make test               creates the necessary database and table node example app js     runs the server  browse http   localhost 3000    TODO     Transaction support     Documentation   See comprehensive tests in  test integration integrationTest js     Quickstart   Require it   var Mapper   require  mapper pg       Define Data Access Objects  DAO  for each table      simple  only table name with optional primary key var Comment   Mapper map  Comments    var Post   Mapper map  Posts    id       Define relationships beetween DAOS  see  lib relation js   Post hasMany  comments   Comment   postId    Comment belongsTo  post   Post   postId       Initialize   var config         user   boo       password   secret       database   app dev       verbose  true            to trace SQL     strict  false            to check for invalid column warnings     driver  require  pg      optional  if not set then pure javascript                              version is used        this must be called after all DAOs have been defined Mapper initialize config  function err           setup express  etc        CRUD Examples   Create   var insertId      insert a new post Post insert  title   First Post    exec function err  result        insertId   result id          OR sugar Post create  title   First Post    function err  result        insertId   result id         Retrieve      select inserted post Post where  id  insertId   one function err  post        assert equal post title   First Post             OR sugar Post findById insertId  function err  post          Update      update inserted post Post    update                              optional since set   is used    set  title   New Title       where  id  insertId      exec function  err  result        assert equal result rowCount  1             OR sugar  updates based on id Post save  title   New Title   id  insertId   function err  result          Delete      delete all posts with a specific title Post delete   where  title   New Title    exec function err  result        assert equal result rowCount  1           OR sugar Post deleteById insertId  function err  result          Gets the first page of posts and populate comments property with the second page of comments for each post retrieved    Post    select  id    title    excerpt      page 0  25     order  id DESC      load  comments   function c        c select  comment    created at         order  id DESC         page 1  50           all function err  posts           boo yah           Or  mix SQL   var sql     SELECT id  title  excerpt FROM Posts ORDER BY id DESC LIMIT 25    Post all sql  function err  posts      Post load  comments   function c        c sql  SELECT comment  createdAt FROM Comments ORDER BY id DESC LIMIT 50 OFFSET 50         in posts  function err           boo yah               SQL goodness   Execute multiple statements in a series   Mapper client series      SELECT   FROM posts WHERE author         1         use commas to break up SQL    SELECT         FROM comments WHERE author         1     function err  results           posts are in results 0  0  n         comments are in results 1  0  n         Execute multiple statements in parallel   Mapper client parallel      SELECT   FROM posts WHERE author         1      SELECT   FROM comments WHERE author         1      function err  results                   vpsbench   Benchmark VPS performance  See  user submissions   A script to run simple and comprehensive benchmarks on CPU and IO performance    Tested on      Debian 6   Debian 7   Debian 8   Ubuntu 10 04 LTS   Ubuntu 12 04 LTS   Ubuntu 14 04 LTS     Usage   Usage  vpsbench  OPTION       a Bench all  d Bench downloads  f Create 100M bigfile  x Remove temporary files  u Bench unixbench    Debian pre requisites   apt get install time bzip2    Example     bash   wget   no check certificate  O   https   raw github com mgutz vpsbench master vpsbench   CPU model   Intel R  Core TM  i7 3770 CPU   3 40GHz Number of cores  4 CPU frequency   3417 879 MHz Total amount of RAM  3265 MB Total amount of swap  1021 MB System uptime    8 41  I O speed   427 MB s Bzip 25MB  4 66s Download 100MB file  1 64MB s    License   Licensed under  The MIT License owncloud ssl   Installs  owncloud   an open source DropBox clone  onto a Debian based server with SSL certificate    Motivation   This is my data  There are many like it  but this one is mine      Without my data  I am useless    I use a  7 mo 250GB  256M RAM storage plan from  BuyVM  running Debian 6 06    How To   Log into your remote server   Download the script   wget https   raw github com mgutz owncloud ssl master install owncloud server    Edit IP ADDR and HOST NAME variables in  install owncloud server   On Ubuntu  recommended with encrypted partition   50MB RAM footprint   sudo bash install owncloud server    Or Debian  as root    40MB RAM footprint   bash install owncloud server    Server Side Encryption   BEWARE  OwnCloud server side encryption is disabled because it is known to have issues and is being addressed in the next major version    My suggestion is to not deal with it and install ownCloud on Ubuntu Server 12 04 or greater and use encrypted LVM partition when installing the OS  Your entire server data then is safe from prying eyes as hackers and even your hosting provider will only see gibberish including file and directory names  Encryption comes with a huge performance cost which is more than acceptable for privacy  Welcome to Log Magic    This project is usable  It doesn t do everything it should  yet    The goal is to have a fast and easy to use logging subsystem that can be dynamically reconfigured to provide insight into production systems    Logmagic does its magic by generating objects with generated functions that are only modified when the logging system is reconfigured   thus your entire logging path is contained within long lived functions that V8 is able to JIT    Getting Started   If you had a file named like   lib foo bar js   at the top of it  you would put the following    var log   require  logmagic   local  mylib foo bar       Then inside bar js  you would just use the logger like any normal logger    log info  Hello    log error  By default  format strings are not used     SOME VAR   myvalue    log errorf  Just add  f  to any log method  and you get format strings too    SOME VAR     SOME VAR   myvalue       In any other part of your application  you can reconfigure the logging subsystem at runtime  making it easy to change log levels for specific modules dynamically       Register an ad hoc sink    var logmagic   require  logmagic    logmagic registerSink  ad hoc   function module  level  message    console log message           Send Info an higher in the root logger to stdout    logmagic route    root     logmagic INFO   console       Reconfigure all children of mylib to log all debug messages to your custom sink    logmagic route  mylib     logmagic DEBUG   ad hoc      Sinks   Sink modules should have this interface          the log message callback      callback  function modulename  level  message  obj           sets options for sink      setOptions  function options           dispose of resources      dispose  function           Registering a sink instance verbosely   var fileLog   new logmagic sinks File  filename    var log myapp log     logmagic registerSink  main   fileLog      Registering the easy way for  Console    File  and   Recipients   logmagic registerFileSink  fileLog     var log myapp log    logmagic registerConsoleSink  console    dark       Setting options on a sink instance   logmagic setSinkOptions  console    plain  true    logmagic setSinkOptions  console    scheme   light        Routing to multiple sinks   logmagic registerRecipientsSink  multi     fileLog    console     logmagic route    root     logmagic INFO   multi      Pre registered sinks     console   grayLog2 stderr     Built in sinks     Console   Logs to console with colors  may be disabled    GrayLog2   Graylog2 style JSON to stderr   File   log to a file   Recipients   log to multiple registered sinks     See  tests t js  for an example  Introduction   The  Dropwizard  example integrated with  Atmosphere  rest chat example    Overview   Dropwizard is a high performance JVM REST framework built upon proven production JVM technologies    Atmosphere provides server side push technology using websockets when it can and gracefully degrades to long polling for browsers that suck    Running The Application   To test the example application run the following commands        To package the example run    mvn package        To run the server run    java  jar target dropwizard example 0 6 2 jar server example yml        To test chat  which purposely resides in its own directory to excercise CORS       Browse the file   app chat html        To test the bad word filter  enter any message with   NET  and it will be replaced with              ansi   Package ansi is a small  fast library to create ANSI colored strings and codes    Install   Get it   sh go get  u github com mgutz ansi   Example      go import  github com mgutz ansi       colorize a string  SLOW msg    ansi Color  foo    red b white        create a FAST closure function to avoid computation of ANSI code phosphorize    ansi ColorFunc  green h black   msg   phosphorize  Bring back the 80s    msg2    phospohorize  Look  I m a CRT         cache escape codes and build strings manually lime    ansi ColorCode  green h black   reset    ansi ColorCode  reset     fmt Println lime   Bring back the 80s    reset        Other examples   go Color s   red                 red Color s   red d               red dim Color s   red b               red bold Color s   red B               red blinking Color s   red u               red underline Color s   red bh              red bold bright Color s   red white           red on white Color s   red b white h       red bold on white bright Color s   red B white h       red blink on white bright Color s   off                 turn off ansi codes   To view color combinations  from project directory in terminal    sh go test   Style format   go  foregroundColor attributes backgroundColor attributes    Colors     black   red   green   yellow   blue   magenta   cyan   white   0   255  256 colors      Foreground Attributes     B   Blink   b   bold   h   high intensity  bright    d   dim   i   inverse   s   strikethrough   u   underline     Background Attributes     h   high intensity  bright      Constants     ansi Reset   ansi DefaultBG   ansi DefaultFG   ansi Black   ansi Red   ansi Green   ansi Yellow   ansi Blue   ansi Magenta   ansi Cyan   ansi White   ansi LightBlack   ansi LightRed   ansi LightGreen   ansi LightYellow   ansi LightBlue   ansi LightMagenta   ansi LightCyan   ansi LightWhite     References   Wikipedia ANSI escape codes  Colors   General  tips and formatting   What about support on Windows  Use  colorable by mattn   Ansi and colorable are used by  logxi  to support logging in color on Windows    MIT License   Copyright  c  2013 Mario Gutierrez mario mgutz com   See the file LICENSE for copying permission  Inj   Inj is a KISS   require  based dependency injector  DI  for node js     no monkey patches   no autowire needed  just use  require     Install   To install from npm   npm install inj   save    Getting started   To use Inj  add a single line to any file into which dependencies are injected      use Inj if container defined  othwerwise use require require   require  inj   module  require      The above line is a noop unless a container is defined  If a dependency is registered with Inj resolve it  otherwise use  require     A container is nothing more than a hash object to contain objects  values and functions later looked up by id    Application Dependency Injection   Inj starts with no containers  To register app wide dependencies  the root container must first be created and dependencies registered  In practice  the single root container is the only container an app creates  Module specific containers are primarily used in testing    container js    Define dependencies      Create ROOT container and register dependencies var root   require  inj   getSetRoot          naming convention emphasizes logical dependencies root register   logger   function name    require  logger   name       root register   connectionString    mysql   foo password localhost db      app js    App entry point   require    container    require    store       store js    Use DI   require   require  inj   module  require    var log   require   logger    store    var connstr   require   connectionString    log log  Connection string   connstr     model js    Use DI   require   require  inj   module  require    var log   require   logger    model    log log  Inside model      Using Depency Injection for Tests   Here s an example of how to mock a built in module    reader js   require   require  inj   module  require   var fs   require  fs    exports text   fs readFileSync   dirname      file txt    utf8       readerTest js      create module specific container var inj   require  inj    var container   inj create require resolve    reader         register a mock fs container register  fs    readFileSync  function     return  foo           test the module var a   require    reader    assert a text   foo       CoffeeScript   CoffeeScript autocreates variable  The statement must be escaped as regular JavaScript with backticks     require   require  inj   module  require      MIT License   Copyright  c  2013 Mario Gutierrez  mario mgutz com   See the file LICENSE for copying permission  plv8 bedrock   Migration and JavaScript foundation for PostgreSQL plv8    Bedrock is an example project that lets you code PostgreSQL functions using JavaScript or CoffeeScript while using many of the available node js npm modules   browserify  bundles your code and dependencies into a self contained bundle  The bundle is loaded into the plv8 runtime via  mygrate   which uses  psql  compatible SQL files    Prerequisites   Requires  node js  and  psql  cli utility   Ubuntu   Tested using  pgdg apt instructions   In addition to installing postgresql 9 3  install plv8 engine   sudo apt get install postgresql 9 3 plv8    Mac   Tested with  Postgres app   Getting Started   Clone the project   git clone https   github com mgutz plv8 bedrock    Install dependencies     use   force to upgrade if mygrate was previously installed    mygrate pre will be updated often until I release 0 2 0  npm install mygrate 0 2 0 pre  g   force npm install    Create the database which requires a superuser with password   mygrate createdb    Run migrations   mygrate up    View migrations history   mygrate    Test it interactively     start psql mygrate console     load JS bundle select plv8 startup        script source in plv8 app example coffee    function wrapper in migrations 201312220022 plv8 startup up functions sql select app hello  world    select app add person     firstName    barney    lastName    rubble    likes     node js    plv8    postgres     meta      eyes    brown      json      Or  run tests   npm test    Workflow   For new tasks       Choose either   a  Create a migration for SQL only  no plv8      up sql and down sql only mygrate new user auth    Add or update tables  functions in  up sql    Add corresponding revert logic in  down sql     b  Create migration with plv8 bundling   mygrate new user auth  t plv8    Edit JS sources in  plv8    Add function declarations to  migrations CURRENT up functions sql     NOTE  The plv8 directory resides outside of migrations for easy version control  A snapshot of the source is bundled by each migration  In practice  you would have many  plv8  migrations corresponding to milestones        To update the database and mygrate down up your last migration created     above   mygrate last        Run unit tests  See  plv8 test index js  and  plv8 test exampleSpec js      which uses the minimal  plv8 microspec  test library    mygrate migrations test sql        Repeat steps 2  3 as needed  A simpler way to edit and test is   npm test      runs 2 and 3    NOTE    The  minHookDate  property in migrations config json is the timestamp for which migration to run  The prehook in this project updates the JavaScript bundle  You do not want those to run for migrations that have already been applied to the production database  To skip previous hooks  set the  minHookDate  to your current folder timestamp    Globals   These globals are added for convenience     App    The app namespace   console    node js console     require    Access packages in bundle   These packages may be required in SQL scripts        test     lib spec     To expose more  edit  migrations MIGRATION prehook   Alternatively  add properties classes to  App  which is a global variable  In JS and Coffee source any package may be required just like in node js    Here is an example of their use   do language plv8      var spec   require    lib spec      spec options  colorful  true      var test   require    test      test run               Best Practices and Tips       Change the  App  namespace in  plv8 index js       Quickest way to reset the dev database is  mygrate createdb       Define functions by delegating the work to a function in the plv8     directory    CREATE OR REPLACE FUNCTION app add person person JSON  RETURNS int AS        return App example addPerson person      LANGUAGE plv8 IMMUTABLE STRICT         To minify   MINIFY 1 npm test        Want to add other language transpilers  If there s a browserify plugin      add it to migrations MIGRATION prehook       To create test or production database   NODE ENV test mygrate createdb NODE ENV production mygrate createdb        LICENSE    The MIT License    Copyright  c  2013 Mario Gutierrez mario mgutz com   Permission is hereby granted  free of charge  to any person obtaining a copy of this software and associated documentation files  the  Software    to deal in the Software without restriction  including without limitation the rights to use  copy  modify  merge  publish  distribute  sublicense  and or sell copies of the Software  and to permit persons to whom the Software is furnished to do so  subject to the following conditions    The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software    THE SOFTWARE IS PROVIDED  AS IS   WITHOUT WARRANTY OF ANY KIND  EXPRESS OR IMPLIED  INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT  IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM  DAMAGES OR OTHER LIABILITY  WHETHER IN AN ACTION OF CONTRACT  TORT OR OTHERWISE  ARISING FROM  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE  plv8 mantle   The underlying core for  plv8 bedrock   a PostgreSQL plv8 project foundation for JavaScript CoffeeScript get it done types    LICENSE   The MIT License   Copyright  c  2013 Mario Gutierrez  mario mgutz com minconf   minconf  is a minimal configuration library    minconf  loads an application s configuration based on the value of an environment variable  e g   NODE ENV    minconf  supports merging of environment variables and command line arguments    Configuration   Define environment configurations as objects or in files    var configs                    defines merge order of object properties  files  command line arguments and the environment     envs          development   common config local json ARGV ENV         test   common test ARGV ENV         production   common production ARGV ENV                common        database          user   superuser        password   password                test        database          password   secret                production   require    config production json         Configure  minconf   In this example  the environment is chosen based on the value of  NODE ENV  environment variable  If  NODE ENV  is not set  it defaults to   development      var config   MinConf load configs  config     When running in  development   config database user     superuser  config database password     password     When running in  NODE ENV test   config database user     superuser  config database password     secret     Merging Precendence   Merging occurs left to right  In this example    development   common config local json ARGV ENV     The base opbject is  common   then overriden by a file  config local json   then overridden by  ARGV   which are the command line arguments and then overriden by  ENV   which are the process environment variables  In other words  environment variables override command line arguments which override a local configuration file which overrides the base object    WARNING  Avoid merging environment variables  Environment variables can be affected by parent process or other init scripts    Overriding Selector   To override the environment variable seletor and default enviroment  set  options envSelector  and  options defaultEnv  respectively   var configs                    options            envSelector   NODE ENV           defaultEnv   development           wd  process cwd                   envs                                   Running your app   Use any combination of config files  environment variables and command line arguments    NODE ENV test node app js   db password  secret     Use  env  command to use periods in environment variables  It is recommended to use command line arguments instead    env  db user foo   db pass secret  node app js    License   The MIT License   Copyright  c  2014 Mario Gutierrez  mario mgutz com   See the file  LICENSE  for copying permissions  dalicious   dalicious  is a toolkit for your data access layer      intuitive transactions and models   little sugar   multi database support in same process     Currently supports    Postgres Sql Server    Example   Initializing   var dalicious   require  dalicious    var Mssql   require  dalicious mssql    var Store   Mssql define  user   sa   password   secret     var Dal   dalicious define Store   var Model   dalicious Model      define your models using prototypical inheritance  Model base can    be your own class intead of dalicious Model function User       Model apply this  arguments     util inherits User   User prototype create   function email  cb      this dao query  insert   email  email   returning    id     one cb           register models Dal dal  User    model  User  table   Users     Dal dalify function err         dal is ready for use        Using   var dal   new Dal        Use your model dal User create function err  id                  Use low level data access object  DAO  dal store UserDao findById id  function err  row                 dal store UserDao query     select    id    name      where   id  id     all function err  rows                    Use plain sql dal    sql  select   from Users where createdAt    1    new Date  1 9 2013        all function err  rows                      use T sql  sql server  dal store    tsql       insert into images  image  output inserted id values   buffer         buffer  buffer         all function err  rows                     Transactions   var tx   dal transactable    var userId   async series     function cb        tx begin cb         function cb        tx User query      function err  id    userId   id  cb            function cb        tx Account query             function cb        tx sql             function err      if  err  return tx rollback      tx commit          Multi DB   var dalicious   require  dalicious    var Mssql   require  dalicious mssql    var MainDb   Mssql define  user   sa   password   secret   database   main     var ReportDb   Mssql define  user   sa   password   secret   database   report      var Dal   dalicious define MainDb   var dal   new Dal    var ReportDal   dalicious define ReportDb   var reportDal   new ReportDal            Understanding   Data Access Layer  DAL      has a database specific store   has 1 or more models     Store     database storage   has 1 or more data access objects     Model     encapsulates business logic   may use one or more Data Access Objects   may use plain SQL or store specific dialects  mssql tsql    has a default Data Access Object named   this dao     Data Access Object  DAO      1 1 mapping to table or collection   data access logic not business logic   has a  schema  for inspection   express api   Express api is an API framework based on Swagger    Express api uses CSON  JSON or YAML files to expose your apis using the eapiger specs  Express eapi reloads schemas in development facilitating schema creation    DO NOT USE  WIP  Documentation is inaccurate    Using   Install the library   npm install express api    Create resource listings schema and resource schema files in a common directory  eg  public api docs   Here are the official examples     petstore resource listing   petstore resource     Implement your resource API  NOTE  the name of your resoure methods must match the  nickname  property in the schema operations        pet json apis       path    pet  petId      operations         summary   Find pet by ID       nickname   getPetById                   resource pet js exports getPetById   function req  res        Wire it all up       var express   require  express    var app   express    var Swag   require  express eapi         create routers for API and docs var apiRouter   express Router    var docsRouter   epxress Router    app use   api   apiRouter   app use   api api docs   docsRouter     var eapi   new Swag     apiRouter  apiRouter    docsRouter  docsRouter    docsDir   public api docs     extname    json        eapi    addApi  pet json   require    resources pet js       addApi  user json   require    resources user js       configureDocs  index json    http   localhost 8000 api      app listen 3000         Browser your json schemas at  http   localhost 3000 api api docs  or through the eapiger UI    Middleware   The primary reason for creating express api is the lack of support for plain middleware in other frameworks    The implementation method can either be a function or array of functions    exports getPetById   function req  res  next            exports getPetById       validationMiddleware   body   id  joi integer         function req  res  next                 That can be a bit tedious  Express api can use middleware as plugins  Let s create one    function validate req  res  next         get the spec for the current operation   var spec   req   eapiger operation    var params   spec parameters    params forEach function param        if  param required    param paramType      path            if   req params param name   res send 400   Require argument missing    param name                  next         To use it   eapi    use validate     addApi  pet json   require    resources pet js       The position matters  All middleware used before  addApi  are applied before a operation method in the pipeline  Post filters look like this    eapi    addApi  pet json   require    resources pet js       use normalizeResult    gosu colors   Installation   Use Vundle or Pathogen   Screens   bluetiful     illigant     License   The MIT License goa   godoc   import  github com mgutz goa     Package goa passes file assets through a pipeline  in which each asset is processed by a one or more filters    Goa can be used in any project but it synergizes well with Godo    p Task  add copyright   function         pi    goa NewPipeline       pi Pipe          Load         go            AddHeader  Copyright 2014 Mario Gutierrez n            Write          Run         Usage   go var Verbose   false   type Asset   go type Asset struct       bytes Buffer     Info  godo FileAsset        WritePath is the write destination of the asset      WritePath string     Pipeline   Pipeline     Asset is any file which can be loaded and processed by a filter    func   Asset  ChangeExt   go func  asset  Asset  ChangeExt newExt string   ChangeExt changes the extension of asset WritePath  ChangExt is used by filters which transpile source  For example  a filter for Markdown would use ChangeExt   html   to write the asset as an HTML file    func   Asset  Dump   go func  asset  Asset  Dump   string  Dump returns a console friendly representation of asset  Note  String   returns the string value of Buffer    func   Asset  Ext   go func  asset  Asset  Ext   string  Ext returns the extension of asset WritePath    func   Asset  IsText   go func  asset  Asset  IsText   bool  IsText return true if it thinks this asset is text based  meaning it can be manipulated with string functions    func   Asset  MimeType   go func  asset  Asset  MimeType   string  MimeType returns an educated guess of the content type of asset    func   Asset  Rewrite   go func  asset  Asset  Rewrite bytes   byte   Rewrite sets the buffer to bytes    func   Asset  RewriteString   go func  asset  Asset  RewriteString s string   RewriteString sets the buffer to a string value    type Pipeline   go type Pipeline struct       Assets     Asset     Filters   interface       Pipeline is a asset flow through which each asset is processed by one or more filters  For text files this could be something as simple as adding a header or minification  Some filters process assets in batches combining them  for example concatenating JavaScript or CSS    func  Pipe   go func Pipe filters    interface      Pipeline  error   Pipe creates a pipeline with filters and runs it    func   Pipeline  AddAsset   go func  pipeline  Pipeline  AddAsset asset  Asset   AddAsset adds an asset   func   Pipeline  Pipe   go func  pipeline  Pipeline  Pipe filters    interface     Pipeline  Pipe adds one or more filters to the pipeline  Pipe may be called more than once    Filters are simple function  Options are handle through closures  The supported handlers are       Single asset handler  Use this to transorm each asset individually    AddHeader filter is an example       signature   func  goa Asset  error       Multi asset handler  Does not modify the number of elements  See   Write filter is an example        signature   func assets    goa Asset  error       Pipeline handler  Use this to have unbridled control  Load filter   is an example       signature   func  Pipeline  error       func   Pipeline  Run   go func  pipeline  Pipeline  Run    Run runs assets through the pipeline    func   Pipeline  Truncate   go func  pipeline  Pipeline  Truncate    Truncate removes all assets  resetting Assets to empty slice  razor   razor  is a CLI code generator to compile  Razor like  templates into go functions    razor  is fast and escapes all values by default    On 2012 i5 Macbook Pro  See  benchfiles  directory   BenchmarkGoTemplate   100000   28250 ns op    3712 B op    60 allocs op BenchmarkRazorByName  500000    7409 ns op    2533 B op    18 allocs op BenchmarkRazorByFunc  500000    7332 ns op    2533 B op    18 allocs op    Layout  views layout go html      html         params  title string           html          title      RenderBody         RenderSection  scripts            Page  views index go html      html         params  name string       return Layout  Welcome     name           Welcome to homepage   This is the body    section scripts                 alert  hello   name                 Run from terminal   sh razor     Start server      go import        views       github com mgutz razor      func main         razor SetAppState razor M           version    1 0 0              http HandleFunc      func w http ResponseWriter  r  http Request            views Render  index    Joe   WriteTo w             or views Index  Joe   WriteTo w            http ListenAndServe   8080   nil          Why   razor     Faster  roughly 3 3 5x faster  see  benchfiles     Compiles as go functions  Spot errors quickly    Use any package of helper functions directly in templates      Note that all inserted values are escaped unless it returns a  razor SafeBuffer      Familiar go syntax        razor      for hobby    range hobbies        p  hobby  p          html template  sometimes   othertimes        range  hobby     Hobbies         p     hobby     p     end           Less reflection       html template     Fast enough   Standard     Usage   Install   sh go get  u github com mgutz razor cmd razor   Running   sh razor  folder or file   output folder or file    Building views efficiently with  gosu     get gosu task runner go get  u github com mgutz gosu cmd gosu  cd  GOPATH src github com mgutz razor gosu example    Restart server on view change   gosu example   watch    Rendering Templates   There are two ways to render a template       By name  This is useful when the name of the view is dynamic such as a value from a URL segment      This function is defined in  razor render go  in each package    views Render  index   data  WriteTo w         By func  slightly faster     views Index data  WriteTo w         The default code generation mode accepts a  single   interface    argument and performs a type assertion inside the function       params  user  model User  func GeneratedFunc   data interface   razor SafeBuffer       user      data   model User               This above signature is necessary for the  Render name string  data interface     function  If you run  razor   strong SOME DIR  no type assertions are used and unlimited arguments are allowed  It is slightly faster too        params  user  model User  other string  func GeneratedFunc user  model User  other string   razor SafeBuffer                Example   See  working example       Description   Template   Generated code                                                 View     index go html     index go      Layout    default go html     default go      What is implemented    While  razor  is based on Micorosoft s implementation   razor  is geared towards being a template engine with layout support  Microsoft helper functions are not implemented  Import a helper package directly in a template    Credit   This package uses the lexer parser of  sipin gorazor   str   import  github com mgutz str     Package str is a comprehensive set of string functions to build more Go awesomeness  Str complements Go s standard packages and does not duplicate functionality found in  strings  or  strconv     Str is based on plain functions instead of object based methods  consistent with Go standard string packages    str Between   a foo  a      a       a        foo     Str supports pipelining instead of chaining   s    str Pipe   nabcdef n   Clean  BetweenF  a    f    ChompLeftF  bc       User defined filters can be added to the pipeline by inserting a function or closure that returns a function with this signature   func string  string    Index     Variables   func  Between   func  BetweenF   func  Camelize   func  Capitalize   func  CharAt   func  CharAtF   func  ChompLeft   func  ChompLeftF   func  ChompRight   func  ChompRightF   func  Classify   func  ClassifyF   func  Clean   func  Dasherize   func  DecodeHTMLEntities   func  EnsurePrefix   func  EnsurePrefixF   func  EnsureSuffix   func  EnsureSuffixF   func  EscapeHTML   func  Humanize   func  Iif   func  IndexOf   func  IsAlpha   func  IsAlphaNumeric   func  IsEmpty   func  IsLower   func  IsNumeric   func  IsUpper   func  Left   func  LeftF   func  LeftOf   func  Letters   func  Lines   func  Map   func  Match   func  Pad   func  PadF   func  PadLeft   func  PadLeftF   func  PadRight   func  PadRightF   func  Pipe   func  QuoteItems   func  ReplaceF   func  ReplacePattern   func  ReplacePatternF   func  Reverse   func  Right   func  RightF   func  RightOf   func  SetTemplateDelimiters   func  Slice   func  SliceContains   func  SliceF   func  SliceIndexOf   func  Slugify   func  StripPunctuation   func  StripTags   func  Substr   func  SubstrF   func  Template   func  TemplateDelimiters   func  TemplateWithDelimiters   func  ToArgv   func  ToBool   func  ToBoolOr   func  ToFloat32Or   func  ToFloat64Or   func  ToIntOr   func  Underscore   func  UnescapeHTML   func  WrapHTML   func  WrapHTMLF     Variables   go var ToFloatOr   ToFloat64Or  ToFloatOr parses as a float64 or returns defaultValue    go var Verbose   false  Verbose flag enables console output for those functions that have counterparts in Go s excellent stadard packages    func   Between   go func Between s  left  right string  string  Between extracts a string between left and right strings    func   BetweenF   go func BetweenF left  right string  func string  string  BetweenF is the filter form for Between    func   Camelize   go func Camelize s string  string  Camelize return new string which removes any underscores or dashes and convert a string into camel casing    func   Capitalize   go func Capitalize s string  string  Capitalize uppercases the first char of s and lowercases the rest    func   CharAt   go func CharAt s string  index int  string  CharAt returns a string from the character at the specified position    func   CharAtF   go func CharAtF index int  func string  string  CharAtF is the filter form of CharAt    func   ChompLeft   go func ChompLeft s  prefix string  string  ChompLeft removes prefix at the start of a string    func   ChompLeftF   go func ChompLeftF prefix string  func string  string  ChompLeftF is the filter form of ChompLeft    func   ChompRight   go func ChompRight s  suffix string  string  ChompRight removes suffix from end of s    func   ChompRightF   go func ChompRightF suffix string  func string  string  ChompRightF is the filter form of ChompRight    func   Classify   go func Classify s string  string  Classify returns a camelized string with the first letter upper cased    func   ClassifyF   go func ClassifyF s string  func string  string  ClassifyF is the filter form of Classify    func   Clean   go func Clean s string  string  Clean compresses all adjacent whitespace to a single space and trims s    func   Dasherize   go func Dasherize s string  string  Dasherize converts a camel cased string into a string delimited by dashes    func   DecodeHTMLEntities   go func DecodeHTMLEntities s string  string  DecodeHTMLEntities decodes HTML entities into their proper string representation  DecodeHTMLEntities is an alias for html UnescapeString   func   EnsurePrefix   go func EnsurePrefix s  prefix string  string  EnsurePrefix ensures s starts with prefix    func   EnsurePrefixF   go func EnsurePrefixF prefix string  func string  string  EnsurePrefixF is the filter form of EnsurePrefix    func   EnsureSuffix   go func EnsureSuffix s  suffix string  string  EnsureSuffix ensures s ends with suffix    func   EnsureSuffixF   go func EnsureSuffixF suffix string  func string  string  EnsureSuffixF is the filter form of EnsureSuffix    func   EscapeHTML   go func EscapeHTML s string  string  EscapeHTML is alias for html EscapeString    func   Humanize   go func Humanize s string  string  Humanize transforms s into a human friendly form    func   Iif   go func Iif condition bool  truthy string  falsey string  string  Iif is short for immediate if  If condition is true return truthy else falsey    func   IndexOf   go func IndexOf s string  needle string  start int  int  IndexOf finds the index of needle in s starting from start    func   IsAlpha   go func IsAlpha s string  bool  IsAlpha returns true if a string contains only letters from ASCII  a z A Z   Other letters from other languages are not supported    func   IsAlphaNumeric   go func IsAlphaNumeric s string  bool  IsAlphaNumeric returns true if a string contains letters and digits    func   IsEmpty   go func IsEmpty s string  bool  IsEmpty returns true if the string is solely composed of whitespace    func   IsLower   go func IsLower s string  bool  IsLower returns true if s comprised of all lower case characters    func   IsNumeric   go func IsNumeric s string  bool  IsNumeric returns true if a string contains only digits from 0 9  Other digits not in Latin  such as Arabic  are not currently supported    func   IsUpper   go func IsUpper s string  bool  IsUpper returns true if s contains all upper case chracters    func   Left   go func Left s string  n int  string  Left returns the left substring of length n    func   LeftF   go func LeftF n int  func string  string  LeftF is the filter form of Left    func   LeftOf   go func LeftOf s string  needle string  string  LeftOf returns the substring left of needle    func   Letters   go func Letters s string    string  Letters returns an array of runes as strings so it can be indexed into    func   Lines   go func Lines s string    string  Lines convert windows newlines to unix newlines then convert to an Array of lines    func   Map   go func Map arr   string  iterator func string  string    string  Map maps an array s iitem through an iterator    func   Match   go func Match s  pattern string  bool  Match returns true if patterns matches the string   func   Pad   go func Pad s  c string  n int  string  Pad pads string s on both sides with c until it has length of n    func   PadF   go func PadF c string  n int  func string  string  PadF is the filter form of Pad    func   PadLeft   go func PadLeft s  c string  n int  string  PadLeft pads s on left side with c until it has length of n    func   PadLeftF   go func PadLeftF c string  n int  func string  string  PadLeftF is the filter form of PadLeft    func   PadRight   go func PadRight s  c string  n int  string  PadRight pads s on right side with c until it has length of n    func   PadRightF   go func PadRightF c string  n int  func string  string  PadRightF is the filter form of Padright   func   Pipe   go func Pipe s string  funcs    func string  string  string  Pipe pipes s through one or more string filters    func   QuoteItems   go func QuoteItems arr   string    string  QuoteItems quotes all items in array  mostly for debugging    func   ReplaceF   go func ReplaceF old  new string  n int  func string  string  ReplaceF is the filter form of strings Replace    func   ReplacePattern   go func ReplacePattern s  pattern  repl string  string  ReplacePattern replaces string with regexp string  ReplacePattern returns a copy of src  replacing matches of the Regexp with the replacement string repl  Inside repl    signs are interpreted as in Expand  so for instance  1 represents the text of the first submatch    func   ReplacePatternF   go func ReplacePatternF pattern  repl string  func string  string  ReplacePatternF is the filter form of ReplaceRegexp    func   Reverse   go func Reverse s string  string  Reverse a string   func   Right   go func Right s string  n int  string  Right returns the right substring of length n    func   RightF   go func RightF n int  func string  string  RightF is the Filter version of Right    func   RightOf   go func RightOf s string  prefix string  string  RightOf returns the substring to the right of prefix    func   SetTemplateDelimiters   go func SetTemplateDelimiters opening  closing string   SetTemplateDelimiters sets the delimiters for Template function  Defaults to      and        func   Slice   go func Slice s string  start  end int  string  Slice slices a string  If end is negative then it is the from the end of the string    func   SliceContains   go func SliceContains slice   string  val string  bool  SliceContains determines whether val is an element in slice    func   SliceF   go func SliceF start  end int  func string  string  SliceF is the filter for Slice    func   SliceIndexOf   go func SliceIndexOf slice   string  val string  int  SliceIndexOf gets the indx of val in slice  Returns  1 if not found    func   Slugify   go func Slugify s string  string  Slugify converts s into a dasherized string suitable for URL segment    func   StripPunctuation   go func StripPunctuation s string  string  StripPunctuation strips puncation from string    func   StripTags   go func StripTags s string  tags    string  string  StripTags strips all of the html tags or tags specified by the parameters   func   Substr   go func Substr s string  index int  n int  string  Substr returns a substring of s starting at index of length n    func   SubstrF   go func SubstrF index  n int  func string  string  SubstrF is the filter form of Substr    func   Template   go func Template s string  values map string interface    string  Template is a string template which replaces template placeholders delimited by      and      with values from map  The global delimiters may be set with SetTemplateDelimiters    func   TemplateDelimiters   go func TemplateDelimiters    opening string  closing string   TemplateDelimiters is the getter for the opening and closing delimiters for Template    func   TemplateWithDelimiters   go func TemplateWithDelimiters s string  values map string interface    opening  closing string  string  TemplateWithDelimiters is string template with user defineable opening and closing delimiters    func   ToArgv   go func ToArgv s string    string  ToArgv converts string s into an argv for exec    func   ToBool   go func ToBool s string  bool  ToBool fuzzily converts truthy values    func   ToBoolOr   go func ToBoolOr s string  defaultValue bool  bool  ToBoolOr parses s as a bool or returns defaultValue    func   ToFloat32Or   go func ToFloat32Or s string  defaultValue float32  float32  ToFloat32Or parses as a float32 or returns defaultValue on error    func   ToFloat64Or   go func ToFloat64Or s string  defaultValue float64  float64  ToFloat64Or parses s as a float64 or returns defaultValue    func   ToIntOr   go func ToIntOr s string  defaultValue int  int  ToIntOr parses s as an int or returns defaultValue    func   Underscore   go func Underscore s string  string  Underscore returns converted camel cased string into a string delimited by underscores    func   UnescapeHTML   go func UnescapeHTML s string  string  UnescapeHTML is an alias for html UnescapeString    func   WrapHTML   go func WrapHTML s string  tag string  attrs map string string  string  WrapHTML wraps s within HTML tag having attributes attrs  Note  WrapHTML does not escape s value    func   WrapHTMLF   go func WrapHTMLF tag string  attrs map string string  func string  string  WrapHTMLF is the filter form of WrapHTML  infra   Infrastructure provides simple  middleware like infrastructure for your application startup    Installation   npm install infra   save    Usage   Define initializers       Initialize the database   exports description    Postgres   exports init   function container  cb        var pg   require  pg          sanity check  ensure database can be connected     pg connect container config postgres  cb             Initialize express   exports description    Express web server   exports init   function container  cb            var app   require  express              app use bodyParser json                register app with container         container app   app          cb                    Initialize routes   exports description    Express routes   exports init   function container  cb        var app   container app      app get      function req  res  next            res render  index               cb              Use the initializers and start the app   var starter   require  infra      var container         config            postgres   postgres   localhost dev            starter use require    database     starter use require    expressPre     starter use require    handlebars     starter use require    expressPost     starter use require    routes     starter use require    processHandlers      starter start container  function err        if  err  trow err      container app listen 3000          Tips     Use your own logger var infra   require  infrastructure   log info bind log        Sample Configuration      js                         these are the default options and this entire section may be omitted          options                    SERVER  PORT    SERVER PORT              dotAlias                        selects config environment based on the value of this environment variable              envSelector    RUN ENV                  default environment if selector is unset or empty              defaultEnv    development                      named config environments  REQUIRED      envs                common    ENV  environment variables     ARGV  command line argv           development    common ENV ARGV            test    common test ENV ARGV             common          server              host    localhost            port   8080            test          server                overrides port 8080 in common          port   80                    To use development runtime environment  default    sh yourapp   To use test runtime environment   sh RUN ENV test yourapp   To override with command line args   sh yourapp   server port 9000   To override using environment variables   sh SERVER  PORT 9000 yourapp minimist   Simple CLI args parser    Port of  minimist  to golang   options     a               a    true     a foo           a     foo      a foo           a     foo      no a            a    false    a                a    true    ab               a    true  b    true    ab foo           a    true  b     foo    license   MIT dat   GoDoc   dat   Data Access Toolkit  is a fast  lightweight Postgres library for Go        Focused on Postgres  See  Insect    Upsert    SelectDoc    QueryJSON       Built on a solid foundation  sqlx   go    child DB is  sqlx DB DB DB Queryx  SELECT   FROM users         SQL and backtick friendly   go DB SQL  SELECT   FROM people LIMIT 10   QueryStructs  people        JSON Document retrieval  single trip to Postgres  requires Postgres 9 3     go DB SelectDoc  id    user name    avatar        Many  recent comments    SELECT id  title FROM comments WHERE id   users id LIMIT 10        Many  recent posts    SELECT id  title FROM posts WHERE author id   users id LIMIT 10        One  account    SELECT balance FROM accounts WHERE user id   users id        From  users        Where  id    1   4       QueryStruct  obj     obj must be agreeable with json Unmarshal     results in   json        id   4       user name    mario        avatar    https   imgur com a23x jpg        recent comments      id   1   title                 recent posts      id   1   title                 account              balance   42 00               JSON marshalable bytes  requires Postgres 9 3       go var b   byte b      DB SQL  SELECT id  user name  created at FROM users WHERE user name    1         mario     QueryJSON        straight into map var obj map string interface   DB SQL  SELECT id  user name  created at FROM users WHERE user name    1        mario     QueryObject  obj            Ordinal placeholders   go DB SQL  SELECT   FROM people WHERE state    1    CA   Exec         SQL like API   go err    DB      Select  id  user name        From  users        Where  id    1   id       QueryStruct  user        Redis caching   go    cache result for 30 seconds key     user     strconv Itoa user id  err    DB      Select  id  user name        From  users        Where  id    1   user id       Cache key  30   time Second  false       QueryStruct  user        Nested transactions       Per query timeout with database cancellation logic  pg cancel backend       SQL and slow query logging       Performant     ordinal placeholder logic is optimized to be nearly as fast as using      dat  can interpolate queries locally resulting in performance increase     over plain database sql and sqlx   Benchmarks         Getting Started   Get it   sh go get  u gopkg in mgutz dat v1 sqlx runner   Use it      go import        database sql       github com lib pq   gopkg in mgutz dat v1   gopkg in mgutz dat v1 sqlx runner            global database  pooling provided by SQL driver  var DB  runner DB   func init            create a normal database connection through database sql     db  err    sql Open  postgres    dbname dat test user dat password  test host localhost sslmode disable       if err    nil           panic err             ensures the database can be pinged with an exponential backoff  15 min  runner MustPing db      set to reasonable values for production db SetMaxIdleConns 4  db SetMaxOpenConns 16      set this to enable interpolation dat EnableInterpolation   true     set to check things like sessions closing     Should be disabled in production release builds  dat Strict   false     Log any query over 10ms as warnings   optional  runner LogQueriesThreshold   10   time Millisecond  DB   runner NewDB db   postgres          type Post struct       ID        int64          db  id       Title     string         db  title       Body      string         db  body       UserID    int64          db  user id       State     string         db  state       UpdatedAt dat NullTime   db  updated at       CreatedAt dat NullTime   db  created at       func main         var post Post     err    DB          Select  id  title            From  posts            Where  id    1   13           QueryStruct  post      fmt Println  Title   post Title          Feature highlights   Use Builders or SQL   Query Builder   go var posts    Post err    DB      Select  title    body        From  posts        Where  created at    1   someTime       OrderBy  id ASC        Limit 10       QueryStructs  posts    Plain SQL   go err   DB SQL       SELECT title  body     FROM posts WHERE created at    1     ORDER BY id ASC LIMIT 10       someTime    QueryStructs  posts    Note   dat  does not trim the SQL string  thus any extra whitespace is transmitted to the database    In practice  SQL is easier to write with backticks  Indeed  the reason this library exists is most SQL builders introduce a DSL to insulate the user from SQL    Query builders shine when dealing with data transfer objects  structs    Fetch Data Simply   Query then scan result to struct s       go var post Post err    DB      Select  id  title  body        From  posts        Where  id    1   id       QueryStruct  post    var posts    Post err   DB      Select  id  title  body        From  posts        Where  id    1   100       QueryStructs  posts        Query scalar values or a slice of values      go var n int64 DB SQL  SELECT count    FROM posts WHERE title  1   title  QueryScalar  n    var ids   int64 DB SQL  SELECT id FROM posts   title  QuerySlice  ids        Field Mapping   dat  DOES NOT map fields automatically like sqlx  You must explicitly set  db  struct tags in your types    Embedded fields are mapped breadth first      go type Realm struct       RealmUUID string db  realm uuid    type Group struct       GroupUUID string db  group uuid        Realm     g     Group Realm   Realm  11    GroupUUID   22     sql  args    InsertInto  groups   Columns  group uuid    realm uuid   Record g  ToSQL   expected     INSERT INTO groups   group uuid    realm uuid       VALUES   1   2         Blacklist and Whitelist   Control which columns get inserted or updated when processing external data      go    userData came in from http Handler  prevent them from setting protected fields DB InsertInto  payments        Blacklist  id    updated at    created at        Record userData       Returning  id        QueryScalar  userData ID       ensure session user can only update his information DB Update  users        SetWhitelist user   user name    avatar    quote        Where  id    1   session UserID       Exec         IN queries   applicable when dat EnableInterpolation    true   Simpler IN queries which expand correctly   go ids      int64 10 20 30 40 50  b    DB SQL  SELECT   FROM posts WHERE id IN  1   ids  b MustInterpolate       SELECT   FROM posts WHERE id IN  10 20 30 40 50     Tracing SQL   dat  uses  logxi  for logging  By default   logxi  logs all warnings and errors to the console   dat  logs the SQL and its arguments on any error  In addition   dat  logs slow queries as warnings if  runner LogQueriesThreshold   0   To trace all SQL  set environment variable   sh LOGXI dat  yourapp   CRUD   Create   Use  Returning  and  QueryStruct  to insert and update struct fields in one trip      go var post Post   err    DB      InsertInto  posts        Columns  title    state        Values  My Post    draft        Returning  id    created at    updated at        QueryStruct  post        Use  Blacklist  and  Whitelist  to control which record  input struct  fields are inserted       go post    Post Title   Go is awesome   State   open   err    DB      InsertInto  posts        Blacklist  id    user id    created at    updated at        Record  post       Returning  id    created at    updated at        QueryStruct  post       use wildcard to include all columns err    DB      InsertInto  posts        Whitelist           Record  post       Returning  id    created at    updated at        QueryStruct  post          Insert Multiple Records      go    create builder b    DB InsertInto  posts   Columns  title        add some new posts for i    0  i   3  i         b Record  Post Title  fmt Sprintf  Article  s   i           OR  this is more efficient as it does not do any reflection  for i    0  i   3  i         b Values fmt Sprintf  Article  s   i          execute statement    err    b Exec         Inserts if not exists or select in one trip to database      go sql  args    DB      Insect  tab        Columns  b    c        Values 1  2       Where  d    1   3       Returning  id    f    g        ToSQL     sql     WITH     sel AS  SELECT id  f  g FROM tab WHERE  d    1        ins AS           INSERT INTO  tab   b   c           SELECT  2  3         WHERE NOT EXISTS  SELECT 1 FROM sel          RETURNING  id   f   g        SELECT   FROM ins UNION ALL SELECT   FROM sel        Read      go var other Post   err   DB      Select  id  title        From  posts        Where  id    1   post ID       QueryStruct  other    published     WHERE user id    1         AND state    published    var posts    Post err   DB      Select  id  title        From  posts        Scope published  100       QueryStructs  posts        Update   Use  Returning  to fetch columns updated by triggers  For example  an update trigger on  updated at  column   go err   DB      Update  posts        Set  title    My New Title        Set  body    markdown text here        Where  id    1   post ID       Returning  updated at        QueryScalar  post UpdatedAt    Upsert   Update or Insert      go sql  args    DB      Upsert  tab        Columns  b    c        Values 1  2       Where  d  1   4       Returning  f    g        ToSQL     expected     WITH     upd AS           UPDATE tab         SET  b     1   c     2         WHERE  d  3          RETURNING  f   g         ins AS           INSERT INTO  tab   b   c           SELECT  1  2         WHERE NOT EXISTS  SELECT 1 FROM upd          RETURNING  f   g        SELECT   FROM ins UNION ALL SELECT   FROM upd        applicable when dat EnableInterpolation    true   To reset columns to their default DDL value  use  DEFAULT   For example  to reset  payment  type   go res  err    DB      Update  payments        Set  payment type   dat DEFAULT       Where  id    1   1       Exec     Use  SetBlacklist  and  SetWhitelist  to control which fields are updated       go    create blacklists for each of your structs blacklist      string  id    created at   p    paymentStructFromHandler   err    DB      Update  payments        SetBlacklist p  blacklist         Where  id    1   p ID       Exec         Use a map of attributes   go attrsMap    map string interface    name    Gopher    language    Go   result  err    DB      Update  developers        SetMap attrsMap       Where  language    1    Ruby        Exec     Delete   go result  err   DB      DeleteFrom  posts        Where  id    1   otherPost ID       Limit 1       Exec     Joins   Define JOINs in argument to  From   go err   DB      Select  u    p          From           users u         INNER JOIN posts p on  p author id   u id              WHERE  p state    published         QueryStructs  liveAuthors    Scopes   Scopes predefine JOIN and WHERE conditions  Scopes may be used with  DeleteFrom    Select  and  Update     As an example  a  published  scope might define published posts by user      go publishedPosts         INNER JOIN users u on  p author id   u id      WHERE         p state     published  AND         p deleted at IS NULL AND         u user name    1     unpublishedPosts     INNER JOIN users u on  p author id   u id      WHERE         p state     published  AND         p deleted at IS NULL AND         u user name    1   err   DB      Select  p                              must qualify columns     From  posts p        Scope publishedPosts   mgutz        QueryStructs  posts        Creating Connections   All queries are made in the context of a connection which is acquired from the underlying SQL driver s pool   For one off operations  use  DB  directly   go err    DB SQL sql  QueryStruct  post    For multiple operations  create a  Tx  transaction   defer Tx AutoCommit    or  defer Tx AutoRollback    MUST be called      go func PostsIndex rw http ResponseWriter  r  http Request        tx       DB Begin       defer tx AutoRollback        Do queries with the session var post Post err    tx Select  id  title        From  posts        Where  id    1   post ID       QueryStruct  post    if err    nil           defer AutoRollback    is used  no need to rollback on error     r WriteHeader 500      return       do more queries with transaction         MUST commit or AutoRollback   will rollback tx Commit              DB  and  Tx  implement  runner Connection  interface to keep code DRY   func getUsers conn runner Connection      dto Users  error        sql              SELECT           FROM users           var users    dto Users     err    conn SQL sql  QueryStructs  users      if err    nil           return err           return users     Nested Transactions   Nested transaction logic is as follows        If  Commit  is called in a nested transaction  the operation results in no operation  NOOP       Only the top level  Commit  commits the transaction to the database        If  Rollback  is called in a nested transaction  then the entire     transaction is rolled back   Tx IsRollbacked  is set to true        Either  defer Tx AutoCommit    or  defer Tx AutoRollback     MUST BE CALLED      for each corresponding  Begin   The internal state of nested transactions is     tracked in these two methods           go func nested conn runner Connection  error       tx  err    conn Begin       if err    nil           return err           defer tx AutoRollback        err    tx SQL  INSERT INTO users  email  values  1    me home com   Exec   if err    nil       return err      prevents AutoRollback tx Commit          func top         tx  err    DB Begin       if err    nil           logger Fatal  Could not create transaction             defer tx AutoRollback     err    nested tx  if err    nil       return      top level commits the transaction tx Commit              Timeouts   A timeout may be set on any  Query   or  Exec  with the  Timeout  method  When a timeout is set  the query is run in a separate goroutine and should a timeout occur dat will cancel the query via Postgres   pg cancel backend     go err    DB Select  SELECT pg sleep 1    Timeout 1   time Millisecond  Exec   err    dat ErrTimedout   Dates   Use  dat NullTime  type to properly handle nullable dates from JSON and Postgres    Constants   applicable when dat EnableInterpolation    true   dat  provides often used constants in SQL statements     dat DEFAULT    inserts  DEFAULT   dat NOW    inserts  NOW       Defining Constants   UnsafeStrings and constants will panic unless   dat EnableInterpolation    true   To define SQL constants  use  UnsafeString   go const CURRENT TIMESTAMP   dat UnsafeString  NOW     DB SQL  UPDATE table SET updated at    1   CURRENT TIMESTAMP    UnsafeString  is exactly that   UNSAFE   If you must use it  create a constant and  NEVER  use  UnsafeString  directly as an argument like this   go DB SQL  UPDATE table SET updated at    1   dat UnsafeString someVar     Primitive Values   Load scalar and slice values       go var id int64 var userID string err    DB      Select  id    user id   From  posts   Limit 1  QueryScalar  id   userID    var ids   int64 err   DB Select  id   From  posts   QuerySlice  ids        Caching   dat implements caching backed by an in memory or Redis store  The in memory store is not recommended for production use  Caching can cache any struct or primitive type that can be marshaled unmarshaled cleanly with the json package due to Redis being a string value store    Time is especially problematic as JavaScript  Postgres and Go have different time formats  Use the type  dat NullTime  if you are getting  cannot parse time  errors    Caching is performed before the database driver lessening the workload on the database       go    key value store  kvs  package import  gopkg in mgutz dat v1 kvs    func init            Redis  namespace is the prefix for keys and should be unique     store  err    kvs NewRedisStore  namespace      6379    passwordOrEmpty        Or  in memory store provided by  go cache  https   github com pmylund go cache  cleanupInterval    30   time Second store   kvs NewMemoryStore cleanupInterval   runner SetCache store            Cache states query for a year using key  namespace states  b  err    DB      SQL  SELECT   FROM states        Cache  states   365   24   time Hour  false       QueryJSON        Without a key  the checksum of the query is used as the cache key     In this example  the interpolated SQL  will contain their user name     if EnableInterpolation is true  effectively caching each user        cacheID    checksum  SELECT   FROM users WHERE user name  mario    b  err    DB      SQL  SELECT   FROM users WHERE user name    1   user       Cache     365   24    time Hour  false       QueryJSON        Prefer using known unique IDs to avoid the computation cost    of the checksum key  key    user    user UserName b  err    DB      SQL  SELECT   FROM users WHERE user name    1   user       Cache key  15   time Minute  false       QueryJSON        Set invalidate to true to force setting the key statesUpdated    true b  err    DB      SQL  SELECT   FROM states        Cache  states   365   24    time Hour  statesUpdated       QueryJSON        Clears the entire cache runner Cache FlushDB     runner Cache Del  fookey         SQL Interpolation   Interpolation is DISABLED by default  Set  dat EnableInterpolation   true  to enable    dat  can interpolate locally to inline query arguments  For example  this statement   go  db Exec       INSERT INTO  a  b  c  d  VALUES   1   2   3   4          interface   1  2  3  4       is sent to the database with inlined args bypassing prepared statement logic in the lib pq layer   sql  INSERT INTO  a  b  c  d  VALUES  1  2  3  4     Interpolation provides these benefits      Performance improvements   Debugging tracing is simpler with interpolated SQL   May use safe SQL constants like  dat NOW  and  dat DEFAULT   Expand placeholders with slice values   1     1  2  3      Read  SQL Interpolation  in wiki for more details and SQL injection    LICENSE   The MIT License  MIT    logxi   log XI is a structured  12 factor app  logger built for speed and happy development      Simpler  Sane no configuration defaults out of the box    Faster  See benchmarks vs logrus and log15    Structured  Key value pairs are enforced  Logs JSON in production    Configurable  Enable disalbe Loggers and levels via env vars    Friendlier  Happy  colorful and developer friendly logger in terminal    Helpul  Traces  warnings and errors are emphasized with file  line     number and callstack    Efficient  Has level guards to avoid cost of building complex arguments      Requirements   Go 1 3     Installation   go get  u github com mgutz logxi v1    Getting Started      go import  github com mgutz logxi v1       create package variable for Logger interface var logger log Logger   func main            use default logger     who     mario      log Info  Hello    who   who       create a logger with a unique identifier which    can be enabled from environment variables logger   log New  pkg       specify a writer  use NewConcurrentWriter if it is not concurrent    safe modelLogger   log NewLogger log NewConcurrentWriter os Stdout    models    db  err    sql Open  postgres    dbname testdb   if err    nil       modelLogger Error  Could not open database    err   err     fruit     apple  languages      string  go    javascript   if log IsDebug            use key value pairs after message     logger Debug  OK    fruit   fruit   languages   languages               logxi defaults to showing warnings and above  To view all logs   LOGXI   go run main go    Highlights   This logger package       Is fast in production environment   A logger should be efficient and minimize performance tax  logxi encodes JSON 2X faster than logrus and log15 with primitive types  When diagnosing a problem in production  troubleshooting often means enabling small trace data in  Debug  and  Info  statements for some period of time      primitive types BenchmarkLogxi          100000    20021 ns op   2477 B op    66 allocs op BenchmarkLogrus          30000    46372 ns op   8991 B op   196 allocs op BenchmarkLog15           20000    62974 ns op   9244 B op   236 allocs op    nested object BenchmarkLogxiComplex    30000    44448 ns op   6416 B op   190 allocs op BenchmarkLogrusComplex   20000    65006 ns op  12231 B op   278 allocs op BenchmarkLog15Complex    20000    92880 ns op  13172 B op   311 allocs op        Is developer friendly in the terminal  The HappyDevFormatter     is colorful  prints file and line numbers for traces  warnings     and errors  Arguments are printed in the order they are coded      Errors print the call stack    HappyDevFormatter  is not too concerned with performance and delegates to JSONFormatter internally        Logs machine parsable output in production environments      The default formatter for non terminals is  JSONFormatter     TextFormatter  may also be used which is MUCH faster than JSON but there is no guarantee it can be easily parsed        Has level guards to avoid the cost of building arguments  Get in the     habit of using guards    if log IsDebug         log Debug  some     key1   expensive             Conforms to a logging interface so it can be replaced    type Logger interface       Trace msg string  args    interface        Debug msg string  args    interface        Info msg string  args    interface        Warn msg string  args    interface    error     Error msg string  args    interface    error     Fatal msg string  args    interface        Log level int  msg string  args   interface         SetLevel int      IsTrace   bool     IsDebug   bool     IsInfo   bool     IsWarn   bool        Error  Fatal not needed  those SHOULD always be logged          Standardizes on key value pair argument sequence      go log Debug  inside Fn      key1   value1   key2   value2           instead of this log WithFields logrus Fields  m    pkg    key1   value1   key2   value2   Debug  inside fn             logxi logs FIX IMBALANCED PAIRS     if key value pairs are imbalanced    log Warn and log Error  are special cases and return error      go    return log Error msg                     fmt Errorf msg  return log Error msg   err   err         err           Supports Color Schemes  256 colors    log New  creates a logger that supports color schemes   logger    log New  mylog      To customize scheme     emphasize errors with white text on red background LOGXI COLORS  ERR white red  yourapp    emphasize errors with pink   200 on 256 colors table LOGXI COLORS  ERR 200  yourapp        Is suppressable in unit tests   go func TestErrNotFound     log Suppress true  defer log Suppress false              Configuration   Enabling Disabling Loggers   By default logxi logs entries whose level is  LevelWarn  or above when using a terminal  For non terminals  entries with level  LevelError  and above are logged    To quickly see all entries use short form     enable all  disable log named foo LOGXI    foo yourapp    To better control logs in production  use long form which allows for granular control of levels     the above statement is equivalent to this LOGXI   DBG foo OFF yourapp    DBG  should obviously not be used in production unless for troubleshooting  See  LevelAtoi  in  logger go  for values  For example  there is a problem in the data access layer in production      Set all to Error and set data related packages to Debug LOGXI   ERR models DBG dat  DBG api DBG yourapp    Format   The format may be set via  LOGXI FORMAT  environment variable  Valid values are   happy    text    JSON    LTSV      Use JSON in production with custom time LOGXI FORMAT JSON t 2006 01 02T15 04 05 000000 0700 yourapp    The  happy  formatter has more options       pretty   puts each key value pair indented on its own line    happy  default to fitting key value pair onto the same line  If result characters are longer than  maxcol  then the pair will be put on the next line and indented       maxcol   maximum number of columns before forcing a key to be on its     own line  If you want everything on a single line  set this to high     value like 1000  Default is 80        context   the number of context lines to print on source  Set to  1     to see only file lineno  Default is 2        Color Schemes   The color scheme may be set with  LOGXI COLORS  environment variable  For example  the default dark scheme is emulated like this     on non Windows  see Windows support below export LOGXI COLORS key cyan h value misc blue h source magenta TRC DBG WRN yellow INF green ERR red h yourapp    color only errors LOGXI COLORS ERR red yourapp    See  ansi  package for styling  An empty value  like  value  and  DBG  above means use default foreground and background on terminal    Keys          default color   TRC   trace color   DBG   debug color   WRN   warn color   INF   info color   ERR   error color   message   message color   key   key color   value   value color unless WRN or ERR   misc   time and log name color   source   source context color  excluding error line      Windows   Use  ConEmu Maximus5   Read this page about  256 colors     Colors in PowerShell and Command Prompt  work  but not very pretty    Extending   What about hooks  There are least two ways to do this     Implement your own  io Writer  to write to external services  Be sure to set     the formatter to JSON to faciliate decoding with Go s built in streaming     decoder    Create an external filter  See  v1 cmd filter  as an example      What about log rotation  12 factor apps only concern themselves with STDOUT  Use shell redirection operators to write to a file    There are many utilities to rotate logs which accept STDIN as input  They can do many things like send alerts  etc  The two obvious choices are Apache s  rotatelogs  utility and  lograte     sh yourapp   rotatelogs yourapp 86400   Testing         install godo task runner   go get  u gopkg in godo v2 cmd godo   install dependencies   godo install  v   run test   godo test   run bench with allocs  requires manual cleanup of output    godo bench allocs       License   MIT License react es5 component   Use  React Component  in ES5    This simple library allows React Component to be used with today s Javascript without  React createClass   React is moving to a simple class in the future so it s better to start using  React Component  now    One nicety this library provides is  bindHandlers   React Component no longer autobinds handlers for you  so you must bind handlers to the instance   bindHandlers  binds all functions in the prototype that match     do on  A Z    to the instance    Instead of this   js this onStoreChange   this onStoreChange bind this  this doClick   this doClick bind this    do this      js reactComponent super this  arguments   this bindHandlers this    OR   reactComponent super this  arguments  true         This library depends on  react  being installed in your project    Example      js var React   require  react    var reactComponent   require  react es5 component    var MyStore   require     stores MyStore      function MyComponent            true means autobind handlers     reactComponent super this  arguments  true       this state   MyStore getState      reactComponent MyComponent     MyComponent propTypes    location  React PropTypes string     var o   MyComponent prototype    o componentDidMount   function         MyStore listen this onStoreChange          do  handlers is our convention for UI events o doClick   function e        e preventDefault                       on  handlers is our conventino for store events o onStoreChange   function state        this setState state       o render   function         return                         Hello  this state name                       module exports   MyComponent      FSX   Function based JSX    Why      Use plain JavaScript inside render    Use attributes  class  and  for     CoffeeScript      coffeescript  fsx    require  react fsx   module exports   class App extends React Component   users   t           mario    grant   map  user             t p user  render     fsx  SideNav  RouteHandler    t         users    users     props    props     t div            t header                t SideNav null             users t          t main  class  container                   t div  class  row                       t RouteHandler props          t footer null          Or      coffeescript  createContext    require  react fsx    SideNav  RouteHandler  fsx  div  header  main  div  footer  text  append        createContext  SideNav  RouteHandler    fsx    div    header    main    footer    text    append     module exports   class App extends React Component   render     fsx        props    props     div            header                text  Hello              SideNav null          main  class  container                   div  class  row                       RouteHandler props          footer null          No Magic   No need for spread literals  Boring wins    js t   RouterHandler    merge this props   pageno  1      ES6      javascript import  fsx  from  react fsx    export default class App extends React Component       render             var user   this state user         var props   this props         return fsx t                       t  arg is optional in the lambda          is ugly though             t div t                      if  user                        t header t                              t text user name                          t   SideNav                                                         t main  className   container    t                      t div  className   row    t                          t   router RouteHandler  props                                                 t footer                                jo   jo is short for JSON Object and is convenience package for dealing with JSON    Get Example       package main   import        fmt       github com mgutz jo      func main         json       jo DecodeStr             a   1           b    cow            c   1 2           d                  e   true               f   false               g                      h    0 1 2 3                    i                           j   1                         k    Moo                                                          Get value in json path casted into different variable types  a       json Int  a   fmt Printf  a   T     v n   a  a   b       json String  b   fmt Printf  a   T     v n   b  b   c       json Float  c   fmt Printf  c   T     v n   c  c   de       json Bool  d e   fmt Printf  d e   T     v n   de  de   dg       json Map  d g   fmt Printf  d g   T     v n   dg  dg   dgh1       json Int  d g h 1    fmt Printf  d g h 1    T     v n   dgh1  dgh1   dgi1k       json String  d g i 1  k   fmt Printf  d g i 1  k   T     v n   dgi1k  dgi1k             This example will generate the following output     a  int    1 a  string    cow c  float64    1 2 d e  bool    true d g  map string interface       map h  0 1 2 3  i  map j 1  map k Moo    d g h 1   int    1 d g i 1  k  string    Moo   Set Example       func main         json    nestedjson New     json Set  a b c   1   json Set  a b d     interface   1  2  3    json Set  a b e   map string interface         f    Hello    g    World       json Set  a b e g    Universe    json Set  a b d 0    6 9   jsonStr       json EncodePrettyStr   fmt Println jsonStr             This will generate the following JSON document          a          b            c   1         d             6 9          2          3                 e              f    Hello            g    Universe                        Misc   Remember a JSON object isn t necessarily a map  These are all valid JSON    string  1  1   s   true    foo   bar     Credit   jo  was originally a port from  nestedjson   The main difference is  jo  is loosely typed and allows conversion to any type  For example  a  Float64  can be retrieved as a  string     TODO     Proper godocs   Delete function   More tests     LICENSE   MIT configpipe   WARNING   Unstable  Still early commits  API may switch to Filter interface instead of Filter function    Simpler configuration using pipe and filters    Motivation       Treat all configuration sources as filters that process and merge map of values      CLI args   Environment variables   YAML file or string   JSON file or string   UCL file or string     The advantage of filters is one can add custom filters such as decrypting some keys        Explicit merge order for overriding values        Run time or remote changes       Usage   import        os       conf  github com mgutz configpipe     var config  conf Configuration  func decryptor input map string  interface     map string interface    error               decrypt some values  add or remove keys    func init         envmode    os Getenv  run env        var prodConfig conf Filter     if envmode     production             prodConfig   conf YAMLFile  conf File Path   config yaml   MustExist  true                 filters execute left to right  which means later filters merge over        earlier filters     config  err    conf Processv             read from config json file  if present          conf JSONFile  conf File Path   config json                 Any nil filter is noop  so this WILL NOT be processed in development mode          prodConfig              read from environment variables that have prefix  CFG   and replace     with     for JSON Path         conf Env  CFG                      read from argv         confg Argv                use custom filter to decrypt encrypted values         conf FilterFunc decryptor              Reading values      idiomatic  verbose way s  err    config String  USER   n  err    config Int64  nested key       default value if missing s   config OrString  USER    peon   n   config OrInt64  nested key   100      zero value if missing s   config AsString  USER      returns    if key is missing or cannot be coerced     panic if key cannot be coerced or is missing s   config MustString  USER    sshtunnel   Simple SSH tunnel for Go    See  example   Using   Uses  glide  for vendoring    sh glide install   License   MIT import sort style eslint compact   Like import sort style eslint without extra lines    A style for  import sort  that conforms to  the  ESLint  rule   sort imports        js    Modules with side effects  not sorted because order may matter  import  a   import  c   import  b        Modules with only namespace member sorted by member import   as aa from  aa   import   as bb from  bb        Modules with multiple members sorted by first member import aaa   bbb  from  aaa   import  ccc  ddd  from  ccc   import eee    as fff from  eee        Modules with single member sorted by member import aaaa from  aaaa   import  bbbb  from  bbbb         README   This is the README for your extension  feeling gray    You can author your README using Visual Studio Code   Here are some useful editor keyboard shortcuts      Split the editor   Cmd    on OSX or  Ctrl    on Windows and Linux    Toggle preview   Shift CMD V  on OSX or  Shift Ctrl V  on Windows and Linux    Press  Ctrl Space   Windows  Linux  or  Cmd Space   OSX  to see a list of Markdown snippets     For more information     Visual Studio Code s Markdown Support   Markdown Syntax Reference     Enjoy  phaser3 starter   Simple React app for creating Phaser 3 games  Has all the goodness of  create react app   Uses ES6    To run   From terminal   npm install npm start  task   task  is simple to full featured async task runner     es6 task files   typescript task files    env parsing   parallelized dependencies can speed some tasks   daemon restarts   watch mode     Install      sh   to run latest   git clone https   github com mgutz task cd task npm link   to run next  which may be outdated   npm install  g  mgutz task next   stable is not yet available         Help   Task Manual   Quick Start   Edit  Taskfile js  or  Taskfile ts    Does not need to be inside a node project      js const sleep   ms    new Promise resolve    resolve    ms    export const name   async   prompt          const answers   await prompt   name   name   message   Name       console log  Hello    answers name         export const clean   async           await sleep 1    console log  clean       export const build       deps   clean     run              console log  build            export const arg     argv          console log argv   0       export const docs             console log  building docs          use shell spawn  shawn  to gracefully restart daemons export const server       run  async   shawn            return shawn  node src main js          watch    src    js        export default        runs  name  then   clean    build   and  docs  in parallel   deps   name   p   build  docs             To run default   task   To invoke arg with an argument   task arg foo   To run server in watch mode   task server  w   To see which tasks will run   task build   dry run   Configuration   task  reads   taskrc  configuration file from same directory as the Taskfile    taskrc  must be a node compatible Javascript file    js module exports       debug  true    file   Taskfile mjs       Be aware that some short flags are aliases for long flags on the command line  Use long flag names in   taskrc   For example  use  file  instead of  f     task   init  creates an example   taskrc   GUI   GUI Manual   Testing   To run tests   sh task test   LICENSE   MIT Licensed Kary Foundation Themes           What is this    This page obviously is about a syntax theme  but for this one we have somehow a back story that is way different than just different coloring    When people say beautiful codes they always mean less complexity or clever code but when we say that we mean it for real  The code must look nice when you read it  In order to manage the code we invented  Kary Foundation s Coding Style  KFCS   and the fact that we have a very restricted way of  coding style     Now beauty matters but also it s the psychology that matters  As developers what you see all day long is code  So it matters that the code be pretty and it matters that your editor is pretty because well it s your life happening behind it  But how should it look  what colors must be there    For what we know we all are playful people  We have an active inner child  Our editors must look like playgrounds  We have developed a rainbow colored theme  With warm colors chosen very carefully for the finest code quality possible    From what we noticed  iA Writer  has the most carefully defined experience  We loved the  gold  and  red   orange  colors used by their highlighter and inspired by their design we shaped a coding experience that for us was the most joyful look of the code ever possible              Your workbench redesigned    Given the possibilities and limitations we have tired our best to fully recolor Visual Studio Code to a new level and make it an entirely new experience which we really hope you like and enjoy     The theme    JavaScript   TypeScript       KaryScript     CSS   Less   SASS     HTML   XML     JSON     Good font to use with this theme    To use it best  we suggest you use the medium weight of  Hasklig  typeface  It s a fork of  Adobe Source Code Pro  with programmer ligatures  Our theme is the best match to this typeface    Thanks to   Thanks to these very awesome people for reporting problems  suggesting ideas and helping us bring new languages to the them       coastermcgee    m thorsen    sladiri    transtone   Alex Soh   Dave Redfern   Jiayi Hu   Mahdi Pourismaiel   Mateusz   Orta   Steve Lombardi           And thanks to these guys at Visual Studio Code s team for always being there when we had a problem and needed hot fixes and or when we asked questions and they answered kindly     Jo o Moreno     Daniel Imms     Martin Aeschlimann     Johannes Rieken     Alexandru Dima     Kai Maetzel     Benjamin Pasero   And thanks to  Sheetal Nandi  for fixing one TypeScript grammar problem for us  D   What s new    16 0 0     New     Much Better Active Tab Colors both in Dark   Light Themes with background color and border colors   New     Matching brace colors added   New     Added custom colors for editor ruler   New     Word hover colors added  fixing   20     New     Improved find match colors  fixing   20     New     Custom Scrollbar is now back with very bold and new colors  fixing   15     New     From this release a forth theme is added to our collection called  Eye Light  which is still under it s initial heavy development phase  This heme brings a more darker look to the light theme for those of you who find the light theme very bright  fixing   18     New     Widget colors added for the light theme   New     Added  F   language support   New     Added  AppleScript  language support   Fix     Fixed the coloring of  in  operator in TypeScript and JavaScript   Fix     Improved Files view colors  fixing   21     Fix     Improved PeekView colors  fixing   19     Fix     Improved Terminal colors   Fix     Improved Panel colors   Fix     Improved status bar colors     15 1 0     New     This version has a new light theme called  Eye Light  which is basically the same as original Kary Light with a darker background for when you feel the light theme is tooo light    Change     Much better panel view coloring    Change     Much much better CSS variable coloring    Change     JS Docs are improved    Change     JSX TSX improvements    Fix     Many many brace problems in CSS are now addressed within this version    New     Better XML tag support    Fix     CSS punctuation are much better now      15 0 0     BIG CHANGES    Dark theme now contains many many many more workbench customizations   we re getting near full workbench theme     NOTE   It actually takes quite a bit to choose colors and decide what is a better design so we re sorry if it s a slow process    New     Picker view is now fully themed to be pretty in your eyes    New     Great PHP support   We know  we know  we promised to bring good PHP support to the theme but unfortunately the support was not good enough  Well within this release you ll see many many improvements     New     Gutter colors are added    New     Notifications are now beautified    New     Button colors are beautified    Fix     HTML  DOCTYPE  recently went on black  This is fixed now    New     CSS   media  properties are now very better looking    Fix     Braces as always  Yes braces sometimes go green in the theme and we have tracked down some new problems with braces and they are all fixed now  Hopping you report the others we couldn t find        14 1 0     Fix     We forgot to have a right peek view in the dark theme which we are very sorry for  That must have made all of you guys very upset  In hope that you ll forgive us there is now a very cool peek view in the dark themes and we hope you like it    New     We ll be supporting haXe language from this version up      14 0 0     New     This version includes a new theme called  Minimal Dark  which features the same preferences as the dark theme but only it has no  orange     red  on the dark  It s really an interesting new look for the dark    Fix     Headers in Markdown had a strange bug that they rendered red color of the light theme in the dark theme  It s now fixed      13 0 0     New     Dark Theme now supports full workbench theme      New     Light Theme now supports full workbench theme      Fix     An  issue  about PHP HTML background coloring was fixed   reported by   m thorsen     12 1 1     Fix     An  issue  with rendering braces of JavaScript embedded in PHP   script   tag   reported by   m thorsen     12 1 0     Change     New simpler name    Fix     We had  a report on PHP support   thanks to  m thorsen   and dear   coastermcgee  fixed it      12 0 0     New     PHP Language is now officially supported   Requested by  Dave Redfern  on   7     New     SASS Language is now officially supported  Requested by   coastermcgee  on   8     Fix     The bug with some braces rendering in red is now fully fixed     11 1 0     New     Now supporting Nearley js Language   Change     Better JS support  having constants and template strings in better shape    Change     Better PEG js support     11 0 0     Hot Fix     As in Visual Studio Code version  1 9 0   KF Themes got broken because of the new highlighting system and the new grammar definitions  After a  quick bug report and some talks an the topics  we we wrote the  Whole JavaScript and TypeScript  definitions from scratch and pushed this hot fix so that you can enjoy the same theme you had  We are very sorry for this break in the theme and hope you still like this theme    New     A completely new Markdown experience based on the look of the pageman language is brought to you in this version      10 3 0     New     JSX just got awesome in TypeScript React  TSX files   Supporting 20 new custom tokens    Fix     Arrow Functions got much better in the recent version    New     PEG js Language tokens are added      10 2 0     New     Great awesome support for JSX in JS and TSX files    Change     CSS Property Values are improved    New     HTML Entities are added    Change     CSS Function colors are improved    Change      This  color is changed    New     TypeScript Enums are greatly supported now    New     More hormonic class keyword colors for JS TS    Change     Better color for     in JS TS imports      10 1 0     New     HTML Entities added    Change     CSS function colors are changed    New     CSS constant property values are added    Change     Better color for  this     New     Type Annotation           added    New      super  got added    New     Node js module support added  JS TS     Change     Selection Colors changed    New     JS Support Functions are added    Change     JS DOM Colors are changed    Change     Logical operators are now colorless    New     JS Braces are improved    New     HTML DOCTYPEs are added    New     In TypeScript deceleration types are added    Change     Light Colors are fixed    New     Pretty new icon for the theme   D      10 0 0     MASSIVE CHANGE    The direction of this theme is changing  The new plan is to have full colored themes without the foreground colors on programming languages as much as possible  For some like Ruby this is impossible but for now we have a full support toward this change on TypeScript and JavaScript and we will work to bring that into other languages as well  Many of the things you knew has been changed due to this and we now have a much more harmonized theme that you will love    Change    The colors in the light theme has been polished a lot and you ll heave super much better experience using them      8 0 0     Change     Dark colors are improved for good    New     CSS   Less are now fully supported  We had great support but with the new definitions they didn t worked  This new version includes the new defs to provide the bust support    Change    TypeScript   JavaScript now have a much better color for the function arguments and types    New     We now have a fully support for Ruby including  Class Variables  Global Variables  Ruby Separators   do             def  keyword  Right string interpolations    Fix     Type colors had so many issues  they are fixed for good    New     We now have full support fur Pageman  The whole definition is included       7 6 0     New     We now have custom support for JSON  true coloring enabled     New     You think more high level than JSON  We also have the same support for YAML now        7 5 0     Fix     Types and type casts in TypeScript and JavaScript are fixed    Fix     Namespace color is fixed    Fix     Coloring of function parameter types are fixed   Upcoming     As you know with the new grammar of Visual Studio Code 1 7 we now have functions in everywhere colored as red  This is due to a limitation of the grammar  We have  opend a pull request  to address the problem and provide a solution  We re hoping to have it fixed in the next versions of visual studio code if possible      7 2 0     Change     Strings are now blue again  Also string interpolation is added    Change     Function export types are orange    Change     Comparison and Relational operators are orange now    Change     Special Keywords are being taken care of    Upcoming    Special Ruby Token will be added soon      7 0 0     Change     This version of the theme is designed for the new version of definitions and highlighter of the vscode that is coming in the v1 7 0  The themes are now compatible with the Babel Definitions and the new JavaScript   TypeScript definitions coming with viscose    New     Object keys now have a color  So you can distinguish stuff more easy right now        5 2 0     Change     Strings are green these days      Change     The dark theme s background is lighted a bit to match the dark background of the vscode  This way it has more eye comfort    Issue  1    Thanks to dear  Murriouz  a bug was reported about invalid code s rendering and it s not fixed    Upcoming    For the next release we ll be having Babel syntax support      5 0 0     Change     The theme is now ported to  themeX   As the themeX project itself is just started it may take weeks till we develop adaptors for other editors but once we reach there  using one code base we will compile the theme for all the other editors at ever single update  So soon we ll have support for other main big editors  Thanks for being with us till now    Change     Operators are now blue      4 0 2     Change     No line highlight for dark theme    New     Versions are now synced across all versions      1 5 2     New     Operator Keywords now render just like normal text   better right        1 5 1     Fix     As  it turns out  Mac uses a  Gamma 1 8 RGB  but the standard for tmThemes are sRGB  Thanks to the great tool   tmTheme Color Convertor   the theme has being fixed and the colors no longer looks dark      1 4 5     New     TypeScript s  Type Cast Expression  now has custom coloring      1 4 4     New     Custom string color for HTML added    Fix     Markdown Heading color changed to be more hormonic      1 4 3     New     Custom color for TypeScript class storage modifiers   public    private                  minline   minline  is a simple status line for Vim   It s meant to be forked  Got tired of issues with using statusline plugins and conflicts with other plugins    Supports these plugins     ale  linter   fugitive  git utilities     Options   g minlineWithGitBranchCharacter   minline statusline  supports Tim Pope s  fugitive  plugin    The  g minlineWithGitBranchCharacter  option specifies whether to display Git branch details using the Unicode Git branch character  U E0A0   By default Git branches displayed in the  statusline  will not use that character since many monospace fonts will not contain it  However  some modern fonts  such as  Fira Code  and  Iosevka   do contain the Git branch character    If  g minlineWithGitBranchCharacter  is unset the default value from the fugitive plugin will be used    To display the Unicode Git branch character please add the following to your  vimrc     viml let g minlineWithGitBranchCharacter   1   g minlineHonorUserDefinedColors   The  g minlineHonorUserDefinedColors  option specifies whether user defined colors should be used instead of the default colors from the minline color scheme    viml let g minlineHonorUserDefinedColors   1   For example  these user defined colors mimic Vim s default statusline colors    viml highlight  link User1 StatusLine highlight  link User2 DiffAdd highlight  link User3 DiffChange highlight  link User4 DiffDelete highlight  link User5 StatusLine highlight  link User6 StatusLine highlight  link User7 StatusLine   License   Mdified from  moonfly   MIT vim0   Lightweight Vim based terminal development environment using the excellent  coc nvim  Vim plugin which provides a Visual Studio Code like experience    Currently tailored for JavaScript  Node  Go and React development    Installation   Pre requisites     node   yarn   Go   BASH     To install  not yet fully automated  Use at yourn own risk    sh bash install sh   License   MIT licensed  GraphQL subscription example   Thi is a basic Go graphql example with subscriptions  It s based on this  original example   Assets are externalized and embedded in the binary with  pkger   A custom  Any  scalar is implemented to represent any valid JSON value which can be string  int  object       This application uses      github com graph gophers graphql go   github com graph gophers graphql transport ws     How to use   Run the application    go run main go   Navigate to  localhost 8080  and use GraphiQL to subscribe using the following example    subscription onHelloSaid     helloSaid       id     msg         On a separate tab run    mutation SayHello    sayHello msg   Hello world          id     msg       Mgutz DapperPg   A  NET 5 core WebApi project using PostgreSQL and Dapper  No Entity Framework  Straight up SQL    Tested on Debian  linux     Technologies      NET 5 WebApi   Dapper   PostgreSQL     Testing   In a terminal start postgres   sh docker compose up   In another terminal  run the project      sh   first DB run migrations with your own utility   dat up   run server   dotnet run   project Mgutz DapperPg       Browse  http   localhost 5000 swagger  to interact with the API through swagger UI    Interact with API via Terminal      sh   create a product   curl  H  Content Type  application json   d    name    Apple    cost   0 50   http   localhost 5000 api product  v   list products   curl http   localhost 5000 api product   get product id 1000   curl http   localhost 5000 api product 1000         Notes     No need to use using statement  Dapper will automatically open  close and dispose of the connection      Credit   Forked from  berkayyerdelen Dapper Webapi   LICENSE   MIT Licensed  Postgres Chinook Sample Image   PostgreSQL docker image seeded with known chinook sample data  The image is useful for learning Postgres queries and or unit testing a Postgres library with known relational data    The sample databases from ftp postgresql org have minimal schemas  Chinook is more representative of a small e commerce site    Usage   Run the image like the oficial PostgreSQL image   sh docker run   rm  e  POSTGRES PASSWORD password    name chinook sample ghcr io mgutz chinook postgres 12   Perform a query   sh docker exec chinook sample psql  U postgres  d chinook  c  select   from album limit 10    Run psql from container   sh docker exec  it chinook sample psql  U postgres  d chinook   NOTE  The name of the database is  chinook  NOT the value of  POSTGRES DB    Instructions to Build Image   Seeding chinook data is time consuming  The backup is made by running a seed image with a persistent moun  then copying the persistent directory to a final image      Build a seed image that downloads chinook SQL script and adds it    to the init directory to be run when image is run    Run the seed image to seed the chinook database  The mount dir is    mntdata   Build final image by copying    mntdata  to postgres data dir      Each step corresponds to a  bake  task  namely   build seed    run seed   and  build final    Credits   Unfortunately I can t remember from where  datasets sh  is based  If you know please submit a PR to give credit    The  chinook seed script  which differs from other scripts by lower casing all tables and identifiers and using  serial  for primary keys to better conform with PostgreSQL conventions    LICENSE   MIT licensed
70,victorquinn,Victor Quinn s Dotfiles   My relevant dotfiles  Idea being that on any new system I can just pull these off of github and be up and running quickly  Public repo in case anyone is interested    These are now managed with  yadm   So it should be as easy as dropping on a new machine  installing yadm then running  yadm clone git github com victorquinn dotfiles SMS Log   Simple SMS Log plugin for OpenVBX   Don t forget the companion module   Call Log   This is a skeleton of a pure RSpec and Capybara setup  no Rails necessary  to connect to and test any website    See my  full post for more setup info   Acknowledgement  Many thanks to  Jeff Nyman for his article on using RSpec and Capybara without Rails   I used this as a starting point while building this  Readme Emma   Your personal switchboard operator   Homepage     Setup   Heroku   This app was written to be run on heroku so setup should be a breeze    Twitter   At current  only supporting login via Twitter for authorization  Plans to support any OAuth in the future  but starting with Twitter for now    In order to work with Twitter  you ll have to create an app for any instance of this app  To do so      Visit  https   dev twitter com  and sign in   Under your profile image in the upper right hand corner  go to  My Applications    Click the  Create a new application  button   Give your app a name description website    For Callback URL specify http    your app url  auth twitter callback   Select Read only under Application type   Check the  Allow this application to be used to Sign in with Twitter      About   This is an  OpenVBX  style switchboard operator in node    At its start it will be a simpler personal switchboard operator routing calls from Twilio numbers to phones in the real world  a la Google Voice  and later possibly expand to be a whole VBX system as OpenVBX is    Misc   This project is named for  Emma Nutt  the first female telephone operator   Backbone CrossDomain   Overview   This is an extension for Backbone js with a Sync that adds support for IE 7 9 CORS requests using IE s XDomainRequest Object while maintaining compatibility with non IE systems    This is intended as a drop in replacement for the default Backbone sync so you should be able to just plop this in and Cross Domain Requests  using CORS  should just magically work on IE    Installation   You can manually download install this library or grab it with npm    npm install backbone crossdomain   Usage   Include Backbone CrossDomain after including Backbone js       html             And that s it  Now anything that uses Backbone sync   internally should work with a cross domain request from IE7 8 9 where they didn t previously  This means model fetch    model save    model sync  collection fetch    etc    Bower   Easy install with Bower    bower install backbone crossdomain   RequireJS   This library has AMD support for use with RequireJS  This is not necessary to use this library but helpful when building an app with AMD    Include  RequireJS        html           RequireJS config  with Backbone and Underscore using a shim    javascript require config       paths            jquery   lib jquery           underscore   lib underscore           backbone   lib backbone           crossdomain   lib Backbone CrossDomain             shim            underscore                exports                         backbone                deps    underscore    jquery                exports   Backbone                        Require crossdomain   javascript require   underscore    backbone    crossdomain    function    Backbone           Define your models and collections as normal       Use Backbone as you would normally  this is a drop in replacement and shouldn t require anything to change    License   Licensed under MIT license   Copyright  c  2013 Victor Quinn   Permission is hereby granted  free of charge  to any person obtaining a copy of this software and associated documentation files  the  Software    to deal in the Software without restriction  including without limitation the rights to use  copy  modify  merge  publish  distribute  sublicense  and or sell copies of the Software  and to permit persons to whom the Software is furnished to do so  subject to the following conditions    The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software    THE SOFTWARE IS PROVIDED  AS IS   WITHOUT WARRANTY OF ANY KIND  EXPRESS OR IMPLIED  INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT  IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM  DAMAGES OR OTHER LIABILITY  WHETHER IN AN ACTION OF CONTRACT  TORT OR OTHERWISE  ARISING FROM  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE  CrimeData   Homepage     Setup API Docs for CrimeData   Forked from Mashery s IODocs project   https   github com mashery iodocs   The code repository for the app these API docs refer to is also open source   https   github com victorquinn CrimeData   These API docs are live on Heroku   http   crimedatadocs herokuapp com crimedata Chance                     Chance   Random generator helper for JavaScript   Homepage   http   chancejs com   Many more details on  http   chancejs com  but this single library can generate random numbers  characters  strings  names  addresses  dice  and pretty much anything else    It includes the basic building blocks for all these items and is built on top of a Mersenne Twister so it can generate these things with repeatability  if desired    Usage   See the  full docs  for details on installation and usage    Dependent tools     Chance CLI    Use Chance on the command line    Chance Token Replacer    Replace tokens in a string with Chance generated items    Dream js    Lightweight json data generator   Fake JSON Schema    Use chance generators to populate JSON Schema samples    Mocker Data Generator    Minimal JSON data generator    swagger mock api    Generate API mocks from a Swagger spec file enriched with Chance types and constraints   fony    A simple command line tool for generating fake data from a template string     Or  view all of the dependents on npm   Know a library that uses Chance that isn t here  Update the README and submit a PR    Author   Victor Quinn   https   www victorquinn com    victorquinn   Please feel free to reach out to me if you have any questions or suggestions    Contributors   THANK YOU    Contribute    Be a part of this project  You can run the test using the following      Install dependencies from package json by running  npm install   Run the test via  npm test   Make some fun new modules      This project is licensed under the  MIT License  so feel free to hack away      Proudly written in Washington  D C    Dynasty  is a clean and simple  Amazon DynamoDB  client for  Node  with baked in Promise support    DynamoDB  is Amazon s high performance  all SSD backed  fully managed NoSQL database offering  This library was built in an attempt to make it a bit more easy to work with    Dynasty is open source software and is released under the developer and business friendly  MIT license     Full documentation   Written by  Victor Quinn   Please feel free to reach out to me if you have any questions or suggestions        Victor Quinn s Resume   This is my resume    Feel free to appropriate any of the styles  ideas  etc  but please do not appropriate my identity    Memcache Plus   Memcache Plus   Better memcache for node   Full Documentation at   http   memcache plus com         What makes it  Plus       Native support for Promises or Callbacks   Elasticache auto discovery baked in   Actively developed and used   Focus on cleanliness and simplicity   Command buffering   start issuing commands right away   memcache plus  will automatically wait until connected then flush that buffer   Ability to disable with just a flag on init   sounds trivial  but nice to test with memcache off without altering any of your code   Compression built in on a per item basis   Cached retrieve   coming soon      simply pass a function for retrieving a value and a key and memcache plus will do the whole  check key  if it exists return it  if not run the function to retrieve it  set the value  and return it  for you   Support for binaries   coming soon    which the other memcache libraries for Node don t support     Proudly developed in Washington  D C  by    victorquinn com   My personal webpage Go UUID   Simple  dumb UUID generator    For now  only generates a v4  random guid  Perhaps more later  but no plans at this time as this works for my current use    Sample Use     go package main   import      fmt     github com victorquinn go uuid      func main          get and print a uuid   fmt Println uuid Generate              Originally pulled from  this post Landing Page Jekyll theme   Jekyll theme based on  landing page bootstrap theme    How to use     Place a image in   img services    Create posts to display your services  Use the follow as an example         txt   layout  default img  ipad png category  Services title  The service title description  The description of this service           Demo   View this jekyll theme in action  here   Screenshot           For more Jekyll details  read  documentation   This Jekyll theme used  Freelancer Jekyll theme  as reference    License   The contents of this repository are licensed under the  Apache 2 0     Version   1 0 1 Pinpoint Pizza   What is this    This is a simple REST API server built using the excellent  Dominos node module  by   RIAEvangelist   API   The REST API this creates when run    Stores     Method   Endpoint                    Description                                              Example                                                                                                                                                             GET       v1 stores                  Get a list of stores near the supplied address            v1 stores address 123 Main St     GET       v1 stores  store id        Get detailed info about the store with the supplied id    v1 stores 4344                    GET       v1 stores  store id menu   Get the menu of the store with the supplied id            v1 stores 4344 menu               Order     Method   Endpoint          Description                                                    Example                                                                                                           POST      v1 orders        Create an order                                                            POST      v1 ordercheese   Convenience method to immediately order a large cheese pizza             Rust NSQ Client   An NSQ client written in Rust       Latest autogenerated documentation   This is very much a work in progress  and  at the time this is written anyway  doesn t even do much of anything yet    Contributions welcome  RAML Mode   A major mode for Emacs created for editing RAML  RESTful API Markup Language  files   raml mode  makes editing  RAML  files with Emacs much easier Sobriquet   Hugo theme designed to be utilized as a personal homepage    Background   Before writing this theme  I searched long and hard for a theme that would essentially serve 2 purposes  On the homepage  it would act like a portfolio page  highlighting things that I ve done  but it would also serve deeper content like blog posts and other miscellaneous tidbits    Finding nothing that fit this bill to my satisfaction  I created Sobriquet    The name  Sobriquet  came as I was brainstorming names and found it both distinct and appropriate for this use as I view my personal website as a kind of virtual nickname for my person    Please feel free to use and or fork this theme as your own    Cheers    Victor Quinn   Configuration   We ll go over each in more detail below  but all of these configurations are intended to be in your Hugo config file and will be assumed to be under the params header  so that ll be omitted below     Here is a full sample config toml which we ll disect below    toml  params      author    Victor Quinn      profile    VictorQuinn png      segment    abcd1234    Personal   Since Sobriquet is intended for use as a Personal website theme  there are some personal specific configurations that aren t present on most more generic blog themes    Name   Some personal items include your name  headline  summary  and more  These are displayed by default on the sidebar    toml     author    Victor Quinn      headline    Leader of Engineers       summary    Specializing in leading software engineers to build scalable backends     Photo   If you re not shy and have a personal photo  it ll be displayed on the sidebar    Sobriquet expects this image to be placed in the   static img   directory    toml     profile img    VictorQuinn png    Social Media   Sobriquet currently supports a bunch of the major social networking platforms    Simply add the username for each network to your config    toml     facebook    victorjquinn      github    victorquinn      google plus     victorquinn      linkedin    victorquinn      reddit    victorquinn      twitter    victorquinn    If any network is not provided  Sobriquet will automatically hide it    Analytics   Sobriquet works with  Segment  or Google Analytics  I ve long been a huge fan of Segment  which can be used to send data to Google Analytics as well if you d like  but also understand that Google Analytics is often the standard as far as Analytics so that s included as well    To include it  add the following to your  config toml      toml   For Segment    params      segment    your segment key    or for Google Analytics    params      google analytics    your google analytics key        Creating Content   Sobriquet includes archetypes for the kinds of content that most people may want on a portfolio site  At current  these include the following      Employment   Education   Projects   Writing   Miscellaneous     If there are others you d like  feel free to submit a PR  happy to add new things  I started with items I wanted on my site  but I m sure there are likely others  particularly for people in different careers    Essentially  add content of each archetype  specifying some of the items in the front matter  and Sobriquet will create a page for that item and add a nicely stylized card to the front page as well    For example  to create a new employment item  run the following in your shell   bash   hugo new employment socialradar md   That ll create a new item using the archetype that will look like this       toml     company img      company name      company website      date    2016 03 06T12 09 47 05 00  role      summary      time      title    socialradar              Fill in the front matter items and a nice looking card will be rendered by Sobriquet on the homepage    For example  this item ends up looking like this for me       toml     company img    SocialRadar png    Note  this is relative to the  static img directory company name    SocialRadar  company website    http   socialradar com  date    2016 03 06T12 09 47 05 00  role    VP of Engineering  summary    Leading the engineering team at a startup focused on location technology   time    Sept 2013   Present  title    socialradar              Any content will be put in a new page at  employment socialradar so you can fill in further details   To Do     Currently there is no way to re order the items on the front page  Trying to figure out a non confusing way to accomplish this in the config    Click here to go to the Firefox Add on page for installation   Firefox extension that enables the  Guests can modify event  setting for Google Calendar by default  when creating a new event    Shamelessly ported from  Robin Drexler s Chrome Extension   Full credit for that awesome extension  it just needed a Firefox port  Chance Go     This is a quasi port of my other library  ChanceJS  to Go    I say quasi port because as of right now it doesn t adhere to the same principles ideals    In the JavaScript version I used a Mersenne Twister under the hood as the source of random    For Go  that doesn t seem to make as much sense  Instead  we are using the  math rand  package as the source of random    Also  this is a WIP so it is lacking many of the generators that the JS version currently has  Some of them will make sense to port over  e g  address  but some less so    Particularly all of the numerical generators make less sense in the statically typed Go than they did in JavaScript and the  math rand  package already has methods for each of the Go numeric types  We may wrap them in this library for ease  but for now they are omitted    Spring Hackathon Project   Backend for the Gizmo iOS app    API Documentation
71,simedw,Master   Master Thesis  Implementing Safety Critical Software Modules Using A Function Language   Since a large portion of the original implementation referenced propreitary code  the stripped version is at themoment not runnable  However it showcases some pretty cool techniques in writting a rule based DSL in Haskell that compiles down to C   code  mecab js   Node js bindings for mecab  japanese word boundaries  The original project  http   mecab googlecode com svn trunk mecab doc index html   Installation   If you read Japanese  follow the instructions on http   mecab googlecode com svn trunk mecab doc index html  do not forget to compile mecab ipadic with unicode support    For the rest of us    Get the latest version of mecab from http   mecab googlecode com svn trunk mecab doc index html download  Bash tar zxfv mecab X X tar gz cd mecab X X   configure  make make check sudo make install   Get the latest version of mecab ipadic  a dictionary  from the same site  There are other dictinaries  but I haven t tried them yet    Bash tar zxfv mecab ipadic 2 7 0 XXXX tar gz mecab ipadic 2 7 0 XXXX   configure   with charset utf 8 make sudo make install   Make sure you have a recent version of Node js installed   Bash git clone https   github com simedw mecab js git cd mecab js npm install   Usage      CoffeeScript mecab    require  mecab  input                       parser   new mecab MeCab   parser parse input   error  result       throw error if error   console log result  Gives the following output CoffeeScript                                                              Todo       Return information for each word segment  if it s an adjective  verb etc    Make the result included the second best result as well   ant lock   File based mutex lock for ant    Use cases   Running several builds in parallel   Using ant contrib s parallel tasks   Example   Create a build xml file with the following code   xml  project default  build      taskdef name  lock  classname  com simedw ant Lock       target name  build        lock name  sleep          echo message  I got the lock           sleep second  10          lock      target    project    From two terminals run   sh ant build  One instance will recieve the lock  print out  I got the lock  and then sleep for 10 seconds before releasing the lock   The other instance will wait for about 10 seconds before it succeeds in claiming the lock  A file named  lock sleep  while be created in the current directory  If any of the nestled task failes  the lock will be released   Search through PR comments for configuration for the current build   For example  if your current PR  frontend repo  needs a specific version of the backend repo for e2e testing    Add  backend  my special branch  in a PR comment     Then in your Jenkinsfile   def branch   searchPR query      backend  s    s         default   master   credentialsId   34f60eac db1d 4d89 84a7 1fed98bc78c7    credentialsId  is your github token  make sure you have the right permissions   PgQuery   A  libpg query  wrapper written in elixir    Useful for parsing postgresql queries    Usage   Fingerprint   Fingerprints queries will ignoring formatting and concrete values   elixir iex  PgQuery fingerprint query  SELECT count    from users where age   50      ok   02dd673649193e38c3e5afb638f51614fee858ab1e   iex  PgQuery fingerprint query  SELECT count    from users where age   50      ok   02dd673649193e38c3e5afb638f51614fee858ab1e   iex  PgQuery fingerprint query  SELECT 1   2                    ok   025a33db2ae7cbfce37fa78c34b32e75b376eb6d16     Query parsing   elixir iex  PgQuery parse query  SELECT name  age from users     ok                 RawStmt                stmt                  SelectStmt                    fromClause                                       RangeVar                          inh     true                    location     22                    relname      users                     relpersistence      p                                                            op     0              targetList                                       ResTarget                          location     7                    val                            ColumnRef                              fields         String        str      name                            location     7                                                                                                          ResTarget                          location     13                    val                            ColumnRef                              fields         String        str      age                            location     13                                                                                                                            Installation   If  available in Hex   the package can be installed by adding  pg query  to your list of dependencies in  mix exs     elixir def deps do           pg query      0 1 0       end   Documentation can be generated with  ExDoc  and published on  HexDocs   Once published  the docs can be found at  https   hexdocs pm pg query
72,rbnx,NAME   UUID  Random  Secure   Cryptographically secure random UUID generator   VERSION   version 0 01   SYNOPSIS       use UUID  Random  Secure qw generate uuid   use feature qw say     say generate uuid    say UUID  Random  Secure  generate    say UUID  Random  Secure  new  generate          DESCRIPTION   UUID  Random  Secure generates cryptographically secure UUID strings   It tries to use one of the following pseudo random number generators      Crypt  PRNG   Crypt  OpenSSL  Random   Bytes  Random  Secure   Net  SSLeay   Crypt  Random   Math  Random  Secure     If none of these modules can be loaded or are already loaded Perl s rand will be used as an unsecure fallback     INSTALLATION   To install this module  run the following commands    perl Build PL   Build   Build test   Build install    AUTHOR   Ruben Navarro  ruben cpan org     COPYRIGHT AND LICENSE   Copyright 2016 Ruben Navarro   This program is free software  you can redistribute it and or modify it under the terms of the the Artistic License  2 0    rvec   RAII dynamic array in C   About   rvec is a lightweight and generic dynamic array library that makes use of GCC Clang s cleanup extension to release the memory being used by the array once it goes out of scope    Usage   c rvec t type  v  rvec init v   rvec push v val        c   include    include  rvec h    int main int argc  const char  argv          rvec t int  v1      rvec init v1     printf  cap   zu  size   zu n   rvec capacity v1   rvec size v1     for size t i   0  i   1000  i        rvec push v1 i      printf  last item   d n  rvec pop v1     printf  1st item   d n  rvec get v1 0       or  printf  1st item   d n  rvec i v1 0     rvec set v1 0 999      or rvec i v1 0    999   rvec resize v1 100        rvec t double    v2    rvec init v2             v2 freed here  return 0              v1 freed here       API   rvec init v    Initialises the array  it must be called before any other function    rvec size v    Evaluates to the size of the array     rvec capacity v    Evaluates to the capacity of the array    rvec push v x    Adds a new element at the end of the array    rvec pop v    Pops and returns the last value    rvec i v i    Expands to   v   data  i     it can be used like this   rvec i v 0    value   or  x   rvec i v 0       rvec set v i x    rvec set v index value       rvec get v i    rvec get v index       rvec begin v    Returns a pointer to the first element of the array    rvec end v    Returns a pointer to one past the end of the array   like std  vector  end     rvec resize v sz    Resizes the array    rvec copy dest src    Copies the elements of  src  into  dest    dest  will be resized as needed    rvec destroy   Destroys the array  Only useful if you are using the library without RAII support  see   RVEC NO RAII  in rvec h     License   https   github com rbnx rvec blob master rvec h L1 L5 skewboost
73,cameres,BrandManagement   A MSAN 692 Project     Introduction   We believe that the dashboard that we previously proposed will allow our company to visualize key information from the Twittersphere  Understanding what the public is saying about our brand  as well as where they are saying it  will give us a competitive edge  In this report we outline proposed features from our initial proposal and how we incorporated them into our final product    Sentiment Analysis   In order to create a sentiment analysis web application for customer service representatives of various companies to gather feedback about their respective airlines  we first needed to be able to gather data from twitter  We found that the twitter streaming API would best suite our use case  because while we wanted our representatives to be able to responded to to potentially every single tweet for their airline  The rest API twitter provided didn t allow us to request more than 100 tweets at a time and our application would have to maintain a data structure or data store to determine whether or not a tweet had been responded to    In order to analyze the sentiment of every tweet we needed to create an additional service that would classify the sentiment of the streaming tweets  In order to cut the cost of using a service like Google s Sentiment Analysis API  we used VADER  VADER is a part of a very popular python package  NLTK  and is designed to correctly classify social media tweets   see paper      After the tweets were analyzed for sentiment  our main server would stream the tweets to all representatives connected to the website  From the website representatives would be able to respond directly to the person tweeting at the airline and address the potential concern the person might have    We found that overall airlines are fairly effective in responding to tweets with sentiment  sometimes responding within minutes of complaints being made  We believe that this indicates some combination of sentiment analysis tools or an overstaffed team    The Data   While we found that the VADER library performed very well for tweets  see correctly classified tweets in figure 1 and figure 2  we found that we would need to tune the algorithm in order to produce more false negatives than positives  under the impression that responding avoided the churning of a customer          The tweet in figure 1 3 shows an example of an incorrectly classified tweet  Two of Dr  Jeffery W  Ross  were correctly classified while this tweet has been classified incorrectly  Tweets like these stood out among other incorrectly classified tweets  Other tweets might have elements like sarcasm which made for difficult classification  but Ross  tweet contains no sarcasm and the sentiment seems straightforward      Figure 1 4 shows an incorrectly classified sarcastic tweet  We found that the great majority of tweets that streamed through our application were incorrectly classified due to the usage of the word thank in a sarcastic manner    Mapping Sentiment to Airports   In inspecting the tweets streaming in  we found that the great majority of users were not including their location in their tweets  We wanted to be able to map out where user s were flying or which airport they might be located at to gauge critical problems at particular locations  In order to achieve our goal we were able to extract the airport codes from the tweets text and mapped that code to a particular latitude and longitude using a dataset source from  openflights org   After determining the sentiment of the tweet  we then were able to pin the user s location or location of arrival departure on the map with their corresponding sentiment      Sentiment Influence   In order to distinguish which airlines were performing particularly well with customers we measured the influence of a particular user based on the number of twitter followers they had  We could not measure the number of views of the tweet like twitter currently does  It was also decided that to preemptively respond to the tweet of a user with more followers would generate more views of a customer service representatives response       text Sentiment Influence     text Sentiment     text Influence        An example of a tweet with a large sentiment influence is Om Malik s tweet  see figure 4   Malik is a registered user on twitter with 1 5M followers  Simply put his tweets will garner more views than someone with a smaller network  Tweets from users like Om Malik become a high priority as the network effect is stronger for responses to these tweets      Lastly  we created a visualization of sentiment analysis with a real time chart of average sentiment influence over time  In the chart in figure 5  we can see exactly when Om Malik tweeted at  JetBlue  gist dl   gist dl is a lightweight command line tool for downloading gists from  gist github com   gist dl has been tested on python 2 7 12 and python 3 5 2    Install   Install the project by downloading the project as a zip file or cloning the repository  After downloading the source  run the following command to install in the root directory of the project      bash pip install  e     Usage   gist dl s functionality is currently fairly limited  but the following functionality is supported     command line argument option   functionality                                            help    list arguments options for tool      sources    list of github users accounts to download gists      destination    directory to download gists to        username    github username for credentials         password    github password for credentials        token    github token for credentials        config    directory to a configuration file        extension    file extension to download ex  ipynb   github dl       github dl is a lightweight command line tool for downloading repositories from  github com  and gists from  gist github com   github dl has been tested on python 2 7 12 and python 3 5 2    github dl     gist dl     Installation   The project was recently added to PYPI  Feel free to submit an issue if there are any issues with downloading via the command below      bash pip install github dl   Installation  Development    Install the project by downloading the project as a zip file or cloning the repository  After downloading the source  run the following command to install in the root directory of the project      bash pip install  e     Usage   github dl s functionality is currently fairly limited  but the following functionality is supported   usage for both commands     command line argument option   functionality                                            help    list arguments options for tool        username    github username for credentials         password    github password for credentials        token    github token for credentials        config    filename for configuration file     config file   The config file is an optional file that stores github credentials in the following  required  format    json      username     username      password     secret password      token     secret token      NOTE    None of the credentials are required in the configuration file and token has the highest priority    usage specific for    github dl     command line argument option   functionality                                          queries    query for filtering github repositories      destination    directory to download repositories to     examples    Download all machine learning related notebooks matching a criteria       github dl  machine learning language jupyter notebook size  1000  github notebooks   config config json   usage specific for    gist dl     command line argument option   functionality                                          sources    list of github users accounts to download gists      destination    directory to download gists to        extension    file extension to download ex  ipynb     examples    Download all public jupyter notebooks       gist dl gist notebooks   config config json   extension ipynb   Download a specific user s public jupyter notebooks       gist dl cameres gist notebooks   config config json   extension ipynb EMR  Spark    Jupyter   In this tutorial  I m going to setup a data environment with Amazon EMR  Apache Spark  and Jupyter Notebook  Apache Spark has gotten extremely popular for big data processing and machine learning and EMR makes it incredibly simple to provision a Spark Cluster in minutes  At Mozilla we frequently spin up Spark clusters to perform data analysis and we have a  repository  for scripts for provisioning our clusters  The scripts contained in my repository extract the functionality that is specific to creating a simple Spark cluster and installing Jupyter Notebook on the main node of the cluster    Assumptions   The major assumption that I make in the following tutorial is that your AWS ACCESS KEY ID and AWS SECRET ACCESS KEY are accessible to  awscli   This can be solved by placing the following environmental variables in the environment file of your respective shell  There might be other solutions to this problem  but I personally use this solution    bash export AWS ACCESS KEY ID     export AWS SECRET ACCESS KEY       Setting Up Key Pair Using EC2   In order to access the cluster via the command line later  you need to generate a Key Pair to ssh into the main node  I haven t been able to figure out a way in which to create a key pair using the  awscli  and have it work with the remainder of the script  Thus  I recommend setting up a Key Pair using the  EC2 User Guide   Make sure to place the private key in this directory in order to run the script    Configuring the Script   The following two variables need to be altered based on your use case  They are found in  install jupyter notebook  and  script sh        bash   bucket should be created and used on s3   SPARK BUCKET  bucket to create on s3    name of the key pair ex  MyKeyPair   SPARK KEY PAIR  key pair created in first step        Configuring Jupyter Notebook   jupyter notebook config py  is used to configure Jupyter Notebook on the main node  As an example  this file can be altered to set a password for access to notebooks  Below is the code for this example       python from notebook auth import passwd   get a hashed password   passwd  password         Running the Script   After following the above steps  you can run the script to provision the cluster using  bash script sh   Each command can also be run separately in your shell if that is preferred    Accesing Jupyter   In order to forward the notebook server and access Jupyter  we invoke the command below  Make sure that the private key being used has the proper permissions before running the command  if you followed the AWS guide it should     bash ssh  L 8888 localhost 8888 hadoop ec2              us west 1 compute amazonaws com  i  key pair  pem   Now we can open localhost 8888 in a web browser and access our Spark context as if it was running locally on our computer    TODO     break  script sh  file into a user configurable file and a template file   allow parameters
75,acmeyer,VoteID   Intro   We currently live in a world where we can do almost anything we want with a mobile phone but we still vote for our elected officials using a paper and pencil  There are many reasons for why this is the case but it s about time we eliminate these reasons and create a modern digital voting platform  There is no better way to do this then to tap into the Github community    Task   The current US voting system in place is ancient and not secure  Voter fraud is rampant  officials want to restrict certain people from voting  and we live in the 21st century yet we vote like its the 18th century  it s about time we make a change  The purpose of this project is to initiate this change  If the US government isn t going to start a Github project dedicated to it then we will and submit it to them when its ready    Namecoin Protocol   Most of you are aware of the power of the Bitcoin protocol as a means to verify digital transactions  The plan is to piggyback on the Namecoin protocol but to customize it for election voting  This will be challenging but having the protocol already implemented in many ways helps us to pick and choose what methods work best for this particular situation    I hope you will join me in helping create a digital voting platform we can finally trust and use in our elections    Contributing   We encourage you to contribute to this project     Note  This is intended to fix the current US voting system but if applicable can be used to improve other government s voting systems as well    License   This program is free software  you can redistribute it and or modify it under the terms of the GNU General Public License as published by the Free Software Foundation  either version 3 of the License  or  at your option  any later version    This program is distributed in the hope that it will be useful  but WITHOUT ANY WARRANTY  without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE  See the GNU General Public License for more details    You should have received a copy of the GNU General Public License along with this program  If not  see http   www gnu org licenses   Connect Nest Thermostat w  IFTTT   nest to ifttt app Trump Block for iOS   This repository contains the code for a Safari iOS 9  content blocking app intended to block all things Trump      About   I was tired of seeing hearing about Donald Trump in the news and Apple had just released iOS 9 with the new content blocking capabilities  so I decided to build this little simple app to see if I could block anything related to Donald Trump on Safari    All it does is use iOS content blocking extension to remove any content from Safari that has to do with Donald Trump  I decide against releasing this on the App Store and instead decided to open source it so that anyone can make improvements  use it on their own device  or use it to make something even better    I think we can all benefit from having a little less of The Donald in our lives    For more read my post about it  here   How to Use   To use or check the app out  you need to follow these simple steps      Clone the repository   Open the project with Xcode    Run the app on your iPhone or the iOS Simulator    You re all set  Make sure to enable the content blocker in iOS Settings   Safari   Content Blockers     Compatibility   This project is written in Swift 2 0 and uses the content blocking extension which requires Xcode 7 to build and run    It is compatible with iOS 9  and iOS devices running arm64    Created by   Alex Meyer   License   Licensed under the MIT License  https   opensource org licenses MIT Conjoint for Qualtrics Offline   Produces conjoint analysis choice tasks that can be used in the Qualtrics offline application  Useful for running surveys using this application in the field without reliable internet data access     Read more about our offline conjoint tools  here     How to Use   In this repository you will find three code files   qualtrics look and feel html    qualtrics random tables js  and  qualtrics tables html    All of these files are required to produce randomized tables of conjoint profile pairs in Qualtrics  offline application    Customize Conjoint Attributes and Levels   Randomize Attribute Order   The code in the code qualtrics look and feel html file is the first thing you will want to edit to customize the code to your specific design  This code sets up the attributes  levels  and randomizes the order of the attributes shown to each respondent  This randomization occurs once and then remains consistent throughout all of the rounds of the presentation of the choice task pairs    In line 22 you will see var attribute array which defines the array of your conjoint attributes  You should replace the placeholder attribute names with yours  You can also delete or add attribute names depending on the number of attributes in your conjoint design     In line 25 the var values array defines the levels of each attribute  Again  you can customize the array to have more than two levels per attribute  For example  for the religion levels  the first array  instead of only two levels you could add another    Christian    Muslim    Jewish    Just make sure that the number of level arrays matches the total number of attributes  otherwise you will receive an error message     If you want to use images instead of text for attribute values  then comment out line 25 and uncomment line 29  Replace the default urls with urls to the images you d like to use in your survey  Be sure to also update the code qualtrics random tables js file to use images too as described below    Note   Images will only work with an online connection  If you want to use it offline  you ll have to store images locally and then use the path to the locally stored images instead of urls    Once you have adapted the code from the code qualtrics look and feel html file copy it and paste the copied code into the  Look and Feel  section of your Qualtrics survey  Click on the  Look   Feel  button on the top left when you re editing the survey  In the pop up window click  Advanced  and then paste the code into the Header text box  Then save      Insert Conjoint Tables as Questions   Copy all the code found in the code qualtrics tables html file  In each Qualtrics question you want a conjoint table to appear  paste the copied code in the HTML view portion of the question s text box  After pasting the code  make sure to replace the round title text with the correct round number  or anything you you want to label the box with   Regardless of how you title the conjoint boxes  be sure to change the conjoint table number in line 4  You can also customize the labels at the top of the tables to reflect the choices you are presenting to people  Currently the choices are presented as  A  and  B  but if you would like to customize this you can easily change the code in lines 8 and 9       Before copying the code found in code qualtrics random tables js  if you would like to use images instead of text for attribute values  be sure to comment out line 28 and uncomment lines 30 33  If you want to use both  update the  values array  to include both    Next  copy all the code found in the code qualtrics random tables js file  Add a JavaScript block to the same question where you added the table html code  Click on the gear button under the question number and then select  Add JavaScript     For this code you can select what s already written in the text box and delete everything  Then paste this code in    After you ve pasted the code be sure to adjust the fill table    at the bottom  line 53  to reflect what round this JavaScript is being pasted into  This number should match the number used in the conjoint table html id tag above  For example  be sure that the second conjoint table with the second set of profiles is changed to  fill table 2   at the bottom of the JavaScript code  Click save    This code randomizes the levels of each attribute  At this point the randomization uses equal probabilities for each attribute level  Feel free to contribute to this code and allow for unequal probabilities       Setup Capturing Conjoint Data   Now that you ve set up the conjoint to randomize levels and produce profile tables you need to make sure that Qualtrics will record what profiles each respondent sees  To embed variables for Qualtrics to record click the Survey Flow button  Click    Add a new element here  and then select  Embedded Data   Be sure to move this element such that the embedded data is the first element in the survey flow  i e  before all your question blocks     The format your embedded variable will take is Rd round number A B attribute name  For example  Rd 1 A Religion will record the Candidate A s randomized religion level in round 1  Note  the attribute name needs to be spelled correctly and exactly match the attribute name provided in the code qualtrics look and feel html file in the var attribute array  To save the conjoint profiles from all rounds you will need to embed a variable that corresponds to each choice  here A and B  for every attribute and every round    If you would like to rename the choices from  A  and  B  to something else you can change the JavaScript code  code qualtrics random tables js   in lines 31 and 33  Note  if you change  A  and  B  in the Javascript code you MUST embed variable names that correspond to your customized choices  For example  if I wanted to record choices as  Policy1  and  Policy2  my embedded variables would instead look like   Rd 1 Policy1 Tax  and  Rd 1 Policy2 Tax       Testing the Code   You may notice when testing on your computer  the order of attributes doesn t change or changes you make don t appear  This is because the code uses browser storage  You will need to clear this storage each time you run a test in order for it to work properly  We recommend using private browsing  aka incognito mode  when testing  This mode will automatically clear any storage  so you will get a fresh test each time you run it  If you do not want to use private browsing or do not have it available  then you will have to clear your browsing data after each run  You can find out how to do this in your browser s settings  This is done automatically if using the Qualtrics Offline mobile or tablet apps  so no need to make any changes if you are using these apps     Known Limitations   Future Improvements     Randomization is currently only done with equal weights   The code currently does not account for restrictions among attribute levels  for example if you wanted to prevent an immigrant profile from having occupation   doctor and education level   none     There is not currently a mechanism of having qualtrics record the order of the attributes that is presented to each respondent  but since the order of attributes is randomized across respondents the order of presentation should not matter    Assumes you have two alternatives from which respondents are supposed to select   Does not automatically produce Qualtrics embedded variables      Contributing     Check out the latest master to make sure the feature hasn t been implemented or the bug hasn t been fixed yet   Check out the issue tracker to make sure someone already hasn t requested it and or contributed it   Fork the project   Start a feature bugfix branch   Commit and push until you are happy with your contribution     License   Released under the MIT License  See  LICENSE  or http   opensource org licenses MIT for more information    Other Open Source Projects for Conjoint     If you also are looking to run conjoint experiments offline with paper PDFs  see our other open source project  here     Credits     Leah Rosenzweig     Alex Meyer     For citations    Meyer   Rosenzweig 2016    Conjoint PDF Generator App   A Rails app for producing offline conjoint PDFs  A hosted version of this app  through MIT GovLab  can be found  here     Read more about our offline conjoint tools  here     Setup   This is a  Ruby on Rails  4 2 0 app  For more information on how to install this app on  Heroku  see below    If you want to run this app locally on your own computer you need to have the 4 2 0 version of Ruby on Rails installed on your local machine  If you are using a web server other than Heroku it will need to have Rails instaled on it  For instructions for how to install Ruby on Rails visit  this link     If you are using images instead of text in your conjoint experiment  you will need to have an Amazon Web Service account and add your credentials  For more information see below    Heroku   The easiest way to use this app is to click on the button below that says  Deploy to Heroku   In order to use this method  you will need to have a Heroku account  It is easy and free to sign up for a Heroku account  which can be done  here   Once you have a Heroku account  just click on the button below and follow the instructions on the Heroku page  On this page you can create a name for your app if you want  this goes into the URL of your webpage that generates conjoint pdfs   Then click  Deploy for Free           Once Heroku is down creating your app it will show the  Manage App  or  View  buttons  Your app lives at a URL of the form https    your app name  herokuapp com    AWS S3   If you would like to use images in place of text for your attribute levels  you will need to have an  AWS  account  Once you have signed up for an AWS account  sign in to your AWS console  click on the S3 link under  Storage   Content Delivery       In the S3 portion of your console  create a new bucket name  Save this name      Next  you need to grab your access key id and secret access key  To do this  navigate to the  Security Credentials  portion of your AWS console  This link can be found under your name in the top right corner of the AWS console      In the  Your Security Credentials  section  open the  Access Keys  panel and click  Create New Access Key   Make sure you copy both the access key id and secret access key in the popup that shows as you can not get the secret access key again after dismissing this window      After you have copied down your access key id and secret access key  you should next go to your app s Heroku dashboard page  Once there  navigate to the Settings tab      In the Settings tab  click on the  Reveal Config Vars  button  That will expose your app s config variables        This is where you are going to add your AWS info  First  you will enter your bucket name that you just created  Click the first empty input box that says  KEY   Enter  AWS S3 BUCKET NAME   without quotes  in that box  Next click into the  VALUE  box and enter your bucket name  Save this configuration by clicking the  Add  button next to the fields  Next  you will do the same for the access key id you copied in the previous step  Enter  AWS ACCESS KEY ID   without quotes  for the  KEY  and your copied access key id in the  VALUE  box  Add that  Finally  you need to do the same with your secret access key  Enter  AWS SECRET ACCESS KEY   without quotes  into the  KEY  field and your copied secret access key into the  VALUE  field  Add that configuration variable  Congrats  you ve set up images for your Conjoint app    How to Use   The app is very simple to use  The homepage just lists all of your previously made conjoint experiments so that you can find them later  To make a new conjoint experiment  click the  New Conjoint Experiment  button at the top    Next  fill out the form with the values for your conjoint experiment  The name of your conjoint experiment is what the project will be saved as but will not appear on the pdf  Choice 1 and 2 will be what the first and second profiles will be labeled as  For example  you can insert  A  for Choice 1 and  B  for Choice 2 to get something that looks like the example below    To use images instead of text for a particular level  you can upload an image by clicking on the  choose file button   If an image is uploaded  the app will only display the image  not the text  on the pdf  However  you need to add a text label to the attribute level that the app can store in the csv file it produces of the profiles  After filling out the form  click the  Create Conjoint experiment  button      You should now see your generated conjoint experiment tables  To print or save PDFs for your experiment  click the  Print  button  To randomize again  click the  Run Again  button  To create a new conjoint experiment  click the  Create New  button      Each time you click  Run Again  the conjoint profiles are re randomized  meaning the order of the profiles is randomized as are the levels of each attribute  The app also produces a unique ID for each packet  which is a randomized 10 character sequence  This unique ID is printed on every page of the conjoint run  A new UID is assigned for each run  i e  each PDF packet   This ID not only helps identify pages to their packets  if  for example  pages are ripped off in the process of data collection   but also allows researchers to connect profiles evaluated to respondents  other survey responses if collected on a tablet      The runs of the conjoint experiment are recorded by the app in a csv file  The specific profiles presented in the pdf are recorded using the unique packet ID   On the home screen under  Trials Data  you can click to download this csv      The csv output has a column for UID  unique packet ID   Round  Choice and each attribute in the profile  The csv records the order the attributes are displayed in the profile  i e  Attribute 1  Attribute 2  etc    Since the order of the attributes is random for each run the attribute levels that populate  Attribute 1    Attribute 2   etc  change depending on which attribute appeared first and second in that run  For example  in the data shown below the packet with UID   512B1536E1 had profiles with the first attribute as Religion and second attribute as Party  We can see that in packet UID   01DE3FE58C the attribute order was reversed and the first attribute presented was Party  while the second was Religion      Known Limitations   Future Improvements     Randomization is currently only done with equal weights   The code currently does not account for restrictions among attribute levels  for example if you wanted to prevent an immigrant profile from having occupation   doctor and education level   none       Contributing     Check out the latest master to make sure the feature hasn t been implemented or the bug hasn t been fixed yet   Check out the issue tracker to make sure someone already hasn t requested it and or contributed it   Fork the project   Start a feature bugfix branch   Commit and push until you are happy with your contribution     License   Released under the MIT License  See  LICENSE  or http   opensource org licenses MIT for more information    Other Open Source Projects for Conjoint     If you also are looking to run conjoint experiments offline on Qualtrics  see our other open source project  here       Credits     Leah Rosenzweig     Alex Meyer     For citations    Meyer   Rosenzweig 2016    The Pragmatic Programmer s Checklist     DRY  Don t Repeat Yourself    Separate concerns   Refractor when necessary   Test as much code as possible   Keep documentation of code   Automate as much as you can     Idea for this checklist comes from  The Pragmatic Programmer   groupme bot   A basic template for creating a GroupMe bot using the Serverless framework Bitcoin payable on demand gigs API   This is an API for interacting with on demand gigs data  Facebook Messenger Bot using Serverless Framework   A starter template for building  Facebook Messenger bots  with the  Serverless framework     A few things you ll need      Facebook Page for your bot  required by Facebook    AWS account and access keys   The Gig Saloon   An app to find gigs  apply  review  and learn more about them    Original blog post   The app is no longer actively maintained or in the App Store so here s the open source code in case anyone else wants to use it in anyway    Features     Login with Facebook or Email using one time code   Search for available gigs by location  role  requirements  and type    Apply to gigs directly from app with prefilled forms   Review gigs   Read articles on specific gigs   Add comments to each gig     How to Use   Install javascript libraries by running  yarn  in terminal    Set up the Database   The server uses a MongoDB for its database  You either need one running locally or using a service like  mLab   Once you have configured your MongoDB  either set an environment variable to the uri where the database is or add the uri to the variable DATABASE URI found in  server server js     In order to start the app with something in it  you may want to pre fill your database with gig data  There are two data dumps to help you get started  The first is the  Jobs json  file which is a bunch of gigs data  The second is the  Locations json  file which includes all the location data for each gig  This data is helpful for geolocation searches  Finally  you ll need the join table for these two classes in order to connect them  That data is in the   Join jobLocations Jobs json  file    In order to import this data into your local or hosted database  follow this mongo import guide  https   docs mongodb com manual reference program mongoimport     Set up the Server   All server related files can be found in the  server   directory at the root of the app    The server for the app is a  Parse Server  with the  Parse Dashboard  for the dashboard UI    To start the server  run  npm run start  in your terminal  If all goes well  your server will be running locally at  http   localhost 8080 parse   You can view the dashboard by visiting  http   localhost 8080 dashboard  and entering  user  for username and  password  for password     If everything was set up correctly  you should see the main dashboard with The Gig Saloon app  Clicking on that should open up the app s dashboard view  If you imported any initial data from above  you should see that data in the dashboard    The server is set up to use AWS S3 for images  In order to store images for you app correctly you ll need to have environment varaibles set for  AWS ACCESS KEY ID    AWS S3 BUCKET    AWS SECRET ACCESS KEY   and  AWS REGION   For more on how to set this up  see the Parse Server guide on it here  http   docs parseplatform org parse server guide  configuring file adapters    The servers also uses AWS SES for sending emails  To see how to set up AWS SES with Parse Server  visit  https   github com parse server modules parse server amazon ses adapter     The app is set up to use OneSignal for push notifications  To learn how to set up OneSignal with Parse Server  visit  https   github com parse server modules parse server onesignal push adapter    The server also has a few interesting background jobs that were run in the original app  This includes importing news articles  found in  server articles js   and reddit posts  found in  server posts js   In order to import articles you ll need to set up your own Google Alert feeds  You can also choose to update the different subreddits where posts are pulled from  You can run these jobs from the command line or using the dashboard    The server can be easily hosted on Heroku  To find out more on how to do that  visit  https   github com parse community parse server running parse server elsewhere    Set up the Mobile App   The mobile app was written using  React Native   It it set up to work on both iOS and Android    The app uses the following libraries     Facebook SDK   for login and importing user data     Fabric   for crash analytics     Codepush   for updating the app without needing an App Store update     Mixpanel   for in app analytics    You ll need to have an account for each library in order for the app to run successfully  Once you have an account  visit the following guides for setting up each with React Native    Facebook SDK  https   github com facebook react native fbsdk   Fabric  https   github com corymsmith react native fabric   Codepush  https   github com Microsoft react native code push   Mixpanel  https   github com davodesign84 react native mixpanel   Set up iOS   In order to run the iOS app  you ll need to set a few things up  The first thing you ll need to set up is CocoaPods  To learn how to install and set up CocoaPods  visit  https   cocoapods org    Once CocoaPods is installed and set up correctly on your machine  in terminal run  cd ios    pod install   This will install all the required libraries    You will also need to turn on the following Capabilities within your app    Push Notifications   Background Modes  remote notifications    Keychain Sharing   Finally  make sure to update the User Defined variables in Build Settings in XCode with your own credentials you got from setting up the accounts above    Set up Android   Similar to iOS  you ll need to add your own credentials for each of the above accounts  These credentials can be set in  android app build gradle  under buildTypes  The Fabric credential set up can be found in  android app src AndroidManifest xml  at the bottom    Run the Mobile App   See the React Native docs for how to run the mobile app  https   facebook github io react native docs running on device html    Libraries Services Used     Parse Server   Parse Dashboard   MongoDB   Reddit API   Google Alerts   AWS S3   AWS SES   OneSignal   React Native   Facebook SDK   Fabric   Codepush   Mixpanel   CocoaPods     License   Released under the MIT License  See  LICENSE  or http   opensource org licenses MIT for more information  Informed Voter   A mobile app for finding your and your friends polling stations and sharing them via iMessage    This app is no longer maintained or on the App Store so it was open sourced    How to Use     Fork the repository   Install Set up  CocoaPods   Install  Fabric   Run  pod install  in the project s root directory   Update  Constants swift  file with you personal credentials   Add sticker images   Run the app     In order to get correct polling station information for an election  you ll have to look up the election s id for  Google s Civic Information API   Note that the API is usually pretty sparse until only a few weeks before an election so you may want to use the test election info until then    Services Used     Google s Civic Information API   Google Places API   Google Maps API   OneSignal     License   Released under the MIT License  See  LICENSE  or http   opensource org licenses MIT for more information  Open Source Coinbase Index Fund   Create your own index fund on Coinbase Pro that replicates  Coinbase s Index Fund     Why   Coinbase released their  Index Fund  in March 2018  However  it is restricted to accredited investors only and has a high 2  annual management fee    The goal of this repository is to give anyone the ability to have an index of all assets listed on Coinbase Pro with lower fees    How to Use   Sandbox   Coinbase has a public sandbox for testing  By default  the app is set to use the Sandbox API so that you don t accidentally execute real trades     All of the below instructions apply to the sandbox as well as real site  The only difference is the urls you use     You can add funds using the web interface deposit and withdraw buttons as you would on the production web interface    Sandbox URLs   Website   https   public sandbox pro coinbase com   API   https   api public sandbox pro coinbase com   Live Site URLs   Website   https   pro coinbase com   API   https   api pro coinbase com   Coinbase Pro   In order to use this code  you ll first need to have a  Coinbase Pro account   Once you have an account  you ll need to  create an API Key   check all permissions  Make note of the API key  secret  and passphrase  you ll need these and you will not be able to see the secret or passphrase after you create it initially    You ll also need to make sure you have enough funds in your Coinbase Pro account to execute the trades    One time Usage vs Automatic Usage   There are two ways you can choose to use this repository  The first is to use it to make a one time purchase of currencies on Coinbase Pro in the amount equal to their relative market cap sizes  see  One time Usage  below   The second way is to use it as an automatic crypto index fund  much like if you were to buy into Coinbase s Index Fund  see  Automatic Usage  below     One time Usage   The one time usage executes trades in your Coinbase Pro account with the fiat currency of your choice to buy an amount of each crypto currency available equal to their relative market cap sizes  Note that this will use all of the fiat currency you have in your account available    Run on your own machine   First clone the repository  git clone https   github com acmeyer open source coinbase index fund      Next  you have to set environment variables for  COINBASE API URL    COINBASE FIAT CURRENCY    COINBASE API KEY    COINBASE API SECRET   and  COINBASE API PASSPHRASE   The fiat currency environment variable should be the fiat currency you want to do trades in  The default is  USD   The api url is provided above for both the sandbox and live environments    Finally  all you have to do to purchase an index fund of available currencies on Coinbase Pro  run  yarn run fund  inside the app s directory     Warning   This will execute trades    Note   it is assumed you followed the instructions above and your Coinbase Pro account has a sufficient balance in it to execute the trades  See above section  Coinbase Pro  if not    If you would like to run this code automatically whenever your Coinbase Pro account has a high enough balance  you can deploy this code to a server and use a scheduler    Deploy on a server   The easiest way to deploy this code to a server is by clicking the button below  Once you do that  you only have to set up the  COINBASE API URL    COINBASE FIAT CURRENCY    COINBASE API KEY    COINBASE API SECRET   and  COINBASE API PASSPHRASE  environment variables and create a scheduler task that runs  yarn run fund     Once the server and scheduler are set  it will automatically make trades whenever your Coinbase Pro account has sufficient funds in the fiat currency you set the  COINBASE FIAT CURRENCY  to          You can also deploy this code to other servers  The setup is very similar    Automatic Usage   Use this set up if you would like to convert your Coinbase Pro account into an Index Fund account  This set up a server to automatically manage your Coinbase Pro account to replicate  Coinbase s Index Fund      Note   This set up will rebalance ALL currencies in your Coinbase Pro account to match the Index s weights  even if you already have different amounts of currencies in your account before running this server  Only use this set up if you don t care that the code will make trades to rebalance your account    Tip   If you would like to keep the current amount of each currency but still want to also have an index of all currencies on Coinbase  transfer your current amounts into their respective wallets to your regular Coinbase account  Then anything in your Coinbase Pro account will be the Index Fund and anything in your regular Coinbase wallets won t be affected    The goal is to add the ability to manage an individual Coinbase Pro account so that it can have an Index Fund and still execute other trades  If you would like the ability to do this  please submit a pull request    Run on your own machine   First clone the repository  git clone https   github com acmeyer open source coinbase index fund     Next  you have to set environment variables for  COINBASE API URL    COINBASE FIAT CURRENCY    COINBASE API KEY    COINBASE API SECRET   and  COINBASE API PASSPHRASE   The fiat currency environment variable should be the fiat currency you want to do trades in  The default is  USD   The api url is provided above for both the sandbox and live environments    The code uses  Parse Server  to manage the server and  Parse Dashboard  for a server dashboard  This repository has defaults for each s set up but you ll likely want to update the environment variables in  src env js  to match your own machine s configuration    Once you have set up the environment variables  you can run the server using the command  yarn run start  inside the app s directory  If you are using a local version of mongodb  make sure that is running before you try and run the server    In order to actually execute any trades  you have to either manually run the background job called  update index  by running  yarn run updateIndex  inside the app s directory  or by clicking the  Run Now  button in the Parse Dashboard under  Jobs     Note   Running this code will execute trades    To manage your Coinbase Pro account automatically  however  you ll need to set this up as a scheduled job  You can do this on your own machine but it s much easier to have a server manage it for you    Deploy on a server   The easiest way to deploy this code to a server is by clicking the button below  Once you do that  you have to set up the all the environment variables found in Heroku s  Config Vars  section under the app s  Settings  tab     Next  create a scheduler task that runs  yarn run updateIndex   If you would like to match Coinbase s Index Fund exactly  then schedule this task to run daily at 12 00am UTC  5 00pm PT     Once the server and scheduler are set  it will automatically check the current currency weights on Coinbase and rebalance your Coinbase Pro account to match  if necessary  See the below section on  Methodology  to understand when these rebalances will happen    Finally  in order to keep your orders up to date with Coinbase  add another scheduler task that runs  yarn run updateOrders   You can set this to run at whatever frequency you desire  This task just keeps the orders in your database in sync with Coinbase    Note   On first set up  the database will have no currencies already in it  That means the first time the  update index  background job is run  it will rebalance your portfolio  This may be desirable if you want your account to be updated as soon as you set this server up  but if you don t want the rebalancing to happen right away and instead wait until either a new currency is added to Coinbase s Index Fund or January 1st of the next year  then you will need to seed your database with the current currencies before the background job runs  This can be easily done by running  yarn run seed  in the app s console          You can also deploy this code to other servers  The setup is very similar    Methodology   This code is meant to replicate  Coinbase s Index Fund   You can read more about their methodology here  https   am coinbase com documents cbi methodology pdf    Currently  the asset weights are pulled from https   index am coinbase com  v1 cbi composition json to match Coinbase s weights  In the future  the goal is to do this all automatically with an official calculation  that uses the same formula Coinbase does  That way  if Coinbase ever changes the url or otherwise removes the ability to get this information  the code can still work properly    The two main takeaways are that the Index Fund gets rebalance when one of two things happens      5 days after a new currency is added to Coinbase Pro  at 5 00pm PT    The 1st day of a new year  at 5 00pm PT      If you run this repository on a server and schedule the background job  update index  to run daily at 5 00pm PT  12 00am UTC  then it will automatically rebalance your Coinbase Pro account as outlined in the Methodology above    License   Released under the MIT License  See  LICENSE  or http   opensource org licenses MIT for more information    Contributing     Check out the latest master to make sure the feature hasn t been implemented or the bug hasn t been fixed yet   Check out the issue tracker to make sure someone already hasn t requested it and or contributed it   Fork the project   Start a feature bugfix branch   Commit and push until you are happy with your contribution   Create a Pull Request on the master branch for your feature fix     Todos      x  Add a method to get weights rather than rely on scraping    x  Automatically rebalance account  annually and any time new asset is added to GDAX       Add option to use different types of orders to avoid trading fees       Add tests       Better error handling    x  Add background job to update executed orders on Coinbase Pro to keep data up to date       Separate index fund vs rest of Coinbase Pro account       Add frontend UI for viewing Index Fund s performance       Add an official calculation of Methodology rather than pulling weights from Coinbase     Disclaimer   This repository is for information purposes only  Use the code at your own risk  You may lose money if you use this code  Trading crypto currencies for real money is extremely risky and likely to result in disappointment  You bear sole responsibility for anything that happens using this code    Support   To support the development of this project  you can make donations to the below addresses    BTC  3FEUPGgdgmzu7w4GkW7vnBwRWqdyCsy53v   BCH  qqkw35llyty9t8gd2aafg6j946xjpaqdkc8adx8q04   ETH  0x93fd6895a4756B6233CAa3B491594d0EB040D189   ZEC  t1Mr3updMscxiw1r9ob726W5VucieVknEcC Cambridge Street Safety Data Analysis   A look at street safety in Cambridge  MA      Blog Post   Interactive Map   Slides     How to Use   If you want to replicate my findings or otherwise explore the data on your own  you ll need to have python  v3    numpy  pandas  ggplot  and jupyter installed on your machine  Then run  jupyter notebook  inside this directory    Under the  docs   directory  you will also find the website code for the interactive map of the data    Data   All data for this project can be found on the City of Cambridge s open data website found here  http   data cambridgema gov     The two datasets used in this project are located here  https   data cambridgema gov Public Safety Police Department Crash Data Historical ybny g9cv and here  https   data cambridgema gov Public Safety Police Department Crash Data Updated gb5w yva3    License   Released under the MIT License  See http   opensource org licenses MIT for more information  Travel Canny   This repository is for a now defunct project I worked on called Travel Canny  This is the Rails app that ran the Wireless Network using  Twilio s Programmable Wireless   The business prospects of it weren t great but the setting up a custom  programmable wireless network was pretty interesting and I thought worth sharing  so I am open sourcing the app    What it is   The main idea for the project was to create a simple  easy to use  works everywhere SIM card  In order to do that  you need a SIM card and a configurable wireless network  Luckily   Twilio s Programmable Wireless product  was perfect for this  Although  it is mostly meant for programming IoT devices  with a few tweaks and customizations you can use it to set up your own wireless network    This app consists of    Admin Dashboard  to manage things like users  shipping  payments  etc    Webhooks  interact with and manage the Twilio Programmable Wireless network   Frontend API  skeleton SPA setup for users to interact with their accounts   How it works   This app allows you to create your own wireless network using Twilio s Programmable Wireless product    The main part of the app is an Admin Dashboard where you can create and edit users  manage and track shipments of SIM cards  to your users   manage and track SIM cards and data usage  and jump to external services like Stripe and Twilio    The actual management of the wireless network is handled by the app code and Twilio  The details for how to set this up are below  As far as the user experience goes  the app works as follows    A user account is set up by an admin   An admin user packages and ships a SIM card to a user  When they update the shipment in the dashboard to shipped and add a tracking link  the user receives an email notifying them that their SIM card is on its way and gives them a link to track it    The admin sets the data plan for the SIM card   Upon receiving the SIM card  the user puts it in their phone and connects to the network    Once a user gets below a set amount of data on their plan  they will receive an automatic text message that tells them  This text message includes a link to a webpage where they can add a credit card for more data  or they can simply respond with  Add  if they already have a credit card on file and their data plan will be updated automatically and their SIM card re enabled if de activated    How to use   TLDR         Configure   The app requires a Twilio account  Stripe account  SendGrid account  and Mailchimp account  Once you have those accounts set up  you ll need to grab their api keys and configure the following project environment variables      TWILIO ACCOUNT SID   TWILIO AUTH TOKEN   STRIPE API KEY   STRIPE PUBLIC KEY   SENDGRID USERNAME   SENDGRID API KEY   MAILCHIMP API KEY   MAILCHIMP MARKETING LIST ID     Mailchimp isn t actually necessary to run the app  it just adds all new users to a marketing email list  but I figured to leave it in to see how that was done  If you don t need this part either leave the environment variables empty and let those methods fail silently or remove them completely  They can be found in the callback methods on the  User  model    Twilio Programmable Wireless   The key part of the app is using it to run and set up the Twilio Programmable Wireless product  In order to actually manage SIM cards  data plans  payment for services  and network tests  you ll have to do a little bit of set up on  Twilio     To do this  log into your Twilio console and navigate to the  Programmable Wireless  section  Once there  the first thing you ll have to do is actually order SIM cards from Twilio  You can do that under the  Order  tab  Assuming you now have some SIM cards from Twilio to use  you can now begin to set them up to work with the app    The first thing you ll need to do is set up some data plans  You ll have to decide how you want to run your network for your specific use case but the way Travel Canny was set up was that I created three different data plans on Twilio    A main data plan for all activated users   A test data plan for testers  not actual users    A data plan to switch users to when they reached their limits to shut off their data connection but still allow text and calls to go through   The way the app works is that all active users with data allowances get put on the main data plan  Because Travel Canny was meant to be as simple to use as possible and work anywhere  that meant the main data plan had all services enabled  Data  Voice  Messaging   allowed both National and International Roaming  and set all data limits to 100 000 MB month    How to actually set up a data plan on Twilio is outside the scope of this Readme but if you would like to learn more about them and how to do it  you can do so  here     Setting the main data plan up in this way allowed for the flexibility to control limitations programmatically instead of having to jump into the Twilio dashboard everytime a change needed to be made  That would be infeasible  So instead  I set it up so that everything was turned on  The app is what controls a user s actual data plan and usage  Set the  TWILIO SIM RATE PLAN SID  environment variable to the SID of the data plan you created on Twilio    The next plan type you ll want to set up is the disabled data plan  This is a plan that allows text messages and calls to go through but does not have data  That way  when a user hits their data limit in the app  instead of having to manually switch them or disable their SIM card  which would prevent texts or calls  you just update the SIM card to use the disabled plan that you set up programmatically  Set the  TWILIO DISABLED DATA SIM RATE PLAN SID  environment variable to the SID of this data plan on Twilio    I also set up a test data plan that was limited in data because it was strictly meant for testing purposes  You ll also find that there is some network testing code in the  app controllers webhooks twilio network test controller rb  file  They use two environment variables called  NETWORK TEST SURVEY CODE  and  NETWORK TEST SURVEY INPUT  in order to verify testing but you can ignore these if you d like  If using a test plan  make sure to set the  TWILIO TEST SIM RATE PLAN SID  environment variable to the SID of it on Twilio    Finally  in order to send out warning text messages to users that their data is running low  you have to buy a Twilio phone number and set the  TWILIO NOTIFICATION PHONE NUMBER  environment variable to it  Notification messages will be sent from that    Here are the Twilio related environment variables again     TWILIO SIM RATE PLAN SID     TWILIO DISABLED DATA SIM RATE PLAN SID     TWILIO TEST SIM RATE PLAN SID     NETWORK TEST SURVEY CODE     NETWORK TEST SURVEY INPUT     TWILIO NOTIFICATION PHONE NUMBER   Once you ve got that all set up you re good to start adding users  Note that when adding users  upon creating a new user  the app will pull all the available SIM cards from your Twilio account and pick one to give to the new user automatically  It will fail if you don t have any available SIM cards to use    The other environment variables that the app uses are      DOMAIN NAME   SECRET KEY BASE   CLIENT API KEY   for frontend app   ANALYTICS API KEY   for Segment analytics     License   Released under the MIT License  See  LICENSE  or http   opensource org licenses MIT for more information  Software Project Management Best Practices   There are a lot of resources and tools out there for the coding side of software development but not a lot on another important aspect of software development  project management  This repository serves to try and address that shortcoming     Table of Contents     Book Reading List   Notes on The Mythical Man Month   Notes on Peopleware   Notes on Extreme Ownership   Article Reading List       The intention is to update this repository continously as I come across new methods and ideas from my own experiences or those of others  Any improvements  additions  and changes are welcome  just submit a pull request  Poker Monte Carlo Simulation   This respository is for the code used in this article  https   medium com  alexcmeyer augmenting thinking 54deff7735c5    The code is released under the MIT License  Shopify App Store Analysis   Code and plots for the analysis done in the post  Reverse Engineering Opportunity in the Shopify App Store   COVID Tracking Add on for Google Sheets   This is the repository for the COVID Tracking Google Sheets Add on    Why   Inspired by  The COVID Tracking Project   this add on tries to make it even easier for others to pull in data from the project directly into a Google Spreadsheet    How to Install   The first step is to open up the sheet you want to use this add on in  Next  click the menu Tools   Script editor      This will open up a new window with an empty Google Script editor view      Delete all of the existing code in the editor  Then copy and paste the code located in the  Code js  file in this repository into the blank editor  Save the file    It will ask you for a name for this project  choose whatever name you want    Next  go to File   New   HTML file      It will then ask you to enter a new file name   Important   set the file name as  Documentation   leave off the  html extension name     This will open a new html file with some code in it  Delete all the code in this file  Then open up the file in this respository called  Documentation html  and copy and paste all of the code in this file into the empty html file in the Google Script editor  Save this file    You are now ready to use the add on    How to Use   After installing the add on for the first time  you ll have to reload your spreadsheet s browser page  If everything was installed properly  you should now see a new menu in your spreadsheet called  COVID Tracking     To use the tool  select the menu COVID Tracking   Documentation    First Use   If this is the first time setting it up  Google sheets will ask for your permission to run the script    Click Continue  then select your user account  and allow the tool access     Note   If you see a warning that the add on is not verified  you can still proceed by clicking  Advanced  and then proceed anyways  This warning shows because the add on has not been verified by Google since it is open source and not an official Google Sheets add on    A side bar menu should now open up on the righthand side of your sheet  with the documentation for how to use the add on    Contributing     Check out the latest master to make sure the feature hasn t been implemented or the bug hasn t been fixed yet   Check out the issue tracker to make sure someone already hasn t requested it and or contributed it   Fork the project   Start a feature bugfix branch   Commit and push until you are happy with your contribution     License   Released under the MIT License  See  MIT License  or http   opensource org licenses MIT for more information
76,soroushmehr,cat vs dog   IFT6266H15  Representation and Deep Learning course   class project BP FFT   Code for backpropagation through FFT operation in Theano   Based on  https   gist github com tscohen c0f29b5ff0b5e978cfc5 SampleRNN   Code accompanying the paper SampleRNN  An Unconditional End to End Neural Audio Generation Model   here  and  here    Samples are available  here       Dependencies   Extensively tested with    cuDNN 5105   Python 2 7 12   Numpy 1 11 1   Theano 0 8 2  0 9 for WaveNet re implementation    Lasagne 0 2 dev1   Datasets   Music dataset was created from all 32 Beethoven s piano sonatas available publicly on  archive org    datasets music  contains scripts to preprocess and build this dataset  It is also available  here  for download  Extract the tar file and put all the numpy files in  datasets music  directory    Training   To train a model on an existing dataset with accelerated GPU processing  you need to run following lines from the root of  sampleRNN ICLR2017  folder which corresponds to the best found set of hyper paramters    Mission control center     pwd  u mehris sampleRNN ICLR2017   SampleRNN  2 tier          python models two tier two tier py  h usage  two tier py   h     exp EXP    n frames N FRAMES   frame size                    FRAME SIZE   weight norm WEIGHT NORM   emb size EMB SIZE                      skip conn SKIP CONN   dim DIM   n rnn  1 2 3 4 5                       rnn type  LSTM GRU    learn h0 LEARN H0   q levels                    Q LEVELS   q type  linear a law mu law    which set                     ONOM BLIZZ MUSIC    batch size  64 128 256     debug                        resume    two tier py No default value  Indicate every argument    optional arguments     h    help            show this help message and exit     exp EXP             Experiment name     n frames N FRAMES   How many  frames  to include in each Truncated BPTT                         pass     frame size FRAME SIZE                         How many samples per frame     weight norm WEIGHT NORM                         Adding learnable weight normalization to all the                         linear layers  except for the embedding layer      emb size EMB SIZE   Size of embedding layer  0 to disable      skip conn SKIP CONN                         Add skip connections to RNN     dim DIM             Dimension of RNN and MLPs     n rnn  1 2 3 4 5    Number of layers in the stacked RNN     rnn type  LSTM GRU                          GRU or LSTM     learn h0 LEARN H0   Whether to learn the initial state of RNN     q levels Q LEVELS   Number of bins for quantization of audio samples                          Should be 256 for mu law      q type  linear a law mu law                          Quantization in linear scale  a law companding  or mu                          law compandig  With mu  a law quantization level shoud                         be set as 256     which set  ONOM BLIZZ MUSIC                          ONOM  BLIZZ  or MUSIC     batch size  64 128 256                          size of mini batch     debug               Debug mode     resume              Resume the same model from the last checkpoint  Order                         of params are important   for now   To run     THEANO FLAGS mode FAST RUN device gpu0 floatX float32 python  u models two tier two tier py   exp BEST 2TIER   n frames 64   frame size 16   emb size 256   skip conn False   dim 1024   n rnn 3   rnn type GRU   q levels 256   q type linear   batch size 128   weight norm True   learn h0 True   which set MUSIC       SampleRNN  3 tier          python models three tier three tier py  h usage  three tier py   h     exp EXP    seq len SEQ LEN   big frame size                      BIG FRAME SIZE   frame size FRAME SIZE   weight norm                      WEIGHT NORM   emb size EMB SIZE   skip conn SKIP CONN                        dim DIM   n rnn  1 2 3 4 5    rnn type  LSTM GRU                         learn h0 LEARN H0   q levels Q LEVELS   q type                       linear a law mu law    which set  ONOM BLIZZ MUSIC                         batch size  64 128 256     debug     resume    three tier py No default value  Indicate every argument    optional arguments     h    help            show this help message and exit     exp EXP             Experiment name     seq len SEQ LEN     How many samples to include in each Truncated BPTT                         pass     big frame size BIG FRAME SIZE                         How many samples per big frame in tier 3     frame size FRAME SIZE                         How many samples per frame in tier 2     weight norm WEIGHT NORM                         Adding learnable weight normalization to all the                         linear layers  except for the embedding layer      emb size EMB SIZE   Size of embedding layer    0      skip conn SKIP CONN                         Add skip connections to RNN     dim DIM             Dimension of RNN and MLPs     n rnn  1 2 3 4 5    Number of layers in the stacked RNN     rnn type  LSTM GRU                          GRU or LSTM     learn h0 LEARN H0   Whether to learn the initial state of RNN     q levels Q LEVELS   Number of bins for quantization of audio samples                          Should be 256 for mu law      q type  linear a law mu law                          Quantization in linear scale  a law companding  or mu                          law compandig  With mu  a law quantization level shoud                         be set as 256     which set  ONOM BLIZZ MUSIC                          ONOM  BLIZZ  or MUSIC     batch size  64 128 256                          size of mini batch     debug               Debug mode     resume              Resume the same model from the last checkpoint  Order                         of params are important   for now   To run     THEANO FLAGS mode FAST RUN device gpu0 floatX float32 python  u models three tier three tier py   exp BEST 3TIER   seq len 512   big frame size 8   frame size 2   emb size 256   skip conn False   dim 1024   n rnn 1   rnn type GRU   q levels 256   q type linear   batch size 128   weight norm True   learn h0 True   which set MUSIC       Reference   If you are using this code  please cite the paper     article mehri2016samplernn  Author    Soroush Mehri and Kundan Kumar and Ishaan Gulrajani and Rithesh Kumar and Shubham Jain and Jose Sotelo and Aaron Courville and Yoshua Bengio   Title    SampleRNN  An Unconditional End to End Neural Audio Generation Model   Year    2016   Journal    arXiv preprint arXiv 1612 07837       Torch implementation   Thanks to  Richard Assar   now we have a Torch implementation available    https   github com richardassar SampleRNN torch   Miscellaneous     Talk by Yoshua Bengio at CBMM  MIT   Deep Generative Models for Speech and Images   Follow up project   Char2Wav  End To End Speech Synthesis     If needed or have interesting related project results  please don t hesitate to contact us
77,SimonSuster,PP attachment disambiguation   Code for resolving PP attachment ambiguity with distributional methods  SimonSuster github io   Personal homepage of Simon  uster arc medtechai   This is a Jekyll based website for a research group    Creating a new page   Add the markdown file in the  root  directory with the necessary front matter  layout  page  title  categories     Categories may include  navi  or  browse      navi  is for adding the page to the navigation bar    browse  is for adding the page to the browsing topics on the home page          layout  page title  News categories   navi                    layout  page title  About categories   navi  browse  browse description  Learn more about our organisation             Creating a nested section       Create a new folder in the  root  directory with the appropriate name of the section  Place the markdown files in the folder  and any further nesting in subfolders   This template allows nesting with depth of at most 2        In the   data  folder  add a   yml  file with the same name as the section folder created previously  Describe the table of contents for navigation that will appear for the section as follows             toc    page  Nested   url   nested    page  Section 1   url   nested section1 html   page  Section 2   url   nested section2    subpages        page  Subsection       url   nested section2 subsection html       page  Subsection 2       url   nested section2 subsection2 html         See sample  nested  directory file     Adding a new research area   Add the markdown file in the   research areas  directory and include the necessary front matter  layout  page  title            layout  page title    Research Area 2             Adding a news post   Simply add a file in the   posts  directory that follows the convention  YYYY MM DD name of post markdown  and includes the necessary front matter  layout  post  title  date  categories            layout  post title    Welcome to the web site   date    2018 10 01 23 55 23  1000 categories  jekyll update
78,arnoldpaye,tesisDoc   Documento de mi tesis   TODO LIST   20130512   Las tesis tienen palabras claves    Como clasifican las Tesis en la Biblioteca de Informatica    Cuantas Tesis existen en la Biblioteca de Informatica    Cuantas Tesis se encuentran disponibles en formato digital                                                                 Jekyll Bootstrap   The quickest way to start and publish your Jekyll powered blog  100  compatible with GitHub pages   Usage   For all usage and documentation please see   http   jekyllbootstrap com   Version   0 3 0   stable and versioned using  semantic versioning     NOTE   0 3 0 introduces a new theme which is not backwards compatible in the sense it won t  look  like the old version  However  the actual API has not changed at all  You might want to run 0 3 0 in a branch to make sure you are ok with the theme design changes    Contributing   This repository tracks 2 projects      Jekyll Bootstrap Framework     The framework for which users should clone and build their blog on top of is available in the master branch      To contribute to the framework please make sure to checkout your branch based on  jb development      This is very important as it allows me to accept your pull request without having to publish a public version release    Small  atomic Features  bugs  etc      Use the  jb development  branch but note it will likely change fast as pull requests are accepted      Please rebase as often as possible when working      Work on small  atomic features bugs to avoid upstream commits affecting breaking your development work    For Big Features or major API extensions edits      This is the one case where I ll accept pull requests based off the master branch    This allows you to work in isolation but it means I ll have to manually merge your work into the next public release    Translation   it might take a bit longer so please be patient   but sincerely thank you       Jekyll Bootstrap Documentation Website       The documentation website at  http   jekyllbootstrap com  is maintained in the gh pages branch    Please fork and contribute documentation additions to this branch only      The master and gh pages branch do not share the same ancestry  Please treat them as completely separate git repositories    License   MIT teamBook   Team Book thesisDoc   My final thesis document othr aketPrototype   Web app for a automatic keyword extraction based TextRank Model textrank   Java implementation of Text Rank Model for spanish language OK OK programming contests RepoTest bicenter   This project is generated with  yo angular generator  version 0 11 1    Build   development   Run  grunt  for building and  grunt serve  for preview    Testing   Running  grunt test  will run the unit tests with karma  networking designer SENCHA TOUCH   Create application   sencha  sdk  sencha touch path  generate app  application name   application path     Start web server   sencha web start    Build application   sencha app build  ExtJSinActionChapter3   http   extjsinaction com v4 examples ch03 using ComponentQuery html  HTML5 Slide Template   Configuring the slides   Much of the deck is customized by changing the settings in  slide config js   Some of the customizations include the title  Analytics tracking ID  speaker information  name  social urls  blog   web fonts to load  themes  and other general behavior    Customizing the   io12  hash   The bottom of the slides include   io12  by default  If you d like to change this  please update the variable   social tags    io12    in   theme scss default scss     See the next section on  Editing CSS  before you go editing things    Editing CSS   Compass  is a CSS preprocessor used to compile SCSS SASS into CSS  We chose SCSS for the new slide deck for maintainability  easier browser compatibility  and because   it s the future    That said  if not comfortable working with SCSS or don t want to learn something new  not a problem  The generated  css files can already be found in  see   theme css    You can just edit those and bypass SCSS altogether  However  our recommendation is to use Compass  It s super easy to install and use    Installing Compass and making changes   First  install compass    sudo gem update   system sudo gem install compass    Next  you ll want to watch for changes to the exiting  scss files in   theme scss  and any new one you add      cd io 2012 slides   compass watch    This command automatically recompiles the  scss file when you make a change  Its corresponding  css file is output to   theme css   Slick    By default   config rb  in the main project folder outputs minified  css  It s a best practice after all  However  if you want unminified files  run watch with the style output flag    compass watch  s expanded    Note   You should not need to edit   base scss     Running the slides   The slides can be run locally from  file     making development easy      Running from a web server   If at some point you should need a web server  use  serve sh   It will launch a simple one and point your default browser to  http   localhost 8000 template html       cd io 2012 slides     serve sh    You can also specify a custom port        serve sh 8080    Presenter mode   The slides contain a presenter mode feature  beta  to view   control the slides from a popup window    To enable presenter mode  add  presentme true  to the URL   http   localhost 8000 template html presentme true   To disable presenter mode  hit  http   localhost 8000 template html presentme false   Presenter mode is sticky  so refreshing the page will persist your settings  Sencha App Architecture   Tutorial App Architecture Part 1   Tutorial App Architecture Part 2   Views   First     NewStation   SongControls   SongProgress   ApplicationTitle   StationsList   RecentlyPlayedScroller   Settings   StationsFilter   ArtistInfoTitle   SongInfo   Menu   Ad   ArtistDetails     Second     Header   SidePanel   SongPanel     Third     NewStation   SongControls   StationsList   RecentlyPlayedScroller   Ad   SongInfo     Models     Song  liked    Song  duration    Station  name    Song  albumCover    Song  artist    Song  album    Song  duration    Song  artistInfo      Stores     SearchResults   Stations   RecentSongs     Controllers     StationController   SongController   Reproductor de musica con ExtJS book container   Book Container Kibana Logger 2DTutorial   unity2d platform game tutorial karma test   Karma Test   Install     npm install     Karma commands     karma init karma conf js   karma start karma conf js     Other commands     npm install  g karma cli   npm install karma jasmine   save dev   npm install karma phantomjs launcher   save dev   npm list  g   depth 0   jKibana grunt project setup   Grunt project setup   Installation   sh npm install  g grunt npm install   Development build   sh grunt dev   Production build   sh grunt dev gulp project setup   Gulp project setup   Installation   console npm install  g gulp npm install   Build   console gulp AP   Sencha Touch 2 3 1 test project webpack project setup   Webpack project setup   Installation   console npm install  g webpack npm install   Build   console webpack proyecto modulo6   Create backup   console mysqldump  u root  p  B concesionario   concesionario sql   Restore from backup   console mysql  u root  p   concesionario sql learn chef   Chef cookbooks   Static Analysis   In order to do static analysis we are use foodcritic it comes along with chef development kit   console foodcritic  cookbook name   For example   console foodcritic mysql diplomado linux   TODO  Enter the cookbook description here  React Kibana   Installation   npm install   Test   npm run test   Run   npm start   TODO         Remove App component along with its test and style files    Chat prototype using React and Bootstrap v3   Requirements     Node v6 11 2   Yarn v0 27 5     Installation   console yarn install   Test   console yarn test   Run   console yarn start React with bootstrap using reactstrap   Requirements     Node v6 11 2   Yarn v0 27 5     Installation   console yarn install   Test   console yarn test   Run   console yarn start Proyecto para la materia de preparacion de datos Your GitHub Learning Lab Repository for Introducing GitHub   Welcome to  your  repository for your GitHub Learning Lab course  This repository will be used during the different activities that I will be guiding you through    Oh  I haven t introduced myself      I m the GitHub Learning Lab bot and I m here to help guide you in your journey to learn and master the various topics covered in this course  I will be using Issue and Pull Request comments to communicate with you  In fact  I already added an issue for you to check out      I ll meet you over there  can t wait to get started  journal   Intended to keep track of my notes                                                                                                                                                                                                                                                            Interesting journal links     http   www robertfeldt net advice se venues    https   link springer com journal 10664   https   www sciencedirect com journal journal of systems and software   https   www sciencedirect com browse journals and books subject software   https   www computer org portal web tse    https   onlinelibrary wiley com journal 13652575   https   jserd springeropen com    especialidad bi Churn classification   Packages     package        version                                pandas         0 24 2      scikit learn   0 20 3      Installation   console pip install pandas sklearn   Run   console python kernel py   Reference     https   www kaggle com becksddf churn in telecoms dataset   calculator OCA Study Guide notes OCP Study Guide  Exam 1Z0 815 Deploy Static Website on AWS   In this project  you will deploy a static website to AWS using S3  CloudFront  and IAM    The files included are       index html   The Index document for the website    post html   Post sample     img   The background image file for the website     vendor   Bootssrap CSS framework  Font  and JavaScript libraries needed for the website to function     css   CSS files for the website    cloud devops engineer Deploy a high availability web app using CloudFormation   Problem   Your company is creating an Instagram clone called Udagram   Developers pushed the latest version of their code in a zip file located in a public S3 Bucket      You have been tasked with deploying the application  along with the necessary supporting software into its matching infrastructure      This need to be done in an automated fashion so that the infrastructure can be discarded as soon as the testing team finishes their tests and gathers their results    Server specs   You ll need to create a  Launch Configuration  for your application servers in order to deploy four servers  two located in each of your private subnets  The launch configuration will be used by an auto scaling group      You ll need two vCPUs and at least 4GB of RAM  The Operating System to be used is Ubuntu 18  So  choose an Instance size and Machine Image  AMI  that best fits this spec  Be sure to allocate at least 10GB of disk space so that you don t run into issues    Run templates   bash     create infrastructure sh   bash     create servers sh   Diagram   Jenkins Pipeline on AWS   In this project we will deploy and run an instance on AWS  configure Jenkins  and create a pipeline to deploy a static website on S3    Outline of this Project   Here is a outline of the steps taken      AWS Steps   Install Jenkins On Ubuntu   Set up Jenkins   Install required plugins   Set up GitHub   Set up AWS credentials in Jenkins   Set up S3 Bucket   Set up pipeline for AWS   Addd another stage in pipeline     Links   Jenkins instance   Static site bucket   Screenshots   Screenshot 01   jenkins user authentication     Screenshot 02   Jenkins installation on EC2 instance     Screenshot 03   Blue Ocen plugin running     Screenshot 04   Github pipelines     Screenshot 05   Static site s file on S3     Screenshot 06   Lint HTML stage fail     Screenshot 07   Lint HTML stage success     Project Overview   In this project  you will apply the skills you have acquired in this course to operationalize a Machine Learning Microservice API     You are given a pre trained   sklearn  model that has been trained to predict housing prices in Boston according to several features  such as average rooms in a home and data about highway access  teacher to pupil ratios  and so on  You can read more about the data  which was initially taken from Kaggle  on  the data source site   This project tests your ability to operationalize a Python flask app in a provided file   app py  that serves out predictions  inference  about housing prices through API calls  This project could be extended to any pre trained machine learning model  such as those for image recognition and data labeling    Project Tasks   Your project goal is to operationalize this working  machine learning microservice using  kubernetes   which is an open source system for automating the management of containerized applications  In this project you will    Test your project code using linting   Complete a Dockerfile to containerize this application   Deploy your containerized application using Docker and make a prediction   Improve the log statements in the source code for this application   Configure Kubernetes and create a Kubernetes cluster   Deploy a container using Kubernetes and make a prediction   Upload a complete Github repo with CircleCI to indicate that your code has been tested   You can find a detailed  project rubric  here     The final implementation of the project will showcase your abilities to operationalize production microservices      Setup the Environment     Create a virtualenv and activate it   Run  make install  to install the necessary dependencies     Running  app py     Standalone    python app py   Run in Docker      run docker sh   Run in Kubernetes      run kubernetes sh     Kubernetes Steps     Setup and Configure Docker locally   Setup and Configure Kubernetes locally   Create Flask app in Container   Run via kubectl   Getting Started with Create React App   This project was bootstrapped with  Create React App     Available Scripts   In the project directory  you can run    npm start   Runs the app in the development mode   Open  http   localhost 3000  to view it in your browser    The page will reload when you make changes   You may also see any lint errors in the console    npm test   Launches the test runner in the interactive watch mode   See the section about  running tests  for more information    npm run build   Builds the app for production to the  build  folder   It correctly bundles React in production mode and optimizes the build for the best performance    The build is minified and the filenames include the hashes   Your app is ready to be deployed    See the section about  deployment  for more information    npm run eject   Note  this is a one way operation  Once you  eject   you can t go back    If you aren t satisfied with the build tool and configuration choices  you can  eject  at any time  This command will remove the single build dependency from your project    Instead  it will copy all the configuration files and the transitive dependencies  webpack  Babel  ESLint  etc  right into your project so you have full control over them  All of the commands except  eject  will still work  but they will point to the copied scripts so you can tweak them  At this point you re on your own    You don t have to ever use  eject   The curated feature set is suitable for small and middle deployments  and you shouldn t feel obligated to use this feature  However we understand that this tool wouldn t be useful if you couldn t customize it when you are ready for it    Learn More   You can learn more in the  Create React App documentation     To learn React  check out the  React documentation     Code Splitting   This section has moved here   https   facebook github io create react app docs code splitting   Analyzing the Bundle Size   This section has moved here   https   facebook github io create react app docs analyzing the bundle size   Making a Progressive Web App   This section has moved here   https   facebook github io create react app docs making a progressive web app   Advanced Configuration   This section has moved here   https   facebook github io create react app docs advanced configuration   Deployment   This section has moved here   https   facebook github io create react app docs deployment   npm run build  fails to minify   This section has moved here   https   facebook github io create react app docs troubleshooting npm run build fails to minify Getting Started with Create React App   This project was bootstrapped with  Create React App     Available Scripts   In the project directory  you can run    npm start   Runs the app in the development mode   Open  http   localhost 3000  to view it in your browser    The page will reload when you make changes   You may also see any lint errors in the console    npm test   Launches the test runner in the interactive watch mode   See the section about  running tests  for more information    npm run build   Builds the app for production to the  build  folder   It correctly bundles React in production mode and optimizes the build for the best performance    The build is minified and the filenames include the hashes   Your app is ready to be deployed    See the section about  deployment  for more information    npm run eject   Note  this is a one way operation  Once you  eject   you can t go back    If you aren t satisfied with the build tool and configuration choices  you can  eject  at any time  This command will remove the single build dependency from your project    Instead  it will copy all the configuration files and the transitive dependencies  webpack  Babel  ESLint  etc  right into your project so you have full control over them  All of the commands except  eject  will still work  but they will point to the copied scripts so you can tweak them  At this point you re on your own    You don t have to ever use  eject   The curated feature set is suitable for small and middle deployments  and you shouldn t feel obligated to use this feature  However we understand that this tool wouldn t be useful if you couldn t customize it when you are ready for it    Learn More   You can learn more in the  Create React App documentation     To learn React  check out the  React documentation     Code Splitting   This section has moved here   https   facebook github io create react app docs code splitting   Analyzing the Bundle Size   This section has moved here   https   facebook github io create react app docs analyzing the bundle size   Making a Progressive Web App   This section has moved here   https   facebook github io create react app docs making a progressive web app   Advanced Configuration   This section has moved here   https   facebook github io create react app docs advanced configuration   Deployment   This section has moved here   https   facebook github io create react app docs deployment   npm run build  fails to minify   This section has moved here   https   facebook github io create react app docs troubleshooting npm run build fails to minify Getting Started with Create React App   This project was bootstrapped with  Create React App     Available Scripts   In the project directory  you can run    npm start   Runs the app in the development mode   Open  http   localhost 3000  to view it in your browser    The page will reload when you make changes   You may also see any lint errors in the console    npm test   Launches the test runner in the interactive watch mode   See the section about  running tests  for more information    npm run build   Builds the app for production to the  build  folder   It correctly bundles React in production mode and optimizes the build for the best performance    The build is minified and the filenames include the hashes   Your app is ready to be deployed    See the section about  deployment  for more information    npm run eject   Note  this is a one way operation  Once you  eject   you can t go back    If you aren t satisfied with the build tool and configuration choices  you can  eject  at any time  This command will remove the single build dependency from your project    Instead  it will copy all the configuration files and the transitive dependencies  webpack  Babel  ESLint  etc  right into your project so you have full control over them  All of the commands except  eject  will still work  but they will point to the copied scripts so you can tweak them  At this point you re on your own    You don t have to ever use  eject   The curated feature set is suitable for small and middle deployments  and you shouldn t feel obligated to use this feature  However we understand that this tool wouldn t be useful if you couldn t customize it when you are ready for it    Learn More   You can learn more in the  Create React App documentation     To learn React  check out the  React documentation     Code Splitting   This section has moved here   https   facebook github io create react app docs code splitting   Analyzing the Bundle Size   This section has moved here   https   facebook github io create react app docs analyzing the bundle size   Making a Progressive Web App   This section has moved here   https   facebook github io create react app docs making a progressive web app   Advanced Configuration   This section has moved here   https   facebook github io create react app docs advanced configuration   Deployment   This section has moved here   https   facebook github io create react app docs deployment   npm run build  fails to minify   This section has moved here   https   facebook github io create react app docs troubleshooting npm run build fails to minify Getting Started with Create React App   This project was bootstrapped with  Create React App     Available Scripts   In the project directory  you can run    npm start   Runs the app in the development mode   Open  http   localhost 3000  to view it in your browser    The page will reload when you make changes   You may also see any lint errors in the console    npm test   Launches the test runner in the interactive watch mode   See the section about  running tests  for more information    npm run build   Builds the app for production to the  build  folder   It correctly bundles React in production mode and optimizes the build for the best performance    The build is minified and the filenames include the hashes   Your app is ready to be deployed    See the section about  deployment  for more information    npm run eject   Note  this is a one way operation  Once you  eject   you can t go back    If you aren t satisfied with the build tool and configuration choices  you can  eject  at any time  This command will remove the single build dependency from your project    Instead  it will copy all the configuration files and the transitive dependencies  webpack  Babel  ESLint  etc  right into your project so you have full control over them  All of the commands except  eject  will still work  but they will point to the copied scripts so you can tweak them  At this point you re on your own    You don t have to ever use  eject   The curated feature set is suitable for small and middle deployments  and you shouldn t feel obligated to use this feature  However we understand that this tool wouldn t be useful if you couldn t customize it when you are ready for it    Learn More   You can learn more in the  Create React App documentation     To learn React  check out the  React documentation     Code Splitting   This section has moved here   https   facebook github io create react app docs code splitting   Analyzing the Bundle Size   This section has moved here   https   facebook github io create react app docs analyzing the bundle size   Making a Progressive Web App   This section has moved here   https   facebook github io create react app docs making a progressive web app   Advanced Configuration   This section has moved here   https   facebook github io create react app docs advanced configuration   Deployment   This section has moved here   https   facebook github io create react app docs deployment   npm run build  fails to minify   This section has moved here   https   facebook github io create react app docs troubleshooting npm run build fails to minify Getting Started with Create React App   This project was bootstrapped with  Create React App     Available Scripts   In the project directory  you can run    npm start   Runs the app in the development mode   Open  http   localhost 3000  to view it in your browser    The page will reload when you make changes   You may also see any lint errors in the console    npm test   Launches the test runner in the interactive watch mode   See the section about  running tests  for more information    npm run build   Builds the app for production to the  build  folder   It correctly bundles React in production mode and optimizes the build for the best performance    The build is minified and the filenames include the hashes   Your app is ready to be deployed    See the section about  deployment  for more information    npm run eject   Note  this is a one way operation  Once you  eject   you can t go back    If you aren t satisfied with the build tool and configuration choices  you can  eject  at any time  This command will remove the single build dependency from your project    Instead  it will copy all the configuration files and the transitive dependencies  webpack  Babel  ESLint  etc  right into your project so you have full control over them  All of the commands except  eject  will still work  but they will point to the copied scripts so you can tweak them  At this point you re on your own    You don t have to ever use  eject   The curated feature set is suitable for small and middle deployments  and you shouldn t feel obligated to use this feature  However we understand that this tool wouldn t be useful if you couldn t customize it when you are ready for it    Learn More   You can learn more in the  Create React App documentation     To learn React  check out the  React documentation     Code Splitting   This section has moved here   https   facebook github io create react app docs code splitting   Analyzing the Bundle Size   This section has moved here   https   facebook github io create react app docs analyzing the bundle size   Making a Progressive Web App   This section has moved here   https   facebook github io create react app docs making a progressive web app   Advanced Configuration   This section has moved here   https   facebook github io create react app docs advanced configuration   Deployment   This section has moved here   https   facebook github io create react app docs deployment   npm run build  fails to minify   This section has moved here   https   facebook github io create react app docs troubleshooting npm run build fails to minify Getting Started with Create React App   This project was bootstrapped with  Create React App     Available Scripts   In the project directory  you can run    npm start   Runs the app in the development mode   Open  http   localhost 3000  to view it in your browser    The page will reload when you make changes   You may also see any lint errors in the console    npm test   Launches the test runner in the interactive watch mode   See the section about  running tests  for more information    npm run build   Builds the app for production to the  build  folder   It correctly bundles React in production mode and optimizes the build for the best performance    The build is minified and the filenames include the hashes   Your app is ready to be deployed    See the section about  deployment  for more information    npm run eject   Note  this is a one way operation  Once you  eject   you can t go back    If you aren t satisfied with the build tool and configuration choices  you can  eject  at any time  This command will remove the single build dependency from your project    Instead  it will copy all the configuration files and the transitive dependencies  webpack  Babel  ESLint  etc  right into your project so you have full control over them  All of the commands except  eject  will still work  but they will point to the copied scripts so you can tweak them  At this point you re on your own    You don t have to ever use  eject   The curated feature set is suitable for small and middle deployments  and you shouldn t feel obligated to use this feature  However we understand that this tool wouldn t be useful if you couldn t customize it when you are ready for it    Learn More   You can learn more in the  Create React App documentation     To learn React  check out the  React documentation     Code Splitting   This section has moved here   https   facebook github io create react app docs code splitting   Analyzing the Bundle Size   This section has moved here   https   facebook github io create react app docs analyzing the bundle size   Making a Progressive Web App   This section has moved here   https   facebook github io create react app docs making a progressive web app   Advanced Configuration   This section has moved here   https   facebook github io create react app docs advanced configuration   Deployment   This section has moved here   https   facebook github io create react app docs deployment   npm run build  fails to minify   This section has moved here   https   facebook github io create react app docs troubleshooting npm run build fails to minify Getting Started with Create React App     This project was bootstrapped with  Create React App     Available Scripts   In the project directory  you can run    npm start   Runs the app in the development mode   Open  http   localhost 3000  to view it in your browser    The page will reload when you make changes   You may also see any lint errors in the console    npm test   Launches the test runner in the interactive watch mode   See the section about  running tests  for more information    npm run build   Builds the app for production to the  build  folder   It correctly bundles React in production mode and optimizes the build for the best performance    The build is minified and the filenames include the hashes   Your app is ready to be deployed    See the section about  deployment  for more information    npm run eject   Note  this is a one way operation  Once you  eject   you can t go back    If you aren t satisfied with the build tool and configuration choices  you can  eject  at any time  This command will remove the single build dependency from your project    Instead  it will copy all the configuration files and the transitive dependencies  webpack  Babel  ESLint  etc  right into your project so you have full control over them  All of the commands except  eject  will still work  but they will point to the copied scripts so you can tweak them  At this point you re on your own    You don t have to ever use  eject   The curated feature set is suitable for small and middle deployments  and you shouldn t feel obligated to use this feature  However we understand that this tool wouldn t be useful if you couldn t customize it when you are ready for it    Learn More   You can learn more in the  Create React App documentation     To learn React  check out the  React documentation     Code Splitting   This section has moved here   https   facebook github io create react app docs code splitting   Analyzing the Bundle Size   This section has moved here   https   facebook github io create react app docs analyzing the bundle size   Making a Progressive Web App   This section has moved here   https   facebook github io create react app docs making a progressive web app   Advanced Configuration   This section has moved here   https   facebook github io create react app docs advanced configuration   Deployment   This section has moved here   https   facebook github io create react app docs deployment   npm run build  fails to minify   This section has moved here   https   facebook github io create react app docs troubleshooting npm run build fails to minify Getting Started with Create React App   This project was bootstrapped with  Create React App     Available Scripts   In the project directory  you can run    npm start   Runs the app in the development mode   Open  http   localhost 3000  to view it in your browser    The page will reload when you make changes   You may also see any lint errors in the console    npm test   Launches the test runner in the interactive watch mode   See the section about  running tests  for more information    npm run build   Builds the app for production to the  build  folder   It correctly bundles React in production mode and optimizes the build for the best performance    The build is minified and the filenames include the hashes   Your app is ready to be deployed    See the section about  deployment  for more information    npm run eject   Note  this is a one way operation  Once you  eject   you can t go back    If you aren t satisfied with the build tool and configuration choices  you can  eject  at any time  This command will remove the single build dependency from your project    Instead  it will copy all the configuration files and the transitive dependencies  webpack  Babel  ESLint  etc  right into your project so you have full control over them  All of the commands except  eject  will still work  but they will point to the copied scripts so you can tweak them  At this point you re on your own    You don t have to ever use  eject   The curated feature set is suitable for small and middle deployments  and you shouldn t feel obligated to use this feature  However we understand that this tool wouldn t be useful if you couldn t customize it when you are ready for it    Learn More   You can learn more in the  Create React App documentation     To learn React  check out the  React documentation     Code Splitting   This section has moved here   https   facebook github io create react app docs code splitting   Analyzing the Bundle Size   This section has moved here   https   facebook github io create react app docs analyzing the bundle size   Making a Progressive Web App   This section has moved here   https   facebook github io create react app docs making a progressive web app   Advanced Configuration   This section has moved here   https   facebook github io create react app docs advanced configuration   Deployment   This section has moved here   https   facebook github io create react app docs deployment   npm run build  fails to minify   This section has moved here   https   facebook github io create react app docs troubleshooting npm run build fails to minify This project was bootstrapped with  Create React App     Available Scripts   In the project directory  you can run    npm start   Runs the app in the development mode   Open  http   localhost 3000  to view it in the browser    The page will reload if you make edits   You will also see any lint errors in the console    npm test   Launches the test runner in the interactive watch mode   See the section about  running tests  for more information    npm run build   Builds the app for production to the  build  folder   It correctly bundles React in production mode and optimizes the build for the best performance    The build is minified and the filenames include the hashes   Your app is ready to be deployed    See the section about  deployment  for more information    npm run eject   Note  this is a one way operation  Once you  eject   you can t go back    If you aren t satisfied with the build tool and configuration choices  you can  eject  at any time  This command will remove the single build dependency from your project    Instead  it will copy all the configuration files and the transitive dependencies  webpack  Babel  ESLint  etc  right into your project so you have full control over them  All of the commands except  eject  will still work  but they will point to the copied scripts so you can tweak them  At this point you re on your own    You don t have to ever use  eject   The curated feature set is suitable for small and middle deployments  and you shouldn t feel obligated to use this feature  However we understand that this tool wouldn t be useful if you couldn t customize it when you are ready for it    Learn More   You can learn more in the  Create React App documentation     To learn React  check out the  React documentation     Code Splitting   This section has moved here  https   facebook github io create react app docs code splitting   Analyzing the Bundle Size   This section has moved here  https   facebook github io create react app docs analyzing the bundle size   Making a Progressive Web App   This section has moved here  https   facebook github io create react app docs making a progressive web app   Advanced Configuration   This section has moved here  https   facebook github io create react app docs advanced configuration   Deployment   This section has moved here  https   facebook github io create react app docs deployment   npm run build  fails to minify   This section has moved here  https   facebook github io create react app docs troubleshooting npm run build fails to minify intro test jasmine   Getting Started With the BLoC Pattern   Tutorial   Getting Started With the BLoC Pattern
79,ericshape,heatmap modification for test   reference heatmap js React   Flux  Starter Kit               This project template is a skeleton for an  isomorphic  web application  SPA  based on Facebook s  React  library and  Flux  architecture  You can use it to quickly bootstrap your web application projects  All the parts of this project template are easily replaceable      Demo   http   reactjs kriasoft com   Documentation     React Style Guide     Directory Layout          build                        The folder for compiled output      docs                         Documentation files for the project      node modules                 3rd party libraries and utilities      src                          The source code of the application          actions                  Action creators that allow to trigger a dispatch to stores          assets                   Static files which are copied to   build on compile          components               React components          constants                Enumerations used in action creators and stores          content                  Website content  plain HTML or Markdown  Jade  you name it           core                     Core components  Flux dispatcher  base classes  utilities           stores                   Stores contain the application state and logic          styles                   CSS styles  deprecated  put CSS into components  folders           templates                HTML templates for server side rendering  emails etc           app js                   Client side startup script          server js                Server side startup script     gulpfile js                   Configuration file for automated builds     package json                  The list of 3rd party libraries and utilities     preprocessor js               ES6 transpiler settings for Jest      webpack config js             Webpack configuration for bundling and optimization   Getting Started   Just  clone  or  fork  the repo and start hacking    shell   git clone  o react starter kit https   github com kriasoft react starter kit git MyApp   cd MyApp   npm install  g gulp             Install Gulp task runner globally   npm install                     Install Node js components listed in   package json   How to Build   shell   gulp build                      or   gulp build   release    By default  it builds in debug mode  If you need to build in release mode  add    release  flag    How to Run   shell   gulp                            or   gulp   release    This will start a lightweight development server with LiveReload and synchronized browsing across multiple devices and browsers    How to Deploy   shell   gulp build   release            Builds the project in release mode   gulp deploy                     or   gulp deploy   production    For more information see  deploy  task in  gulpfile js     How to Update   You can always fetch and merge the recent changes from this repo back into your own project    shell   git checkout master   git fetch react starter kit   git merge react starter kit master   npm install   How to Test   Run unit tests powered by  Jest  with the following  npm  command    shell   npm test   Test any javascript module by creating a    tests     directory where the file is  Name the test by appending   test js  to the js file   Jest  will do the rest    Learn More     Getting Started with React js   React js Wiki on GitHub   React js Questions on StackOverflow   React js Discussion Board   Flux Architecture for Building User Interfaces   Jest   Painless Unit Testing   Flow   A static type checker for JavaScript   The Future of React   Learn ES6    ES6 Features     Support   Have feedback  feature request or need help  Contact me on  codementor io koistya     Copyright   Source code is licensed under the MIT License  MIT   See  LICENSE txt  file in the project root  Documentation to the project is licensed under the  CC BY 4 0  license  React logo image is a trademark of Facebook  Inc  try mean   try mean stack Crawling Tweets     Crawling tweets via specific user IDs     Getting started     add Twitter API keys and secrets to  twitterOauth py   You will need to  create a Twitter application     add user IDs to  user list csv     run  getTweets py  in the shell    shell python getTweets py user list csv     Contributing   If you d like to contribute  please fork the repository and use a feature branch  Pull requests are warmly welcome    Licensing   The code in this project is licensed under MIT license  rnn deck   Re design tweetdeck with RNN model for hashtag prediction and tweet embedding   VLDB Summer School 2021 Labs   Introduction   This is the labs of VLDB Summer School 2021  The target is to build a distributed database    There are several modules in a distributed database      TinyKV  the storage engine of the system    TinyScheduler  it is used to manager and schedule TinyKV cluster    TinySQL  the SQL layer of TinyKV engine      Labs   There are 4 labs in this course      Lab 1   implement the storage and log layer in TinyKV    Lab 2   implement the transaction layer in TinyKV    Lab 3   implement the Percolator protocol    Lab 4  implement the SQL execution layer    Lab 4 A   implement SQL protocol    Lab 4 B   implement update executor    Lab 4 C   implement select and projection executor          The code is separated into 2 parts  TinyKV and TinyScheduler is in  tinykv   and TinySQL is in  tinysql      You need to follow the order in the  labs  chapter  You may learn more from the README files in  TinyKV  and  TinySQL     Autograding   The details of classroom usage can be found in the  classroom doc     Autograding is a workflow which can automatically run test cases  However there are some limitations in Github classroom  in order to make golang works and run it in our self hosted machines   you need to overwrite the workflow generated by Github classroom and commit it     sh cp scripts classroom yml  github workflows classroom yml   If you don t use  GitHub classroom   just fork this repo  work in your repo  test locally and send a  email  with your repository address to us after complete some or all the tasks    Getting started   First  please clone the repository with git to get the source code of the project    bash git clone https   github com vldbss 2021 vldb 2021 labs   username   git   Then make sure you have installed  go     1 13 toolchains  You should also have installed  make   Now you can run  make  under  tinykv  or  tinysql  dir to check that everything is working as expected  You should see it runs successfully    Deploy a cluster   Rather than a course  you can try TinyKV by deploying a real cluster  and interact with it through TinySQL    Build   cd tinykv make kv   It builds the binary of  tinykv server  and  tinyscheduler server  to  bin  dir    cd tinysql make server  It buillds the binary of  tinysql server  to  bin  dir    Deploy By Hand   Put the binary of  tinyscheduler server    tinykv server  and  tinysql server  into a single dir  Under the binary dir  run the following commands    mkdir  p data     tinyscheduler server     tinykv server  path data     tinysql server   store tikv   path  127 0 0 1 2379    Deploy Use Cluster Command   See  TinyUp
80,stuhlmueller,Scheme Transforms   Required      scheme tools     Available transforms      Letrec to set    Assignment conversion   Continuation passing style  cps    Redex reduction   Return conversion   Closure conversion  cc    Untag transform     Sequence of transforms   The following sequence shows the interface for each transform    tag   top level begin define   self eval   primitive   lambda   if    A B    begin   set    letrec     letrec to set      tag   top level begin define   self eval   primitive   lambda   if    A B    begin   set      assignment      tag   top level begin define   self eval   primitive   lambda   begin   if    A B      cps      tag   top level begin define   self eval   primitive   lambda   if    A B    apply   let     redex      tag   top level begin define   self eval   primitive   lambda   if    A B    apply   let     return      tag   top level begin define   self eval   primitive   lambda   if    A B    apply   let     cc      tag   top level begin define   self eval   primitive   lambda   if    A B    apply   let    The untag transform can be applied at any point within this sequence    tag         untag         Scheme Tools   Required  one of the following       ikarus scheme   vicare scheme     Optional      To solve systems of linear equations    numpy       To visualize graphs    ubigraph         Includes      xitomatl library   srfi library   mit church  math and ffi library  by David Wingate    scsh  module for strongly connected components   merge sort   AD tools   by Jeffrey Mark Siskind      Changes      xitomatl profiler srfi time sls   show total time   Cosh   Cosh is an experimental Church implementation that uses dynamic programming based on hashing factored continuations    Cosh is deprecated  If you are looking to write probabilistic programs in Scheme syntax   webchurch  is your best bet  You can use it online at  probmods org   or on your computer via nodejs  If you are not tied to Scheme syntax  I recommend  webppl   our most recent probabilistic programming language  webppl is significantly faster than previous implementations  and additionally supports particle filtering and best first enumeration inference strategies    Installation   This installation assumes that you have  git  and a R6RS Scheme installed  The  instructions on the Church wiki  describe how to install  Vicare Scheme  with foreign function interface enabled        Install  scheme tools         Clone the scheme tools repository using  git clone git   github com stuhlmueller scheme tools git         Add the scheme tools directory to your   VICARE LIBRARY PATH      To add a directory to your   VICARE LIBRARY PATH   change into the directory and type  echo  e   nexport VICARE LIBRARY PATH  pwd    VICARE LIBRARY PATH        bashrc   Replace     bashrc  with the location of your shell config file        Add the scheme tools bin directory to your   PATH     To add a directory to your   PATH    cd  into the directory and type  echo  e   nexport PATH  pwd    PATH        bashrc             Install  scheme transforms         Clone the repository using  git clone git   github com stuhlmueller scheme transforms git         Add the scheme transforms directory to your   VICARE LIBRARY PATH   see above             Install  cosh         Clone the repository using  git clone git   github com stuhlmueller cosh git         Add the cosh directory to your   VICARE LIBRARY PATH   see above         Add the cosh bin directory to your   PATH   see above             Reload your shell config file  e g   via  source    bashrc         Usage   Create a file called myprogram church with the following content     rejection query   define x  flip     define y  flip     list x y    or x y      Then  on the command line  type    cosh myprogram church    You should see the following output      f  t   0 3333333333333332   1 09861228866811    t  f   0 3333333333333332   1 09861228866811    t  t   0 3333333333333332   1 09861228866811     This shows the probability  and log probability  of each possible program return value    Options     cosh   help Usage  cosh  file   options   Options     h    help            show this help message and exit    d    debug           run all scheme commands in debug mode    k    keep            do not delete compiled file    l LIMIT    limit LIMIT                         restrict graph size    v    verbose         display all executed commands    t    time            record runtime  Board   Board is a thin layer of syntactic sugar on top of  Church  and  Cosh  that makes it easy to represent and solve simple coordination games  Board         separates the definition of games from the solution method    enforces factorization of policies into goal predicates and world models   allows computing the policies for all players simultaneously     Installation     Install  cosh     Check out board   git clone git   github com stuhlmueller board git   Add the board directory to your   VICARE LIBRARY PATH       Example   This is an example of how to use board to compute policies for a simple coordination game  We model the following situation      John and Jim agreed to meet at a bar  but did not specify which of two bars to go to  the popular bar or the  unpopular bar  John wants to meet Jim  Jim doesn t want to meet John  but John doesn t know this  Which bar will John and Jim  choose        r6rs   import  board           rnrs     define bar game    make game             define actions          popular bar other bar          define  jim 1          solve goal avoid               identity world                list john 0           define  jim 0          solve goal match               identity world                list john 0                 lambda     sample action  6            define  john 0          solve goal match               identity world                list jim 0                 lambda     sample action  6                run game bar game            recursion solver 5 2      Running the above code in  Ikarus Scheme  or  Vicare Scheme  results in the following output    jim 1   other bar  0 9999999999878807   popular bar  1 2087651728459907e 11  jim 0   other bar  8 058434485672552e 12   popular bar  0 9999999999919361  john 0   other bar  8 058434485672552e 12   popular bar  0 9999999999919361    For more examples  see the  examples   directory    Syntax   The template for a simple board game looks like this      r6rs   import  board            rnrs     define my game     make game  game definition      run game my game  solver      A  game definition  is a list of policy definitions and definitions of arbitrary other variables and functions that are used within the policies    A  policy definition  has the following form     define  policy name     solve goal          world          policies          action prior      The arguments to  solve  must satisfy the following interface      goal  is a function that takes as input a state and returns a Boolean depending on whether the state satisfies the goal    world  is a function that takes as input an action for the current player  first argument  and for the other players  remaining arguments  and returns a state    policies  is a list of all players  policies  that the current player interacts with  This specifies the order of the remaining arguments to  world     action prior  is optional  If given  it is a thunk that returns an action  If not given  actions will be sampled uniformly from the list referenced by the variable  actions       Currently  the only supported  solver  is  recursion solver   It takes two arguments   depth  of recursion and  hardness  of the conditions  optimization strength   Python Tools   Installation   Add the  python tools  directory to your   PYTHONPATH      To add a directory to your   PYTHONPATH    cd  into the directory and type  echo  e   nexport PYTHONPATH  pwd    PYTHONPATH        bashrc   Replace     bashrc  with the location of your shell config file    Dependencies     To use the statistics utilities  install  R  and  rpy2       Includes     Simon Willison s  optfunc   StoppableThread   asyncproc   instancemethod pickling   JSChurch   JSChurch is deprecated  If you are looking to write probabilistic programs in Scheme syntax   webchurch  is your best bet  You can use it online at  probmods org   or on your computer via nodejs  If you are not tied to Scheme syntax  I recommend  webppl   our most recent probabilistic programming language  webppl is significantly faster than previous implementations  and additionally supports particle filtering and best first enumeration inference strategies    Instructions     jschurch make  builds the Church compiler in Javascript    jschurch webservice  additionally combines all Javascript needed to run Church code  via scheme2js web service  into a single file    jschurch run filename church  compiles filename church to Javascript using the Church compiler generated by make  then runs the program using node      Installation   To get access to the  bher  and  jschurch  command line tools and scheme libraries from any directory   cd  into the bher directory and type    echo  e   nexport PATH  pwd    PATH        bashrc  echo  e   nexport IKARUS LIBRARY PATH  pwd    IKARUS LIBRARY PATH        bashrc source    bashrc    Replace     bashrc  with the path to your shell configuration file  Scheme listings in LaTeX   Scheme syntax highlighting for the LaTeX listings package   Example   LaTeX code     documentclass article   usepackage listings   usepackage color   usepackage textcomp    lstset    language Scheme     begin document    title A Scheme Listing in  LaTeX   date    maketitle   begin lstlisting     The Y combinator  define Y    lambda  f        lambda  x   f  lambda arg  apply  x x  arg           lambda  x   f  lambda arg  apply  x x  arg            Factorial function using the Y combinator  define fact    Y  lambda  f          lambda  n            if     n 0               1                 n  f    n 1               end lstlisting    end document     As an alternative to  lstlisting   Scheme code can also be included from an external file     lstinputlisting ycombinator ss     Rendered      Installation   The  scheme listings  directory must be on your TeX search path    To use LaTeX from the Bash shell  add this to your shell config file  e g    bashrc      export TEXINPUTS  path to scheme listings   TEXINPUTS    For Emacs  use     setenv  TEXINPUTS   concat   path to scheme listings     getenv  TEXINPUTS      Requires manual installation of inference engines  bher  mit church  sine   PyReject   A tiny Python library for probabilistic models with inference using rejection sampling    Usage example   Model    import random  from pyreject import    def flip p     return random random     p   query def foo      x   flip  5    y   flip  5    observe x or y    return x  y    Estimating distributions        dist foo  n 10000     False  True   0 3345   True  False   0 3313   True  True   0 3342     Expectations    def f value  prob     x  y   value   return 1 if x or y else 0      expectation foo  f  n 10000   1 0    Entropy        entropy foo  n 10000   1 58455505656  i3   Learning stochastic inverses for amortized inference in Bayesian networks      This package is no longer maintained    isosmc    How to use    cd my project  npm install webppl isosmc webppl my file wppl   require webppl isosmc    See  examples   folder for use cases  webppl timeit   This package provides a function  timeit  that takes a thunk and returns an object with its return value and its runtime in milliseconds  For example    timeit function      var x   1    sleep 750     return x        returns     value  1    runtimeInMilliseconds  751      Installation   To globally install  webppl timeit   run    mkdir  p    webppl npm install   prefix    webppl webppl timeit    This may print warnings   npm WARN ENOENT      which can be ignored    To upgrade to the latest version  run    npm install   prefix    webppl webppl timeit   force    Usage   Once installed  you can make  timeit  available to  program wppl  by running    webppl program wppl   require webppl timeit    License   MIT webppl dp   This package provides a function  dp cache  that memoizes functions in a way compatible with mutual recursion    var even   dp cache function x      console log  Called  even  with argument     x     if  x    0        return true      else       return odd x   1            var odd   dp cache function x      console log  Called  odd  with argument     x     if  x    0        return false      else       return even x   1            console log odd 3    console log odd 5       returns   Called  odd  with argument 3 Called  even  with argument 2 Called  odd  with argument 1 Called  even  with argument 0 true Called  odd  with argument 5 Called  even  with argument 4 true    Installation   To globally install  webppl dp   run    mkdir  p    webppl npm install   prefix    webppl webppl dp    This may print warnings   npm WARN ENOENT      which can be ignored    To upgrade to the latest version  run    npm install   prefix    webppl webppl dp   force    Usage   Once installed  you can make  dp cache  available to  program wppl  by running    webppl program wppl   require webppl dp    License   MIT webppl intercache     Use with webppl daipp branch     To test  run         cd webppl intercache webppl tests test wppl   require webppl timeit   require        webppl json   This package provides two functions  json read  and  json write  for reading and writing JSON files       Read data  var testData   json read  data test file json        Write data  json write  data test file copy json   testData      Installation   To globally install  webppl json   run    mkdir  p    webppl npm install   prefix    webppl webppl json    This may print warnings   npm WARN ENOENT      which can be ignored    To upgrade to the latest version  run    npm install   prefix    webppl webppl json   force    Usage   Once installed  you can make  json read  and  json write  available to  program wppl  by running    webppl program wppl   require webppl json    Testing   Run the included test using    webppl test wppl   require      License   MIT This project is work in progress  I don t recommend that you try to use it just yet      Logistician   Logistician makes it easy to prototype computational experiments locally and to deploy them to the cloud at scale     Interface       Usage  logistician  OPTIONS  COMMAND  ARGS       Options      help  Show this message and exit    Commands    create     Run interactive setup for a new experiment   deploy     Run experiment in the cloud   run        Run experiment locally   setup      Run initial interactive setup for Logistician   shell      Open shell in experiment environment   status     Show deployment status   sync       Sync all data from cloud to local machine   terminate  Terminate cloud experiment       Features       Fast local development loop     For local development  Logistician directly uses code from your local project directory in order to make it easy to test changes without committing to repositories or constantly rebuilding docker images        Reproducible experiments   Logistician uses  Docker  to ensure that local and remote executions run in the same environment  Remote experiments always run based on particular commits in Git repositories        Large scale experiments in the cloud   For exploring large parameter spaces  cloud deployment is parameterized by parameter sets  Logistician creates a cloud instance for each element of the set and uses  Terraform  to automate setting up and shutting down large numbers of cloud machines        Transparently built on standard tools   Logistician is a thin wrapper around Docker and Terraform and prints out all commands it issues  You can always switch to managing the containers and cloud instances using these native tools if additional functionality is needed        What it does   You have a project  let s say it s called  adversarial rnn        sh   Go to project folder   cd  home jane adversarial rnn   Make sub folder for experiments   mkdir experiments    cd experiments   Create a new experiment   logistician create rnn experiment 1       Logistician then asks you some questions about your experiment        This script will interactively create a new experiment stored at   home jane adversarial rnn experiments rnn experiment 1   Globally unique experiment name  rnn experiment 1   Remote Git URL  https   github com jane adversarial rnn git   Experiment command  relative to project root   python src run rnn py   Experiment created        Now the experiments folder contains a sub folder  rnn experiment 1  with two files     Dockerfile  describes the software environment for your experiment    parameters json  describes the cloud machine setup  instance type  region  etc  and experiment conditions   These two files are sufficient to run your experiment locally and in the cloud  You can edit the  Dockerfile  to set up the environment for your experiment    To run it locally  passing arguments to your experiment script    sh logistician run   rnn experiment 1  o    learning rate 0 01   optimizer adam    Once you re ready to run your experiment in the cloud  you edit  parameters json  to add the arguments for all conditions    json      experiment conditions            learning rate 0 1   optimizer adam          learning rate 0 01   optimizer adam          learning rate 0 001   optimizer adam          Create three AWS instances for the conditions above and run one condition on each    sh logistician deploy   rnn experiment 1   Sync the logs and results from the instances to your local experiments folder    sh logistician sync   rnn experiment 1   Shut down the instances    sh logistician terminate   rnn experiment 1   Installation   Setup       Install  Docker  and log in using  docker login       Install  Terraform       Install and configure Logistician    sh git clone https   github com stuhlmueller logistician git cd logistician pip install   logistician setup       Test   To make sure everything is set up correctly  try running the example experiment       sh   Go to experiment folder   cd logistician examples addition experiments 1   Run locally  directly using project directory    logistician run  o  1 2    Run locally  cloned from Github    logistician run   clone  o  1 2    Run remotely on AWS  retrieve the data  shut down  this will take a while    logistician deploy logistician sync logistician terminate       Note that running on AWS will incur  small  costs    Upgrade   To upgrade to the latest version of Logistician  run  git pull  in the cloned repository    Usage   Assumptions   Your project lives in a local directory  say   home jane my project     This directory is a git repository that is mirrored remotely  say at  https   github com jane my project git     The following assumes that there is an  experiments  folder in  my project   but you can use any folder under  my project to store your experiments and results    Creating a new experiment   Creating a new experiment with name  my new experiment       Copy  logistician examples addition experiments 1  to  my project experiments 1   Edit  my project experiments 1 Dockerfile  to reflect your experimental setup     Make sure to preserve the switch in  CMD  that supports both local and remote project directories    Local development      sh   Go to project directory   cd  home jane my project   Make changes to files in my project   Run the experiment script with command line args  foo bar baz    logistician run  o  foo bar baz  experiments 1       To interact with the dev environment  try this    sh docker shell   Running experiments in the cloud   Once local development indicates that the experiment is ready to be run on the cloud       sh   Go to project directory   cd  home jane my project    Go to experiment folder   cd experiments 1   Update parameters json to reflect parameters we want to run in the cloud   Commit   push to remote Git repository   git add  A  git commit  m  Created experiment 1   git push   Run experiment on AWS   logistician deploy   Sync data from AWS to experiment directory   logistician sync   Shut down AWS instances   logistician terminate       FAQ   Why is Docker  push  failing on a connection with limited upload speed    Add    max concurrent uploads   1   to your  daemon json   If you re on Mac  you can do this using the Docker GUI via Preferences   Daemon   Advanced    What should I do if something goes wrong during  terraform apply     Run  logistician terminate  in the experiment directory to clean up    How can I log into the cloud instances manually    Logistician saves the instance IP addresses in  machines txt  in the experiment directory  Using these  you can log in using SSH    sh ssh  i    logistician ssh key ubuntu 54 153 54 33   Limitations   Logistician currently does not support        Private git repositories   Docker registries other than public Docker Hub   Cloud providers other than AWS     To discuss     Recording experimental setup  git commit id  and results  terminal log  files    For docker builds that clone from github  either need to point to particular commits  preferable  or rebuild with   no cache   Might want to use Docker Machine to orchestrate cloud deployments   To run locally  set up  Logistician   then run    logistician run experiments babi 1 a   o    random seed 1      optimize    To analyze  trace json  files generated from cloud runs  use  jq   For example    jq  c   input filename     1  devStats responses   data   results trace json Optimal Experimental Design in Church   Main files      screencast   mov   Demonstration of the system  modeling design expt run evaluate loop    exemplar prototype experiment     Generator for stimuli  features and responses    Prototype model   Exemplar model  GCM    oed church   Main Metropolis Hastings loop for sampling informative stimuli   test models church   Verify that the  random  response distribution of the prototype model is closer to another run of the prototype model than to a run of the exemplar model    test prt gcm church   Run both prototype and exemplar model on a stimulus that discriminates between the two models  Show the predictions of the two models    oed utils church    lib oed church   Various helper functions for Church   oed utils ss   Sorting  various divergence measures   scholar summaries   How to run      Check out the repository   git clone git github com stuhlmueller scholar summaries git     cd scholar summaries   Make a file  secrets  with contents like this   export semantic scholar api key       export serpapi api key       export openai api key         Install  Poetry  if you don t have it   Run  poetry install   Run  poetry shell   Run  source   secrets   Run  streamlit run scholarize py  which will open a web browser   Fast claims       gpt el   gpt el is an Emacs package that lets you interact with instruction following language models like GPT 3 Instruct   text davinci 002   from your editor  You can type a natural language command  with history and completion support  and optionally use the current region as input for the model  The package displays the output of the model in a temporary buffer that inherits the major mode of the original buffer  and updates it as the model generates more text  You can also browse and save the command history to a file for later reference    Installation   To use gpt el  you need to have the  openai  Python package installed and a valid OpenAI API key  You can install the package with  pip install openai   and get an API key from https   beta openai com     From MELPA package repository   MELPA is a popular third party package repository for Emacs  To install gpt el from MELPA  first add MELPA as a source in your Emacs init file    elisp  require  package   add to list  package archives                 melpa     https   melpa org packages    t    Then  use the built in package manager to install gpt el    M x package install RET gpt RET   Once installed  you can require the package in your init file    elisp  require  gpt    Alternatively  you can use  use package     elisp  use package gpt    From source   To install gpt el  clone this repository and add the following to your Emacs init file    elisp  add to list  load path   path to gpt el    require  gpt    Alternatively  you can use  use package     elisp  use package gpt    load path   path to gpt el     Configuration   You need to set the variable  gpt openai key  to your OpenAI API key use gpt el  For example    elisp  setq gpt openai key  sk Aes     AV8qzL     You can also customize the engine  by setting the variable  gpt openai engine     elisp  setq gpt openai engine  code davinci 002     By default  it is set to  text davinci 002     Usage   To run a generative model command  use the  gpt dwim  function  You can bind it to a key of your choice  for example    elisp  global set key  kbd  M C g    gpt dwim    When you invoke  gpt dwim   you will be prompted for a command  with history and completion  The command can be any text  For example    Write a haiku about Emacs    If you have an active region  it will be used as contextual input to the command  The output of GPT running the command will be displayed in a temporary buffer  with the same major mode as the original buffer  The output will be streamed as it is produced by the generative model  You can switch back to the original buffer at any time    You can view the command history by calling  display gpt command history   which will show the commands in a buffer  You can also export the command history to a file by calling  gpt export history   which will prompt you for a file name    License   gpt el is licensed under the MIT License  See  LICENSE md  for details
81,aaronabramov,backbone uploader   backbone html5 uploader js logger clojure haml Start  npm install    nodemon app coffee   Run tests    scripts test Some math to convert World Goedetic System coordinates to Mercator projection   https   clojars org clojure mercator projection   clojure clojure mercator projection core    wgs84  mercator   lat 41 897865137687376  lng  87 64455200350926     x  9756546 899835236   y 5117150 255835393          app coffee     data csv   pbcopy                                PROFIT               script to get list of users who don t follow you back on instagram    lein run  user id  pbpaste   coffee format coffee   pbcopy Convert 15 chararter Salesforce Ids to 18 character Id in C   based on binary representation of lower upper case symbols   shel c   sfid cpp  o sfid     shel   sfid 006C000000jjKuE      006C000000jjKuEIAU        translation service   brew install wget   ruby lib parse rb node charts   Dependencies     xQuartz https   xquartz macosforge org landing    Cairo  brew install cairo     export PKG CONFIG PATH  opt X11 lib pkgconfig npm install Fractaljs     Bundle javascript modules   lazy load in browser      javascript    app js     require fractal     require   jquery js     require   submodule js module     reference   lazy loaded bundle js   exports f   function      return  app            javascript    submodule js module exports    submodule     javascript    lazy loaded bundle js     require   react js module      html          var submodule   require  submodule js       require synchonously     console log submodule         submodule      use    react js   function        load  lazy loaded bundle js  first       var react   require    react js       then require module when it s available                  Express middleware   javascript var fractal   require  fractaljs    fractal config   assetPath   path resolve   dirname     client    app use   assets     fractal middleware     development   shell make install    make dev   run all tests   shell make test   watch mocha test   shell make watch mocha   run karma tests   shell brew install phantomjs make karma   Features TODO     Q    ES 6 Promises   get rid of mutable config   bulding bundles for prod   chch   FIXME   Usage   Launch the application by issuing one of the following commands    shell lein run  host  host    port  port     You can generate a standalone jar and run it    shell    lein uberjar java  jar target chch 0 1 0 SNAPSHOT standalone jar   You can also generate a war to deploy on a server like Tomcat  Jboss      shell lein ring uberwar   License   Copyright   2014 FIXME   Distributed under the Eclipse Public License  the same as Clojure  clojure lastfm      clojure  def api key  1234567890      def api secret  0987654321      let  token  get token api key api secret      get auth url api key token        Your application needs to open a web browser and    send the user to last fm api auth with your API    key and auth token as parameters  Use an HTTP GET    request  Your request will look like this       http   www last fm api auth  api key xxxxxxxxxxx token xxxxxxxx    If the user is not logged in to Last fm  they will be    redirected to the login page before being asked to grant    your application permission to use their account  On this    page they will see the name of your application  along with    the application description and logo as supplied in Section 1     Once the user has granted your application permission to use    their account  the browser based process is over and the user    is asked to close their browser and return to your application     get session api key api secret token          body   session    name   name    key   8482ded6ddf53598391c02a0c2c57b6b    subscriber   0           License   Copyright   2014 FIXME   Distributed under the Eclipse Public License either version 1 0 or  at your option  any later version  karma json fixtures preprocessor       Preprocessor for converting  json files into  js files and making them accessible from karma test environment   Installation   json        devDependencies              karma     0 12 1            karma json fixtures preprocessor    0 0 4            Configuration      js    karma conf js module exports   function config      config set       plugins           karma json fixtures preprocessor           preprocessors         fixtures      json     json fixtures       files         fixtures      json     jsonFixturesPreprocessor         strip this from the file path   fixture name   stripPrefix   test fixtures        strip this to the file path   fixture name   prependPrefix   mock         change the global fixtures variable name   variableName     mocks          camelize fixture filenames  e g  fixtures aa bb cc json  becames   fixtures    fixtures aaBbCc      camelizeFilenames  true       transform the filename   transformPath  function path        return path     js                         How it works   Preprocessor requires  json files and converts them into  js files by storing json data as javascript objects under    fixtures    namespace    the following file     fixtures test json   json        a    test     will be accessible in your test environment   js var fixture   window   fixtures    fixtures test    fixture  a          test  interactive esl tests     Initial Setup     Set up Node  see below    make setup   ask someone for key    Add the following line to   etc hosts     127 0 0 1       localhost       local esl com   Set up postgres  see below    Start postgres  postgres  D  usr local var postgres   While postgres is running  perform migrations  make migrate   npm install  g grunt   npm install  g grunt cli   Open new shell and start the web server   grunt   Go to http   local esl com 3009     Setting up Node     Install nvm from  here     nvm install 0 10 30   nvm use 0 10 30   Add  source    nvm nvm sh  to your    profile  load nvm when you open a shell    Add  nvm use 0 10 30  to your    profile  load the correct version of Node automatically when you open a shell      Setting up postgres     brew install postgres   npm install   postgres  D  usr local var postgres   In psql   create database esl development      If things aren t working with your user     In a new shell  start psql  psql   username  USERNAME   d template1     use unix username   In psql   create database  USERNAME       use unix username     Config   to decrypt and setup config files run  make setup   ask someone for key    to edit config    modify  config  config name  template json    run  make encrypt CONFIG NAME  config name   using the same key   Database migrations   make migrate     migrate the database   make rollback     node modules sequelize cli bin sequelize migration create   name  TABLE NAME    drop database esl development    create database esl development    ORM   javascript var models   require    models    models users create  facebook id   12345    complete function err  user              Running     npm install   nvm use 0 10 30   Open new shell and start postgres   postgres  D  usr local var postgres   Open new shell and start the web server   grunt   go to  http   localhost 3009   Sharkhorse       Javascript Test factories   Summary   Defining Factories   Factory definitions are plain javascript objects that hold generator objects       js    message factory js import  generators  from  sharkhorse     export const Message         id  generators sequence        subject  generators lorem   words 2       from            name  generators name   full            email  generators email                  Building objects from factories   To build an object from a factory definition use a  create  or  createMany  function the function would iterate through object s nested properties and evaluate all generators to their values    js import  create  createMany  from  sharkhorse   import  Message  from    message factory     create Message         id  1  subject   Lorem ipsum   from   name   Nickolas Conrad   email   random 0 example com    create Message         id  2  subject   Lorem ipsum   from   name   Seth Edwards   email   random 1 example com          createMany    will create an array of objects   js createMany Message  3                             generators   sequence     generates an incrementing or decrementing number every time it s evaluated      js generators sequence      1  2     generators sequence   decrement      1  0   1      generators sequence   startFrom 100     100  101  102              number     generates a random number   js generators number      285 generators number   min 500     24029 generators number   max 2  generators number   min 0  max 2    randomItem list    generators randomItem  1  2  3      one of the numbers   name     generates a random name   js generators name      Seth Edwards generators name   full      Seth Edwards generators name   first      Seth generators name   last      Edwards   email     generates a random unique email every time it s evaluated   js generators email      random 0 example com  random 1 example com   templateString     Tagged template string generator  Any generator passed as a template string value will be evaluated when passing the generator into  create  function   js let MyStr   generators templateString test  generators sequence      create MyStr      test1 create MyStr      test2 create MyStr      test3   lorem     generates random text  js generators lorem      Lorem ipsum dolor sit amet  per in mazim    generators lorem   word   generators lorem   words n  generators lorem   paraghaph   generators lorem   paraghaphs n    date     js generators date      Date   object generators date   jsTimestamp    1457241758397 generators date   unixTimestamp    1457241758   create FactoryDefinition    generates a new factory object from the passed argument   js generate create MessageFactory      id  1  subject   lorem          createMany FactoryDefinition  n    same as  create    but generates an array of factories Seedable pseudo random number generator written in javascript       javascript var PRNG   require  prng        prng   new PRNG 123456789     prng rand 9        random number in a range from 0 to 9 prng rand 10  100        random number in a range from 10 to 100 prng rand  1000   700        random number in a range from  1000 to  700   prng   new PRNG       use default seed prng rand 5555        930     LFSR   Linear feedback shift register using exclusive or  XOR  as a feedback function written in javascript        javascript LFSR   require  lfsr      var bitLength   10      initialState   parseInt  1010101010   2      seed     lfsr   new LFSR bitLength  initialState        get sequence of 10 pseudo random bits lfsr seq 10         1018      get string representing sequence of 10 pseudo random bits lfsr seqString 10          00001010100       Initialize register with default values    n   31 lfsr   LFSR    lfsr seqString 15         001000010100111          Runing tests   npm test ssh kit     minimalist  efficient ssh client for javascript      javascript var SSH   require  ssh kit        ssh   new SSH      ssh set  username    dmitriiabramov    ssh set  host    rheia us    ssh set  sshKey       ssh id rsa      ssh exec  pwd    ssh exec  ls  la    ssh exec  ls    ssh on  finish   console log bind console   all done               javascript    TODO       local command execution    ssh local  npm prune   production         remote copy    ssh scp    app tar gz      app tar gz         multiple servers in parallel ssh addServer  s1    ssh addServer  s2         ssh options ssh set  forward agent   true        additional parameters ssh with  dir     test   env   ENV   test    servers    s1    s2    function         ssh exec  forever start   run js             utils   ssh mkdirIfNotExists    test1       etc         Testing create temporary  authorized keys  file and restore original in after hook     carpool     Initial Setup     Set up Node  see below    Add the following line to   etc hosts     127 0 0 1       localhost       local carpool com   Set up postgres  see below    Start postgres  postgres  D  usr local var postgres   While postgres is running  perform migrations  gulp migrate   npm install  g gulp   npm install  g gulp cli   Open new shell and start the web server   gulp   Go to http   local carpool com 3000     Running     npm install   nvm use 0 10 36   Open new shell and start postgres   postgres  D  usr local var postgres   Open new shell and start the web server   gulp   go to  http   localhost 3000     Setting up Node     Install nvm from  here     nvm install 0 10 36   nvm use 0 10 36   Add  source    nvm nvm sh  to your    profile  load nvm when you open a shell    Add  nvm use 0 10 36  to your    profile  load the correct version of Node automatically when you open a shell      Setting up postgres     brew install postgres   npm install   postgres  D  usr local var postgres   In psql   create database carpool dev      Migrations   Create a migration   node   scripts migrate js create migration name  it will create two   sql  files  up and down  inside    migrations  directory   Run migrations   node   scripts migrate js migrate   Undo migrations   node   scripts migrate js rollback  will roll back the latest migration   Add dummy data  node   scripts seed js   Tests   gulp test gulp test unit gulp test integration gulp test browser   If things aren t working with your user     In a new shell  start psql  psql   username  USERNAME   d template1     use unix username   In psql   create database  USERNAME       use unix username     Using psql     Connect to database    c carpool dev   List all databases    dt   node sql migrations   raw SQL migrations for node   Example   In your project    js    migrate js var path   require  path      require  sql migrations   run          configuration here  See the Configuration section           CLI   run  node   migrate js  with arguments     node   migrate create migration name   will create two migration files  up and down     migrations 1415860098827 up migration name sql   migrations 1415860098827 down migration name sql     node   migrate migrate   will run all pending migrations     node   migrate js rollback   will rollback the last migration if there is one   Programmatic API   Migrate   In your project  js require  sql migrations   migrate          configuration here  See the Configuration section      This returns a promise which resolves rejects whenever the migration is complete    Rollback   In your project  js require  sql migrations   rollback          configuration here  See the Configuration section      This returns a promise which resolves rejects whenever the rollback is complete    Configuration   Configuration should be specified as below   js var configuration         migrationsDir  path resolve   dirname   migrations       This is the directory that should contain your SQL migrations      host   localhost      Database host     port  5432     Database port     db   sql migrations      Database name     user   dabramov      Database username     password   password      Database password     adapter   pg      Database adapter  pg  mysql        Parameters are optional  If you provide them then any occurrences of the parameter  i e  FOO  in the SQL scripts will be replaced by the value  i e  bar       parameters             FOO    bar             minMigrationTime  new Date  2018 01 01   getTime      Optional  Skip migrations before this before this time       You can also swap out the default logger  the  console  object  for another one that supports the log and error methods  You should do this before running any other commands   js require  sql migrations   setLogger       log  function           error  function            Migration files   Write raw sql in your migrations  You can also include placeholders which will be substituted  example    sql      migrations 1415860098827 up migration name sql create table  test table   id bigint  name varchar 255      sql      migrations 1415860098827 down migration name sql drop table  test table       Regex       Explanation                                                             abc        A single character of  a  b  or c   abc       Any single character except  a  b  or c  a z        Any single character in the range a z  a zA Z     Any single character in the range a z or A Z             Start of line             End of line  A          Start of string  z          End of string             Any single character  s          Any whitespace character  S          Any non whitespace character  d          Any digit  D          Any non digit  w          Any word character  letter  number  underscore  W          Any non word character  b          Any word boundary             Capture everything enclosed  a b        a or b a           Zero or one of a a           Zero or more of a a           One or more of a a 3         Exactly 3 of a a 3         3 or more of a a 3 6       Between 3 and 6 of a js structs   Javascript  structs  records    Type checks    contract programming      js var MyStruct   Struct define       fieldA  Struct Types string      fieldB  Struct Types object      FieldC  function contract value            if  value    99                return  precondition violated  expected value to be   99  got      value                       var myObj   new MyStruct       fieldA   My Field A       fieldB   a   test        fieldC  44       myObj get  fieldA          My Field A  myObj set  fieldA    test         myObj myObj get  fieldA          test  myObj set  fieldB   99        Error   Type violation  myObj set  fieldC   1000        Error   Contract violation  myObj set  undefinedField   40        Error   Undefined field        Fluxible Addons for React             Fluxible  addons for use with  React     bash   npm install   save fluxible addons react   Join the  fluxible channel of the  Reactiflux  Slack community      Docs     connectToStores   FluxibleComponent   FluxibleMixin   legacy    provideContext     License   This software is free to use under the Yahoo Inc  BSD license  See the  LICENSE file  for license text and copyright information    React PropTypes validation outside of React      js import check from  react prop types check     const type         a  React PropTypes array isRequired      b  React PropTypes string      c  React PropTypes shape           d  React PropTypes number          e  React PropTypes bool isRequired             check       a  5      b   abc       c   d  1     type        Invalid prop  a  of type  number  supplied to    a  5  b   abc   c    d  1     expected  array      Required prop  e  was not specified in    a  5  b   abc   c    d  1               esfmt formats  beautifies  pretty prints  javascript  es6  jsx  code    Install   npm install  g esfmt   Example   js    echo  if  a   b    4  return  e       return a b c  Component a  5  b   1  b  0    br     Component     else   return  a  5  b  8  c  9         bin esfmt if   a   b    4        return  e               return a b c  Component a  5  b   1  b  0                 br              Component             else       return           a  5          b  8          c  9            Development   Testing   Most of the testing is done by formatting code snippets and matching resulting output with expected string value    Code snippet files are located at  test code snippets   js   The DSL has the following format   js    input  test case description  e g  variable declaration  let a                  5     output  let a   5    this will produce a test case that will look like this   code snippets       code snippets filename  test case description  e g  variable declaration    if any specific test needs to be whitelisted  mocha it only  or blacklisted  mocha it skip  skip or only line can be added after the test description  js    input  arrays    skip  1   a   null      output   1   a   null     or   js    input  arrays    only  1   a   null      output   1   a   null     in addition to that  if any specific configuration values need to be used for the test  they can be passed using a config line     config    settingName    value     Note that the values should be provided in JSON format  meaning double quotes around the keys   js    input  arrays    only    config    max len   5   1   a   null      output   1   a   null     TODO     CLI   comments in   class definitions   arguments  lists        itretry   Retry individual mocha tests  Useful for e2e tests on sketchy connections       js import itretry from  itretry     it retry   itretry    let i   0    describe  something   function         it retry 5   retries until it succeeds   function             i            expect i  to equal 5                    something           retries the thing 5 times       works with async tests too      js it retry 5   tests something   function         return Promise resolve          it retry 9999   async with done   function done        set timeout                 expect Math random    to be below  1           done           100           atom esfmt   Format your JavaScript using ES6  using  esfmt  in atom  FFI lib in node to parse a json object      js var ffi   require  ffi    var path   require  path      var lib   ffi Library path join   dirname     target release libparsejson        json    string              var start    new Date    var json   lib json    console log  Parsing in rust     new Date     start   ms      start    new Date    require    data json    console log  Parsing in node     new Date     start   ms            p parse json in rust    cargo build   release    node index js Parsing in rust  32883 ms Parsing in node  5805 ms   Jest       Painless JavaScript Unit Testing       Adaptable   Jest uses Jasmine assertions by default and Jest is modular  extendible and configurable        Sandboxed and Fast   Jest virtualizes JavaScript environments  provides browser mocks and runs tests in parallel across workers        Mock by Default   Jest  automatically mocks  JavaScript  modules   making most existing code testable        Getting Started    First install Jest with npm by running    npm install   save dev jest cli   Great  Now let s get started by writing a test for a hypothetical  sum js  file    javascript function sum a  b      return a   b    module exports   sum    Create a directory    tests     with a file  sum test js        javascript jest unmock     sum       unmock to use the actual implementation of sum   describe  sum             it  adds 1   2 to equal 3               const sum   require     sum        expect sum 1  2   toBe 3                   Add the following to your  package json     js  scripts        test    jest      Run  npm test      PASS    tests   sum test js  0 010s    The code for this example is available at  examples getting started     And you are ready to enjoy working with Jest    Babel Integration   If you d like to use  Babel   it can easily be enabled    npm install   save dev babel jest babel polyfill   Don t forget to add a   babelrc  file in your project s root folder  For example  if you are using ES2015 and  React js  with the  babel preset es2015  and  babel preset react  presets    js      presets     es2015    react       You are now set up to use all ES2015 features and React specific syntax  for example       js jest unmock     CheckboxWithLabel      import React from  react   import ReactDOM from  react dom   import TestUtils from  react addons test utils   import CheckboxWithLabel from     CheckboxWithLabel     describe  CheckboxWithLabel             it  changes the text after click                  Render a checkbox with label in the document     const checkbox   TestUtils renderIntoDocument                   const checkboxNode   ReactDOM findDOMNode checkbox       Verify that it s Off by default expect checkboxNode textContent  toEqual  Off                             Check out the  React tutorial  for more    And you are good to go   The next time you run Jest it will print something like   Using Jest CLI v version   jasmine2  babel jest   The  React    Relay  and  react native  repositories have excellent examples of tests written by Facebook engineers    Advanced Features   Only run test files related to changes with  jest  o   On large projects and applications it is often not feasible to run thousands of tests when a single file changes  Jest uses static analysis to look up dependency trees in reverse starting from changed JavaScript files only  During development  it is recommended to use  jest  o  or  jest   onlyChanged  which will find tests related to changed JavaScript files and only run relevant tests    Install Jest globally   Jest can be installed globally   npm install  g jest cli  which will make a global  jest  command available that can be invoked from anywhere within your project    Async testing   Promises and even async await can be tested easily    Assume a  user getUserName  function that returns a promise  now consider this async test with Babel and  babel plugin transform async to generator  or  babel preset stage 3        js jest unmock     user      import   as user from     user     describe  async tests                The promise that is being tested should be returned    it  works with promises               return user getUserName 5         then name    expect name  toEqual  Paul             it  works with async await   async             const userName   await user getUserName 4       expect userName  toEqual  Mark                    Check out the  Async tutorial  for more    Automated Mocking and Sandboxing   Jest isolates test files into their own environment and isolates module execution between test runs  Jest swaps out  require    to inject mocks that were either  created manually  by the user or  automatic mocks  through the automocking feature    Use the    watch  option to automatically re run tests   Jest can automatically re run tests when files change    jest   watch   Use    bail  to abort after the first failed test    If you don t want to wait until a full test run completes    bail  can be used to abort the test run after the first error    Use    coverage  to generate a code coverage report   Code coverage can be generated easily with    coverage                                                                          File                        Stmts     Branch      Funcs      Lines                                                                         react                       91 3      60 61        100        100     CheckboxWithLabel js       91 3      60 61        100        100                                                                          Use    json  for CI integrations   Jest can be integrated into Continuous Integration test runs and wrapped with other scripts to further analyze test results    Example Output    js      success   true     startTime   1456983486661     numTotalTests   1     numTotalTestSuites   1     numRuntimeErrorTestSuites   0     numPassedTests   1     numFailedTests   0     numPendingTests   0     testResults                 name    react   tests   CheckboxWithLabel test js          status    passed          startTime   1456983488908         endTime   1456983493037               Remote debugging with Web Inspector   Web Inspector can be used to debug both the application code and the testing code  To set this up  first verify that you have both  node inspector  and  node debug  installed    which node inspector which node debug   Install packages that you are missing    npm install  g node inspector node debug   Run  node debug  with the Jest binary as the target    node debug node modules jest cli bin jest js  i   You should see that Node Inspector has been started in the prompt       Node Inspector Visit http   127 0 0 1 8080  port 5858 to start debugging  Debugging node modules jest cli bin jest js    Debugger listening on port 5858       A new Chrome window should open and subsequently open a page to localhost on port 5858  If this does not happen  you can go to the debugging URL directly in a Chrome browser tab  The program should stop in the  jest js  file in the first line of code  Press the play button in the inspector window to continue execution    To stop the code at an arbitrary point  set a  debugger  expression in the code where you would like the program to stop  Stop   ctrl c   and restart  node debug  in the command line to catch at the debugger expression you just added     API     The  jest  object     jest clearAllTimers     jest disableAutomock     jest enableAutomock     jest fn  implementation    jest isMockFunction fn    jest genMockFromModule moduleName    jest mock moduleName   factory    jest runAllTicks     jest runAllTimers     jest runOnlyPendingTimers     jest setMock moduleName  moduleExports    jest unmock moduleName    jest useFakeTimers     jest useRealTimers       Mock functions   Mock functions can be created using  jest fn         mockFn mock calls   mockFn mock instances   mockFn mockClear     mockFn mockImplementation fn    mockFn mockReturnThis     mockFn mockReturnValue value    mockFn mockReturnValueOnce value      Jasmine API   Jest uses Jasmine 2 by default  An introduction to Jasmine 2 can be found  here     require extensions     require requireActual moduleName    require requireMock moduleName      Configuration Options     automock   boolean    browser   boolean    bail   boolean    cacheDirectory   string    coverageDirectory   string    collectCoverage   boolean    collectCoverageOnlyFrom   object    coveragePathIgnorePattern   array     coverageThreshold   object    globals   object    mocksPattern   string    moduleDirectories   array     moduleFileExtensions   array     moduleNameMapper   object     modulePaths   array     modulePathIgnorePatterns   array     notify   boolean    preprocessorIgnorePatterns   array     rootDir   string    scriptPreprocessor   string    setupFiles   array    setupTestFrameworkScriptFile   string    testEnvironment   string    testPathDirs   array     testPathIgnorePatterns   array     testPathPattern   string    testRegex   string    testResultsProcessor   string    testRunner   string    unmockedModulePathPatterns   array     verbose   boolean      Globally injected variables     afterEach fn    beforeEach fn    check   describe name  fn    expect value    gen   it name  fn    fit name  fn   executes only this test  Useful when investigating a failure   jest   require module    require requireActual module    xdescribe name  fn    xit name  fn      expect value       not  inverse the next comparison    toThrow  message     toBe value   comparison using         toEqual value   deep comparison  Use  jasmine any type   to be softer    toBeFalsy      toBeTruthy      toBeNull      toBeUndefined      toBeDefined      toMatch regexp     toContain string     toBeCloseTo number  delta     toBeGreaterThan number     toBeLessThan number     toBeCalled      toBeCalledWith arg1  arg2          lastCalledWith arg1  arg2             Jest API   jest clearAllTimers     Removes any pending timers from the timer system    This means  if any timers have been scheduled  but have not yet executed   they will be cleared and will never have the opportunity to execute in the future    jest disableAutomock     Disables automatic mocking in the module loader    After this method is called  all  require   s will return the real versions of each module  rather than a mocked version     This is usually useful when you have a scenario where the number of dependencies you want to mock is far less than the number of dependencies that you don t  For example  if you re writing a test for a module that uses a large number of dependencies that can be reasonably classified as  implementation details  of the module  then you likely do not want to mock them    Examples of dependencies that might be considered  implementation details  are things ranging from language built ins  e g  Array prototype methods  to highly common utility methods  e g  underscore lo dash  array utilities etc  and entire libraries like React js    Note  this method was previously called  autoMockOff   When using  babel jest   calls to  disableAutomock  will automatically be hoisted to the top of the code block  Use  autoMockOff  if you want to explicitly avoid this behavior    jest enableAutomock     Re enables automatic mocking in the module loader    Note  this method was previously called  autoMockOn   When using  babel jest   calls to  enableAutomock  will automatically be hoisted to the top of the code block  Use  autoMockOn  if you want to explicitly avoid this behavior    jest fn  implementation    Returns a new  unused  mock function   Optionally takes a mock implementation       js   const mockFn   jest fn      mockFn      expect mockFn  toBeCalled         With a mock implementation    const returnsTrue   jest fn       true     console log returnsTrue       true        jest isMockFunction fn    Determines if the given function is a mocked function    jest genMockFromModule moduleName    Given the name of a module  use the automatic mocking system to generate a mocked version of the module for you    This is useful when you want to create a  manual mock  that extends the automatic mock s behavior    jest mock moduleName   factory    Indicates that the module system should always return a mocked version of the specified module from  require     e g  that it should never return the real module        js   jest mock  moduleName      const moduleName   require  moduleName       moduleName will be explicitly mocked       The second argument can be used to specify an explicit module factory that is being run instead of using Jest s automocking feature       js   jest mock  moduleName               return jest fn       42           const moduleName   require  moduleName       This runs the function specified as second argument to  jest mock     moduleName       Will return  42         Note  When using  babel jest   calls to  mock  will automatically be hoisted to the top of the code block  Use  doMock  if you want to explicitly avoid this behavior    jest runAllTicks     Exhausts the  micro  task queue  usually interfaced in node via  process nextTick      When this API is called  all pending micro tasks that have been queued via  process nextTick  will be executed  Additionally  if those micro tasks themselves schedule new micro tasks  those will be continually exhausted until there are no more micro tasks remaining in the queue    jest runAllTimers     Exhausts the  macro  task queue  i e   all tasks queued by  setTimeout    and  setInterval        When this API is called  all pending  macro tasks  that have been queued via  setTimeout    or  setInterval    will be executed  Additionally if those macro tasks themselves schedule new macro tasks  those will be continually exhausted until there are no more macro tasks remaining in the queue    This is often useful for synchronously executing setTimeouts during a test in order to synchronously assert about some behavior that would only happen after the  setTimeout    or  setInterval    callbacks executed  See the  Timer mocks  doc for more information    jest runOnlyPendingTimers     Executes only the macro tasks that are currently pending  i e   only the tasks that have been queued by  setTimeout    or  setInterval    up to this point   If any of the currently pending macro tasks schedule new macro tasks  those new tasks will not be executed by this call    This is useful for scenarios such as one where the module being tested schedules a  setTimeout    whose callback schedules another  setTimeout    recursively  meaning the scheduling never stops   In these scenarios  it s useful to be able to run forward in time by a single step at a time    jest setMock moduleName  moduleExports    Explicitly supplies the mock object that the module system should return for the specified module    On occasion there are times where the automatically generated mock the module system would normally provide you isn t adequate enough for your testing needs  Normally under those circumstances you should write a  manual mock  that is more adequate for the module in question  However  on extremely rare occasions  even a manual mock isn t suitable for your purposes and you need to build the mock yourself inside your test    In these rare scenarios you can use this API to manually fill the slot in the module system s mock module registry    Note It is recommended to use  jest mock    instead  The  jest mock  API s second argument is a module factory instead of the expected exported module object    jest unmock moduleName    Indicates that the module system should never return a mocked version of the specified module from  require     e g  that it should always return the real module     The most common use of this API is for specifying the module a given test intends to be testing  and thus doesn t want automatically mocked     Note  this method was previously called  dontMock   When using  babel jest   calls to  unmock  will automatically be hoisted to the top of the code block  Use  dontMock  if you want to explicitly avoid this behavior    jest useFakeTimers     Instructs Jest to use fake versions of the standard timer functions   setTimeout    setInterval    clearTimeout    clearInterval    nextTick    setImmediate  and  clearImmediate    Fake timers are used by default    jest useRealTimers     Instructs Jest to use the real versions of the standard timer functions    Mock API   mockFn mock calls   An array that represents all calls that have been made into this mock function  Each call is represented by an array of arguments that were passed during the call    For example  A mock function  f  that has been called twice  with the arguments  f  arg1    arg2     and then with the arguments  f  arg3    arg4    would have a  mock calls  array that looks like this    js       arg1    arg2        arg3    arg4       mockFn mock instances   An array that contains all the object instances that have been instantiated from this mock function    For example  A mock function that has been instantiated twice would have the following  mock instances  array       js var mockFn   jest fn      var a   new mockFn    var b   new mockFn      mockFn mock instances 0      a     true mockFn mock instances 1      b     true       mockFn mockClear     Resets all information stored in the  mockFn mock calls  and  mockFn mock instances  arrays    Often this is useful when you want to clean up a mock s usage data between two assertions    mockFn mockImplementation fn    Accepts a function that should be used as the implementation of the mock  The mock itself will still record all calls that go into and instances that come from itself   the only difference is that the implementation will also be executed when the mock is called    Note   jest fn implementation   is a shorthand for  mockImplementation     For example       js const mockFn   jest fn   mockImplementation scalar    42   scalar      or  jest fn scalar    42   scalar     const a   mockFn 0   const b   mockFn 1     a     42     true b     43     true   mockFn mock calls 0  0      0     true mockFn mock calls 1  0      1     true       mockImplementation  can also be used to mock class constructors           SomeClass js module exports   class SomeClass     m a  b            OtherModule test js const SomeClass   require  SomeClass   const mMock   jest fn   SomeClass mockImplementation           return       m  mMock          const some   new SomeClass   some m  a    b   console log  Calls to m     mMock mock calls        mockFn mockReturnThis     Just a simple sugar function for    js jest fn function       return this        mockFn mockReturnValue value    Deprecated  Use  jest fn       value   instead       js const mockNumberFn   jest fn       42   mockNumberFn       42      Deprecated behavior  jest genMockFunction   mockImplementation       value         mockFn mockReturnValueOnce value    Just a simple sugar function for    js const valueReturned   false  jest fn           if   valueReturned        valueReturned   true      return value            require  Extensions   require requireActual moduleName    Returns the actual module instead of a mock  bypassing all checks on whether the module should receive a mock implementation or not    require requireMock moduleName    Returns a mock module instead of the actual module  bypassing all checks on whether the module should be required normally or not    Property testing   Jest supports property testing with the  testcheck js  library  The API is the same as that of  jasmine check     check it name   options   generators  fn    Creates a property test  Test cases will be created by the given  generators  and passed as arguments to  fn   If any test case fails  a shrunken failing value will be given in the test output  For example    js check it  can recover encoded URIs      gen string     s    expect s  toBe decodeURI encodeURI s        If  options  are provided  they override the corresponding command line options  The possible options are        times  number       The number of test cases to run  Default  100    maxSize  number     The maximum size of sized data such as numbers                        their magnitude  or arrays  their length   This can be                       overridden with  gen resize   Default  200    seed  number        The random number seed  Defaults to a random value      check fit name   options   generators  fn    Executes this test and skips all others  Like  fit   but for property tests    check xit name   options   generators  fn    Skips this test  Like  xit   but for property tests    gen   A library of generators for property tests  See the  testcheck  documentation     Configuration   Jest s configuration can be defined in the  package json  file of your project or through the    config  path to json   option  If you d like to use your  package json  to store Jest s config  the  jest  key should be used on the top level so Jest will know how to find your settings    js      name    my project      jest          verbose   true         When using the   config option  the JSON file must not contain a  jest  key    js      bail   true     verbose   true     automock   boolean     default  true    By default  Jest mocks every module automatically  If you are building a small JavaScript library and would like to use Jest  you may not want to use automocking  You can disable this option and create manual mocks or explicitly mock modules using  jest mock moduleName      browser   boolean     default  false    Respect the Browserify s   browser   field in  package json  when resolving modules  Some modules export different versions based on whether they are operating in Node or a browser    bail   boolean     default  false    By default  Jest runs all tests and produces all errors into the console upon completion  The bail config option can be used here to have Jest stop running tests after the first failure    cacheDirectory   string     default    tmp       The directory where Jest should store its cached dependency information    Jest attempts to scan your dependency tree once  up front  and cache it in order to ease some of the filesystem raking that needs to happen while running tests  This config option lets you customize where Jest stores that cache data on disk    coverageDirectory   string     default   undefined     The directory where Jest should output its coverage files    collectCoverage   boolean     default   false     Indicates whether the coverage information should be collected while executing the test  Because this retrofits all executed files with coverage collection statements  it may significantly slow down your tests    collectCoverageOnlyFrom   object     default   undefined     An object that  when present  indicates a set of files for which coverage information should be collected  Any files not present in this set will not have coverage collected for them  Since there is a performance cost for each file that we collect coverage information from  this can help prune this cost down to only the files in which you care about coverage  such as the specific modules that you are testing     coveragePathIgnorePattern   array      default      node modules        An array of regexp pattern strings that are matched against all file paths before executing the test  If the file path matches any of the patterns  coverage information will be skipped    These pattern strings match against the full path  Use the   rootDir   string token to  include the path to your project s root directory to prevent it from accidentally ignoring all of your files in different environments that may have different root directories  Example      rootDir  build      rootDir  node modules        coverageThreshold   object     default   undefined     This will be used to configure minimum threshold enforcement for coverage results  If the thresholds are not met  jest will return failure  Thresholds  when specified as a positive number are taken to be the minimum percentage required  When a threshold is specified as a negative number it represents the maximum number of uncovered entities allowed    For example  statements  90 implies minimum statement coverage is 90   statements   10 implies that no more than 10 uncovered statements are allowed    js            jest          coverageThreshold            global              branches   50           functions   50           lines   50           statements   50                       globals   object     default          A set of global variables that need to be available in all test environments    For example  the following would create a global    DEV    variable set to  true  in all test environments    js            jest          globals              DEV     true               Note that  if you specify a global reference value  like an object or array  here  and some code mutates that value in the midst of running a test  that mutation will  not  be persisted across test runs for other test files    mocksPattern   string     default                mocks            A pattern that is matched against file paths to determine which folder contains manual mocks    moduleFileExtensions   array      default     js    json    node       An array of file extensions your modules use  If you require modules without specifying a file extension  these are the extensions Jest will look for    If you are using TypeScript this should be    js    json    ts     modulePathIgnorePatterns   array      default          An array of regexp pattern strings that are matched against all module paths before those paths are to be considered  visible  to the module loader  If a given module s path matches any of the patterns  it will not be  require    able in the test environment    These pattern strings match against the full path  Use the   rootDir   string token to  include the path to your project s root directory to prevent it from accidentally ignoring all of your files in different environments that may have different root directories  Example      rootDir  build        modulePaths   array      default          An alternative API to setting the  NODE PATH  env variable   modulePaths  is an array of absolute paths to additional locations to search when resolving modules    moduleDirectories   array      default     node modules       An array of directory names to be searched recursively up from the requiring module s location  Setting this option will  override  the default  if you wish to still search  node modules  for packages include it along with any other options     node modules    bower components     moduleNameMapper   object      default   null     A map from regular expressions to module names that allow to stub out resources  like images or styles with a single module    Use   rootDir   string token to refer to  rootDir  value if you want to use file paths    Additionally  you can substitute captured regex groups using numbered backreferences    Example   js    moduleNameMapper           image  a zA Z0 9          GlobalImageStub            a zA Z0 9       png      rootDir  RelativeImageStub js        module name          rootDir  substituted module  1 js        notify   boolean     default   false     Activates notifications for test results    rootDir   string     default  The root of the directory containing the  package json   or  the  pwd  if no  package json  is found    The root directory that Jest should scan for tests and modules within  If you put your Jest config inside your  package json  and want the root directory to be the root of your repo  the value for this config param will default to the directory of the  package json     Oftentimes  you ll want to set this to   src   or   lib    corresponding to where in your repository the code is stored    Note that using    rootDir    as a string token in any other path based config settings to refer back to this value  So  for example  if you want your  setupFiles  config entry to point at the  env setup js  file at the root of your project  you could set its value to     rootDir  env setup js       scriptPreprocessor   string     default   undefined     The path to a module that provides a synchronous function from pre processing source files  For example  if you wanted to be able to use a new language feature in your modules or tests that isn t yet supported by node  like  for example  ES6 classes   you might plug in one of many transpilers that compile ES6 to ES5 here    Examples of such compilers include  jstransform    recast    regenerator   and  traceur     Note  Jest s preprocessor is only ran once per file unless the file has changed  During development of a  scriptPreprocessor  it can be useful to run Jest with    no cache  or to frequently  delete Jest s cache     preprocessorIgnorePatterns   array      default      node modules        An array of regexp pattern strings that are matched against all source file paths before preprocessing  If the test path matches any of the patterns  it will not be preprocessed    These pattern strings match against the full path  Use the   rootDir   string token to  include the path to your project s root directory to prevent it from accidentally ignoring all of your files in different environments that may have different root directories  Example      rootDir  bower components      rootDir  node modules        Note  if this option is not specified by the user and Jest detects the project as a  react native  project  it will ignore the default and process every file  It is common on react native projects to ship npm modules without pre compiling JavaScript    setupFiles   array     default          The paths to modules that run some code to configure or set up the testing environment before each test  Since every test runs in its own environment  these scripts will be executed in the testing environment immediately before executing the test code itself    It s worth noting that this code will execute  before   setupTestFrameworkScriptFile     setupTestFrameworkScriptFile   string     default   undefined     The path to a module that runs some code to configure or set up the testing framework before each test  Since  setupFiles  executes before the test framework is installed in the environment  this script file presents you the opportunity of running some code immediately after the test framework has been installed in the environment    For example  Jest ships with several plug ins to  jasmine  that work by monkey patching the jasmine API  If you wanted to add even more jasmine plugins to the mix  or if you wanted some custom  project wide matchers for example   you could do so in this module    testEnvironment   string     default    jsdom      The test environment that will be used for testing  The default environment in Jest is a browser like environment through  jsdom   If you are building a node service  you can use the  node  option to use a node like environment instead    testPathDirs   array      default      rootDir        A list of paths to directories that Jest should use to search for tests in    There are times where you only want Jest to search in a single sub directory  such as cases where you have a  src   directory in your repo   but not the rest of the repo    testPathIgnorePatterns   array      default      node modules        An array of regexp pattern strings that are matched against all test paths before executing the test  If the test path matches any of the patterns  it will be skipped    These pattern strings match against the full path  Use the   rootDir   string token to  include the path to your project s root directory to prevent it from accidentally ignoring all of your files in different environments that may have different root directories  Example      rootDir  build      rootDir  node modules        testPathPattern   string     default            See notes below for more details on the default setting    A regexp pattern string that is matched against all test paths before executing the test  If the test path does not match the pattern  it will be skipped    This is useful if you need to override the default  If you are testing one file at a time the default will be set to         however if you pass a blob rather than a single file the default will then be the absolute path of each test file  The override may be needed on windows machines where  for example  the test full path would be  C  myproject   tests   mystest jsx jest  and the default pattern would be set as   C  myproject   tests   mystest jsx jest      testRegex   string     default      tests       js       The pattern Jest uses to detect test files  By default it looks for   js  files inside of    tests    folders    testResultsProcessor   string     default   undefined     This option allows the use of a custom results processor  This processor must be a node module that exports a function expecting an object with the following structure as the first argument         success   bool     startTime   epoch     numTotalTestSuites   number     numPassedTestSuites   number     numFailedTestSuites   number     numRuntimeErrorTestSuites   number     numTotalTests   number     numPassedTests   number     numFailedTests   number     numPendingTests   number     testResults           numFailingTests   number       numPassingTests   number       numPendingTests   number       testResults             title   string  message in it block          status    failed     pending     passed          ancestorTitles    string  message in describe blocks           failureMessages    string          numPassingAsserts   number                            perfStats            start   epoch         end   epoch             testFilePath   absolute path to test file       coverage                         testRunner   string     default   jasmine2     This option allows use of a custom test runner  The default is jasmine2  Jest also ships with jasmine1 which can enabled by setting this option to  jasmine1   A custom test runner can be provided by specifying a path to a test runner implementation    unmockedModulePathPatterns   array      default          An array of regexp pattern strings that are matched against all modules before the module loader will automatically return a mock for them  If a module s path matches any of the patterns in this list  it will not be automatically mocked by the module loader    This is useful for some commonly used  utility  modules that are almost always used as implementation details almost all the time  like underscore lo dash  etc   It s generally a best practice to keep this list as small as possible and always use explicit  jest mock     jest unmock    calls in individual tests  Explicit per test setup is far easier for other readers of the test to reason about the environment the test will run in    It is possible to override this setting in individual tests by explicitly calling  jest mock    at the top of the test file    verbose   boolean     default   false     Indicates whether each individual test should be reported during the run  All errors will also still be shown on the bottom after execution   Jest Cheat Sheet   Test structure   Basic tests      js test  something         expect something  toBe true    test  something else         expect somethingElse  toBe true         use describe to group tests together     NOTE  Don t use  describe  if you have only one top level group  describe  group tests together                use  test  to describe things   test  something         expect something  toBe true         use  it  to describe actions   it  does something         expect something  toBe true              before after hooks     js beforeEach function          this  is shared between all tests and their hooks  and gets reset automatically after a test is finished executing   this state   longComplicatedSetup          test  one   function       doSomething this state       this state  is recreated for each test   expect this state  toEqual something         test  two   function       doSomething this state     expect this state  toEqual something             skipping focusing      js test only  will run only this test      test skip  this test will be skipped      describe only  will run only tests inside this block                              Matchers   http   facebook github io jest docs api html writing assertions with expect    toBe value    js    will pass expect 1  toBe 1   expect  a   toBe  a       will fail expect  a  1   toBe  a  1    expect 1  not toBe 1      toEqual value     js    will pass expect  a  1   toEqual  a  1    expect 1  toEqual 1      will fail expect  a  1   toEqual  a  2       toBeInstanceOf Class    js class A       will pass expect new A  toBeInstanceOf A      will fail expect  aaa   toBeInstanceOf Number      toContain item   toContainEqual item       js    will pass expect  1  2  3   toContain 3   expect   a  1    a  2    toContainEqual  a  1       will fail expect   a  1    a  2    toContain  a  1             toThrowError     js    will pass expect         throw new Error  lol      toThrowError  lol       toMatchSnapshot     js    Works on everything that can be serialized     If different value is passed in the next run  the matcher will fail  expect  1  2  3   toMatchSnapshot       toThrowErrorMatchingSnapshot     js    Will create a snapshot containing error class and message expect         throw new Error  lol      toThrowErrorMatchingSnapshot      Other matchers    toBeFalsy    toBeGreaterThan number   toBeGreaterThanOrEqual number   toBeLessThan number   toBeLessThanOrEqual number   toBeNull    toBeTruthy    toEqual value   toMatch regexp  Haste Map   Haste Map parsing in Rust    Scripts     yarn run build artifacts  path     Make both the node and rust versions generate a deterministic version of their Haste Map  Useful for asserting parity    yarn run build release    Build the rust binary      A simple webapp that calculates guitar bass string tension for different variations of scale gauge tunnings egghead io creator MDX Blog Starter Project   This is based on Robin Wieruch s https   github com rwieruch gatsby mdx blog starter project   Lots of nice pieces are also borrowed from Jason Lengstorf https   github com jlengstorf lengstorf com   A starter project in  Gatsby js  with  MDX       Features     MDX  JavaScript React in Markdown   Prism js  Syntax Highlighting   Pagination   Emotion   Typography js   Self hosted fonts   Inter UI     Social media share buttons   Site   Theme config files   ConvertKit subscribe form  Formik and Yup    Placeholder illustrations by  Katerina Limpitsouni  from  undraw co       Demo   Setup     git clone git github com eggheadio gatsby starter egghead blog git   cd gatsby starter egghead blog   yarn   gatsby develop   visit http   localhost 8000     Setup via Gatsby CLI     gatsby new gatsby starter egghead blog git github com eggheadio gatsby starter egghead blog git   cd gatsby starter egghead blog   yarn   gatsby develop   visit http   localhost 8000   LL   Rust Logging Library         K9   Rust Testing Library           Snapshot testing   better assertions   Available test macros     snapshot   assert equal   assert greater than   assert greater than or equal   assert lesser than   assert lesser than or equal   assert matches regex   assert err matches regex   assert matches snapshot   assert matches inline snapshot   assert ok   assert err     See  https   docs rs k9  for API documentation   snapshot     macro   Snapshot macro provides the functionality to capture the  Debug  representation of any value and make sure it does not change over time     If it does change  the test will fail and print the difference between  old  and  new  values    If the change is expected and valid  running  cargo test  with  K9 UPDATE SNAPSHOTS 1  env variable set will automatically take the new value and insert it into the test source code file as a second argument  after which all subsequent test runs should start passing again      assert equal     macro   Rust already provides a good built in test runner and a set of assertion macros like  assert   and  assert eq    They work great for for quick unit tests  but once the codebase and test suites grows to a certain point it gets harder and harder to test things and keep tests readable    For example  when testing that two structs are equal using  assert eq   macro the output does not provide a lot of help in understanding why exactly this test failed       rust    derive PartialEq  Debug     struct Person       name    static str      age  usize       test    fn test eq         let person1   Person  name   Bob   age  12        let person2   Person  name   Alice   age  20        assert eq  person1  person2   These two must be the same person             All we get is usually a wall of text collapsed into a single line and you have to find the difference between two structs yourself  Which becomes very time consuming when structs are 10  fields         eq  test eq stdout      thread  eq  test eq  panicked at  assertion failed    left    right     left   Person   name   Bob   age  12      right   Person   name   Alice   age  20     These two must be the same person    src eq rs 13 5   using  k9  assert equal  macro improves this output and prints the difference between two structs    rust use k9  assert equal  assert equal  person1  person2   These two must be the same person         Non equality based assertions   Testing equality is very simple and can definitely work for most of the cases  but one of the disadvantages of only using  assert   and  assert eq   is the error messages when something fails  For example  if you re testing that your code produces valid URL   rust let url   generate some url    assert eq  URL REGEX is match url   true     What you get is   thread  eq  test eq3  panicked at  assertion failed    left    right     left   false    right   true    src eq rs 19 5   Which doesn t help much  Especially  if you re new to the code base  seeing things like  expected  true  but got  false   will make you go and look at the code before you even know what the problem can be  which can be very time consuming    What we probably want to see is      Which gives us enough context on what the problem is and how to fix it without for us having to go and run debug the test first  interview questions   npm test eslint plugin fb www   this is just a stub for internal facebook eslint rules that resolve into  fb www  plugin  It is here to make sure eslint does not blow up when it sees unresolved eslint rules like  eslint disable next line fb www react hooks Pavel s personal blog at https   ptimofeev com gull   Static types definitions code generator
82,eobodo,Configrouter   Flexible router for quickly building testing web services  Powered by  route parser     Installation     npm install configrouter     Usage   First import the module into your code    js let ConfigRouter   require  configrouter      Then define your routes following this simple schema        handler   Function req  res  params    Object     String httpMethod1  Object       String  route1  handler     optional     String  route2  handler     optional     String  noMatch  handler    required              Example       js let myHandler    req  res  params         if  params number      42         res end  Hello World             let routes        GET              myHandler         number   myHandler        noMatch    req  res               Handle request                POST           noMatch    req  res               Handle request                   Routes resolve in insertion order  In the case of duplicate entries  the latter route wins    Best practice is to define your routes handlers in a separate file  then import it into your server       js   const ConfigRouter   require  configrouter      const routes   require  path to routes js     let router   new ConfigRouter routes     let server   http createServer  req  res           router route req  res             node reload   Configuration based file watcher reloader for node js    javascript npm install  g node reload   Usage   Here s a sample configuration file  Current convention is to name it  reload config js        javascript let config        entry    test main js      args          source    path to somewhere js           watch         js      test   js       log   true       module exports   config        Load exported configuration    javascript node reload  path to reload config js    API   javascript       This file will reload on change    entry   String       Object will be convert into an argument string    args   Object  Any       Array of files to watch    watch   Array  String       Log events to console    log   Boolean
84,aniljava,smtp2json   Inbound SMTP Server   Forwards parsed JSON over HTTP   Overview   smtp2json is stand alone service written in java  It listens on port 25 as a standard SMTP Server  Once there is an emai  it checks the recepient s domain with the domain list in domains list  1  configured  If the domain exists in the list  The incoming email is converted to JSON format from MIME format  The result data is sent to the a url  2  as a post data    It is a simple form of  http   postmarkapp com inbound   Features     Standalone   TLS Support   Email parsing to extract txt plain  txt html and binary attachments   Multiple chained processors  backup    postjson           Domain based or Wildcard filtering   Per domain Rule     Installation   java  version   should be 1 7      Create a working directory   cd  opt smtp2jspn    cd  opt smtp2json     Download binary file with dependencies   wget https   github com downloads aniljava smtp2json smtp2json 1 0 jar with dependencies jar     Download sample configuration file  and change settings  Downloaded files have documentation on each option    wget https   raw github com aniljava smtp2json master config properties      Create atleast one configuration file   mkdir config   echo    domain   domainname    processors     postjson    https     remoteurl         config domainname json     Run server as a background job      java  jar smtp2json 1 0 jar with dependencies jar      Configuration   There are two files that needs to be configured        config properties It configures the actual server s information such as port numbers  ssl support  See config properties for the details        processor config files Configures the email processing rules  These rules are defined as a json files inside config folder for each domain  Example of a processor config file           domain   reddit com        processors     logger      backup    backupdir      postjson   https     url    true                         Processors     logger  Logs each reques   backup  creates the backup of each email at given directory  argument backupdir required    postjson   Creates json and forwards to given url      See smtp2json processors package for details    JSON Format      text   TEST BODY    html    font  font face   trebuchet ms sans serif   TEST BODY  font   font     From   Example Last Name  example exampledomain com     To   example Last Name  example exampledomain com         Subject   TEST EMAIL WITH ATTACHMENT    attachments          data    9j 4AAQSkZJRgABAQEAYA   REST OF THE CONTENT IN BASE 64    UUUUUV  9k         filename   image002 jpg        Content Type   image jpeg        MIME Version   1 0    Message ID    long gmail internal id mail gmail com     partsCount  2   body0         Content Transfer Encoding   base64        Content Disposition   attachment  filename   image002 jpg          X Attachment Id   f h3sdw3ux0        Content Type   image jpeg  name   image002 jpg        Delivered To   example exampledomain com    Date   Sat  23 Jun 2012 02 44 25  0500    Received    by 10 205 114 141 with HTTP  Sat  23 Jun 2012 00 44 25  0700  PDT      Content Type   multipart mixed  boundary bcaec517c662fe165004c31eea3f        Following fields are added   text  if text plain part exists or if text plain does not exist and text   exists    html  if text html part exists    attachments   Headers  To  From  Subject are copied as they are  Nested body s headers are copied in nexted body and named body0  body1       TODO     Auto Reload Configuration Files   Revise JSON Format for next version      CHANGES     Added raw support   mgsv ws server   1 0   SOAP Web Service for  mGSV  and  GSV   It is standalone java application  that can be run alongside mGSV GSV or in seperate machine  Allows clients to upload files and urls programmatically  A client reference implementation can be found at  mgsv ws client     Web Service exports two methods    uploadURL String syntenyUrl  String annotationUrl  String email  String  uploadData String syntenyData  String annotationData  String email  String    WSDL can be obtained from     http   cas bioinfo cas unt edu 8081 MGSVService wsdl    Dependencies     JDK 1 6    Maven 3 0   Source Build      Running   Download or produce binary using mvn package  Inside the directory where you have binary jar      shell  echo  mgsv upload url http    YOUR SERVER DOMAIN NAME  mgsv    config properties shell  echo  ws publish url http     YOUR SERVER DOMAIN NAME   8081 MGSVService     config properties shell  java  jar ws server 1 0RC1 jar with dependencies jar    Running for GSV       Server config properties   mgsv upload url http    cas bioinfo cas unt edu gsv homepage php ws publish url http    97 94 192 248  8082 GSVService        Client config properties   remote http    97 94 192 248  8082 GSVService      mgsv ws client 1 0 RC3   Client application for mGSV Web Service  This application act as a guide for the mGSV Web Service as well as a fully functional file upload tool  Application is implemented in Java  and requires JVM or JDK 1 6  to run    Overview   Application makes use of default Web Service API that comes with standard JVM Distribution  Except for the JDK binary distribution does not require anything else to run  Client application connects to the mGSV Web Service to make upload requests and upon successful upload prints the id of the upload  The id can be used later to browse the visualization    The main purpose of this client tool is to allow biologists to upload several files and public dataset at once to mGSV server  The sources and documentation within can also be studied to implement similar clients that uses mGSV Web Service    Technical Overview   Application is divided into two parts  1  Implementation 2  Helper codes generated from the Web Service WSDL    Implementation code makes use of the generated code that provides all networking and interfacing with the remote web service  The program flow is       If config properties is provided  load it to replace the remote mGSV Server s address    If argument starts with http  call uploadURL or else call uploadData with the content of the file given in argument     The main class is  src main edu unt mgsv uploader UploadFile java   The code is documented with low level details    Installation   Binary   Requirements 1  JVM or JDK 1 6    Download prebuilt jar from    https   github com downloads aniljava mgsv ws client ws client 1 0 RC3 jar with dependencies jar  Execute the jar with arguments and configuration  See Configuration Arguments Section Below    Example     java  jar ws client 1 0 RC3 jar with dependencies jar synteny txt annotation txt sample sample com     java  jar ws client 1 0 RC3 jar with dependencies jar http   cas bioinfo cas unt edu mgsv sample synteny txt http   cas bioinfo cas unt edu mgsv sample annotation txt sample sample com   Building from Source   Requirements 1  JDK 1 5   http   java sun com  2  Maven 3 0   http   maven apache org    If  git  is present   git clone https   github com aniljava mgsv ws client git cd mgsv ws client mvn package cp target ws client 1 0 RC3 jar with dependencies jar    java  jar ws client 1 0 RC3 jar with dependencies jar  ARGUMENTS     From Source archive without git   wget https   github com downloads aniljava mgsv ws client mgsv ws client tar gz tar  xzf mgsv ws client tar gz cd mgsv ws client mvn package cp target ws client 1 0 RC3 jar with dependencies jar    java  jar ws client 1 0 RC3 jar with dependencies jar  ARGUMENTS     Configuration and Installation   Configuration   Program checks for config properties in the local folder  Remote URL for webservice can be configured using config properties  Below is the example of the content in config properties  If not present localhost 8081 is assumed    remote http    127 0 0 1  8081 MGSVService    Arguments   Program takes three arguments seperated by space  Except for first parameter other two are optional  If none is provided  a demo mode is activated that uses s gz and a gz for synteny and annotation file from current directory if they exist       synteny   synteny file  if starts with http    url is assumed   annotation   annotation file  if starts with http    url is assumed   email   email   sansj   Simple Authoritative Name Server   Java   Features     Standalone and lightweight  no dependencies except Java 1 7    JSON Based Simple DNS Configuration   Supports   A  CNAME  MX  NS  TXT Queries   Only Authoritive DNS  No forwarding   Auto reload of configuration files   Per file Zone definitions     Configuration          zone   aniljava com         data              mx    300  30    aspmx l google com     300  40   aspmx2 googlemail com               a                  aniljava com    6522  127 0 0 1                  www aniljava com    300  184 154 161 21    600  184 154 161 22                       300  184 154 161 21                        c                  www aniljava com   aniljava com                       txt                 aniljava com   300  v spf1 mx  all                 test aniljava com   300  v spf1 mx  all                                ns    163255  ns1 godaddy com     163255  ns2 godaddy com                 Running     sudo java  jar sansj 1 0RC1 jar with dependencies jar  config   reloadInterval   port     DEFAULT IS   sudo java  jar sansj 1 0RC1 jar with dependencies jar config 300000 53    Suggested Conventions     use   opt sansj   as a root folder and config as a configuration folder inside it    use a template   sed to create new zone files    DocDB   Base project for KeyValue to JSON Doc database  Provides common routines for Set  List  Counter implementation on KV Store  The project itself is not functional  instead provides common interfaces for its implementation    Implementations     docdb kch   docdb kct   docdb leveldb   docdb kt   Simple JSON   Servlet RPC   Simple Servlet  JSON and Java Reflection based RPC Mechanism  This project provides dispatcher servlet and helper api to call remote services    SERVER SIDE   At the server side of the RPC  A servlet listens to a path usually  host api   Example configuration on web xml is     servlet       servlet name api  servlet name       servlet class simplejsonrpc server RPCServlet  servlet class    servlet   servlet mapping       servlet name api  servlet name       url pattern  api  url pattern    servlet mapping      RPCServlet takes three parameters   1  service   2 method  3 arguments     service  is the full class name of the service  it needs to be in same class path as that of RPCServlet  usually deployed as a jar file in  WEB INF lib  folder  It is the only required parameter    If any class implementing INative exists  it will be initiated Only once in lifetime  and it s serve method is called    If not  if class with name passed in  service  is looked and loaded if exists  Appropriate method is looked and called according to the method and parameters values    Example Service   package example   public class Calculator      public int add int i  int j           return i j       Calculator class needs to be present in webapp classpath    CLIENT SIDE   Plain java or curl wget to uri with parameters  A Helper class  Client  is available that uses HTTP Components to make connections to remote servers    Client client   new Client  http   SERVICE URL       int result   client call  example Calculator   fn  add   invoke 2 3      Same thing can be executed as     http   SERVICE URL service example Calculator method add arguments  1 2    See  INative    simplejsonrpc example  package   for example  if add method is not found on above example  method with following signature is searched    public int add ServletContext sc  HttpServletRequest req  HttpServletResponse res     NOTES  TODO and LIMITATIONS     No Exceptions over RPC   POJO can only have primitive arguments on service methods   No inbuilt authentication  use Servlet specification instead    pom xml has list of depenencies    TODO List as Argument    TODO Prefix based filtering of services  currently exposes entire classpath   chrome style files   Personal collection of Style override for different sites  authorize net standalone   simple authorize net aim without any external dependencies  in one java class    Like  stripe   authorize net provides simple HTTP Post based charging of the credit card  This project is a single java class that provides short hand methods to make a such post requests    Usage   Charging a credit card     Map of login settings  map method is a shorthand to create java Map Map String  String  settings   Authorize map               x Delim Data    TRUE                x login   x login    Get from authorize net              x tran key   x tran key     Get from authorize net              x version   3 1                    actual transaction Map String  String  transaction   Authorize map settings                x type            AUTH CAPTURE                x amount          27 00                 x card num        4111111111111111                 x exp date        1215                 x card code       000                 x invoice num     001                 x address         123 TEST ST                 x zip             75244                   CHARGE Map result   Authorize process Authorize SANDBOX URL  transaction    if result get  Response Code   equals  1         System out println  CHARGED           Notes     The direct response with     delimiter is required  Authorize net web control panel has option to change delimiter to pipe    either copy paste Authroize java file  or jar file after  mvn package  or use maven after  mvn install  to use      Resources     Authorize Net Advance Integration Method  AIM    AIM Manual PDF   semantics3 java   Unofficial Semantics3 Java API   EXAMPLES   ProductsRequest   ProductsRequest products   new ProductsRequest  KEY    SECRET    products setField  cat id   4992   products setField  offset   100   for  Map String  Object  product   products getProducts          System out println product get  name          From semantics3 python   CategoriesRequest categories   new CategoriesRequest key  secret   categories setField  cat id   4992   for  Map category   categories getCategories          System out println category get  name           NESTED QUIERIES         cat id    4992        brand      Toshiba        weight       gte  1000000   lt  1500000         sitedetails               name     newegg com            latestoffers                   currency    USD                price         gte    100                        import static semantics3 APIRequest map   products setField map       cat id   4992       brand    Toshiba        weight   map  gte   1000000   lt   1500000        sitedetails   map           name    newegg com            latestoffers   map               currency    USD                price   map  gte   100                                 TODO     return Navigable List for all results on products and categories   Offers   docdb hm   A key value backend for  docdb  based on java HashMap    About   docdb hm provides a in memory key value  All updates are persisted on an append only file  During the first use  all the key value mappings are read on memory    File Format   File consists of series of entries  Entries can either be add  or delete     1 BYTE FOR META  1 BYTE LENGTH OF KEY  KEY  4 BYTE LENGTH OF VALUE  VALUE    META   add   0 last bit  delete 1 last bit    The value part is not present if delete    Example Use   DocDB db   new HMDB  somefile db       Will be created if not exist  Map post   new HashMap    post put  title   Example Post           db save  post   1   post      id   1     Getting  Map gpost   db get  post   Map class  1   db close       Defragment and File Size   When the file is opened the first time  if the number of entries in file is larger than the twice file is optimized  by rewriting   IMPORTANT   This is an incomplete project  set up to switch between my desktop and laptop  This message will be removed once the code is minimally usable    Goal   Create a simple Key Value data store backed by a file  Based on configuration  can either be cached or inmemory system    NOTES     MMAAP is not used  all reads and writes using plain rear write   Lock system is not implemented     Notes on data HashTable   How keys are stored    A top level structure of the database file      METADATA  FIXED LENGTH KEYSPACE  BLOCKS    An inmemory copy of keyspace with same size is kept   Collection of golang shorthands  Most of the functions are dirty and intended to be used for scripting  Overview   MemKV is a in memory Key value store backed by append only file with following mechanism      Get is served from in memory Go s map implemenation   Set updates the map as well as append to file    Remove removes the entry from Map  appends delete entry to file   Depends on OS to synchronize the file in normal cases   Sync method force syncs the file   Optimize   optimizes the file to trim its size   Entire file is read during Open to populate the in memory map   Not tested extensively  but it can be used by single process and multiple threads     Example   SEE memkv test go    Auto Optimization   Auto optimization of the file occurs when the size of the backing file exceeds by  optimization factor  Default 3   times  Auto optimization is skipped if the file size is less than 64MB  To disable or slow down the auto  optimization  increase  optimization factor  to a large number  Overview   Mirror of  https   code google com p vitess source browse go cache    See original project for License and other info  Copied for the ease of import    Changes     Package name is changed to lrucache from cache to match the project name    lrucache ByteValue is introduced for   byte as value     Usage   package main  import        fmt       github com aniljava lrucache     func main         cache    lrucache NewLRUCache 200       key     foo      value      byte  BAR        cache Set key  lrucache ByteValue value        data       cache Get key      fmt Println data  lrucache ByteValue       bottle gzip   Bottle GZip  clone of bitbucket org ibkon bottle compressor parts   select mergejson    a    b    c   d    e    f   g         a    b    c   k                         mergejson                                                                 a     b     c    k     e     f    g      1 row   parts    psoc4 linux   Collection of scripts to facilitate PSoC4 development in linux    WORK IN PROGRESS  pure   Collection of python functions and shorthands  Intended for copy paste   pure py  contains the functions and docs  pykifu   Python Script to convert SGF games with commentary to PDF Postscript    About   An ad hoc script to convert the sgf game into multipage pdfs  Was created with kindle ebook reader in mind  But can be used for printing and other usage    Raw postscript is generated  Basically lines  stones and texts at appropriate coordinates   The generated postscript needs to be converted to pdf using  ps2pdf   Usage     pykifu  input sgf file sgf    ps2pdf    output pdf file pdf    Samples     Sample pdf  Lee Sedol vs AlphaGo   https   github com aniljava pykifu releases has all other pdfs and sgfs from www gogameguru com untill 05 28 2016     FEATURES     Currently Splits moves based on the comments    If comments are not found  splits moves by next stone distance  upto 10 moves  or every 5 moves   Tested against all the game records at gogameguru com  Works goods with the comment convention used    Recognizes Alfa  Numeric  TR labels   Captured stones are removed on normal page  and shown in last page with index      TODO   LIMITATIONS     Allow command line options to change configurations like splitting  comments   Add support for variations   Split if comments does not properly splits the game   Support other markers like square     Dependencies     gomill    https   github com mattheww gomill    pip install gomill   mako   http   www makotemplates org    pip install mako     License   All games records  pdf  sgf  with commentries are copyrighted work of YOUNGGIL AN at www gogameguru com  licensed under Creative Commons CC BY NC SA 4 0    The script is licensed under Apache License  Please check the official page of License for details  About   Collection of SGF and PDF of 21st LG Cup 1st and 2nd Round   Notes   The collection is complete except for following three matches from first round      tuo jiaxi vs lee taehyeon   zhou ruiyang vs daisuke murakawa   dang yifei vs lin junyan   darkforest variations   Script to generate different variations on each move from a SGF file using Facebook darkforest engine     For each move  if it is different from the one suggested by the engine  a variation tree is generated upto depth  VARIATION DEPTH    default 20     collections  folder has generated samples   EXECUTING SCRIPT   Tested on Ubuntu 15 04     Install torch and darkforestgo  for instructions see  https   github com facebookresearch darkforestGo   Edit   start nn service  and  darkforest gtp  to update the installation folders   Run  start nn service as    start nn service   run generator as  generator source sgf dest sgf   replace source and dest sgf with actual sgf file names   Once generated  stop the neural network service either as  killall luajit  or kill the process id using  ps aux   To change the variation depth from 20 to more or less  edit generator   To change strength  Currently the time limit   edit generator     TODO     Increase the number of variations from 1 branch each move to more depending on win rate on alternative branch   Generate the branch on demand using comments   Copy existing variations from source file to the generated variation  currently only main branch and generated variations are saved on dest    Add metadata such as winrates  possible candidates without variations as a comment     CONVENTIONS     File name convention    event detail    black player  vs  white player    optional meta info  sgf  Eg   ing cup semifinal game 2  park jungwhan vs lee sedol sgf   Player names from gokifu com   Simple cluster framework for python using ZMQ and cloudpickle   Introduction   zero cluster is a very minimal toolkit  intended for non critical day to day cluster tasks  It is composed of only three files  One library support file  zerocluster py  and two scripts  worker  and  router     Features and Limitations     Tasks instances and minimal dependencies are deployed to worker automatically at runtime    Supports deployment of class instances and functions   Same instance of class is used for later invocation  Allows remote state between calls  example DB Connections    Python 2 and 3 Compatible  but can not be intermixed    No error handling  Disconnects  delivery guarantees      Example Task       python import zerocluster   class AddService      def add self  x  y           return x   y   cluster   zerocluster init  tcp   host 8989   service   cluster deploy AddService      job   service add 1 2  job wait   print job result          Setup and Running      Install  pyzmq    cloudpickle  on all nodes of the cluster   Copy  worker  script to all nodes of the cluster   Run  router  script at  host  machine   Run  worker  script at all nodes of the cluster   Run tasks  see example above      Dependencies     pip install pyzmq   pip install cloudpickle     TODO     Error handling   Complete example test script   Allow decorators for auto deployment   Fine grained dependency deployment  Currently using cloudpickle s  main  hack    Scripts to start stop remote workers  SSH and kill signals    Notification of disconnects  Heartbeats    retry policies for remote workers   Task expirations on router and workers   Policy based result retention and transfers  Currently using ZMQ defaults    Fine grained logging   Task cancellation   Common operations in new digital ocean server  git test git test subproject   TEST COMMIT rust cards   Collection of rust topics in a index card   swipe like structure    Directory structure leela variations   Generates variations for each move in given SGF file  The variations are the top branches Leela engine visited before picking the best one among them  Leela suggests the first top move and a expected list of move beyond that  It is not confirmed as further evaluation is done in next move but provides a close picture of moves to come    Running the program  Tested in Ubuntu Linux      If new version of Leela is available  9 0 is included in  leela binary  folder   download the binary and replace contents of  leela binary  folder  Make edit main script  leela variation  to change the name of binary file    Execute scrupt as    leela variations  source sgf   destination sgf      Takes a while to complete the playout producting destination sgf with variations along the way    Running on windows  Not tested      Download windows version of leela and edit script to change the filename  Other things should work without problem    Download GTP Version from   https   sjeng org leela html     Further improvement     Extend variation using actual extended play in branches as well  Will take forever to finish a game though       License   This project is copyleft  The see leela binary LICENSE for leela s license  nvidia 375 39 patch linux 4 10   Patch and guide installing Nvidia 375 39 on Ubuntu 17 04   4 10 kernel  Ubuntu Zesty Zapus   Script copied from   https   devtalk nvidia com default topic 995429 failed installed nvidia with kernel 4 10  Patch and script mirrored here in a case if source patch is removed    If in console  using phone   and can not copy script below  use following   wget goo gl 3c7WU5  o downloader chmod  x downloader   Edit  Copy or Execute     Following script is also available in  downloader      wget http   fr download nvidia com XFree86 Linux x86 64 375 39 NVIDIA Linux x86 64 375 39 run sudo chmod  x   NVIDIA Linux x86 64 375 39 run   NVIDIA Linux x86 64 375 39 run  x   cd   NVIDIA Linux x86 64 375 39 wget https   raw githubusercontent com aniljava nvidia 375 39 patch linux 4 10 master kernel 4 10 patch  patch  p1   kernel 4 10 patch     Ensure Xorg is not running cd      NVIDIA Linux x86 64 375 39 nvidia install  melody graph   Create a video using melody graph of audio       Sample Video  Youtube     Workflow with single sound   music grapher audio file m4a    Helpful settings for instruments     mel minpeaksalience 1 0      mel voicing 5 00       freq min 50      freq max 700    Workflow With Split sound      Create temp directory mkdir  p raw     Split sound 29 seconds each ffmpeg  i audio m4a  f segment  segment time 29  c copy raw sound  3d m4a     produce videos  ls raw   m4a   parallel music grapher       see music grapher  h for all args such as tonic adjustments     merge with sound    Remove old files if exists   rm raw   merged mp4 ls raw sound   m4a   parallel ffmpeg  i    mp4  i      c v copy  c a copy    merged mp4     merge all    Remove old if exists rm parts txt  for f in   raw   merged mp4  do echo  file   f      parts txt  done ffmpeg  f concat  safe 0  i parts txt  c copy final mp4  stm32f4xx dac   USB Sound Card   DAC   Based on ST stm32f4xx Discovery Board  CIRRUS CS43l22 DAC    Overview   Based on SMT Cube Demo  and works of www tjaekel com DiscoveryUSB index html   Installation   Pre compiled binary can be flashed by downloading latest release from releases section   https   github com aniljava stm32f4xx dac releases download 1 0 stm32f4xx dac bin   and flashing as    st flash write stm32f4xx dac bin 0x8000000   Source modification and installation   make clean     make flash    Notes on power   Needs to be powered using miniUSB  CN1   or connecting pin PA9 to 5V like picture below       PIN PA9    PIN 5V stm32f7xx dac   USB Sound Card   DAC   Based on ST 32F746GDISCOVERY Discovery Board  CIRRUS CS43l22 DAC    Overview   Based on SMT Cube Demo  and works of www tjaekel com  scripts aoc 2020   Advent of Code 2020 in rust   Each solution is implemented as rust test cases  Organized by day  src day xx rs     Running     cargo test
85,peteralcock,DEFINITION    Bush leaguer     noun        Also called busher  Baseball    a player in a minor league  an incompetent player  as one who behaves or plays as if he or she belonged in a minor league        a person who performs at an inferior level or in an inferior manner          QUICKSTART   To setup with Eclipse  tested on Ubuntu 14 04     Install JDK Java8  sudo apt get install oracle java8 installer Get Spark      Download 1 1 0 for Hadoop 2 4  We will not be using Hadoop even though this build supports it      Untar the spark tarball   E g   in   dev      Test the installation with   bin run example SparkPi  Right click on pom xml  choose Maven   install  This will now download Spark jars  it will take a while  It will also set your Eclipse project s source level to Java 8     Dataset   ratings csv is generated from ratings ods  which is a spreadsheet for synthesizing data sets to test and fine tune your model  Adjust ratings ods and save as CSV  See readme txt in data directory for instructions     Cassandra   Run Cassandra  sudo  usr bin cassandra Then create schema by running attached SQL as follows      In workspace root  run cqlsh  f   collabfilter src sql collab filter schema sql    Running tests    Run collabfilter CollabFilterCassandraDriver main or the CollabFilterTest unit test     Maintaining a list of domains that IT recruiter scumbags send from makes it easy to filter messages into their own folder   It s easy to process the list into formats suitable for different mail systems  gmail and sieve rulesets are currently supported    GMail support is achieved via filters  you can import gmailFilters xml from the filters page in the interface      Filters have a max query length  so we break them up into chunks of 70    By default we add a  Recruitment  label and archive any matches  you ll probably want to edit the filters created    If you reimport the filters  the originals remain    Update the XML based on domains txt by running domains2gmail rb     Use the  addnew sh  script to add emails and domains   This takes care of duplicate checking   sorting and automatically strips any username  portion of the string you feed it      scumboxer toolbox addnew sh shinysuit noisyrecruiters com   or     scumboxer toolbox addnew sh  p shinysuit noisyrecruiters com   ROADMAP     Impliment text analysis engine to recognize common recruiter communication patterns terms   Centralize domain database to facilitate a crowdsourced blacklist of scumbag company domains   Create auto replier that responds to recruiting spam with an obfuscated link to something awful   Headless Integration Testing NLP JSON API with Rails 4 2   Demonstration of an NLP JSON API using Rails 4 2     Tokenize text with words counted   Extract phone numbers from text   Calculate average word sentence lengths   moneygrubber   Scrape Corporate Investor Relations Leads   Avoid Bot Detection  bin bash run sh Proxsneak   Create multiple proxy instances with load balancing for sneakerbots   Description   It provides one single endpoint for clients  Supports  HAProxy    socks  protocol and  http proxy  servers   polipo    privoxy  and  hpts     In addition  you can  view  previously running  TOR  processes and create a  new identity  for all or selected processes    Introduction   multitor  was created with the aim of initialize many  TOR  processes as quickly as possible  I could use many instances for my daily use programs  web browsers  messangers and other   In addition  I was looking for a tool that would increase anonymity when conducting penetration tests and testing the security of infrastructure    Before using the  multitor  you need to remember      TOR  does attempt to generate a bunch of streams for you already  From this perspective  it is already load balancing  and it s much smarter at it than  HAproxy     the main goal is masking from where we get by sending requests to multiple streams  It is not so easy to locate where an attacker comes from  If you used http https servers e g  proxy servers  you will know what is going on but      using multiple  TOR  instances can increase the probability of using a compromised circuit   multitor  getting some bandwidth improvements just because it s a different way of connecting to  TOR  network   in  multitor  configuration mostly  HAProxy  checks the local  syn  syn ack  socket   not all  TOR  nodes  also exist nodes   If there is a problem with the socket it tries to send traffic to others available without touching what s next   it does not ensure that the data will arrive   TOR  network is a separate organism on which the  multitor  has no effect If one of the nodes is damaged and somehow the data can not leave the exit node  it is likely that a connection error will be returned or  at best  the data will be transferred through another local socket   HAProxy  load balance network traffic between local  TOR  or  http proxy  processes   not nodes inside  TOR  network       TOR  is a fine security project and an excellent component in a strategy of defence in depth but it isn t  sadly  a cloak of invisibility  When using the  TOR   always remember about ssl  e g  https  wherever it is possible      Look also at  Limitations     How To Use      heavy exclamation mark  For a more detailed understanding of  multitor   its parameters  functions and how it all works  see the  Manual       It s simple       bash   Install     setup sh install   Run the app   multitor   init 2   user debian tor   socks port 9000   control port 9900   proxy privoxy   haproxy           symlink to  bin multitor  is placed in   usr local bin   man page is placed in   usr local man man8       Parameters   Provides the following options       bash   Usage      multitor    Examples      multitor   init 2   user debian tor   socks port 9000   control port 9900     multitor   init 10   user debian tor   socks port 9000   control port 9900   proxy socks     multitor   show id   socks port 9000   Options            help                        show this message           debug                       displays information on the screen  debug mode            verbose                     displays more information about TOR processes      i    init                    init new tor processes      k    kill                        kill all multitor processes      s    show id                     show specific tor process id      n    new id                      regenerate tor circuit      u    user                 set the user  only with  i   init            socks port     set socks port number           control port       set control port number           proxy            set socks or http  polipo  privoxy  hpts  proxy server           haproxy                     set HAProxy as a frontend for http proxies  only with   proxy        Limitations     each  TOR    http proxy  and  HAProxy  processes needs a certain number of memory  If the number of  TOR  processes is too big  the oldest one will be automatically killed by the system   Polipo  is no longer supported but it is still a very good and light proxy  In my opinion the best http proxy solution is  Privoxy     Getting Started   This whole thing is containerized into a few core services  CRM  Dashboard  and the Engine    Deploymentu   PostgreSQL stores the records created from crawls  Redis is used in combination with a bloom filter and hiredis client  for performance  to keep track of all the pending crawl jobs and to track previously crawled URLs  ElasticSearch is used to make all of the returned results searchable  with each added record being asynchroniously indexed by the background workers to prevent bottlenecks  You will wanna tune the Redis configuration to use the LRU cache policy  least recently used  for when it needs to choose what links to ultimately abandon after multiple retries  Using LRU will result in punting problematic URLs first when memory starts to bloat on the EC2 instance    Dashboard   The dashboard uses websockets and AJAX to deliver live visual indicators of your crawlers progress        Engine     Background processing engines are broken into separate parts for separate purposes  Social Media crawling is network intensive  cached page crawling is as well  returned data is very write heavy on the I O for RDS instances    Scaling   If you want to crawl millions of websites in an evening with this you will need to use the deployment scripts I ve included for Amazon Web Services  Auto Scaling EC2 clusters  This will cost you a pretty penny  but I ve optimized these scripts to dynamically configure themselves to whatever size EC2 you choose to use by making the deployment script aware of the number of cores and available memory on their server and adjusting the multi threading configuration accordingly  Make sure you database connections match your worker threads counts  10 25 is typically the appropriate range    Testing   Capyabara s headless browser simulations are dependency nightmares  Use my https   github com peteralcock rubylab repo to jump start TDD      Scalable routing  authentication  rate limiting  and usage metrics for any service   Getting started   vagrant up    Using Docker      shell     docker compose up         From Source      shell   Initial Setup   apt get  y update    apt get  y upgrade git clone http   github com eclecticLabs Trafficker cd Trafficker   pip install  r requirements txt sudo pip install forever  g apt get  y   force yes install vim nginx python dev    python flup python pip python ldap expect git memcached     sqlite3 libcairo2 libcairo2 dev python cairo pkg config nodejs npm   pip install django  1 5 12     python memcached  1 53     django tagging  0 3 1     twisted  11 1 0     txAMQP  0 6 2   Graphite   git clone  b 0 9 15   depth 1 https   github com graphite project graphite web git  usr local src graphite web  usr local src graphite web python   setup py install cp conf opt graphite conf   conf  opt graphite conf  cp conf opt graphite webapp graphite local settings py  opt graphite webapp graphite local settings py   Whisper   git clone  b 0 9 15   depth 1 https   github com graphite project whisper git  usr local src whisper cd  usr local src whisper python   setup py install   Carbon   git clone  b 0 9 15   depth 1 https   github com graphite project carbon git  usr local src carbon cd  usr local src carbon python   setup py install   StatsD   git clone  b v0 7 2 https   github com etsy statsd git  opt statsd cp conf opt statsd config js  opt statsd config js forever start  opt statsd stats js  opt statsd config js   nginx   rm  etc nginx sites enabled default cp conf etc nginx nginx conf  etc nginx nginx conf cp conf etc nginx sites enabled graphite statsd conf  etc nginx sites enabled graphite statsd conf   Admin panel   cp conf usr local bin django admin init exp  usr local bin django admin init exp   usr local bin django admin init exp   Logging   mkdir  p  var log carbon  var log graphite  var log nginx cp conf etc logrotate d graphite statsd  etc logrotate d graphite statsd   Daemons   cp conf etc service carbon run  etc service carbon run cp conf etc service carbon aggregator run  etc service carbon aggregator run cp conf etc service graphite run  etc service graphite run cp conf etc service statsd run  etc service statsd run cp conf etc service nginx run  etc service nginx run cp conf  etc graphite statsd conf cp conf etc my init d 01 conf init sh  etc my init d 01 conf init sh   Analytics   cd  opt dashboard  sudo gem install bundler  bundle install dashing start  d   Management GUI   cd  opt webui npm install bin dashboard js build forever start bin dashboard js start   p 8080    Cleanup   apt get clean    rm  rf  var lib apt lists    tmp    var tmp           Usage   Manage your APIs using these endpoints      Management GUI  http   localhost 8080   WebSocket Proxy  http   localhost 7999   API Endpoint  http   localhost 8000   Admin Endpoint  http   localhost 8001     Slammer   A collection of micro services that allows developers to centralize authentication as a RESTful HTTP proxy into LDAP compatible cloud on premises directory systems such as Microsoft Active Directory  It makes use of CAS for enterprise single sign on  which is an open and well documented authentication protocol  It also has a user sync application and a web portal application which can heirarchy authenicated service chains  allowing for the use of multiple credential sources and types    For more information  including instructions on LDAP  Active Directory  the CAS protocal  and setting up Slammer for getting all these things to play nicely with one another for the purposes of your applications  please  visit the wiki   10X AI      Welcome to the machine  bitch      API Endpoints       List candidates   Find a resume   Upload a resume   Editing resumes   Removing candidates   Listing open jobs   Find a specific job   Adding new jobs   Edit a job listing   Filling a position   Detect hotdogs     Models      Candidate model   Job model   Match model       Getting Started   Score any candidates  potential for any tech job listing using the candidate s email address followed by the job ID for the position        GET  score candidate example com 1234  will return the score of the candidates potential for the job with an ID of  1234         GET  jobs  will return a list of all available jobs posted          Optional query parameters       attribute1    when set to true  will only return resources that       attribute2    when set to true  will only return resources that       Finding a specific job     GET  jobs 1  will return the job with an ID of  1       Create new job listings     POST  jobs  creates job      Required parameters       title    title of the job    description    content of the job    email    email of the job      This endpoint will return  201 Created  with the current JSON representation of the job if the creation was a success  See the  Job model  for more info on the payload    Update job listing     PUT  jobs 1  allows changing the job with an ID of  1       You may change any of the required or optional parameters as listed in the  create job  endpoint    This endpoint will return  200 OK  with the current JSON representation of the job if the update was a success  See the  Job model  for more info on the payload    Delete job listings     DELETE  jobs 1  will delete the job with an ID of  1       This endpoint will return  204 No Content  if successful  No parameters are required    Candidates   List all resumes     GET  candidates  will return a  paginated list  of candidates        Optional query parameters       company    when set to true  will only return resources that       title    when set to true  will only return resources that       Find candidate by ID     GET  candidates 1  will return the candidate with an ID of  1       Lookup candidates by email     GET  candidate nerd example com  will return the candidate with an email of  nerd example com       Upload a resume     POST  candidates  creates candidate         Required parameters        name    real name of the candidate     resume    resume text for the candidate     email    email address of the candidate     Required parameters       name    real name of the candidate    resume    resume text for the candidate    email    email address of the candidate      This endpoint will return  201 Created  with the current JSON representation of the candidate if the creation was a success  See the  Candidate model  for more info on the payload    Update resume     PUT  candidates 1  allows changing the candidate with an ID of  1       You may change any of the required or optional parameters as listed in the  create candidate  endpoint    This endpoint will return  200 OK  with the current JSON representation of the candidate if the update was a success  See the  Candidate model  for more info on the payload    Removing candidates     DELETE  candidates 1  will delete the candidate with an ID of  1       This endpoint will return  204 No Content  if successful  No parameters are required    Detect Hotdog     GET  hotdog eatmorehotdogs  will return a percentile value representing the amount of hotdog found in  eatmorehotdogs   GET  hotdog ilikepizza  will return  NOT HOTDOG   because it has nothing to do with hotdog      Data Models   Candidate Resume   json       id    string      company    string      title    string      email    string      phone    string      name    string      resume    text      linkedin    string      Job Listing   json       id    string      title    string      description    text      company    string      email    string      phone    string      url    string       Hiring Match   json      job id    string      candidate id    string      score    float        Deploying to Production   Lamda is an AWS service which lets you run code without provisioning full servers  You pay only for the compute time and not when your code is not running  The price of execution depends of how much memory you allocate and how long it will took to finish your code s execution  Duration is calculated from the time your code begins executing until it returns or otherwise terminates  rounded up to the nearest 100ms  If you need to run the same Lambda function simultaneously 10 times you have to spin up 10 separate Lamba invocations of the same functionality  However next execution of AWS lamda function will be executed on waiting AWS Lambda  no spin up is necessary you just invoke code params on span up one that is not performing anything   and depending on what programming lang you use  Java  C  would have slower cold starts than Python or Ruby    Connecting API Gateway   AWS Lambda has no routing inside as you would have in Ruby on Rails application  What you need to do is to plug routing solution to your individual Lambda Functions  AWS provides another product called AWS API Gateway in which you define what route will call what AWS Lambda   Lambdas  For example      GET  api v1 candidates    calls list candidates AWS Lambda function   POST  api v1 candidates    calls create candidate AWS Lambda function and pass the JSON request       You can also configure proxy routes with   where anything  POST GET PUT DELETED can be directed to a particular Lambda Function   When you call ruby api rb you will start the web server  but in production on Lambda the API Gateway is your web server  You just need to call the Rack part of Sinatra with your AWS Lambda function passing the request params body from the AWS API Gateway  which will proxy any every request to this one AWS Lambda that will spin up and execute one route of the Sinatra application  After the response is returned  Lambda will die  That means if this Sinatra app needs to receive 50 requests  it will spin up 50 AWS Lambda Functions  and next requests  may be  executed on loaded up Lambdas with Sinatra dependancies in memory  If there are 1000 requests concurrently  AWS will try to run 1000 invocations for the same Lambda function  However  if it s 1000 requests second  and each request only needs 200 ms to process  there could only be 200 concurrent invocations at any point of time  You need to have your Lambda functions load up fast otherwise you will have slow response times and pay more than you would with server running 24h a day    Preparing your application   From here onward Sinatra will carry on application execution as normal Sinatra webserver  That means it will find the get   hello world  route and execute the code  Add this before block at the top of the file  As in first step we were setting the ENV variable rack input with the body of API Gateway  Sinatra would not effectively parse the body as it would be in raw JSON format  That s why this block will parse the JSON to hash as would Sinatra normally do with HTTP form params  This will be needed when you do POST  api v1 candidates to create new records  You must set enviroment variables related to the replication of the server  Most important is the setting of the PATH INFO    what will end up in Sinatra routing and the  rack input     what will become params        env          REQUEST METHOD     event  httpMethod         SCRIPT NAME              PATH INFO     event  path               QUERY STRING     event  queryStringParameters               SERVER NAME      localhost        SERVER PORT     443     rack version     Rack  VERSION   rack url scheme      https    rack input     StringIO new event  body            rack errors      stderr       before do   if request body size   0     request body rewind      params   Sinatra  IndifferentHash new      params merge  JSON parse request body read     end end   status  headers  body    app call env          Further Reading     https   aws amazon com blogs compute announcing ruby support for aws lambda    https   github com aws samples serverless sinatra sample   identif ai
86,btran55,Interactive Katacoda Scenarios     Visit https   www katacoda com btran10 to view the profile and interactive scenarios   Writing Scenarios   Visit https   www katacoda com docs to learn more about creating Katacoda scenarios   For examples  visit https   github com katacoda scenario example
88,alexguo,hello pytorch
89,pwelch,Bite Byte   I built this Android application for a college project and to learn the basics of Android development  It gets the current GPS location and locates restaurants in the area of the user by using Google Places     The results are displayed using Google Places JavaScript V3 API and an Android WebView  The html file is stored locally in  the assets folder  To pass the GPS coordinates to the Google API I implemented the Java JavaScript interface in the WebView to allow JavaScript on the webpage access to the variables    This application was tested on an HTC EVO running Android 2 3    References   Android Developers Documentation   Hello  Android by Ed Burnette   Android Developers    Google Map View    Google Code    Google Maps JavaScript API V3    Google Code    Google Places API  Description   Installs a list of programs to make Windows useful    Requirements   Platform     Windows XP   Windows Vista   Windows Server 2003 R2   Windows 7   Windows Server 2008  R1  R2      Cookbooks     windows     Attributes   Set  url  checksum  package name and home  attributes for each program to be installed    Add program to array in recipes default rb     Usage   Download and install  Opscode Chef Full Installer for Windows     Download and install  Git Full Installer   create  c  chef cookbooks   and add cookbook to directory   create  c  chef node json        run list        recipe windows       recipe chef windows programs            create  c  chef solo rb   log level        info log location    STDOUT file cache path  c  chef chef cache  cookbook path    c  chef cookbooks     execute chef solo   c   chef solo  c c  chef solo rb  j c  chef node json   DESCRIPTION   Installs  Jailkit     Platform   The following platforms have been test with this cookbook      ubuntu  10 04      Attributes     default  jailkit    url            Location to fetch jailkit    default  jailkit    version        Version of jailkit  Default set to 2 14   default  jailkit    checksum       sha256sum of the jailkit download      USAGE   Put  recipe chef jailkit   in a run list    Limitations     Only installs jailkit  Does not use any of jailkit s functionality      Tests     MVT  Foodcritic and Travis CI   MVT  Knife Test and Travis CI     Running locally      BUNDLE GEMFILE test support Gemfile rake knife   BUNDLE GEMFILE test support Gemfile rake foodcritic     License and Author   Author   Paul Welch   pwelch2004 gmail com     Licensed under the Apache License  Version 2 0  the  License    you may not use this file except in compliance with the License  You may obtain a copy of the License at   http   www apache org licenses LICENSE 2 0    Unless required by applicable law or agreed to in writing  software distributed under the License is distributed on an  AS IS  BASIS  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND  either express or implied  See the License for the specific language governing permissions and limitations under the License    DESCRIPTION   Installs  Janus  from github using the bootstrap method  Currently only for Ubuntu console    REQUIREMENTS   Platform   The following platforms have been test with this cookbook      ubuntu  10 04 12 04       Attributes     default  janus    packages      list of packages to install   default  janus    users         list of users   default  janus    home dir      home directory location     Development   Development with VM requires  Vagrant  and  VirtualBox     Current versions used in development      Vagrant v1 5 3   Vagrant Berkshelf Plugin v2 0 1   Vagrant Omnibus v1 4 1      Testing   To run Spec tests   bash bundle exec rpsec spec   Contributing     Fork it from the  GitHub Repo   Create your feature branch   git checkout  b my new feature     Commit your changes   git commit  am  Add some feature      Push to the branch   git push origin my new feature     Create new Pull Request     Contributors      nclark        Updated bootstrap link     akalyaev    Added missing package     patcon    Add missing gem package    Description   Installs  LXC  packages    Platform   The following platforms have been test with this cookbook      Ubuntu  12 04      Attributes     set  lxc    packages       List of packages to install for LXC      Usage       recipe lxc        sudo lxc create  t ubuntu  n my container     sudo lxc start  n my container     License   See LICENSE file    Description   Installs  Trouble maker  which is used as a study tool for RHCE    WARNING   DO NOT RUN ON A PRODUCTION MACHINE   Requirements     Vagrant  or a chef node you don t care about      Platforms   Trouble maker is targeted towards RHEL based Linux distrobutions    Cookbook tested on the following      Centos 5 8   Centos 6 3     Usage   After Installation   For Centos 5 3    bash  usr local trouble maker bin trouble maker pl   version RHEL 5   For Centos 6 3    bash  usr local trouble maker bin trouble maker pl   version RHEL 6   License and Author   Author   Paul Welch   pwelch2004 gmail com     Licensed under the Apache License  Version 2 0  the  License    you may not use this file except in compliance with the License  You may obtain a copy of the License at   http   www apache org licenses LICENSE 2 0    Unless required by applicable law or agreed to in writing  software distributed under the License is distributed on an  AS IS  BASIS  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND  either express or implied  See the License for the specific language governing permissions and limitations under the License        Description   Chef cookbook that installs and configures  avahi  the zeroconf software    Usage   Include  avahi  default  recipe in the  run list     To disable the avahi daemon set the attribute like so   node default  avahi    disable service     true   Development   Development requires  ChefDK   Testing   Running tests   bash chef exec rake chef exec kitchen verify   Contributing     Fork it   Create your feature branch   git checkout  b my new feature     Commit your changes   git commit  am  Add some feature      Push to the branch   git push origin my new feature     Create new Pull Request     Source code availabe  here Captain s Log is a Rails application that provides a central place to log infrastructure changes      Development     ruby 2 0 0 p353   Rails 4 0 1     bundle install     rake db migrate   rake db test prepare    rake db migrate RAILS ENV test     rake db seed     bundle exec guard     Demo   demo clogapp com   bash username  user example com password  password123   API Development   Generate API Key from Rails Console        rails console   ApiKey create        Posting to API  curl  X POST  H  Content Type  application json          H  Authorization  Token token  TOKEN           d    event    entry   ENTRY TEXT     http   localhost 3000 api v1 events   Configuration   Deployment   Still under development  Deploy instructions will be written when application is production ready  Clog   Clog is a Ruby gem CLI for the  Captain s Log  Rails application        Configuration   Clog looks for a configuration file   etc clog clog config yml  by default    To specify a different file use   c  path to config     Example clog configuration file      bash   clog config yml     server   http   example com api  api key   API KEY        Usage   bash  clog  m  Restarted apache in staging    Contributing     Fork it   Create your feature branch   git checkout  b my new feature     Commit your changes   git commit  am  Add some feature      Push to the branch   git push origin my new feature     Create new Pull Request   Clog   Clog is a CLI client written in Go for the  Captain s Log  Rails application    Configuration   Clog looks for a configuration file   etc clog clog config json  by default    To specify a different file use   c  path to config     Example clog configuration file      bash   clog config json       server    http   example com api     api key    API KEY          Usage   bash   clog  m  Restarted apache in staging    Contributing     Fork it   Create your feature branch   git checkout  b my new feature     Commit your changes   git commit  am  Add some feature      Push to the branch   git push origin my new feature     Create new Pull Request       Description   Chef Cookbook to install the  Glances Monitoring Tool   Usage   glances  default   Include  glances  in your node s  run list     json      run list          recipe glances  default           Contributing     Fork the repository on Github   Create a named feature branch  i e   add new recipe     Write you change   Write tests for your change  if applicable    Run the tests  ensuring they all pass   Submit a Pull Request     Future Features     Add tunables to configuration file    Add support for other platforms     pwelch dotfiles   Setup      sh git clone https   github com pwelch dotfiles git    dotfiles cd    dotfiles   brew bundle   Setup dotfiles  Use   no for no action   stow git jrnl ruby tmux vim zsh   verbose   run install script to install oh my zsh   vundle vim plugins   scripts install       This will symlink the appropriate files in   dotfiles  to your home directory    Everything is configured and tweaked within     dotfiles     External Programs     GNU Stow   Jrnl   Overview   Ansible Playbook for  avahi  the zeroconf software    Role Variables   Available variables are listed below  along with default values  see  defaults main yml     avahi hostname      ansible hostname     avahi domain   local  avahi useipv4   yes  avahi useipv6   no  avahi install utils  false   Dependencies   None    Example Playbook           hosts  all   roles    avahi             Development   Requires  Vagrant     vagrant up     Contributing     Fork it   Create your feature branch   git checkout  b my new feature     Commit your changes   git commit  am  Add some feature      Push to the branch   git push origin my new feature     Create new Pull Request     Source code availabe  here Overview   Scripts using Mac OS X JavaScript for Automation  JXA    Automate Mac OS X scripts with JavaScript instead of AppleScript   Requires Mac OS X Yosemite   Development   Mac OS X Script Editor can be used to test scripts    Adding Application Object Reference    Script Editor   Window   Library   Add application if not listed   Not all applications have automation API    Running via command line   osascript spotify playpause scpt   Reference     Apple JavaScript for Automation Notes   Getting Started Post   JXA Cookbook   VirustotalAPI   Ruby Gem for  VirusTotal   V3 API   If you want the version 2  check out the gem versions up to  0 4 0         Installation   Add this line to your application s Gemfile    ruby gem  virustotal api    And then execute      bundle    Or install it yourself as      gem install virustotal api    Usage   VirusTotal only allows 4 queries per minute for their Public API  https   www virustotal com en faq    You will need a Private API Key if you require more queries per minute    File Find      ruby require  virustotal api    sha256     01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b  api key    MY API KEY    vtreport   VirustotalAPI  File find sha256  api key    Does the resource have any results    vtreport exists       true   URL for File Report  if it exists    vtreport report url       https   www virustotal com api v3 files 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b    Report results  if they exist  are available via  report   vtreport report  data    attributes    last analysis results    ClamAV          category    undetected    engine name    ClamAV    engine update    20200826      engine version    0 102 4 0    method    blacklist    result   nil    Check whether an Antivirus detected this sample or not   vtreport detected by  ClamAV        false         File Upload      ruby require  virustotal api    file        path to file  api key    MY API KEY    for upload file   vtscan   VirustotalAPI  File upload file  api key    or large file  more than 32MB    vtscan   VirustotalAPI  File upload large file  api key    Virustotal ID of file   vtscan id       01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b 1419454668    Response results are available via  response   vtscan report          data        id    MTkxNDBmMjU4ZGY1OGZiYzZjNmU2ODcyMWNhYjhkZTM6MTU5ODUzMTE5OQ         type    analysis          File Analyse      ruby require  virustotal api    sha256     01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b  api key    MY API KEY    vtrescan   VirustotalAPI  File analyse sha256  api key    Virustotal ID of file   vtrescan id       MTkxNDBmMjU4ZGY1OGZiYzZjNmU2ODcyMWNhYjhkZTM6MTU5ODUzMTE5OQ      Response results are available via  response   vtrescan report          data        id    MTkxNDBmMjU4ZGY1OGZiYzZjNmU2ODcyMWNhYjhkZTM6MTU5ODUzMTE5OQ         type    analysis          URL find      ruby require  virustotal api    url        http   www google com  api key    MY API KEY    vturl report   VirustotalAPI  URL find url  api key    Does the resource have any results    vturl report exists       true   URL for Report  if it exists    vturl report report url       https   www virustotal com api v3 urls dd014af5ed6b38d9130e3f466f850e46d21b951199d53a18ef29ee9341614eaf    Report results  if they exist  are available via  report   vturl report report  data    attributes    last analysis results    Avira          category    harmless    engine name    Avira    method    blacklist    result    clean           URL Upload      ruby require  virustotal api    url        http   www google com  api key    MY API KEY    vturl scan   VirustotalAPI  URL upload url  api key    Virustotal ID of file   vturl scan id       u dd014af5ed6b38d9130e3f466f850e46d21b951199d53a18ef29ee9341614eaf 1598531929    Response results are available via  response   vturl scan report          data        id         u dd014af5ed6b38d9130e3f466f850e46d21b951199d53a18ef29ee9341614eaf 1598531929       type    analysis          IP Find      ruby require  virustotal api    ip         8 8 8 8  api key    MY API KEY    vtip report   VirustotalAPI  IP find ip  api key    Does the resource have any results    vtip report exists       true   URL for Report  if it exists    vtip report report url       https   www virustotal com api v3 ip addresses 8 8 8 8    Report results  if they exist  are available via  report   vtip report report      Hash of report results         Domain Find      ruby require  virustotal api    domain     virustotal com  api key    MY API KEY    vtdomain report   VirustotalAPI  Domain find domain  api key    Does the resource have any results    vtdomain report exists       true   URL for Report  if it exists    vtdomain report report url       https   www virustotal com api v3 domains virustotal com    Report results  if they exist  are available via  report   vtdomain report report      Hash of report results         User Find      ruby require  virustotal api    user key     user key    user id or api key api key    MY API KEY    vtuser report   VirustotalAPI  User find user key  api key    Does the resource have any results    vtuser report exists       true   Report results  if they exist  are available via  report   vtuser report report      Hash of report results         Group Find      ruby require  virustotal api    group id     GROUP id  api key    MY API KEY    vtgroup report   VirustotalAPI  Group find group id  api key    Does the resource have any results    vtgroup report exists       true   Report results  if they exist  are available via  report   vtgroup report report      Hash of report results         Contributors      jonnynux    crondaemon    postmodern    mkunkel    Grandman     Contributing     Fork it   https   github com pwelch virustotal api fork     Create your feature branch   git checkout  b my new feature     Commit your changes   git commit  am  Add some feature      Push to the branch   git push origin my new feature     Create a new Pull Request   Puppet Avahi   Table of Contents     Overview   Setup   The basics of getting started with avahi   What avahi affects       Usage   Configuration options and additional functionality   Limitations   OS compatibility  etc    Development   Guide for contributing to the module   Running Tests         Overview   Installs and Configures  Avahi  zeroconf networking software    Setup   What avahi affects     package service configuration files for Avahi     Usage   ruby class    avahi  init      Limitations   Only Supports Ubuntu Debian   Development     Fork it   Create your feature branch   git checkout  b my new feature     Commit your changes   git commit  am  Add some feature      Push to the branch   git push origin my new feature     Create new Pull Request     Running Tests      ruby   Install Requirements   bundle install   Validate Files   bundle exec rake validate   Run Rspec Puppet   bundle exec rake spec   Run Puppet Lint   bundle exec rake lint       Source code availabe  here       Description   Chef cookbook to install and configure  YARA   the pattern matching swiss knife for malware researchers    Usage   Include  yara  default  recipe in the  run list     To disable installing yara python set the following attribute   default  yara   install yara python    false   Testing         bundle install     Run Unit and Lint Tests   bundle exec rake     Run Test Kitchen Integration Tests   kitchen test       Contributing     Fork it   Create your feature branch   git checkout  b my new feature     Commit your changes   git commit  am  Add some feature      Push to the branch   git push origin my new feature     Create new Pull Request     Source code available  here Overview     Chef cookbook to install  Ruby     This cookbook assumes you want to compile Ruby from source and only have a single Ruby version on your system    Usage   Basic   ruby include recipe  ruby compile  default    Changing Ruby Version   ruby node default  ruby compile   ruby version          2 3 1  node default  ruby compile   major version         2 3  node default  ruby compile   source   checksum     b87c738cb2032bf4920fef8e3864dc5cf8eae9d89d8d523ce0236945c5797dcd  node default  ruby compile   install gems          w  bundler   node default  ruby compile   extra pkgs                Ruby versions updated  here     Contributing     Fork it   Create your feature branch   git checkout  b my new feature     Commit your changes   git commit  am  Add some feature      Push to the branch   git push origin my new feature     Create new Pull Request     Source code available  here PizzaCalc   Util for calculating pizza order requirements    API   bash   curl  X POST http   pizzacalc website api pizza calc 3   Running Locally   Start Sinatra App  rackup  or  bundle exec foreman start   Visit  http   localhost 9292   Development   Run All Tests   bundle exec rake   Local API   curl  X POST http   localhost 9292 api pizza calc 3   Contributing     Fork it   https   github com pwelch pizza calc fork     Create your feature branch   git checkout  b my new feature     Commit your changes   git commit  am  Add some feature      Push to the branch   git push origin my new feature     Create a new Pull Request   URLVoid API Gem   Ruby Gem for  URLVoid API         Installation   Add this line to your application s Gemfile    ruby gem  urlvoid    And then execute      bundle    Or install it yourself as      gem install urlvoid    Usage   URLVoid Host Response      ruby require  urlvoid    api key    MY API KEY    urlvoid   URLVoid  Host info api key   google com     API Response   urlvoid response          details        host    google com       updated    1438544337       http response code    200       domain age    874296000       google page rank    9       alexa rank    1       connect time    0 028592       header size    840       download size    44685       speed download    309084       external url redirect   nil      ip          addr    216 58 211 78         hostname    par03s14 in f14 1e100 net         asn    15169         asname    Google Inc          country code    US         country name    United States         region name    California         city name    Mountain View         continent code    NA         continent name    North America         latitude    37 4192         longitude     122 057       detections     engines     engine    PhishTank     count    1      page load    0 00     API Reponse Detections   urlvoid detections         engines     engine    PhishTank     count    1           URLVoid API Queries Remaining      ruby require  urlvoid    api key    MY API KEY    URLVoid  Stats remaining queries api key       901         Contributing     Fork it   https   github com pwelch urlvoid fork     Create your feature branch   git checkout  b my new feature     Commit your changes   git commit  am  Add some feature      Push to the branch   git push origin my new feature     Create a new Pull Request   pwelch workstation   Resources for setting up workstation and dotfiles   This is a personal fork of the  Chef Pantry Project   Usage   Run   sudo   bin pantry   To perform the installation and run Chef with the default Policyfile rb  use the   c  option    sudo   bin pantry  c   Installing Packages     bin pantry  will use the Policyfile rb to add these attributes to the node  Then update the Policyfile lock json and export the repository    chef update chef export   force zero repo sudo chef client  z   Issues   Need to fix the bootstrap script  If   HOME  chefdk  is owned by root  the permissions need to be fixed or chef will not work   sudo chown  R  USER  HOME  chefdk    Fixes for OS X 10 11   https   github com Homebrew homebrew blob master share doc homebrew El Capitan and Homebrew md   sudo chown  R   whoami  admin  usr local   sudo chown  R  USER  Library Caches Homebrew  audit osx   This cookbook implements recipes that perform a Chef Audit Mode check for OS X 10 10 systems    Not all of the CIS Benchmark security controls have been added  It just uses the benchmarks as suggestions    This cookbook is intended to be used only with Chef s audit mode      Chef Audit Mode   CIS Benchmarks   CIS Benchmarks for OS X 10 10   Misc Scripts   Miscellaneous scripts and code    Links   Links to other repos with useful scripts     AWS Labs   RedShift Utils   Dockerfiles   Repo for Dockerfiles   Usage   Install or pull latest Docker images from Docker Hub      dim       zmap   Description   Chef cookbook that installs and configures  zmap     Usage   Include  zmap  default  recipe in the  run list     Contributing     Fork it   Create your feature branch   git checkout  b my new feature     Commit your changes   git commit  am  Add some feature      Push to the branch   git push origin my new feature     Create new Pull Request     Source code availabe  here Cuckoo     Installs and configures  Cuckoo Malware Sandbox   Overview   Currently it just installs and configures basic Cuckoo Host    Usage   Add the default recipe to the node run list   run list     recipe apt       recipe cuckoo  default       Cuckoo Sandbox Resources will be available at these locations    Cuckoo Web  http   NODE IP ADDRESS    Cuckoo API  http   NODE IP ADDRESS api cuckoo status   Documentation   See the  repo wiki  for documentation resources    Contributing     Fork it   https   github com pwelch chef cuckoo fork     Create your feature branch   git checkout  b my new feature     Commit your changes   git commit  am  Add some feature      Push to the branch   git push origin my new feature     Create a new Pull Request   Inspec macOS     Security checks for macOS using the  InSpec  Auditing and testing framework    Usage         Check   inspec check profile    Run   inspec exec profile       Setup   Install  InSpec  OR Install  ChefDK   References   Checks are pulled from the following sources     CIS Benchmark OSX 10 11     NIST OSX 10 10 Habitat Jaeger Agent   Description   Habitat  plan for  Jaeger Agent   Building   hab studio enter build habitat  Hab Solo   Resources for basic configuration management using the Hab Solo Pattern   Usage       hab studio run  build  vagrant up vagrant ssh cd  src hab solo sudo   bootstrap YOUR HAB ORIGIN YOUR PKG NAME  n   Run it as a service   hab sup run   b svc load pwelch chef base     Advent of Code   Solutions for  Advent of Code terraform do dnsimple   Simple Terraform module that setups a Digital Ocean Droplet and DNSimple Record test workspace     Simple random terraform plan for testing   terraform init terraform plan terraform apply pwelch github io   Local Dev      shell bundle install   bundle exec jekyll serve     Random Test Module Hi there         Software Engineer         Pronouns  he him     I m currently working on software supply chain security      How to reach me      Mastodon   Fediverse   Twitter   Terraform   ServiceNow Demo   Requirements     Terraform     Setup     Install Terraform if you don t already have it   You can use  Homebrew   brew install terraform   Make a copy of the example  envrc file   cp  envrc example  envrc   Edit the   envrc  file to have the correct AWS credentials for the AWS provider you will use   Load the variables in the shell   source   envrc     You should now be able to use the example AWS provider configurations   Usage       terraform init terraform plan   terraform apply     Terraform AWS EC2   Sets up an AWS VPC   Windows EC2 instance with  Chummer  for  Shadowrun   Requirements     Terraform     Setup     Install Terraform if you don t already have it   You can use  Homebrew   brew install terraform   Make a copy of the example  envrc file   cp  envrc example  envrc   Edit the   envrc  file to have the correct credentials for the provider you will use   Load the variables in the shell   source   envrc     Usage      shell terraform init terraform plan   terraform apply
90,jfsantos,mushra ruby   Sinatra based app to store results from mushraJS      This package provides auditory filter implementations in Julia  The following implementations are available      A gammatone filterbank based on Malcolm Slaney s  Auditory Toolbox     a gammatone like spectrogram implementation  based on Dan Ellis   implementation     a modulation filterbank based on the paper titled  Characterizing frequency selectivity for envelope fluctuations   2000   by Ewert and Dau      torch signal   A signal processing toolbox for Torch 7     Fourier Transforms  real   complex   1D  2D  3D    Cosine Transforms  1D  2D  3D    Short time Fourier Transforms   Spectrogram   Hilbert Transform   Complex Cepstral Analysis  Real Cepstrums     Quickstart   Install fftw3 on your OS    OSX  Homebrew    bash brew install fftw   Ubuntu   bash sudo apt get install libfftw3 OR sudo apt get install libfftw3 3   Install torch signal   bash luarocks install https   raw github com soumith torch signal master rocks signal scm 1 rockspec    add sudo for ubuntu    For documentation  go to  http   soumith github io torch signal signal    For examples  see tests  ift6266h14   Personal experiments done for the course IFT6266   Representation Learning at Universit  de Montr al    The course website is http   ift6266h14 wordpress com     My personal journal is hosted at http   www seaandsailor com tag ift6266 html This repository hosts the material used for our internal Summer Workshop  All the material is available under the Creative Commons   Attribution License  except the IPython notebooks used for the Python section  which are under the Creative Commons   Attribution   Share Alike license     The workshop website was generated using the infrastructure provided by Software Carpentry for their bootcamps  note that our workshop is  not  sponsored by Software Carpentry   The scripts used to generate the website are under the MIT license     The material on Python comes from the  Advanced Statistical Computing course by Christopher Fonnesbeck   Some adaptations were done  namely adding some examples and material on the signal processing functions in Scipy    The material on Git is exactly the one used at Software Carpentry bootcamps    The IPython notebooks and Python examples for the Best Practices section are based on the paper   Best Practices for Scientific Computing    by Greg Wilson et al  Additionally  some text excerpts come from a  presentation  about the same paper  prepared by Paul Wilson for a Software Carpentry bootcamp in UW Madison  Thanks to Greg Wilson and Paul Wilson for the suggestions and feedback on this part of the material  WARNING   This is just a hack for now and completely untested     There is only support for a DBN where the first layer is a Gaussian Bernoulli RBM and the subsequent layers are Bernoulli Bernoulli RBMs  The goal is implement methods for training Deep Belief Networks in Julia  We are using the Restricted Boltzmann Machine implementation from  Boltzmann jl  and a HDF5 data source based on an implementation in  Mocha jl   We may either replace the latter with a simpler HDF5 source implementation or implement a training algorithm in a way that s more compatible with Mocha    SRMRpy  a Python implementation of the SRMR Toolbox   The speech to reverberation modulation energy ratio  SRMR  is a non intrusive metric for speech quality and intelligibility based on a modulation spectral representation of the speech signal  The metric was proposed by Falk et al  and recently updated for variability reduction and improved intelligibility estimation both for normal hearing listeners and cochlear implant users    This toolbox is a Python port of  SRMRToolbox   and includes the following implementations of the SRMR metric      The original SRMR metric  used as one of the objective metrics in the  REVERB Challenge      The updated SRMR metric  incorporating updates for reduced variability    A fast implementation of the original SRMR metric  using a  gammatonegram  to replace the time domain gammatone filterbank  The fast implementation can also optionally use the updates for reduced variability      These implementations have been shown to perform well with sampling rates of 8 and 16 kHz  They will run for other sampling rates  but a warning will be shown as the metrics have not been tested under such conditions    Setup   Simply run  python setup py install  from inside the  SRMRpy  folder to install this package and its dependencies    Usage   You can use SRMR as a function or with the  srmr  wrapper  which can be called from the command line  The parameters for the wrapper are the following        positional arguments    path                  Path of the file or files to be processed  Can also be                         a folder    optional arguments     h    help            show this help message and exit    f    fast            Use the faster version based on the gammatonegram    n    norm            Use modulation spectrum energy normalization     ncochlearfilters N COCHLEAR FILTERS                         Number of filters in the acoustic filterbank     mincf MIN CF        Center frequency of the first modulation filter     maxcf MAX CF        Center frequency of the last modulation filter       The  srmr  function accepts the same arguments  and the API is the following    srmr x  fs  n cochlear filters 23  low freq 125  min cf 4  max cf 128  fast True  norm False    where  x  is a Numpy array containing the signal and  fs  is an integer with the sampling rate    References   If you use this toolbox in your research  please cite the reference below     TASLP2010   Tiago H  Falk  Chenxi Zheng  and Way Yip Chan  A Non Intrusive Quality and Intelligibility Measure of Reverberant and Dereverberated Speech  IEEE Trans Audio Speech Lang Process  Vol  18  No  7  pp  1766 1774  Sept  2010   doi 10 1109 TASL 2010 2052247   If you use the normalized version of the metric  please cite the following reference in addition to   TASLP2010       IWAENC2014   Jo o F  Santos  Mohammed Senoussaoui  and Tiago H  Falk  An updated objective intelligibility estimation metric for normal hearing listeners under noise and reverberation  In International Workshop on Acoustic Signal Enhancement  IWAENC   September 2014    Likewise  if you use the CI tailored version of the metric  with or without normalization   please cite this reference in addition to   TASLP2010       TASLP2014   Jo o F  Santos and Tiago H  Falk  Updating the SRMR metric for improved intelligibility prediction for cochlear implant users  IEEE Transactions on Audio  Speech  and Language Processing  December 2014   doi 10 1109 TASLP 2014 2363788   AuditoryFilterbanks   Gammatone and modulation filterbanks in C    plus MEX files for MATLAB Octave    These are multithreaded implementations of gammatone and modulation filterbanks using C  11 and  Eigen   This repository also includes MEX wrappers for MATLAB and Octave    Build instructions   Clone the repository and then do as follows    cd AuditoryFilterbanks mkdir build cd build cmake     make   Eigen must be in your default include path  or wherever you configured CMake to look for include files   On Linux  you can probably install it using your package manager  in Ubuntu Debian  the package is called  libeigen3 dev    For OS X  it is available from  Homebrew   In case you use Windows  you probably have to download the library yourself    For building the MEX files for Octave  use the  build sh  scripts included in the wrapper folders  For building MEX files for MATLAB on Windows  you will need a recent version of Microsoft Visual Studio that supports C  11  VS2013 should do the job   I needed to use the  Source def  files to compile the MEX files directly from Visual Studio  you may need to do that if your MATLAB version does not support the Visual Studio version you have installed   Folk music style modelling using LSTMs   This code was used for the following published works        Sturm  Santos and Korshunova    Folk Music Style Modelling by Recurrent Neural Networks with Long Short Term Memory Units    Late breaking demo at the 2015 Int  Symposium on Music Information Retrieval       Sturm  Santos  Ben Tal and Korshunova   Music transcription modelling and composition using deep learning   in Proc   1st Conf  Computer Simulation of Musical Creativity   Huddersfield  UK  July 2016      seq2seq   A library for sequence modeling and sequence to sequence modeling    RNN lua  allows for forward or backward recurrency  requires fixed sequence length for now  LSTM lua  allows for peephole connections   mushra ruby server   mushra ruby server is a Sinatra based server to store results of tests performed using  mushraJS     maracas is a library for corrupting audio files with additive and convolutive noise  Its objective is to simplify reproducible dataset generation for speech processing  mainly enhancement and ASR     The usage is really simple and based on the  maracas dataset Dataset  class  Here is a short example       python from maracas dataset import Dataset import numpy as np   Make sure this is reproducible   np random seed 42    d   Dataset     All files can be added one by one or by folder  Adding a folder will add   all speech files inside that folder recursively if recursive True    d add speech files   home jfsantos data speech files    recursive True    When adding noises  you can give a  nickname  to each noise file  If you do not   give it a name  the name will be the file name without the   wav  extension   d add noise files   home jfsantos data multichannel noises restaurant ch01 wav   name  restaurant   d add noise files   home jfsantos data multichannel noises cafeteria ch01 wav   name  cafeteria   d add noise files   home jfsantos data multichannel noises traffic ch01 wav   name  traffic     Adding reverb files works like adding noise files   d add reverb files   home jfsantos data RIR sim rir 0 2 1 wav   d add reverb files   home jfsantos data RIR sim rir 0 8 1 wav     When generating a dataset  you can choose which SNRs will be used and how many   files per condition you want to be generated    d generate dataset   6   3  0  3  6     tmp noise plus reverb dataset   files per condition 5      Keras tutorial   This is a basic Keras tutorial  teaching the basics of feedforward  convolutional  and recurrent neural networks  There are also sections on regularization and how to use the Keras backend to write portable code that runs both in Theano and Tensorflow  dragan pytorch   PyTorch implementation of DRAGAN  https   arxiv org abs 1705 07215    Code based on the  original implementation  by the authors    The following repositories were also used as a reference on how to implement the gradient penalty in PyTorch    https   github com t vi pytorch tvmisc   https   github com caogang wgan gp task4     Competition link  http   www cs tut fi sgn arg dcase2017 challenge task large scale sound event detection   Summarization   Requirment  https   github com pytorch pytorch git   strongly recommand use miniconda2 and install pytorch from source    This model can be trained by the using command    bash python main py   cuda   data  data lisa data sheny ParaNews    In order to see word is generated by which memory  demo py is provided    bash python demo py   You need copy a random row from  data lisa data sheny ParaNews test txt as input of demo py   In order to add new model  you may need to modify or change MemorySeq2Seq py file  irasl2018   Code for the paper   Investigating the effect of residual and highway connections in speech enhancement models    For generating the dataset       Download the  IEEE dataset  and save it to data IEEE dataset  Copy sentence lists 01 to 67 to IEEE dataset train and 68 to 72 to IEEE dataset test     Download the  DEMAND dataset  and save it to data DEMAND    Run the script  gen noisy dataset py     Use the script  create desc json noisy py  to create a JSON file that defines the dataset for the training script      For training the models  use  main py   For testing the model and generating visualizations  use  test model vis py   Both scripts have a list of parameters that define the model type  number of blocks  GRU layers per block  and other parameters    This code was tested using PyTorch 0 4 1  For generating the dataset  you will need my library  maracas   ataritools   Tools to convert text files from ASCII to ATASCII   These are a couple of tools to convert text files from ASCII to ATASCII and back   ATASCII is different from ASCII mainly in that it does not have lowercase characters and the new line character is  155  instead of  10   there are other differences  but they do not affect most Atari BASIC code     This project provides two command line tools to convert files back and forth      asc2atari input bas OUTPUT LST  converts an ASCII file into ATASCII  that you can  e g   load using  ENTER     atari2asc INPUT LST output bas  converts an ATASCII file into ASCII  so you can edit it on your computer       Simulations of guitar amplifiers and effects in Python    Audio files on  data         chords wav   recorded by myself  public domain       riff wav   From  Freesound   CC0 license       ir wav   Impulse response from a Fender Deluxe Reverb  downloaded from  Soundwoofer   public domain  seguranca rpg   Esse reposit rio cont m a tradu  o parcial do guia TTRPG Safety Toolkit  por Kienna Sunrise  traduzido com permiss o   bem como a submiss o ao 200 Word RPG Challenge sobre CATS  de Patrick O Leary  dispon vel sob a licen a  CC BY 4 0      Acesse a  p gina  com a vers o mais recente da tradu  o    mais f cil de compartilhar j  que nem todo mundo   um nerd que gosta de ler Markdown    uxnatr   Port of the uxn virtual machine to Atari computers  800 1200XL     This project s objective is to implement an interpreter  and possibly a compiler of sorts  to run uxntal code written for the Uxn virtual machine on Atari computers such as the 800 1200XL  based on the 6502 processor     Interpreter   The interpreter works by parsing an uxntal program and converting its instructions to 6502 instructions  Whenever devices or specific memory addresses are accessed in uxntal  the interpreter will perform the necessary conversions to make the Atari hardware do a similar thing  e g  change the background colour  make a noise  print something   Due to the hardware limitations of Atari computers  not all features of the uxn virtual machine can be supported    Compiler  ish    The compiler will go one step ahead and  instead of parsing the code and running conversions on the fly  will emit 6502 Assembly for each operation being executed in the source uxntal code  One can think of it as an inlined interpreter  Certain optimizations can be performed to reduce code size and etc   although that is not a primary goal of this experiment
91,jordanbangia,HashSearch   Twitter Hashtag Search App for Xtreme Labs   HashSearch searches Twitter for Tweets with a specific Hashtag    BieberTweet   iOS App to Search for Tweets with  Bieber WhatsForLunch   Keep track of your favourite restaurant and follow local foodtrucks MyGradeTracker   MyGradeTracker Android app   Available on the Goolge Play Store    My Grade Tracker is a weighted grade tracker for Android   MGT uses SQLite to store a database of grades and classes   MusicDiff   Music Diff Windows Server App    The windows server side user interface   The  server  app should be installed on the computer that holds the main music library that all other libraries are compared against   Works in conjunction with MusicDiff Android app and MusicDiff Mac OSX app  MusicDiff Android   Music Diff Android App  to be used in conjunction with the MusicDiff Windows Server   Sends information to server  to have a centralized list of where your music is located  PdfSplit   Python Script to Split a Pdf into Single Pages   To use as a script  simply download the pdfsplit py file   To use as a standalone exe  without the use of python   download the dist folder  and run pdfsplit exe out of it  DailyPlay   DailyPlay Android App   Using your Google Play Music library  DailyPlay will download a set lenght of songs  i e  enough music for the subway ride     Currently using  gmusic api  to access Google Play Music  and  mp3agic  for mp3 ID3v1 tagging  project wildfire   Backend Django server for Wildfire   Installation   Requirements   This project assumes that the following are already installed        Python ver 2 7 X   pip ver 6 0    PostgreSQL ver 9 3     Installation   To install  first clone the repo  or unzip the folder     https   github com jordanbang project wildfire git    Then  change into the directory and install the requirements    cd project wildfire pip install  r requirements txt    This will install the required Python packages used within the project    Then  got to the hellodjango directory  and edit the databases section of the settings file to contain your database information   Specifically  you will need to change the user field and password field   If the database is not being run locally  then that information will need to change as well    DATABASES          default              ENGINE      django db backends postgresql psycopg2            NAME    mysite            USER     your username             PASSWORD     your password             HOST    127 0 0 1            PORT    5432              Once the database information is correct  migrate the database schema and run the Django application    python manage py migrate python manage py runserver    This will start the server on localhost  port 8000    The API can now be queried using CURL or a browser  by hitting any of the provided endpoints  such as    http   127 0 0 1 8000 wildfire question   ThesisApp music therapy   This repo is archived and no longer supported    Alzheimer Music Therapy Website for Alzheimers Society of Peel   Built with Hamza Sami and Ali El Hamouly    This website implements a series of forms for online tracking of Alzheimer s clients goals  assessments  and provides views of their progress    The site is hosted on Heroku  after making any kind of database changes  they need to be updated on Heroku   That can be done by running    heroku run python manage py migrate   app App    You can also re add the initial domain goal data by running    heroku run python musictherapy initial data py   app App  ThesisServer   GCM Server for use with ThesisApp MedMS   Break Poverty Hackathon 2015  http   www devswithoutborders org breakpoverty   NoteName   NoteName is simple python script to combine a list of images into a pdf   It uses reportlabs python package and pdfgen to create the pdf    NoteName can be run using the following command    python images2pdf py name   images image1 jpg image2 jpg image3 png    Or it can be run using a directory  only including the image files found    python images2pdf py name   dir path to some directory    The result is a pdf in the current working directory  Interview   These documents contain a study guide for doing interviews    Table of Contents     General   Bitwise Operations   Arrays  Strings  and Dictionaries   Linked Lists   Stacks and Queues   Trees   node uno   An Uno clone written in Typescript using React for the frontend and Express for the backend  fastmac     Get a MacOS for Linux shell  for free  in around 2 minutes     I don t have a Mac  but I often want to test my software on a Mac  or build software for folks using Macs  Rather than shelling out thousands of dollars to buy a Mac  it turns out we can use GitHub Actions to give us access to one for free   fastmac  makes this process as simple as possible  Note that this only gives us access to a terminal shell  not a full GUI  See below for how to get started   Click here  for a little video that shows all the steps    Clone template   First   click here  to create a copy of this repo in your account  Type  fastmac  under  repository name  and then click  Create repository from template   After about 10 seconds  you ll see a screen that looks just like the one you re looking at now  except that it ll be in your repo copy    NB   Follow the  rest of the instructions in repo copy you just made  not in the  fastai fastmac  repo    Run the  mac  workflow   Next   click here  to go to the GitHub actions screen for the  mac  workflow  and then click the  Run workflow  dropdown on the right  and then click the green  Run workflow  button that appears      Access the shell using ssh or browser   After a few seconds  you ll see a spinning orange circle  Click the  mac  hyperlink next to it    On the next screen  you ll  see another spinning orange circle  this time with  build  next to it  Click  build     This will show the progress of your Mac that s getting ready for you  After a while  the  Setup tmate session  section will open  and once it s done installing itself  it will repeatedly print lines like this      WebURL  https   tmate io t rXbusP3qkYsfALDSLMQZVwG3d   SSH  ssh rXbusP3qkYsfALDSLMQZVwG3d sfo2 tmate io       Copy and paste the ssh line  e g  ssh rXbusP3qkYsfALDSLMQZVwG3d sfo2 tmate io  in this case  into your terminal  Windows users  I strongly recommend you use  WSL  if possible  and press  Enter     You ll see a welcome message  Press  q  to remove it  and you ll be in a Mac shell  The shell already has  brew  installed  so you can easily add any software you need    Instead of using ssh in your terminal  you can paste the  WebURL  value into your browser  to get a terminal in your browser  Whilst this is adequate if you re in a situation that you can t access a terminal  e g  you have to do some emergency work on your phone or tablet   it s less reliable than the ssh approach and not everything works    Stop your session   Your session will run for up to six hours  When you re finished  you should close it  since otherwise you re taking up a whole computer that someone else could otherwise be using    To close the session  click the red  Cancel workflow  on the right hand side of the Actions screen  the one you copied the  ssh  line from     Use a Linux  Ubuntu  shell   If you need to access a Linux shell  instead of MacOS  follow all the same steps as above  except  click this link  instead of the one mentioned above  And click  linux  instead of  mac  to access your session    Using ssh to connect to other servers   fastmac   or linux  can be handy if you re out and about but need to safely access one of your servers  You can ssh from your fastmac linux instance to your servers  First you have to set up a GitHub secret containing the ssh private key needed to connect to your server  To set one up   click here  to create a new secret  name it  SSH KEY   it must be that exact name   and paste your private key file  e g      ssh id rsa   contents as the value  Save your secret  and then when you connect using the fastmac linux steps  you ll find that your terminal has your key ready for use    NB   anyone who has access to your GitHub account can access your  SSH KEY  contents by running this action  Therefore  you should only use a key which is not security sensitive  or at the very least ensure that it is password protected with a strong password    Auto configuration of your sessions   In your  fastmac  repo  edit the  script  linux mac  sh  files to add configuration commands that you want run automatically when you create a new session  These are bash scripts that are run whenever a new session is created    Furthermore  any files that you add to your repo will be available in your sessions  So you can use this to any any data  scripts  information  etc that you want to have access to in your fastmac linux sessions    Behind the scenes   fastmac  is a very thin wrapper around  tmate   so all the features of tmate are available  tmate itself is based on  tmux   so you have all that functionality too  In practice  that means other people can connect to the same ssh session  and you ll all be sharing the same screen  This can be very handy for debugging and support  The integration with Github Actions is provided by  action tmate   advent of code   https   adventofcode com
92,liuchang8am,NeuralTalk2   Recurrent Neural Network captions your images  Now much faster and better than the original  NeuralTalk   Compared to the original NeuralTalk this implementation is  batched  uses Torch  runs on a GPU  and supports CNN finetuning   All of these together result in quite a large increase in training speed for the Language Model   100x   but overall not as much because we also have to forward a VGGNet  However  overall very good models can be trained in 2 3 days  and they show a much better performance    This is an early code release that works great but is slightly hastily released and probably requires some code reading of inline comments  which I tried to be quite good with in general   I will be improving it over time but wanted to push the code out there because I promised it to too many people    This current code  and the pretrained model  gets  0 9 CIDEr  which would place it around spot  8 on the  codalab leaderboard   I will submit the actual result soon      You can find a few more example results on the  demo page   These results will improve a bit more once the last few bells and whistles are in place  e g  beam search  ensembling  reranking     There s also a  fun video  by   kcimc   where he runs a neuraltalk2 pretrained model in real time on his laptop during a walk in Amsterdam    Requirements   For evaluation only   This code is written in Lua and requires  Torch   If you re on Ubuntu  installing Torch in your home directory may look something like     bash   curl  s https   raw githubusercontent com torch ezinstall master install deps   bash   git clone https   github com torch distro git   torch   recursive   cd   torch       install sh        and enter  yes  at the end to modify your bashrc   source    bashrc   See the Torch installation documentation for more details  After Torch is installed we need to get a few more packages using  LuaRocks   which already came with the Torch install   In particular    bash   luarocks install nn   luarocks install nngraph    luarocks install image   We re also going to need the  cjson  library so that we can load save json files  Follow their  download link  and then look under their section 2 4 for easy luarocks install    If you d like to run on an NVIDIA GPU using CUDA  which you really  really want to if you re training a model  since we re using a VGGNet   you ll of course need a GPU  and you will have to install the  CUDA Toolkit   Then get the  cutorch  and  cunn  packages    bash   luarocks install cutorch   luarocks install cunn   If you d like to use the cudnn backend  the pretrained checkpoint does   you also have to install  cudnn   First follow the link to  NVIDIA website   register with them and download the cudnn library  Then make sure you adjust your  LD LIBRARY PATH  to point to the  lib64  folder that contains the library  e g   libcudnn so 7 0 64    Then git clone the  cudnn torch  repo   cd  inside and do  luarocks make cudnn scm 1 rockspec  to build the Torch bindings    For training   If you d like to train your models you will need  loadcaffe   since we are using the VGGNet  First  make sure you follow their instructions to install  protobuf  and everything else  e g   sudo apt get install libprotobuf dev protobuf compiler    and then install via luarocks    bash luarocks install loadcaffe   Finally  you will also need to install  torch hdf5   and  h5py   since we will be using hdf5 files to store the preprocessed data    Phew  Quite a few dependencies  sorry no easy way around it      I just want to caption images   In this case you want to run the evaluation script on a pretrained model checkpoint   I trained a decent one on the  MS COCO dataset  that you can run on your images  The pretrained checkpoint can be downloaded here   pretrained checkpoint link   600MB   It s large because it contains the weights of a finetuned VGGNet  Now place all your images of interest into a folder  e g   blah   and run the eval script    bash   th eval lua  model  path to model  image folder  path to image directory  num images 10   This tells the  eval  script to run up to 10 images from the given folder  If you have a big GPU you can speed up the evaluation by increasing  batch size   default   1   Use   num images  1  to process all images  The eval script will create an  vis json  file inside the  vis  folder  which can then be visualized with the provided HTML interface    bash   cd vis   python  m SimpleHTTPServer   Now visit  localhost 8000  in your browser and you should see your predicted captions    You can see an  example visualization demo page here     Running in Docker   If you d like to avoid dependency nightmares  running the codebase from Docker might be a good option  There is one  third party   docker repo here      I only have CPU    Okay  in that case download the  cpu model checkpoint   Make sure you run the eval script with   gpuid  1  to tell the script to run on CPU  On my machine it takes a bit less than 1 second per image to caption in CPU mode    Beam Search   Beam search is enabled by default because it increases the performance of the search for argmax decoding sequence  However  this is a little more expensive  so if you d like to evaluate images faster  but at a cost of performance  use   beam size 1   For example  in one of my experiments beam size 2 gives CIDEr 0 922  and beam size 1 gives CIDEr 0 886    Running on MSCOCO images   If you train on MSCOCO  see how below   you will have generated preprocessed MSCOCO images  which you can use directly in the eval script  In this case simply leave out the  image folder  option and the eval script and instead pass in the  input h5    input json  to your preprocessed files  This will make more sense once you read the section below      I d like to train my own network on MS COCO   Great  first we need to some preprocessing  Head over to the  coco   folder and run the IPython notebook to download the dataset and do some very simple preprocessing  The notebook will combine the train val data together and create a very simple and small json file that contains a large list of image paths  and raw captions for each image  of the form       file path   path img jpg   captions    a caption                  Once we have this  we re ready to invoke the  prepro py  script  which will read all of this in and create a dataset  an hdf5 file and a json file  ready for consumption in the Lua code  For example  for MS COCO we can run the prepro file as follows    bash   python prepro py   input json coco coco raw json   num val 5000   num test 5000   images root coco images   word count threshold 5   output json coco cocotalk json   output h5 coco cocotalk h5   This is telling the script to read in all the data  the images and the captions   allocate 5000 images for val test splits respectively  and map all words that occur    5 times to a special  UNK  token  The resulting  json  and  h5  files are about 30GB and contain everything we want to know about the dataset    Warning   the prepro script will fail with the default MSCOCO data because one of their images is corrupted  See  this issue  for the fix  it involves manually replacing one image in the dataset    The last thing we need is the  VGG 16 Caffe checkpoint    under Models section   16 layer model  bullet point   Put the two files  the prototxt configuration file and the proto binary of weights  somewhere  e g  a  model  directory   and we re ready to train    bash   th train lua  input h5 coco cocotalk h5  input json coco cocotalk json   The train script will take over  and start dumping checkpoints into the folder specified by  checkpoint path   default   current folder   You also have to point the train script to the VGGNet protos  see the options inside  train lua      If you d like to evaluate BLEU METEOR CIDEr scores during training in addition to validation cross entropy loss  use   language eval 1  option  but don t forget to download the  coco caption code  into  coco caption  directory    A few notes on training   To give you an idea  with the default settings one epoch of MS COCO images is about 7500 iterations  1 epoch of training  with no finetuning   notice this is the default  takes about 1 hour and results in validation loss  2 7 and CIDEr score of  0 4  By iteration 70 000 CIDEr climbs up to about 0 6  validation loss at about 2 5  and then will top out at a bit below 0 7 CIDEr  After that additional improvements are only possible by turning on CNN finetuning  I like to do the training in stages  where I first train with no finetuning  and then restart the train script with   finetune cnn after 0  to start finetuning right away  and using   start from  flag to continue from the previous model checkpoint  You ll see your score rise up to about 0 9 CIDEr over  2 days or so  on MS COCO     I d like to train on my own data   No problem  create a json file in the exact same form as before       file path   path img jpg   captions    a caption                  and invoke the  prepro py  script to preprocess all the images and data into and hdf5 file and json file  Then invoke  train lua   see detailed options inside code     I d like to distribute my GPU trained checkpoints for CPU   Use the script  convert checkpoint gpu to cpu lua  to convert your GPU checkpoints to be usable on CPU  See inline documentation for why this separate script is needed  For example    bash th convert checkpoint gpu to cpu lua gpu checkpoint t7   write the file  gpu checkpoint t7 cpu t7   which you can now run with   gpuid  1  in the eval script    License   BSD License    Acknowledgements   Parts of this code were written in collaboration with my labmate  Justin Johnson      I m very grateful for  NVIDIA  s support in providing GPUs that made this work possible    I m also very grateful to the maintainers of Torch for maintaining a wonderful deep learning library  dp   Forked from https   github com nicholas leonard dp Doc at http   dp readthedocs org en latest  rnn torch Details about this assignment can be found  on the course webpage   under Assignment  1 of Winter 2016    cs231n
93,lpcvoid,bmw best2 vm   A Virtual Machine implementation capable of executing BMW s proprietary BEST2 instruction set    This was deprecated by my rewrite in C    which is way more feature complete  https   github com lpcvoid bmw better vm   Intro   I wrote this in order to execute jobs that are located in BMW s   prg files  These jobs are actually programs compiled in BMW s own proprietary instruction set  called BEST2  which is a word game for  BEschreibungssprache f r STeuerger te   or  description language for ECUs  in English  The goal of this project was to create an environment which could run these jobs  in order to be independant from BMW s own software tools  namely EDIABAS    So how does it work    I ll soon commit a sample application which shows how to execute jobs  Please note that this VM is far from done  as very many features of the BEST2 language specification are still missing  It is uncertain if i ll ever finish it completly  Feel free to attempt for yourself    What works already    Simply put  every job that doesn t actually send data to the car works pretty well  Basics such as arithmetic operations  shifts and copy opcodes are implemented  and should work fine with all types of operands  For example  the IDENT jobs work nicely  as do most of the INIT and  LESEN    jobs  For everything else  you ll need to implement the missing opcodes  The VM tells you if it runs into one which isn t implemented yet    Can I use it in a project of mine    Sure  if you want  Just do me a favor and attribute me somewhere     Why Delphi    Dunno  It was my favourite language back when I started this  Nowadays  i d probably write it in C   or something  ogl engine agent   My first OpenGL Engine  written in Delphi    What s this    This is a 3D engine  written in Delphi  It was never meant to be used for games  more for visualization of geospatial datasets  That s why the terrain engine is by far the most advanced feature of this engine    It was supposed to be used in my Aion Online client  which aimed to be a complete replacement for the Aion client  a Korean MMORPG    Please do ignore any swear words you may encounter  since I had a lot of problems concerning Aion s Geometry format  and am too lazy to dig through comments    Show me something cool    Here you go  terrain rendering in action    pw packet stream reader   Hooks the Perfect World client  detours receiving function  and dumps whatever client receives after decryption  I wrote this about a year ago  not even sure if it works anymore  It attempts to find the correct address though  I tried commenting it at interesting places    Use this as is  do whatever the hell you want with it  I m done with PW    Greetings pw emulator   A project that seeks to emulate the server of Perfect World    Abandoned project  Was fun while it lasted  I started this before I went to university  but time was scarse and the market for an emulator died  so I abandoned the project a short while after    What works   Login works  character database works  item deletion rentention  basic map engine  watermaps heightmap checking  a few other small things    This should basically be seen as a proof of concept to implementing a PW emulator  Many approaches I took should be seen as controversial at best    What doesn t work   Pretty much everything else  Also  I have not included my MPPC compressor in this repo  since I do not want it public yet     Pretty pictures                             Use this as is  but do credit me somewhere where nobody will ever read it anyways  such as a readme txt    Greetings bezier casteljau   Just a quick and dirty program that visualizes the bezier casteljau algorithm    Should compile out of the box with any delphi version newer than a couple of years  I used XE2    Click anywhere you wish  three times  and the program will generate a nice function for you    lamoo   A web scraper for Lovoo  a Dating website app    This shows how you can access lovoo over their  pretty bad  api  Just edit your username password    It can scrape profiles into a sqlite database for further analysis    I am not affiliated with Lovoo in any way  Use at own discretion  ogl noobgl   My second OGL Engine  Supports Shadowmapping Phong Lightning Terrain rendering  OGL 3 1 Core    Features   Pretty basic  as it was initially only for some simulation tasks at my old employer      Material file format that describes shaders and texture   Complete Scene graph with world local transform support    Simple shadowmapping engine    Mathematically correct directional ambient lightning   Support for meshes in own ngl format designed to be loaded quickly   Basic terrain rendering   Arcball Freefloat Camera system   Optimization such as Oriented Bounding Boxes for Meshes and Terrain blocks      And some other stuff     I hope you have fun with this  I know the code is pretty crappy at some points    Plans   Probably none  If I get back into OpenGL  I will probably write a new engine as I have learned a bit more about software architecture now    Notes   Requires my rax framework to compile  which is in another repository of mine      Some colorful pictures           py chatserver   A basic chatserver written in Python  Was more of a practice project than anything else    Features     Probably the worst chatserver you ve ever seen  that s for sure   Custom binary protocol for chat and auth   Basic auth   List of online participants   Uses select   multiplexing     Notes   No client included  since frontend programming sucks  elm327 interface   Some basic interface for the ELM327 BT OBD2 Protocol    Features     Attempts to abstract the protocol handling of ELM327  which can be a bit crappy sometimes     No need to deal with string protocol  which is always a plus   I don t expect anybody to actually use it  but maybe it can serve as an example      Notes   It s missing some classes  but I don t want to publish that  Just comment out whatever is missing  it is not vital to the actual operation  booman   BOOlean MANager   What does this do    It parses boolean expressions  This one for example       S0009 S000A  S0006 S0009   S0002 S0003 S0060  S0061 S0062 S012D   S000A S0009 S000A S0029     S0018 S0006   S01E9 S01E0  S0244   You can tell booman what literals  the Sxxxx  are true and false  and you can define what other operators look like in your input data  For example  in the above string      is a NOT expression    Okay and how can I use it    booman was written to be used as a dll on windows  Check the booExportDll stuff  Here is a usage example      init booman boo init       define token types boo set token pairing booTokenType and        boo set token pairing booTokenType or        boo set token pairing booTokenType not        boo set token pairing booTokenType paranthesis close        boo set token pairing booTokenType paranthesis open        boo set token pairing booTokenType literal   S       set token length to 0  thatw ay the tokenizer attempts to determine length automatically boo set literal length 0      To be continued    delphi php serializer   Create serialized PHP structs with Delphi   What does this do    It solves a problem I had some time ago  I wanted to build structs in Delphi Object Pascal which could be deserialized on PHP side  It s nothing much  but maybe somebody out there has exactly this problem too  and saves a bit time  My pleasure    Can you demonstrate    Sure    var serializer   TPHPSerializer  begin serializer    TPHPSerializer Create    serializer Clear     serializer ArrayStart Length obj configs    for sw    0 to Length obj configs    1 do begin   serializer AddInt sw      array index    serializer CreateClass  EQUIP LEVELUP CONFIG config t   3     serializer AddField  req exp      serializer AddInt obj configs sw  req exp     serializer AddField  addon group      serializer AddInt obj configs sw  addon group     serializer AddField  money cost      serializer AddInt obj configs sw  money cost     serializer CloseClass  end  serializer CloseClass     close array    Result can then be retrieved by    serializer GetFinalSWON       You can then use this string with unserialize   PHP call  and like magic  have the object you described with delphi    Cheers  wxwidgets solit   A simple and quick implementation of the French board game  Solitaire  using wxwidgets  All in one cpp file  That s not normally my style  but this was more a quick test regarding wxwidgets  I hope it may be useful for somebody    Abandoned software    I don t use Windows anymore  nor do I own the laptop this was developed for anymore  Please fork if you wish to continue development      Knife   An open source  lightweight customization software for your Razer Blade      Should work on every Blade laptop currently avaliable  You can also add support for other Razer keyboards via the devices csv file    Why    I love my Blade  but I dislike Synapse  Bloated  needs Internet access  and looks like something a 13 year old would design  in my opinion  Knife aims to be something that your grandpa would design   small  gets the job done and out of your face afterwards    Knife uses 4 4MB RAM when running  and no visible levels of CPU    How    First  we need to change the USB driver to WinUSB  since that s easy to work with  I recommend using Zadig  https   zadig akeo ie      Change the driver of the Razer Device  Interface 2 on my Blade 2018 Advanced  to WinUSB  by clicking  Install Driver   You may need to enable  List all devices  in the Zadig options first  It should look like this afterwards  note that I already changed to WinUSB for this screenshot       Then simply download the source and compile yourself  or grab a precompiled release   link    The zip contains two files  and exe and a dll   unpack both into the same directory  Run the exe  Done  Does not need any administrator privileges    Please be advised that you need to revert this process for Synapse to work again  Simply uninstall the Razer device in device manager  and tick the  Remove driver software  box when doing so    Credits   This project stands on the shoulder of giants  I want to thank Openrazer for the initial idea  and a chap named  rsandoz for a lot of the headway into making Openrazer work on windows  I just hacked together both of their approaches into an easy to use DLL and GUI which runs on windows    Have fun  Making the KSGER T12 soldering iron safer   I recently bought a KSGER T12 iron due to its good reviews  The PCB itself is already pretty good   beefy isolation slots  dual fused  and easily servicable  but I did two things additionally      Adding a ground connection to the case   If using a connector with ground capabilities  which should be standard for metal cased and not double insulated devices such as this   the device grounds the tip for ESD safety  The case is not grounded though  so I recommend a small modification      Increase isolation distance between heatsink and trace   As title suggests  I cut away a small part of the heatsink for the mosfet  so there s a larger distance to the live trace  Here is the problem      Removed the heatsink      I chose to cut three fins  that gave enough clearance      Resolder and screw back in place  You might also want to add thermal paste  that way both TO 220 packages won t get as hot  Afterwards  I think this is a extremly good iron that s very cheap  and if something craps out it s easy to replace  bmw better vm   Better reimplementation of my BEST2 Virtual Machine   This project is a reimplementation of my first VM which I wrote in Pascal  It can now successfully execute a broad amount of jobs  Please note that some opcodes are not implemented  as I have not stumbled upon them yet    For  ongoing  documentation about the instruction set  please see my blog post  https   lpcvoid com blog 0005 best2 instructionset index html   This VM was used as a backend for my dissasembler  which I also open sourced here  bmw best2 disassembler   Disassembler project for the proprietary BEST2 Instruction set     This is my disassembler written for analysing the BEST2 instruction set  It uses wxwidgets for platform independent GUI rendering  Used with my BEST2 VM which I also released  lpcvoid github io   This is my website  It runs under lpcvoid com  or lpcvoid github io  to which the CNAME points   I use blogc to compile it to static html pages  This happens in the site directory using a makefile  Blogposts are written in blogc markup  hackerrank solutions   My solutions to various Hackerrank problems  Some may be missing  i ll try and upload all of them from now on though  strahler   simple 3d raytracer just for giggles wows deobfuscator    Work in progress  Deobfuscator for wows pyc files   Usage   python deobfuscator py  INPUT    Description   This aims to deobfuscate a single  pyc file from WoWs  I described some of the inner workings on my blog   here  and  here      It will attempt to create multiple files  all of which correspond to a certain obfuscation layer  Please be aware that the final deobfuscation   decrypted stage3 fixed pyc   will not be decompileable via uncompyle6 yet  as it stil contains junk code in a few places and this thing is not intelligent enough to NOP that yet  valheim tools   Just some valheim tools just for personal use    building   Requires cmake and a non prehistoric c   compiler    mkdir build cd build cmake  DCMAKE BUILD TYPE Release    make   playerdb reader   Prints information about a given player database   portal finder   Prints information about all portals in the world and reports if any portals are unconnected   mapdatasync   Takes a list of character  fcl  files  and synchronizes the exploration state of the map between them   Generates new files which can then be replaced on each players computer  Does not change anything else   cpp uri parser   This is a small  modern header only URI parsing library for C       Why    My main motivation for writing this is that all the other libs I found where part of large frameworks   which is not a bad thing  but I didn t  want to include a lot of other things in my project    Tests   There are tests which attempt to cover edge cases  You can run them by  cloning the repo  with submodules  so add    recurse submodules  when cloning   and then running    cmake  B build cmake   build build    This will build the tests in a subdirectory called  build    You can then execute  cpp uri parser tests   cpp net lib     Modern  header only  compact and cross platform C   network sockets library   Don t mind the crappy name  I suck at naming things    Why    I needed a small  portable  and easy to use networking library   For years  I just used the POSIX socket api directly  since it s easy and portable  some small platform differences notwithstanding   Since I kept reinventing the wheel  I decided to write this library  after trying ASIO  which I found way to heavy for my tastes    How    You can just add this repo as a git submodule  which at this point is probably  IMO  the best way to handle C   dependencies    shell git submodule add https   github com lpcvoid cpp net lib git extern cpp net lib   This will check out the lib as a submodule within your project  Now just   include  extern netlib hpp   somewhere    Alternatively  you can run the examples and tests like you would any other CMake based project    shell cmake  B build cmake   build build   There are some cmake flags you can use      Option   Default   Description                     BUILD TESTS     ON    Builds tests using  doctest  which is then introduced as a dependency        BUILD EXAMPLES     ON     Builds some small example programs      Short introduction   The library consists of only a few user facing classes    netlib  client  is a network client implementation that offers async and blocking capabilities  Async is done efficiently via a threadpool    netlib  server  is a network server which can handle client requests in an easy manner via callbacks  It uses a threadpool internally   See the examples for some usecases  it s really self explaining    netlib  endpoint accessor  is used to retrieve information like IP and port from an incoming client connection  See examples    netlib  thread pool  is the threadpool implementation used throughout this lib  You can use it for other stuff too   if you like    netlib  socket  is a platform independent socket wrapper over the POSIX socket api    netlib  server response  is a struct that you can return in your server callbacks  which instructs the server how to handle your response  You can pass it some data to relay to clients  or instruct server to terminate the connection  after sending data  if any     Examples   In all of these examples  I assume you have  using namespace std  chrono literals   somewhere  If you don t want to  pollute  the  std  namespace  you need to instead use something like  std  chrono  milliseconds n      Synchronous client  blocking       c   netlib  client client  std  error condition connect result    client connect  example com                       can also be an ip                 http                              can also be a uint16 t port                netlib  AddressFamily  IPv4        use IPv4                netlib  AddressProtocol  TCP       use TCP since we are interested in http                1000ms                             timeout   if  connect result        std  cerr     Connection failed  Error       connect result message      std  endl      return      static const std  string basic get   R  GET   HTTP 1 1 r nHost  example com r n r n    std  pair  send res                                client send  basic get begin    basic get end     1000ms   if  send res second        std  cerr     Sending failed  Error       send res second message      std  endl      return      std  pair   std  error condition   recv res   client recv 2048  3000ms   if  recv res second        std  cerr     Recv failed  Error       recv res second message      std  endl      return    std  string website recv res first begin    recv res first end     std  cout    website    std  endl        Asynchronous client  non blocking       c   netlib  client client  auto connect future   client connect async  example com                                            static cast  80                                            netlib  AddressFamily  IPv4                                           netlib  AddressProtocol  TCP                                           10000ms   while  connect future wait for 10ms     std  future status  ready           do something cool while waiting for connect to finish   std  error condition connect error   connect future get    if  connect error        std  cerr     Connection failed  error       connect error message      std  endl      return      static const std  string basic get   R  GET   HTTP 1 1 r nHost  example com r n r n    auto send future   client send async  basic get begin    basic get end    1000ms   while  send future wait for 10ms     std  future status  ready           do something cool while waiting for send to finish   std  pair  send result   send future get    if  send result second        std  cerr     Send error  error       send result second message      std  endl      return    std  cout     We sent      send result first      bytes      std  endl    auto recv future   client recv async 2048  3000ms   while  recv future wait for 10ms     std  future status  ready           do something cool while waiting for data to come in   auto recv result   recv future get    std  cout     We got      recv result first size        bytes      std  endl  std  string website recv result first begin    recv result first end     std  cout    website    std  endl        Echo server   This code sample creates a TCP server on localhost 1337  which will echo whatever you send to it  Each of the callbacks will be executed by a random thread internally     You may want to disconnect each client after they get their echo   this can be done by setting   terminate  to  true  in the  server response  struct you return from within your callback  Leaving the  answer  vector empty will not send any data    c   netlib  server server  server register callback on connect        netlib  client endpoint endpoint     netlib  server response       std  string ip   netlib  endpoint accessor  ip to string endpoint addr  endpoint addr len  value        std  cout     Client connected  IP       ip    std  endl      return           server register callback on recv        netlib  client endpoint endpoint        const std  vector uint8 t   data     netlib  server response      std  cout     Client sent some data  echoing it back      std  endl      return   answer   data         std  error condition server create res   server create  localhost                                                           static cast uint16 t  1337                                                           netlib  AddressFamily  IPv4                                                         netlib  AddressProtocol  TCP   if  server create res        std  cerr     Error initializing server       server create res message      std  endl    gnat   A gnat is any of many species of tiny flying insects     Reasonably  modern C   wrapper around mosquitto  a C library for MQTT    I didn t like mosquittopp very much  so I made my own  very small  wrapper around mosquitto  Comes with a few tests and works well for me and my very limited usecase    Building   In case you want to run tests  please clone this repo with submodules so you also get  doctest   Then execute    shell cmake  B build cmake   build build
94,jsenellart,CAMFR   Forked from  Sourceforge project  for maintenance    Originally written by  Peter Bienstman at Ghent University  Belgium     Introduction   CAMFR  CAvity Modelling FRamework  is a Python module providing a fast  flexible  full vectorial Maxwell solver for electromagnetics simulations  Its main focus is on applications in the field of nanophotonics  like   wavelength scale microstructures  like photonic crystal devices  optical waveguides    lasers  like vertical cavity surface emitting lasers    light emitting diodes  like resonant cavity LEDs    It is based on a combination of eigenmode expansion  EME  and advanced boundary conditions like perfectly matched layers  PML       Using an intuitive python scripting interface one can create and solve for the optical modes fields in various wavelength scale structures  Additional math and plotting can then be performed via the SciPy stack  The Eigenmode Expansion  EME  method is especially well suited to solving for very thin layers  or structures in which the X and Y dimensions are very different  where typical methods like FDTD and FEM have trouble with the vastly differing X Y discretization    You can find more information  publications and details  here     Features   CAMFR was a research project  started at the photonics group of the Department of Information Technology  INTEC  at Ghent University in Belgium  CAMFR can be used to calculate   the scattering matrix of a structure   the field inside a structure  for any given excitation   band diagrams of an infinite periodic structure   threshold material gain and resonance wavelength of laser modes   the response to a current source in an arbitrary cavity   structures terminated by a semi infinite repetition of another structure   This functionality is currently available for two types of geometries    2D Cartesian structures   3D cylindrical symmetric structures   Additionally  there is code to model the extraction from light emitting diodes  either planar devices  or 3D devices which incorporate 2D periodic structures    Defining structures is quite straightforward  either layer by layer  or using geometric primitive shapes  There are also integrated plotting routines for rapid simulation feedback    One of the main benefits of the Eigenmode Expansion  EME  method is that very thin  nm  and thick  um  structures can be combined without incurring significant numerical errors  as is often the case for Finite Difference meshing  in which large differences in the X Y grids cause calculation problems  Also  once the modes scattering matrices of a 1D slab or 2D Section have been calculated  extending those regions over an additional dimension does not require large amounts of computational power  as most of the work was in calculating the initial eigenmodes of the structure  This means that repeating structures can be simulated fairly quickly  since the eigenmodes are only calculated once for a repeating section    Framework Module Character   CAMFR is utilized as a Python module  although internally it is conceived as a C   framework  with all the algorithms implemented in terms of abstract waveguides and scatterers  This makes it extremely easy to extend CAMFR to new geometries    The end user does not deal with this C   code directly  but rather through bindings to the Python scripting language  This makes the code very clear and flexible  and allows e g  to seamlessly integrate CAMFR with Python aware visualistion tools such as  matplotlib  and  numpy     Examples   Silicon Waveguide Mode Solver   Silicon waveguide  Power  Ex and Ey plotted with matplotlib      See the file  examples contrib Example   Silicon Waveguide ModeSim v2018 01 py  for a full working example    Brief Example   Example of rectangular waveguide construction syntax  We will create a rectangular waveguide of SiO2 cladding and Silicon core  calculate the first 4 modes mode   plot them          import camfr                  import the module    First  create some Materials with some refractive index        SiO   camfr Material  1 45        refractive index of SiO2     Si   camfr Material  3 4        refractive index of Silicon    Then  create some 1 D slabs  by calling those Materials with a thickness value  and adding them together from bottom to top in a Slab        clad   camfr Slab   SiO 15 75            Thicknesses in microns     core   camfr Slab   SiO 10 0    Si 2 5    SiO 5 0        This created an imaginary  Slab  structure from bottom to top  For example  core  looks like            top                                       SiO     5 0 um thick                              Si    2 50 um thick                              SiO    10 0 um thick                             bottom    Then make a 2 D structure by calling these Slabs with a width value  and adding them together from left to right in a Waveguide        WG   camfr Section   clad 3 0    core 1 0    clad 4 0         Widths in microns    Which creates this imaginary 2 D Waveguide structure from left to right                                top                                                                            3 0um              1 0um              4 0um                                    SiO                                                   5 0 um thick                                                                                                        SiO                 SiN               SiO               15 75um          2 50 um thick        15 75um              thick                                 thick                                    SiO                                                  10 0 um thick                                                                                                          bottom    You can then have CAMFR calculate the modes as so        WG calc      And plot the modes like so        WG plot       plots the fundamental mode with MatPlotLib      fig   WG plot field   P   Ex   Ey    mode  0 1 2       plots the Power and fields of 3 modes    See the Examples directory for full examples  as some details are missing here    Installation   CAMFR currently only supports Python 2 7    To use CAMFR  download one of the released versions  see the  releases  or  tags  section of this github repo   or the bleeding edge code  and extract the archive into a directory   Follow the instruction in the  INSTALL  text file for your system   You will have to compile the CAMFR library  as it compiles C code to generate the Python library   A number of dependencies are required  which you can hopefully install easily through your system s package manager  or download directly from the developer websites    The preferred method to run your scripts is through a Python IDE like Spyder  a matlab like IDE    The simplest installation of Spyder  along with all required scientific python modules  can be accomplished via  Python x y    Win  or  Anaconda   Mac Win Linux   or from source  for example via MacPorts  port install py27 spyder  on Mac OS     CAMFR scripts can also be run like any typical Python script on the command line via  python myScript py  or  python  i myScript py  to make it interactive afterwards    License and support   All the code is released under the GPL  papers   This repo is containing notes and implementation for cherry picked publications of my particular interest    My notes and interpretation are fully personal and might not reflect the authors ideas  The code is available as is and meant for me to better understand the papers and check the results   if you want to take it  or and improve it  you are welcome  luatorch onnx convert   Overview   This repository provides an utility to extract model s  from a serialized lua torch model    t7  file  and convert it them to onnx format      Requirements     You need first to install lua torch as well as the library used in the model that are necessary to load the model from torch  for instance   nn    nngraph    onmt  or your own libraries     Install protobuf lua library from this repository https   github com jsenellart protobuf lua  this version is mandatory since it fixes some issues with the original implementation       You can regenerate the lua interpreter of the onnx proto file by doing    protoc   lua out   onnx proto   This will generate an updated version of  onnx pb lua   Extracting model     th convert lua  t7 test data model1 t7  require nn   force   convert lua  goes through the serialized torch object and looks for supported modules or models  Each of them is converted into  test data model1 onnxdir  directory with the corresponding name    model linear bias onnx model linear nobias onnx   The option   require nn  indicates which library are necessary to deserialize the model  GPU model needs  cunn  installation to be read    Convertors   To perform conversion to onnx  each of the used module must be described with onnx operators  New convertors for specific library can be added in  convertors   Their name must match  onnx class  where  class  is the torch classname of the modules
