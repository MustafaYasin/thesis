{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import copy\n",
    "import pickle\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/myasin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USERNAMES</th>\n",
       "      <th>READMES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alexbw</td>\n",
       "      <td>Sampling Inference for Bayesian HSMMs and HMMs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iassael</td>\n",
       "      <td>DEARanking \\n Proposing a hybrid DEA/Polynomia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bshillingford</td>\n",
       "      <td>id3tag-fix codehackathon2014 \\n Team members:\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hmansell</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clementfarabet</td>\n",
       "      <td>sys \\n Has moved to a more community friendly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ajabri</td>\n",
       "      <td>pytorch-maml \\n This is a PyTorch implementati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>USERNAMES</td>\n",
       "      <td>READMES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>coryf</td>\n",
       "      <td>homebrew \\n Personal homebrew formula Sudoku C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jrodrigomg</td>\n",
       "      <td>SERVIDOR NODEJS PROYECTO 1, SISTEMAS OPERATIVO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>vascofernandes</td>\n",
       "      <td>CustomersVRPF \\n The project consists of two m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        USERNAMES                                            READMES\n",
       "0          alexbw  Sampling Inference for Bayesian HSMMs and HMMs...\n",
       "1         iassael  DEARanking \\n Proposing a hybrid DEA/Polynomia...\n",
       "2   bshillingford  id3tag-fix codehackathon2014 \\n Team members:\\...\n",
       "3        hmansell                                                NaN\n",
       "4  clementfarabet  sys \\n Has moved to a more community friendly ...\n",
       "5          ajabri  pytorch-maml \\n This is a PyTorch implementati...\n",
       "6       USERNAMES                                            READMES\n",
       "7           coryf  homebrew \\n Personal homebrew formula Sudoku C...\n",
       "8      jrodrigomg  SERVIDOR NODEJS PROYECTO 1, SISTEMAS OPERATIVO...\n",
       "9  vascofernandes  CustomersVRPF \\n The project consists of two m..."
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the csv file with all downloaded READMES\n",
    "df = pd.read_csv(\"../user_data_csv/csv_readme_per_user.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply different methods to clean the text e.g. remove appestorph and unnessary signs\n",
    "def clean_df(df):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    df['READMES'] = df['READMES'].str.replace('[^a-zA-Z0-9]', ' ', regex=True).str.strip()\n",
    "    df[\"READMES\"] = df[\"READMES\"].apply(str)\n",
    "    df[\"READMES\"] = df[\"READMES\"].str.replace(r\"[\\\"\\',]\", '')\n",
    "    df[\"READMES\"] = df[\"READMES\"].apply(lambda x: ''.join(ch for ch in x if ch not in set(symbols)))\n",
    "    df['READMES'] = df['READMES'].str.replace('\\d+', '') \n",
    "    df['READMES'] = df['READMES'].str.lower()\n",
    "    df = df.dropna(how='all')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctiation\n",
    "def remove_punctuation(df):\n",
    "    df[\"READMES\"] = df['READMES'].str.replace('[^\\w\\s]','')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remvoe all stopwords from the READMES column\n",
    "def remove_stop_words(df):\n",
    "    # import the stopwords from the nltk library\n",
    "    stop_words = stopwords.words('english')\n",
    "    df[\"READMES\"] = df[\"READMES\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all single characters from the READMES column\n",
    "def remove_single_characters(df):\n",
    "    df['READMES'] = df['READMES'].str.replace(r'\\b\\w\\b', '').str.replace(r'\\s+', ' ')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stemming the READMES form the dataframe column\n",
    "# def stemming(df):\n",
    "#     stemmer= PorterStemmer()\n",
    "#     df['READMES'] = df['READMES'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call all function to clean and preprosses the READMES\n",
    "def preprocess(df):\n",
    "    df = clean_df(df)\n",
    "    df = remove_stop_words(df)\n",
    "    df = remove_punctuation(df) \n",
    "    df = remove_single_characters(df)\n",
    "    df = df[df['READMES'].notna()]\n",
    "    # df = stemming(df)\n",
    "    df.to_csv(\"cleaned_df.csv\", index=False)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ml/8_bkc7k91cb777tm6mxkvfbc0000gn/T/ipykernel_24469/2633761482.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[\"READMES\"] = df[\"READMES\"].str.replace(r\"[\\\"\\',]\", '')\n",
      "/var/folders/ml/8_bkc7k91cb777tm6mxkvfbc0000gn/T/ipykernel_24469/2633761482.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['READMES'] = df['READMES'].str.replace('\\d+', '')\n",
      "/var/folders/ml/8_bkc7k91cb777tm6mxkvfbc0000gn/T/ipykernel_24469/1226715876.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[\"READMES\"] = df['READMES'].str.replace('[^\\w\\s]','')\n"
     ]
    }
   ],
   "source": [
    "df = preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['READMES'].notna()]\n",
    "df.to_csv(\"cleaned_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef word_frequency(df):\\n    word_frequency_dict = {}\\n    for username,readme in zip(df[\"USERNAMES\"], df[\"READMES\"].str.lower()):\\n        word_frequency_dict[username] = {}\\n        for word in readme.split():\\n            if word not in word_frequency_dict[username].keys():\\n                word_frequency_dict[username][word] = 1\\n            else:\\n                word_frequency_dict[username][word] += 1\\n        word_frequency_dict[username] = (sorted(word_frequency_dict[username].items(), key=lambda x: -x[1]))\\n    return word_frequency_dict\\n'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def word_frequency(df):\n",
    "    word_frequency_dict = {}\n",
    "    for username,readme in zip(df[\"USERNAMES\"], df[\"READMES\"].str.lower()):\n",
    "        word_frequency_dict[username] = {}\n",
    "        for word in readme.split():\n",
    "            if word not in word_frequency_dict[username].keys():\n",
    "                word_frequency_dict[username][word] = 1\n",
    "            else:\n",
    "                word_frequency_dict[username][word] += 1\n",
    "        word_frequency_dict[username] = (sorted(word_frequency_dict[username].items(), key=lambda x: -x[1]))\n",
    "    return word_frequency_dict\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USERNAMES</th>\n",
       "      <th>READMES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alexbw</td>\n",
       "      <td>sampling inference bayesian hsmms hmms python ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iassael</td>\n",
       "      <td>dearanking proposing hybrid dea polynomial int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bshillingford</td>\n",
       "      <td>idtag fix codehackathon team members brendan s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clementfarabet</td>\n",
       "      <td>sys moved community friendly repo xlua moved c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ajabri</td>\n",
       "      <td>pytorch maml pytorch implementation supervised...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>USERNAMES</td>\n",
       "      <td>readmes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>coryf</td>\n",
       "      <td>homebrew personal homebrew formula sudoku curses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jrodrigomg</td>\n",
       "      <td>servidor nodejs proyecto sistemas operativos e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>vascofernandes</td>\n",
       "      <td>customersvrpf project consists two main parts ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>seba-1511</td>\n",
       "      <td>arduino chrono arduino based chronometer toosk...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         USERNAMES                                            READMES\n",
       "0           alexbw  sampling inference bayesian hsmms hmms python ...\n",
       "1          iassael  dearanking proposing hybrid dea polynomial int...\n",
       "2    bshillingford  idtag fix codehackathon team members brendan s...\n",
       "4   clementfarabet  sys moved community friendly repo xlua moved c...\n",
       "5           ajabri  pytorch maml pytorch implementation supervised...\n",
       "6        USERNAMES                                            readmes\n",
       "7            coryf   homebrew personal homebrew formula sudoku curses\n",
       "8       jrodrigomg  servidor nodejs proyecto sistemas operativos e...\n",
       "9   vascofernandes  customersvrpf project consists two main parts ...\n",
       "10       seba-1511  arduino chrono arduino based chronometer toosk..."
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"cleaned_df.csv\")\n",
    "df = df[df['READMES'].notna()]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/Users/myasin/nltk_data'\n    - '/opt/anaconda3/envs/thesis/nltk_data'\n    - '/opt/anaconda3/envs/thesis/share/nltk_data'\n    - '/opt/anaconda3/envs/thesis/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mzip_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     85\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/Users/myasin/nltk_data'\n    - '/opt/anaconda3/envs/thesis/nltk_data'\n    - '/opt/anaconda3/envs/thesis/share/nltk_data'\n    - '/opt/anaconda3/envs/thesis/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/myasin/Documents/thesis/project/backend/recommender_system/tdf_if.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/myasin/Documents/thesis/project/backend/recommender_system/tdf_if.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlemmatize_text\u001b[39m(df):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/myasin/Documents/thesis/project/backend/recommender_system/tdf_if.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [lemmatizer\u001b[39m.\u001b[39mlemmatize(w) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m w_tokenizer\u001b[39m.\u001b[39mtokenize(df[\u001b[39m'\u001b[39m\u001b[39mREADMES\u001b[39m\u001b[39m'\u001b[39m])]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/myasin/Documents/thesis/project/backend/recommender_system/tdf_if.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mREADMES\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mapply(lemmatize_text, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/pandas/core/frame.py:9558\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9547\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9549\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[1;32m   9550\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   9551\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9556\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[1;32m   9557\u001b[0m )\n\u001b[0;32m-> 9558\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/pandas/core/apply.py:741\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[1;32m    739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[0;32m--> 741\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/pandas/core/apply.py:868\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 868\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[1;32m    870\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/pandas/core/apply.py:884\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    881\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    882\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[1;32m    883\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 884\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[1;32m    885\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    886\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    887\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    888\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32m/Users/myasin/Documents/thesis/project/backend/recommender_system/tdf_if.ipynb Cell 17\u001b[0m in \u001b[0;36mlemmatize_text\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/myasin/Documents/thesis/project/backend/recommender_system/tdf_if.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlemmatize_text\u001b[39m(df):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/myasin/Documents/thesis/project/backend/recommender_system/tdf_if.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [lemmatizer\u001b[39m.\u001b[39mlemmatize(w) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m w_tokenizer\u001b[39m.\u001b[39mtokenize(df[\u001b[39m'\u001b[39m\u001b[39mREADMES\u001b[39m\u001b[39m'\u001b[39m])]\n",
      "\u001b[1;32m/Users/myasin/Documents/thesis/project/backend/recommender_system/tdf_if.ipynb Cell 17\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/myasin/Documents/thesis/project/backend/recommender_system/tdf_if.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlemmatize_text\u001b[39m(df):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/myasin/Documents/thesis/project/backend/recommender_system/tdf_if.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [lemmatizer\u001b[39m.\u001b[39;49mlemmatize(w) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m w_tokenizer\u001b[39m.\u001b[39mtokenize(df[\u001b[39m'\u001b[39m\u001b[39mREADMES\u001b[39m\u001b[39m'\u001b[39m])]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/nltk/stem/wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlemmatize\u001b[39m(\u001b[39mself\u001b[39m, word: \u001b[39mstr\u001b[39m, pos: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mn\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m     34\u001b[0m     \u001b[39m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     lemmas \u001b[39m=\u001b[39m wn\u001b[39m.\u001b[39;49m_morphy(word, pos)\n\u001b[1;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmin\u001b[39m(lemmas, key\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m) \u001b[39mif\u001b[39;00m lemmas \u001b[39melse\u001b[39;00m word\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[1;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfind(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubdir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mzip_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     82\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/Users/myasin/nltk_data'\n    - '/opt/anaconda3/envs/thesis/nltk_data'\n    - '/opt/anaconda3/envs/thesis/share/nltk_data'\n    - '/opt/anaconda3/envs/thesis/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(df):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(df['READMES'])]\n",
    "\n",
    "df['READMES'] = df.apply(lemmatize_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0.01, stop_words='english')\n",
    "vectors = vectorizer.fit_transform(df[\"READMES\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "termfreq = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Computer vison\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vec = vectorizer.transform([query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cosine_similarity(vectors,query_vec).reshape((-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([257, 220, 195, 329, 259, 338, 171, 150, 225, 148])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.argsort()[-10:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(362,)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "themadarchitect 0.07904647986752661\n",
      "edgarriba 0.06879900769740685\n",
      "intermilan 0.05401933393845209\n",
      "rAm1n 0.05207870749500482\n",
      "amirsaffari 0.05203149885115723\n",
      "mirceamironenco 0.0487829589616814\n",
      "SalemAmeen 0.04647568237169033\n",
      "alxndrkalinin 0.04030317722675246\n",
      "gr33ndata 0.03867786058278458\n",
      "valthom 0.03691244395486756\n"
     ]
    }
   ],
   "source": [
    "# Print Top 10 results\n",
    "for i in results.argsort()[-10:][::-1]:\n",
    "    print(df.iloc[i,0], results[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "themadarchitect 0.07904647986752661\n",
      "edgarriba 0.06879900769740685\n",
      "intermilan 0.05401933393845209\n",
      "rAm1n 0.05207870749500482\n",
      "amirsaffari 0.05203149885115723\n",
      "mirceamironenco 0.0487829589616814\n",
      "SalemAmeen 0.04647568237169033\n",
      "alxndrkalinin 0.04030317722675246\n",
      "gr33ndata 0.03867786058278458\n",
      "valthom 0.03691244395486756\n"
     ]
    }
   ],
   "source": [
    "# Print Top 10 results\n",
    "for i in results.argsort()[-10:][::-1]:\n",
    "    print(df.iloc[i,0], results[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'themadarchitect': 0.07904647986752661, 'edgarriba': 0.06879900769740685, 'intermilan': 0.05401933393845209, 'rAm1n': 0.05207870749500482, 'amirsaffari': 0.05203149885115723, 'mirceamironenco': 0.0487829589616814, 'SalemAmeen': 0.04647568237169033, 'alxndrkalinin': 0.04030317722675246, 'gr33ndata': 0.03867786058278458, 'valthom': 0.03691244395486756}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "recommended_users_plus_score = {}\n",
    "query_vec = vectorizer.transform([query])\n",
    "results = cosine_similarity(vectors,query_vec).reshape((-1,))\n",
    "for name in results.argsort()[-10:][::-1]:\n",
    "    recommended_users_plus_score[df.iloc[name,0]] = results[name]\n",
    "print(recommended_users_plus_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectoriz_dataframe(df):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0.01, stop_words='english')\n",
    "    vectorize_dataframe = vectorizer.fit_transform(df[\"READMES\"])\n",
    "\n",
    "    return vectorize_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jobtitle_recommended(vectorize_dataframe, query):\n",
    "    \n",
    "    query_vec = vectorizer.transform([query])\n",
    "    results = cosine_similarity(vectorize_dataframe, query_vec).reshape((-1,))\n",
    "    \n",
    "    for i in results.argsort()[-10:][::-1]:\n",
    "        print(df.iloc[i,0], results[i])\n",
    "        \n",
    "        recommended_users_plus_score = results[i]\n",
    "    print(recommended_users_plus_score)   \n",
    "    return recommended_users_plus_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_per_user = jobtitle_recommended(vectorize_dataframe, query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (main, Mar 31 2022, 03:38:35) [Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "474cabef28ef5f39850ae0283db27a2b2b64f747313c5bcb210a7b2692fe8216"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
